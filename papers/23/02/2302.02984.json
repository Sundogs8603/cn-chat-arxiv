{
    "title": "Robust Subtask Learning for Compositional Generalization. (arXiv:2302.02984v2 [cs.LG] UPDATED)",
    "abstract": "Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-th",
    "link": "http://arxiv.org/abs/2302.02984",
    "context": "Title: Robust Subtask Learning for Compositional Generalization. (arXiv:2302.02984v2 [cs.LG] UPDATED)\nAbstract: Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-th",
    "path": "papers/23/02/2302.02984.json",
    "total_tokens": 908,
    "translated_title": "组合泛化的鲁棒子任务学习",
    "translated_abstract": "组合强化学习是培养执行复杂任务的策略的一种有前途的方法。在传统方法中，一个高层任务被分解成一系列子任务，并训练单独的策略来执行每个子任务。本文关注的问题是以一种能够执行任何任务的方式训练子任务策略；这里定义的任务是由一系列子任务组成的。我们旨在最大化所有任务的最坏情况表现，而不是平均情况表现。我们把该问题表述为一个二人零和博弈，其中对手挑选子任务序列。我们提出了两个RL算法来解决这个问题：一个是现有多智能体RL算法的改编，适用于我们的情况，另一个是异步版本，能够并行训练子任务策略。我们在两个具有连续状态和动作的多任务环境中评估了我们的方法，并展示了我们的算法在鲁棒性和泛化性方面优于现有的基线算法。",
    "tldr": "本文提出了两种RL算法来解决子任务是否能执行任何任务的问题，并在两个多任务环境中进行了评估，表明这些算法在鲁棒性和泛化性方面优于基线算法。",
    "en_tdlr": "This paper proposes two RL algorithms to address the problem of whether the subtask can perform any task, and evaluates them in two multi-task environments, showing that they outperform baseline algorithms in terms of robustness and generalization."
}