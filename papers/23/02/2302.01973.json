{
    "title": "Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)",
    "abstract": "Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the",
    "link": "http://arxiv.org/abs/2302.01973",
    "context": "Title: Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)\nAbstract: Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the",
    "path": "papers/23/02/2302.01973.json",
    "total_tokens": 956,
    "translated_title": "测量编程语言分布的影响",
    "translated_abstract": "目前用于评估神经代码模型的基准测试只集中在很少的一部分编程语言上，不包括许多流行的语言，例如Go或Rust。为了解决这个问题，我们提出了BabelCode框架，用于基于执行的评估任何语言中的任何基准测试。BabelCode使得可以对模型的内存、运行时间和单个测试案例结果进行新的定性性能调查。此外，我们还提供了一个名为Translating Python Programming Puzzles（TP3）的新代码翻译数据集，该数据集来自Python Programming Puzzles（Schuster等人，2021）基准测试，涉及将专家级Python函数翻译成任何语言。通过对BabelCode和TP3基准测试的研究，我们探讨了在训练数据集中平衡14种语言的分布是否可以提高大型语言模型在低资源语言上的性能。在平衡语料库上训练模型，平均而言，相对于不平衡分布的情况，所有任务和语言的$pass@k$结果平均提高了12.34%。",
    "tldr": "该研究提出了BabelCode框架和Translating Python Programming Puzzles（TP3）基准测试，探讨了平衡训练数据集中14种编程语言分布的影响。结果显示平衡分布有助于大型语言模型在低资源语言上的性能提升。",
    "en_tdlr": "This research presents the BabelCode framework and the Translating Python Programming Puzzles (TP3) benchmark, investigates the impact of balancing the distribution of 14 programming languages in a training dataset, and shows that a balanced distribution can improve the performance of large language models on low-resource languages."
}