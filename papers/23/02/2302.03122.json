{
    "title": "State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)",
    "abstract": "Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia",
    "link": "http://arxiv.org/abs/2302.03122",
    "context": "Title: State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)\nAbstract: Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia",
    "path": "papers/23/02/2302.03122.json",
    "total_tokens": 875,
    "translated_title": "基于状态的安全强化学习：综述",
    "translated_abstract": "尽管强化学习在仿真环境中取得了巨大的成功，但将其应用于实际场景仍然面临许多挑战。其中一个主要关注点是安全性，也就是约束满足。状态约束是实际应用中最常见且最具挑战性的约束之一，这对于许多挑战性任务，如自动驾驶、机器人操作等而言是必要和关键的。本文综述了现有的解决基于状态的约束的强化学习方法，并在状态约束马尔可夫决策过程的框架下，从安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面，讨论了现有方法的联系、差异和权衡。我们还总结了当前方法的局限性并讨论了未来发展方向。",
    "tldr": "本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。",
    "en_tdlr": "This paper provides an overview of existing approaches that address state-wise constraints in reinforcement learning and discusses their connections, differences, and trade-offs in terms of safety guarantee and scalability, safety and reward performance, and safety after convergence and during training."
}