{
    "title": "FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)",
    "abstract": "Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as ",
    "link": "http://arxiv.org/abs/2302.08484",
    "context": "Title: FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)\nAbstract: Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as ",
    "path": "papers/23/02/2302.08484.json",
    "total_tokens": 869,
    "translated_title": "FOSI：混合一阶和二阶优化",
    "translated_abstract": "尽管二阶优化方法非常有效，但在高维空间中计算曲率的困难导致在机器学习中流行的方法（如SGD和Adam）仅使用一阶信息。我们提出了FOSI，一种新颖的元算法，在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能。在每个迭代中，FOSI隐含地将函数分为两个定义在正交子空间上的二次函数，然后使用二阶方法最小化一个函数，使用基本优化器最小化另一个函数。我们证明FOSI收敛，并进一步表明它在一类优化器中改善了条件数。我们的实证评估证明，对于音频分类，迁移学习和物体分类等几个深度神经网络训练任务，将FOSI应用于GD，Heavy-Ball和Adam等算法可以提高收敛速度和优化时间。",
    "tldr": "FOSI是一种元算法，它在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能，并可改善一类优化器的条件数",
    "en_tdlr": "FOSI is a meta-algorithm that efficiently incorporates second-order information during the optimization process to improve the performance of any first-order optimizer and improves the condition number for a large family of optimizers. FOSI can increase the convergence rate and optimization time of several deep neural network training tasks, such as audio classification, transfer learning, and object classification, when applied to algorithms such as GD, Heavy-Ball, and Adam."
}