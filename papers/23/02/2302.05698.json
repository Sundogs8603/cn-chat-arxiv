{
    "title": "Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)",
    "abstract": "Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language",
    "link": "http://arxiv.org/abs/2302.05698",
    "context": "Title: Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)\nAbstract: Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language",
    "path": "papers/23/02/2302.05698.json",
    "total_tokens": 906,
    "translated_title": "用于上下文学习的组合范例",
    "translated_abstract": "大型预训练语言模型已经表现出令人印象深刻的上下文学习能力，其中模型通过输入输出示例作为演示，在不进行任何参数更新的情况下学习执行看不见的任务。上下文学习的性能高度受到所选上下文示例的质量所支配。然而，以前的选择方法基本上是基于简单的启发式，导致性能次优。在这项工作中，我们将上下文示例选择形式化为子集选择问题。我们提出CEIL（Compositional Exemplars for In-context Learning），它通过决定性点过程（DPP）对所给输入和上下文示例之间的交互进行建模，并通过精心设计的对比学习目标进行优化，从而获得来自LM的偏好。我们在来自7个不同自然语言处理任务的12个分类和生成数据集上验证了CEIL，包括情感分析、释义检测、自然语言生成等任务。",
    "tldr": "该论文提出了CEIL（Compositional Exemplars for In-context Learning）框架，利用决定性点过程（DPP）模型处理上下文示例选择问题，从而提高了大型预训练语言模型（LMs）进行上下文学习的性能。"
}