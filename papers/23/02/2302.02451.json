{
    "title": "KDEformer: Accelerating Transformers via Kernel Density Estimation. (arXiv:2302.02451v2 [cs.LG] UPDATED)",
    "abstract": "Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na\\\"ive exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, a",
    "link": "http://arxiv.org/abs/2302.02451",
    "context": "Title: KDEformer: Accelerating Transformers via Kernel Density Estimation. (arXiv:2302.02451v2 [cs.LG] UPDATED)\nAbstract: Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na\\\"ive exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, a",
    "path": "papers/23/02/2302.02451.json",
    "total_tokens": 847,
    "translated_title": "KDEformer: 通过核密度估计加速变换器",
    "translated_abstract": "点积注意力机制在现代深度体系结构（例如Transformer）中对于序列建模起到至关重要的作用，然而，对该模型的朴素精确计算在序列长度上具有二次时间和内存复杂度，在训练长序列模型方面存在阻碍。关键瓶颈是因为在softmax函数的分母中计算分区函数以及在softmax矩阵与值矩阵之间的乘法。我们的关键发现是前者可以被简化为核密度估计（KDE）问题的变种，而高效的KDE求解器可以通过基于子采样的快速矩阵乘积来进一步加速后者。我们提出的KDEformer可以在次二次时间内近似计算注意力，并提供可证明的谱范数界限，而之前的结果只提供逐个元素的误差界限。在实证上，我们验证了KDEformer在准确性，内存和计算开销方面优于其他注意力近似方法。",
    "tldr": "KDEformer通过核密度估计加速变换器的注意力计算，提供了次二次时间内的近似计算，并在实证验证中显示出优异的性能。"
}