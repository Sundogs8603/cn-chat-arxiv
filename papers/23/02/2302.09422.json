{
    "title": "Neural Attention Memory. (arXiv:2302.09422v2 [cs.LG] UPDATED)",
    "abstract": "We propose a novel perspective of the attention mechanism by reinventing it as a memory architecture for neural networks, namely Neural Attention Memory (NAM). NAM is a memory structure that is both readable and writable via differentiable linear algebra operations. We explore three use cases of NAM: memory-augmented neural network (MANN), few-shot learning, and efficient long-range attention. First, we design two NAM-based MANNs of Long Short-term Memory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational powers in algorithmic zero-shot generalization tasks compared to other baselines such as differentiable neural computer (DNC). Next, we apply NAM to the N-way K-shot learning task and show that it is more effective at reducing false positives compared to the baseline cosine classifier. Finally, we implement an efficient Transformer with NAM and evaluate it with long-range arena tasks to show that NAM can be an efficient and effective alternative for scaled dot-produ",
    "link": "http://arxiv.org/abs/2302.09422",
    "context": "Title: Neural Attention Memory. (arXiv:2302.09422v2 [cs.LG] UPDATED)\nAbstract: We propose a novel perspective of the attention mechanism by reinventing it as a memory architecture for neural networks, namely Neural Attention Memory (NAM). NAM is a memory structure that is both readable and writable via differentiable linear algebra operations. We explore three use cases of NAM: memory-augmented neural network (MANN), few-shot learning, and efficient long-range attention. First, we design two NAM-based MANNs of Long Short-term Memory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational powers in algorithmic zero-shot generalization tasks compared to other baselines such as differentiable neural computer (DNC). Next, we apply NAM to the N-way K-shot learning task and show that it is more effective at reducing false positives compared to the baseline cosine classifier. Finally, we implement an efficient Transformer with NAM and evaluate it with long-range arena tasks to show that NAM can be an efficient and effective alternative for scaled dot-produ",
    "path": "papers/23/02/2302.09422.json",
    "total_tokens": 923,
    "translated_title": "神经注意力记忆",
    "translated_abstract": "我们提出了一种新颖的注意力机制视角，将其重新设计为神经网络的记忆架构，即神经注意力记忆（NAM）。NAM是一种可通过可微分线性代数运算进行读写的记忆结构。我们探索了NAM的三个用例：记忆增强型神经网络（MANN）、小样本学习和高效的长程关注。首先，我们设计了两种基于NAM的MANN，分别是长短期记忆（LSAM）和NAM图灵机（NAM-TM），在算法性零样本泛化任务中展现出比其他基线（如可微分神经计算机）更好的计算能力。接下来，我们将NAM应用于N-way K-shot学习任务，并展示其在减少误报的效果上比基线余弦分类器更有效。最后，我们实现了一种带有NAM的高效Transformer，并通过长距离竞技任务对其进行评估，表明NAM可以成为缩放点积的高效和有效的替代方案。",
    "tldr": "神经注意力记忆（NAM）是一种记忆架构，通过可微分线性代数操作可被读写，可应用于记忆增强型神经网络、小样本学习和高效的长程关注。实验证明NAM在各方面均具有优越性能。",
    "en_tdlr": "Neural Attention Memory (NAM) is a memory architecture that is both readable and writable through differentiable linear algebra operations. It can be applied to memory-augmented neural networks, few-shot learning, and efficient long-range attention. Experimental results show that NAM performs better than baselines in various tasks."
}