{
    "title": "AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations. (arXiv:2302.06419v2 [eess.AS] UPDATED)",
    "abstract": "Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.",
    "link": "http://arxiv.org/abs/2302.06419",
    "context": "Title: AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations. (arXiv:2302.06419v2 [eess.AS] UPDATED)\nAbstract: Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.",
    "path": "papers/23/02/2302.06419.json",
    "total_tokens": 755,
    "translated_title": "AV-data2vec: 使用上下文化目标表示的自监督学习音视频语音表示",
    "translated_abstract": "自监督学习已经显示出在语音识别方面具有很大的潜力，通过大大减少构建好的系统所需的标记数据量。然而，现有的方法要么不完全端到端，要么不能同时训练两种模态的联合表示。在本文中，我们引入了AV-data2vec，它解决了这些挑战，并基于预测上下文化表示构建音视频表示，这在单模态情况下取得了成功。该模型使用共享的Transformer编码器对音频和视频进行表示，并可以结合两种模态来改进语音识别。在LRS3上的结果表明，AV-data2vec在所有设置下都比现有方法表现更好，而使用的数据量和模型大小相同。",
    "tldr": "AV-data2vec是一种使用自监督学习来构建音视频语音表示的方法，能够同时训练音频和视频的联合表示，并在语音识别任务中表现出优越性能。"
}