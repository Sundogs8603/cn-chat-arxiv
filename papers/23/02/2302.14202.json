{
    "title": "Mixtures of All Trees. (arXiv:2302.14202v2 [cs.LG] UPDATED)",
    "abstract": "Tree-shaped graphical models are widely used for their tractability. However, they unfortunately lack expressive power as they require committing to a particular sparse dependency structure. We propose a novel class of generative models called mixtures of all trees: that is, a mixture over all possible ($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it is possible to parameterize this Mixture of All Trees (MoAT) model compactly (using a polynomial-size representation) in a way that allows for tractable likelihood computation and optimization via stochastic gradient descent. Furthermore, by leveraging the tractability of tree-shaped models, we devise fast-converging conditional sampling algorithms for approximate inference, even though our theoretical analysis suggests that exact computation of marginals in the MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art performance on density estimation benchmarks when compared against powerful probabili",
    "link": "http://arxiv.org/abs/2302.14202",
    "context": "Title: Mixtures of All Trees. (arXiv:2302.14202v2 [cs.LG] UPDATED)\nAbstract: Tree-shaped graphical models are widely used for their tractability. However, they unfortunately lack expressive power as they require committing to a particular sparse dependency structure. We propose a novel class of generative models called mixtures of all trees: that is, a mixture over all possible ($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it is possible to parameterize this Mixture of All Trees (MoAT) model compactly (using a polynomial-size representation) in a way that allows for tractable likelihood computation and optimization via stochastic gradient descent. Furthermore, by leveraging the tractability of tree-shaped models, we devise fast-converging conditional sampling algorithms for approximate inference, even though our theoretical analysis suggests that exact computation of marginals in the MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art performance on density estimation benchmarks when compared against powerful probabili",
    "path": "papers/23/02/2302.14202.json",
    "total_tokens": 883,
    "translated_title": "所有树的混合模型",
    "translated_abstract": "树形图模型因可计算性而被广泛使用，但不幸的是，它们在表达能力上存在局限性，因为它们需要承诺于特定的稀疏依赖结构中。我们提出了一种新的生成模型类别，称为所有树的混合模型：即对$n$个变量上的所有可能($n^{n-2}$)的树形图模型进行混合。我们展示了如何在可计算的方式下对混合模型(MoAT)进行紧凑的参数化（使用多项式大小的表示方式），从而实现了通过随机梯度下降优化的可计算似然值。此外，通过充分利用树形图模型的可计算性，我们设计了快速收敛的条件采样算法进行近似推理，尽管我们的理论分析表明，MoAT模型中边际的精确计算是NP难问题。实际上，在与强大的概率密度估计基准的比较中，MoAT实现了最先进的性能。",
    "tldr": "提出了一种新的生成模型类别——所有树的混合模型——它在处理$n$个变量的情况时能实现最先进的性能，尽管其边际估计的精确计算是NP难问题。"
}