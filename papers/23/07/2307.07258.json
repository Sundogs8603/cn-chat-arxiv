{
    "title": "Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])",
    "abstract": "Transformer-based pre-trained language models, such as BERT, achieve great success in various natural language understanding tasks. Prior research found that BERT captures a rich hierarchy of linguistic information at different layers. However, the vanilla BERT uses the same self-attention mechanism for each layer to model the different contextual features. In this paper, we propose a HybridBERT model which combines self-attention and pooling networks to encode different contextual features in each layer. Additionally, we propose a simple DropMask method to address the mismatch between pre-training and fine-tuning caused by excessive use of special mask tokens during Masked Language Modeling pre-training. Experiments show that HybridBERT outperforms BERT in pre-training with lower loss, faster training speed (8% relative), lower memory cost (13% relative), and also in transfer learning with 1.5% relative higher accuracies on downstream tasks. Additionally, DropMask improves accuracies ",
    "link": "http://arxiv.org/abs/2307.07258",
    "context": "Title: Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])\nAbstract: Transformer-based pre-trained language models, such as BERT, achieve great success in various natural language understanding tasks. Prior research found that BERT captures a rich hierarchy of linguistic information at different layers. However, the vanilla BERT uses the same self-attention mechanism for each layer to model the different contextual features. In this paper, we propose a HybridBERT model which combines self-attention and pooling networks to encode different contextual features in each layer. Additionally, we propose a simple DropMask method to address the mismatch between pre-training and fine-tuning caused by excessive use of special mask tokens during Masked Language Modeling pre-training. Experiments show that HybridBERT outperforms BERT in pre-training with lower loss, faster training speed (8% relative), lower memory cost (13% relative), and also in transfer learning with 1.5% relative higher accuracies on downstream tasks. Additionally, DropMask improves accuracies ",
    "path": "papers/23/07/2307.07258.json",
    "total_tokens": 988,
    "translated_title": "用混合池化网络和Drop Mask提升BERT",
    "translated_abstract": "基于Transformer的预训练语言模型BERT在各种自然语言理解任务中取得了巨大成功。先前的研究发现，BERT在不同层次捕捉到丰富的语言信息层次结构。然而，普通的BERT在每一层使用相同的自注意力机制来建模不同的上下文特征。在本文中，我们提出了一个HybridBERT模型，它结合了自注意力和池化网络来编码每一层中的不同上下文特征。此外，我们提出了一个简单的DropMask方法，以解决由于在掩蔽语言建模预训练过程中过度使用特殊掩蔽标记引起的预训练和微调之间的不匹配问题。实验证明，HybridBERT在预训练中表现优于BERT，具有更低的损失、更快的训练速度（相对降低8%）、更低的内存成本（相对降低13%），并且在迁移学习中在下游任务中具有相对更高的精度（相对增加1.5%）。此外，DropMask能够提高精度。",
    "tldr": "本文提出了HybridBERT模型，通过结合自注意力和池化网络来编码每一层中的不同上下文特征，并且引入DropMask方法来解决预训练和微调之间的不匹配问题。实验证明，HybridBERT在预训练和迁移学习任务中均优于BERT，并且DropMask能够提高精度。",
    "en_tdlr": "This paper proposes the HybridBERT model, which combines self-attention and pooling networks to encode different contextual features in each layer, and introduces the DropMask method to address the mismatch between pre-training and fine-tuning. Experiments demonstrate that HybridBERT outperforms BERT in both pre-training and transfer learning tasks, and DropMask improves the accuracy."
}