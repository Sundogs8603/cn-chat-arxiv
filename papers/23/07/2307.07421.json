{
    "title": "Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])",
    "abstract": "Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m",
    "link": "http://arxiv.org/abs/2307.07421",
    "context": "Title: Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])\nAbstract: Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m",
    "path": "papers/23/07/2307.07421.json",
    "total_tokens": 883,
    "translated_title": "Sumformer: 一种用于语音识别的线性复杂度代替自注意力的方法",
    "translated_abstract": "现代语音识别系统依赖于自注意力。然而，使用自注意力进行令牌混合的计算复杂度与语音语句的长度呈二次关系，导致推理、训练和内存占用速度变慢。虽然已经开发出了比自注意力更便宜的替代方法，但很难保证达到相同的准确性水平。实际上，经过训练的语音识别器的自注意力权重在时间上呈全局平均化的形式。因此，本文提出了一种用于语音识别的线性时间替代自注意力的方法。它用所有时间步长的向量的平均值来总结整个语句。然后将这个单一的总结与特定时间的信息结合起来。我们将这种方法称为“总结混合”。在最先进的ASR模型中引入总结混合，可以在降低训练和推理时间多达27%的同时，保持或超过先前的语音识别性能水平。",
    "tldr": "Sumformer提出了一种线性时间代替自注意力的方法，用总结混合来处理语音识别任务，可以在保持准确性的同时降低训练和推理时间。",
    "en_tdlr": "Sumformer proposes a linear-time alternative to self-attention called \"Summary Mixing\" for speech recognition, which combines a single summary with time-specific information, achieving comparable accuracy while reducing training and inference times."
}