{
    "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])",
    "abstract": "Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we des",
    "link": "http://arxiv.org/abs/2307.09474",
    "context": "Title: ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])\nAbstract: Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we des",
    "path": "papers/23/07/2307.09474.json",
    "total_tokens": 966,
    "translated_title": "ChatSpot: 通过精确的指示调整引导进行多模态LLMs引导初创",
    "translated_abstract": "人工智能与人类的互动是反映多模态大型语言模型（MLLMs）可用性的重要方面。然而，现有的端到端MLLMs只允许用户通过语言指令与其交互，从而限制了交互的准确性和效率。在本研究中，我们提出了精确的引用指令，利用点和框等多样化的引用表示方法来引用特殊区域。这使得MLLMs能够聚焦于感兴趣的区域，并实现更精细的交互。基于精确的引用指令，我们提出了ChatSpot，一个统一的端到端多模态大型语言模型，支持包括鼠标点击、拖放和绘制框等多种形式的互动，提供更灵活和无缝的互动体验。我们还根据现有数据集和GPT-4生成了一个多层次的视觉语言指令跟随数据集。",
    "tldr": "本研究提出了一种精确的指示方法，利用点和框等多样化的引用表示方法来引用特殊区域，从而使得多模态大型语言模型（MLLMs）能够实现更精细的交互。基于此，我们提出了ChatSpot，一个统一的多模态大型语言模型，支持多种形式的互动，提供更灵活和无缝的交互体验。",
    "en_tdlr": "This study proposes a precise referring method using diverse reference representations to refer to special regions, enabling multimodal large language models (MLLMs) to achieve finer-grained interaction. Based on this, ChatSpot, a unified multimodal large language model, is introduced to support various forms of interaction, providing a more flexible and seamless interactive experience."
}