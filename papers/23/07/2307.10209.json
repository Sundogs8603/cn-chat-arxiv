{
    "title": "On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])",
    "abstract": "Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load disaggregation algorithms, are fundamental tools for effective energy management. Despite the success of deep models in load disaggregation, they face various challenges, particularly those pertaining to privacy and security. This paper investigates the sensitivity of prominent deep NILM baselines to adversarial attacks, which have proven to be a significant threat in domains such as computer vision and speech recognition. Adversarial attacks entail the introduction of imperceptible noise into the input data with the aim of misleading the neural network into generating erroneous outputs. We investigate the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to perturb the input sequences fed into two commonly employed CNN-based NILM baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models. Our findings provide compelling evidence for the vulnerability of these models, partic",
    "link": "http://arxiv.org/abs/2307.10209",
    "context": "Title: On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])\nAbstract: Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load disaggregation algorithms, are fundamental tools for effective energy management. Despite the success of deep models in load disaggregation, they face various challenges, particularly those pertaining to privacy and security. This paper investigates the sensitivity of prominent deep NILM baselines to adversarial attacks, which have proven to be a significant threat in domains such as computer vision and speech recognition. Adversarial attacks entail the introduction of imperceptible noise into the input data with the aim of misleading the neural network into generating erroneous outputs. We investigate the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to perturb the input sequences fed into two commonly employed CNN-based NILM baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models. Our findings provide compelling evidence for the vulnerability of these models, partic",
    "path": "papers/23/07/2307.10209.json",
    "total_tokens": 928,
    "translated_title": "关于深度负载分解算法对敌对攻击的敏感性研究",
    "translated_abstract": "非侵入式负载监测（NILM）算法，通常称为负载分解算法，是有效能量管理的基本工具。尽管深度模型在负载分解方面取得了成功，但它们面临着诸多挑战，特别是与隐私和安全有关的挑战。本文研究了著名的深度NILM基线对敌对攻击的敏感性，敌对攻击已被证明在计算机视觉和语音识别等领域具有重要威胁。敌对攻击涉及将不可感知的噪声引入输入数据，以误导神经网络生成错误的输出。我们研究了快速梯度符号方法（FGSM），这是一种著名的敌对攻击方法，来扰乱输入序列，这些序列被用于两个常用的基于CNN的NILM基线模型：序列到序列（S2S）和序列到点（S2P）模型。我们的研究结果提供了这些模型的易受攻击的有力证据。",
    "tldr": "本研究调查了深度负载分解算法对敌对攻击的敏感性，并发现常用的CNN-based NILM模型易受攻击，这对于负载分解算法的隐私和安全具有重要影响。"
}