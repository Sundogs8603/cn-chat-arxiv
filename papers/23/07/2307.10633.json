{
    "title": "Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])",
    "abstract": "Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performanc",
    "link": "http://arxiv.org/abs/2307.10633",
    "context": "Title: Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])\nAbstract: Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performanc",
    "path": "papers/23/07/2307.10633.json",
    "total_tokens": 883,
    "translated_title": "多方法自训练：通过文本改进代码生成，反之亦然",
    "translated_abstract": "大型语言模型有许多解决同一问题的方法。这引入了新颖的优点（不同的方法可能对不同的问题有效），以及缺点（用户可能难以知道使用哪种方法）。在本文中，我们介绍了多方法自训练（MMST）方法，其中一种方法是在另一种方法的筛选输出上进行训练，从而增强每种方法的优点并改善它们的缺点。使用176B参数的语言和代码训练模型，我们证明MMST可以1）改善性能较差的方法（高达30%），使模型更易于使用，2）改善性能较好的方法（高达32.2%），使模型性能更优秀，以及3）通过改善模型生成解释能力，提高相关但不同任务的性能（高达10.3%）。然后，我们进行消融分析，探讨MMST的工作原理。我们证明MMST比传统的自训练方法生成更多数据，但性能提升更明显。",
    "tldr": "多方法自训练可以通过在不同方法之间训练和生成数据来改善代码生成的性能，使模型更易于使用，并提高相关任务的性能。"
}