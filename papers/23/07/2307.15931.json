{
    "title": "Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes. (arXiv:2307.15931v1 [cs.LG])",
    "abstract": "Reinforcement learning has been greatly improved in recent studies and an increased interest in real-world implementation has emerged in recent years. In many cases, due to the non-static disturbances, it becomes challenging for the agent to keep the performance. The disturbance results in the environment called Partially Observable Markov Decision Process. In common practice, Partially Observable Markov Decision Process is handled by introducing an additional estimator, or Recurrent Neural Network is utilized in the context of reinforcement learning. Both of the cases require to process sequential information on the trajectory. However, there are only a few studies investigating the effect of information to consider and the network structure to handle them. This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process. Several structures and approaches are proposed to extend one of the latest deep reinforcement learning algori",
    "link": "http://arxiv.org/abs/2307.15931",
    "context": "Title: Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes. (arXiv:2307.15931v1 [cs.LG])\nAbstract: Reinforcement learning has been greatly improved in recent studies and an increased interest in real-world implementation has emerged in recent years. In many cases, due to the non-static disturbances, it becomes challenging for the agent to keep the performance. The disturbance results in the environment called Partially Observable Markov Decision Process. In common practice, Partially Observable Markov Decision Process is handled by introducing an additional estimator, or Recurrent Neural Network is utilized in the context of reinforcement learning. Both of the cases require to process sequential information on the trajectory. However, there are only a few studies investigating the effect of information to consider and the network structure to handle them. This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process. Several structures and approaches are proposed to extend one of the latest deep reinforcement learning algori",
    "path": "papers/23/07/2307.15931.json",
    "total_tokens": 888,
    "translated_title": "部分可观测马尔可夫决策过程中的动态深度强化学习算法",
    "translated_abstract": "在最近的研究中，强化学习取得了很大的进步，并且在实际应用中引起了越来越多的兴趣。在许多情况下，由于非静态干扰，使得智能体难以保持性能。这种干扰产生了被称为部分可观测马尔可夫决策过程的环境。在实践中，部分可观测马尔可夫决策过程通过引入额外的估计器或在强化学习的上下文中使用递归神经网络来处理。这两种情况都需要处理轨迹上的序列信息。然而，目前只有很少有研究探讨要考虑的信息的影响以及处理它们的网络结构。本研究展示了在解决部分可观测马尔可夫决策过程时包含动作序列的好处，并提出了几种结构和方法来扩展最新的深度强化学习算法。",
    "tldr": "本研究研究了在部分可观测马尔可夫决策过程中解决的动作序列的好处，并提出了几种扩展深度强化学习算法的结构和方法。",
    "en_tdlr": "This study investigates the benefits of including action sequences in solving Partially Observable Markov Decision_Process and proposes several structures and approaches to extend deep reinforcement learning algorithms."
}