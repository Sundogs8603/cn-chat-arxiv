{
    "title": "HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])",
    "abstract": "Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,",
    "link": "http://arxiv.org/abs/2307.09653",
    "context": "Title: HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])\nAbstract: Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,",
    "path": "papers/23/07/2307.09653.json",
    "total_tokens": 971,
    "translated_title": "HAT-CL: 用于连续学习的基于任务的硬注意力PyTorch库",
    "translated_abstract": "连续学习中的灾难性遗忘现象，即神经网络在学习新任务时丧失先前获得的知识，给人们带来了重大挑战。硬注意力任务(HAT)机制在减轻这个问题方面已经显示出潜力，但其实际实现受到了可用性和兼容性问题的影响，并且缺乏对现有网络复用的支持。在本文中，我们介绍了HAT-CL，这是HAT机制的用户友好、与PyTorch兼容的重新设计。HAT-CL不仅自动化了梯度操作，还简化了PyTorch模块转化为HAT模块的过程。它通过提供一套全面的模块，可以无缝地集成到现有的架构中。此外，HAT-CL还提供了与TIMM库平滑集成的可用的HAT网络。除了对HAT的重新设计和重新实现之外，我们还介绍了用于HAT的新颖的掩码操作技术。",
    "tldr": "HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。",
    "en_tdlr": "HAT-CL is a task-based hard-attention PyTorch library that provides a solution to catastrophic forgetting in continual learning. It automates gradient manipulation and module transformation, improves usability and compatibility of the HAT mechanism, and introduces novel mask manipulation techniques."
}