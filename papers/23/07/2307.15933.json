{
    "title": "GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. (arXiv:2307.15933v1 [cs.CL])",
    "abstract": "Large-scale language models such as DNABert and LOGO aim to learn optimal gene representations and are trained on the entire Human Reference Genome. However, standard tokenization schemes involve a simple sliding window of tokens like k-mers that do not leverage any gene-based semantics and thus may lead to (trivial) masking of easily predictable sequences and subsequently inefficient Masked Language Modeling (MLM) training. Therefore, we propose a novel masking algorithm, GeneMask, for MLM training of gene sequences, where we randomly identify positions in a gene sequence as mask centers and locally select the span around the mask center with the highest Normalized Pointwise Mutual Information (NPMI) to mask. We observe that in the absence of human-understandable semantics in the genomics domain (in contrast, semantic units like words and phrases are inherently available in NLP), GeneMask-based models substantially outperform the SOTA models (DNABert and LOGO) over four benchmark gene",
    "link": "http://arxiv.org/abs/2307.15933",
    "context": "Title: GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. (arXiv:2307.15933v1 [cs.CL])\nAbstract: Large-scale language models such as DNABert and LOGO aim to learn optimal gene representations and are trained on the entire Human Reference Genome. However, standard tokenization schemes involve a simple sliding window of tokens like k-mers that do not leverage any gene-based semantics and thus may lead to (trivial) masking of easily predictable sequences and subsequently inefficient Masked Language Modeling (MLM) training. Therefore, we propose a novel masking algorithm, GeneMask, for MLM training of gene sequences, where we randomly identify positions in a gene sequence as mask centers and locally select the span around the mask center with the highest Normalized Pointwise Mutual Information (NPMI) to mask. We observe that in the absence of human-understandable semantics in the genomics domain (in contrast, semantic units like words and phrases are inherently available in NLP), GeneMask-based models substantially outperform the SOTA models (DNABert and LOGO) over four benchmark gene",
    "path": "papers/23/07/2307.15933.json",
    "total_tokens": 976,
    "translated_title": "GeneMask: 快速预训练基因序列以实现少样本学习",
    "translated_abstract": "大规模语言模型如DNABert和LOGO旨在学习最佳基因表示，并在整个人类参考基因组上进行训练。然而，标准的分词方案仅涉及类似k-mers的简单滑动窗口，不能利用任何基于基因的语义，因此可能会导致（平凡的）屏蔽易于预测序列，从而导致低效的屏蔽语言建模（MLM）训练。因此，我们提出了一种新颖的屏蔽算法GeneMask，用于基因序列的MLM训练，其中我们随机识别基因序列中的屏蔽中心位置，并局部选择中心位置周围具有最高规一化点对点互信息（NPMI）的跨度进行屏蔽。我们观察到，在基因组学领域缺乏人类理解的语义的情况下（相比之下，NLP中固有地提供单词和短语等语义单位），基于GeneMask的模型在四个基准基因上明显优于SOTA模型（DNABert和LOGO）。",
    "tldr": "GeneMask提出了一种新颖的基因序列屏蔽算法，通过选择具有最高规一化点对点互信息的跨度来优化屏蔽语言建模训练，相比于DNABert和LOGO等模型，在缺乏基因组学领域的人类可理解的语义的情况下表现更好。"
}