{
    "title": "Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])",
    "abstract": "In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed resi",
    "link": "http://arxiv.org/abs/2307.15247",
    "context": "Title: Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])\nAbstract: In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed resi",
    "path": "papers/23/07/2307.15247.json",
    "total_tokens": 850,
    "translated_title": "这个模型对每个人都可靠吗？测试强校准",
    "translated_abstract": "在一个校准良好的风险预测模型中，对于任何给定的子群体，平均预测概率与真实事件率接近。这样的模型适用于异质人群，并满足强算法公平性的概念。然而，对于强校准，对模型进行审核是一个已知困难的任务，特别是对于机器学习算法来说，由于潜在的子群体数量庞大。因此，常见做法是只根据少数预定义的子群体评估校准。最近在拟合度检验方面的发展提供了潜在的解决方案，但对于信号较弱或校准不良的子群体较小的情况，这些方法要么过度细分数据，要么根本不进行细分。我们引入了一种新的测试过程，基于以下洞察：如果我们能够按预期的残差对观测进行重新排序，预测值和观察值之间的关联性应该会发生变化。",
    "tldr": "通过重新排序观测值的预期残差，我们引入了一种新的测试程序来评估模型的强校准性能。",
    "en_tdlr": "We introduce a new testing procedure based on reordering observations by their expected residuals to assess the strong calibration performance of the model."
}