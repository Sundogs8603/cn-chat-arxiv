{
    "title": "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation. (arXiv:2307.00997v2 [cs.CV] UPDATED)",
    "abstract": "The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense attention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings",
    "link": "http://arxiv.org/abs/2307.00997",
    "context": "Title: RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation. (arXiv:2307.00997v2 [cs.CV] UPDATED)\nAbstract: The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense attention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings",
    "path": "papers/23/07/2307.00997.json",
    "total_tokens": 934,
    "translated_title": "RefSAM：高效适应任何模型的指代视频对象分割",
    "translated_abstract": "Segment Anything Model (SAM)因其在图像分割中出色的性能而引起了广泛关注。然而，在指代视频对象分割（RVOS）方面，由于需要精确的用户交互提示以及对语言和视觉等不同形态的有限理解能力，SAM缺乏熟练度。本文提出了RefSAM模型，通过在线方式从不同时间戳的多视图信息中加入SAM的潜力，探索其在RVOS中的应用。我们的方法对原始SAM模型进行了适应，通过使用轻量级的跨模态MLP将指代表达的文本嵌入投影为稀疏和密集嵌入，作为用户交互提示，以增强跨模态学习。此外，我们还引入了分层稠密注意模块，以将分层视觉语义信息与稀疏嵌入融合，以获得细粒度的密集嵌入。",
    "tldr": "本文介绍了RefSAM模型，该模型通过在线方式从不同时间戳的多视图信息中加入SAM的潜力，探索其在指代视频对象分割（RVOS）中的应用。通过使用跨模态MLP和分层稠密注意模块，我们改进了SAM模型，实现了对不同形态的精确理解，并取得了令人印象深刻的性能表现。",
    "en_tdlr": "This paper presents the RefSAM model, which explores the potential of the Segment Anything Model (SAM) for referring video object segmentation (RVOS) by incorporating multi-view information from diverse modalities and successive frames. The proposed approach adapts the SAM model by employing a lightweight Cross-Modal MLP and a hierarchical dense attention module, enhancing the understanding of different modalities and achieving impressive performance."
}