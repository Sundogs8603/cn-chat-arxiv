{
    "title": "Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])",
    "abstract": "This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.",
    "link": "http://arxiv.org/abs/2307.05812",
    "context": "Title: Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])\nAbstract: This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.",
    "path": "papers/23/07/2307.05812.json",
    "total_tokens": 1037,
    "translated_title": "虚拟电力厂在日前电力市场中的战略竞标的安全增强学习算法",
    "translated_abstract": "本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。提出的算法采用深度确定性策略梯度（DDPG）方法，学习具有竞争力的竞标策略，而不需要精确的市场模型。此外，为了考虑虚拟电力厂的复杂内部物理约束，我们对DDPG方法进行了两个增强。首先，推导了一种基于投影的安全屏蔽，将代理的行为限制在分布式能源资源的非线性功率流方程和运行约束所定义的可行空间内。其次，引入了奖励函数中的屏蔽激活惩罚，鼓励代理学习更安全的策略。基于IEEE 13-bus网络的案例研究证明了所提方法在使代理学习到高竞争力，安全的战略策略方面的有效性。",
    "tldr": "本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。该算法不需要精确的市场模型，通过深度确定性策略梯度方法学习具有竞争力的竞标策略，并通过引入安全屏蔽和奖励函数的惩罚机制来考虑虚拟电力厂的物理约束，使代理能够学习到更安全的策略。",
    "en_tdlr": "This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. It incorporates a projection-based safety shield and a penalty mechanism in the reward function to account for the complex internal physical constraints of VPPs, enabling the agent to learn a safer and highly competitive strategic policy."
}