{
    "title": "STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])",
    "abstract": "Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's pr",
    "link": "http://arxiv.org/abs/2307.09692",
    "context": "Title: STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])\nAbstract: Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's pr",
    "path": "papers/23/07/2307.09692.json",
    "total_tokens": 957,
    "translated_title": "STRAPPER：通过自训练增强和同伴正则化实现基于偏好的强化学习",
    "translated_abstract": "基于偏好的强化学习（PbRL）承诺通过二进制人类偏好学习复杂的奖励函数。然而，这种人类参与的形式需要大量的人力来为片段对分配偏好标签，从而阻碍了其大规模应用。最近的方法尝试重复使用未标记的片段，隐含地阐明了片段的分布，从而减轻了人们的努力。并且进一步考虑了一致性正则化来提高半监督学习的性能。然而，我们注意到，与普通的分类任务不同，PbRL中存在一个我们在本文中定义为相似性陷阱的独特现象。直观地说，人类对于相似的片段对可能会存在截然相反的偏好，但这种相似性可能会导致一致性正则化在PbRL中失败。由于相似性陷阱的存在，这样的一致性正则化不适当地增强了模型预测的一致性可能性。",
    "tldr": "本文提出了STRAPPER方法，通过自训练增强和同伴正则化实现基于偏好的强化学习。与其他方法不同的是，作者发现基于偏好的强化学习中存在相似性陷阱现象，即相似的片段对可能会存在截然相反的偏好，对一致性正则化造成影响。",
    "en_tdlr": "This paper proposes the STRAPPER method, which achieves preference-based reinforcement learning through self-training augmentation and peer regularization. Unlike other methods, the authors discovered the unique phenomenon of similarity trap in preference-based reinforcement learning, where similar segment pairs may have diametrically opposite preferences, affecting consistency regularization."
}