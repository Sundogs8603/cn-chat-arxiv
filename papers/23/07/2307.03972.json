{
    "title": "Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])",
    "abstract": "Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chines",
    "link": "http://arxiv.org/abs/2307.03972",
    "context": "Title: Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])\nAbstract: Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chines",
    "path": "papers/23/07/2307.03972.json",
    "total_tokens": 873,
    "translated_title": "评估大规模语言模型在中文语法错误修正任务上的能力",
    "translated_abstract": "大规模语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出了令人瞩目的能力，并在最近受到了广泛关注。然而，一些研究表明，大型语言模型在英文语法错误修正任务中未能达到超越最先进模型的良好结果。本报告旨在探究大型语言模型在中文语法错误修正任务中的表现，并为未来的工作提供指导。我们在4个中文语法错误修正数据集上使用了3个不同模型规模的LLMs进行实验。我们的实验结果表明，LLMs在自动评估指标上的表现不及之前的最佳模型，因为存在过度修正的问题。此外，我们还发现LLMs在评估不同数据分布时的性能存在显著变化。我们的发现表明，需要进一步研究LLMs在中文语法错误修正任务上的应用。",
    "tldr": "本研究评估了大规模语言模型在中文语法错误修正任务上的表现，并发现存在过度修正的问题。此外，我们还发现在评估不同数据分布时，大型语言模型的性能有显著变化。"
}