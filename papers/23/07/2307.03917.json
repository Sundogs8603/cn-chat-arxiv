{
    "title": "On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])",
    "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli",
    "link": "http://arxiv.org/abs/2307.03917",
    "context": "Title: On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])\nAbstract: Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli",
    "path": "papers/23/07/2307.03917.json",
    "total_tokens": 976,
    "translated_title": "关于仅解码器架构在语音到文本和大型语言模型集成中的应用",
    "translated_abstract": "大型语言模型在自然语言处理领域取得了显著的成功，能够使用自然语言实现更好的人机交互。然而，如何将语音信号无缝地集成到大型语言模型中尚未得到很好的探索。同时，关于语音处理任务的“仅解码器”架构也没有得到很好的研究。在这项研究中，我们介绍了Speech-LLaMA，一种新颖的方法，有效地将声学信息融入基于文本的大型语言模型中。我们的方法利用了连接主义时序分类和简单的音频编码器，将压缩的声学特征映射到大型语言模型的连续语义空间中。此外，我们进一步探索了仅解码器架构在语音到文本任务中的应用，通过仅使用语音-文本配对数据训练一个较小规模、随机初始化的Speech-LLaMA模型。我们在多语言语音到文本翻译任务上进行了实验，证明与强基准相比有明显的改进。",
    "tldr": "该论文介绍了一种新颖的方法Speech-LLaMA，将声学信息有效地融入基于文本的大型语言模型中。通过使用连接主义时序分类和简单的音频编码器，将压缩的声学特征映射到大型语言模型的连续语义空间中，实现了语音到文本任务中的实质性提升。",
    "en_tdlr": "This paper introduces a novel approach, Speech-LLaMA, that effectively incorporates acoustic information into text-based large language models. By leveraging Connectionist Temporal Classification and a simple audio encoder, it maps compressed acoustic features to the continuous semantic space of the large language model, resulting in significant improvement in speech-to-text tasks."
}