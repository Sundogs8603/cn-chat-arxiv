{
    "title": "Modify Training Directions in Function Space to Reduce Generalization Error. (arXiv:2307.13290v1 [stat.ML])",
    "abstract": "We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demo",
    "link": "http://arxiv.org/abs/2307.13290",
    "context": "Title: Modify Training Directions in Function Space to Reduce Generalization Error. (arXiv:2307.13290v1 [stat.ML])\nAbstract: We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demo",
    "path": "papers/23/07/2307.13290.json",
    "total_tokens": 800,
    "translated_title": "在函数空间中修改训练方向以降低泛化误差",
    "translated_abstract": "我们提出了在神经网络函数空间中基于神经切换核和Fisher信息矩阵的特征分解的修改自然梯度下降法的理论分析。我们首先给出了在高斯分布和无限宽度极限的假设下，通过理论方法从特征分解和统计理论中显式推导出该修改自然梯度所学习的函数的表达式。因此，我们通过将总的泛化误差分解为函数空间中不同特征空间的误差，提出了一个平衡训练集误差和训练集与真实数据之间分布差异的准则。通过这种方法，我们建立了在函数空间中修改神经网络的训练方向会导致总的泛化误差的减少。",
    "tldr": "本文提出了在函数空间中修改训练方向的方法，通过在神经网络函数空间中进行特征分解和统计理论的理论分析，我们证明了这种方法可以降低总的泛化误差。",
    "en_tdlr": "This paper proposes a method to modify the training directions in the function space, and through theoretical analysis based on eigendecomposition and statistics theory, it has been demonstrated that this method can reduce the total generalization error."
}