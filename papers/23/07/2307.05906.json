{
    "title": "Mini-Batch Optimization of Contrastive Loss. (arXiv:2307.05906v1 [cs.LG])",
    "abstract": "Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experime",
    "link": "http://arxiv.org/abs/2307.05906",
    "context": "Title: Mini-Batch Optimization of Contrastive Loss. (arXiv:2307.05906v1 [cs.LG])\nAbstract: Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experime",
    "path": "papers/23/07/2307.05906.json",
    "total_tokens": 926,
    "translated_title": "对比损失的小批量优化",
    "translated_abstract": "对比学习作为一种自监督学习方法，引起了广泛关注。对比损失函数确保正样本对的嵌入（例如，同一类别的不同样本或同一对象的不同视图）相似，而负样本对的嵌入不相似。实际约束条件，如大内存需求，使得考虑所有可能的正负样本对变得具有挑战性，因此使用小批量优化。在本文中，我们研究了对比学习中小批量优化的理论方面。我们证明了如果且仅当选择了所有 $\\binom{N}{B}$ 小批量，小批量优化与全批量优化等效，而仅考虑子集可能导致次优。我们随后证明了利用高损失小批量可以加速随机梯度下降的收敛，并提出了一种基于谱聚类的方法来识别这些高损失小批量。我们的实验表明，这种方法在多个数据集和模型上都取得了优越的性能。",
    "tldr": "本文研究了对比学习中小批量优化的理论方面，证明了全部选择小批量与全批量优化等效，提出了一种利用高损失小批量加速随机梯度下降收敛的方法，并通过谱聚类识别高损失小批量。",
    "en_tdlr": "This paper investigates the theoretical aspects of mini-batch optimization in contrastive learning, proves the equivalence between selecting all mini-batches and full-batch optimization, proposes a method to speed up SGD convergence by utilizing high-loss mini-batches, and identifies these mini-batches through spectral clustering."
}