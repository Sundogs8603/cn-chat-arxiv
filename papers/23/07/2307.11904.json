{
    "title": "Model Compression Methods for YOLOv5: A Review. (arXiv:2307.11904v1 [cs.CV])",
    "abstract": "Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we ident",
    "link": "http://arxiv.org/abs/2307.11904",
    "context": "Title: Model Compression Methods for YOLOv5: A Review. (arXiv:2307.11904v1 [cs.CV])\nAbstract: Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we ident",
    "path": "papers/23/07/2307.11904.json",
    "total_tokens": 877,
    "translated_title": "YOLOv5的模型压缩方法综述",
    "translated_abstract": "在过去几年中，人们对提升YOLO物体检测器进行了大量研究。自引入以来，已经推出了八个主要版本的YOLO，旨在提高其准确性和效率。尽管YOLO的明显优点使其在许多领域被广泛使用，但将其部署在资源有限的设备上仍面临挑战。为了解决这个问题，研究人员开发了各种神经网络压缩方法，分为网络裁剪、量化和知识蒸馏三类。利用模型压缩方法取得的成果，如降低内存使用和推理时间，使其在硬件受限的边缘设备上部署大型神经网络变得可行甚至必要。在本综述论文中，我们重点研究网络裁剪和量化方法，因为它们具有相对较好的模块性。我们将它们进行分类，并分析将这些方法应用于YOLOv5的实际结果。",
    "tldr": "本文综述了针对YOLOv5的模型压缩方法，重点关注了网络裁剪和量化。这些方法在降低内存使用和推理时间方面取得了积极的效果。",
    "en_tdlr": "This review paper discusses model compression methods for YOLOv5, with a focus on network pruning and quantization. These methods have shown positive results in reducing memory usage and inference time."
}