{
    "title": "First-order Policy Optimization for Robust Policy Evaluation. (arXiv:2307.15890v1 [math.OC])",
    "abstract": "We adopt a policy optimization viewpoint towards policy evaluation for robust Markov decision process with $\\mathrm{s}$-rectangular ambiguity sets. The developed method, named first-order policy evaluation (FRPE), provides the first unified framework for robust policy evaluation in both deterministic (offline) and stochastic (online) settings, with either tabular representation or generic function approximation. In particular, we establish linear convergence in the deterministic setting, and $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity in the stochastic setting. FRPE also extends naturally to evaluating the robust state-action value function with $(\\mathrm{s}, \\mathrm{a})$-rectangular ambiguity sets. We discuss the application of the developed results for stochastic policy optimization of large-scale robust MDPs.",
    "link": "http://arxiv.org/abs/2307.15890",
    "context": "Title: First-order Policy Optimization for Robust Policy Evaluation. (arXiv:2307.15890v1 [math.OC])\nAbstract: We adopt a policy optimization viewpoint towards policy evaluation for robust Markov decision process with $\\mathrm{s}$-rectangular ambiguity sets. The developed method, named first-order policy evaluation (FRPE), provides the first unified framework for robust policy evaluation in both deterministic (offline) and stochastic (online) settings, with either tabular representation or generic function approximation. In particular, we establish linear convergence in the deterministic setting, and $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity in the stochastic setting. FRPE also extends naturally to evaluating the robust state-action value function with $(\\mathrm{s}, \\mathrm{a})$-rectangular ambiguity sets. We discuss the application of the developed results for stochastic policy optimization of large-scale robust MDPs.",
    "path": "papers/23/07/2307.15890.json",
    "total_tokens": 823,
    "translated_title": "鲁棒策略评估的一阶策略优化",
    "translated_abstract": "我们采用一种策略优化的视角来进行具有s-rectangular模糊集的鲁棒马尔可夫决策过程的策略评估。所开发的方法，名为一阶策略评估（FRPE），提供了对确定性（离线）和随机（在线）设置下的鲁棒策略评估的第一个统一框架，可以使用表格表示或通用函数逼近。特别地，在确定性设置中建立了线性收敛，而在随机设置中具有近似O(1/ε^2)的样本复杂度。FRPE还可以自然地扩展到评估带有(s, a)-rectangular模糊集的鲁棒状态-动作值函数。我们讨论了将开发的结果应用于大规模鲁棒MDP的随机策略优化。",
    "tldr": "该论文提出了一种名为FRPE的一阶策略评估方法，用于鲁棒马尔可夫决策过程的策略评估。该方法在确定性和随机设置下都能提供统一的框架，并具有较小的样本复杂度。",
    "en_tdlr": "This paper introduces a first-order policy evaluation method called FRPE for robust Markov decision process. It provides a unified framework for policy evaluation in both deterministic and stochastic settings, with low sample complexity."
}