{
    "title": "DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training. (arXiv:2307.07649v1 [cs.LG])",
    "abstract": "Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput.",
    "link": "http://arxiv.org/abs/2307.07649",
    "context": "Title: DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training. (arXiv:2307.07649v1 [cs.LG])\nAbstract: Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput.",
    "path": "papers/23/07/2307.07649.json",
    "total_tokens": 906,
    "translated_title": "DistTGL：基于分布式内存的时间图神经网络训练",
    "translated_abstract": "基于内存的时间图神经网络是动态图表示学习中强大的工具，在许多现实世界应用中已经展示出卓越的性能。然而，其节点内存更适合较小的批量大小以捕捉图事件中的更多依赖关系，并且需要在所有训练器之间同步维护。因此，当扩展到多个GPU时，现有框架会出现精度损失的问题。更糟糕的是，同步节点内存的巨大开销使得将其部署到分布式GPU集群变得不可行。在这项工作中，我们提出了DistTGL - 一种在分布式GPU集群上训练基于内存的TGNN的高效可扩展解决方案。DistTGL相比现有解决方案有三个改进：增强的TGNN模型，新颖的训练算法和优化的系统。在实验中，DistTGL实现了近线性的收敛加速，精度比最先进的单机方法提高了14.5％，训练吞吐量提高了10.17倍。",
    "tldr": "DistTGL是一种在分布式GPU集群上训练内存化TGNN的高效可扩展解决方案，相比现有方法在精度和训练吞吐量上都有显著提高。",
    "en_tdlr": "DistTGL is an efficient and scalable solution for training memory-based TGNN on distributed GPU clusters, achieving significant improvements in accuracy and training throughput compared to existing methods."
}