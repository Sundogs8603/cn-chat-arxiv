{
    "title": "Text vectorization via transformer-based language models and n-gram perplexities. (arXiv:2307.09255v1 [cs.CL])",
    "abstract": "As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error. Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts. As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input. Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and the",
    "link": "http://arxiv.org/abs/2307.09255",
    "context": "Title: Text vectorization via transformer-based language models and n-gram perplexities. (arXiv:2307.09255v1 [cs.CL])\nAbstract: As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error. Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts. As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input. Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and the",
    "path": "papers/23/07/2307.09255.json",
    "total_tokens": 928,
    "translated_title": "基于Transformer语言模型和n-gram困惑度的文本向量化",
    "translated_abstract": "由于文本的概率（及困惑度）是基于各个标记的概率的乘积计算的，因此可能会出现一个不太可能的标记显著降低一些原本高概率输入的概率（增加困惑度），同时可能表示一个简单的打字错误。另外，鉴于困惑度是一个标量值，它指的是整个输入的概率分布信息在计算中丢失了（一个相对较好的文本如果有一个不太可能的标记，以及每个标记等可能的另一个文本，它们可以具有相同的困惑度值），尤其是对于较长的文本。作为对标量困惑度的替代，该研究提出了一种简单的算法，用于计算基于输入中的n-gram困惑度的向量值。这样的表示方法考虑了前面提到的各个方面，而不是唯一的值，计算了每个文本标记的相对困惑度，",
    "tldr": "该研究提出了一种基于n-gram困惑度的简单算法，用于计算文本向量化。这种方法可以解决在计算标量困惑度时由于个别标记的不太可能性导致概率的降低的问题，并且能够保留文本中每个标记的相对困惑度信息。",
    "en_tdlr": "This research proposes a simple algorithm based on n-gram perplexities to calculate text vectorization. This method can address the issue of probability reduction caused by unlikely tokens in scalar perplexity calculation and preserve the relative perplexity information of each token in the text."
}