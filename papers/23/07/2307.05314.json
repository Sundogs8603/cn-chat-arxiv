{
    "title": "Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering. (arXiv:2307.05314v1 [cs.CV])",
    "abstract": "Medical visual question answering (VQA) is a challenging task that requires answering clinical questions of a given medical image, by taking consider of both visual and language information. However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance. In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pretraining objectives. The pre-trained model is then transferred to downstream medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA) performance on three publicly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conduct a compr",
    "link": "http://arxiv.org/abs/2307.05314",
    "context": "Title: Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering. (arXiv:2307.05314v1 [cs.CV])\nAbstract: Medical visual question answering (VQA) is a challenging task that requires answering clinical questions of a given medical image, by taking consider of both visual and language information. However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance. In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pretraining objectives. The pre-trained model is then transferred to downstream medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA) performance on three publicly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conduct a compr",
    "path": "papers/23/07/2307.05314.json",
    "total_tokens": 882,
    "translated_title": "使用单模态和多模态对比损失进行医学视觉问答的视觉与语言预训练",
    "translated_abstract": "医学视觉问答(VQA)是一项具有挑战性的任务，需要通过考虑视觉和语言信息来回答给定医学图像的临床问题。然而，由于医学VQA训练数据规模较小，预训练微调范式已成为改善模型泛化性能的常用解决方案。本文提出了一种新的自监督方法，利用医学图像字幕数据集，通过采用单模态和多模态对比损失以及掩码语言建模和图像文本匹配作为预训练目标，学习输入图像和文本的单模态和多模态特征表示。然后将预训练模型迁移到下游医学VQA任务。所提出的方法在三个公开可用的医学VQA数据集上取得了最先进的性能，分别提高了2.2％，14.7％和1.7％的准确性。",
    "tldr": "本文提出了一种使用单模态和多模态对比损失进行医学视觉问答的自监督预训练方法，在医学VQA任务中取得了最先进的性能。",
    "en_tdlr": "This paper presents a self-supervised pre-training approach using unimodal and multimodal contrastive losses for medical visual question answering, achieving state-of-the-art performance on medical VQA tasks."
}