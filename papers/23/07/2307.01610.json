{
    "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])",
    "abstract": "Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving h",
    "link": "http://arxiv.org/abs/2307.01610",
    "context": "Title: Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])\nAbstract: Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving h",
    "path": "papers/23/07/2307.01610.json",
    "total_tokens": 971,
    "translated_title": "过度自信是一件危险的事情：通过强制不太自信的预测来缓解成员推断攻击",
    "translated_abstract": "机器学习（ML）模型容易受到成员推断攻击（MIAs）的威胁，这些攻击确定给定的输入是否被用于训练目标模型。尽管有很多努力来缓解MIAs，但它们往往会受到有限的隐私保护、大幅降低准确性和/或需要难以获得的额外数据的困扰。本文提出了一种防御技术HAMP，可以在不需要额外数据的情况下实现强大的成员隐私和高准确性。为了缓解不同形式的MIAs，我们观察到它们可以统一，因为它们都利用了ML模型在通过不同的代理预测训练样本时的过度自信。这促使我们设计了一种通过模型强制进行不太自信预测的方法，从而迫使模型在训练样本和测试样本上表现类似。HAMP包括一个新颖的训练框架，使用高熵软标签和基于熵的正则化器来约束模型的预测，同时实现高准确性和成员隐私保护。",
    "tldr": "本文提出了一种防御技术HAMP，可以在不需要额外数据的情况下，通过强制模型进行不太自信的预测，达到强大的成员隐私保护和高准确性的目标。",
    "en_tdlr": "The paper proposes a defense technique called HAMP that achieves strong membership privacy and high accuracy by enforcing less confident predictions from the model, without requiring extra data. The technique unifies different forms of membership inference attacks by addressing the model's overconfidence in predicting training samples through various proxies."
}