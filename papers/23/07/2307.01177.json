{
    "title": "Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space. (arXiv:2307.01177v1 [cs.LG])",
    "abstract": "The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation fun",
    "link": "http://arxiv.org/abs/2307.01177",
    "context": "Title: Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space. (arXiv:2307.01177v1 [cs.LG])\nAbstract: The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation fun",
    "path": "papers/23/07/2307.01177.json",
    "total_tokens": 1077,
    "translated_title": "神经希尔伯特阶梯：函数空间中的多层神经网络",
    "translated_abstract": "神经网络(NNs)所探索的函数空间的特征化是深度学习理论的重要方面。本文将具有任意宽度的多层NN视为定义特定层次的再生核希尔伯特空间(RKHS)的神经希尔伯特阶梯(NHL)。这使得我们能够定义一个函数空间和一个复杂度度量，该度量推广了浅层NNs的先前结果，并研究了它们在几个方面的理论特性和影响。首先，我们证明了L层NNs表示的函数与属于L层NHLs的函数之间的对应关系。其次，我们证明了学习具有受控复杂度度量的NHL的泛化保证。第三，对应于在无穷宽均场极限下训练多层NNs，我们导出了NHL的特征动力学，该动力学被描述为多个随机场的演化。第四，在ReLU和二次激活函数下展示了NHLs中的深度分离示例。",
    "tldr": "本文提出了神经希尔伯特阶梯(NHL)的概念，它将多层神经网络描述为一系列的再生核希尔伯特空间，进一步推广了浅层神经网络的理论研究，并探讨了其在函数空间内的性质和应用。通过证明不同层次的NHL与多层NNs之间的对应关系，证明了学习NHL的泛化保证，并提出了NHL的特征动力学模型。最后，在ReLU和二次激活函数下展示了NHLs中的深度分离现象。",
    "en_tdlr": "This paper introduces the concept of Neural Hilbert Ladders (NHL), which views multi-layer neural networks as a series of reproducing kernel Hilbert spaces. It extends the theory of shallow neural networks and explores their properties and applications in function space. By proving correspondences between NHLs and multi-layer NNs, guarantees for learning NHLs, and deriving the dynamics of NHLs, the paper contributes to the understanding of depth separation in NHLs under different activation functions."
}