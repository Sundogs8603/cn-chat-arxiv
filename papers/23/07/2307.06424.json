{
    "title": "Robust scalable initialization for Bayesian variational inference with multi-modal Laplace approximations. (arXiv:2307.06424v1 [stat.ME])",
    "abstract": "For predictive modeling relying on Bayesian inversion, fully independent, or ``mean-field'', Gaussian distributions are often used as approximate probability density functions in variational inference since the number of variational parameters is twice the number of unknown model parameters. The resulting diagonal covariance structure coupled with unimodal behavior can be too restrictive when dealing with highly non-Gaussian behavior, including multimodality. High-fidelity surrogate posteriors in the form of Gaussian mixtures can capture any distribution to an arbitrary degree of accuracy while maintaining some analytical tractability. Variational inference with Gaussian mixtures with full-covariance structures suffers from a quadratic growth in variational parameters with the number of model parameters. Coupled with the existence of multiple local minima due to nonconvex trends in the loss functions often associated with variational inference, these challenges motivate the need for ro",
    "link": "http://arxiv.org/abs/2307.06424",
    "context": "Title: Robust scalable initialization for Bayesian variational inference with multi-modal Laplace approximations. (arXiv:2307.06424v1 [stat.ME])\nAbstract: For predictive modeling relying on Bayesian inversion, fully independent, or ``mean-field'', Gaussian distributions are often used as approximate probability density functions in variational inference since the number of variational parameters is twice the number of unknown model parameters. The resulting diagonal covariance structure coupled with unimodal behavior can be too restrictive when dealing with highly non-Gaussian behavior, including multimodality. High-fidelity surrogate posteriors in the form of Gaussian mixtures can capture any distribution to an arbitrary degree of accuracy while maintaining some analytical tractability. Variational inference with Gaussian mixtures with full-covariance structures suffers from a quadratic growth in variational parameters with the number of model parameters. Coupled with the existence of multiple local minima due to nonconvex trends in the loss functions often associated with variational inference, these challenges motivate the need for ro",
    "path": "papers/23/07/2307.06424.json",
    "total_tokens": 896,
    "translated_title": "基于多模态Laplace近似的贝叶斯变分推断的鲁棒可扩展初始化",
    "translated_abstract": "在依赖贝叶斯反演的预测建模中，通常使用完全独立或“均场”高斯分布作为变分推断中的近似概率密度函数，因为变分参数的数量是未知模型参数数量的两倍。由于具有对角协方差结构和单峰行为，当处理高度非高斯行为时，包括多峰性时，这种方法可能过于限制性。形式上为高斯混合物的高保真代理后验分布能够以任意精度捕获任何分布，同时仍保留一定的分析可追踪性。具有完全协方差结构的高斯混合物的变分推断随着模型参数数量的增加而出现变分参数的二次增长。由于与变分推断常常相关的损失函数中非凸趋势引起的多个局部极小值的存在，这些挑战促使需要进行鲁棒可扩展初始化。",
    "tldr": "本论文提出了基于多模态Laplace近似的贝叶斯变分推断的鲁棒可扩展初始化方法，以应对在高度非高斯行为中，包括多峰性时，“均场”高斯分布近似过于限制性的问题。"
}