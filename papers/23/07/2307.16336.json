{
    "title": "Anatomy of an AI-powered malicious social botnet. (arXiv:2307.16336v1 [cs.CY])",
    "abstract": "Large language models (LLMs) exhibit impressive capabilities in generating realistic text across diverse subjects. Concerns have been raised that they could be utilized to produce fake content with a deceptive intention, although evidence thus far remains anecdotal. This paper presents a case study about a Twitter botnet that appears to employ ChatGPT to generate human-like content. Through heuristics, we identify 1,140 accounts and validate them via manual annotation. These accounts form a dense cluster of fake personas that exhibit similar behaviors, including posting machine-generated content and stolen images, and engage with each other through replies and retweets. ChatGPT-generated content promotes suspicious websites and spreads harmful comments. While the accounts in the AI botnet can be detected through their coordination patterns, current state-of-the-art LLM content classifiers fail to discriminate between them and human accounts in the wild. These findings highlight the thr",
    "link": "http://arxiv.org/abs/2307.16336",
    "context": "Title: Anatomy of an AI-powered malicious social botnet. (arXiv:2307.16336v1 [cs.CY])\nAbstract: Large language models (LLMs) exhibit impressive capabilities in generating realistic text across diverse subjects. Concerns have been raised that they could be utilized to produce fake content with a deceptive intention, although evidence thus far remains anecdotal. This paper presents a case study about a Twitter botnet that appears to employ ChatGPT to generate human-like content. Through heuristics, we identify 1,140 accounts and validate them via manual annotation. These accounts form a dense cluster of fake personas that exhibit similar behaviors, including posting machine-generated content and stolen images, and engage with each other through replies and retweets. ChatGPT-generated content promotes suspicious websites and spreads harmful comments. While the accounts in the AI botnet can be detected through their coordination patterns, current state-of-the-art LLM content classifiers fail to discriminate between them and human accounts in the wild. These findings highlight the thr",
    "path": "papers/23/07/2307.16336.json",
    "total_tokens": 901,
    "translated_title": "AI助力的恶意社交机器人网络解剖",
    "translated_abstract": "大型语言模型（LLM）在生成各种主题上的逼真文本方面表现出令人印象深刻的能力。有人担心它们可能被用于生成带有欺骗意图的虚假内容，尽管迄今为止的证据仍是片面的。本文通过一个关于Twitter机器人网络的案例研究，阐述了似乎采用ChatGPT生成类似人类内容的机器人网络。通过启发式方法，我们识别出1140个账号，并通过手动注释对其进行验证。这些账号形成了一个密集的虚假人物群集，表现出类似的行为，包括发布机器生成的内容和盗用图片，并通过回复和转发来互相交流。ChatGPT生成的内容宣传可疑网站并传播有害评论。虽然AI机器人网络中的账号可以通过它们的协调模式进行检测，但目前最先进的LLM内容分类器无法将它们与野外的人类账号区分开来。这些发现突显了恶意社交机器人网络的威胁。",
    "tldr": "本文通过案例研究揭示了使用AI语言模型生成人类内容的Twitter机器人网络。这些机器人账号构成了一个密集的虚假人物群集，宣传可疑网站并传播有害评论。",
    "en_tdlr": "This paper presents a case study on a Twitter botnet that utilizes AI language models to generate human-like content. These bot accounts form a dense cluster of fake personas, promoting suspicious websites and spreading harmful comments."
}