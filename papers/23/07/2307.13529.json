{
    "title": "Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])",
    "abstract": "Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact",
    "link": "http://arxiv.org/abs/2307.13529",
    "context": "Title: Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])\nAbstract: Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact",
    "path": "papers/23/07/2307.13529.json",
    "total_tokens": 910,
    "translated_title": "Re-mine, Learn and Reason: 探索语言引导下跨模态语义相关性的人物-物体交互检测",
    "translated_abstract": "人物-物体交互（HOI）检测是一项具有挑战性的计算机视觉任务，需要视觉模型解决人物和物体之间复杂的交互关系，并预测HOI三元组。本文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识来增强HOI检测。首先，我们定性和定量分析了两阶段HOI检测器中交互信息的损失，并提出了一种再挖掘策略来生成更全面的视觉表示。其次，我们设计了更细粒度的句子和词级对齐以及知识转移策略，以有效解决多个交互和多个文本之间的多对多匹配问题。这些策略减轻了多个交互导致的匹配混淆问题。",
    "tldr": "本论文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识增强人物-物体交互检测，通过再挖掘策略生成更全面的视觉表示，并设计了细粒度的句子和词级对齐以及知识转移策略来解决多对多匹配问题。"
}