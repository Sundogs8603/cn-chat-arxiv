{
    "title": "RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)",
    "abstract": "Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Rando",
    "link": "http://arxiv.org/abs/2307.02251",
    "context": "Title: RanPAC: Random Projections and Pre-trained Models for Continual Learning. (arXiv:2307.02251v3 [cs.LG] UPDATED)\nAbstract: Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Rando",
    "path": "papers/23/07/2307.02251.json",
    "total_tokens": 866,
    "translated_title": "RanPAC: 随机投影和预训练模型在连续学习中的应用",
    "translated_abstract": "连续学习旨在在非稳态数据流中增量学习不同的任务（如分类），而不会忘记旧的任务。大多数连续学习方法都致力于从头开始学习的灾难性遗忘问题。然而，随着基础模型的日益重要，在各种下游需求中都可以利用具有信息丰富表示的预训练模型。已经探索了一些基于预训练模型的连续学习方法，要么直接利用预提取的特征（这使得弥合分布差距变得困难），要么融入适配器（这可能会导致遗忘问题）。在本文中，我们提出了一种简洁而有效的预训练模型连续学习方法。鉴于遗忘问题发生在参数更新期间，我们考虑了一种替代方法，利用无需进行训练的随机投影器和类原型累积来避免这个问题。具体而言，我们注入了一个固定的...",
    "tldr": "这项研究提出了一种利用预训练模型进行连续学习的方法，通过使用随机投影器和类原型累积来避免遗忘问题。"
}