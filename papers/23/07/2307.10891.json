{
    "title": "Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])",
    "abstract": "Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.",
    "link": "http://arxiv.org/abs/2307.10891",
    "context": "Title: Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])\nAbstract: Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.",
    "path": "papers/23/07/2307.10891.json",
    "total_tokens": 835,
    "translated_title": "句法与语义线性抽象和细化神经网络",
    "translated_abstract": "抽象是一种提高可扩展性的关键验证技术。然而，其在神经网络中的使用迄今非常有限。以前的分类网络抽象方法将多个神经元替换为足够相似的其中一个神经元。我们可以将相似性定义为句法上（使用神经元之间的连接数量）或语义上（对于各种输入的神经元激活值）的相似性。然而，以往的方法仅能达到适度的减少，而且实现起来困难。在这项工作中，我们提供了一个更灵活的框架，其中神经元可以被其他神经元的线性组合替换，从而改善减少效果。我们在句法和语义抽象上应用了这种方法，并通过实验进行了实现和评估。此外，我们引入了一种改进我们抽象的方法，以寻找更好的减少和精确度平衡。",
    "tldr": "这项工作提供了一个灵活的框架，通过线性组合替换神经元，实现了神经网络的句法和语义抽象，并引入了一种改进方法来平衡减少和精确度。",
    "en_tdlr": "This work provides a flexible framework for syntactic and semantic abstraction of neural networks by replacing neurons with linear combinations, and introduces a refinement method to balance reduction and precision."
}