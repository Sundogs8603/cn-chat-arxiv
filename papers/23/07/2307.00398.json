{
    "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model ",
    "link": "http://arxiv.org/abs/2307.00398",
    "context": "Title: ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)\nAbstract: Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model ",
    "path": "papers/23/07/2307.00398.json",
    "total_tokens": 922,
    "translated_title": "ProbVLM: 冻结视觉-语言模型的概率适配器",
    "translated_abstract": "大规模视觉-语言模型（VLM）如CLIP成功地在图像和文本之间找到对应关系。通过标准的确定性映射过程，将图像或文本样本映射到嵌入空间中的一个向量。这是有问题的：由于多个样本（图像或文本）可以抽象出物理世界中的相同概念，确定性嵌入不反映嵌入空间中的固有歧义性。我们提出了ProbVLM，一种概率适配器，通过事后方式在预训练的VLM中通过内部/外部模态对齐估计嵌入的概率分布，而无需大规模数据集或计算。在四个具有挑战性的数据集上，即COCO、Flickr、CUB和Oxford-flowers，我们估计了两个VLM（CLIP和BLIP）的多模态嵌入不确定性，量化了嵌入不确定性在检索任务中的校准，并表明ProbVLM优于其他方法。此外，我们提出了主动学习和模型...",
    "tldr": "ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。",
    "en_tdlr": "ProbVLM is a probabilistic adapter that estimates probability distributions for embeddings in large-scale vision-language models, addressing the inherent ambiguity in embeddings, and demonstrates superior performance in retrieval tasks on multiple datasets."
}