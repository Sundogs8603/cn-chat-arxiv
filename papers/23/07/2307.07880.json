{
    "title": "Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding. (arXiv:2307.07880v1 [cs.CL])",
    "abstract": "Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the ",
    "link": "http://arxiv.org/abs/2307.07880",
    "context": "Title: Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding. (arXiv:2307.07880v1 [cs.CL])\nAbstract: Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the ",
    "path": "papers/23/07/2307.07880.json",
    "total_tokens": 933,
    "translated_title": "提示为基础的微调总是比原始微调更好吗？来自跨语言理解的见解。",
    "translated_abstract": "多语种预训练语言模型（MPLM）通过在源语言（例如英语）上针对特定任务的标注数据上对MPLM进行微调，并在各种目标语言上进行评估，已经在零转化跨语言传递的各种自然语言理解任务中展现出了显著的性能提升。最近的研究表明，在少样本场景下，基于提示的微调超过了常规微调。然而，在多语种任务中，提示为基础的学习的探索仍然有限。在本研究中，我们提出了ProFiT流程，以研究基于提示的微调的跨语言能力。我们在多样跨语言语言理解任务（情感分类、释义识别和自然语言推断）上进行了全面的实验，并在不同的少样本和全数据设置下经验性地分析了基于提示的微调性能的变化趋势。我们的结果揭示了提示为基础的微调在跨语言传递中的性能变化趋势。",
    "tldr": "提示为基础的微调在多语种任务中的表现仍然有限，本研究通过ProFiT流程对此进行了深入研究，实验证明在不同的少样本和全数据设置下，提示为基础的微调具有不同的性能变化趋势。"
}