{
    "title": "Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])",
    "abstract": "The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much ste",
    "link": "http://arxiv.org/abs/2307.11434",
    "context": "Title: Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])\nAbstract: The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much ste",
    "path": "papers/23/07/2307.11434.json",
    "total_tokens": 879,
    "translated_title": "为环保人工智能而批处理 - 探索推理过程的研究",
    "translated_abstract": "在开发新的神经网络时，批大小是一个需要调整的重要参数。除了其他质量指标外，它对模型的准确性、泛化能力、训练时间和并行性具有很大的影响。这个事实是众所周知并被广泛研究的。然而，在深度学习模型应用阶段，当模型被最终用户用于推理时，我们发现对引入批大小的潜在好处存在忽视。在这项研究中，我们考察了输入批处理对于五个在计算机视觉领域被认为是最先进的完全训练的神经网络的能源消耗和响应时间的影响。结果表明，批处理对这两个指标都有显著影响。此外，我们还呈现了过去十年神经网络的能源效率和准确性的时间线。我们发现，总体上，能源消耗在这段时间内明显上升。",
    "tldr": "这项研究探讨了在深度学习模型推理阶段引入批处理对能源消耗和响应时间的影响，并发现批处理对这两个指标都有显著影响。",
    "en_tdlr": "This study examines the impact of input batching on energy consumption and response times during the inference phase of deep learning models, and finds that batching has a significant effect on both metrics."
}