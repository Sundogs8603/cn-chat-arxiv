{
    "title": "Entropy regularization in probabilistic clustering. (arXiv:2307.10065v1 [stat.ME])",
    "abstract": "Bayesian nonparametric mixture models are widely used to cluster observations. However, one major drawback of the approach is that the estimated partition often presents unbalanced clusters' frequencies with only a few dominating clusters and a large number of sparsely-populated ones. This feature translates into results that are often uninterpretable unless we accept to ignore a relevant number of observations and clusters. Interpreting the posterior distribution as penalized likelihood, we show how the unbalance can be explained as a direct consequence of the cost functions involved in estimating the partition. In light of our findings, we propose a novel Bayesian estimator of the clustering configuration. The proposed estimator is equivalent to a post-processing procedure that reduces the number of sparsely-populated clusters and enhances interpretability. The procedure takes the form of entropy-regularization of the Bayesian estimate. While being computationally convenient with res",
    "link": "http://arxiv.org/abs/2307.10065",
    "context": "Title: Entropy regularization in probabilistic clustering. (arXiv:2307.10065v1 [stat.ME])\nAbstract: Bayesian nonparametric mixture models are widely used to cluster observations. However, one major drawback of the approach is that the estimated partition often presents unbalanced clusters' frequencies with only a few dominating clusters and a large number of sparsely-populated ones. This feature translates into results that are often uninterpretable unless we accept to ignore a relevant number of observations and clusters. Interpreting the posterior distribution as penalized likelihood, we show how the unbalance can be explained as a direct consequence of the cost functions involved in estimating the partition. In light of our findings, we propose a novel Bayesian estimator of the clustering configuration. The proposed estimator is equivalent to a post-processing procedure that reduces the number of sparsely-populated clusters and enhances interpretability. The procedure takes the form of entropy-regularization of the Bayesian estimate. While being computationally convenient with res",
    "path": "papers/23/07/2307.10065.json",
    "total_tokens": 841,
    "translated_title": "概率聚类中的熵正则化",
    "translated_abstract": "贝叶斯非参数混合模型被广泛用于聚类观测。然而，这种方法的一个主要缺点是估计的分区经常呈现不平衡的簇频率，其中只有少数几个簇占主导地位，而大量稀疏填充的簇存在。除非我们接受忽略一些观测和簇，否则这种特点会导致结果无法解释。将后验分布解释为惩罚似然度，我们展示了不平衡性可以解释为估计分区涉及的代价函数直接后果。根据我们的发现，我们提出了一种新颖的贝叶斯聚类配置估计器。该估计器等同于一种后处理过程，减少了稀疏填充簇的数量并增强了可解释性。该过程采取了贝叶斯估计的熵正则化形式。虽然在计算上很方便...",
    "tldr": "该论文提出了一种新颖的贝叶斯聚类配置估计器，通过熵正则化后处理过程，减少了稀疏填充簇的数量并增强了可解释性。",
    "en_tdlr": "This paper proposes a novel Bayesian estimator for clustering that reduces the number of sparsely-populated clusters and enhances interpretability through an entropy-regularization post-processing procedure."
}