{
    "title": "FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])",
    "abstract": "Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal ",
    "link": "http://arxiv.org/abs/2307.13214",
    "context": "Title: FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])\nAbstract: Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal ",
    "path": "papers/23/07/2307.13214.json",
    "total_tokens": 886,
    "translated_title": "FedMEKT: 基于蒸馏的多模态联邦学习中的嵌入知识传输",
    "translated_abstract": "联邦学习（FL）使多个客户端能够在不共享私有数据的情况下协同训练一个广义全局模型，从而实现分散式的机器学习范式。现有的大部分工作只是针对单模态数据提出了典型的FL系统，因此限制了它对于利用宝贵的多模态数据进行未来个性化应用的潜力。此外，大多数FL方法仍然依赖于客户端的标记数据，由于用户无法进行自注释，这在实际应用中是有限的。鉴于这些限制，我们提出了一种新型的多模态FL框架，采用半监督学习方法利用不同模态的表示。将这个概念引入一个系统中，我们开发了一种基于蒸馏的多模态嵌入知识传输机制，称为FedMEKT，它允许服务器和客户端交换从小型多模态数据集中提取的学习模型的联合知识。",
    "tldr": "FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。"
}