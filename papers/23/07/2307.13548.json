{
    "title": "Node Injection Link Stealing Attack. (arXiv:2307.13548v1 [cs.CR])",
    "abstract": "In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.",
    "link": "http://arxiv.org/abs/2307.13548",
    "context": "Title: Node Injection Link Stealing Attack. (arXiv:2307.13548v1 [cs.CR])\nAbstract: In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.",
    "path": "papers/23/07/2307.13548.json",
    "total_tokens": 885,
    "translated_title": "节点注入链接窃取攻击",
    "translated_abstract": "本文提出了一种隐秘且有效的攻击方法，通过推断图结构数据中的私有链接来暴露图神经网络（GNNs）的隐私漏洞。我们关注新节点加入图并使用API查询预测的归纳设置，研究私有边缘信息的潜在泄露。我们还提出了在保持模型效能的同时保护隐私的方法。与现有技术相比，我们的攻击在推断链接方面表现出优越性能。此外，我们还研究了将差分隐私（DP）机制应用于减轻我们所提出的攻击的影响，并分析了隐私保护与模型效能之间的权衡。我们的工作突出了GNNs中固有的隐私漏洞，强调了为其应用开发强大的保护隐私机制的重要性。",
    "tldr": "本文提出了一种隐秘且有效的攻击方法，通过推断图结构数据中的私有链接来暴露GNNs的隐私漏洞，并提出了保护隐私和保持模型效能的方法。同时，我们研究了将差分隐私机制应用于减轻攻击的影响，并分析了隐私保护和模型效能的权衡。",
    "en_tdlr": "This paper presents a stealthy and effective attack that exposes privacy vulnerabilities in GNNs by inferring private links within graph-structured data. It also proposes methods to preserve privacy while maintaining model utility. The study examines the application of differential privacy mechanisms to mitigate the impact of the attack and analyzes the trade-off between privacy preservation and model utility."
}