{
    "title": "Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v1 [cs.CL])",
    "abstract": "Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sens",
    "link": "http://arxiv.org/abs/2307.11764",
    "context": "Title: Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v1 [cs.CL])\nAbstract: Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sens",
    "path": "papers/23/07/2307.11764.json",
    "total_tokens": 865,
    "translated_title": "Sensi-BERT: 面向敏感度驱动的参数高效BERT微调",
    "translated_abstract": "近年来，由于在文本分类和问答等各种下游任务上的改进表现，大型预训练语言模型逐渐受到关注，只需进行很少次数的微调。然而，其庞大的模型大小常常限制了它们在资源受限的边缘设备上的应用。现有的参数高效BERT模型解决方案大多依赖于计算密集的训练和微调，并且常常依赖于额外的计算密集型模型来弥补性能差距。本文介绍了Sensi-BERT，一种敏感度驱动的BERT模型高效微调方法，可以使用现成的预训练BERT模型，生成适用于下游任务的高度参数高效的模型。具体而言，我们进行敏感度分析以对每个单独的参数张量进行排序，然后在微调过程中根据给定的参数或FLOPs预算进行相应的裁剪。实验结果表明Sensi-BERT的有效性。",
    "tldr": "Sensi-BERT是一种面向敏感度驱动的参数高效BERT微调方法，通过敏感度分析和裁剪参数张量，可生成适用于下游任务的高度参数高效的模型。"
}