{
    "title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation. (arXiv:2307.03214v1 [cs.CL])",
    "abstract": "We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.",
    "link": "http://arxiv.org/abs/2307.03214",
    "context": "Title: PREADD: Prefix-Adaptive Decoding for Controlled Text Generation. (arXiv:2307.03214v1 [cs.CL])\nAbstract: We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.",
    "path": "papers/23/07/2307.03214.json",
    "total_tokens": 819,
    "translated_title": "PREADD: 控制性文本生成的前缀自适应解码方法",
    "translated_abstract": "我们提出了一种名为PREADD的前缀自适应解码方法，用于控制性文本生成。与现有的使用辅助专家模型来控制属性的方法不同，PREADD不需要外部模型，而是依靠线性组合多个提示的输出概率来实现。具体来说，PREADD通过比较使用原始提示生成的输出概率和使用前缀前置提示生成的输出概率来实现对任何由前缀封装的属性的正向和负向控制。我们在三个任务上评估了PREADD，包括毒性输出减轻、性别偏见减少和情感控制，并发现PREADD在每个任务的主要指标上相对收益比提示基线和辅助专家控制方法高出12%或更多。",
    "tldr": "PREADD是一种前缀自适应解码方法，用于控制性文本生成，相比现有方法，PREADD不需要外部模型，能够实现对任何属性的正向和负向控制，并在多个任务上表现出较高的性能提升。",
    "en_tdlr": "PREADD is a method for controlled text generation that uses prefix-adaptive decoding. It does not require an external model and can achieve positive and negative control for any attribute. PREADD outperforms existing methods in multiple tasks and shows significant performance gain of 12% or more."
}