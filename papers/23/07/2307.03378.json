{
    "title": "A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification. (arXiv:2307.03378v1 [cs.CL])",
    "abstract": "Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of seven pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance.",
    "link": "http://arxiv.org/abs/2307.03378",
    "context": "Title: A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification. (arXiv:2307.03378v1 [cs.CL])\nAbstract: Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of seven pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance.",
    "path": "papers/23/07/2307.03378.json",
    "total_tokens": 920,
    "translated_title": "英语隐式篇章关系分类的Transformer模型比较",
    "translated_abstract": "尽管篇章解析可以帮助多个自然语言处理领域，但对于隐式篇章关系分类，尚未进行全面的语言模型搜索。这阻碍了研究人员充分利用公开可用的模型进行篇章分析。本研究是对七个预训练语言模型的直接细调比较。我们使用了PDTB-3数据集，这是一个流行的篇章关系注释数据集。通过我们的模型搜索，我们将SOTA提升到了0.671的准确度，并获得了新的观察结果。其中一些与之前的报道相反（Shi and Demberg, 2019b），即句子级预训练目标（NSP, SBO, SOP）通常无法产生最佳的隐式篇章关系分类模型。出乎意料的是，具有类似规模的PLMs，并且使用了MLM和完全注意机制，表现更好。",
    "tldr": "本研究对七个预训练语言模型进行了直接细调比较，提出了一种针对英语隐式篇章关系分类的新方法，并获得了显著提升的准确度。与之前的报道不同，本研究发现句子级预训练目标失败的情况下，采用了类似规模的PLMs，并且使用了MLM和完全注意机制的模型表现更好。",
    "en_tdlr": "This study presents a direct fine-tuned comparison of seven pre-trained language models for English implicit discourse relation classification, achieving significant accuracy improvement. Contrary to previous findings, it is observed that models with similar sizes, MLM, and full attention perform better when sentence-level pre-training objectives fail."
}