{
    "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)",
    "abstract": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-se",
    "link": "http://arxiv.org/abs/2307.02486",
    "context": "Title: LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)\nAbstract: Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-se",
    "path": "papers/23/07/2307.02486.json",
    "total_tokens": 939,
    "translated_title": "LongNet: 将Transformer扩展到10亿个标记",
    "translated_abstract": "在大语言模型的时代，扩展序列长度已经成为一个关键需求。然而，现有的方法在计算复杂度或模型表达力上存在困难，导致序列长度受限。为了解决这个问题，我们引入了LongNet，它是一种Transformer的变体，可以将序列长度扩展到10亿个标记以上，而不会牺牲对较短序列的性能。具体而言，我们提出了扩张注意力，随着距离的增大，它将注意范围指数级扩展。LongNet具有显著的优势：1）它具有线性计算复杂度和序列中任意两个标记之间的对数依赖关系；2）它可以作为用于极长序列的分布式训练器；3）它的扩张注意力是标准注意力的即插即用替代品，可以与现有的基于Transformer的优化无缝集成。实验证明LongNet在长序列和短序列上都具有强大的性能。",
    "tldr": "LongNet是一种可以扩展到10亿个标记的Transformer变体，通过扩张注意力解决了序列长度受限的问题，具有线性计算复杂度和对数依赖关系，可以作为分布式训练器使用并无缝集成到现有的Transformer优化中。实验证明LongNet在长序列和短序列上性能强大。"
}