{
    "title": "Towards a Visual-Language Foundation Model for Computational Pathology. (arXiv:2307.12914v2 [cs.CV] UPDATED)",
    "abstract": "The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-th",
    "link": "http://arxiv.org/abs/2307.12914",
    "context": "Title: Towards a Visual-Language Foundation Model for Computational Pathology. (arXiv:2307.12914v2 [cs.CV] UPDATED)\nAbstract: The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-th",
    "path": "papers/23/07/2307.12914.json",
    "total_tokens": 889,
    "translated_title": "为计算病理学构建以视觉语言为基础的模型",
    "translated_abstract": "数字病理学的广泛应用和深度学习的进步使得在各种疾病和患者群体中开发强大的模型成为可能。然而，在医学领域中，由于标签稀缺性，模型的训练往往困难，并且该模型的使用仅限于特定的任务和疾病。此外，大多数组织病理学模型仅使用图像数据，与人类相互教导和推理组织病理学实体的方式形成鲜明对比。我们引入了一种基于视觉语言的基础模型CONCH，它使用多种来源的组织病理学图像、生物医学文本，并通过无任务预训练获得了117万个图像-标题对。经过13种不同基准测试，CONCH 可以迁移到涉及组织病理学图像和文本的各种下游任务上，实现了最新的水平。",
    "tldr": "这项研究介绍了一种基于视觉语言的基础模型CONCH，通过图像、文本和图像-标题对的预训练，可以在涉及组织病理学图像和文本的各种任务上取得良好的性能。",
    "en_tdlr": "This research introduces a visual-language foundation model called CONCH, which achieves good performance on various tasks involving histopathology images and text through pretraining with images, text, and image-caption pairs."
}