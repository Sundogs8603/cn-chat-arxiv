{
    "title": "PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])",
    "abstract": "Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses",
    "link": "http://arxiv.org/abs/2307.05891",
    "context": "Title: PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])\nAbstract: Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses",
    "path": "papers/23/07/2307.05891.json",
    "total_tokens": 866,
    "translated_title": "受PID控制器启发的偏差归纳法在部分可观测控制任务中的深度强化学习",
    "translated_abstract": "深度强化学习（RL）已经展现出通过数据自己学习控制系统的巨大潜力。然而，深度RL面临的一个挑战是系统的完整状态通常不可观测。当出现这种情况时，策略需要利用观察历史来推断当前状态。同时，训练和测试环境之间的差异使得策略不会过度拟合训练时观察到的序列。因此，在历史记录编码器灵活提取相关信息的同时，要对环境变化具有鲁棒性，这是一个重要的平衡。为了达到这个平衡，我们寻求PID控制器的启发。我们断定PID控制器的成功表明，许多控制任务只需要求和和求差来累积信息。基于这个原则，我们提出了两种用于编码历史记录的架构：一种直接使用...",
    "tldr": "该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。",
    "en_tdlr": "This paper is inspired by the PID controller and proposes two architectures for encoding history, striking a balance between flexibility in extracting relevant information and robustness to environmental changes."
}