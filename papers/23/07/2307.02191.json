{
    "title": "Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])",
    "abstract": "For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities",
    "link": "http://arxiv.org/abs/2307.02191",
    "context": "Title: Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])\nAbstract: For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities",
    "path": "papers/23/07/2307.02191.json",
    "total_tokens": 928,
    "translated_title": "在不确定的基准事实下评估AI系统：皮肤病例研究",
    "translated_abstract": "为了安全起见，在部署之前，卫生领域的AI系统需要经过全面的评估，将其预测结果与假定为确定的基准事实进行验证。然而，实际情况并非如此，基准事实可能是不确定的。不幸的是，在标准的AI模型评估中，这一点被大部分忽视了，但是它可能会产生严重后果，如高估未来的性能。为了避免这种情况，我们测量了基准事实的不确定性，我们假设它可以分解为两个主要部分：注释不确定性是由于缺乏可靠注释，以及由于有限的观测信息而导致的固有不确定性。在确定地聚合注释时，通常会忽视这种基准事实的不确定性，例如通过多数投票或平均值来聚合。相反，我们提出了一个框架，在该框架中使用统计模型进行注释的聚合。具体而言，我们将注释的聚合框架解释为所谓可能性的后验推断。",
    "tldr": "这项研究总结了在健康领域中评估AI系统时的一个重要问题：基准事实的不确定性。现有的方法通常忽视了这一点，而该研究提出了一种使用统计模型聚合注释的框架，以更准确地评估AI系统的性能。",
    "en_tdlr": "This study addresses the issue of uncertainty in the ground truth when evaluating AI systems in the healthcare domain. The proposed framework utilizes a statistical model to aggregate annotations, providing more accurate performance assessment."
}