{
    "title": "KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding. (arXiv:2307.03115v1 [cs.CL])",
    "abstract": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the",
    "link": "http://arxiv.org/abs/2307.03115",
    "context": "Title: KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding. (arXiv:2307.03115v1 [cs.CL])\nAbstract: Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the",
    "path": "papers/23/07/2307.03115.json",
    "total_tokens": 972,
    "translated_title": "KoRC: 面向深度文本理解的知识导向阅读理解基准",
    "translated_abstract": "近年来，许多基准测试都强调了深度文本理解对于给定文档和文本以外的先验知识之间的联系的需求。然而，这些基准测试遇到了两个主要的限制。一方面，大多数基准测试需要人工注释知识，导致了知识覆盖的限制。另一方面，它们通常使用文本中的选项或跨度作为答案，导致了狭窄的答案空间。为克服这些限制，我们在本文中建立了一个新的具有挑战性的基准测试，名为KoRC。与先前的基准测试相比，KoRC具有两个优点，即广泛的知识覆盖和灵活的答案格式。具体来说，我们利用海量知识库来指导注释者或大型语言模型（LLM）构建有见地的问题。此外，我们使用知识库中的标签作为最终答案，而不是跨度或选项。我们在KoRC上测试了最先进的模型，实验结果表明",
    "tldr": "近年来，深度文本理解的重要性在许多基准测试中得到了强调。为了克服已有基准测试的限制，本论文提出了一个新的基准测试，KoRC，在知识覆盖和答案格式上具有优势。实验结果表明，KoRC可以帮助改进文本理解模型的性能。"
}