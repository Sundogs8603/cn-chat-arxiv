{
    "title": "Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])",
    "abstract": "The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce mono",
    "link": "http://arxiv.org/abs/2307.07512",
    "context": "Title: Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])\nAbstract: The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce mono",
    "path": "papers/23/07/2307.07512.json",
    "total_tokens": 873,
    "translated_title": "表达性单调神经网络",
    "translated_abstract": "神经网络输出对某些输入的单调依赖是许多场景中的重要归纳偏见，其中领域知识要求这种行为。这对解释性和公平性考虑尤为重要。在更广泛的背景下，单调性重要的情况可在金融、医学、物理等学科中找到。因此，构建能够可靠实现这种归纳偏见的神经网络架构是值得期望的。在本文中，我们提出了一种带有单个残差连接的权重约束架构，以实现任意子集的输入的精确单调依赖。权重约束方案直接控制神经网络的Lipschitz常数，从而提供额外的鲁棒性。与目前用于单调性的现有技术相比，我们的方法在实现和理论基础上更简单，计算开销几乎可以忽略不计，并且保证产生单调输出。",
    "tldr": "提出了一种带有单个残差连接的权重约束架构，可以实现任意子集的输入的精确单调依赖。与现有技术相比，该方法实施简单且理论基础更简化，计算开销很小，并且保证产生单调输出。",
    "en_tdlr": "A weight-constrained architecture with a single residual connection is proposed to achieve exact monotonic dependence in any subset of the inputs. Compared to existing techniques, this method is simpler to implement and has negligible computational overhead, while guaranteeing monotonic outputs."
}