{
    "title": "Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])",
    "abstract": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release o",
    "link": "http://arxiv.org/abs/2307.06166",
    "context": "Title: Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])\nAbstract: Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release o",
    "path": "papers/23/07/2307.06166.json",
    "total_tokens": 980,
    "translated_title": "Vision-Language Models能成为良好猜测器吗？探索VLMs用于时间和位置推理",
    "translated_abstract": "期望Vision-Language Models（VLMs）能像人一样具备常识知识进行推理。一个例子是，人类可以根据他们的知识推断出一张图片的拍摄地点和时间。这让我们想知道，基于视觉线索，使用大规模图像-文本资源进行预训练的Vision-Language Models是否能够达到甚至超过人类在时间和位置推理方面的能力。为了回答这个问题，我们提出了一个两阶段的识别和推理探测任务，应用于鉴别性和生成性的VLMs，以发现VLMs能否识别出与时间和位置相关的特征，并进一步进行推理。为了方便这项研究，我们引入了WikiTiLo，一个包含丰富社会文化线索的精心策划的图像数据集。在广泛的实验研究中，我们发现虽然VLMs能够有效地保留视觉编码器中的相关特征，但在推理方面仍存在不完善的问题。我们将发布...",
    "tldr": "本研究探索了使用Vision-Language Models（VLMs）进行时间和位置推理的能力，并提出了一个两阶段的识别和推理探测任务来研究VLMs的推理能力。实验发现，尽管VLMs能够有效地识别时间和位置相关特征，但在推理方面仍存在改进的空间。",
    "en_tdlr": "This study investigates the capability of Vision-Language Models (VLMs) in reasoning about times and locations, and proposes a two-stage recognition and reasoning probing task to explore their reasoning ability. Experimental results reveal that while VLMs can effectively recognize time and location-related features, there is still room for improvement in their reasoning capabilities."
}