{
    "title": "Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \\textit{flatter} minima has a better generalization ability than the \\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.",
    "link": "http://arxiv.org/abs/2307.08910",
    "context": "Title: Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \\textit{flatter} minima has a better generalization ability than the \\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.",
    "path": "papers/23/07/2307.08910.json",
    "total_tokens": 869,
    "translated_title": "锐度感知的图协同过滤",
    "translated_abstract": "图神经网络(GNNs)在协同过滤中取得了令人印象深刻的性能。然而，当训练和测试数据的分布不太一致时，GNNs往往会产生较差的性能。此外，训练GNNs需要优化非凸神经网络，有大量局部最小值和全局最小值，这些在测试时可能性能差异很大。因此，选择最小值非常重要。在这里，我们提出了一种有效的训练方案，称为{gSAM}，根据“平坦”最小值比“锐利”最小值具有更好的泛化能力的原则。为了实现这个目标，gSAM通过形成双层优化来正则化权重损失函数的平坦程度：外部问题进行标准模型训练，而内部问题则帮助模型跳出锐利的最小值。实验结果显示了我们的gSAM的优越性。",
    "tldr": "锐度感知的图协同过滤中，提出了一种名为gSAM的训练方案，通过正则化权重损失函数的平坦程度，选择平坦最小值以提高泛化能力，在协同过滤中取得了优越的性能。",
    "en_tdlr": "In the field of graph collaborative filtering, a training schema called gSAM is proposed, which selects flatter minima to enhance generalization ability by regularizing the flatness of weight loss landscape, achieving superior performance."
}