{
    "title": "Rethinking Backdoor Attacks. (arXiv:2307.10163v1 [cs.CR])",
    "abstract": "In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.  In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to \"detect\" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this a",
    "link": "http://arxiv.org/abs/2307.10163",
    "context": "Title: Rethinking Backdoor Attacks. (arXiv:2307.10163v1 [cs.CR])\nAbstract: In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.  In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to \"detect\" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this a",
    "path": "papers/23/07/2307.10163.json",
    "total_tokens": 945,
    "translated_title": "重新思考后门攻击",
    "translated_abstract": "在后门攻击中，对手会将恶意构造的后门示例插入训练集中，使得生成的模型容易受到操纵。防御这种攻击通常涉及将这些插入的示例视为训练集中的异常值，并使用鲁棒统计学的技术来检测和删除它们。在这项工作中，我们提出了一种不同的解决后门攻击问题的方法。具体而言，我们展示了在没有关于训练数据分布的结构信息的情况下，后门攻击与数据中自然产生的特征是不可区分的--因此无法在一般意义上“检测”它们。然后，根据这一观察，我们重新审视现有的抵御后门攻击的方法，并表征它们所做出的（常常是潜在的）假设以及它们依赖的假设。最后，我们探索了一种关于后门攻击的替代视角：假设这些攻击对应于训练数据中最强的特征。",
    "tldr": "本文重新思考了后门攻击问题，发现在没有关于训练数据分布的结构信息的情况下，后门攻击与数据中自然产生的特征是不可区分的，因此难以检测。作者还重新审视现有的抵御后门攻击的方法，并探索了一种关于后门攻击的替代视角。",
    "en_tdlr": "This paper rethinks the problem of backdoor attacks and discovers that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data, making them difficult to detect. The authors also reexamine existing defenses against backdoor attacks and explore an alternative perspective on backdoor attacks."
}