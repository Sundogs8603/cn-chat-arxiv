{
    "title": "Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])",
    "abstract": "As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.",
    "link": "http://arxiv.org/abs/2307.10575",
    "context": "Title: Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])\nAbstract: As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.",
    "path": "papers/23/07/2307.10575.json",
    "total_tokens": 901,
    "translated_title": "使用原型正则化提升联邦学习的收敛性",
    "translated_abstract": "作为一种分布式机器学习技术，联邦学习（FL）要求客户端在不泄露本地数据的情况下与边缘服务器共同训练共享模型。然而，客户端之间的异构数据分布往往导致模型性能下降。为了解决这个问题，本文引入了一种基于原型的正则化策略来解决数据分布的异质性。具体而言，正则化过程涉及服务器从分布式客户端聚合本地原型以生成全局原型，然后将其发送回个体客户端以指导其本地训练。在MNIST和Fashion-MNIST上的实验结果表明，与最流行的基准FedAvg相比，我们的提议分别在平均测试准确率上实现了3.3%和8.9%的改进。此外，我们的方法在异构环境中具有快速收敛速度。",
    "tldr": "本论文通过引入原型正则化策略来解决联邦学习中异质数据分布的问题，实验证明在MNIST和Fashion-MNIST数据集上相比基准模型FedAvg，我们的方法分别提高了3.3%和8.9%的平均测试准确率，并且在异构设置下具有快速收敛速度。",
    "en_tdlr": "This paper proposes a prototype-based regularization strategy to address the heterogeneity in data distribution in federated learning. Experimental results on MNIST and Fashion-MNIST datasets show that the proposed method achieves improvements of 3.3% and 8.9% in average test accuracy compared to the baseline model FedAvg, and it also has a fast convergence rate in heterogeneous settings."
}