{
    "title": "Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation. (arXiv:2307.12219v1 [cs.LG])",
    "abstract": "Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly ",
    "link": "http://arxiv.org/abs/2307.12219",
    "context": "Title: Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation. (arXiv:2307.12219v1 [cs.LG])\nAbstract: Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly ",
    "path": "papers/23/07/2307.12219.json",
    "total_tokens": 915,
    "translated_title": "通过生成插值改进分类器的超领域鲁棒性",
    "translated_abstract": "深度神经网络在学习独立同分布（i.i.d.）数据时表现出优越的性能。然而，当处理超领域（OoD）数据时，其性能显著下降，即训练和测试数据来自不同的分布。本文通过利用生成模型作为数据增强源，探索了改进神经分类器的超领域鲁棒性的方法。具体而言，我们提出了一种简单而有效的方法，称为生成插值，用于合成多个域的生成模型训练的多样化OoD样本。直接在源域上训练生成模型往往容易出现模式崩溃，并且有时会放大数据偏差。因此，我们首先在一个源域上训练StyleGAN模型，然后在其他域上微调，从而得到许多相关的生成器，它们的模型参数具有相同的初始化，因此是对齐的。",
    "tldr": "本文提出了一种通过生成插值方法，利用生成模型合成多样化超领域样本来改进分类器的鲁棒性。通过在一个源域上训练StyleGAN模型，然后在其他域上微调，得到了多个相关的生成器，从而提升了性能。",
    "en_tdlr": "This paper proposes a method called Generative Interpolation to improve the out-of-distribution (OoD) robustness of classifiers by synthesizing diverse OoD samples using generative models. By training a StyleGAN model on one source domain and fine-tuning it on other domains, multiple correlated generators are obtained, which enhances the performance."
}