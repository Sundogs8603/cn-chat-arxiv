{
    "title": "SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces",
    "abstract": "Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility",
    "link": "https://arxiv.org/abs/2307.12445",
    "context": "Title: SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces\nAbstract: Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility",
    "path": "papers/23/07/2307.12445.json",
    "total_tokens": 880,
    "translated_title": "SCRAPS: 声音和语音空间的对比表征",
    "translated_abstract": "文献中的众多例子证明了深度学习模型在多模态数据上的良好表现。最近，CLIP使得深度学习系统能够学习图像和文本描述之间的共享潜在空间，在下游任务中具有杰出的零次或少次测试结果。在本文中，我们探索了CLIP提出的相同思想，但应用于语音领域，其中语音和声学空间通常共存。我们训练了一个基于CLIP的模型，旨在学习语音和声学空间的共享表征。结果表明，所提出的模型对语音变化敏感，将20％随机替换的音素得分下降了91％，同时对不同类型的噪声提供了实质性的鲁棒性，将音频与75％的高斯噪声混合时性能下降了10％。我们还提供了经验证据表明，所得到的嵌入对于各种下游应用是有用的，比如可懂度。",
    "tldr": "本文提出了一个基于CLIP的模型，旨在学习语音和声学空间的共享表征。实验证明该模型对语音变化敏感，具有鲁棒性并适用于多种下游应用。",
    "en_tdlr": "This paper presents a CLIP-based model to learn shared representations of phonetic and acoustic spaces. The model is shown to be sensitive to phonetic changes, robust against different types of noise, and useful for various downstream applications."
}