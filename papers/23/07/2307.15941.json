{
    "title": "Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])",
    "abstract": "Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo",
    "link": "http://arxiv.org/abs/2307.15941",
    "context": "Title: Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])\nAbstract: Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo",
    "path": "papers/23/07/2307.15941.json",
    "total_tokens": 841,
    "translated_title": "在预测自动缩放中的持续学习",
    "translated_abstract": "预测自动缩放被用于预测服务器的工作负载，并提前准备资源，以确保在动态云环境中的服务水平目标（SLOs）。然而，在实践中，其预测任务常常在外部事件（如促销活动和应用程序重新配置）引起的异常流量下性能下降，常见的解决方案是使用长时间历史数据重新训练模型，但代价是高计算和存储成本。为了更好地解决这个问题，我们提出了一种基于重放的持续学习方法，即基于密度的记忆选择和基于提示的网络学习模型（DMSHM），只使用一小部分历史日志数据来实现准确的预测。首先，我们发现了在预测任务中应用重放式持续学习时的样本重叠现象。为了克服这一挑战并有效地整合新的样本分布，我们提出了一种基于密度的记忆选择和基于提示的网络学习模型（DMSHM）。",
    "tldr": "本论文提出了一种基于重放的持续学习方法，使用少量历史数据，解决了预测自动缩放中的性能下降问题。"
}