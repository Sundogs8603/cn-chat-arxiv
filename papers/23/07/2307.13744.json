{
    "title": "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization. (arXiv:2307.13744v1 [cs.LG])",
    "abstract": "Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block",
    "link": "http://arxiv.org/abs/2307.13744",
    "context": "Title: mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization. (arXiv:2307.13744v1 [cs.LG])\nAbstract: Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block",
    "path": "papers/23/07/2307.13744.json",
    "total_tokens": 925,
    "translated_title": "mL-BFGS:一种基于动量的大规模分布式神经网络优化的L-BFGS算法",
    "translated_abstract": "鉴于Hessian相关计算中的额外计算成本和随机训练中的不稳定性问题，在训练大规模神经网络时，拟牛顿方法仍面临着重大挑战。已知的高效近似Hessian的L-BFGS方法，在随机训练中会出现收敛不稳定的问题。迄今为止，将L-BFGS适应于大规模随机训练的尝试带来了相当大的额外开销，这抵消了其在实际时间上的收敛优势。在本文中，我们提出了mL-BFGS算法，一种轻量级基于动量的L-BFGS算法，为大规模分布式深度神经网络(DNN)优化中的拟牛顿方法铺平了道路。mL-BFGS将近乎免费的动量方案引入到L-BFGS更新中，并大大减少Hessian中的随机噪声，从而在随机优化过程中稳定收敛。对于大规模模型训练，mL-BFGS使用块近似Hessian。",
    "tldr": "mL-BFGS是一种轻量级的基于动量的L-BFGS算法，通过引入动量方案和减少Hessian中的随机噪声，稳定了大规模分布式深度神经网络的优化过程。",
    "en_tdlr": "mL-BFGS is a lightweight momentum-based L-BFGS algorithm that stabilizes large-scale distributed deep neural network optimization by introducing a momentum scheme and reducing stochastic noise in the Hessian."
}