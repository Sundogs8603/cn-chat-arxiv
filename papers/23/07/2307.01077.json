{
    "title": "Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities. (arXiv:2307.01077v1 [stat.ML])",
    "abstract": "Manifold learning approaches seek the intrinsic, low-dimensional data structure within a high-dimensional space. Mainstream manifold learning algorithms, such as Isomap, UMAP, $t$-SNE, Diffusion Map, and Laplacian Eigenmaps do not use data labels and are thus considered unsupervised. Existing supervised extensions of these methods are limited to classification problems and fall short of uncovering meaningful embeddings due to their construction using order non-preserving, class-conditional distances. In this paper, we show the weaknesses of class-conditional manifold learning quantitatively and visually and propose an alternate choice of kernel for supervised dimensionality reduction using a data-geometry-preserving variant of random forest proximities as an initialization for manifold learning methods. We show that local structure preservation using these proximities is near universal across manifold learning approaches and global structure is properly maintained using diffusion-based",
    "link": "http://arxiv.org/abs/2307.01077",
    "context": "Title: Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities. (arXiv:2307.01077v1 [stat.ML])\nAbstract: Manifold learning approaches seek the intrinsic, low-dimensional data structure within a high-dimensional space. Mainstream manifold learning algorithms, such as Isomap, UMAP, $t$-SNE, Diffusion Map, and Laplacian Eigenmaps do not use data labels and are thus considered unsupervised. Existing supervised extensions of these methods are limited to classification problems and fall short of uncovering meaningful embeddings due to their construction using order non-preserving, class-conditional distances. In this paper, we show the weaknesses of class-conditional manifold learning quantitatively and visually and propose an alternate choice of kernel for supervised dimensionality reduction using a data-geometry-preserving variant of random forest proximities as an initialization for manifold learning methods. We show that local structure preservation using these proximities is near universal across manifold learning approaches and global structure is properly maintained using diffusion-based",
    "path": "papers/23/07/2307.01077.json",
    "total_tokens": 886,
    "translated_title": "通过随机森林保持几何特性的近似来进行监督流形学习",
    "translated_abstract": "流形学习方法旨在在高维空间中寻找内在的低维数据结构。主流的流形学习算法，例如Isomap，UMAP，t-SNE，Diffusion Map和Laplacian Eigenmaps，不使用数据标签，因此被认为是无监督的。现有的这些方法的有监督扩展仅适用于分类问题，并且由于使用了不保持顺序的类条件距离而未能揭示有意义的嵌入。在本文中，我们定量和可视化地展示了类条件流形学习的弱点，并提出了一种替代选择，在流形学习方法中使用数据几何保持的随机森林近似作为初始化。我们展示了使用这些近似方法进行局部结构保持在几乎所有流形学习方法中都是普遍的，并且使用基于扩散的方法能够正确地维护全局结构。",
    "tldr": "本文通过使用随机森林近似的几何保持特性作为流形学习方法的初始化，展示了类条件流形学习的局限性，并提出了一种替代选择。这种方法能够在几乎所有流形学习方法中保持局部结构，并正确地维护全局结构。",
    "en_tdlr": "This paper shows the limitations of class-conditional manifold learning and proposes an alternative approach using random forest approximations with geometry-preserving properties for initialization. The proposed method preserves local structure in almost all manifold learning approaches and maintains global structure accurately."
}