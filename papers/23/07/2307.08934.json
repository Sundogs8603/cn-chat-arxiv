{
    "title": "Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])",
    "abstract": "Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-",
    "link": "http://arxiv.org/abs/2307.08934",
    "context": "Title: Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])\nAbstract: Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-",
    "path": "papers/23/07/2307.08934.json",
    "total_tokens": 856,
    "translated_title": "多阶段神经网络：机器精度的函数逼近器",
    "translated_abstract": "深度学习技术越来越多地应用于科学问题，网络的精度至关重要。尽管神经网络被认为是通用的函数逼近器，但在实践中，即使使用大型网络和扩展的训练迭代，它们仍然难以将预测误差降低到10的负5次方以下。为了解决这个问题，我们开发了多阶段神经网络，将训练过程分为不同的阶段，每个阶段使用一个新的网络，针对前一阶段的残差进行优化拟合。在连续的阶段中，残差幅度显著减小，并且与残差频率呈反幂律关系。多阶段神经网络有效地减小了常规神经网络中的谱偏差，使其能够捕捉目标函数的高频特征。我们证明了多阶段训练在回归问题和物理问题的预测误差上的有效性。",
    "tldr": "多阶段神经网络通过将训练过程分为不同阶段，并优化拟合残差，成功解决了常规神经网络中的谱偏差问题，提高了预测精度。",
    "en_tdlr": "The multi-stage neural networks address the spectral biases issue in regular neural networks by dividing the training process into different stages, optimizing the fit of residues, and effectively improving the prediction accuracy."
}