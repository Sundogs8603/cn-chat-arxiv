{
    "title": "Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])",
    "abstract": "We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo",
    "link": "http://arxiv.org/abs/2307.09883",
    "context": "Title: Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])\nAbstract: We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo",
    "path": "papers/23/07/2307.09883.json",
    "total_tokens": 899,
    "translated_title": "VAE的对称均衡学习",
    "translated_abstract": "我们将变分自动编码器（VAE）视为解码器-编码器对，将数据空间中的分布映射到潜在空间中的分布，反之亦然。VAEs的标准学习方法，即最大化证据下界（ELBO），存在明显的不对称性。此外，它需要一个闭合形式的先验潜在分布。这限制了VAEs在更复杂的情况下的适用性，如一般的半监督学习和使用复杂的生成模型作为先验。我们提出了一种纳什均衡学习方法，放宽了这些限制，在只能通过采样获得数据和潜在分布的情况下学习VAEs。这种方法的灵活性和简单性使其适用于广泛的学习场景和下游任务。实验证明，通过这种方法学习的模型与ELBO学习获得的模型相当，并展示了其在实践中的适用性。",
    "tldr": "本文提出了一种对称均衡学习方法，允许在只能通过采样获得数据和潜在分布的情况下学习VAEs。实验证明该方法与传统ELBO学习方法获得的模型相当，并具有广泛的应用性。"
}