{
    "title": "Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])",
    "abstract": "Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.",
    "link": "http://arxiv.org/abs/2307.04887",
    "context": "Title: Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])\nAbstract: Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.",
    "path": "papers/23/07/2307.04887.json",
    "total_tokens": 837,
    "translated_title": "测量和缓解强化学习中的干扰",
    "translated_abstract": "灾难性干扰在许多基于网络的学习系统中很常见，并且存在许多减轻干扰的建议。在克服干扰之前，我们必须更好地理解它。在这项工作中，我们为Fitted Q-Iteration和DQN等基于值的强化学习方法提供了干扰的定义和新型度量。我们系统地评估了我们的干扰度量，在各种网络架构上显示出它与控制性能的不稳定性相关。我们的新干扰度量使我们能够提出关于常用深度学习架构的新科学问题，并研究减轻干扰的学习算法。最后，我们概述了一类我们称为在线感知算法的算法，旨在减轻干扰，并且根据我们的度量显示它们减少了干扰，并在几个经典的控制环境中提高了稳定性和性能。",
    "tldr": "本文提供了一种衡量强化学习中干扰的新方法，并且提出了一类在线感知算法来减轻干扰，这些算法在经典控制环境中提高了稳定性和性能。",
    "en_tdlr": "This paper presents a novel measure of interference in value-based reinforcement learning methods and proposes a class of online-aware algorithms to mitigate interference, which improve stability and performance in classic control environments."
}