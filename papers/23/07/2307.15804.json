{
    "title": "On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])",
    "abstract": "Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \\phi( x \\cdot \\theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\\phi$ applied to an unknown one-dimensional projection $\\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \\cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl",
    "link": "http://arxiv.org/abs/2307.15804",
    "context": "Title: On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])\nAbstract: Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \\phi( x \\cdot \\theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\\phi$ applied to an unknown one-dimensional projection $\\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \\cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl",
    "path": "papers/23/07/2307.15804.json",
    "total_tokens": 837,
    "translated_title": "关于高斯数据之外的单指数模型",
    "translated_abstract": "稀疏高维函数已成为研究使用浅层神经网络的梯度下降方法行为的丰富框架，展示了它们在线性模型之外进行特征学习的能力。其中最简单的是单指数模型 $f(x) = \\phi( x \\cdot \\theta^*)$，其中标签由一个未知的一维投影 $\\theta^*$ 应用于任意非线性标量连接函数 $\\phi$ 产生。通过专注于高斯数据，最近几项研究工作建立了一个引人注目的图景，将信息指数（与连接函数的正则性相关）与所需的样本复杂性进行了控制。实质上，这些工具利用了高斯分布的稳定性和球对称性。在本研究中，我们从 \\cite{arous2020online} 的框架出发，探索了超越高斯设定的这个图景的扩展，其中稳定性或对称性可能被违反。",
    "tldr": "该论文研究了超越高斯数据的单指数模型，探索了对稳定性和对称性的违反情况下的样本复杂性控制。",
    "en_tdlr": "This paper explores extensions of single index models beyond Gaussian data and investigates how the violation of stability and symmetry affects the sample complexity control."
}