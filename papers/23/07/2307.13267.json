{
    "title": "Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization. (arXiv:2307.13267v1 [math.OC])",
    "abstract": "The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering traini",
    "link": "http://arxiv.org/abs/2307.13267",
    "context": "Title: Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization. (arXiv:2307.13267v1 [math.OC])\nAbstract: The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering traini",
    "path": "papers/23/07/2307.13267.json",
    "total_tokens": 879,
    "translated_title": "基于双分解的分布式优化的联邦K均值聚类",
    "translated_abstract": "在机器学习中，分布式优化的使用可以通过保护隐私或提高计算效率来进行动机。一方面，训练数据可能存储在多个设备上。在每个节点只能访问其保密数据的网络中训练全局模型需要使用分布式算法。即使数据不是机密的，由于带宽限制，共享数据可能是禁止的。另一方面，不断增加的可用数据量导致了大规模的机器学习问题。通过将训练过程分割成多个节点，可以显著提高其效率。本文旨在演示如何使用双分解来进行分布式训练K均值聚类问题。在对分布式和联邦机器学习进行概述后，给出了基于混合整数二次约束规划的K均值聚类训练的公式化描述。",
    "tldr": "本论文介绍了如何使用双分解方法进行分布式训练K均值聚类问题，以提高隐私保护或计算效率。论文通过对分布式和联邦机器学习进行概述，并给出了基于规划方法的K均值聚类训练的公式化描述。",
    "en_tdlr": "This paper demonstrates how dual decomposition can be applied for distributed training of K-means clustering problems, aiming to improve privacy preservation or computational efficiency. The paper provides an overview of distributed and federated machine learning and presents a formulation based on mixed-integer quadratically constrained programming for K-means clustering training."
}