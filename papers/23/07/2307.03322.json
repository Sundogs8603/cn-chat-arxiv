{
    "title": "BiPhone: Modeling Inter Language Phonetic Influences in Text. (arXiv:2307.03322v1 [cs.CL])",
    "abstract": "A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1). We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web. We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also",
    "link": "http://arxiv.org/abs/2307.03322",
    "context": "Title: BiPhone: Modeling Inter Language Phonetic Influences in Text. (arXiv:2307.03322v1 [cs.CL])\nAbstract: A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1). We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web. We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also",
    "path": "papers/23/07/2307.03322.json",
    "total_tokens": 983,
    "translated_title": "BiPhone:模拟语音学关系模型 L2 文本中的跨语言影响",
    "translated_abstract": "由于技术不对称性，许多人被迫在自己不擅长的语言中使用网络。这些用户在第二语言(L2)中的书面文本通常包含大量受其母语(L1)影响的错误。我们提出了一种方法，用于挖掘L1和L2对之间的音素歧义(可能导致L1说话者混淆的L2语音)。然后，将这些歧义插入一个生成模型(Bi-Phone)中，用于合成被破坏的L2文本。通过人工评估，我们展示了Bi-Phone生成的破坏是可信的，并且在不同的L1上有广泛的覆盖性。我们还通过使用我们的技术(Phonetically Noised GLUE的FunGLUE)破坏了流行的语言理解基准SuperGLUE，并展示了当前最佳的语言理解模型性能较差。最后，我们还引入了一项新的音素预测预训练任务，帮助字节模型恢复接近SuperGLUE的性能。",
    "tldr": "这篇论文提出了一种模拟跨语言影响的方法，通过挖掘L1和L2之间的音素歧义，生成合成的受干扰的L2文本。通过人工评估和实验结果表明，该方法可以生成可信的受干扰的L2文本，并对流行的语言理解模型造成负面影响。"
}