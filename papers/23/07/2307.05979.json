{
    "title": "Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])",
    "abstract": "Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i",
    "link": "http://arxiv.org/abs/2307.05979",
    "context": "Title: Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])\nAbstract: Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i",
    "path": "papers/23/07/2307.05979.json",
    "total_tokens": 962,
    "translated_title": "基于Transformer的强化学习综述",
    "translated_abstract": "Transformer在自然语言处理、计算机视觉和机器人等领域具有显著的影响，在性能上优于其他神经网络。本综述探讨了Transformer在强化学习（RL）中的应用，它被视为解决不稳定训练、信用分配、缺乏可解释性和部分可观测性等挑战的有希望解决方案。我们首先简要介绍了RL领域的概述，然后讨论了传统RL算法的挑战。接下来，我们深入探讨了Transformer及其变体的特性，并讨论了使其适合应对RL中固有挑战的特点。我们还研究了Transformer在RL中各个方面的应用，包括表示学习、状态转移和奖励函数建模以及策略优化。此外，我们还讨论了最近的研究，旨在提高Transformer的可解释性和效率。",
    "tldr": "本综述介绍了基于Transformer的强化学习的应用。Transformer在解决强化学习中的挑战上具有潜在优势，包括不稳定训练、信用分配、缺乏可解释性和部分可观测性等方面。研究还探讨了其在表示学习、状态转移和奖励函数建模以及策略优化等方面的应用。此外，还有一些最新的研究旨在提高Transformer的可解释性和效率。",
    "en_tdlr": "This survey explores the application of Transformers in reinforcement learning, highlighting their potential in addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. It discusses their use in representation learning, transition and reward function modeling, and policy optimization. Recent research aimed at improving the interpretability and efficiency of Transformers is also discussed."
}