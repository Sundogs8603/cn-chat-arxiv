{
    "title": "On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])",
    "abstract": "Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model's overall performance, by simply changing the data order for a single epoch.",
    "link": "http://arxiv.org/abs/2307.04138",
    "context": "Title: On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])\nAbstract: Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model's overall performance, by simply changing the data order for a single epoch.",
    "path": "papers/23/07/2307.04138.json",
    "total_tokens": 971,
    "translated_title": "关于机器学习随机性对群体公平性影响的论文",
    "translated_abstract": "机器学习中用于衡量群体公平性的统计指标反映了不同群体算法性能之间的差距。然而，这些指标在不同训练实例之间存在很高的方差，使得它们在公平性的经验评估中不可靠。是什么导致了这种高方差？我们研究了训练神经网络中不同随机因素对群体公平性的影响。我们发现群体公平性指标的方差根植于在代表性不足的群体上的学习过程的高易变性。此外，我们确定了随机性主要来源是训练期间数据顺序的随机性。基于这些发现，我们展示了如何通过改变单个时期的数据顺序来控制群体级准确性（即模型公平性），并且对模型整体性能几乎没有影响。",
    "tldr": "本文研究了在机器学习中，随机性对群体公平性的影响。研究发现，群体公平性指标的方差主要来自于在代表性不足的群体上的学习过程的高易变性，其中最主要的随机性来源是训练期间数据顺序的随机性。基于这些发现，可以通过改变数据顺序来控制模型的群体级准确性，而几乎不影响模型的整体性能。",
    "en_tdlr": "This paper investigates the impact of randomness on group fairness in machine learning. The study finds that the variance in group fairness measures is mainly due to the high volatility of the learning process on under-represented groups, with the stochasticity of data order during training being the dominant source of randomness. Based on these findings, control over group-level accuracy (model fairness) can be achieved by changing the data order, with negligible impact on the model's overall performance."
}