{
    "title": "LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])",
    "abstract": "Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained feature descriptions to produce visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct",
    "link": "http://arxiv.org/abs/2307.14142",
    "context": "Title: LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])\nAbstract: Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained feature descriptions to produce visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct",
    "path": "papers/23/07/2307.14142.json",
    "total_tokens": 870,
    "translated_title": "LOIS: 在视觉问答中观察实例语义",
    "translated_abstract": "视觉问答(VQA)作为一项多模态任务，需要在视觉和语言之间进行努力，以正确地推断答案。最近的研究尝试开发了各种基于注意力的模块来解决VQA任务。然而，模型推断的性能主要受限于对语义理解的视觉处理。大多数现有的检测方法依赖于边界框，这对于VQA模型理解图像中物体语义的因果关系并正确推断上下文信息仍然是一个严峻的挑战。为此，我们在这项工作中提出了一个更精细的模型框架，名为Looking Out of Instance Semantics (LOIS)，以解决这个重要问题。LOIS能够生成更精细的特征描述，以产生视觉事实。此外，为了克服实例掩码引起的标签不确定性，我们设计了两种类型的关系注意力模块：1) 内部模态和2) 交互模态，用于推断正确的上下文关系。",
    "tldr": "LOIS是一个新的模型框架，用于解决视觉问答中的语义理解问题。它不依赖于边界框，并使用精细的特征描述来生成视觉事实。另外，LOIS通过两种类型的关系注意力模块来解决由实例掩码引起的标签不确定性。"
}