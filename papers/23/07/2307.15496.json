{
    "title": "From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])",
    "abstract": "The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous ",
    "link": "http://arxiv.org/abs/2307.15496",
    "context": "Title: From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])\nAbstract: The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous ",
    "path": "papers/23/07/2307.15496.json",
    "total_tokens": 884,
    "translated_title": "从连续时间表述到离散化方案：张量列车和鲁棒回归用于BSDEs和抛物线PDEs",
    "translated_abstract": "在高维度中数值逼近偏微分方程（PDEs）面临着巨大的挑战，因为传统的基于网格的方法受到所谓维数诅咒的限制。最近的尝试依赖于蒙特卡罗方法和变分表述的组合，利用神经网络进行函数逼近。延续之前的工作（Richter等，2021），我们认为张量列车为抛物型PDE提供了一种吸引人的框架：通过以逆向随机微分方程和回归类型方法的形式重新表述，结合潜在的低秩结构，有望实现压缩和高效计算的目标。强调连续时间观点，我们开发了迭代方案，其在计算效率和鲁棒性方面有所不同。我们从理论和数值上都证明了我们的方法能够在精度和计算效率之间取得有利的折中。",
    "tldr": "本论文通过使用张量列车方法结合回归类型方法来解决高维度偏微分方程的数值逼近问题。实验结果表明，该方法在精度和计算效率之间取得了有利的折中。",
    "en_tdlr": "This paper addresses the numerical approximation of high-dimensional partial differential equations using tensor train methods combined with regression-type methods. Experimental results show that the proposed approach achieves a favorable trade-off between accuracy and computational efficiency."
}