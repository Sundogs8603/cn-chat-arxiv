{
    "title": "Multi-Similarity Contrastive Learning. (arXiv:2307.02712v1 [cs.LG])",
    "abstract": "Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain",
    "link": "http://arxiv.org/abs/2307.02712",
    "context": "Title: Multi-Similarity Contrastive Learning. (arXiv:2307.02712v1 [cs.LG])\nAbstract: Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain",
    "path": "papers/23/07/2307.02712.json",
    "total_tokens": 837,
    "translated_title": "多相似度对比学习",
    "translated_abstract": "在给定相似度度量的情况下，对比学习方法学习一种表示，其中相似的样本被推到一起，不相似的样本被拉开。对比学习技术已被广泛应用于学习用于图像分类到字幕生成等任务的表示。然而，现有的对比学习方法可能在泛化方面存在问题，因为它们没有考虑到不同相似性关系的可能性。在本文中，我们提出了一种新颖的多相似度对比损失（MSCon），通过联合利用多个相似度度量的监督来学习可泛化的嵌入。我们的方法根据相应相似度的不确定性自动学习对比相似度的权重，降低不确定任务的权重，从而实现对新任务的更好领域外泛化。我们的实验证明，使用MSCon训练的网络在领域内优于最先进的基线模型。",
    "tldr": "本文提出了一种多相似度对比损失方法（MSCon），通过联合利用多个相似度度量的监督来学习可泛化的嵌入，从而实现对新任务的更好领域外泛化。",
    "en_tdlr": "This paper proposes a novel multi-similarity contrastive loss (MSCon) that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity, leading to better out-of-domain generalization to new tasks."
}