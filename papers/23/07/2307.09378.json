{
    "title": "Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])",
    "abstract": "A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.",
    "link": "http://arxiv.org/abs/2307.09378",
    "context": "Title: Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])\nAbstract: A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.",
    "path": "papers/23/07/2307.09378.json",
    "total_tokens": 857,
    "translated_title": "为口语评估适应ASR基础模型",
    "translated_abstract": "准确可靠的口语评估系统的关键部分是底层的ASR模型。最近，大规模预训练的ASR基础模型如Whisper已经可用。由于这些模型的输出是人类可读的，所以会添加标点符号，数字呈现为阿拉伯数字形式，包括缩写。此外，这些模型往往会跳过输出中的不流畅和犹豫。虽然对于可读性很有用，但这些属性对于评估候选人的能力和提供反馈并不有用。在这篇论文中，我们对Whisper的输出进行了详细分析，并提出了两种解决方案：微调和软提示微调。在公共语音语料库和英语学习者数据集上进行了实验。结果表明，我们可以有效地改变Whisper的解码行为，生成候选人实际说出的单词。",
    "tldr": "本论文主要针对ASR基础模型Whisper的输出进行了详细分析，并提出了两种解决方案：微调和软提示微调。实验结果表明，可以有效改变Whisper的解码行为，生成候选人实际说出的单词。"
}