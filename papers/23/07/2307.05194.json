{
    "title": "Differentially Private Statistical Inference through $\\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])",
    "abstract": "Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\\beta$-divergence between the model and the data generating process. This provides private estimation t",
    "link": "http://arxiv.org/abs/2307.05194",
    "context": "Title: Differentially Private Statistical Inference through $\\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])\nAbstract: Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\\beta$-divergence between the model and the data generating process. This provides private estimation t",
    "path": "papers/23/07/2307.05194.json",
    "total_tokens": 831,
    "translated_title": "通过$\\beta$-分解一后验采样实现差分计算机学习",
    "translated_abstract": "差分私密性确保了包含敏感数据的统计分析结果可以在不损害任何个体隐私的情况下进行发布。实现这种保证通常需要在参数估计或估计过程中直接注入噪音。而采样来自贝叶斯后验分布已被证明是指数机制的一种特殊情况，可以产生一致且高效的私密估计，而不会改变数据生成过程。然而，当前方法的应用受到较强的边界假设的限制，这些假设对于基本模型（如简单的线性回归器）并不成立。为了改善这一点，我们提出了$\\beta$D-Bayes，一种从广义后验中进行后验采样的方案，目标是最小化模型与数据生成过程之间的$\\beta$-分解。这提供了私密估计的方法。",
    "tldr": "通过对数据生成过程和模型之间的$\\beta$-分解进行后验采样，我们提出了$\\beta$D-Bayes，一种能够实现差分机器学习的方法。",
    "en_tdlr": "We propose $\\beta$D-Bayes, a posterior sampling scheme targeting the minimization of the $\\beta$-divergence between the model and the data generating process, to achieve differentially private machine learning."
}