{
    "title": "A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])",
    "abstract": "Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the",
    "link": "http://arxiv.org/abs/2307.15054",
    "context": "Title: A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])\nAbstract: Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the",
    "path": "papers/23/07/2307.15054.json",
    "total_tokens": 823,
    "translated_title": "一种几何观念的因果探测",
    "translated_abstract": "大型语言模型依赖于文本的实值表示来进行预测。这些表示包含了模型在训练数据上学到的信息，包括语言属性和基于性别的人口偏见等。越来越多的研究关注通过在表示空间的子空间上进行正交投影来获得关于这些概念的信息。我们通过提出语言模型表示空间子空间的内在信息的形式定义，为这项研究贡献了新的内容。我们提出了一种反事实方法来避免虚假相关的失效模式，通过独立处理子空间中的分量和其正交补空间中的分量。我们展示了在子空间中的反事实信息概念是由一个因果概念子空间进行优化的。此外，这种干预使我们能够通过操作来尝试概念控制生成。",
    "tldr": "本文提出了一种几何观念的因果探测方法，通过在语言模型表示空间的子空间上进行反事实干预，优化了因果概念子空间，以实现概念控制生成。"
}