{
    "title": "Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case. (arXiv:2307.11782v1 [math.OC])",
    "abstract": "Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\\sqrt{K})$. More importantly, we prove, for the first time, that the las",
    "link": "http://arxiv.org/abs/2307.11782",
    "context": "Title: Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case. (arXiv:2307.11782v1 [math.OC])\nAbstract: Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\\sqrt{K})$. More importantly, we prove, for the first time, that the las",
    "path": "papers/23/07/2307.11782.json",
    "total_tokens": 923,
    "translated_title": "Adam算法在非凸目标中的收敛性：放松的超参数和非遗传性情况",
    "translated_abstract": "Adam是机器学习中常用的随机优化算法。然而，其收敛性仍然没有得到完全理解，尤其是在非凸设置下。本文主要关注探索Adam的收敛性超参数设置，并解决与实际应用相关的非遗传性收敛的挑战。主要贡献总结如下：首先，我们引入了遗传性和非遗传性收敛的精确定义，涵盖了几乎所有随机优化算法收敛的形式。同时，我们强调非遗传性收敛优于遗传性收敛。其次，我们建立了Adam遗传性收敛保证的一个较弱的充分条件，允许更放松的超参数选择。在此基础上，我们实现了Adam的几乎确定性遗传收敛速度，这个速度可以任意接近$o(1/\\sqrt{K})$。更重要的是，我们首次证明了las",
    "tldr": "该论文研究了Adam算法在非凸目标中的收敛性，通过探索超参数设置和解决非遗传性收敛的挑战，提出了几乎确定性的遗传收敛速率，证明了非遗传性收敛优于遗传性收敛。",
    "en_tdlr": "This paper investigates the convergence of Adam algorithm for non-convex objectives. It proposes a relaxed hyperparameter setting and tackles the challenge of non-ergodic convergence. The primary contribution is achieving an almost sure ergodic convergence rate for Adam, proving the superiority of non-ergodic convergence over ergodic convergence."
}