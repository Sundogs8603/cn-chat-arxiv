{
    "title": "Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])",
    "abstract": "Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi",
    "link": "http://arxiv.org/abs/2307.14988",
    "context": "Title: Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])\nAbstract: Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi",
    "path": "papers/23/07/2307.14988.json",
    "total_tokens": 684,
    "translated_title": "增量计算的神经网络：处理动态输入的高效推理方法",
    "translated_abstract": "深度学习在处理动态输入（例如传感器数据或用户输入）时常面临着高效处理的挑战。本论文提出了一种增量计算的方法，通过重复使用计算来适应输入变化，以解决这个问题。我们使用向量量化来离散化网络中的中间值，并过滤噪声和不必要的隐藏神经元修改，从而促进值的重用。我们将此方法应用于Transformer架构，创建了一个高效的增量推理算法。",
    "tldr": "本论文介绍了一种增量计算的神经网络方法，通过离散化中间值并过滤不必要的修改，实现了对动态输入的高效推理。",
    "en_tdlr": "This paper presents an incrementally-computable neural network approach that achieves efficient inference for dynamic inputs by discretizing intermediate values and filtering unnecessary modifications."
}