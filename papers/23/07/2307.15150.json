{
    "title": "R-Block: Regularized Block of Dropout for convolutional networks. (arXiv:2307.15150v1 [cs.CV])",
    "abstract": "Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.",
    "link": "http://arxiv.org/abs/2307.15150",
    "context": "Title: R-Block: Regularized Block of Dropout for convolutional networks. (arXiv:2307.15150v1 [cs.CV])\nAbstract: Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.",
    "path": "papers/23/07/2307.15150.json",
    "total_tokens": 846,
    "translated_title": "R-Block: 用于卷积网络的正则化Dropout块",
    "translated_abstract": "Dropout作为一种正则化技术，在全连接层中被广泛使用，但在卷积层中效果较差。因此，提出了更有结构性的Dropout形式来正则化卷积网络。这些方法的缺点是引入的随机性导致训练和推断之间的不一致性。本文应用了一种互学习训练策略，即R-Block，来进行卷积层的正则化。它强制两个生成的最大化差异的子模型的输出彼此一致。具体而言，R-Block通过最小化训练数据集中每个样本的两个具有不同Drop区域的子模型的输出分布之间的损失来实现。我们设计了两种构建这样子模型的方法。我们的实验证明，R-Block比其他现有的结构化Dropout变体具有更好的性能。我们还证明了我们构建子模型的方法优于其他方法。",
    "tldr": "本文提出了一种名为R-Block的新型正则化Dropout块，通过互学习训练策略强制两个子模型的输出一致性，以解决卷积层中Dropout效果较差的问题。",
    "en_tdlr": "This paper introduces a new regularization dropout block called R-Block, which addresses the poor performance of dropout in convolutional layers by enforcing consistency between two sub-models' outputs through a mutual learning training strategy."
}