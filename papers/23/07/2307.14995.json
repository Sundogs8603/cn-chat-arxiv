{
    "title": "Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])",
    "abstract": "We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive ",
    "link": "http://arxiv.org/abs/2307.14995",
    "context": "Title: Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])\nAbstract: We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive ",
    "path": "papers/23/07/2307.14995.json",
    "total_tokens": 943,
    "translated_title": "将TransNormer扩展到1750亿参数",
    "translated_abstract": "我们提出了TransNormerLLM，这是第一个基于线性注意力的大型语言模型（LLM），在准确性和效率两方面都优于传统的基于softmax注意力的模型。TransNormerLLM从之前的线性注意力架构TransNormer发展而来，通过引入位置嵌入、线性注意力加速、门控机制、张量归一化、推理加速和稳定化等先进改进。具体地，我们使用LRPE与指数衰减结合，既避免了注意力稀释问题，又使模型保留了标记之间的全局交互。此外，我们提出了Lightning Attention，一种先进的技术，可以将线性注意力加速超过两倍，并将内存使用量减少了四倍。为了进一步提高TransNormer的性能，我们利用门控机制平滑训练，并采用新的张量归一化方案加速模型，从而取得了令人印象深刻的效果。",
    "tldr": "本论文提出了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，在准确性和效率方面优于传统基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制等先进改进，并利用Lightning Attention技术加速线性注意力，以及采用张量归一化方案加速模型，提高了TransNormer的性能。",
    "en_tdlr": "This paper presents TransNormerLLM, the first linear attention-based large language model (LLM) that outperforms conventional softmax attention-based models in terms of accuracy and efficiency. It introduces advanced modifications such as positional embedding, linear attention acceleration, gating mechanism, and tensor normalization, and utilizes Lightning Attention technique to accelerate linear attention. The performance of TransNormer is further enhanced with the use of a gating mechanism and tensor normalization scheme."
}