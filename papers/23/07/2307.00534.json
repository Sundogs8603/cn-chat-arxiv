{
    "title": "Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves ",
    "link": "http://arxiv.org/abs/2307.00534",
    "context": "Title: Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)\nAbstract: Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves ",
    "path": "papers/23/07/2307.00534.json",
    "total_tokens": 945,
    "translated_title": "通过自由方向知识蒸馏在图神经网络中实现共同增长",
    "translated_abstract": "知识蒸馏（KD）已被证明对提升图神经网络（GNNs）性能有效，其中典型目标是将深层教师GNN的知识蒸馏到更浅的学生GNN中。然而，由于众所周知的过参数化和过平滑问题，训练一个令人满意的深层GNN通常很具挑战性，导致在实际应用中无效的知识转移。在本文中，我们提出了一种基于强化学习的图神经网络自由方向知识蒸馏框架，称为FreeKD，它不再需要提供一个更深层次的优化良好的教师GNN。我们的核心思想是通过层次化的强化学习来协同学习两个较浅的GNN，以便在它们之间交换知识。由于我们观察到一个典型的GNN模型在训练过程中在不同节点表现出更好和更差的性能，我们设计了一种动态和自由方向的知识传递策略，它涉及",
    "tldr": "本文提出了一种基于自由方向知识蒸馏的图神经网络共同增长的框架，通过强化学习同时训练两个较浅的GNN模型，实现了它们之间的知识交流和共享。",
    "en_tdlr": "This paper proposes a framework for the shared growth of graph neural networks via free-direction knowledge distillation. It uses reinforcement learning to train two shallower GNN models simultaneously, enabling knowledge exchange and sharing between them."
}