{
    "title": "Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])",
    "abstract": "Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\\texttt{FUVAL}$, a variant of $\\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then pr",
    "link": "http://arxiv.org/abs/2307.14528",
    "context": "Title: Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])\nAbstract: Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\\texttt{FUVAL}$, a variant of $\\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then pr",
    "path": "papers/23/07/2307.14528.json",
    "total_tokens": 1018,
    "translated_title": "函数值学习：基于Polyak步长和函数分割的自适应学习率的研究",
    "translated_abstract": "在本研究中，我们开发了一种具有自适应步长的SGD（随机梯度下降）变体，利用了采样损失值。特别地，我们专注于解决有限和问题，也称为经验风险最小化。我们首先详细介绍了一种理念上的自适应方法$\\texttt{SPS}_+$，该方法利用了采样损失值，并假设对采样损失值在最优情况下有了解。这种$\\texttt{SPS}_+$是SPS（随机Polyak步长）方法的一个小修改，其中步长被强制为正。然后，我们证明了$\\texttt{SPS}_+$在Lipschitz非光滑中实现了SGD的最佳收敛速度。接下来，我们开发了$\\texttt{FUVAL}$，这是$\\texttt{SPS}_+$的一个变体，其中损失值在最优情况下逐渐学习，而不是给定的。我们给出了$\\texttt{FUVAL}$的三个视角，作为基于投影的方法，作为近似线性方法的变体，以及作为特定的在线SGD方法。然后，我们继续研究了如何实现$\\texttt{FUVAL}$。",
    "tldr": "本论文提出了一种基于采样损失值和自适应步长的SGD变体，通过解决有限和问题，达到了较佳的收敛速度，并开发了一种逐渐学习损失值的算法。",
    "en_tdlr": "This paper introduces a variant of SGD with adaptive step size that uses sampled loss values, achieving better convergence rates for solving finite sum-of-terms problems. It also presents an algorithm that gradually learns the loss values at optimality."
}