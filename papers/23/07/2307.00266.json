{
    "title": "Hierarchical Pretraining for Biomedical Term Embeddings. (arXiv:2307.00266v1 [cs.CL])",
    "abstract": "Electronic health records (EHR) contain narrative notes that provide extensive details on the medical condition and management of patients. Natural language processing (NLP) of clinical notes can use observed frequencies of clinical terms as predictive features for downstream applications such as clinical decision making and patient trajectory prediction. However, due to the vast number of highly similar and related clinical concepts, a more effective modeling strategy is to represent clinical terms as semantic embeddings via representation learning and use the low dimensional embeddings as feature vectors for predictive modeling. To achieve efficient representation, fine-tuning pretrained language models with biomedical knowledge graphs may generate better embeddings for biomedical terms than those from standard language models alone. These embeddings can effectively discriminate synonymous pairs of from those that are unrelated. However, they often fail to capture different degrees o",
    "link": "http://arxiv.org/abs/2307.00266",
    "context": "Title: Hierarchical Pretraining for Biomedical Term Embeddings. (arXiv:2307.00266v1 [cs.CL])\nAbstract: Electronic health records (EHR) contain narrative notes that provide extensive details on the medical condition and management of patients. Natural language processing (NLP) of clinical notes can use observed frequencies of clinical terms as predictive features for downstream applications such as clinical decision making and patient trajectory prediction. However, due to the vast number of highly similar and related clinical concepts, a more effective modeling strategy is to represent clinical terms as semantic embeddings via representation learning and use the low dimensional embeddings as feature vectors for predictive modeling. To achieve efficient representation, fine-tuning pretrained language models with biomedical knowledge graphs may generate better embeddings for biomedical terms than those from standard language models alone. These embeddings can effectively discriminate synonymous pairs of from those that are unrelated. However, they often fail to capture different degrees o",
    "path": "papers/23/07/2307.00266.json",
    "total_tokens": 867,
    "translated_title": "分层预训练用于生物医学术语嵌入",
    "translated_abstract": "电子健康记录（EHR）包含了关于患者的医疗状况和管理的详细说明。通过自然语言处理（NLP）对临床记录进行处理，可以利用临床术语的观察频率作为预测特征，用于临床决策和患者轨迹预测等下游应用。然而，由于大量相似且相关的临床概念，更有效的建模策略是通过表示学习将临床术语表示为语义嵌入，并将低维嵌入作为特征向量用于预测建模。为了实现高效的表示，利用与生物医学知识图谱进行预训练语言模型的微调，可能会生成比仅使用标准语言模型获得的生物医学术语嵌入更好的嵌入。这些嵌入可以有效区分同义词对和不相关的词对。然而，它们常常无法捕捉到不同的程度",
    "tldr": "分层预训练用于生物医学术语嵌入，通过将临床术语表示为语义嵌入并利用低维嵌入作为特征向量，可以提高临床记录的自然语言处理（NLP）效果。",
    "en_tdlr": "Hierarchical pretraining is used for biomedical term embeddings. By representing clinical terms as semantic embeddings and using low-dimensional embeddings as feature vectors, the natural language processing (NLP) of clinical notes can be improved."
}