{
    "title": "Concept-based explainability for an EEG transformer model. (arXiv:2307.12745v1 [cs.LG])",
    "abstract": "Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation",
    "link": "http://arxiv.org/abs/2307.12745",
    "context": "Title: Concept-based explainability for an EEG transformer model. (arXiv:2307.12745v1 [cs.LG])\nAbstract: Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation",
    "path": "papers/23/07/2307.12745.json",
    "total_tokens": 915,
    "translated_title": "EEG转换器模型的基于概念的可解释性",
    "translated_abstract": "深度学习模型由于其规模、结构以及训练过程中的内在随机性而变得复杂。选择数据集和归纳偏见也增加了额外的复杂性。为了解释这些挑战，Kim等人（2018）引入了概念激活向量（CAVs），旨在从人类对齐的概念角度理解深度模型的内部状态。这些概念对应于潜在空间中的方向，使用线性判别法进行识别。尽管该方法首先应用于图像分类，但后来被适应到包括自然语言处理在内的其他领域。在本研究中，我们尝试将该方法应用于Kostas等人的BENDR（2021）的脑电图（EEG）数据，以实现可解释性。这项努力的关键部分包括定义解释性概念和选择相关数据集以将概念与潜在空间相对应。我们的重点是EEG概念形成的两个机制。",
    "tldr": "本研究尝试将基于概念激活向量（CAVs）的方法应用于脑电图（EEG）数据的解释性，通过定义解释性概念和选择相关数据集，以实现对大规模转换器模型中深度学习模型的理解。",
    "en_tdlr": "This research attempts to apply the concept activation vectors (CAVs) method for explainability in electroencephalogram (EEG) data, by defining explanatory concepts and selecting relevant datasets, to understand deep learning models in large-scale transformer models."
}