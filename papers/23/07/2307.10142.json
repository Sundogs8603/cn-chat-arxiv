{
    "title": "Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])",
    "abstract": "The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly",
    "link": "http://arxiv.org/abs/2307.10142",
    "context": "Title: Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])\nAbstract: The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly",
    "path": "papers/23/07/2307.10142.json",
    "total_tokens": 939,
    "translated_title": "对于学习人形机械行走的潜在基于奖励的基准测试",
    "translated_abstract": "在开发有效的强化学习(RL)流程中，主要挑战往往是设计和调整奖励函数。良好设计的塑形奖励可以加快学习速度。然而，简单地制定奖励可能与期望的行为相冲突，如果没有适当调整，可能导致过度拟合甚至不稳定的性能。从理论上讲，潜在基于奖励的塑形(PBRS)可以在不影响最优策略的情况下指导学习过程。尽管有几项研究探索了使用潜在基于奖励的塑形来加快学习收敛的方法，但大多数研究局限于网格世界和低维系统，而在机器人强化学习中，主要依赖于标准形式的奖励塑造。在本文中，我们对使用PBRS的标准形式和塑形进行了人形机器人的基准测试。我们发现，在这个高维系统中，PBRS的收敛速度只有微小的好处。然而，PBRS奖励项具有重大的意义。",
    "tldr": "本文对人形机器人使用标准形式的奖励塑造和潜在基于奖励的塑造进行了基准测试。在高维系统中，潜在基于奖励的塑造（PBRS）对于收敛速度的提升效果较小。",
    "en_tdlr": "This paper benchmarks standard forms of reward shaping and potential based reward shaping (PBRS) for humanoid robots. In a high-dimensional system, PBRS has marginal benefits in convergence speed."
}