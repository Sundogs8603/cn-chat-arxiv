{
    "title": "Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems. (arXiv:2307.12975v1 [cs.LG])",
    "abstract": "A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.",
    "link": "http://arxiv.org/abs/2307.12975",
    "context": "Title: Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems. (arXiv:2307.12975v1 [cs.LG])\nAbstract: A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.",
    "path": "papers/23/07/2307.12975.json",
    "total_tokens": 840,
    "translated_title": "从人类偏好中学习的政策在情境多臂赌博问题中的可证明优势",
    "translated_abstract": "在决策问题中，奖励工程是一个关键的任务。在实践中，往往不存在明显的奖励函数选择。因此，一种常见的方法是在训练过程中引入人类反馈，并利用这种反馈来学习奖励函数。在使用人类反馈的所有政策学习方法中，基于偏好的方法在最近的实证应用中取得了显著的成功，如InstructGPT。在这项工作中，我们开发了一个理论，可以证明在离线情境多臂赌博问题中，基于偏好的方法具有显著的优势。具体而言，我们改进了在人类评分样本上运行政策学习方法的建模和次优性分析。然后，我们将其与基于偏好的方法的次优性保证进行比较，并表明基于偏好的方法享有更低的次优性。",
    "tldr": "该论文研究了基于偏好的政策学习方法在离线情境多臂赌博问题中的优势，并通过改进建模和分析，证明了这一方法相比其他政策学习方法具有更低的次优性。",
    "en_tdlr": "This paper investigates the advantages of preference-based policy learning methods in offline contextual bandit problems and provides theoretical proof that it achieves lower suboptimality compared to other policy learning methods through improved modeling and analysis."
}