{
    "title": "TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])",
    "abstract": "On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\\geq$10\\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti",
    "link": "http://arxiv.org/abs/2307.09988",
    "context": "Title: TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])\nAbstract: On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\\geq$10\\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti",
    "path": "papers/23/07/2307.09988.json",
    "total_tokens": 925,
    "translated_title": "TinyTrain：在极端边缘进行深度神经网络训练",
    "translated_abstract": "设备上的训练对于用户个性化和隐私至关重要。随着物联网设备和微控制器单元（MCU）的普及，由于受限的内存和计算资源以及标注的用户数据的有限可用性，这项任务变得更加具有挑战性。尽管如此，先前的研究忽视了数据稀缺问题，需要过长的训练时间（例如几个小时），或者导致重大的准确性损失（≥10%）。我们提出了TinyTrain，一种设备上的训练方法，通过选择性更新模型的部分，并明确处理数据稀缺问题，大幅缩短了训练时间。TinyTrain引入了一种任务自适应的稀疏更新方法，根据多目标准则动态选择层/通道，同时捕捉用户数据、内存和目标设备的计算能力，从而在未知任务上获得高准确性，并减小计算和内存占用。TinyTrain在整体微调的基础上表现出色。",
    "tldr": "TinyTrain是一种在设备上进行训练的方法，通过选择性更新模型的部分并处理数据稀缺问题，大大缩短了训练时间。通过任务自适应的稀疏更新方法，TinyTrain能够在高准确性的同时减小计算和内存占用，对未知任务表现出色。"
}