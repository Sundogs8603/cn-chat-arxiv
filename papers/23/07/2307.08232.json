{
    "title": "Learning for Counterfactual Fairness from Observational Data. (arXiv:2307.08232v1 [cs.LG])",
    "abstract": "Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is ri",
    "link": "http://arxiv.org/abs/2307.08232",
    "context": "Title: Learning for Counterfactual Fairness from Observational Data. (arXiv:2307.08232v1 [cs.LG])\nAbstract: Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is ri",
    "path": "papers/23/07/2307.08232.json",
    "total_tokens": 925,
    "translated_title": "从观测数据中学习反事实公平性",
    "translated_abstract": "公平性感知机器学习在许多领域引起了广泛关注，如在线广告、个性化推荐和社交媒体分析。公平性感知机器学习旨在消除学习模型对特定子群体的偏见，这些子群体由特定的保护（敏感）属性描述，例如种族、性别和年龄。在许多现有的公平性概念中，反事实公平性是从因果透视定义的一种流行概念。它通过比较原始世界中每个个体的预测和在修改敏感属性值的反事实世界中的预测来衡量预测器的公平性。现有方法实现反事实公平性的先决条件是掌握关于数据的因果模型的先验人类知识。然而，在现实世界的场景中，潜在的因果模型通常是未知的，并且获取这样的人类知识可能非常困难。在这些情况下，实现反事实公平性是非常具有挑战性的。",
    "tldr": "该论文提出了一种从观测数据中学习反事实公平性的方法。现有方法需要先验人类知识来实现反事实公平性，但在实际场景中，获取这样的知识往往非常困难。这项研究为解决这一问题提供了新的思路。"
}