{
    "title": "Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])",
    "abstract": "Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o",
    "link": "http://arxiv.org/abs/2307.05834",
    "context": "Title: Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])\nAbstract: Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o",
    "path": "papers/23/07/2307.05834.json",
    "total_tokens": 886,
    "translated_title": "用经验共享来扩展分布式多任务强化学习",
    "translated_abstract": "最近，DARPA推出了ShELL计划，旨在探索经验共享如何使分布式终身学习代理在适应新挑战方面受益。本文通过对分布式多任务强化学习（RL）进行理论和实证研究，解决了这个问题，其中一组N个代理协作解决了M个任务，而不需要先验知识。我们将问题表述为线性参数化的上下文马尔可夫决策过程（MDP），其中每个任务由指定转移动力学和奖励的上下文表示。为了解决这个问题，我们提出了一种名为DistMT-LSVI的算法。首先，代理识别任务，然后通过中央服务器交换信息，为任务导出ε-最优策略。我们的研究证明，为了实现所有M个任务的ε-最优策略，使用DistMT-LSVI的单个代理需要运行一定数量的操作。",
    "tldr": "本文研究了分布式多任务强化学习中的经验共享问题，提出了一种算法DistMT-LSVI，并通过理论和实证研究表明，使用DistMT-LSVI的单个代理可以实现所有任务的ε-最优策略。",
    "en_tdlr": "This paper investigates the issue of experience sharing in distributed multi-task reinforcement learning and proposes an algorithm called DistMT-LSVI. The research shows that a single agent using DistMT-LSVI can achieve ε-optimal policies for all tasks."
}