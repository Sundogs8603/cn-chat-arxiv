{
    "title": "Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])",
    "abstract": "Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generat",
    "link": "http://arxiv.org/abs/2307.14928",
    "context": "Title: Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])\nAbstract: Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generat",
    "path": "papers/23/07/2307.14928.json",
    "total_tokens": 844,
    "translated_title": "基于图的多音轨音乐生成",
    "translated_abstract": "图可以用来建模多音轨的符号音乐，其中音符、和弦和整个片段可以按照音调和节奏的关系在不同层次上连接。然而，在音乐生成的深度学习系统中，缺乏考虑图表示的工作。本文通过引入一种新颖的图表示和深度变分自动编码器来弥补这一差距，该自动编码器分别生成音乐图的结构和内容，其层次结构与音乐的结构先验相匹配。通过将音乐图的结构和内容分离，可以通过指定特定时间播放的乐器来条件生成，为音乐共创的人机互动打开了新的可能性。实验表明，在现有的MIDI数据集上训练模型后，该模型能够生成...",
    "tldr": "这篇论文介绍了一种基于图的多音轨音乐生成方法，通过引入深度变分自动编码器，分别生成音乐图的结构和内容，实现了通过指定乐器来条件生成音乐，为音乐共创的人机互动提供了新的可能性。"
}