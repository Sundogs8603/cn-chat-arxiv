{
    "title": "Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])",
    "abstract": "We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\\epsilon)^{O(k)}$, where $\\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.",
    "link": "http://arxiv.org/abs/2307.12840",
    "context": "Title: Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])\nAbstract: We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\\epsilon)^{O(k)}$, where $\\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.",
    "path": "papers/23/07/2307.12840.json",
    "total_tokens": 913,
    "translated_title": "通过舒尔多项式高效学习具有一个隐藏层的ReLU网络",
    "translated_abstract": "我们研究了在标准高斯分布下，关于平方损失的PAC学习$k$个ReLU激活的线性组合的问题。我们的主要结果是针对这个学习任务的一种高效算法，其样本和计算复杂性为$(dk/\\epsilon)^{O(k)}$，其中$\\epsilon>0$是目标精度。之前的工作给出了一个复杂性为$(dk/\\epsilon)^{h(k)}$的算法，其中函数$h(k)$在$k$上的规模是超多项式的。有趣的是，我们的算法在相关统计查询算法类中接近最优。总体而言，我们的算法使用张量分解来识别一个子空间，使得所有$O(k)$阶矩在正交方向上都很小。其分析基于舒尔多项式理论，以显示较低阶误差张量的情况下，更高阶的误差张量也很小。",
    "tldr": "通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。",
    "en_tdlr": "We propose an efficient algorithm for learning a linear combination of $k$ ReLU activations using tensor decomposition and Schur polynomial theory. Our algorithm achieves near-optimal sample and computational complexity, and can find small higher-moment error tensors in high-dimensional spaces."
}