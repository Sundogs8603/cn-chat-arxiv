{
    "title": "On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])",
    "abstract": "We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\\Theta(\\alpha)$ loss in accuracy, where $\\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\\sqrt{\\alpha})$ loss, and give a matching $\\Omega(\\sqrt{\\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\\Omega(1)$. The key technical novelty ",
    "link": "http://arxiv.org/abs/2307.11892",
    "context": "Title: On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])\nAbstract: We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\\Theta(\\alpha)$ loss in accuracy, where $\\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\\sqrt{\\alpha})$ loss, and give a matching $\\Omega(\\sqrt{\\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\\Omega(1)$. The key technical novelty ",
    "path": "papers/23/07/2307.11892.json",
    "total_tokens": 972,
    "translated_title": "关于受恶意噪声影响的公正约束学习的脆弱性",
    "translated_abstract": "我们考虑了公正约束学习对训练数据中微小恶意噪声的脆弱性。Konstantinov和Lampert (2021)在这个问题上进行了研究，并展示了负面结果，表明在不平衡的群组大小下存在一些数据分布，任何适当的学习器都会表现出较高的脆弱性。在这里，我们展示了更乐观的观点，如果允许随机分类器，则情况更加细致。例如，对于人口统计学平等性，我们显示只会产生$\\Theta(\\alpha)$的精度损失，其中$\\alpha$是恶意噪声率，甚至可以与没有公正约束的情况完全匹配。对于机会均等性，我们显示只会产生$O(\\sqrt{\\alpha})$的损失，并给出一个匹配的$\\Omega(\\sqrt{\\alpha})$的下界。相比之下，Konstantinov和Lampert (2021)示范了对于适当的学习器，这两个概念的精度损失都是$\\Omega(1)$。关键的技术创新是",
    "tldr": "这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\\Theta(\\alpha)$和$O(\\sqrt{\\alpha})$，对应不同的公正约束要求。",
    "en_tdlr": "This study examines the vulnerability of fairness-constrained learning to malicious noise and finds that using randomized classifiers can incur only a $\\Theta(\\alpha)$ loss in accuracy for Demographic Parity and an $O(\\sqrt{\\alpha})$ loss for Equal Opportunity, matching the best possible without fairness constraints."
}