{
    "title": "Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])",
    "abstract": "We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peak at high-performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high-dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer function evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization,",
    "link": "http://arxiv.org/abs/2307.04065",
    "context": "Title: Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])\nAbstract: We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peak at high-performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high-dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer function evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization,",
    "path": "papers/23/07/2307.04065.json",
    "total_tokens": 846,
    "translated_title": "基于生成性神经网络的超高维非凸景观的大规模全球优化",
    "translated_abstract": "我们提出了一种基于深度生成网络训练的非凸优化算法元启发式，能够在连续的超高维景观中进行有效搜索。在网络训练过程中，利用采样的局部梯度群体，在定制损失函数中演化网络输出分布函数，使其趋向于高性能最优点。深度网络结构经过定制，能够在训练过程中逐步增长，从而使算法能够处理高维景观中的维度灾难特征。我们将这个概念应用于一系列标准优化问题，其中维度高达一千，结果表明我们的方法在较少的函数评价次数下表现更好，相比最先进的算法基准。我们还讨论了深度网络过参数化、损失函数设计和适当的网络架构选择在优化中的作用。",
    "tldr": "本论文提出了一种基于深度生成网络的非凸优化算法，通过演化网络输出分布函数，实现了在超高维度景观中的有效搜索。实验证明该方法在较少的函数评价次数下表现优于其他算法。",
    "en_tdlr": "This paper presents a non-convex optimization algorithm based on deep generative networks, which enables effective search in ultra-high-dimensional landscapes. Experimental results show that this method outperforms other algorithms with fewer function evaluations."
}