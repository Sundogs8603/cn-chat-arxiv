{
    "title": "Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])",
    "abstract": "Our goal is for robots to follow natural language instructions like \"put the towel next to the microwave.\" But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir",
    "link": "http://arxiv.org/abs/2307.00117",
    "context": "Title: Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])\nAbstract: Our goal is for robots to follow natural language instructions like \"put the towel next to the microwave.\" But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir",
    "path": "papers/23/07/2307.00117.json",
    "total_tokens": 895,
    "translated_title": "指导目标表征：一种半监督语言接口控制机器人的方法",
    "translated_abstract": "我们的目标是使机器人能够按照自然语言指令行动，例如“将毛巾放在微波炉旁边”。但是获取大量带有语言指令标签的标注数据非常困难。相比之下，获取对图像目标作出响应的策略要容易得多，因为任何自主尝试或演示都可以在事后用最终状态作为目标进行标记。在这项工作中，我们提出了一种方法，只利用少量的语言数据，利用联合图像和目标信息的策略来处理语言接口。以前的工作在使用视觉-语言模型或联合训练语言-目标-条件策略方面取得了一些进展，但迄今为止，这两种方法都没有有效地扩展到实际机器人任务中，而不需要大量的人工注释。我们的方法通过从标记数据中学习一种将语言与目标图像对齐的嵌入，实现了在真实世界中的稳健性能。",
    "tldr": "本文介绍了一种半监督语言接口控制机器人的方法，通过联合图像和目标信息的策略以及少量的语言数据实现了在真实世界中的稳健性能。",
    "en_tdlr": "This paper presents a semi-supervised language interface control method for robots, which achieves robust performance in the real world by utilizing joint image and goal information policies, as well as a small amount of language data."
}