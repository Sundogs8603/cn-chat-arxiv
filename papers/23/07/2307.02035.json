{
    "title": "Ranking with Abstention. (arXiv:2307.02035v1 [cs.LG])",
    "abstract": "We introduce a novel framework of ranking with abstention, where the learner can abstain from making prediction at some limited cost $c$. We present a extensive theoretical analysis of this framework including a series of $H$-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. These theoretical guarantees are the state-of-the-art consistency guarantees in the literature, which are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. We further argue that our proposed abstention methods are important when using common equicontinuous hypothesis sets in practice. We report the results of experiments illustrating the effectiveness of ranking with abstention.",
    "link": "http://arxiv.org/abs/2307.02035",
    "context": "Title: Ranking with Abstention. (arXiv:2307.02035v1 [cs.LG])\nAbstract: We introduce a novel framework of ranking with abstention, where the learner can abstain from making prediction at some limited cost $c$. We present a extensive theoretical analysis of this framework including a series of $H$-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. These theoretical guarantees are the state-of-the-art consistency guarantees in the literature, which are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. We further argue that our proposed abstention methods are important when using common equicontinuous hypothesis sets in practice. We report the results of experiments illustrating the effectiveness of ranking with abstention.",
    "path": "papers/23/07/2307.02035.json",
    "total_tokens": 815,
    "translated_title": "使用弃权进行排名",
    "translated_abstract": "我们介绍了一种新的弃权排名框架，学习者可以以有限成本$c$放弃对某些数据进行预测。我们对这个框架进行了广泛的理论分析，包括线性函数族和具有一层隐藏层的神经网络的一系列$H$-一致性界限。这些理论保证是文献中最先进的一致性保证，它们是预测器在假设集$H$中的目标损失估计误差的上界，以预测器的替代损失估计误差为表达形式。我们进一步指出，在实践中使用常见等连续假设集时，我们提出的弃权方法是重要的。我们报告了一系列实验结果，展示了使用弃权进行排名的有效性。",
    "tldr": "提出了一种使用弃权进行排名的新框架，并对该框架进行了广泛的理论分析。我们的方法在文献中给出了最先进的一致性保证，可以有效地估计目标损失，并在使用等连续假设集时具有重要的应用价值。我们的实验证明了弃权方法的有效性。"
}