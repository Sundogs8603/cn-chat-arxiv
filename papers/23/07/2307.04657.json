{
    "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)",
    "abstract": "In this paper, we introduce the \\textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our ",
    "link": "http://arxiv.org/abs/2307.04657",
    "context": "Title: BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)\nAbstract: In this paper, we introduce the \\textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our ",
    "path": "papers/23/07/2307.04657.json",
    "total_tokens": 841,
    "translated_title": "BeaverTails：通过人类偏好数据集改善LLM的安全对齐",
    "translated_abstract": "本文介绍了“BeaverTails”数据集，旨在促进大型语言模型（LLM）的安全对齐研究。该数据集独特地对问答对的有用性和无害性进行了分开注释，从而为这些关键属性提供了不同的观点。总共，我们为30,207个问答对和30,144对专家比较数据收集了安全元标签，用于衡量有用性和无害性指标。我们进一步展示了BeaverTails在内容管理和强化学习与人类反馈（RLHF）中的应用，强调其在LLM中实施实际安全措施的潜力。我们相信这个数据集为社区提供了重要资源，为LLM的安全开发和部署做出了贡献。",
    "tldr": "本文介绍了BeaverTails数据集，用于研究LLM的安全对齐。该数据集分开注释了问答对的有用性和无害性，为安全开发提供了重要资源。",
    "en_tdlr": "This paper introduces the BeaverTails dataset for studying safety alignment in LLMs. The dataset separates annotations of usefulness and harmlessness for question-answering pairs, providing important resources for safe development."
}