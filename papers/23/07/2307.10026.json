{
    "title": "Contextual Reliability: When Different Features Matter in Different Contexts. (arXiv:2307.10026v1 [cs.LG])",
    "abstract": "Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the \"right\" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a mod",
    "link": "http://arxiv.org/abs/2307.10026",
    "context": "Title: Contextual Reliability: When Different Features Matter in Different Contexts. (arXiv:2307.10026v1 [cs.LG])\nAbstract: Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the \"right\" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a mod",
    "path": "papers/23/07/2307.10026.json",
    "total_tokens": 1011,
    "translated_title": "上下文可靠性：在不同上下文中不同特征的重要性",
    "translated_abstract": "深度神经网络经常因依赖噪声相关性而产生灾难性错误。大部分的研究假设有一个明确的虚假和可靠特征的二分法，然而这通常是不现实的。例如，大部分情况下我们不想让自动驾驶汽车简单地复制周围车辆的速度 -- 如果邻近的车辆违章，我们不想让我们的车也闯红灯。然而，我们不能简单地强制要求不变性以排除下一条车道的速度，因为这可能提供了关于一个无法观测到的人行横道上行人的宝贵信息。因此，普遍地忽视有时可靠（但并非总是）的特征会导致性能非鲁棒。我们提出了一个新的称为上下文可靠性的设定，考虑到使用“正确”特征可能会因上下文的不同而变化。我们提出并分析了一个称为显式非虚假特征预测（ENP）的两阶段框架，该框架首先识别给定上下文中要使用的相关特征，然后训练一个模型。",
    "tldr": "本论文提出了一个新的概念，即上下文可靠性，它考虑到在不同上下文中使用的“正确”特征可能会有所变化。作者提出的两阶段框架ENP能够首先识别给定上下文中要使用的相关特征，然后训练模型，从而提高性能的鲁棒性。",
    "en_tdlr": "This paper introduces a new concept called contextual reliability, which takes into account the variation of \"right\" features to use in different contexts. The proposed two-stage framework, ENP, first identifies the relevant features for a given context and then trains a model, improving performance robustness."
}