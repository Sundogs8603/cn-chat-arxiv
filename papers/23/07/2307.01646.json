{
    "title": "SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])",
    "abstract": "Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge",
    "link": "http://arxiv.org/abs/2307.01646",
    "context": "Title: SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])\nAbstract: Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge",
    "path": "papers/23/07/2307.01646.json",
    "total_tokens": 959,
    "translated_title": "SwinGNN:重新思考在图生成的扩散模型中的置换不变性",
    "translated_abstract": "基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。",
    "tldr": "本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。",
    "en_tdlr": "This paper proposes a new diffusion model for graph generation called SwinGNN, which utilizes an efficient 2-WL message passing network and shifted window-based self-attention. It significantly improves the sample quality of graph generation by combining critical training and sampling techniques, and introduces a post-processing trick of randomly permuting the generated graphs to convert them into graph statistics."
}