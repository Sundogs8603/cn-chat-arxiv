{
    "title": "Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels. (arXiv:2307.03930v1 [cs.LG])",
    "abstract": "We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.  Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achie",
    "link": "http://arxiv.org/abs/2307.03930",
    "context": "Title: Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels. (arXiv:2307.03930v1 [cs.LG])\nAbstract: We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.  Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achie",
    "path": "papers/23/07/2307.03930.json",
    "total_tokens": 998,
    "translated_title": "Rosko: 使用行跳过外积进行稀疏矩阵乘法内核计算",
    "translated_abstract": "我们提出了Rosko，即使用行跳过外积（Row Skipping Outer Products），用于在减少深度神经网络（DNN）的计算和内存访问需求方面推导出稀疏矩阵乘法（SpMM）内核。Rosko允许在程序执行期间跳过整行计算，并具有低稀疏管理开销。我们从分析上推导出适应给定硬件特性的稀疏CPU内核，以有效利用处理器核心并最小化数据移动，而无需自动调优或搜索空间探索。Rosko可以与其他外积调度方法集成，通过使用Rosko的打包格式来跳过不必要的计算，从而利用行跳过。在各种神经网络工作负载下，Rosko内核在真实硬件上胜过现有的自动调优和基于搜索的解决方案以及最先进的供应商优化库。对于在机器学习中通常出现的稀疏度从65％到99.8％的矩阵，Rosko内核可以实现...（摘要未完整）",
    "tldr": "Rosko提出了一种稀疏矩阵乘法（SpMM）内核方法，通过使用行跳过外积实现深度神经网络（DNN）的计算和内存访问需求的减少。它可以适应不同硬件特性，并与其他外积调度方法相结合，胜过自动调优和搜索的解决方案，并能在不同稀疏度的矩阵上实现高效的计算。"
}