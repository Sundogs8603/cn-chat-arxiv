{
    "title": "Near-Linear Time Projection onto the $\\ell_{1,\\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])",
    "abstract": "Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\\ell_{1,2}$ and $\\ell_{1,\\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\\ell_{1,\\infty}$ norm ball. The worst-case time complexity of this algorithm is $\\mathcal{O}\\big(nm+J\\log(nm)\\big)$ for a matrix in $\\mathbb{R}^{n\\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\\ell_{1,\\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\\%$) of the data is relevan",
    "link": "http://arxiv.org/abs/2307.09836",
    "context": "Title: Near-Linear Time Projection onto the $\\ell_{1,\\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])\nAbstract: Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\\ell_{1,2}$ and $\\ell_{1,\\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\\ell_{1,\\infty}$ norm ball. The worst-case time complexity of this algorithm is $\\mathcal{O}\\big(nm+J\\log(nm)\\big)$ for a matrix in $\\mathbb{R}^{n\\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\\ell_{1,\\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\\%$) of the data is relevan",
    "path": "papers/23/07/2307.09836.json",
    "total_tokens": 1060,
    "translated_title": "在几乎线性时间内投影到 $\\ell_{1,\\infty}$ 球面；稀疏自编码器的应用",
    "translated_abstract": "现在寻找稀疏性对于加速大规模神经网络的训练至关重要。投影到 $\\ell_{1,2}$ 和 $\\ell_{1,\\infty}$ 是稀疏化和降低神经网络整体成本的最高效技术之一。本文介绍了一种新的 $\\ell_{1,\\infty}$ 范数球面的投影算法。该算法的最坏时间复杂度为 $\\mathcal{O}\\big(nm+J\\log(nm)\\big)$，其中矩阵为 $\\mathbb{R}^{n\\times m}$。$J$ 是一个在稀疏性高时趋近于0，在稀疏性低时趋近于 $nm$ 的项。该算法易于实现，并保证在有限时间内收敛到精确解。此外，我们提出在训练自编码器时将 $\\ell_{1,\\infty}$ 球面投影纳入其中，以强制进行特征选择和权重的稀疏化。在我们的生物学应用中，稀疏化主要出现在编码器中，以实现特征选择，因为只有非常小的一部分数据（<2%）是相关的。",
    "tldr": "本文提出了一种投影算法，能够在几乎线性时间内将矩阵投影到 $\\ell_{1,\\infty}$ 球面。该算法易于实现，能够在有限时间内收敛到精确解。同时，将该算法应用于自编码器训练中可以实现特征选择和权重的稀疏化。"
}