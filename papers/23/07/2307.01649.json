{
    "title": "Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])",
    "abstract": "Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.",
    "link": "http://arxiv.org/abs/2307.01649",
    "context": "Title: Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])\nAbstract: Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.",
    "path": "papers/23/07/2307.01649.json",
    "total_tokens": 939,
    "translated_title": "低维流形上过参数化卷积残差网络的非参数分类",
    "translated_abstract": "卷积残差神经网络(ConvResNets)虽然过参数化，但在实践中能够获得显著的预测性能，这不能被常规智慧很好地解释。为了弥合这一差距，我们从非参数分类的角度研究了使用权重衰减训练的ConvResNeXts（覆盖ConvResNets作为一种特殊情况）的性能。我们的分析允许ConvResNeXts中有无限多的构建模块，并显示权重衰减隐含地强制这些模块的稀疏性。具体而言，我们考虑在低维流形上的平滑目标函数，然后证明ConvResNeXts可以适应函数的平滑性和低维结构，并且能够高效地学习函数而不受维度诅咒的困扰。我们的发现部分证明了过参数化的ConvResNeXts相对于常规机器学习模型的优势。",
    "tldr": "本文研究了卷积残差网络在非参数分类任务中的性能。研究表明，通过使用权重衰减的ConvResNeXts，可以隐含地实现对模块的稀疏性，从而使网络能够适应低维流形的平滑性和结构，并高效地学习函数。"
}