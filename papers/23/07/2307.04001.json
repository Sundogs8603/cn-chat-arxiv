{
    "title": "Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])",
    "abstract": "Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio",
    "link": "http://arxiv.org/abs/2307.04001",
    "context": "Title: Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])\nAbstract: Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio",
    "path": "papers/23/07/2307.04001.json",
    "total_tokens": 889,
    "translated_title": "多项式宽度对于具有高维特征的集合表示足够",
    "translated_abstract": "集合表示在深度学习中已经变得普遍，用于建模神经网络对输入顺序不敏感的归纳偏差。DeepSets是最常用的集合表示神经网络架构，它将每个集合元素嵌入到具有维度L的潜在空间中，然后进行求和池化以获得整个集合的嵌入，最后将整个集合的嵌入映射到输出。在这项工作中，我们研究了维度L对DeepSets表达能力的影响。之前的分析要么将高维特征过于简化为一维特征，要么局限于分析激活函数，从而脱离实际应用或导致L随着集合大小N和特征维度D呈指数增长。为了研究达到足够表达能力的最小L值，我们提出了两种集合元素嵌入层：（a）线性+幂激活（LP）和（b）线性+指数激活。",
    "tldr": "本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。"
}