{
    "title": "Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])",
    "abstract": "This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.",
    "link": "http://arxiv.org/abs/2307.13158",
    "context": "Title: Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])\nAbstract: This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.",
    "path": "papers/23/07/2307.13158.json",
    "total_tokens": 921,
    "translated_title": "多无人机速度控制兼顾避障和交接感知的小区关联：带有动作分支的深度强化学习研究",
    "translated_abstract": "本文提出了一种深度强化学习方法，用于优化三维空中高速公路上多无人机的小区关联决策和移动速度，以提升交通和通信性能，包括避障、连接性和交接效果。问题被建模为一个马尔可夫决策过程（MDP），其中无人机的状态由速度和通信数据速率定义。我们提出了一种神经结构，其中包含一个共享的决策模块和多个网络分支，每个分支专门处理二维交通-通信空间中的特定动作维度。这种设计有效地处理了多维动作空间，使得各个动作维度可以独立决策。我们介绍了两个模型，分支型对抗性Q网络（BDQ）和分支型对抗性双重深度Q网络（Dueling DDQN），来证明这种方法。仿真结果显示，与现有基准相比，性能提高了18.32%。",
    "tldr": "本文提出了一种深度强化学习方法，用于优化多无人机在三维空中高速公路上的小区关联决策和移动速度，以提升交通和通信性能。仿真结果显示，性能提高了18.32%。",
    "en_tdlr": "This paper presents a deep reinforcement learning solution to optimize multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway, improving transportation and communication performance. Simulation results show an 18.32% improvement."
}