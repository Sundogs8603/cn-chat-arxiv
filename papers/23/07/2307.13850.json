{
    "title": "MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])",
    "abstract": "Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.",
    "link": "http://arxiv.org/abs/2307.13850",
    "context": "Title: MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])\nAbstract: Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.",
    "path": "papers/23/07/2307.13850.json",
    "total_tokens": 871,
    "translated_title": "MAEA: 基于多模态的智能体人工智能的归因",
    "translated_abstract": "理解多模态感知对于智能体人工智能是一个开放性问题，因为这样的输入可能包含高度互补和冗余的信息用于任务。多模态政策的一个相关方向是理解融合层中每种模态的全局趋势。为此，我们在ALFRED数据集训练的不同政策上解开了视觉、语言和先前动作输入的归因。归因分析可以用于排名和分组故障场景、调查建模和数据集偏见，并在部署之前对多模态EAI政策的稳健性和用户信任进行关键分析。我们提出了MAEA，一个用于计算任何可微分政策的每个模态的全局归因的框架。此外，我们展示了归因如何在EAI政策的语言和视觉归因中启用更底层的行为分析。",
    "tldr": "MAEA是一个用于计算任何可微分的多模态智能体人工智能政策的全局归因的框架。它可以通过归因分析来排名和分组故障场景，调查建模和数据集偏见，并对多模态智能体人工智能政策的稳健性和用户信任进行关键分析。",
    "en_tdlr": "MAEA is a framework for computing global attributions of any differentiable multimodal embodied AI policy. It enables ranking and grouping of failure scenarios, investigation of modeling and dataset biases, and critical analysis of the robustness and user trust of multimodal embodied AI policies."
}