{
    "title": "Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations th",
    "link": "http://arxiv.org/abs/2307.15411",
    "context": "Title: Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations th",
    "path": "papers/23/07/2307.15411.json",
    "total_tokens": 958,
    "translated_title": "研究上下文学习的学习行为：与监督学习的比较",
    "translated_abstract": "大型语言模型（LLM）展示了令人注目的上下文学习（ICL）能力，在没有明确预训练的情况下，仅通过少量训练样例就可以学习新任务。然而，尽管LLM取得了成功，对于ICL如何从给定的提示中学习知识的了解还很少。在本文中，为了更好地理解ICL的学习行为，我们使用相同的演示样例通过ICL和监督学习（SL）分别训练相同的LLM，并研究它们在一系列分类任务上在标签扰动（噪声标签和标签不平衡）下的性能。首先，通过广泛的实验，我们发现金标签对于下游的上下文性能有重大影响，尤其是对于大型语言模型；然而，对于ICL来说，标签不平衡对所有模型大小都不太重要。其次，在与SL进行比较时，我们经验性地表明ICL对标签扰动的敏感性较低。",
    "tldr": "本研究通过对比监督学习和上下文学习，发现大型语言模型在学习行为上受到金标签的显著影响，但对于上下文学习来说，标签不平衡影响较小。实证结果显示上下文学习对标签扰动的敏感性较低。",
    "en_tdlr": "This study compares supervised learning and in-context learning, finding that large language models are significantly influenced by gold labels, but label imbalance has little impact on in-context learning. Empirical results demonstrate that in-context learning is less sensitive to label perturbations."
}