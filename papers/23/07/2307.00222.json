{
    "title": "Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals. (arXiv:2307.00222v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) are widely used in domains like social networks and biological systems. However, the locality assumption of GNNs, which limits information exchange to neighboring nodes, hampers their ability to capture long-range dependencies and global patterns in graphs. To address this, we propose a new inductive bias based on variational analysis, drawing inspiration from the Brachistochrone problem. Our framework establishes a mapping between discrete GNN models and continuous diffusion functionals. This enables the design of application-specific objective functions in the continuous domain and the construction of discrete deep models with mathematical guarantees. To tackle over-smoothing in GNNs, we analyze the existing layer-by-layer graph embedding models and identify that they are equivalent to l2-norm integral functionals of graph gradients, which cause over-smoothing. Similar to edge-preserving filters in image denoising, we introduce total variation (TV) to ali",
    "link": "http://arxiv.org/abs/2307.00222",
    "context": "Title: Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals. (arXiv:2307.00222v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) are widely used in domains like social networks and biological systems. However, the locality assumption of GNNs, which limits information exchange to neighboring nodes, hampers their ability to capture long-range dependencies and global patterns in graphs. To address this, we propose a new inductive bias based on variational analysis, drawing inspiration from the Brachistochrone problem. Our framework establishes a mapping between discrete GNN models and continuous diffusion functionals. This enables the design of application-specific objective functions in the continuous domain and the construction of discrete deep models with mathematical guarantees. To tackle over-smoothing in GNNs, we analyze the existing layer-by-layer graph embedding models and identify that they are equivalent to l2-norm integral functionals of graph gradients, which cause over-smoothing. Similar to edge-preserving filters in image denoising, we introduce total variation (TV) to ali",
    "path": "papers/23/07/2307.00222.json",
    "total_tokens": 984,
    "translated_title": "在连续图扩散函数空间中重新思考和重新设计图神经网络",
    "translated_abstract": "图神经网络（GNN）被广泛应用于社交网络和生物系统等领域。然而，GNN的局部性假设将信息交换限制在相邻节点之间，制约了其捕捉图中长程依赖和全局模式的能力。为了解决这个问题，我们提出了一种基于变分分析的新归纳偏置，受到了Brachistochrone问题的启发。我们的框架建立了离散GNN模型和连续扩散函数之间的映射关系。这使得能够在连续域设计应用特定的目标函数，并构建具有数学保证的离散深度模型。为了解决GNN中的过度平滑问题，我们分析了现有的逐层图嵌入模型，并发现它们等价于图梯度的l2-范数积分泛函，导致过度平滑。类似于图像去噪中的边保持滤波器，我们引入了总变差（TV）来解决这个问题。",
    "tldr": "本论文重新思考和重新设计图神经网络，通过在连续图扩散函数空间中引入变分分析的归纳偏置，解决了GNN在捕捉长程依赖和全局模式上的局限性，并通过引入总变差（TV）来解决过度平滑问题，从而构建了具有数学保证的离散深度模型。",
    "en_tdlr": "This paper re-thinks and re-designs graph neural networks by introducing a variational analysis-based inductive bias in the space of continuous graph diffusion functionals. It addresses the limitations of GNNs in capturing long-range dependencies and global patterns, and tackles over-smoothing by introducing total variation (TV) regularization, resulting in discrete deep models with mathematical guarantees."
}