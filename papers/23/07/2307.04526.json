{
    "title": "Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)",
    "abstract": "The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the \"rate\" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.",
    "link": "http://arxiv.org/abs/2307.04526",
    "context": "Title: Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)\nAbstract: The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the \"rate\" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.",
    "path": "papers/23/07/2307.04526.json",
    "total_tokens": 829,
    "translated_title": "自扩展神经网络",
    "translated_abstract": "神经网络的训练结果严重依赖于所选择的架构；即使只是对网络大小做微小修改，通常也需要重新开始训练过程。相比之下，我们以一个小的架构开始训练，只在问题需要时扩展其容量，并避免干扰先前的优化过程。因此，我们引入了一种基于自然梯度的方法，当这样做可能大幅降低假设收敛训练损失时，直观地扩展了神经网络的宽度和深度。我们证明了神经元添加的“速率”上界，并且给出了计算廉价的扩展评分的下界。我们在分类和回归问题中展示了自扩展神经网络的优势，包括那些合适的架构大小在先验上相当不确定的问题。",
    "tldr": "这项研究提出了一种自扩展神经网络的方法，通过自然梯度来自动增加神经网络的宽度和深度，以在训练损失降低的情况下提高性能，并在分类和回归问题中展示了其优势。",
    "en_tdlr": "This study proposes a self-expanding neural network approach that automatically increases the width and depth of the network using natural gradient to improve performance when the training loss decreases. The benefits of such networks are demonstrated in classification and regression problems, especially in cases where the appropriate architecture size is uncertain."
}