{
    "title": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])",
    "abstract": "Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\\rho$ and an alternative adversarial action with probability $\\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i",
    "link": "http://arxiv.org/abs/2307.07666",
    "context": "Title: Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])\nAbstract: Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\\rho$ and an alternative adversarial action with probability $\\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i",
    "path": "papers/23/07/2307.07666.json",
    "total_tokens": 958,
    "translated_title": "具有概率策略执行不确定性的高效鲁棒增强学习",
    "translated_abstract": "鲁棒增强学习旨在在不确定性面前找到优化最坏情况下性能的策略。本文关注具有概率策略执行不确定性的行动鲁棒增强学习，其中代理机器不总是按照策略指定的动作进行，而是以概率$1-\\rho$执行策略指定的动作，以概率$\\rho$执行替代的对抗动作。我们证明了具有概率策略执行不确定性的行动鲁棒马尔可夫决策过程存在最优策略，并提供了解决其的行动鲁棒贝尔曼最优方程。此外，我们开发了具有证书的行动鲁棒增强学习(ARRLC)算法，该算法实现了极小极大遗憾和样本复杂度最优。此外，我们进行了数值实验来验证我们的方法的鲁棒性，结果表明ARRLC优于非鲁棒增强学习算法，并且比鲁棒TD算法收敛更快。",
    "tldr": "本文研究了具有概率策略执行不确定性的行动鲁棒增强学习问题，并提出了ARRLC算法，该算法在遗憾和样本复杂度上达到了极小极大最优，实验证明其优于非鲁棒算法并且收敛更快。"
}