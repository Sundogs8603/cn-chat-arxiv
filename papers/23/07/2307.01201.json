{
    "title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])",
    "abstract": "In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at d",
    "link": "http://arxiv.org/abs/2307.01201",
    "context": "Title: Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])\nAbstract: In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at d",
    "path": "papers/23/07/2307.01201.json",
    "total_tokens": 895,
    "translated_title": "在上下文学习和出现中的模式学习和重新绑定机制",
    "translated_abstract": "上下文学习（ICL）是近期基于Transformer的大型语言模型（LLMs）中最强大且最令人意外的能力之一。然而，它的基础机制尚不清楚。在本文中，我们证明可以通过使用克隆结构因果图（CSCGs）这种替代的序列预测学习方法获得可比较的ICL能力。此外，CSCGs的一个关键特性是，与基于Transformer的LLMs不同，它们是可解释的，这大大简化了解释ICL工作原理的任务。具体而言，我们显示它使用了以下组合：（a）学习模板（模式）电路进行模式完成，（b）以上下文敏感方式检索相关模板，以及（c）将新标记重新绑定到模板的适当位置。我们进一步提出了ICL在LLMs中采用类似机制的假设证据。例如，我们发现，与LLMs一样，使用CSCGs时会出现不同的能力。",
    "tldr": "本文研究了上下文学习的机制，发现使用克隆结构因果图可以实现与transformer-based语言模型相似的能力，并且这种方法可以解释上下文学习的工作原理。"
}