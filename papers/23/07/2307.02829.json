{
    "title": "Policy Contrastive Imitation Learning. (arXiv:2307.02829v1 [cs.LG])",
    "abstract": "Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and generates a smooth cosine-similarity-based reward. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship le",
    "link": "http://arxiv.org/abs/2307.02829",
    "context": "Title: Policy Contrastive Imitation Learning. (arXiv:2307.02829v1 [cs.LG])\nAbstract: Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and generates a smooth cosine-similarity-based reward. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship le",
    "path": "papers/23/07/2307.02829.json",
    "total_tokens": 899,
    "translated_title": "政策对比仿真学习",
    "translated_abstract": "敌对仿真学习（AIL）是一种最近取得了很大成功的流行方法。然而，AIL在更具挑战性的任务上的表现仍然不令人满意。我们发现其中一个主要原因是由于AIL鉴别器表示的质量较低。由于AIL鉴别器通过二元分类进行训练，并不一定以有意义的方式区分政策和专家，因此得到的奖励可能也是没有意义的。为了解决这个问题，我们提出了一种新的方法，称为政策对比仿真学习（PCIL）。PCIL通过锚定不同的策略来学习对比表示空间，并生成基于余弦相似度的平滑奖励。我们提出的表示学习目标可以被看作是AIL目标的更强版本，并提供了更有意义的代理与政策之间的比较。从理论的角度出发，我们使用学徒技艺证明了我们方法的有效性。",
    "tldr": "政策对比仿真学习(PCIL)是一种解决敌对仿真学习(AIL)性能不佳问题的新方法。PCIL通过学习对比表示空间，并生成平滑的余弦相似度奖励，提供更有意义的代理与政策之间的比较。"
}