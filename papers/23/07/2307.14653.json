{
    "title": "Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])",
    "abstract": "State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a",
    "link": "http://arxiv.org/abs/2307.14653",
    "context": "Title: Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])\nAbstract: State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a",
    "path": "papers/23/07/2307.14653.json",
    "total_tokens": 946,
    "translated_title": "深度学习的速度限制",
    "translated_abstract": "现阶段的神经网络需要极大的计算能力才能进行训练。因此很自然地想知道它们是否被最优化地训练。在本文中，我们应用了最近在随机热力学中的一个进展，允许根据它们的Wasserstein-2距离的比率和连接它们的动态过程的熵产生速率，对从初始权重分布到完全训练的网络的最大速度进行界定。考虑了梯度流和Langevin训练动力学，我们为线性和可线性化的神经网络（例如神经切向核(NTK)）提供了这些速度限制的解析表达式。值得注意的是，如果对NTK谱和标签的谱分解做出一些合理的缩放假设，学习在某种程度上是最优化的。我们的结果与在CIFAR-10上使用卷积神经网络(CNNs)和全连接神经网络(FCNs)进行的小规模实验一致，显示了",
    "tldr": "研究使用随机热力学方法，根据权重分布间的Wasserstein-2距离和熵产生速率，提供了对深度学习网络从初始状态到完全训练的最大速度限制。通过应用于线性和可线性化的神经网络，结果表明，在某些缩放假设下，学习在某种程度上是最优的。"
}