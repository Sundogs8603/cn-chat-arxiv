{
    "title": "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard",
    "link": "http://arxiv.org/abs/2307.11729",
    "context": "Title: OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard",
    "path": "papers/23/07/2307.11729.json",
    "total_tokens": 950,
    "translated_title": "OUTFOX: 基于上下文学习和对抗生成例子的LLM生成论文检测",
    "translated_abstract": "大型语言模型(LLMs)已经达到了与人类写作相当的流利程度，很难区分人类写作和LLM生成的文本。这增加了LLMs被误用的风险，并需要开发检测器来识别LLM生成的文本。然而，现有的检测器通过简单地改写LLM生成的文本来降低检测准确性。此外，这些检测器在学生在写作作业（如论文）中使用LLMs并迅速学会如何规避这些检测器的真实生活情况下的有效性尚未被探讨。在本文中，我们提出了OUTFOX，一个新的框架，通过允许检测器和攻击者考虑彼此的输出并将其应用于学生论文领域来提高LLM生成文本检测器的鲁棒性。在我们的框架中，攻击者使用检测器的预测标签作为上下文学习的示例，并对难以检测的对抗生成论文进行生成。",
    "tldr": "OUTFOX是一个新的框架，通过允许检测器和攻击者考虑彼此的输出，提高了LLM生成文本检测器的鲁棒性。攻击者利用检测器的预测标签作为示例进行上下文学习，并生成难以检测的对抗生成的论文。",
    "en_tdlr": "OUTFOX is a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. The attacker uses the detector's prediction labels as examples for in-context learning and generates adversarial essays that are hard to detect."
}