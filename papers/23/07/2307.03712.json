{
    "title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])",
    "abstract": "The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim",
    "link": "http://arxiv.org/abs/2307.03712",
    "context": "Title: INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])\nAbstract: The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim",
    "path": "papers/23/07/2307.03712.json",
    "total_tokens": 1002,
    "translated_title": "INT-FP-QSim: 大型语言模型和视觉转换器的混合精度和格式",
    "translated_abstract": "大型语言模型的崛起导致了减少精度的运行的增加。降低精度的运行方式支持资源约束，并促进其民主化，使用户可以在个人设备上运行数十亿参数的语言模型。为了补充这一持续努力，我们提出了INT-FP-QSim：一个开源模拟器，可以灵活评估不同数值精度和格式下的语言模型和视觉转换器。INT-FP-QSim利用现有的开源库，如TensorRT、QPytorch和AIMET，实现了支持各种浮点和整数格式的组合模拟器。借助我们的模拟器，我们调查了不同数值格式对4位权重和4位或8位激活的语言模型和视觉转换器性能的影响。我们还比较了最近提出的方法，如自适应块浮点、SmoothQuant、GPTQ和RPTQ在模型性能上的差异。我们希望INT-FP-QSim能为研究人员和从业者在低精度环境下评估大型语言模型和视觉转换器的性能提供帮助。",
    "tldr": "INT-FP-QSim是一个开源模拟器，用于评估大型语言模型和视觉转换器在不同精度和格式下的性能。通过使用不同的数值格式，我们研究了4位权重和4位或8位激活对模型性能的影响，并比较了不同方法的效果。",
    "en_tdlr": "INT-FP-QSim is an open-source simulator for evaluating the performance of large language models and vision transformers at different precisions and formats. By using various numerical formats, we investigate the impact of 4-bit weights and 4-bit or 8-bit activations on model performance, and compare the effectiveness of different methods."
}