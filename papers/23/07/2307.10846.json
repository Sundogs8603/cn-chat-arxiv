{
    "title": "Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning. (arXiv:2307.10846v1 [cs.RO])",
    "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to ",
    "link": "http://arxiv.org/abs/2307.10846",
    "context": "Title: Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning. (arXiv:2307.10846v1 [cs.RO])\nAbstract: Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to ",
    "path": "papers/23/07/2307.10846.json",
    "total_tokens": 911,
    "translated_title": "具有基于解耦的可达性规划的目标条件强化学习",
    "translated_abstract": "目标条件强化学习（GCRL）可以使Agent自发地设定不同的目标来学习一系列技能。尽管在各个领域中都有出色的研究成果，但是在时间延展任务中达到远距离目标仍然是GCRL面临的挑战。目前的研究通过利用规划算法来计划中间子目标来增强GCRL来解决这个问题。他们的方法需要两个关键要求：（i）状态表示空间来搜索有效的子目标，（ii）距离函数来测量子目标的可达性。然而，由于非紧凑的表示，它们很难在高维状态空间上扩展。此外，它们无法通过标准的GC策略收集高质量的训练数据，这导致了不准确的距离函数。这两个问题都会影响规划和策略学习的效率和性能。在本文中，我们提出了一种结合了解耦可达性规划（REPlan）的目标条件RL算法。",
    "tldr": "本研究提出了一种结合了解耦可达性规划的目标条件强化学习算法，用于解决现有方法在高维状态空间中扩展和收集高质量训练数据的问题。"
}