{
    "title": "Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])",
    "abstract": "Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f",
    "link": "http://arxiv.org/abs/2307.01377",
    "context": "Title: Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])\nAbstract: Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f",
    "path": "papers/23/07/2307.01377.json",
    "total_tokens": 972,
    "translated_title": "可转移上下文：解决同时语音翻译中的训练-推理上下文不匹配问题",
    "translated_abstract": "在同时语音翻译中，使用分段处理的Transformer模型已经成为一种有效的架构。然而，这种模型在训练和推理环境之间创建了上下文不匹配的问题，阻碍了潜在的翻译准确性。我们通过提出可转移上下文来解决这个问题，这是一种简单而有效的方案，可以确保在训练和推理过程中始终维持一致的段落和上下文大小，即使由于流式翻译的性质导致部分填充的段落存在。可转移上下文在流式任务的分段Transformer中也是广泛适用的。我们在MUST-C数据集的英德、英法和英西语言对上进行的实验表明，将该方案应用于Augmented Memory Transformer（一种用于同时语音翻译的最先进模型）可以平均提高2.09、1.83和1.95个BLEU分数。",
    "tldr": "该论文提出了一种名为\"可转移上下文\"的简单而有效的方案，用于解决同时语音翻译中的训练-推理上下文不匹配问题。通过保持一致的段落和上下文大小，即使存在部分填充的段落，该方案在流式任务的分段Transformer中也是广泛适用的。实验证明，应用于Augmented Memory Transformer后可以提高BLEU得分。"
}