{
    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)",
    "abstract": "Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \\emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on th",
    "link": "http://arxiv.org/abs/2307.08072",
    "context": "Title: Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)\nAbstract: Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \\emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on th",
    "path": "papers/23/07/2307.08072.json",
    "total_tokens": 876,
    "translated_title": "在量子化的大型语言模型中是否存在新兴能力：一项经验研究",
    "translated_abstract": "尽管大型语言模型（LLMs）具有出色的性能，但需要大量的计算资源进行部署和使用。为了解决这个问题，已经广泛应用量子化方法来减少LLMs的内存占用以及增加推理速度。然而，一个主要的挑战是低位量子化方法往往会导致性能下降。了解量子化对LLMs能力的影响是重要的。与以往专注于总体性能的研究不同，本研究旨在调查量子化对“新兴能力”的影响，这些能力是区分LLMs和小型语言模型的重要特征。具体而言，我们研究了量子化LLMs中的上下文学习、思维连贯和遵循指令的能力。我们的实证实验表明，这些新兴能力在4位量化模型中仍然存在，而2位模型在这些能力上遇到了严重的性能下降。",
    "tldr": "本研究旨在研究量子化对大型语言模型中的新兴能力的影响，结果显示在4位量化模型中这些新兴能力仍然存在。",
    "en_tdlr": "This study aims to investigate the impact of quantization on emergent abilities in large language models, and the results show that these emergent abilities still exist in 4-bit quantization models."
}