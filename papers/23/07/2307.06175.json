{
    "title": "Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])",
    "abstract": "Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical",
    "link": "http://arxiv.org/abs/2307.06175",
    "context": "Title: Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])\nAbstract: Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical",
    "path": "papers/23/07/2307.06175.json",
    "total_tokens": 928,
    "translated_title": "学习分散式部分可观察场均控制来实现人工集体行为",
    "translated_abstract": "近期，强化学习方法在各个领域取得了成功。然而，多智能体强化学习在分散化、部分可观察以及面对众多智能体时仍然存在挑战。与此同时，集体行为要解决上述问题，并且对于许多最前沿的应用，如活动物质物理、自组织系统、舆论动态以及生物或机器人群体来说非常重要。在这篇论文中，我们通过提出新的分散式部分可观察场均控制（Dec-POMFC）模型，实现了代理在部分信息下的分散行为，这是一类允许将问题简化为可解决的单智能体马尔可夫决策过程（MDP）的排列不变代理的广泛问题，以及单智能体强化学习解决方案。",
    "tldr": "本文提出了一种分散式部分可观察的场均控制模型（Dec-POMFC），用于解决多智能体强化学习中的分散化、部分可观察和可扩展性等挑战。该模型可将问题简化为可解决的单智能体马尔可夫决策过程，为实现人工集体行为提供了解决方案。",
    "en_tdlr": "This paper proposes a decentralized partially observable mean field control (Dec-POMFC) model to address the challenges of decentralization, partial observability, and scalability in multi-agent reinforcement learning. The model reduces the problem to tractable single-agent Markov decision processes and provides a solution for artificial collective behavior."
}