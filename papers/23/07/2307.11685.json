{
    "title": "Towards Generalizable Reinforcement Learning for Trade Execution. (arXiv:2307.11685v1 [q-fin.TR])",
    "abstract": "Optimized trade execution is to sell (or buy) a given amount of assets in a given time with the lowest possible trading cost. Recently, reinforcement learning (RL) has been applied to optimized trade execution to learn smarter policies from market data. However, we find that many existing RL methods exhibit considerable overfitting which prevents them from real deployment. In this paper, we provide an extensive study on the overfitting problem in optimized trade execution. First, we model the optimized trade execution as offline RL with dynamic context (ORDC), where the context represents market variables that cannot be influenced by the trading policy and are collected in an offline manner. Under this framework, we derive the generalization bound and find that the overfitting issue is caused by large context space and limited context samples in the offline setting. Accordingly, we propose to learn compact representations for context to address the overfitting problem, either by levera",
    "link": "http://arxiv.org/abs/2307.11685",
    "context": "Title: Towards Generalizable Reinforcement Learning for Trade Execution. (arXiv:2307.11685v1 [q-fin.TR])\nAbstract: Optimized trade execution is to sell (or buy) a given amount of assets in a given time with the lowest possible trading cost. Recently, reinforcement learning (RL) has been applied to optimized trade execution to learn smarter policies from market data. However, we find that many existing RL methods exhibit considerable overfitting which prevents them from real deployment. In this paper, we provide an extensive study on the overfitting problem in optimized trade execution. First, we model the optimized trade execution as offline RL with dynamic context (ORDC), where the context represents market variables that cannot be influenced by the trading policy and are collected in an offline manner. Under this framework, we derive the generalization bound and find that the overfitting issue is caused by large context space and limited context samples in the offline setting. Accordingly, we propose to learn compact representations for context to address the overfitting problem, either by levera",
    "path": "papers/23/07/2307.11685.json",
    "total_tokens": 952,
    "translated_title": "面向通用化的交易执行的强化学习方法",
    "translated_abstract": "优化的交易执行是在给定时间内以最低的交易成本卖出（或买入）给定资产的过程。最近，强化学习方法被应用于优化的交易执行，以从市场数据中学习更智能的策略。然而，我们发现许多现有的强化学习方法存在显著的过拟合问题，从而阻碍了它们的实际应用。在本文中，我们对优化的交易执行中的过拟合问题进行了广泛研究。首先，我们将优化的交易执行建模为带有动态上下文（ORDC）的离线强化学习问题，其中上下文表示不能受到交易策略影响并以离线方式收集的市场变量。在这个框架下，我们推导了泛化界限，并发现过拟合问题是由于离线环境中上下文空间巨大且上下文样本有限所导致的。因此，我们提出了学习上下文的紧凑表示来解决过拟合问题，可以通过...",
    "tldr": "本论文提出了一种面向通用化的交易执行的强化学习方法。研究表明，现有的强化学习方法存在过拟合问题，阻碍了实际应用。作者通过使用离线强化学习和动态上下文建模来解决过拟合问题，并提出了学习上下文的紧凑表示方法。",
    "en_tdlr": "This paper proposes a generalizable reinforcement learning method for trade execution. Research shows that existing RL methods suffer from overfitting, limiting their practical deployment. The authors address this issue by modeling trade execution as offline RL with dynamic context and propose learning compact representations for context to mitigate overfitting."
}