{
    "title": "Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])",
    "abstract": "Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the ",
    "link": "http://arxiv.org/abs/2307.04927",
    "context": "Title: Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])\nAbstract: Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the ",
    "path": "papers/23/07/2307.04927.json",
    "total_tokens": 915,
    "translated_title": "安全强化学习的概率性反例引导",
    "translated_abstract": "安全探索旨在解决强化学习在安全关键场景中的局限性，其中在试错学习过程中的失败可能会导致高成本。存在多种方法来整合外部知识或使用近距离传感器数据来限制对不安全状态的探索。然而，在未知环境中减少探索风险仍然具有挑战性，因为代理必须在探索过程中发现安全威胁。本文通过采用安全需求的反例引导训练来解决安全探索问题。我们的方法将连续和离散状态空间系统抽象为紧凑的抽象模型，代表代理在探索过程中获得的与安全相关的知识。然后，我们利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况，代理可以使用这些模型在离线环境中进行高效的训练，以优化其策略，减小安全风险。",
    "tldr": "本文提出了一种安全强化学习方法，通过引导训练中的反例来解决安全探索的问题，该方法将连续和离散状态空间系统抽象为紧凑的模型，并利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况。",
    "en_tdlr": "This paper proposes a safe reinforcement learning method that addresses the problem of safe exploration by guiding training with counterexamples. The method abstracts continuous and discrete state-space systems into compact models and utilizes probabilistic counterexample generation to construct minimal simulation submodels that reveal violations of safety requirements."
}