{
    "title": "Improved sampling via learned diffusions. (arXiv:2307.01198v1 [cs.LG])",
    "abstract": "Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\\\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered a",
    "link": "http://arxiv.org/abs/2307.01198",
    "context": "Title: Improved sampling via learned diffusions. (arXiv:2307.01198v1 [cs.LG])\nAbstract: Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\\\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered a",
    "path": "papers/23/07/2307.01198.json",
    "total_tokens": 819,
    "translated_title": "通过学习扩散改进采样",
    "translated_abstract": "最近，一系列论文提出了基于深度学习的方法，使用控制扩散过程从非标准化目标密度中采样。在本研究中，我们将这些方法视为Schrödinger桥问题的特例，寻求给定先验分布和指定目标之间最可能的随机演化。我们进一步通过引入基于时间反演扩散过程的路径空间度量之间的差异的变分形式来推广这个框架。这个抽象的视角导致了可以通过梯度优化的实际损失，并将先前的目标作为特例。与此同时，它允许我们考虑除了已知存在模式坍缩问题的反向Kullback-Leibler差别之外的其他差别。特别地，我们提出了所谓的对数方差损失，它具有良好的数值特性，并显著提高了在所有考虑的情况下的性能。",
    "tldr": "通过学习扩散的方法改进了采样过程，引入了基于变分形式的路径空间度量，提出了对数方差损失，优化了采样性能。",
    "en_tdlr": "Improved sampling process using learned diffusions, by introducing a variational formulation based on path space measures and proposing the log-variance loss, leads to enhanced sampling performance."
}