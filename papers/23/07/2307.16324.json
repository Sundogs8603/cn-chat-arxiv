{
    "title": "Mispronunciation detection using self-supervised speech representations. (arXiv:2307.16324v1 [cs.CL])",
    "abstract": "In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.",
    "link": "http://arxiv.org/abs/2307.16324",
    "context": "Title: Mispronunciation detection using self-supervised speech representations. (arXiv:2307.16324v1 [cs.CL])\nAbstract: In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.",
    "path": "papers/23/07/2307.16324.json",
    "total_tokens": 883,
    "translated_title": "利用自监督语音表示进行发音错误检测",
    "translated_abstract": "近年来，自监督学习模型在各种语音处理任务中取得了有希望的结果，特别是在数据稀缺的情况下。本文研究了在第二语言学习者的发音错误检测任务中使用自监督学习模型的方法。我们比较了两种下游方法：1）使用本地英文数据训练模型进行音素识别（PR）；2）使用非本地英文数据直接训练模型进行目标任务。我们比较了这两种方法在不同的自监督学习表示以及从传统的基于DNN的语音识别模型中提取的表示下的表现。我们在L2Arctic和EpaDB两个标注了音素级发音标签的非本地语音数据集上评估了这些模型。总的来说，我们发现使用为目标任务训练的下游模型能够得到最佳性能，而大多数上游模型在此任务中表现相似。",
    "tldr": "本文研究了在第二语言学习者的发音错误检测任务中使用自监督学习模型的方法。研究发现，使用为目标任务训练的下游模型能够得到最佳性能，而大多数上游模型在此任务中表现相似。",
    "en_tdlr": "This paper investigates the use of self-supervised learning models for mispronunciation detection in second language learners. The study finds that training a downstream model specifically for the target task yields the best performance, while most upstream models perform similarly in this task."
}