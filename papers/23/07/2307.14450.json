{
    "title": "Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation. (arXiv:2307.14450v1 [cs.IR])",
    "abstract": "We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields re",
    "link": "http://arxiv.org/abs/2307.14450",
    "context": "Title: Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation. (arXiv:2307.14450v1 [cs.IR])\nAbstract: We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields re",
    "path": "papers/23/07/2307.14450.json",
    "total_tokens": 822,
    "translated_title": "将离线强化学习与Transformer相结合的顺序推荐方法",
    "translated_abstract": "我们考虑了顺序推荐的问题，即根据过去的互动进行当前推荐。这个推荐任务需要对顺序数据进行高效处理，并旨在提供最大化长期奖励的推荐。为此，我们通过使用离线RL算法和我们模型架构中的策略网络进行训练，该模型架构从预训练的Transformer模型初始化。预训练模型利用Transformer处理顺序信息的优秀能力。与依赖在线交互的模拟方法相比，我们专注于实现一种完全离线的RL框架，能够快速稳定地收敛。通过对公共数据集进行大量实验，我们证明了我们的方法在各种推荐场景中（包括电子商务和电影推荐）的稳健性。与最先进的监督学习算法相比，我们的算法产生了更好的推荐性能。",
    "tldr": "本研究将离线强化学习与Transformer相结合，提出了一种顺序推荐方法。通过使用预训练的Transformer模型和离线RL算法训练，我们的方法能够快速稳定地收敛，并在广泛的推荐场景中表现出较好的推荐性能。"
}