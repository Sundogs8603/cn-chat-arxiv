{
    "title": "Rates of Approximation by ReLU Shallow Neural Networks. (arXiv:2307.12461v1 [cs.LG])",
    "abstract": "Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\\\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\\\"older space $W_\\infty^r([-1, 1]^d)$ with rates $O((\\log m)^{\\frac{1}{2} +d}m^{-\\frac{r}{d}\\frac{d+2}{d+4}})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\\frac{r}{d}})$ in the sense that $\\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.",
    "link": "http://arxiv.org/abs/2307.12461",
    "context": "Title: Rates of Approximation by ReLU Shallow Neural Networks. (arXiv:2307.12461v1 [cs.LG])\nAbstract: Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\\\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\\\"older space $W_\\infty^r([-1, 1]^d)$ with rates $O((\\log m)^{\\frac{1}{2} +d}m^{-\\frac{r}{d}\\frac{d+2}{d+4}})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\\frac{r}{d}})$ in the sense that $\\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.",
    "path": "papers/23/07/2307.12461.json",
    "total_tokens": 882,
    "translated_title": "ReLU浅层神经网络的逼近率",
    "translated_abstract": "由修正线性单元（ReLU）激活的神经网络在深度学习的最新发展中发挥着核心作用。通过这些网络逼近H\\\"older空间中的函数的话题对于理解所引发学习算法的效率至关重要。虽然在具有多层隐藏神经元的深层神经网络的设置中该话题已经得到很好的研究，但对于仅有一个隐藏层的浅层网络仍然是未解之谜。在本文中，我们提供了这些网络的均匀逼近率。我们表明，当$r<d/2 +2$时，ReLU浅层神经网络可以以率$O((\\log m)^{\\frac{1}{2} +d}m^{-\\frac{r}{d}\\frac{d+2}{d+4}})$均匀逼近H\\\"older空间$W_\\infty^r([-1, 1]^d)$中的函数。当$d$很大时，这样的逼近率非常接近最优逼近率$O(m^{-\\frac{r}{d}})$，因为$\\frac{d+2}{d+4}$接近于$1$。",
    "tldr": "本文提供了ReLU浅层神经网络的均匀逼近率，可以以接近于最优逼近率的速度在H\\\"older空间中逼近函数。"
}