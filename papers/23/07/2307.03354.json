{
    "title": "Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])",
    "abstract": "In real-world applications, users often require both translations and transcriptions of speech to enhance their comprehension, particularly in streaming scenarios where incremental generation is necessary. This paper introduces a streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. To produce ASR and ST content effectively with minimal latency, we propose a joint token-level serialized output training method that interleaves source and target words by leveraging an off-the-shelf textual aligner. Experiments in monolingual (it-en) and multilingual (\\{de,es,it\\}-en) settings demonstrate that our approach achieves the best quality-latency balance. With an average ASR latency of 1s and ST latency of 1.3s, our model shows no degradation or even improves output quality compared to separate ASR and ST models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the multilingual case.",
    "link": "http://arxiv.org/abs/2307.03354",
    "context": "Title: Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])\nAbstract: In real-world applications, users often require both translations and transcriptions of speech to enhance their comprehension, particularly in streaming scenarios where incremental generation is necessary. This paper introduces a streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. To produce ASR and ST content effectively with minimal latency, we propose a joint token-level serialized output training method that interleaves source and target words by leveraging an off-the-shelf textual aligner. Experiments in monolingual (it-en) and multilingual (\\{de,es,it\\}-en) settings demonstrate that our approach achieves the best quality-latency balance. With an average ASR latency of 1s and ST latency of 1.3s, our model shows no degradation or even improves output quality compared to separate ASR and ST models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the multilingual case.",
    "path": "papers/23/07/2307.03354.json",
    "total_tokens": 981,
    "translated_title": "在联合流畅的ASR和ST中，基于文本对齐的标记级串行输出训练",
    "translated_abstract": "在实际应用中，用户通常需要同时翻译和转录语音以增强其理解能力，特别是在需要增量生成的流式场景中。本文介绍了一种流式Transformer-Transducer，它利用一个单一的解码器同时生成自动语音识别（ASR）和语音翻译（ST）输出。为了以最小的延迟有效地产生ASR和ST内容，我们提出了一种联合的标记级串行输出训练方法，通过利用现成的文本对齐器交错源词和目标词。在单语（it-en）和多语（{de,es,it}-en）设置下的实验证明，我们的方法实现了最佳的质量-延迟平衡。在平均ASR延迟为1秒和ST延迟为1.3秒的情况下，我们的模型与单独的ASR和ST模型相比，没有降低，甚至提高了输出质量，在多语言情况下，平均WER提高了1.1，BLEU提高了0.4。",
    "tldr": "本文提出了一种流式Transformer-Transducer，同时生成自动语音识别（ASR）和语音翻译（ST）输出的方法。通过联合的标记级串行输出训练方法，结合现成的文本对齐器，实现了最佳的质量-延迟平衡，并在多语环境下取得了良好的效果。"
}