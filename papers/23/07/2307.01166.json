{
    "title": "Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)",
    "abstract": "We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-",
    "link": "http://arxiv.org/abs/2307.01166",
    "context": "Title: Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)\nAbstract: We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-",
    "path": "papers/23/07/2307.01166.json",
    "total_tokens": 1028,
    "translated_title": "针对战略非局部分布偏移的耦合梯度流动",
    "translated_abstract": "我们提出了一种新颖的框架，用于分析现实世界系统中分布偏移的动态过程，该框架捕捉了学习算法与其应用的分布之间的反馈循环。以往的研究主要以对抗或过度简化的分布偏移结构来建模反馈引起的分布偏移。相比之下，我们提出了一种耦合的偏微分方程模型，通过考虑由于对算法决策的战略性反应、非局部内生人口互动和其他外生分布偏移来源而产生的复杂动态，捕捉分布随时间的细微变化。我们考虑机器学习中的两种常见设置：信息不对称的合作设置以及学习者面对战略用户的竞争设置。对于这两种设置，当算法通过梯度下降进行重新训练时，我们证明了重新训练过程收敛到一个稳定状态的渐近性。",
    "tldr": "该论文提出了一种框架，用于分析现实世界系统中的分布偏移动态，并且捕捉了学习算法与其应用的分布之间的反馈循环。通过耦合偏微分方程模型，考虑了战略性反应、非局部内生人口互动和其他外生分布偏移来源，以实现对时间上细微变化的捕捉。研究证明了在合作设置和竞争设置中，当算法通过梯度下降进行重新训练时，重新训练过程渐近收敛到一个稳定状态。",
    "en_tdlr": "This paper presents a framework for analyzing the dynamics of distribution shift in real-world systems by capturing the feedback loop between learning algorithms and the deployed distributions. It proposes a coupled partial differential equation model that considers strategic responses, non-local endogenous population interactions, and other sources of distribution shift to capture fine-grained changes over time. The research proves the asymptotic convergence of the retraining procedure to a stable state in cooperative and competitive settings."
}