{
    "title": "PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])",
    "abstract": "Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, gener",
    "link": "http://arxiv.org/abs/2307.06018",
    "context": "Title: PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])\nAbstract: Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, gener",
    "path": "papers/23/07/2307.06018.json",
    "total_tokens": 849,
    "translated_title": "PolyLM:一个开源的多语言大语言模型",
    "translated_abstract": "大语言模型（LLMs）展现了出色的能力，能够理解、推理和生成自然语言指令。然而，LLMs的开发主要集中在高资源语言，如英语，限制了它们在其他语言中的适用性和研究。因此，我们提出了PolyLM，一个训练了6400亿个标记的多语言LLM，有两种模型大小：1.7B和13B。为了增强其多语言能力，我们1）将双语数据整合到训练数据中；2）采用课程学习策略，在预训练的第一阶段将非英语数据的比例从30%增加到最后阶段的60%。此外，我们提出了一种多语言自学习方法，自动为模型的微调生成了132.7K个多语言指令。为了评估模型的性能，我们收集了一些现有的多语言任务，包括多语言理解、问题回答、生成等。",
    "tldr": "PolyLM是一个开源的多语言大语言模型，通过整合双语数据和采用课程学习策略进行训练，提高了多语言能力，并且使用了多语言自学习方法进行模型微调。模型性能得到了多个现有多语言任务的确认。"
}