{
    "title": "Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.",
    "link": "http://arxiv.org/abs/2307.14712",
    "context": "Title: Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])\nAbstract: Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.",
    "path": "papers/23/07/2307.14712.json",
    "total_tokens": 900,
    "translated_title": "评估生成模型在图文生成中的应用",
    "translated_abstract": "大型语言模型（LLM）已广泛应用于图文生成任务。然而，微调LLM的过程需要大量的训练资源和注释工作。本文中，我们探索了生成模型在无需微调的情况下，从图数据中生成描述性文本的能力。具体而言，我们评估了GPT-3和ChatGPT在两个图文数据集上的性能，并将其与微调的LLM模型（如T5和BART）进行了比较。我们的结果表明，生成模型能够生成流畅并连贯的文本，在AGENDA和WebNLG数据集上分别实现了10.57和11.08的BLEU分数。然而，我们的错误分析揭示了生成模型仍然难以理解实体之间的语义关系，而且它们还倾向于生成具有幻觉或无关信息的文本。作为错误分析的一部分，我们利用BERT检测机器生成的文本，并取得了较高的宏F1分数。",
    "tldr": "本文评估了生成模型在图文生成中的应用，并发现生成模型能够生成流畅并连贯的文本。然而，生成模型仍然困难理解实体之间的语义关系，并且容易生成带有幻觉或无关信息的文本。"
}