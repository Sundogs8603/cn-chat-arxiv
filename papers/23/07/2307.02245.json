{
    "title": "Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)",
    "abstract": "Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the",
    "link": "http://arxiv.org/abs/2307.02245",
    "context": "Title: Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)\nAbstract: Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the",
    "path": "papers/23/07/2307.02245.json",
    "total_tokens": 946,
    "translated_title": "准确和校准模型的集合学习",
    "translated_abstract": "模型过度自信和校准不良在机器学习中很常见，并且在应用标准经验风险最小化时很难解决。在这项工作中，我们提出了一种新的方法来缓解这些问题，我们称之为奇数-$k$-去除学习（OKO），它通过最小化集合的交叉熵误差而不是单个示例的误差来实现。这自然地使模型能够捕捉数据示例之间的相关性，并在有限的训练数据和类别不平衡的情况下实现更好的准确性和校准。令人惊讶的是，即使使用硬标签进行训练并且不进行任何额外的校准参数调整，如温度缩放，OKO通常也能获得更好的校准效果。我们提供了理论上的证明，证明OKO自然地能够获得更好的校准效果，并进行了广泛的实验分析以验证我们的理论发现。我们强调，OKO是一个通用的框架，可以很容易地适应许多不同的情境。",
    "tldr": "提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。"
}