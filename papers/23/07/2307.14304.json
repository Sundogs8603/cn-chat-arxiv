{
    "title": "A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch. (arXiv:2307.14304v1 [eess.SY])",
    "abstract": "The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the ",
    "link": "http://arxiv.org/abs/2307.14304",
    "context": "Title: A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch. (arXiv:2307.14304v1 [eess.SY])\nAbstract: The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the ",
    "path": "papers/23/07/2307.14304.json",
    "total_tokens": 888,
    "translated_title": "一种用于最优能量存储系统调度的约束执行深度强化学习框架",
    "translated_abstract": "由于动态价格波动、需求消耗和基于可再生能源的能量生成引入的不确定性，能量存储系统(ESSs)的最优调度面临巨大挑战。通过利用深度神经网络(DNNs)的泛化能力，深度强化学习(DRL)算法可以学习适应配电网络随机性的高质量控制模型。然而，当前的DRL算法缺乏严格执行操作约束的能力，甚至常常提供不可行的控制动作。为了解决这个问题，我们提出了一个DRL框架，在在线操作期间有效处理连续动作空间，同时严格执行环境和动作空间的操作约束。首先，所提出的框架训练一个由DNNs模拟的动作价值函数。随后，该动作价值函数被制定为一个混合整数规划(MIP)问题，使其能够进行操作约束的严格执行。",
    "tldr": "提出了一个约束执行深度强化学习框架，能够有效地处理能量存储系统调度中的操作约束，通过训练深度神经网络来学习高质量的控制模型，并将其转化为混合整数规划问题进行约束执行。"
}