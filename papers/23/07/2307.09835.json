{
    "title": "Deep Operator Network Approximation Rates for Lipschitz Operators. (arXiv:2307.09835v1 [math.NA])",
    "abstract": "We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\\\"older) continuous maps $\\mathcal G:\\mathcal X\\to\\mathcal Y$ between (subsets of) separable Hilbert spaces $\\mathcal X$, $\\mathcal Y$. The DON architecture considered uses linear encoders $\\mathcal E$ and decoders $\\mathcal D$ via (biorthogonal) Riesz bases of $\\mathcal X$, $\\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\\ell^2(\\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\\\"older) continu",
    "link": "http://arxiv.org/abs/2307.09835",
    "context": "Title: Deep Operator Network Approximation Rates for Lipschitz Operators. (arXiv:2307.09835v1 [math.NA])\nAbstract: We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\\\"older) continuous maps $\\mathcal G:\\mathcal X\\to\\mathcal Y$ between (subsets of) separable Hilbert spaces $\\mathcal X$, $\\mathcal Y$. The DON architecture considered uses linear encoders $\\mathcal E$ and decoders $\\mathcal D$ via (biorthogonal) Riesz bases of $\\mathcal X$, $\\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\\ell^2(\\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\\\"older) continu",
    "path": "papers/23/07/2307.09835.json",
    "total_tokens": 816,
    "translated_title": "即使G是Lipschitz或Holder连续的，我们也建立了一类神经深度算子网络(DON)对Lipschitz算子的普遍性和逼近速率界的结果",
    "translated_abstract": "我们在可分Hilbert空间之间建立了一个类似于Lipschitz（或Holder）连续映射G：X→Y的神经深度算子网络(DON)的普遍性和表达速率界. DON架构使用线性编码器E和解码器D通过X，Y的（双正交）Riesz基，并使用一个在序列空间l^2(N)上是Lipschitz连续的无限维参数坐标映射的近似网络. 与以前的工作不同，现在的表达速率结果不需要G是全纯的。",
    "tldr": "该论文研究了一类神经深度算子网络(DON)在对Lipschitz算子的逼近速率界上的普适性，不需要G是全纯的。",
    "en_tdlr": "This paper establishes universality and approximation rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz operators, without requiring G to be holomorphic."
}