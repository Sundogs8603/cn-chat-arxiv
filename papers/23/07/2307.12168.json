{
    "title": "Hallucination Improves the Performance of Unsupervised Visual Representation Learning. (arXiv:2307.12168v1 [cs.CV])",
    "abstract": "Contrastive learning models based on Siamese structure have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learning relies on two conditions, a sufficient number of positive pairs and adequate variations between them. If the conditions are not met, these frameworks will lack semantic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast. The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible computation. Moreover, we reduce the mutual information of hallucinated pairs and smooth them through non-linear operations. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the propo",
    "link": "http://arxiv.org/abs/2307.12168",
    "context": "Title: Hallucination Improves the Performance of Unsupervised Visual Representation Learning. (arXiv:2307.12168v1 [cs.CV])\nAbstract: Contrastive learning models based on Siamese structure have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learning relies on two conditions, a sufficient number of positive pairs and adequate variations between them. If the conditions are not met, these frameworks will lack semantic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast. The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible computation. Moreover, we reduce the mutual information of hallucinated pairs and smooth them through non-linear operations. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the propo",
    "path": "papers/23/07/2307.12168.json",
    "total_tokens": 820,
    "translated_title": "幻觉改进了无监督视觉表示学习的性能",
    "translated_abstract": "基于连体结构的对比学习模型在自监督学习中取得了显著的性能。对比学习的成功依赖于两个条件：足够数量的正对和适当的变化。如果这些条件不满足，这些框架将缺乏语义对比，并且容易过拟合。为了解决这两个问题，我们提出了Hallucinator，它能够有效地生成额外的正样本以进一步进行对比。Hallucinator是可微分的，并在特征空间中创建新的数据。因此，它可以直接与预训练任务一起进行优化，并引入几乎可忽略的计算量。此外，我们通过非线性操作减少了幻觉对的互信息并使其平滑。这个过程有助于避免训练过程中对比学习模型过于自信，并实现更具变换不变性的特征嵌入。值得注意的是，我们通过实验证明了这个提议。",
    "tldr": "幻觉生成模型可以为对比学习提供额外的正样本，改进了无监督视觉表示学习的性能。",
    "en_tdlr": "The Hallucinator model generates additional positive samples for contrastive learning, improving the performance of unsupervised visual representation learning."
}