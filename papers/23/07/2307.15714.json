{
    "title": "Synthetic pre-training for neural-network interatomic potentials. (arXiv:2307.15714v1 [physics.comp-ph])",
    "abstract": "Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of \"synthetic\" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the appr",
    "link": "http://arxiv.org/abs/2307.15714",
    "context": "Title: Synthetic pre-training for neural-network interatomic potentials. (arXiv:2307.15714v1 [physics.comp-ph])\nAbstract: Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of \"synthetic\" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the appr",
    "path": "papers/23/07/2307.15714.json",
    "total_tokens": 830,
    "translated_title": "用于神经网络原子间势的合成预训练",
    "translated_abstract": "基于机器学习的原子间势已经改变了原子材料建模领域。然而，机器学习势方法严重依赖于训练过程中的量子力学参考数据的质量和数量，因此开发数据集和训练流程是一个日益重要的挑战。在其他机器学习研究领域常见的“合成”（人工）数据的思想的基础上，我们通过展示自身使用现有机器学习势方法生成的大规模合成原子数据进行预训练，以提高神经网络原子势模型的数值精度和计算稳定性。我们展示了一系列关于碳的等变图神经网络势方法的可行性，并进行初步实验测试了该方法的局限性。",
    "tldr": "这项研究通过利用合成原子数据进行预训练，提高了神经网络原子势模型在计算实践中的数值精度和稳定性。初步实验表明了该方法在碳的等变图神经网络势方法中的可行性。",
    "en_tdlr": "This research enhances the numerical accuracy and stability of neural-network interatomic potential models in computational practice by pre-training them with synthetic atomistic data. Initial experiments demonstrate the feasibility of this approach for equivariant graph-neural-network potentials for carbon."
}