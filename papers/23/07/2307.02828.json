{
    "title": "Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2307.02828v1 [cs.CV])",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We",
    "link": "http://arxiv.org/abs/2307.02828",
    "context": "Title: Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2307.02828v1 [cs.CV])\nAbstract: Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We",
    "path": "papers/23/07/2307.02828.json",
    "total_tokens": 936,
    "translated_title": "基于采样的快速梯度重标定方法用于高度可转移的对抗攻击",
    "translated_abstract": "深度神经网络对通过向无害输入添加人类无法察觉的扰动来制造的对抗样本是脆弱的。在白盒环境中几乎实现了100％的攻击成功率后，更多的关注点转向了黑盒攻击，其中对抗样本的可转移性引起了重要关注。无论哪种情况，常见的基于梯度的方法通常使用符号函数来生成梯度更新的扰动，这提供了大致正确的方向并取得了巨大的成功。但是很少有研究关注它可能存在的局限性。在这项工作中，我们观察到原始梯度和生成噪声之间的偏差可能导致梯度更新估计不准确和对抗转移的次优解。为此，我们提出了一种基于采样的快速梯度重标定方法（S-FGRM）。具体而言，我们使用数据重标定来替代符号函数，而不需要额外的计算成本。",
    "tldr": "本文提出了一种名为采样-based快速梯度重标定方法（S-FGRM）的对抗攻击算法。通过使用数据重标定来替代梯度更新中的符号函数，我们解决了梯度更新估计不准确和对抗转移次优解的问题。",
    "en_tdlr": "This paper proposes a sampling-based fast gradient rescaling method (S-FGRM) for adversarial attacks. By using data rescaling instead of the sign function in gradient updates, the issue of inaccurate gradient estimation and suboptimal solutions for adversarial transferability is addressed."
}