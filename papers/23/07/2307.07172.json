{
    "title": "FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])",
    "abstract": "Federated Learning (FL) emerges as a distributed machine learning paradigm without end-user data transmission, effectively avoiding privacy leakage. Participating devices in FL are usually bandwidth-constrained, and the uplink is much slower than the downlink in wireless networks, which causes a severe uplink communication bottleneck. A prominent direction to alleviate this problem is federated dropout, which drops fractional weights of local models. However, existing federated dropout studies focus on random or ordered dropout and lack theoretical support, resulting in unguaranteed performance. In this paper, we propose Federated learning with Bayesian Inference-based Adaptive Dropout (FedBIAD), which regards weight rows of local models as probability distributions and adaptively drops partial weight rows based on importance indicators correlated with the trend of local training loss. By applying FedBIAD, each client adaptively selects a high-quality dropping pattern with accurate app",
    "link": "http://arxiv.org/abs/2307.07172",
    "context": "Title: FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])\nAbstract: Federated Learning (FL) emerges as a distributed machine learning paradigm without end-user data transmission, effectively avoiding privacy leakage. Participating devices in FL are usually bandwidth-constrained, and the uplink is much slower than the downlink in wireless networks, which causes a severe uplink communication bottleneck. A prominent direction to alleviate this problem is federated dropout, which drops fractional weights of local models. However, existing federated dropout studies focus on random or ordered dropout and lack theoretical support, resulting in unguaranteed performance. In this paper, we propose Federated learning with Bayesian Inference-based Adaptive Dropout (FedBIAD), which regards weight rows of local models as probability distributions and adaptively drops partial weight rows based on importance indicators correlated with the trend of local training loss. By applying FedBIAD, each client adaptively selects a high-quality dropping pattern with accurate app",
    "path": "papers/23/07/2307.07172.json",
    "total_tokens": 1009,
    "translated_title": "FedBIAD: 具有贝叶斯推断自适应丢弃的通信高效和准确性保证的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习范式，避免了隐私泄露问题，无需将最终用户数据传输。参与FL的设备通常带宽受限，且无线网络中上行速度远慢于下行速度，导致上行通信瓶颈严重。缓解这一问题的一个重要方向是联邦丢弃，即丢弃局部模型的部分权重。然而，现有的联邦丢弃研究集中在随机或有序丢弃上，并缺乏理论支持，导致性能无法保证。本文提出了一种基于贝叶斯推断和自适应丢弃的联邦学习方法（FedBIAD），该方法将局部模型的权重行视为概率分布，并根据与局部训练损失趋势相关的重要度指标自适应地丢弃部分权重行。通过应用FedBIAD，每个客户端可以自适应地选择高质量的丢弃模式，从而获得准确的论文摘要。",
    "tldr": "本文提出了一种基于贝叶斯推断和自适应丢弃的联邦学习方法（FedBIAD），通过将局部模型的权重行视为概率分布，并根据与局部训练损失趋势相关的重要度指标自适应地丢弃部分权重行，解决了联邦学习中的通信效率和性能保证问题。",
    "en_tdlr": "This paper proposes FedBIAD, a federated learning method based on Bayesian inference and adaptive dropout, which addresses the issues of communication efficiency and performance guarantee in federated learning by treating weight rows of local models as probability distributions and adaptively dropping partial weight rows based on importance indicators correlated with the trend of local training loss."
}