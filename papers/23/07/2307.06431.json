{
    "title": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])",
    "abstract": "Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e",
    "link": "http://arxiv.org/abs/2307.06431",
    "context": "Title: Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])\nAbstract: Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e",
    "path": "papers/23/07/2307.06431.json",
    "total_tokens": 895,
    "translated_title": "能量差异：一种适用于能量模型的独立于评分的损失函数",
    "translated_abstract": "能量模型是一种简单而强大的概率模型，但它们的普及受到了训练的计算负担的限制。我们提出了一种新的损失函数称为能量差异（ED），它不依赖于分数的计算或昂贵的马尔可夫链蒙特卡罗。我们证明了在不同的极限下，ED接近于显式分数匹配和负对数似然损失，有效地在两者之间插值。因此，最小化ED估计克服了在基于分数的估计方法中遇到的近视问题，同时还享有理论保证。通过数值实验证明，与显式分数匹配或对比散度相比，ED能够更快速、更准确地学习低维数据分布。对于高维图像数据，我们描述了流形假设对我们方法的限制，并通过对e模型的训练，证明了能量差异的有效性。",
    "tldr": "我们提出了一种新的能量模型损失函数，能够在不依赖分数计算或昂贵的蒙特卡罗方法的情况下，近似实现显式分数匹配和负对数似然损失，并在学习低维数据分布时具有更好的性能。",
    "en_tdlr": "We propose a novel loss function for energy-based models that approximates explicit score matching and negative log-likelihood loss without relying on score computation or expensive Monte Carlo methods, providing better performance in learning low-dimensional data distributions."
}