{
    "title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])",
    "abstract": "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
    "link": "http://arxiv.org/abs/2307.15190",
    "context": "Title: f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])\nAbstract: Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
    "path": "papers/23/07/2307.15190.json",
    "total_tokens": 898,
    "translated_title": "f-Divergence最小化用于序列级知识蒸馏",
    "translated_abstract": "知识蒸馏（KD）是将知识从大模型转移到小模型的过程。在自然语言处理领域，由于对不断增长的语言模型进行压缩的需求，它受到越来越多的关注。在这项工作中，我们提出了一个f-DISTILL框架，将序列级知识蒸馏建模为最小化广义f-分歧函数。我们在我们的框架下提出了四种蒸馏变种，并表明现有的 SeqKD 和 ENGINE 方法是我们f-DISTILL方法的近似。我们进一步推导出了我们的f-DISTILL的逐步分解，将难以处理的序列级分歧简化为可以以一种可处理的方式计算的词级损失。在四个数据集上的实验证明我们的方法优于现有的KD方法，并且我们对称的蒸馏损失可以更好地强迫学生从教师分布中学习。",
    "tldr": "本文提出了一个f-DISTILL框架，将序列级知识蒸馏建模为最小化广义f-分歧函数。通过在词级上计算损失，能够更好地压缩语言模型并使学生模型从教师模型中学习。提出的方法在多个数据集上表现出色。",
    "en_tdlr": "This paper proposes an f-DISTILL framework that formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. By calculating losses at the word-level, it can better compress language models and enable the student model to learn from the teacher model. The proposed method performs well on multiple datasets."
}