{
    "title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])",
    "abstract": "Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure th",
    "link": "http://arxiv.org/abs/2307.06869",
    "context": "Title: DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])\nAbstract: Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure th",
    "path": "papers/23/07/2307.06869.json",
    "total_tokens": 878,
    "translated_title": "DecompEval：将生成的文本作为无监督分解问答进行评估",
    "translated_abstract": "现有的自然语言生成（NLG）任务评估指标面临着泛化能力和可解释性的挑战。具体而言，大多数表现良好的指标需要在特定的NLG任务和评估维度的评估数据集上进行训练，这可能导致对任务特定数据集的过拟合。此外，现有的指标仅提供每个维度的评估分数，而不揭示如何获得该分数的证据。为了应对这些挑战，我们提出了一种简单而有效的指标称为DecompEval。这个指标将NLG评估建模为一种类似指令的问答任务，并利用经过指令调整的预训练语言模型（PLMs）而不是在评估数据集上进行训练，旨在增强泛化能力。为了使评估过程更具可解释性，我们将关于生成文本质量的设计指令式问题分解为衡量子问题的问题",
    "tldr": "DecompEval提出了一种简单而有效的指标来评估自然语言生成任务，它将评估建模为一种类似指令的问答任务，并利用预训练语言模型进行衡量，以增强泛化能力和可解释性。",
    "en_tdlr": "DecompEval proposes a simple yet effective metric to evaluate natural language generation tasks. It formulates evaluation as an instruction-style question answering task and utilizes pre-trained language models for measurement, aiming to enhance generalization ability and interpretability."
}