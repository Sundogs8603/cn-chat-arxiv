{
    "title": "Detecting Morphing Attacks via Continual Incremental Training. (arXiv:2307.15105v1 [cs.CV])",
    "abstract": "Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and ",
    "link": "http://arxiv.org/abs/2307.15105",
    "context": "Title: Detecting Morphing Attacks via Continual Incremental Training. (arXiv:2307.15105v1 [cs.CV])\nAbstract: Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and ",
    "path": "papers/23/07/2307.15105.json",
    "total_tokens": 749,
    "translated_title": "通过持续增量训练检测变形攻击",
    "translated_abstract": "在数据传输和存储限制的情况下，无法使用单个数据集组合多个数据来源进行批次训练，这对于开发鲁棒性模型来说是一种挑战。本文假设最近的持续学习（CL）范式可以有效解决通过多个站点进行增量训练的问题。实验结果表明，一种特殊的CL方法，即无遗忘学习（LwF），是性能最好的算法之一。",
    "tldr": "本文通过模拟更新学习模型以适应可变大小的新数据块的情景，研究了不同持续学习方法在检测变形攻击中的性能。实验结果表明，无遗忘学习（LwF）是表现最好的算法之一。",
    "en_tdlr": "This paper investigates the performance of different Continual Learning methods in detecting morphing attacks by simulating the scenario where a learning model is updated with variable-sized chunks of new data. Experimental results show that Learning without Forgetting (LwF) is one of the best-performing algorithms."
}