{
    "title": "Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])",
    "abstract": "A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.",
    "link": "http://arxiv.org/abs/2307.10654",
    "context": "Title: Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])\nAbstract: A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.",
    "path": "papers/23/07/2307.10654.json",
    "total_tokens": 804,
    "translated_title": "条件期望网络用于SHAP",
    "translated_abstract": "SHAP是一种非常流行的模型无关技术，用于解释预测模型。SHAP的两个最受欢迎的版本是条件期望版本和无条件期望版本（后者也称为干预SHAP）。除了基于树的方法之外，通常使用无条件版本（出于计算原因）。我们提供了一种（代理）神经网络方法，可以高效地计算神经网络和其他回归模型的条件版本，并正确考虑特征组件之间的依赖结构。这个方法还可以用于提供与广义线性模型（GLM）类似的复杂回归模型的drop1和anova分析，并提供考虑特征组件中正确依赖结构的偏依赖图（PDP）的对应版本。",
    "tldr": "这项工作提出了一种用于计算条件版本的（代理）神经网络方法，该方法可以有效地解释神经网络和其他回归模型的预测结果，并考虑特征之间的依赖关系。同时，该方法还可以应用于复杂回归模型的分析，并提供正确的偏依赖图表示。",
    "en_tdlr": "This work proposes a (surrogate) neural network approach for efficiently calculating the conditional version of SHAP, considering the dependence structure in the feature components. It can also be applied to analyze complex regression models and provide accurate partial dependence plots."
}