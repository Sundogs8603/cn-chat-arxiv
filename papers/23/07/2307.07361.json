{
    "title": "Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])",
    "abstract": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose \\emph{gloss attention}, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-s",
    "link": "http://arxiv.org/abs/2307.07361",
    "context": "Title: Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])\nAbstract: Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose \\emph{gloss attention}, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-s",
    "path": "papers/23/07/2307.07361.json",
    "total_tokens": 921,
    "translated_title": "无需注释的手语翻译中的注释关注机制",
    "translated_abstract": "迄今为止，大多数手语翻译方法都需要使用注释来提供额外的监督信息，然而，注释的获取并不容易。为了解决这个问题，我们首先对现有模型进行分析，确认注释如何使手语翻译更容易。我们发现，注释可以为模型提供两个方面的信息：1）它可以帮助模型隐式地学习连续手语视频中的语义边界位置，2）它可以帮助模型全局理解手语视频。然后，我们提出了“注释关注”机制，使得模型能够在具有相同语义的视频片段内局部关注，就像注释帮助现有模型一样。此外，我们将自然语言模型中句子间相似性的知识转移到我们的注释关注手语翻译网络（GASLT）中，以帮助其在句子层面上理解手语视频。在多个大规模数据集上进行的实验证明了我们的方法的有效性。",
    "tldr": "本论文提出了一种无需注释的手语翻译中的注释关注机制（Gloss Attention），通过学习语义边界位置和全局理解手语视频，实现了对手语视频的准确翻译。同时，通过将句子间相似性的知识转移，提高了翻译网络的理解能力。"
}