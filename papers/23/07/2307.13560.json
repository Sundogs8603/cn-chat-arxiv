{
    "title": "XDLM: Cross-lingual Diffusion Language Model for Machine Translation. (arXiv:2307.13560v1 [cs.CL])",
    "abstract": "Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.",
    "link": "http://arxiv.org/abs/2307.13560",
    "context": "Title: XDLM: Cross-lingual Diffusion Language Model for Machine Translation. (arXiv:2307.13560v1 [cs.CL])\nAbstract: Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.",
    "path": "papers/23/07/2307.13560.json",
    "total_tokens": 901,
    "translated_title": "XDLM: 用于机器翻译的跨语言扩散语言模型",
    "translated_abstract": "最近，扩散模型在图像生成任务中表现出色，并且已经应用于神经语言处理（NLP）中的可控文本生成。然而，扩散模型在跨语言环境中的应用相对较少。此外，尽管已经研究了在单一语言中使用扩散模型进行预训练，但跨语言预训练的潜力仍未被深入研究。为了填补这些空白，我们提出了XDLM，一种新颖的用于机器翻译的跨语言扩散模型，包括预训练和微调阶段。在预训练阶段，我们提出了TLDM，一种新的训练目标，用于掌握不同语言之间的映射关系；在微调阶段，我们基于预训练模型构建了翻译系统。我们在几个机器翻译基准上进行了评估，并超过了扩散和Transformer基线模型。",
    "tldr": "本文介绍了XDLM，一种用于机器翻译的跨语言扩散语言模型。通过预训练和微调阶段，我们成功地提高了在不同语言之间的翻译性能，超过了传统扩散模型和Transformer模型。",
    "en_tdlr": "This paper presents XDLM, a cross-lingual diffusion language model for machine translation. Through pretraining and fine-tuning stages, XDLM achieves improved translation performance across different languages compared to traditional diffusion models and Transformer models."
}