{
    "title": "Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data. (arXiv:2307.03347v1 [cs.LG])",
    "abstract": "For many real-world time series tasks, the computational complexity of prevalent deep leaning models often hinders the deployment on resource-limited environments (e.g., smartphones). Moreover, due to the inevitable domain shift between model training (source) and deploying (target) stages, compressing those deep models under cross-domain scenarios becomes more challenging. Although some of existing works have already explored cross-domain knowledge distillation for model compression, they are either biased to source data or heavily tangled between source and target data. To this end, we design a novel end-to-end framework called Universal and joint knowledge distillation (UNI-KD) for cross-domain model compression. In particular, we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains from the teacher to the student model via an adversarial learning scheme. More specifically, a featur",
    "link": "http://arxiv.org/abs/2307.03347",
    "context": "Title: Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data. (arXiv:2307.03347v1 [cs.LG])\nAbstract: For many real-world time series tasks, the computational complexity of prevalent deep leaning models often hinders the deployment on resource-limited environments (e.g., smartphones). Moreover, due to the inevitable domain shift between model training (source) and deploying (target) stages, compressing those deep models under cross-domain scenarios becomes more challenging. Although some of existing works have already explored cross-domain knowledge distillation for model compression, they are either biased to source data or heavily tangled between source and target data. To this end, we design a novel end-to-end framework called Universal and joint knowledge distillation (UNI-KD) for cross-domain model compression. In particular, we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains from the teacher to the student model via an adversarial learning scheme. More specifically, a featur",
    "path": "papers/23/07/2307.03347.json",
    "total_tokens": 1030,
    "translated_title": "跨领域时间序列数据的模型压缩中蒸馏出通用和联合知识",
    "translated_abstract": "在许多实际的时间序列任务中，流行的深度学习模型的计算复杂性常常阻碍了在资源有限的环境（例如智能手机上）的部署。而且，由于模型训练（源）和部署（目标）阶段之间不可避免的领域漂移，在跨领域场景下压缩这些深度模型变得更加具有挑战性。尽管已经存在一些现有的工作探索了用于模型压缩的跨领域知识蒸馏，但它们要么偏向于源数据，要么在源数据和目标数据之间交织在一起。为此，我们设计了一个新的端到端框架，称为通用和联合知识蒸馏（UNI-KD）用于跨领域模型压缩。具体而言，我们提出通过对抗学习方案将教师模型的通用特征级知识和双领域共享的联合logit级知识传输到学生模型中。具体来说，一个特征级别的蒸馏网络被用来在源领域和目标领域之间转移通用特征知识，一个对抗学习模块被用来将联合logit级知识传递给学生模型。",
    "tldr": "这篇论文介绍了一种用于跨领域模型压缩的新的框架，通过蒸馏教师模型中的通用特征级知识和双领域共享的联合logit级知识，实现了在资源有限环境中部署深度学习模型的难题。"
}