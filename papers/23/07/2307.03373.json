{
    "title": "All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])",
    "abstract": "Current mainstream vision-language (VL) tracking framework consists of three parts, \\ie a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, \\eg similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate",
    "link": "http://arxiv.org/abs/2307.03373",
    "context": "Title: All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])\nAbstract: Current mainstream vision-language (VL) tracking framework consists of three parts, \\ie a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, \\eg similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate",
    "path": "papers/23/07/2307.03373.json",
    "total_tokens": 886,
    "translated_title": "一体化视觉-语言跟踪的探索：多模态对齐",
    "translated_abstract": "当前主流的视觉-语言跟踪框架包括三个部分，即视觉特征提取器、语言特征提取器和融合模型。为了追求更好的性能，视觉-语言跟踪常常使用定制和更重的单模态编码器和多模态融合模型。尽管有效，现有的视觉-语言跟踪器将特征提取和特征集成分开，导致提取的特征缺乏语义引导，在复杂场景下具有有限的目标感知能力，例如相似的干扰物和极端光照。在这项研究中，受到近期在自然语言和计算机视觉任务中统一架构探索的成功启发，我们提出了一种一体化框架，通过采用统一的Transformer主干网络来学习联合特征提取和交互。具体而言，我们混合原始的视觉和语言信号来生成注入语言的视觉单元，然后将它们连接起来。",
    "tldr": "本文提出了一种一体化视觉-语言跟踪框架，采用统一的Transformer主干网络，实现联合特征提取和交互，提高了在复杂场景下的目标感知能力。",
    "en_tdlr": "This paper proposes an All-in-One framework for vision-language tracking, using a unified Transformer backbone to achieve joint feature extraction and interaction, improving target awareness in complex scenarios."
}