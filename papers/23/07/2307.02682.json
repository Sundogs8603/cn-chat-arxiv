{
    "title": "Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])",
    "abstract": "Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks c",
    "link": "http://arxiv.org/abs/2307.02682",
    "context": "Title: Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])\nAbstract: Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks c",
    "path": "papers/23/07/2307.02682.json",
    "total_tokens": 910,
    "translated_title": "通过联合优化文本和时刻实现零样本密集视频字幕生成",
    "translated_abstract": "密集视频字幕生成是一项将有意义的时刻定位和相关字幕生成应用于视频中的任务，通常需要一个昂贵的带有标注的视频片段和文本的语料库。为了降低标注成本，我们提出了一种新颖的零样本密集视频字幕生成方法ZeroTA。我们的方法在训练阶段不需要任何视频或标注，而是通过仅在输入上进行优化，在测试时定位和描述每个输入视频中的事件。这通过引入一个表示视频中的时间段的软时刻掩码，并与语言模型的前缀参数进行联合优化来实现。这种联合优化通过最大化生成文本与视频中的某个时刻之间的匹配分数，将固定的语言生成模型（即GPT-2）与固定的视觉-语言对比模型（即CLIP）进行对齐。我们还引入了一对一的时间IoU损失，使一组软时刻掩码之间进行比对。",
    "tldr": "通过在训练阶段不使用视频和标注，而是在测试时仅优化输入，我们提出了一种零样本的密集视频字幕生成方法。通过联合优化文本和时刻，我们的方法能够在视频中准确地定位和描述事件。",
    "en_tdlr": "We propose a zero-shot approach for dense video captioning that does not require any training videos or annotations. Instead, it optimizes solely on the input at test time, localizing and describing events within each video. By jointly optimizing text and moments, our method achieves accurate event localization and description."
}