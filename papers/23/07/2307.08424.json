{
    "title": "Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (arXiv:2307.08424v2 [cs.AI] UPDATED)",
    "abstract": "Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.  In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced t",
    "link": "http://arxiv.org/abs/2307.08424",
    "context": "Title: Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (arXiv:2307.08424v2 [cs.AI] UPDATED)\nAbstract: Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.  In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced t",
    "path": "papers/23/07/2307.08424.json",
    "total_tokens": 876,
    "translated_title": "无法阻止的攻击: 基于条件扩散模型的标签仅模型逆推",
    "translated_abstract": "模型逆推攻击(MIAs)旨在从目标模型的训练集中恢复私密数据，这对深度学习模型的隐私构成威胁。MIAs主要关注白盒情景，在此情况下，攻击者可以完全访问目标模型的结构和参数。然而，实际应用是黑盒情景，对手很难获取与模型相关的参数，许多模型仅输出预测标签。现有的黑盒MIAs主要集中在设计优化策略上，而生成模型只是从白盒MIA中使用的GAN迁移而来。据我们所知，我们的研究是标签仅黑盒情景下可行攻击模型的开创性研究。在本文中，我们使用条件扩散模型开发了一种新颖的MIA方法，可以在目标模型输出标签的情况下无需任何额外的优化来恢复目标的精确样本。引入了两个主要技术",
    "tldr": "本论文提出了一种在标签仅黑盒场景下的模型逆推攻击方法，使用条件扩散模型恢复目标的精确样本，无需额外的优化。"
}