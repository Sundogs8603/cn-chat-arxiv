{
    "title": "Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent. (arXiv:2307.11939v1 [cs.LG])",
    "abstract": "Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\\em a ne",
    "link": "http://arxiv.org/abs/2307.11939",
    "context": "Title: Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent. (arXiv:2307.11939v1 [cs.LG])\nAbstract: Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\\em a ne",
    "path": "papers/23/07/2307.11939.json",
    "total_tokens": 944,
    "translated_title": "差分隐私随机梯度下降的批次剪裁和自适应逐层剪裁",
    "translated_abstract": "在差分隐私随机梯度下降（DPSGD）中，每一轮传输的剪裁梯度之和被高斯噪声混淆，用于更新全局模型，常表示为深度神经网络。由于剪裁梯度是分别计算的，我们称之为个体剪裁（IC），深度神经网络如resnet-18 无法使用批次正则化层（BNL），这是实现高准确性的深度神经网络的重要组成部分。为了利用BNL，我们引入了批次剪裁（BC），在原始的DPSGD中，我们将单个梯度进行平均和剪裁。此外，不同层的模型条目对添加的高斯噪声具有不同的敏感性。因此，引入了自适应逐层剪裁方法（ALC），其中每一层都有自己适应调整的剪裁常数，但迄今为止没有严密的差分隐私证明。在本文中，我们提出了一种新的方法",
    "tldr": "这篇论文提出了一种用于差分隐私随机梯度下降的批次剪裁和自适应逐层剪裁方法。通过引入批次剪裁，可以解决深度神经网络无法使用批次正则化层的问题。同时，通过自适应逐层剪裁，可以根据不同层的敏感性调整剪裁常数。"
}