{
    "title": "Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])",
    "abstract": "We report a controlled study investigating the effect of visual information (i.e., seeing the speaker) on spoken language comprehension. We compare the ERP signature (N400) associated with each word in audio-only and audio-visual presentations of the same verbal stimuli. We assess the extent to which surprisal measures (which quantify the predictability of words in their lexical context) are generated on the basis of different types of language models (specifically n-gram and Transformer models) that predict N400 responses for each word. Our results indicate that cognitive effort differs significantly between multimodal and unimodal settings. In addition, our findings suggest that while Transformer-based models, which have access to a larger lexical context, provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting. This highlights the significant impact of local lexical context on cognitive processing in a multimodal environmen",
    "link": "http://arxiv.org/abs/2307.07277",
    "context": "Title: Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])\nAbstract: We report a controlled study investigating the effect of visual information (i.e., seeing the speaker) on spoken language comprehension. We compare the ERP signature (N400) associated with each word in audio-only and audio-visual presentations of the same verbal stimuli. We assess the extent to which surprisal measures (which quantify the predictability of words in their lexical context) are generated on the basis of different types of language models (specifically n-gram and Transformer models) that predict N400 responses for each word. Our results indicate that cognitive effort differs significantly between multimodal and unimodal settings. In addition, our findings suggest that while Transformer-based models, which have access to a larger lexical context, provide a better fit in the audio-only setting, 2-gram language models are more effective in the multimodal setting. This highlights the significant impact of local lexical context on cognitive processing in a multimodal environmen",
    "path": "papers/23/07/2307.07277.json",
    "total_tokens": 922,
    "translated_title": "语音和视听理解中的单词是否同样令人惊讶？",
    "translated_abstract": "我们报告了一项受控研究，调查了视觉信息（即看到说话者）对口头语言理解的影响。我们比较了相同口头刺激的仅音频和音频-视觉呈现中每个单词相关的ERP标记（N400）。我们评估了不同类型的语言模型（具体来说是n-gram和Transformer模型）基于词汇上下文预测N400响应的可预测性度量（surprisal）。我们的结果表明，认知努力在多模态和单模态环境中存在显著差异。此外，我们的研究结果表明，在仅音频环境中，具有较大词汇上下文的Transformer模型提供更好的拟合，而2-gram语言模型在多模态环境中更有效。这凸显了局部词汇上下文对多模态环境下认知处理的显著影响。",
    "tldr": "本研究调查了视觉信息对口头语言理解的影响，并评估了不同语言模型在多模态和单模态环境下的表现。结果表明，多模态和单模态环境下的认知努力存在差异，局部词汇上下文对多模态环境下的认知处理有显著影响。",
    "en_tdlr": "This study investigates the impact of visual information on spoken language comprehension and evaluates the performance of different language models in multimodal and unimodal settings. The results show that cognitive effort varies between multimodal and unimodal environments, highlighting the significant influence of local lexical context on cognitive processing in a multimodal environment."
}