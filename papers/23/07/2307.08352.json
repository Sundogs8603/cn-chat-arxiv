{
    "title": "Zero-th Order Algorithm for Softmax Attention Optimization. (arXiv:2307.08352v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have brought about significant transformations in human society. Among the crucial computations in LLMs, the softmax unit holds great importance. Its helps the model generating a probability distribution on potential subsequent words or phrases, considering a series of input words. By utilizing this distribution, the model selects the most probable next word or phrase, based on the assigned probabilities. The softmax unit assumes a vital function in LLM training as it facilitates learning from data through the adjustment of neural network weights and biases.  With the development of the size of LLMs, computing the gradient becomes expensive. However, Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-s",
    "link": "http://arxiv.org/abs/2307.08352",
    "context": "Title: Zero-th Order Algorithm for Softmax Attention Optimization. (arXiv:2307.08352v1 [cs.LG])\nAbstract: Large language models (LLMs) have brought about significant transformations in human society. Among the crucial computations in LLMs, the softmax unit holds great importance. Its helps the model generating a probability distribution on potential subsequent words or phrases, considering a series of input words. By utilizing this distribution, the model selects the most probable next word or phrase, based on the assigned probabilities. The softmax unit assumes a vital function in LLM training as it facilitates learning from data through the adjustment of neural network weights and biases.  With the development of the size of LLMs, computing the gradient becomes expensive. However, Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-s",
    "path": "papers/23/07/2307.08352.json",
    "total_tokens": 810,
    "translated_title": "针对Softmax注意力优化的零阶算法",
    "translated_abstract": "大规模语言模型（LLM）在人类社会中带来了重大的变革。在LLMs中，softmax单元的计算非常重要。它帮助模型在一系列输入单词中生成潜在的下一个单词或短语的概率分布。通过利用这个分布，模型根据分配的概率选择最有可能的下一个单词或短语。softmax单元在LLM训练中起到关键作用，因为它通过调整神经网络的权重和偏差来实现从数据中学习。随着LLMs的规模的发展，计算梯度变得昂贵。然而，零阶方法可以通过仅进行前向传递来近似计算梯度。在本文中，我们提出了一个专门针对Softmax优化的零阶算法。我们证明了我们的算法的收敛性，并强调其在高效计算大规模语言模型的梯度方面的有效性。",
    "tldr": "这篇论文介绍了一个针对Softmax优化的零阶算法，能够有效地计算大规模语言模型的梯度。"
}