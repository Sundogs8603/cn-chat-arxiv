{
    "title": "Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting",
    "abstract": "arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.",
    "link": "https://arxiv.org/abs/2307.00493",
    "context": "Title: Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting\nAbstract: arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.",
    "path": "papers/23/07/2307.00493.json",
    "total_tokens": 915,
    "translated_title": "傅里叶混合窗口注意力：加速长序列时间序列预测的Informer方法",
    "translated_abstract": "我们研究了一种快速的本地全局窗口注意力方法，用于加速Informer在长序列时间序列预测中的应用。虽然窗口注意力是局部的和具有相当大的计算节约，但它缺乏捕获全局令牌信息的能力，这通过后续的傅里叶变换块进行补偿。我们的方法名为FWin，不依赖于Informer的ProbSparse注意力中的查询稀疏性假设和经验性近似。通过对单变量和多变量数据集的实验，我们证明了FWin transformers可以提高Informer的整体预测准确性，同时将其推断速度加速40%至50%。我们还在非线性回归模型中展示了学习到的FWin类型注意力在时间序列数据上通过从Informer模型的全注意力层中提取的关键向量来逼近甚至胜过基于Softmax全注意力的方法。",
    "tldr": "本文提出了一种名为FWin的快速本地全局窗口注意力方法，用于加速长序列时间序列预测的Informer方法。通过实验证明，该方法可以提高预测准确性并加速推断速度，同时在非线性回归模型中表现出与Softmax全注意力相媲美甚至更优的效果。",
    "en_tdlr": "This paper proposes a method called FWin, which is a fast local-global window attention method, to accelerate the Informer model for long sequence time-series forecasting. Through experiments, it is shown that FWin improves prediction accuracy and speeds up inference, and even performs as well as or better than the Softmax full attention method in a non-linear regression model."
}