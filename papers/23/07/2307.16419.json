{
    "title": "Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)",
    "abstract": "An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe",
    "link": "http://arxiv.org/abs/2307.16419",
    "context": "Title: Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)\nAbstract: An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe",
    "path": "papers/23/07/2307.16419.json",
    "total_tokens": 839,
    "translated_title": "持续学习的子空间蒸馏",
    "translated_abstract": "在持续学习中，一个最终的目标是保留在前面任务中学到的知识，同时学习新任务。为了减轻对先前知识的遗忘，我们提出了一种新颖的知识蒸馏技术，该技术考虑了神经网络潜在/输出空间的流形结构在学习新任务中。为了实现这一点，我们提出了一种近似数据流形的方法，从而通过线性子空间来建模结构并保持神经网络在学习新概念时的知识。我们证明了使用子空间建模具有一些有趣的特性，包括对噪声的鲁棒性，因此在持续学习中可以有效地减轻灾难性遗忘。我们还讨论并展示了我们的方法如何适应分类和分割问题。经验上，我们观察到我们的方法在几个具有挑战性的持续学习方法上表现优于其他方法。",
    "tldr": "该论文提出了一种新颖的知识蒸馏技术，通过近似数据流形，并用线性子空间建模结构，来在持续学习中减轻灾难性遗忘。实验证明，该方法优于其他方法，在多个具有挑战性的持续学习任务中表现出色。"
}