{
    "title": "Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])",
    "abstract": "Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep",
    "link": "http://arxiv.org/abs/2307.09615",
    "context": "Title: Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])\nAbstract: Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep",
    "path": "papers/23/07/2307.09615.json",
    "total_tokens": 994,
    "translated_title": "深入探究可解释性深度学习在神经影像学中的应用：一项全面调查",
    "translated_abstract": "深度学习（DL）模型因其能够直接从原始数据中进行端到端学习，减轻了对错误易发特征提取阶段的担忧而备受青睐。最近的DL基于神经影像学的研究也在传统机器学习算法上取得了显著的性能提升。然而，深度学习模型的挑战仍然存在，因为这些模型缺乏透明度，无法成功应用于现实世界的应用中。最近几年，可解释的人工智能（XAI）经历了快速发展，主要用于揭示模型如何作出决策的直觉，这对于安全关键领域如医疗保健、金融和执法机构至关重要。尽管解释性领域取得了显著的进展，但研究人员仍不清楚后期方法揭示了模型学习的哪个方面以及如何验证其可靠性。本文综合评述了可解释的深度学习方法在神经影像学中的应用。",
    "tldr": "本文对可解释的深度学习在神经影像学中的应用进行了全面调查，深度学习模型的易解释性仍然存在挑战，近年来的研究主要集中在如何理解模型决策的直觉，还需探索如何验证解释性方法的可靠性。"
}