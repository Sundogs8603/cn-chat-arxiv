{
    "title": "FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])",
    "abstract": "The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Sc",
    "link": "http://arxiv.org/abs/2307.04905",
    "context": "Title: FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])\nAbstract: The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Sc",
    "path": "papers/23/07/2307.04905.json",
    "total_tokens": 904,
    "translated_title": "FedYolo: 使用预训练的Transformer模型增强联邦学习",
    "translated_abstract": "机器学习应用的增长和多样性促使我们重新思考使用移动和边缘设备进行学习的方式。我们如何解决不同客户目标和稀缺异构数据的学习问题？虽然联邦学习旨在解决这些问题，但它面临着统一解决方案的挑战。大型Transformer模型已被证明可以在各种任务中工作，并实现了显著的少样本适应能力。这引发了一个问题：客户能否使用单个通用模型，而不是为每个任务创建定制模型，并遵守设备和网络的限制？在这项工作中，我们使用预训练的Transformer模型来实现这些设备上的学习目标，并深入探讨模型大小和模块化的作用，其中模块化指的是通过提示或适配器等模块进行适应。在联邦学习方面，我们证明了：（1）规模较大可以缩小不同方法之间的准确性差距，并提高异构性的鲁棒性。",
    "tldr": "本文研究了在联邦学习中使用预训练的Transformer模型来实现在设备上的学习目标，探讨了模型大小和模块化的作用，并证明了规模较大可以提高异构性的鲁棒性。",
    "en_tdlr": "This paper investigates the use of pretrained Transformer models for on-device learning in federated learning, explores the impact of model size and modularity, and demonstrates that larger scale can improve heterogeneity robustness."
}