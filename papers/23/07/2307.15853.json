{
    "title": "Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise. (arXiv:2307.15853v1 [cs.LG])",
    "abstract": "Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators. Through a formal analysis of the ",
    "link": "http://arxiv.org/abs/2307.15853",
    "context": "Title: Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise. (arXiv:2307.15853v1 [cs.LG])\nAbstract: Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators. Through a formal analysis of the ",
    "path": "papers/23/07/2307.15853.json",
    "total_tokens": 889,
    "translated_title": "通过使用正确截尾高斯噪声训练，改进NVCiM DNN加速器的真实最坏情况性能",
    "translated_abstract": "基于非易失性存储器（NVM）设备构建的计算存储一体（CiM）技术，由于其原地数据处理能力和卓越的能源效率，对于加速深度神经网络（DNN）具有很大潜力。然而，将经过良好训练的模型参数映射到NVM设备后，往往会出现与预期值相差较大的偏差，导致CiM DNN加速器性能显著降低。目前已经提出了很多解决方案来解决此问题，但主要集中于改进CiM DNN加速器的平均性能。如何在设备变化的影响下保证最坏情况性能，对于许多安全关键的应用，如自动驾驶汽车，至关重要，但远未得到充分探索。",
    "tldr": "通过使用k-th百分位性能（KPP）来捕捉在CiM加速器上执行的DNN模型的真实最坏情况性能，从而改善NVCiM DNN加速器的性能。",
    "en_tdlr": "The paper proposes the use of the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators, in order to improve the performance of NVCiM DNN accelerators."
}