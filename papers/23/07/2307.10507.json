{
    "title": "FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])",
    "abstract": "Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve",
    "link": "http://arxiv.org/abs/2307.10507",
    "context": "Title: FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])\nAbstract: Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve",
    "path": "papers/23/07/2307.10507.json",
    "total_tokens": 866,
    "translated_title": "FedSoup:通过选择性模型插值改善联邦学习中的泛化和个性化能力",
    "translated_abstract": "跨机构的联邦学习使得可以在分布在数据中心（如医院和临床研究实验室）的数据集上开发机器学习模型。然而，最近的研究发现，当面临分布偏移时，当前的联邦学习算法在本地和全局性能之间存在一种权衡。具体而言，个性化的联邦学习方法往往倾向于对本地数据过拟合，导致局部模型发生转折，并抑制其对非分布式数据的泛化能力。在本文中，我们提出了一种新颖的联邦模型汤方法（即，选择性插值模型参数）来优化本地和全局性能之间的权衡。具体而言，在联邦训练阶段，每个客户端通过监控本地和全局模型之间的插值模型的表现来维护自己的全局模型池。这使得我们能够减轻过拟合，并寻求平坦的极小值，从而可以显著提高系统的泛化能力和个性化能力。",
    "tldr": "本文通过选择性模型插值的方式，在联邦学习中改善了泛化能力和个性化能力。",
    "en_tdlr": "This paper improves the generalization and personalization in federated learning through selective model interpolation."
}