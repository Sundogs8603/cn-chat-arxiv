{
    "title": "AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])",
    "abstract": "Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from ANNs, exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Traditional approaches to discovering optimal architectures primarily rely on either manual procedures, which are time-consuming, or Neural Architecture Search (NAS) methods, which are usually expensive in terms of memory footprints and computation time. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance and energy-efficient Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a ",
    "link": "http://arxiv.org/abs/2307.00293",
    "context": "Title: AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])\nAbstract: Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from ANNs, exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Traditional approaches to discovering optimal architectures primarily rely on either manual procedures, which are time-consuming, or Neural Architecture Search (NAS) methods, which are usually expensive in terms of memory footprints and computation time. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance and energy-efficient Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a ",
    "path": "papers/23/07/2307.00293.json",
    "total_tokens": 813,
    "translated_title": "AutoST：无需训练的脉冲变压器神经架构搜索",
    "translated_abstract": "脉冲变压器因同时具备脉冲神经网络（SNN）的能效和变压器的高容量而备受关注。然而，现有的脉冲变压器架构基于人工神经网络（ANN）的推导，存在明显的架构差距，导致性能不及其ANN对应物。传统方法通常依赖手动过程或神经架构搜索（NAS）方法来寻找最优架构，但这些方法要么耗时，要么在内存占用和计算时间方面代价高昂。为解决这些限制，我们提出了AutoST，一种用于脉冲变压器的无需训练的NAS方法，可以快速识别高性能和能效的脉冲变压器架构。与现有的无需训练NAS方法不同，我们提出利用浮点运算（FLOPs）作为一种指导可行性的度量，",
    "tldr": "AutoST是一种无需训练的神经架构搜索方法，用于识别高性能和能效的脉冲变压器架构。",
    "en_tdlr": "AutoST is a training-free neural architecture search method for identifying high-performance and energy-efficient spiking transformer architectures."
}