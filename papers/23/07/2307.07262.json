{
    "title": "MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])",
    "abstract": "Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.",
    "link": "http://arxiv.org/abs/2307.07262",
    "context": "Title: MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])\nAbstract: Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.",
    "path": "papers/23/07/2307.07262.json",
    "total_tokens": 812,
    "translated_title": "MorphPiece: 远离统计语言表示的一步",
    "translated_abstract": "分词是现代自然语言处理流程中至关重要的一部分。然而，用于大型语言模型的当代分词器基于对文本语料库的统计分析，对语言特征的考虑较少。我们提出了一种基于语言学动机的分词方案MorphPiece，部分基于底层文本的形态分割。使用该分词器（称为MorphGPT）训练的类GPT的因果语言模型显示出比在标准BPE分词器上训练时更优越的收敛性。具体来说，我们获得了与规模大6倍的模型相媲美的语言建模性能。此外，我们在监督和无监督的条件下对MorphGPT在各种NLP任务上进行了评估，并发现在各个方面与GPT-2模型相比有更出色的性能。",
    "tldr": "本文提出了一种基于语言学动机的分词方案MorphPiece，并使用该方案训练了一个称为MorphGPT的语言模型。MorphGPT在语言建模以及各种NLP任务上都表现出了比传统模型更优异的性能。",
    "en_tdlr": "This paper proposes a linguistically motivated tokenization scheme called MorphPiece and trains a language model called MorphGPT based on this scheme. MorphGPT shows superior performance compared to traditional models in language modeling as well as various NLP tasks."
}