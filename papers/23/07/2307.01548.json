{
    "title": "Knowledge Graph for NLG in the context of conversational agents. (arXiv:2307.01548v1 [cs.AI])",
    "abstract": "The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for",
    "link": "http://arxiv.org/abs/2307.01548",
    "context": "Title: Knowledge Graph for NLG in the context of conversational agents. (arXiv:2307.01548v1 [cs.AI])\nAbstract: The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for",
    "path": "papers/23/07/2307.01548.json",
    "total_tokens": 812,
    "translated_title": "对话代理中自然语言生成的知识图谱",
    "translated_abstract": "知识图谱（KG）的使用提高了对话代理提供的响应的准确性和全面性。在对话中生成答案包括从这些知识图谱生成文本，这仍被视为一个具有重要意义的具有挑战的任务。在本文中，我们对用于知识图谱到文本生成的不同架构进行了回顾，包括图神经网络、图转换器和带有seq2seq模型的线性化。我们讨论了每种架构的优点和局限性，并得出结论，选择架构将取决于具体任务的要求。我们还强调了考虑诸如执行时间和模型有效性等约束的重要性，特别是在对话代理的背景下。基于这些约束以及DAVI领域的标记数据的可用性，我们选择使用基于seq2seq的Transformer模型（PLMs）。",
    "tldr": "本文回顾了对话代理中使用的不同的知识图谱到文本生成架构，并讨论了每种架构的优点和局限性。根据具体任务的要求选择合适的架构，并强调了考虑执行时间和模型有效性的重要性，特别是在对话代理背景下。"
}