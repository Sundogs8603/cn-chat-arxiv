{
    "title": "Review helps learn better: Temporal Supervised Knowledge Distillation. (arXiv:2307.00811v2 [cs.CV] UPDATED)",
    "abstract": "Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and ",
    "link": "http://arxiv.org/abs/2307.00811",
    "context": "Title: Review helps learn better: Temporal Supervised Knowledge Distillation. (arXiv:2307.00811v2 [cs.CV] UPDATED)\nAbstract: Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and ",
    "path": "papers/23/07/2307.00811.json",
    "total_tokens": 940,
    "translated_title": "评论帮助更好地学习：基于时间的监督知识蒸馏",
    "translated_abstract": "在学习知识时，评论发挥了重要作用。在某个时间点获取的知识可能在之前的经验帮助下得到极大的启发。因此，知识增长过程应该在时间维度上展现出强烈的关联性。在我们的研究中，我们发现在网络训练过程中，特征图的演化遵循时间序列特性。适当的时间监督可以进一步提高网络训练性能。受到这一观察的启发，我们提出了基于时间的监督知识蒸馏（TSKD）。具体而言，我们通过卷积长短期记忆网络（Conv-LSTM）提取学生网络在不同训练阶段的时空特征。然后，我们通过动态目标训练学生网络，而不是静态的教师网络特征。这个过程实现了学生网络中旧知识的优化，并将其用于辅助当前的学习。广泛的实验证实了该方法的有效性。",
    "tldr": "本文提出了一种基于时间的监督知识蒸馏方法，利用评论来帮助学生网络的学习。通过提取学生网络在不同训练阶段的时空特征，并通过动态目标进行训练，实现了对学生网络中旧知识的优化和利用，从而提高了网络的训练性能。"
}