{
    "title": "Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])",
    "abstract": "Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st",
    "link": "http://arxiv.org/abs/2307.11714",
    "context": "Title: Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])\nAbstract: Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st",
    "path": "papers/23/07/2307.11714.json",
    "total_tokens": 945,
    "translated_title": "使用切片Wasserstein损失来训练神经网络的SGD收敛性分析",
    "translated_abstract": "最优输运近年来引发了广泛的兴趣，特别是由于Wasserstein距离，它提供了一种几何上合理和直观的比较概率测度的方法。出于计算原因，切片Wasserstein（SW）距离作为Wasserstein距离的一种替代方法被引入，并且已经被用于训练生成式神经网络（NNs）。虽然在这样的设置中实际观察到了随机梯度下降（SGD）的收敛性，但据我们所知，对于这一观察没有理论保证。借鉴Bianchi等人（2022）关于SGD在非光滑和非凸函数上收敛性的最新工作，我们旨在填补这一知识空白，并提供一个具有实际意义的上下文，使得SW损失对NN参数的固定步长SGD轨迹收敛到（次）梯度流方程的集合。更准确地说，我们证明了随着步长减小，这些轨迹逼近了梯度流方程的集合。",
    "tldr": "本论文研究了使用切片Wasserstein损失训练神经网络时，随机梯度下降算法的收敛性，并证明了在特定条件下，SGD轨迹逼近了梯度流方程的集合。",
    "en_tdlr": "This paper investigates the convergence of stochastic gradient descent (SGD) algorithm when training neural networks with sliced Wasserstein loss. It provides theoretical guarantees and shows that under specific conditions, the SGD trajectories approach the set of gradient flow equations."
}