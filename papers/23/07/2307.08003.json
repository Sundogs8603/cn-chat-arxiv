{
    "title": "SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])",
    "abstract": "The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, w",
    "link": "http://arxiv.org/abs/2307.08003",
    "context": "Title: SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])\nAbstract: The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, w",
    "path": "papers/23/07/2307.08003.json",
    "total_tokens": 900,
    "translated_title": "SHAMSUL: 利用本地可解释性方法进行同时热图分析以研究医学意义",
    "translated_abstract": "深度神经网络的可解释性已成为医疗和健康领域的一个热门议题。这种关注来源于对透明度、法律和伦理考虑以及这些深度神经网络在临床决策支持系统中生成的预测的医学意义的担忧。为了解决这个问题，我们的研究深入探讨了四种已建立的解释性方法: 局部可解释模型无关解释(LIME)、Shapley加性解释(SHAP)、梯度加权类别激活映射(Grad-CAM)和层内相关传播(LRP)。我们利用多标签多类胸部放射学数据集的迁移学习方法，旨在解释与特定病理类别相关的预测。我们的分析涵盖了单标签和多标签预测，通过定量和定性研究提供了全面和公正的评估，",
    "tldr": "本研究利用四种解释性方法探索深度神经网络在医学领域的可解释性，通过热图分析解释神经网络的预测结果，并针对特定病理类别进行定量和定性研究。"
}