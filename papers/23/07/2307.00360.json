{
    "title": "BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer. (arXiv:2307.00360v2 [cs.CL] UPDATED)",
    "abstract": "BatGPT is a large-scale language model designed and trained jointly by Wuhan University and Shanghai Jiao Tong University. It is capable of generating highly natural and fluent text in response to various types of input, including text prompts, images, and audio. In the modeling level, we employ a bidirectional autoregressive architecture that allows the model to efficiently capture the complex dependencies of natural language, making it highly effective in tasks such as language generation, dialog systems, and question answering. Moreover, the bidirectional autoregressive modeling not only operates from left to right but also from right to left, effectively reducing fixed memory effects and alleviating model hallucinations.  In the training aspect, we propose a novel parameter expansion method for leveraging the pre-training of smaller models and employ reinforcement learning from both AI and human feedback, aimed at improving the model's alignment performance. Overall, these approach",
    "link": "http://arxiv.org/abs/2307.00360",
    "context": "Title: BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer. (arXiv:2307.00360v2 [cs.CL] UPDATED)\nAbstract: BatGPT is a large-scale language model designed and trained jointly by Wuhan University and Shanghai Jiao Tong University. It is capable of generating highly natural and fluent text in response to various types of input, including text prompts, images, and audio. In the modeling level, we employ a bidirectional autoregressive architecture that allows the model to efficiently capture the complex dependencies of natural language, making it highly effective in tasks such as language generation, dialog systems, and question answering. Moreover, the bidirectional autoregressive modeling not only operates from left to right but also from right to left, effectively reducing fixed memory effects and alleviating model hallucinations.  In the training aspect, we propose a novel parameter expansion method for leveraging the pre-training of smaller models and employ reinforcement learning from both AI and human feedback, aimed at improving the model's alignment performance. Overall, these approach",
    "path": "papers/23/07/2307.00360.json",
    "total_tokens": 939,
    "translated_title": "BatGPT: 从生成性预训练转换器中得到的双向自回归对话生成模型",
    "translated_abstract": "BatGPT是由武汉大学和上海交通大学共同设计和训练的大规模语言模型。它可以对各种类型的输入（包括文本提示、图像和音频）生成自然流畅的文本。在建模层面上，我们采用了双向自回归架构，使模型能够高效地捕捉自然语言的复杂依赖关系，从而在语言生成、对话系统和问答等任务中具有较高的效果。此外，双向自回归模型不仅从左到右运行，还从右到左运行，有效减少了固定记忆效应并缓解了模型产生虚假输出的问题。在训练方面，我们提出了一种新颖的参数扩展方法，用于利用较小模型的预训练，并采用了来自人工智能和人类反馈的强化学习，旨在改善模型的对齐性能。总体而言，这些方法在对话生成方向上取得了显著的贡献。",
    "tldr": "BatGPT是一个双向自回归对话生成模型，能够高效捕捉自然语言的复杂依赖关系，并通过双向建模和参数扩展方法来改善对话生成的效果。这是一个在语言生成领域取得重要贡献的模型。"
}