{
    "title": "Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])",
    "abstract": "We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo",
    "link": "http://arxiv.org/abs/2307.08875",
    "context": "Title: Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])\nAbstract: We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo",
    "path": "papers/23/07/2307.08875.json",
    "total_tokens": 972,
    "translated_title": "自然演员-评论家算法用于带有函数逼近的鲁棒强化学习",
    "translated_abstract": "本文研究了鲁棒强化学习，在确定一个对训练模拟器和测试环境之间的模型不匹配具有良好性能的策略的目标下。以前基于策略的鲁棒强化学习算法主要关注不确定性集合下的表格设置，该集合便于鲁棒策略评估，但在状态数量增加时变得不可行。为此，我们提出了两种新的不确定性集合形式，一种基于双重抽样，另一种基于积分概率度量。两者都使得即使只能访问模拟器，也能处理大规模的鲁棒强化学习。我们提出了一个鲁棒的自然演员-评论家算法（RNAC），它结合了新的不确定性集合，并使用函数逼近。我们提供了对于这个RNAC算法在有限时间内收敛到最佳鲁棒策略的收敛性保证，考虑函数逼近误差。最后，我们展示了通过我们的方法学习到的策略的鲁棒性能。",
    "tldr": "本文研究了鲁棒强化学习问题，提出了两种新的不确定性集合形式，使得大规模鲁棒强化学习变得可行。同时，提出了一个鲁棒的自然演员-评论家算法，通过函数逼近，该算法能够在有限时间内收敛到最佳鲁棒策略。",
    "en_tdlr": "The paper proposes two novel uncertainty set formulations to address the scalability issue of robust reinforcement learning. It also introduces a robust natural actor-critic algorithm that incorporates these new uncertainty sets and employs function approximation, achieving finite-time convergence to the optimal robust policy."
}