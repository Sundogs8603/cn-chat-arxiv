{
    "title": "Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])",
    "abstract": "Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-or",
    "link": "http://arxiv.org/abs/2307.11108",
    "context": "Title: Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])\nAbstract: Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-or",
    "path": "papers/23/07/2307.11108.json",
    "total_tokens": 930,
    "translated_title": "针对领域推广的平坦度感知最小化算法",
    "translated_abstract": "领域推广旨在学习在未知分布偏移下能够良好泛化的鲁棒模型。作为领域推广的关键方面之一，优化器的选择尚未深入研究。目前，大部分领域推广方法都遵循广泛使用的基准测试 DomainBed，并在所有数据集上使用Adam作为默认优化器。然而，我们揭示出Adam并不一定是当前大多数领域推广方法和数据集的最佳选择。基于损失函数平坦度的视角，我们提出了一种新颖的方法——针对领域推广的平坦度感知最小化算法（FAD），可以高效地同时优化零阶和一阶平坦度。我们对FAD的离分布泛化误差和收敛性进行了理论分析。实验结果表明，FAD在各种领域推广数据集上具有明显的优越性。此外，我们证实FAD能够发现更平坦的最优解，相对于其他零阶和一阶方法。",
    "tldr": "本文提出了一种平坦度感知最小化算法（FAD），用于针对领域推广问题。通过同时优化零阶和一阶平坦度，FAD在各种领域推广数据集上表现出卓越的性能，并能发现更平坦的最优解。"
}