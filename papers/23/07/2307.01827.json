{
    "title": "Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])",
    "abstract": "Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training",
    "link": "http://arxiv.org/abs/2307.01827",
    "context": "Title: Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])\nAbstract: Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training",
    "path": "papers/23/07/2307.01827.json",
    "total_tokens": 928,
    "translated_title": "解构数据重建：多类别、权重衰减和通用损失",
    "translated_abstract": "训练数据的记忆是一个活跃的研究领域，然而我们对神经网络内部运作的理解还处于初级阶段。最近，Haim等人提出了一种方案，可以从多层感知器二元分类器中重建训练样本，有效地证明了这样的网络参数中编码了大部分训练样本。在本研究中，我们在多个方向上扩展了他们的发现，包括从多类别和卷积神经网络中进行重建。我们推导出了一种更通用的重建方案，可适用于更广泛的损失函数，如回归损失。此外，我们研究了影响网络易受此类重建方案影响的各种因素。有趣的是，我们观察到在训练过程中使用权重衰减会增加重建能力，无论是在数量还是质量方面。此外，我们还检验了神经元数量相对于训练数量的影响。",
    "tldr": "这项研究探讨了神经网络内部对训练数据的记忆过程，并在多个方向上扩展了已有的研究。研究发现，使用权重衰减可以增加重建能力，同时还分析了神经元数量对网络易受重建方案影响的影响。"
}