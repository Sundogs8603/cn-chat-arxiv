{
    "title": "Performance of $\\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])",
    "abstract": "Despite widespread adoption in practice, guarantees for the LASSO and Group LASSO are strikingly lacking in settings beyond statistical problems, and these algorithms are usually considered to be a heuristic in the context of sparse convex optimization on deterministic inputs. We give the first recovery guarantees for the Group LASSO for sparse convex optimization with vector-valued features. We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function $l$, then the minimizer is a sparse vector supported on vector-valued features with the largest $\\ell_2$ norm of the gradient. Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function $l$ with restricted strong convexity and smoothness via weak submodularity arguments. This answers open questions of Tibshirani et al. and Yasuda et al. Our result is the first to theoretically expla",
    "link": "http://arxiv.org/abs/2307.07405",
    "context": "Title: Performance of $\\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])\nAbstract: Despite widespread adoption in practice, guarantees for the LASSO and Group LASSO are strikingly lacking in settings beyond statistical problems, and these algorithms are usually considered to be a heuristic in the context of sparse convex optimization on deterministic inputs. We give the first recovery guarantees for the Group LASSO for sparse convex optimization with vector-valued features. We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function $l$, then the minimizer is a sparse vector supported on vector-valued features with the largest $\\ell_2$ norm of the gradient. Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function $l$ with restricted strong convexity and smoothness via weak submodularity arguments. This answers open questions of Tibshirani et al. and Yasuda et al. Our result is the first to theoretically expla",
    "path": "papers/23/07/2307.07405.json",
    "total_tokens": 916,
    "translated_title": "$\\ell_1$正则化在稀疏凸优化中的性能",
    "translated_abstract": "虽然LASSO和Group LASSO在实践中被广泛采用, 但是对于除了统计问题以外的其他情况, 这些算法的保证令人震惊地缺乏, 并且在确定性输入的稀疏凸优化背景下通常被认为是一种启发式算法。我们为具有向量值特征的稀疏凸优化的Group LASSO给出了第一个恢复保证。我们证明了，如果在最小化严格凸函数$l$时应用足够大的Group LASSO正则化，那么极小化器是在具有最大梯度的$\\ell_2$范数的向量值特征上支持的稀疏向量。因此，重复此过程选择与正交匹配追踪算法相同的特征集，通过弱次模性证明了对于具有受限强凸性和光滑性的任何函数$l$都具有恢复保证。这回答了Tibshirani等人和Yasuda等人的开放问题。我们的结果首次在理论上解释了",
    "tldr": "本论文研究了稀疏凸优化中$\\ell_1$正则化的性能，给出了Group LASSO的恢复保证，并且发现了Group LASSO选择相同特征集的机制。"
}