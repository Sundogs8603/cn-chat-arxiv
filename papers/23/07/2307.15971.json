{
    "title": "You Can Backdoor Personalized Federated Learning. (arXiv:2307.15971v1 [cs.CR])",
    "abstract": "Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \\citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting t",
    "link": "http://arxiv.org/abs/2307.15971",
    "context": "Title: You Can Backdoor Personalized Federated Learning. (arXiv:2307.15971v1 [cs.CR])\nAbstract: Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \\citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting t",
    "path": "papers/23/07/2307.15971.json",
    "total_tokens": 777,
    "translated_title": "你可以通过后门攻击个性化联邦学习",
    "translated_abstract": "后门攻击对联邦学习系统的安全性构成重大威胁。然而，现有研究主要关注通用FL场景中的后门攻击和防御，即所有客户端合作训练一个全局模型。本文中，我们揭示了部分模型共享的个性化联邦学习方法仍然容易受到后门攻击的问题。我们提出了三种后门攻击方法：BapFL，BapFL+和Gen-BapFL，并经验证明它们可以有效攻击个性化联邦学习方法。",
    "tldr": "该论文研究了后门攻击对个性化联邦学习的影响，并揭示了部分模型共享的个性化联邦学习方法容易受到后门攻击。研究者提出了三种后门攻击方法，并验证了它们的有效性。"
}