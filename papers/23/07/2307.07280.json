{
    "title": "Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])",
    "abstract": "While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti",
    "link": "http://arxiv.org/abs/2307.07280",
    "context": "Title: Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])\nAbstract: While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti",
    "path": "papers/23/07/2307.07280.json",
    "total_tokens": 897,
    "translated_title": "回放以回忆：针对德语语音识别的持续层特定微调",
    "translated_abstract": "虽然自动语音识别（ASR）模型在引入无监督或自监督训练技术方面取得了显著进展，但这些改进仍然仅限于某些语言和说话者。迁移学习使得大规模多语言模型能够适应不仅是低资源语言，还包括更特定的说话者群体。然而，对新领域的数据进行微调通常会导致在原始领域的性能下降。因此，在我们的实验中，我们研究了大规模ASR模型在较小领域中的性能可以有多好，使用我们自己的德语高级语音命令数据集（SVC-de），以及在训练过程中通过选择性地冻结模型的部分来保留多少通用语音识别性能。为了进一步增加ASR模型对微调领域之外的词汇和说话者的鲁棒性，我们应用经验回放进行连续训练。",
    "tldr": "该论文探讨了如何通过持续层特定微调和经验回放技术来改善德语语音识别模型在较小领域中的性能，并提高模型的鲁棒性。",
    "en_tdlr": "This paper explores how to improve the performance of German speech recognition models in smaller domains through continual layer-specific fine-tuning and experience replay, while also increasing the robustness of the model to vocabulary and speakers outside of the fine-tuned domain."
}