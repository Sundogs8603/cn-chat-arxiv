{
    "title": "Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])",
    "abstract": "Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\\textit{memorize}$ \"hard\" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\\textit{",
    "link": "http://arxiv.org/abs/2307.09542",
    "context": "Title: Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])\nAbstract: Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\\textit{memorize}$ \"hard\" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\\textit{",
    "path": "papers/23/07/2307.09542.json",
    "total_tokens": 931,
    "translated_title": "神经网络的记忆化是否可以被局部化？",
    "translated_abstract": "最近的研究努力在解释深度超参数化网络中记忆化和概括之间的相互作用时，提出了神经网络在模型的最后几层中$\\textit{记忆化}$“困难”样本的能力。记忆化是指在训练集的$\\textit{非典型}$样本上能够正确预测的能力。在这项工作中，我们展示了记忆化现象并不局限于个别层，而是在模型的各个层中的一小组神经元中发生。首先，通过三种实验方面的收敛证据，我们发现大多数层对于样本的记忆化是冗余的，而对样本记忆化的贡献较大的层，并不一定是最后的层。这三个证据来源包括$\\textit{梯度追踪}$（测量梯度范数来自于记忆化和干净样本的贡献），$\\textit{层重置}$（将训练过程中特定模型权重替换为先前的训练检查点），以及$\\textit{...}$",
    "tldr": "本文研究了深度神经网络中记忆化的局部化现象，通过实验证据表明，记忆化并不局限于个别层，而是在模型的多个层中的一小部分神经元中发生。",
    "en_tdlr": "The paper investigates the localization of memorization in deep neural networks and provides experimental evidence that memorization occurs not only in individual layers but also in a small subset of neurons across different layers of the model."
}