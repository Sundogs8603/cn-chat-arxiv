{
    "title": "Efficient Learning of Discrete-Continuous Computation Graphs. (arXiv:2307.14193v1 [cs.LG])",
    "abstract": "Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Seco",
    "link": "http://arxiv.org/abs/2307.14193",
    "context": "Title: Efficient Learning of Discrete-Continuous Computation Graphs. (arXiv:2307.14193v1 [cs.LG])\nAbstract: Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Seco",
    "path": "papers/23/07/2307.14193.json",
    "total_tokens": 822,
    "translated_title": "高效学习离散-连续计算图的方法",
    "translated_abstract": "监督学习和强化学习中，离散和连续模型组合应用广泛。端到端可学习的离散-连续模型具有组合性、更好的泛化性能和更可解释。现有工作主要关注计算图中每个执行路径上只有一个离散组件的情况。我们分析具有多个连续离散组件的复杂随机计算图的行为。我们发现，由于梯度较小和局部最小值问题，优化这些模型的参数非常具有挑战性。然后，我们提出了两种克服这些挑战的新策略。首先，我们发现在训练过程中增加Gumbel噪声扰动的尺度参数可以改善学习行为。",
    "tldr": "本文研究了具有多个离散组件的随机计算图的行为，并提出了两种优化策略:增加Gumbel噪声扰动的尺度参数和使用多个离散组件。这些策略有效地提高了模型的学习能力。",
    "en_tdlr": "This paper analyzes the behavior of stochastic computation graphs with multiple discrete components and proposes two optimization strategies: increasing the scale parameter of Gumbel noise perturbations and using multiple discrete components. These strategies effectively improve the learning capability of the models."
}