{
    "title": "Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])",
    "abstract": "The future of machine learning lies in moving data collection along with training to the edge. Federated Learning, for short FL, has been recently proposed to achieve this goal. The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model. The latter is subsequently redistributed to clients for further training. A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity. In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates. Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective.",
    "link": "http://arxiv.org/abs/2307.06283",
    "context": "Title: Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])\nAbstract: The future of machine learning lies in moving data collection along with training to the edge. Federated Learning, for short FL, has been recently proposed to achieve this goal. The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model. The latter is subsequently redistributed to clients for further training. A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity. In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates. Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective.",
    "path": "papers/23/07/2307.06283.json",
    "total_tokens": 809,
    "translated_title": "处理FL中的计算异质性：一些理论见解",
    "translated_abstract": "机器学习的未来在于将数据收集与训练移至边缘。最近提出了联邦学习（FL）以实现这一目标。该方法的原则是聚合在大量分布式客户端上学习的模型，即从其环境中收集数据的资源受限移动设备，以获得新的更通用的模型。后者随后重新分发给客户端进行进一步的训练。联邦学习与数据中心的分布式训练之间的关键特征是内在的异质性。在这项工作中，我们引入并分析了一种新的聚合框架，以便形式化和解决联邦优化中的计算异质性，包括异构数据和本地更新。所提出的聚合算法从理论和实验角度进行了广泛分析。",
    "tldr": "本论文介绍并分析了一种新的聚合框架，用于处理联邦学习中的计算异质性。该框架涉及到处理异构数据和本地更新，通过理论和实验分析进行了广泛验证。",
    "en_tdlr": "This paper introduces and analyzes a new aggregation framework for handling computational heterogeneity in federated learning. The framework involves dealing with heterogeneous data and local updates, and has been extensively validated through theoretical and experimental analysis."
}