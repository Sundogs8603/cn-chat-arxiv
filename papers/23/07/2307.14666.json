{
    "title": "Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])",
    "abstract": "This paper addresses the classification of Arabic text data in the field of Natural Language Processing (NLP), with a particular focus on Natural Language Inference (NLI) and Contradiction Detection (CD). Arabic is considered a resource-poor language, meaning that there are few data sets available, which leads to limited availability of NLP methods. To overcome this limitation, we create a dedicated data set from publicly available resources. Subsequently, transformer-based machine learning models are being trained and evaluated. We find that a language-specific model (AraBERT) performs competitively with state-of-the-art multilingual approaches, when we apply linguistically informed pre-training methods such as Named Entity Recognition (NER). To our knowledge, this is the first large-scale evaluation for this task in Arabic, as well as the first application of multi-task pre-training in this context.",
    "link": "http://arxiv.org/abs/2307.14666",
    "context": "Title: Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])\nAbstract: This paper addresses the classification of Arabic text data in the field of Natural Language Processing (NLP), with a particular focus on Natural Language Inference (NLI) and Contradiction Detection (CD). Arabic is considered a resource-poor language, meaning that there are few data sets available, which leads to limited availability of NLP methods. To overcome this limitation, we create a dedicated data set from publicly available resources. Subsequently, transformer-based machine learning models are being trained and evaluated. We find that a language-specific model (AraBERT) performs competitively with state-of-the-art multilingual approaches, when we apply linguistically informed pre-training methods such as Named Entity Recognition (NER). To our knowledge, this is the first large-scale evaluation for this task in Arabic, as well as the first application of multi-task pre-training in this context.",
    "path": "papers/23/07/2307.14666.json",
    "total_tokens": 855,
    "translated_title": "改进基于Transformer模型和语言学预训练的阿拉伯语自然语言推理",
    "translated_abstract": "本文针对自然语言处理领域中阿拉伯文本数据的分类问题进行研究，特别关注自然语言推理和矛盾检测。阿拉伯语被认为是资源匮乏的语言，意味着数据集稀缺，导致NLP方法有限。为了克服这个限制，我们从公开可用资源中创建了专门的数据集。随后，我们训练和评估了基于Transformer的机器学习模型。我们发现，当应用命名实体识别等语言学知识预训练方法时，一种特定于语言的模型（AraBERT）与最先进的多语言方法具有竞争力。据我们所知，这是阿拉伯语领域中这一任务的首次大规模评估，也是多任务预训练在此环境中的首次应用。",
    "tldr": "本文针对阿拉伯语自然语言处理中的自然语言推理问题进行研究，通过构建专门的数据集和应用语言学知识预训练方法，我们的语言特定模型在与最先进的多语言方法竞争时表现出色。",
    "en_tdlr": "This paper explores natural language inference in Arabic by creating a dedicated dataset and using linguistically informed pre-training methods. The language-specific model outperforms state-of-the-art multilingual approaches in this task."
}