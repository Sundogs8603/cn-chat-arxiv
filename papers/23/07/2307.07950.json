{
    "title": "Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])",
    "abstract": "In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \\texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \\texttt{SelSync} to improve convergence in the context of \\textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14",
    "link": "http://arxiv.org/abs/2307.07950",
    "context": "Title: Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])\nAbstract: In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \\texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \\texttt{SelSync} to improve convergence in the context of \\textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14",
    "path": "papers/23/07/2307.07950.json",
    "total_tokens": 920,
    "translated_title": "通过选择性同步加速分布式机器学习训练",
    "translated_abstract": "在分布式训练中，深度神经网络（DNN）同时在多个工作者上启动，并使用批量同步并行（BSP）训练中的每个步骤聚合其本地更新。然而，由于聚合的通信成本较高，BSP无法线性扩展。为了减轻这种开销，FedAvg和SSP等替代方案要么降低同步频率，要么完全消除同步，通常以降低最终准确性为代价。在本文中，我们提出了一种名为SelSync的实用、低开销的DNN训练方法，它根据每一步的重要性动态选择是否进行通信。作为\\texttt{SelSync}的一部分，我们提出了各种优化方法，以提高在\\textit {半同步}训练环境中的收敛性。我们的系统在减少训练时间最多14％的同时，达到与BSP相同或更高的准确性。",
    "tldr": "本文介绍了一种名为SelSync的方法，通过选择性同步，在保持准确性的前提下减少了分布式深度神经网络训练的时间开销。该方法根据每一步的重要性动态选择是否进行通信，达到了与批量同步并行（BSP）相同或更高的准确性。"
}