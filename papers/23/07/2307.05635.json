{
    "title": "Fundamental limits of overparametrized shallow neural networks for supervised learning. (arXiv:2307.05635v1 [cs.LG])",
    "abstract": "We carry out an information-theoretical analysis of a two-layer neural network trained from input-output pairs generated by a teacher network with matching architecture, in overparametrized regimes. Our results come in the form of bounds relating i) the mutual information between training data and network weights, or ii) the Bayes-optimal generalization error, to the same quantities but for a simpler (generalized) linear model for which explicit expressions are rigorously known. Our bounds, which are expressed in terms of the number of training samples, input dimension and number of hidden units, thus yield fundamental performance limits for any neural network (and actually any learning procedure) trained from limited data generated according to our two-layer teacher neural network model. The proof relies on rigorous tools from spin glasses and is guided by ``Gaussian equivalence principles'' lying at the core of numerous recent analyses of neural networks. With respect to the existing",
    "link": "http://arxiv.org/abs/2307.05635",
    "context": "Title: Fundamental limits of overparametrized shallow neural networks for supervised learning. (arXiv:2307.05635v1 [cs.LG])\nAbstract: We carry out an information-theoretical analysis of a two-layer neural network trained from input-output pairs generated by a teacher network with matching architecture, in overparametrized regimes. Our results come in the form of bounds relating i) the mutual information between training data and network weights, or ii) the Bayes-optimal generalization error, to the same quantities but for a simpler (generalized) linear model for which explicit expressions are rigorously known. Our bounds, which are expressed in terms of the number of training samples, input dimension and number of hidden units, thus yield fundamental performance limits for any neural network (and actually any learning procedure) trained from limited data generated according to our two-layer teacher neural network model. The proof relies on rigorous tools from spin glasses and is guided by ``Gaussian equivalence principles'' lying at the core of numerous recent analyses of neural networks. With respect to the existing",
    "path": "papers/23/07/2307.05635.json",
    "total_tokens": 877,
    "translated_title": "表面层神经网络在监督学习中的超参数化基本限制",
    "translated_abstract": "我们对一个两层神经网络在超参数化情况下，从与其结构类似的教师网络生成的输入-输出对进行信息论分析。我们的结果以界限的形式给出，将i)训练数据和网络权重之间的互信息，或ii)贝叶斯最优泛化误差与对应的简单（广义）线性模型之间的相同量联系起来，而这个线性模型的显式表达式已经得到严格确定。我们的界限以训练样本数量、输入维度和隐藏单元数量为参数，从而为任何神经网络（实际上是任何学习过程）在有限数据下通过我们的两层教师神经网络模型进行训练提供了基本性能限制。证明依赖于自旋玻璃的严格工具，并由处于众多最新神经网络分析核心的“高斯等效原理”指导。",
    "tldr": "本文通过信息论分析，研究了超参数化情况下两层神经网络在监督学习中的基本限制。研究结果通过界限将训练数据的互信息或贝叶斯最优泛化误差与简单线性模型联系起来，并提供了神经网络训练的基本性能限制。证明方法利用严格的自旋玻璃工具和“高斯等效原理”。"
}