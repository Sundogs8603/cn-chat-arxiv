{
    "title": "Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])",
    "abstract": "We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\\% of the tokens via random",
    "link": "http://arxiv.org/abs/2307.15593",
    "context": "Title: Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])\nAbstract: We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\\% of the tokens via random",
    "path": "papers/23/07/2307.15593.json",
    "total_tokens": 1054,
    "translated_title": "语言模型的鲁棒无畸变水印方法",
    "translated_abstract": "我们提出了一种在自回归语言模型中添加水印的方法，并且这些水印对扰动具有鲁棒性，而不会改变文本的分布，同时保证生成预算在一定范围内。我们用随机水印密钥计算的随机数序列映射到语言模型的样本来生成带水印的文本。要检测水印文本，只要知道密钥的任何一方都可以将文本与随机数序列对齐。我们使用两种采样方案来实例化水印方法：反变换采样和指数最小采样。我们将这些水印应用于三个语言模型——OPT-1.3B、LLaMA-7B和Alpaca-7B，以实验证明它们的统计功效和对各种改写攻击的鲁棒性。值得注意的是，对于OPT-1.3B和LLaMA-7B模型，即使在随机扰动了40-50%的词元后，我们仍然可以可靠地检测到带水印的文本（$p \\leq 0.01$），只需要35个词元。",
    "tldr": "该论文提出了一种在语言模型中添加鲁棒无畸变水印的方法，通过映射随机数序列到语言模型的样本，可以实现在不改变文本分布的前提下对水印文本进行检测，并且在多种改写攻击下依然保持较高的鲁棒性，实验证明在40-50%的随机扰动下仍可可靠地检测到水印文本。",
    "en_tdlr": "This paper proposes a method for adding robust distortion-free watermarks to language models. By mapping a sequence of random numbers to a sample from the language model, the watermarked text can be detected without changing the distribution over text, and it remains robust even under various paraphrasing attacks. Experimental results show that the watermarked text can be reliably detected even after corrupting 40-50% of the tokens."
}