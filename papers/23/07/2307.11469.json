{
    "title": "Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])",
    "abstract": "Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions\" (KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classif",
    "link": "http://arxiv.org/abs/2307.11469",
    "context": "Title: Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])\nAbstract: Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions\" (KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classif",
    "path": "papers/23/07/2307.11469.json",
    "total_tokens": 858,
    "translated_title": "使用网络收集的图像进行知识蒸馏中的分布偏移问题",
    "translated_abstract": "知识蒸馏旨在从预先训练好的教师网络学习一个轻量级的学生网络。在实践中，现有的知识蒸馏方法通常在原始训练数据由于隐私问题和数据管理考虑不可用时变得不可行。因此，无数据知识蒸馏方法通过从互联网收集训练样本。然而，其中大部分忽视了原始训练数据和网络收集数据之间的常见分布偏移，影响了训练好的学生网络的可靠性。为了解决这个问题，我们提出了一种名为“不同分布下的知识蒸馏”（KD$^{3}$）的新方法，它由三个组成部分组成。具体而言，我们首先根据教师网络和学生网络的综合预测动态选择有用的训练样本。随后，我们对加权特征和分类进行对齐。",
    "tldr": "本研究提出了一种解决知识蒸馏中分布偏移问题的新方法，通过动态选择有用的训练样本并对特征和分类进行对齐，提高了训练出的学生网络的可靠性。"
}