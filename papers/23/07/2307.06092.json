{
    "title": "Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])",
    "abstract": "We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\\gamma}$ for $\\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.",
    "link": "http://arxiv.org/abs/2307.06092",
    "context": "Title: Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])\nAbstract: We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\\gamma}$ for $\\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.",
    "path": "papers/23/07/2307.06092.json",
    "total_tokens": 865,
    "translated_title": "深度神经网络中的定量中心极限定理",
    "translated_abstract": "我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\\gamma}$ 缩放，其中 $\\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。",
    "tldr": "本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。",
    "en_tdlr": "This paper studies the distribution of fully connected neural networks with random Gaussian weights and biases. It provides quantitative bounds on normal approximations, showing that the distance between a random fully connected network and the corresponding infinite width Gaussian process scales like $n^{-\\gamma}$, with stronger bounds in terms of network width dependence than previous literature."
}