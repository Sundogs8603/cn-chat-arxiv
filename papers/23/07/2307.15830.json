{
    "title": "A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])",
    "abstract": "Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time",
    "link": "http://arxiv.org/abs/2307.15830",
    "context": "Title: A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])\nAbstract: Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time",
    "path": "papers/23/07/2307.15830.json",
    "total_tokens": 993,
    "translated_title": "基于距离相关性的方法来刻画循环神经网络在时间序列预测中的有效性",
    "translated_abstract": "时间序列预测受到了广泛关注，循环神经网络(RNNs)作为处理序列数据的一种常用模型之一，具有很强的能力。然而，之前关于RNNs在时间序列预测中的研究结果不一致，并且对于不同数据集的性能差异缺乏深入洞察。本文提出了一种通过距离相关性这一多功能指标来将时间序列的特征与RNNs的组成部分联系起来的方法。该指标允许我们通过RNN激活层的信息流来解释和说明其性能。我们实证表明，RNN的激活层能够很好地学习时间序列的滞后结构。然而，在连续的几层中，它们逐渐丧失了这些信息，从而使具有大滞后结构的序列的预测质量变差。我们还显示，激活层不能充分建模移动平均和异方差时间。",
    "tldr": "本文通过距离相关性的方法来研究循环神经网络对于时间序列预测的有效性，发现激活层能够学习时间序列的滞后结构，但是在连续的几层中逐渐丧失这些信息，导致预测质量变差，同时激活层也不能很好地建模移动平均和异方差时间。",
    "en_tdlr": "This paper presents a distance correlation-based approach to characterize the effectiveness of recurrent neural networks (RNNs) for time series forecasting. The study shows that the activation layers of RNNs can learn the lag structures of time series well, but gradually lose this information over consecutive layers, resulting in degraded forecast quality for series with large lag structures. Furthermore, the activation layers cannot adequately model moving average and heteroskedastic time."
}