{
    "title": "All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])",
    "abstract": "The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi",
    "link": "http://arxiv.org/abs/2307.00290",
    "context": "Title: All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])\nAbstract: The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi",
    "path": "papers/23/07/2307.00290.json",
    "total_tokens": 763,
    "translated_title": "全能SAM：从弱注释到基于提示的微观细胞核分割",
    "translated_abstract": "目前，Segment Anything Model (SAM)是一种使用提示的通用零样本分割模型。然而，该流程在推理阶段仍然需要手动提示，对于生物医学图像分割仍然资源密集。本文介绍了一种称为全能SAM的流程，它在整个AI开发工作流程中使用了SAM，并且在推理阶段无需手动提示。具体而言，SAM首先利用弱提示（例如点、边界框）生成像素级注释，然后使用像素级注释对SAM分割模型进行微调。实验结果表明了两个关键发现：1）所提出的pi",
    "tldr": "本论文介绍了一种称为全能SAM的流程，通过在整个AI开发工作流程中使用SAM，并且在推理阶段无需手动提示，实现了从弱注释到基于提示的像素级细胞核分割的目标。",
    "en_tdlr": "This paper introduces an all-in-SAM pipeline that enables pixel-wise nuclei segmentation from weak annotation to prompt-based segmentation without requiring manual prompts during inference."
}