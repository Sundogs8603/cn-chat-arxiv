{
    "title": "Instruction-following Evaluation through Verbalizer Manipulation. (arXiv:2307.10558v1 [cs.CL])",
    "abstract": "While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately ",
    "link": "http://arxiv.org/abs/2307.10558",
    "context": "Title: Instruction-following Evaluation through Verbalizer Manipulation. (arXiv:2307.10558v1 [cs.CL])\nAbstract: While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately ",
    "path": "papers/23/07/2307.10558.json",
    "total_tokens": 882,
    "translated_title": "通过口述方式操作进行指示遵循评估",
    "translated_abstract": "虽然调整指令模型在各种自然语言处理任务中取得了显著的成功，但准确评估其遵循指令的能力仍然具有挑战性。现有的基准主要关注与模型在训练过程中学习的内容相吻合的常见指令。然而，对这些指令的回应能力并不一定意味着强大的遵循指令能力。在本文中，我们提出了一种新颖的指示遵循评估协议，称为口述者操作。它要求模型用与模型先验知识不同程度吻合的单词口述任务标签，从高度吻合（例如，对于积极情绪输出“积极”）到最少吻合（例如，对于积极情绪输出“消极”）。口述者操作可以与任何分类基准无缝集成，以检查模型对先验知识的依赖程度以及覆盖它们的能力，从而准确地进行指示遵循。",
    "tldr": "本文提出了一种新的指示遵循评估协议，口述者操作，通过口述任务标签来检查模型对先验知识的依赖程度，以及覆盖它们的能力，从而准确地进行指示遵循。",
    "en_tdlr": "This paper proposes a novel instruction-following evaluation protocol called verbalizer manipulation, which examines the model's reliance on prior knowledge and its ability to override them to accurately follow instructions by verbalizing task labels."
}