{
    "title": "AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)",
    "abstract": "The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha",
    "link": "http://arxiv.org/abs/2307.01146",
    "context": "Title: AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)\nAbstract: The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha",
    "path": "papers/23/07/2307.01146.json",
    "total_tokens": 915,
    "translated_title": "AVSegFormer: 基于Transformer的音视频分割",
    "translated_abstract": "音频与视觉的结合长期以来一直是多模态领域的一个研究课题。最近，引入了一项新的音频-视觉分割（AVS）任务，旨在定位和分割给定视频中的有声对象。这个任务首次要求在像素级别对音频驱动的场景进行理解，存在着重大挑战。在本文中，我们提出了AVSegFormer，这是一种利用Transformer架构进行AVS任务的新框架。具体来说，我们在变压器解码器中引入了音频查询和可学习查询，使网络能够有选择地关注感兴趣的视觉特征。此外，我们还设计了一个音频-视觉混合器，通过增强相关的空间通道和抑制无关的空间通道来动态调整视觉特征。此外，我们设计了一个中间掩模损失，以增强解码器的监督，鼓励网络产生更准确的中间预测。大量实验证明了我们方法的有效性。",
    "tldr": "AVSegFormer是一种基于Transformer的音视频分割框架，通过引入音频查询和可学习查询来选择性地关注视觉特征，还使用音频-视觉混合器动态调整视觉特征，并通过中间掩模损失增强解码器的监督。实验证明该方法的有效性。"
}