{
    "title": "MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)",
    "abstract": "Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that MedCPT sets new state-of-the-art performance on six biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical article and sentence ",
    "link": "http://arxiv.org/abs/2307.00589",
    "context": "Title: MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)\nAbstract: Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that MedCPT sets new state-of-the-art performance on six biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical article and sentence ",
    "path": "papers/23/07/2307.00589.json",
    "total_tokens": 1011,
    "translated_title": "MedCPT: 使用大规模PubMed搜索日志的对比预训练转换器进行零样本生物医学信息检索",
    "translated_abstract": "信息检索在生物医学知识获取和临床决策支持中至关重要。尽管最近的进展表明语言模型编码器在语义检索方面表现更好，但训练这些模型需要大量的查询-文章注释，在生物医学领域很难获得。因此，大多数生物医学信息检索系统只进行词汇匹配。为此，我们引入了MedCPT，这是一种首创的用于生物医学领域零样本语义信息检索的对比预训练转换器模型。为了训练MedCPT，我们从PubMed收集了255 million个用户点击日志，这是前所未有的规模。利用这些数据，我们使用对比学习来训练一对密切集成的检索器和重排器。实验结果显示，MedCPT在六个生物医学信息检索任务中取得了新的最佳性能，优于包括更大模型（如GPT-3大小的cpt-text-XL）在内的各种基线模型。此外，MedCPT还能够生成更好的生物医学文章和句子。",
    "tldr": "MedCPT是一种用于生物医学领域零样本语义信息检索的对比预训练转换器模型。通过使用大规模PubMed搜索日志进行训练，MedCPT在六个生物医学信息检索任务中创造了新的最佳性能，超过了其他基线模型，同时还能生成更好的生物医学文章和句子。",
    "en_tdlr": "MedCPT is a contrastive pre-trained transformer model for zero-shot semantic information retrieval in biomedicine. Trained on a large-scale PubMed search log, MedCPT achieves state-of-the-art performance on six biomedical retrieval tasks, outperforming other baseline models, and generates better biomedical articles and sentences."
}