{
    "title": "Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])",
    "abstract": "Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previo",
    "link": "http://arxiv.org/abs/2307.09742",
    "context": "Title: Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])\nAbstract: Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previo",
    "path": "papers/23/07/2307.09742.json",
    "total_tokens": 839,
    "translated_title": "提升数据集压缩的分布匹配方法",
    "translated_abstract": "数据集压缩旨在将大型数据集压缩成较小的数据集，同时保持其训练模型的能力，减少深度学习应用中的存储成本和训练工作量。然而，传统的数据集压缩方法是以优化为导向的，通过在模型优化过程中执行梯度或参数匹配来压缩数据集，即使在小型数据集和模型上也需要大量的计算资源。本文提出了一种基于分布匹配的新型数据集压缩方法，它更加高效和有前景。具体来说，我们针对朴素分布匹配的两个重要缺点（即不平衡的特征数量和无效的嵌入距离计算）提出了三种新技术（即分区和扩展增强、高效和丰富的模型采样和类别感知的分布正则化）。我们的简单而有效的方法优于大多数先前的方法。",
    "tldr": "本文提出了一种基于分布匹配的新型数据集压缩方法，通过解决朴素分布匹配的两个缺点，提出了三种新技术，实现了更高效和有前景的数据集压缩。",
    "en_tdlr": "This paper proposes a novel dataset condensation method based on distribution matching, which addresses two shortcomings of naive distribution matching and introduces three new techniques, leading to a more efficient and promising data compression method."
}