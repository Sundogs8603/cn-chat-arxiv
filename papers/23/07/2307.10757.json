{
    "title": "Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition. (arXiv:2307.10757v1 [cs.SD])",
    "abstract": "This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper emplo",
    "link": "http://arxiv.org/abs/2307.10757",
    "context": "Title: Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition. (arXiv:2307.10757v1 [cs.SD])\nAbstract: This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper emplo",
    "path": "papers/23/07/2307.10757.json",
    "total_tokens": 849,
    "translated_title": "Vesper：一种用于语音情感识别的紧凑高效预训练模型",
    "translated_abstract": "本文提出了一种将通用大规模预训练模型（PTMs）适应语音情感识别任务的范式。尽管PTMs为人工智能提供了新的思路，但它们是为通用任务构建的，因此在特定任务上的效果仍有提升空间。此外，由于PTMs体积较大，在实际应用中使用可能面临挑战。针对以上限制，本文提出了另一个研究方向，即优化大规模PTMs以生成紧凑且高效的任务特定PTMs。本文聚焦于语音情感识别任务，提出了一种改进的情感特定预训练编码器Vesper。Vesper在基于WavLM的语音数据集上进行预训练，并考虑了情感特征。为了增强对情感信息的敏感性，Vesper采用情感引导的掩蔽策略来识别需要掩蔽的区域。",
    "tldr": "本文提出了一种用于语音情感识别的紧凑高效预训练模型Vesper，通过优化大规模预训练模型生成任务特定模型，考虑情感特征并采用情感引导的掩蔽策略来增强对情感信息的敏感性。"
}