{
    "title": "An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])",
    "abstract": "Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat",
    "link": "http://arxiv.org/abs/2307.07674",
    "context": "Title: An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])\nAbstract: Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat",
    "path": "papers/23/07/2307.07674.json",
    "total_tokens": 918,
    "translated_title": "在GFlowNets中使用回放缓冲区对模式发现的有效性的实证研究",
    "translated_abstract": "强化学习（RL）算法旨在通过迭代采样动作从而学习如何最大化总期望回报$R（x）$来学习最优策略。GFlowNets是一类特殊的算法，通过学习一个近似于$R（x）$的概率采样策略，从离散集合中生成多样的候选样本$x$。与传统的RL算法相比，GFlowNets表现出更好的模式发现能力，对于药物发现和组合搜索等应用非常有用。然而，由于GFlowNets是一个相对较新的算法类别，许多在RL中有用的技术尚未与其关联起来。本文研究了在GFlowNets中利用回放缓冲区。我们通过实证研究了各种回放缓冲区采样技术的影响，评估了模式发现速度和发现的模式质量。在Hypergrid模拟环境和分子合成环境中的实验结果证明了我们的观察结论。",
    "tldr": "本文通过实证研究，探讨了在GFlowNets中使用回放缓冲区的有效性，评估了不同回放缓冲区采样技术对模式发现速度和质量的影响。",
    "en_tdlr": "This paper empirically investigates the effectiveness of using a replay buffer in GFlowNets and assesses the impact of different replay buffer sampling techniques on the speed and quality of mode discovery."
}