{
    "title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])",
    "abstract": "Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu",
    "link": "http://arxiv.org/abs/2307.01381",
    "context": "Title: Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])\nAbstract: Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu",
    "path": "papers/23/07/2307.01381.json",
    "total_tokens": 868,
    "translated_title": "隐式内存变换器用于计算高效的同时语音翻译",
    "translated_abstract": "同时语音翻译是一项困难的人类交流任务，即在进行语音输入的同时生成翻译。对于这样的流式任务，使用块处理将输入序列分割成片段的Transformer在降低成本的同时实现了最先进的性能。当前的方法允许信息在片段之间传播，包括左上下文和存储器库，但它们既是不充分的表示又是不必要的计算开销。在本文中，我们提出了一种隐式内存变换器，通过一种新的左上下文方法隐式保留记忆，从而消除了需要用存储器库显式表示记忆的需求。我们通过前一个片段的注意力输出生成左上下文，并将其包含在当前片段的注意力计算的键和值中。对MuST-C数据集的实验证明，隐式内存变换器提供了显著的速度提升和少量的性能损失。",
    "tldr": "本文提出了一种隐式内存变换器，通过新的左上下文方法隐式保留记忆，从而实现了计算高效的同时语音翻译。在MuST-C数据集上的实验表明，该方法提供了显著的速度提升和少量的性能损失。"
}