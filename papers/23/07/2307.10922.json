{
    "title": "Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)",
    "abstract": "Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.",
    "link": "http://arxiv.org/abs/2307.10922",
    "context": "Title: Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)\nAbstract: Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.",
    "path": "papers/23/07/2307.10922.json",
    "total_tokens": 865,
    "translated_title": "基于语言的动作概念空间改进视频自监督学习",
    "translated_abstract": "最近的对比语言图像预训练方法已经实现了学习可传递和鲁棒的图像表示。然而，如何在少量监督的情况下将这些模型应用于视频领域仍然是一个未解决的问题。我们探索了朝着这个方向的简单步骤，使用与语言相关的自我监督学习，将图像CLIP模型调整为视频领域。我们修改了适用于时间建模的骨干网络，通过在动作概念空间中使用训练目标进行自蒸馏训练。使用相关文本提示从语言编码器提取的各个动作概念的特征向量构成了这个空间。我们提出了两个训练目标，概念蒸馏和概念对齐，既保留了原始表示的广泛性，又强化了动作和其属性之间的关系。我们的方法改进了三个动作识别基准上的零样本和线性推测性能。",
    "tldr": "这项研究使用语言相关的自监督学习方法，将图像CLIP模型调整为适用于视频领域，并通过在动作概念空间中进行自蒸馏训练，提高了零样本和线性推测性能。",
    "en_tdlr": "This research adapts an image CLIP model to the video domain using language tied self-supervised learning, improving zero-shot and linear probing performance by training in an action concept space."
}