{
    "title": "Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])",
    "abstract": "We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\\epsilon}$,${\\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further ",
    "link": "http://arxiv.org/abs/2307.11280",
    "context": "Title: Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])\nAbstract: We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\\epsilon}$,${\\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further ",
    "path": "papers/23/07/2307.11280.json",
    "total_tokens": 931,
    "translated_title": "Epsilon*: 机器学习模型的隐私度量",
    "translated_abstract": "我们引入了Epsilon*，一种新的隐私度量方法，用于在隐私减轻策略部署之前、期间或之后，测量单个模型实例的隐私风险。该度量不需要访问训练数据采样或模型训练算法。Epsilon*是一个关于真阳性和假阳性率的函数，用于敌手在成员推断攻击中使用的假设检验中。我们区分了量化经过训练的模型实例的隐私损失和量化产生该模型实例的训练机制的隐私损失。现有的隐私审计文献中的方法为后者提供了下界，而我们的度量方法通过依赖于训练模型实例的隐私的（ε，δ）型量化，为前者提供了下界。我们建立了这些下界之间的关系，并展示了如何实现Epsilon*以避免数值和噪声放大不稳定性。",
    "tldr": "Epsilon*是一种用于测量机器学习模型隐私风险的新度量方法，不需要访问训练数据或模型训练算法，能与成员推断攻击中的假设检验相结合，提供对经过训练的模型实例隐私损失的下界，避免数值和噪声放大不稳定性。",
    "en_tdlr": "Epsilon* is a new privacy metric for measuring the privacy risk of machine learning models, which does not require access to training data or the model training algorithm. It can be combined with hypothesis testing in membership inference attacks to provide a lower bound on the privacy loss of trained model instances and avoids numerical and noise amplification instability."
}