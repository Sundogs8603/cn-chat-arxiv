{
    "title": "Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])",
    "abstract": "Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \\ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous",
    "link": "http://arxiv.org/abs/2307.13944",
    "context": "Title: Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])\nAbstract: Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \\ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous",
    "path": "papers/23/07/2307.13944.json",
    "total_tokens": 848,
    "translated_title": "图对比学习的熵神经估计",
    "translated_abstract": "图对比学习旨在提取节点的可区分的高层表示。本文理论上证明了数据集的熵可以通过最大化不同视图下的互信息下界来近似估计，即通过神经网络估计熵。基于这一发现，我们提出了一种简单而有效的子集抽样策略，用于对比数据集各视图之间的成对表示。具体而言，我们随机从给定的图中抽样节点和边来构建视图的输入子集。两个视图被输入到参数共享的连体网络中，以提取高维嵌入并估计整个图的信息熵。对于学习过程，我们提出了同时使用两个目标优化网络的方法。具体而言，对比损失函数的输入由正负对组成。我们的对比对策略与以前的不同",
    "tldr": "本文提出了一种基于熵神经估计的图对比学习方法，通过最大化互信息下界来近似估计数据集的熵。通过简单但有效的子集抽样策略对比数据集不同视图中的节点表示，同时使用两个目标优化网络的学习过程。",
    "en_tdlr": "This paper proposes an entropy neural estimation for graph contrastive learning, approximating the entropy of a dataset by maximizing the lower bound of mutual information. It introduces a subset sampling strategy to contrast pairwise representations between views of a dataset, optimizing the network using two objectives simultaneously."
}