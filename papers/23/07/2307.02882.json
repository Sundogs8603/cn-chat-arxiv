{
    "title": "Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])",
    "abstract": "In this study, we analyze data-scarce classification scenarios, where available labeled legal data is small and imbalanced, potentially hurting the quality of the results. We focused on two finetuning objectives; SetFit (Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla finetuning setup on a legal provision classification task. Additionally, we compare the features that are extracted with LIME (Local Interpretable Model-agnostic Explanations) to see which particular features contributed to the model's classification decisions. The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results. Thus a model finetuned with a contrastive objective seems to base its decisions more confidently on legally informa",
    "link": "http://arxiv.org/abs/2307.02882",
    "context": "Title: Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])\nAbstract: In this study, we analyze data-scarce classification scenarios, where available labeled legal data is small and imbalanced, potentially hurting the quality of the results. We focused on two finetuning objectives; SetFit (Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla finetuning setup on a legal provision classification task. Additionally, we compare the features that are extracted with LIME (Local Interpretable Model-agnostic Explanations) to see which particular features contributed to the model's classification decisions. The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results. Thus a model finetuned with a contrastive objective seems to base its decisions more confidently on legally informa",
    "path": "papers/23/07/2307.02882.json",
    "total_tokens": 914,
    "translated_title": "对比就是你所需的一切",
    "translated_abstract": "在这项研究中，我们分析了数据稀缺的分类场景，其中可用的标记法律数据很少且不平衡，可能会影响结果的质量。我们重点关注了两个微调目标：SetFit（句子转换器微调），一种对比学习设置，以及在法律条款分类任务上的普通微调设置。此外，我们使用LIME（局部可解释模型无关解释）比较了提取的特征，以查看哪些特定特征对模型的分类决策有贡献。结果显示，与使用相同数量的训练样本的普通微调相比，使用SetFit的对比设置表现更好。LIME的结果显示，对比学习方法有助于提升对正面和负面特征的认知，这些特征在法律上具有信息量，并对分类结果有贡献。因此，使用对比目标进行微调的模型似乎更自信地基于法律信息进行决策。",
    "tldr": "对比学习方法在数据稀缺的法律分类场景中表现更好，使用SetFit微调的模型比普通微调使用更少的训练样本。LIME的结果显示，对比学习方法有助于提升对正面和负面特征的认知，这些特征在法律上具有信息量，并对分类结果有贡献。",
    "en_tdlr": "Contrastive learning method performs better in data-scarce legal classification scenarios, where a model finetuned with SetFit requires fewer training samples compared to vanilla finetuning. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results."
}