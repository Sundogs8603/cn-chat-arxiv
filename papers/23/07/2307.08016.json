{
    "title": "Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making. (arXiv:2307.08016v1 [cs.CV])",
    "abstract": "Vision language decision making (VLDM) is a challenging multimodal task. The agent have to understand complex human instructions and complete compositional tasks involving environment navigation and object manipulation. However, the long action sequences involved in VLDM make the task difficult to learn. From an environment perspective, we find that task episodes can be divided into fine-grained \\textit{units}, each containing a navigation phase and an interaction phase. Since the environment within a unit stays unchanged, we propose a novel hybrid-training framework that enables active exploration in the environment and reduces the exposure bias. Such framework leverages the unit-grained configurations and is model-agnostic. Specifically, we design a Unit-Transformer (UT) with an intrinsic recurrent state that maintains a unit-scale cross-modal memory. Through extensive experiments on the TEACH benchmark, we demonstrate that our proposed framework outperforms existing state-of-the-art",
    "link": "http://arxiv.org/abs/2307.08016",
    "context": "Title: Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making. (arXiv:2307.08016v1 [cs.CV])\nAbstract: Vision language decision making (VLDM) is a challenging multimodal task. The agent have to understand complex human instructions and complete compositional tasks involving environment navigation and object manipulation. However, the long action sequences involved in VLDM make the task difficult to learn. From an environment perspective, we find that task episodes can be divided into fine-grained \\textit{units}, each containing a navigation phase and an interaction phase. Since the environment within a unit stays unchanged, we propose a novel hybrid-training framework that enables active exploration in the environment and reduces the exposure bias. Such framework leverages the unit-grained configurations and is model-agnostic. Specifically, we design a Unit-Transformer (UT) with an intrinsic recurrent state that maintains a unit-scale cross-modal memory. Through extensive experiments on the TEACH benchmark, we demonstrate that our proposed framework outperforms existing state-of-the-art",
    "path": "papers/23/07/2307.08016.json",
    "total_tokens": 961,
    "translated_title": "将任务分解：视觉和语言决策的单元级混合训练框架",
    "translated_abstract": "视觉语言决策（VLDM）是一项具有挑战性的多模态任务。代理需要理解复杂的人类指令，并完成涉及环境导航和物体操纵的组合任务。然而，VLDM中涉及的长期行动序列使任务难以学习。从环境的角度来看，我们发现任务可以被细分为精细的“单元”，每个单元包含一个导航阶段和一个交互阶段。由于单位内的环境保持不变，我们提出了一种新颖的混合训练框架，使得在环境中进行主动探索，并减少了曝光偏差。这样的框架利用了单元级配置，是模型无关的。具体而言，我们设计了一个带有内在循环状态的单元转换器（UT），该状态维护着一个单元级的跨模态内存。通过在TEACH基准测试上进行广泛实验，我们证明了我们提出的框架优于现有的最先进技术",
    "tldr": "该论文提出了一个单元级混合训练框架来解决视觉和语言决策任务。通过将任务分解为精细的单元，并利用单位内环境的不变性，使得训练更加有效，减少了曝光偏差，并在 TEACH基准测试上超越了现有的最先进技术。",
    "en_tdlr": "This paper proposes a unit-grained hybrid training framework for vision and language decision making. By dividing the task into fine-grained units and leveraging the environment's invariance within each unit, this framework reduces exposure bias and achieves superior performance compared to existing state-of-the-art methods on the TEACH benchmark."
}