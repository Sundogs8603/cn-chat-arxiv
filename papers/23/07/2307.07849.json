{
    "title": "Variational Inference with Gaussian Score Matching. (arXiv:2307.07849v1 [stat.ML])",
    "abstract": "Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, wh",
    "link": "http://arxiv.org/abs/2307.07849",
    "context": "Title: Variational Inference with Gaussian Score Matching. (arXiv:2307.07849v1 [stat.ML])\nAbstract: Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, wh",
    "path": "papers/23/07/2307.07849.json",
    "total_tokens": 869,
    "translated_title": "用高斯评分匹配进行变分推理",
    "translated_abstract": "变分推理（VI）是一种逼近贝叶斯统计中的计算困难后验分布的方法。通常，VI通过最小化适当的目标函数（例如证据下界ELBO）将简单的参数分布拟合到目标后验分布中。在本文中，我们提出了一种基于评分匹配原理的新型VI方法，即如果两个分布相等，则它们的评分函数（即对数密度的梯度）在其支持集的每个点上都相等。基于这一原理，我们开发了评分匹配VI，这是一个迭代算法，旨在匹配变分近似与精确后验之间的评分。在每次迭代中，评分匹配VI解决了一个内部优化问题，即最小调整当前变分估计，使其与新抽取的潜变量值处的评分匹配。我们证明，当变分分布是高斯分布时，这个内部优化问题有一个闭式解。",
    "tldr": "本文提出了一种用高斯评分匹配的方法来进行变分推理，通过迭代算法将变分近似与精确后验的评分匹配。当变分分布是高斯分布时，内部优化问题有闭式解。",
    "en_tdlr": "This paper presents a method of variational inference using Gaussian score matching, which iteratively matches the scores between the variational approximation and the exact posterior. When the variational family is Gaussian, the inner optimization problem has a closed-form solution."
}