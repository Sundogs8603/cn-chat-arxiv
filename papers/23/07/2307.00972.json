{
    "title": "MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)",
    "abstract": "Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively. The superior results highlight the immens",
    "link": "http://arxiv.org/abs/2307.00972",
    "context": "Title: MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)\nAbstract: Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively. The superior results highlight the immens",
    "path": "papers/23/07/2307.00972.json",
    "total_tokens": 976,
    "translated_title": "MoVie: 基于视觉模型的策略自适应用于视图泛化",
    "translated_abstract": "在有限的视图上训练的视觉强化学习（RL）智能体在将其学到的能力推广到未见过的视图时面临着巨大的挑战。这个固有的困难被称为$\\textit{view generalization}$问题。在这项工作中，我们将这个基本问题系统地分为四个不同的，高度具有挑战性的情景，这些情景与实际情况很相似。随后，我们提出了一个简单而有效的方法，在测试时使基于视觉的$\\textbf{Mo}$del-based策略能够成功适应$\\textbf{Vie}$w generalization ($\\textbf{MoVie}$)，而无需显式的奖励信号和训练过程中的任何修改。我们的方法在来自DMControl、xArm和Adroit的总共$\\textbf{18}$个任务中的所有四个情景中展示了显著进展，相对改进分别为$\\mathbf{33}$%，$\\mathbf{86}$%和$\\mathbf{152}$%。优越的结果凸显出其巨大的潜力。",
    "tldr": "本论文提出了一种名为MoVie的方法，通过基于视觉模型的策略自适应实现了视图泛化。该方法在不需要显式奖励信号和训练过程修改的情况下，在多个实际场景中表现出卓越的性能，相对改进达到了33%至152%。",
    "en_tdlr": "This paper presents MoVie, an approach that achieves view generalization by visual model-based policy adaptation. The method demonstrates superior performance in multiple real-world scenarios without the need for explicit reward signals and modifications during training, with relative improvements ranging from 33% to 152%."
}