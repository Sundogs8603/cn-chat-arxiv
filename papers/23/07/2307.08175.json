{
    "title": "Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models. (arXiv:2307.08175v1 [cs.LG])",
    "abstract": "We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonic",
    "link": "http://arxiv.org/abs/2307.08175",
    "context": "Title: Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models. (arXiv:2307.08175v1 [cs.LG])\nAbstract: We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonic",
    "path": "papers/23/07/2307.08175.json",
    "total_tokens": 862,
    "translated_title": "在表格化监督式机器学习模型中多目标优化性能和可解释性",
    "translated_abstract": "我们提出了一个模型无关的框架，用于同时优化表格数据的监督式机器学习模型的预测性能和可解释性。可解释性通过三个指标进行量化：特征稀疏性、特征交互稀疏性和非单调特征影响的稀疏性。通过将机器学习算法的超参数优化视为多目标优化问题，我们的框架允许在单次优化运行中生成高性能和易解释性权衡的多样模型。通过将特征选择、交互和单调性约束集成到超参数搜索空间中，实现了高效的优化。我们证明了优化问题有效地转化为找到被允许在模型中交互的选定特征组的 Pareto 最优集，并找到它们的最佳单调性。",
    "tldr": "这个研究提出了一个模型无关的框架，用于优化表格数据的监督式机器学习模型的性能和可解释性。通过将机器学习算法的超参数优化视为多目标优化问题，该框架生成了同时具有高性能和易解释性权衡的多样模型。",
    "en_tdlr": "This paper presents a model-agnostic framework for optimizing the performance and interpretability of supervised machine learning models for tabular data. By treating the hyperparameter optimization as a multi-objective problem, the framework generates diverse models that balance high performance and ease of interpretation."
}