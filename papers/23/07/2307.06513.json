{
    "title": "Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])",
    "abstract": "Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts",
    "link": "http://arxiv.org/abs/2307.06513",
    "context": "Title: Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])\nAbstract: Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts",
    "path": "papers/23/07/2307.06513.json",
    "total_tokens": 968,
    "translated_title": "利用上下文反事实推理实现信念校准",
    "translated_abstract": "通过调整数据采集原则或正则化训练过程中的损失函数等方式，人的信念和价值越来越多地被融入到我们的AI系统中。然而，元对齐问题是人类信念的多样性以及跨群体的不对齐性，而且即使在人类之间，每个信念的隐含强度也可能不好校准，特别是在尝试跨上下文进行推广时。具体而言，在高后悔情况下，我们观察到上下文反事实和补救成本对于更新决策者的信念以及所持信念的强度至关重要。因此，我们认为在对齐过程中引入反事实推理是准确校准信念的关键。为此，我们首先将信念的多样性分为两类：主观性（同一群体内的个体间）和认识不确定性（同一人在不同环境中）",
    "tldr": "本文讨论了通过引入上下文反事实推理来准确校准AI系统中的信念的问题。研究发现，在高后悔情况下，上下文反事实和补救成本对于更新决策者的信念以及所持信念的强度至关重要。通过将信念的多样性分成两类:主观性和认识不确定性，可以更好地理解和处理信念的校准问题。"
}