{
    "title": "CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking",
    "abstract": "Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi",
    "link": "https://arxiv.org/abs/2307.16847",
    "context": "Title: CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking\nAbstract: Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi",
    "path": "papers/23/07/2307.16847.json",
    "total_tokens": 842,
    "translated_title": "CroSSL: 跨模态的自监督学习在时间序列上的应用通过隐藏掩码",
    "translated_abstract": "机器学习多模态时间序列的标注数据的有限可用性严重阻碍了该领域的进展。自监督学习（SSL）是一种无需依赖标签学习数据表示的有前景的方法。然而，现有的SSL方法需要计算昂贵的负样本对，并且通常仅适用于单模态，从而限制了它们的多功能性。我们引入了CroSSL（跨模态自监督学习），它提出了两个创新概念：通过模态特定编码器产生的中间嵌入的隐藏掩码，以及通过跨模态聚合器将其聚合为全局嵌入，可以提供给下游分类器。CroSSL允许处理缺失模态和端到端的跨模态学习，无需进行预处理以处理缺失输入或进行对比学习的负样本采样。我们对各种数据进行了评估，包括加速度计或陀螺仪等运动传感器和生物传感器。",
    "tldr": "CroSSL是一种跨模态的自监督学习方法，通过隐藏掩码和跨模态聚合器实现对时间序列的学习，无需负样本对和数据预处理。",
    "en_tdlr": "CroSSL is a cross-modal self-supervised learning method for time-series that utilizes masking and cross-modal aggregation without the need for negative pairs or data preprocessing."
}