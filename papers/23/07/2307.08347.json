{
    "title": "M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)",
    "abstract": "Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\\%. Notably, M-FLAG achieves outstanding performance o",
    "link": "http://arxiv.org/abs/2307.08347",
    "context": "Title: M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)\nAbstract: Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\\%. Notably, M-FLAG achieves outstanding performance o",
    "path": "papers/23/07/2307.08347.json",
    "total_tokens": 896,
    "translated_title": "M-FLAG：使用冻结语言模型和潜空间几何优化的医学视觉语言预训练",
    "translated_abstract": "医学视觉语言模型可以实现医学影像和临床文本的特征共学习和集成。然而，这些模型训练起来并不容易，并且潜空间表示可以非常复杂。本文提出了一种新颖的医学视觉语言模型预训练和正则化方法。该方法命名为医学视觉语言预训练与冻结语言模型和潜空间几何优化（M-FLAG），利用冻结语言模型来稳定和高效地进行训练，并引入了一种新颖的正交损失函数来协调潜空间几何关系。我们在三个下游任务上展示了预训练模型的潜力：医学影像分类、分割和目标检测。通过对五个公开数据集进行广泛的实验证明，M-FLAG在减少78％的参数的同时，明显优于现有的医学视觉语言预训练方法。值得注意的是，M-FLAG表现出了出色的性能。",
    "tldr": "M-FLAG是一种新颖的医学视觉语言预训练方法，通过利用冻结语言模型和引入正交损失函数来优化潜空间几何关系。在医学影像分类、分割和目标检测任务上，M-FLAG在性能上显著优于现有方法，并且减少了78%的参数量。"
}