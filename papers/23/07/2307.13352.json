{
    "title": "High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])",
    "abstract": "Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar",
    "link": "http://arxiv.org/abs/2307.13352",
    "context": "Title: High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])\nAbstract: Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar",
    "path": "papers/23/07/2307.13352.json",
    "total_tokens": 889,
    "translated_title": "高维分布式梯度下降算法在任意数量拜占庭攻击者下的研究",
    "translated_abstract": "近年来，具有拜占庭故障的强鲁棒分布式学习引起了广泛关注。然而，现有方法大多受到维度诅咒的限制，随着现代机器学习模型复杂性的增加，这个问题变得越来越严重。在本文中，我们设计了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法。我们的设计核心是一种直接的高维半验证均值估计方法。我们的想法是首先识别一个子空间，通过工作机上传的梯度向量估计与该子空间垂直的均值分量，而通过辅助数据集估计该子空间内的均值分量。然后，我们将我们的新方法用作分布式学习问题的聚合器。我们的理论分析表明，新方法具有极小极值统计率。特别地，对维度的依赖性得到了显著改善。",
    "tldr": "本文提出了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法，核心是一种直接的高维半验证均值估计方法，具有极小极值统计率。"
}