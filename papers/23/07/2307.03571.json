{
    "title": "Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])",
    "abstract": "This paper introduces a smooth method for (structured) sparsity in $\\ell_q$ and $\\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\\ell_q$ or $\\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m",
    "link": "http://arxiv.org/abs/2307.03571",
    "context": "Title: Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])\nAbstract: This paper introduces a smooth method for (structured) sparsity in $\\ell_q$ and $\\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\\ell_q$ or $\\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m",
    "path": "papers/23/07/2307.03571.json",
    "total_tokens": 933,
    "translated_title": "平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架",
    "translated_abstract": "本文介绍了一种用于（结构化）稀疏正则化问题中的$\\ell_q$和$\\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\\ell_q$或$\\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。",
    "tldr": "本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。",
    "en_tdlr": "This paper presents a general framework for smooth optimization in sparse regularization, which is compatible with prevalent first-order optimization methods and is able to find matching global minima and equivalent local minima."
}