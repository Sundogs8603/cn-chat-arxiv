{
    "title": "Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])",
    "abstract": "Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on ",
    "link": "http://arxiv.org/abs/2307.06721",
    "context": "Title: Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])\nAbstract: Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on ",
    "path": "papers/23/07/2307.06721.json",
    "total_tokens": 903,
    "translated_title": "为什么导向式对话策略学习表现优秀？理解对抗学习及其替代方法的作用。",
    "translated_abstract": "对话策略是根据每个对话轮次的当前状态确定系统动作的关键。近年来，强化学习 (RL) 已成为对话策略学习 (DPL) 的一种有前途的选择。在基于 RL 的 DPL 中，根据奖励更新对话策略。对于具有大量状态动作对组合的多领域任务导向型对话场景，精细构建像状态-动作相关的奖励来有效指导对话策略是具有挑战性的。一种从收集到的数据中估计奖励的方式是使用对抗学习 (AL) 同时训练奖励估计器和对话策略。尽管这种方法在实验中表现出优越的性能，但其固有的对抗学习问题（例如模式坍缩）也十分棘手。本文通过对对话策略和奖励估计器的目标函数进行详细分析，首先确定了 AL 在 DPL 中的作用。接下来，基于此，该论文提出了一种替代方法。",
    "tldr": "本论文通过分析对话策略和奖励估计器的目标函数，解释了对抗学习在对话策略学习中的作用，并提出了一种替代方法。"
}