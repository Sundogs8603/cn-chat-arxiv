{
    "title": "Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])",
    "abstract": "A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \\in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.  We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction ",
    "link": "http://arxiv.org/abs/2307.08999",
    "context": "Title: Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])\nAbstract: A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \\in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.  We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction ",
    "path": "papers/23/07/2307.08999.json",
    "total_tokens": 891,
    "translated_title": "Oracle高效的在线多校准和全面预测",
    "translated_abstract": "最近的一系列研究表明，多校准（multicalibration）这一多组公平性概念与全面预测（omniprediction）这一为大量损失函数提供同时损失最小化保证的学习范式之间存在意想不到的联系。先前的研究主要集中在批处理设置下的全面预测。我们首次在在线对抗设置下研究了全面预测。尽管已经存在用于在线对抗设置下获取多校准概念的算法，但与批处理算法不同，它们只适用于有限的基准函数类$F$，因为它们要求每一轮枚举每个函数$f \\in F$。相反，全面预测对于学习理论的假设类$F$最有趣，而这些类通常是连续大的。我们开发了一种新的在线多校准算法，可以适用于无限的基准函数类$F$，并且是Oracle高效的（即对于任何类$F$，算法都可以转化为高效的约简形式）。",
    "tldr": "本文研究了在线对抗背景下的全面预测问题，提出了一种新的在线多校准算法，可以适用于无限的基准函数类，并且是Oracle高效的。"
}