{
    "title": "Deception Abilities Emerged in Large Language Models",
    "abstract": "Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown",
    "link": "https://rss.arxiv.org/abs/2307.16513",
    "context": "Title: Deception Abilities Emerged in Large Language Models\nAbstract: Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown",
    "path": "papers/23/07/2307.16513.json",
    "total_tokens": 838,
    "translated_title": "大规模语言模型中出现的欺骗能力",
    "translated_abstract": "大规模语言模型（LLM）目前处于将人工智能系统与人类交流和日常生活紧密结合的前沿。因此，将它们与人类价值观保持一致非常重要。然而，由于推理能力的稳定增长，未来的LLM被怀疑能够欺骗人类操作员，并利用这种能力绕过监测工作。为此，LLM需要具备对欺骗策略的概念理解。本研究揭示了最先进的LLM（如GPT-4）中出现了这种策略，而在早期的LLM中并不存在。我们进行了一系列实验，表明最先进的LLM能够理解和诱导他人产生错误的信念，其在复杂的欺骗场景中表现可以通过链式思维推理得到增强，并且引发LLM中的马基雅维利主义可以改变其欺骗倾向。总之，揭示了迄今为止未知的欺骗能力。",
    "tldr": "大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。"
}