{
    "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])",
    "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and inst",
    "link": "http://arxiv.org/abs/2307.15818",
    "context": "Title: RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])\nAbstract: We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and inst",
    "path": "papers/23/07/2307.15818.json",
    "total_tokens": 1041,
    "translated_title": "RT-2：视觉-语言-行动模型将网络知识转化为机器人控制",
    "translated_abstract": "本文研究了如何将在互联网规模数据上训练的视觉-语言模型直接应用于端到端的机器人控制，以提升泛化能力并实现新兴的语义推理。我们的目标是让单一的端到端训练模型既能学会将机器人观测映射到行为，又能享受来自网络的语言和视觉-语言数据的益处。为此，我们提出在机器人轨迹数据和互联网规模的视觉-语言任务（如视觉问答）上共同微调最先进的视觉-语言模型。与其他方法不同，我们提出了一个简单的通用方法来实现这个目标：为了使自然语言回答和机器人行为都能以相同的格式进行处理，我们将行为表示为文本标记，并将它们直接纳入模型的训练集中，与自然语言标记相同。我们将这类模型称为视觉-语言-行动模型（VLA）。",
    "tldr": "本文研究了将互联网规模数据上训练的视觉-语言模型直接应用于机器人控制的方法，实现了泛化能力的提升和新兴的语义推理。通过在机器人轨迹数据和互联网规模的视觉-语言任务上共同微调最先进的视觉-语言模型，为单一的端到端训练模型提供了同时学习机器人观测到行为映射和利用语言和视觉-语言数据的益处的能力。",
    "en_tdlr": "This paper explores the application of vision-language models trained on Internet-scale data to robotic control, improving generalization and enabling emergent semantic reasoning. By co-fine-tuning state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, the paper achieves the capability for a single end-to-end trained model to learn to map robot observations to actions and leverage the benefits of language and vision-language data."
}