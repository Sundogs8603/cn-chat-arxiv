{
    "title": "Investigating the Edge of Stability Phenomenon in Reinforcement Learning. (arXiv:2307.04210v1 [cs.LG])",
    "abstract": "Recent progress has been made in understanding optimisation dynamics in neural networks trained with full-batch gradient descent with momentum with the uncovering of the edge of stability phenomenon in supervised learning. The edge of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches the divergence threshold of the underlying optimisation algorithm for a quadratic loss, after which it starts oscillating around the threshold, and the loss starts to exhibit local instability but decreases over long time frames. In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms across a variety of data regimes, from offline to online RL. Our experiments reveal that, despite significant differences to supervised learning, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can be present in off-policy deep RL. Unlike supervised learnin",
    "link": "http://arxiv.org/abs/2307.04210",
    "context": "Title: Investigating the Edge of Stability Phenomenon in Reinforcement Learning. (arXiv:2307.04210v1 [cs.LG])\nAbstract: Recent progress has been made in understanding optimisation dynamics in neural networks trained with full-batch gradient descent with momentum with the uncovering of the edge of stability phenomenon in supervised learning. The edge of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches the divergence threshold of the underlying optimisation algorithm for a quadratic loss, after which it starts oscillating around the threshold, and the loss starts to exhibit local instability but decreases over long time frames. In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms across a variety of data regimes, from offline to online RL. Our experiments reveal that, despite significant differences to supervised learning, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can be present in off-policy deep RL. Unlike supervised learnin",
    "path": "papers/23/07/2307.04210.json",
    "total_tokens": 928,
    "translated_title": "探究在强化学习中的稳定边界现象",
    "translated_abstract": "最近在理解全批量梯度下降优化动力学方面取得了进展，通过有动量的全批量梯度下降在监督学习中揭示了稳定边界现象。稳定边界现象发生在Hessian的主导特征值达到二次损失的发散阈值时，此后它开始在阈值周围振荡，并且损失开始表现出局部不稳定性但在较长时间内逐渐减小。本研究中，我们探索了稳定边界现象在强化学习中的体现，特别是在各种数据场景中的离线到在线强化学习的非政策Q学习算法。我们的实验发现，尽管与监督学习存在显著的差异，如数据分布的非稳定性和引导bootstrap的使用，稳定边界现象仍然可能在非政策深度强化学习中存在。与监督学习不同的是，监督学习删除了误差减小的稳态（误差减小阶段下美在稳定边界的情况）。",
    "tldr": "本研究探究了强化学习中的稳定边界现象，发现即使在非政策深度强化学习中，稳定边界现象仍然存在，这有助于我们更好地理解和优化深度强化学习算法。",
    "en_tdlr": "This paper investigates the edge of stability phenomenon in reinforcement learning and finds that it exists even in off-policy deep reinforcement learning algorithms, providing insights for understanding and optimizing such algorithms."
}