{
    "title": "Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?. (arXiv:2307.02732v1 [cs.LG])",
    "abstract": "Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how e",
    "link": "http://arxiv.org/abs/2307.02732",
    "context": "Title: Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?. (arXiv:2307.02732v1 [cs.LG])\nAbstract: Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how e",
    "path": "papers/23/07/2307.02732.json",
    "total_tokens": 899,
    "translated_title": "评估评估器：当前的少样本学习基准适用吗？",
    "translated_abstract": "在过去的十年中，提出了许多少样本学习的基准。然而，所有这些基准都集中在对许多任务平均性能的评估上，但在这种情况下如何可靠地评估和调整针对个别任务训练的模型的问题尚未解决。本文首次对任务级别的评估进行了研究，这在部署模型时是一个基本步骤。我们测量了少样本场景中性能估计器的准确性，考虑了模型选择的策略，并检查了通常被认为是鲁棒的评估器失败的原因。我们得出结论，用较少的折叠进行交叉验证是直接估计模型性能的最佳选择，而用自助法或大量折叠进行交叉验证更适合于模型选择的目的。总的来说，我们发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。",
    "tldr": "本文首次对任务级别的评估进行了研究，发现现有的少样本学习基准并不能以可靠的方式设计，无法获取关于如何评估和选择模型的可靠情况的完整画面。"
}