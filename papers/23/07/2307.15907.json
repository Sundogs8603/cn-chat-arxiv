{
    "title": "An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks. (arXiv:2307.15907v1 [cs.LG])",
    "abstract": "Deep neural networks, (DNNs, a.k.a. NNs), have been widely used in various tasks and have been proven to be successful. However, the accompanied expensive computing and storage costs make the deployments in resource-constrained devices a significant concern. To solve this issue, quantization has emerged as an effective way to reduce the costs of DNNs with little accuracy degradation by quantizing floating-point numbers to low-width fixed-point representations. Quantized neural networks (QNNs) have been developed, with binarized neural networks (BNNs) restricted to binary values as a special case. Another concern about neural networks is their vulnerability and lack of interpretability. Despite the active research on trustworthy of DNNs, few approaches have been proposed to QNNs. To this end, this paper presents an automata-theoretic approach to synthesizing BNNs that meet designated properties. More specifically, we define a temporal logic, called BLTL, as the specification language. W",
    "link": "http://arxiv.org/abs/2307.15907",
    "context": "Title: An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks. (arXiv:2307.15907v1 [cs.LG])\nAbstract: Deep neural networks, (DNNs, a.k.a. NNs), have been widely used in various tasks and have been proven to be successful. However, the accompanied expensive computing and storage costs make the deployments in resource-constrained devices a significant concern. To solve this issue, quantization has emerged as an effective way to reduce the costs of DNNs with little accuracy degradation by quantizing floating-point numbers to low-width fixed-point representations. Quantized neural networks (QNNs) have been developed, with binarized neural networks (BNNs) restricted to binary values as a special case. Another concern about neural networks is their vulnerability and lack of interpretability. Despite the active research on trustworthy of DNNs, few approaches have been proposed to QNNs. To this end, this paper presents an automata-theoretic approach to synthesizing BNNs that meet designated properties. More specifically, we define a temporal logic, called BLTL, as the specification language. W",
    "path": "papers/23/07/2307.15907.json",
    "total_tokens": 902,
    "translated_title": "用自动机理论方法合成二进制神经网络",
    "translated_abstract": "深度神经网络(DNNs，也称为NNs)在各种任务中被广泛使用，并且已被证明是成功的。然而，伴随而来的昂贵的计算和存储成本使得在资源受限设备上部署成为一个重要问题。为了解决这个问题，量化已经成为一种有效的方法，通过将浮点数量化为低宽度的定点表示来减少DNNs的成本，同时几乎不会降低准确性。发展了量化神经网络(QNNs)，而将二进制神经网络(BNNs)限制为二进制值作为一个特殊情况。神经网络的另一个关注点是它们的脆弱性和缺乏可解释性。尽管在DNN的可信性方面有许多活跃的研究，但对于QNNs的方法却很少提出。为此，本文提出了一个用自动机理论方法合成满足指定属性的BNNs的方法。更具体地说，我们定义了一种称为BLTL的时态逻辑作为规范语言。",
    "tldr": "这篇论文提出了一种用自动机理论方法合成满足指定属性的二进制神经网络的方法，并定义了一种时态逻辑作为规范语言。",
    "en_tdlr": "This paper presents an automata-theoretic approach for synthesizing binarized neural networks that meet designated properties, and defines a temporal logic as the specification language."
}