{
    "title": "Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])",
    "abstract": "Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,",
    "link": "http://arxiv.org/abs/2307.07171",
    "context": "Title: Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])\nAbstract: Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,",
    "path": "papers/23/07/2307.07171.json",
    "total_tokens": 911,
    "translated_title": "带有自我净化的大型语言模型的认证鲁棒性",
    "translated_abstract": "尽管大型语言模型（LLM）在广泛的实际应用中取得了巨大的成功，但它们对于噪声输入的脆弱性显著限制了它们的应用，尤其是在高风险环境中。在这些环境下，确保大型语言模型的每个预测都是稳定的非常重要，即在输入的微小差异情况下，LLM的预测应该是一致的。这主要涉及到认证鲁棒LLM的研究，即在输入周围的局部区域中，所有LLM的预测都得到认证是正确的。随机平滑已经展示了在认证LLM的鲁棒性和预测稳定性方面的巨大潜力。然而，随机平滑在进行模型预测之前需要对输入添加噪声，其认证性能在很大程度上取决于模型在受损数据上的表现。因此，它直接应用于LLM仍然具有挑战性，并且通常会导致很小的认证半径。为了解决这个问题，",
    "tldr": "传统的随机平滑方法对于大型语言模型的直接应用具有挑战性，为了解决认证半径很小的问题，提出了一种带有自我净化的新方法。"
}