{
    "title": "On Regularization and Inference with Label Constraints. (arXiv:2307.03886v1 [cs.LG])",
    "abstract": "Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.",
    "link": "http://arxiv.org/abs/2307.03886",
    "context": "Title: On Regularization and Inference with Label Constraints. (arXiv:2307.03886v1 [cs.LG])\nAbstract: Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.",
    "path": "papers/23/07/2307.03886.json",
    "total_tokens": 1012,
    "translated_title": "关于正则化和标签约束推理的研究",
    "translated_abstract": "先验知识和符号规则在机器学习中通常以标签约束的形式表达，特别是在结构预测问题中。本文通过量化其对模型性能的影响，比较了机器学习流程中两种常见的编码标签约束的策略：带约束的正则化和约束推理。对于正则化，我们展示了它通过排除与约束不一致的模型来缩小泛化差距的效果。然而，正则化对小违规的偏好导致了对次优模型的偏置。对于约束推理，我们展示了它通过纠正模型的违规行为来减小总体风险，并将违规行为转化为优势。鉴于这些差异，我们进一步探索了将这两种方法结合使用，并提出了约束推理来补偿正则化引入的偏置的条件，旨在提高模型复杂性和最优风险。",
    "tldr": "本文研究了在机器学习中将先验知识和符号规则以标签约束的形式表达的方法。通过比较正则化和约束推理两种常见的编码标签约束的策略，发现正则化缩小了泛化差距但引入了对次优模型的偏置，而约束推理通过纠正模型的违规行为将违规行为转化为优势。进一步探索了将这两种方法结合使用的可能，并提出了用约束推理来补偿正则化引入的偏置的条件，旨在提高模型复杂性和最优风险。",
    "en_tdlr": "This paper investigates the encoding of prior knowledge and symbolic rules in machine learning as label constraints. By comparing regularization with constraints and constrained inference, it is found that regularization reduces the generalization gap but introduces bias towards suboptimal models, while constrained inference turns violations into advantages by correcting model behavior. The study also explores combining these two approaches and proposes conditions for constrained inference to compensate for regularization bias, aiming to improve model complexity and optimal risk."
}