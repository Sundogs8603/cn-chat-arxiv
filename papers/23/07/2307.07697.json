{
    "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring ",
    "link": "http://arxiv.org/abs/2307.07697",
    "context": "Title: Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])\nAbstract: Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring ",
    "path": "papers/23/07/2307.07697.json",
    "total_tokens": 871,
    "translated_title": "Think-on-Graph: 利用知识图谱进行大型语言模型的深度和负责任的推理",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中取得了重大进展，但在需要知识追溯性、及时性和准确性至关重要的场景中，它们经常在复杂推理和表现方面遇到困难。为了解决这些限制，我们提出了Think-on-Graph（ToG），这是一个利用知识图谱增强LLMs深度和负责任推理能力的新框架。通过使用ToG，我们可以确定与给定问题相关的实体，并对外部知识数据库进行探索和推理，以检索相关三元组。这个迭代过程生成包含顺序连接的三元组的多个推理路径，直到收集到足够的信息来回答问题或达到最大深度为止。通过在复杂的多跳推理问答任务上的实验证明，ToG优于现有方法，有效地解决了LLMs的前述限制。",
    "tldr": "Think-on-Graph是一个利用知识图谱增强大型语言模型深度和负责任推理能力的新框架，在复杂的多跳推理问答任务上表现出色，解决了现有方法中存在的限制。"
}