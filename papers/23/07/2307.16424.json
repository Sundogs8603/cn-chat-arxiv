{
    "title": "MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning. (arXiv:2307.16424v2 [cs.LG] UPDATED)",
    "abstract": "Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the targe",
    "link": "http://arxiv.org/abs/2307.16424",
    "context": "Title: MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning. (arXiv:2307.16424v2 [cs.LG] UPDATED)\nAbstract: Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the targe",
    "path": "papers/23/07/2307.16424.json",
    "total_tokens": 882,
    "translated_title": "MetaDiff: 使用条件扩散进行元学习，以进行少样本学习",
    "translated_abstract": "在人工智能中，让深度模型具备少样本学习的能力，即从少量样本快速学习，是一个核心挑战。基于梯度的元学习方法通过学习如何学习新任务有效地解决了这个挑战。其关键思想是以双层优化的方式学习深度模型，其中外部循环过程学习共享的梯度下降算法（即其超参数），而内部循环过程利用它来通过仅使用少量有标签数据对任务特定的模型进行优化。尽管这些现有方法已经显示出卓越的性能，但外部循环过程需要沿着内部优化路径计算二阶导数，这会带来可观的内存负担和梯度消失的风险。受到扩散模型的最新进展的启发，我们发现内部循环的梯度下降过程实际上可以看作是一个扩散的反过程（即去噪），这个扩散的目标是恢复任务特定的模型参数。",
    "tldr": "使用条件扩散作为元学习的一种方法，能够解决深度模型在少样本学习中遇到的梯度计算和消失风险问题。",
    "en_tdlr": "MetaDiff proposes using conditional diffusion as a method for meta-learning, which addresses the challenges of gradient computation and vanishing gradients in few-shot learning."
}