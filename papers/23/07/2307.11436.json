{
    "title": "Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs. (arXiv:2307.11436v1 [math.OC])",
    "abstract": "The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations.",
    "link": "http://arxiv.org/abs/2307.11436",
    "context": "Title: Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs. (arXiv:2307.11436v1 [math.OC])\nAbstract: The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations.",
    "path": "papers/23/07/2307.11436.json",
    "total_tokens": 928,
    "translated_title": "延迟补偿超bolic PIDE控制的神经算子",
    "translated_abstract": "最近引入的DeepONet算子学习框架从基本的超bolic和拟bolic PDE结果扩展到了一个高级超bolic类，其中包括状态和系统输出或输入的延迟。PDE反向设计产生的增益函数是非线性算子的输出，将空间域上的函数映射到空间域上的函数，其中该增益生成算子的输入是PDE的系数。使用DeepONet神经网络逼近该算子，可以证明其任意精度紧密。一旦在无限维度上产生了这个逼近理论结果，我们就可以在使用近似增益的反馈下建立封闭环的稳定性。除了提供全状态反馈下的这些结果外，我们还发展了DeepONet逼近的观测者和输出反馈定律，并证明了它们在神经算子逼近下的稳定性质。",
    "tldr": "该论文提出了延迟补偿超bolic PIDE控制的神经算子框架，扩展了基本PDE控制结果到一个涉及状态和系统延迟的高级超bolic类。使用DeepONet逼近算子，可以在无限维度上建立稳定的闭环反馈，并开发了逼近的观测者和输出反馈定律。",
    "en_tdlr": "This paper presents a neural operator framework for delay-compensating control of hyperbolic PIDEs, extending the results for basic PDEs to a more advanced class with delays. The framework approximates the gain-generating operator with a DeepONet neural network, enabling stability in closed loop under feedback and developing approximated observers and output-feedback laws."
}