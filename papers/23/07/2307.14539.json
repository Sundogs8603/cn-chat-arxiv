{
    "title": "Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])",
    "abstract": "The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial ",
    "link": "http://arxiv.org/abs/2307.14539",
    "context": "Title: Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])\nAbstract: The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial ",
    "path": "papers/23/07/2307.14539.json",
    "total_tokens": 933,
    "translated_title": "插入并祈祷：利用多模型模型的现成组件进行攻击",
    "translated_abstract": "将额外的模态（如视觉）加入大型语言模型（LLM）的快速增长和日益受欢迎引起了严重的安全问题。这种模态的扩展类似于在房子上增加更多的门，无意中为对抗性攻击创建了多个访问点。在本文中，通过引入对抗性嵌入空间攻击，我们强调多模型系统中的漏洞，这些漏洞源于以插拔方式将现成组件（如公共预训练编码器）引入这些系统。与现有的工作相比，我们的方法不需要访问多模型系统的权重或参数，而是依赖于这些预训练编码器的庞大且未完全开发的嵌入空间。我们提出的嵌入空间攻击是通过寻找位于这些预训练组件的广泛嵌入空间的危险或目标区域的输入图像来进行的。",
    "tldr": "本文通过引入对抗性嵌入空间攻击，探讨了将现成组件插入多模型模型中的漏洞和风险，并提出了一种不需要多模型系统权重和参数的攻击方法，通过寻找位于预训练组件嵌入空间危险区域的输入图像进行攻击。",
    "en_tdlr": "This paper investigates the vulnerabilities and risks introduced by incorporating off-the-shelf components into multi-modal models, and proposes an adversarial embedding space attack method that does not require access to the weights and parameters of the multi-modal system. The attack is conducted by searching for input images in the dangerous regions of the embedding space of pre-trained components."
}