{
    "title": "Implicit Interpretation of Importance Weight Aware Updates. (arXiv:2307.11955v1 [cs.LG])",
    "abstract": "Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as",
    "link": "http://arxiv.org/abs/2307.11955",
    "context": "Title: Implicit Interpretation of Importance Weight Aware Updates. (arXiv:2307.11955v1 [cs.LG])\nAbstract: Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as",
    "path": "papers/23/07/2307.11955.json",
    "total_tokens": 853,
    "translated_title": "隐式解释重要性权重感知更新",
    "translated_abstract": "鉴于其速度和简单性，子梯度下降是凸优化机器学习算法中最常用的优化算法之一。然而，调整其学习率可能是实现一致良好性能的最严重瓶颈。减少对学习率的依赖的常见方法是使用隐式/近端更新。其中一种变体是重要性权重感知（IWA）更新，其由每个损失函数上无限多个无穷小更新组成。然而，IWA更新的经验成功并不能完全通过其理论来解释。在本文中，我们首次展示了IWA更新在在线学习设置中具有严格更好的遗憾上界，优于普通梯度更新。我们的分析基于新框架：广义隐式Follow-the-Regularized-Leader（FTRL）（Chen和Orabona, 2023），使用对偶表述来分析广义隐式更新。特别地，我们的结果暗示了IWA更新可以被视为",
    "tldr": "本文首次证明了在在线学习环境中，重要性权重感知（IWA）更新对于凸优化机器学习算法具有更好的遗憾上界，优于普通梯度更新。"
}