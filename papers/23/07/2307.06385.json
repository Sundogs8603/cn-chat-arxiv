{
    "title": "Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])",
    "abstract": "Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \\emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \\emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor",
    "link": "http://arxiv.org/abs/2307.06385",
    "context": "Title: Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])\nAbstract: Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \\emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \\emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor",
    "path": "papers/23/07/2307.06385.json",
    "total_tokens": 888,
    "translated_title": "弱监督条件下的音视频事件定位的时间标签优化",
    "translated_abstract": "音视频事件定位是指在视频中对同时可见和可听到的事件进行时间定位和分类的任务。本文解决了在弱监督条件下的音视频事件定位问题，训练过程中只有视频级别的事件标签（仅有事件是否出现，但没有时间位置信息）可用于监督。我们的思路是使用一个基础模型在训练数据上以更细的时间分辨率估计标签，并使用这些标签重新训练模型。具体来说，我们通过以下步骤确定训练视频中每个帧片段的标签子集: (i) 用另一个视频中与视频级别标签没有重叠的帧替换片段外的帧， (ii) 将这个合成视频输入基础模型，仅提取该片段的标签。为了处理合成视频的分布外特性，我们提出了一个辅助目标，用于引入更多多样化的标签。",
    "tldr": "本文解决了弱监督条件下的音视频事件定位问题，通过使用基础模型在训练数据上以更细的时间分辨率估计标签，并提出辅助目标来处理合成视频的分布外特性。",
    "en_tdlr": "This paper tackles the problem of weakly-supervised audio-visual event localization by using a base model to estimate labels at a finer temporal resolution in the training data, and proposing an auxiliary objective to handle the out-of-distribution nature of synthetic videos."
}