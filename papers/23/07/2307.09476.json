{
    "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])",
    "abstract": "Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some \"critical layer\", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces ",
    "link": "http://arxiv.org/abs/2307.09476",
    "context": "Title: Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])\nAbstract: Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some \"critical layer\", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces ",
    "path": "papers/23/07/2307.09476.json",
    "total_tokens": 924,
    "translated_title": "过度思考真相：理解语言模型如何处理虚假演示",
    "translated_abstract": "现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。",
    "tldr": "该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。",
    "en_tdlr": "This paper investigates the phenomena of overthinking and false induction heads in modern language models when processing false demonstrations. By analyzing the models' internal representations, the study reveals a decrease in accuracy when decoding predictions from intermediate layers given incorrect demonstrations, and identifies the possible mechanism of false induction heads contributing to overthinking."
}