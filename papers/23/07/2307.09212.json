{
    "title": "How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])",
    "abstract": "We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\\mathcal{O}(\\log(\\log(d)))$ and width $\\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight",
    "link": "http://arxiv.org/abs/2307.09212",
    "context": "Title: How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])\nAbstract: We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\\mathcal{O}(\\log(\\log(d)))$ and width $\\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight",
    "path": "papers/23/07/2307.09212.json",
    "total_tokens": 993,
    "translated_title": "需要多少个神经元才能近似计算最大值？",
    "translated_abstract": "我们研究了在使用ReLU激活函数的网络中，近似计算$L_2$范数下连续分布$d$个输入的最大值所需的神经网络的大小。我们提供了关于近似所需宽度的新的下界和上界，以及不同深度之间的深度分离，包括深度2和3以及深度3和5网络之间的分离。此外，我们提供了一个深度为$\\mathcal{O}(\\log(\\log(d)))$，宽度为$\\mathcal{O}(d)$的构造，可以近似计算最大值函数，显著改进了线性宽度限制条件下已知最优的深度需求。我们的深度分离结果通过对均匀分布下深度为2的网络近似最大值函数的新的下界得到，并假设权重大小具有指数上界。此外，我们能够利用这个深度为2的下界提供紧致的上界。",
    "tldr": "本研究研究了使用ReLU激活函数的神经网络近似计算连续分布下$d$个输入的最大值所需的网络大小，并提供了新的界限和分离结果。通过使用深度为2的网络近似最大值函数的新下界，我们提供了一种深度为$\\mathcal{O}(\\log(\\log(d)))$，宽度为$\\mathcal{O}(d)$的构造来改善深度需求。",
    "en_tdlr": "This study investigates the neural network size required to approximate the maximum function over $d$ inputs, using ReLU activations. New bounds and depth separations are provided, including improved construction with depth $\\mathcal{O}(\\log(\\log(d)))$ and width $\\mathcal{O}(d)$. These results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over a uniform distribution."
}