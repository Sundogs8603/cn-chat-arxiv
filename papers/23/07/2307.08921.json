{
    "title": "Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])",
    "abstract": "We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparame",
    "link": "http://arxiv.org/abs/2307.08921",
    "context": "Title: Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])\nAbstract: We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparame",
    "path": "papers/23/07/2307.08921.json",
    "total_tokens": 883,
    "translated_title": "乐观估计揭示了非线性模型的潜力",
    "translated_abstract": "我们提出了一种乐观估计方法，用于评估非线性模型的最佳拟合性能。通过这种方法，我们可以得到一个乐观样本大小，用于确定使用非线性模型来拟合或恢复目标函数所需的最小样本大小。我们估计了矩阵因式分解模型、深度模型和具有全连接或卷积结构的深度神经网络(DNN)的乐观样本大小。对于每个非线性模型，我们的估计预测了可以在过度参数化情况下拟合的特定目标子集，这得到了我们的实验证实。我们的乐观估计揭示了DNN模型的两个特殊属性--宽度上的自由表达和连接上的昂贵表达。这些属性提示了DNN的以下架构设计原则：(i)随意增加神经元/核；(ii)避免连接神经元。总体上，我们的乐观估计在理论上揭示了非线性模型在过度参数化拟合中的巨大潜力。",
    "tldr": "通过乐观估计方法，研究揭示了非线性模型在拟合目标函数时的潜力，并提出了DNN的架构设计原则。"
}