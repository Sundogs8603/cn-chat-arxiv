{
    "title": "Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc",
    "link": "http://arxiv.org/abs/2307.15967",
    "context": "Title: Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc",
    "path": "papers/23/07/2307.15967.json",
    "total_tokens": 772,
    "translated_title": "图形压缩方法用于归纳节点表示学习",
    "translated_abstract": "大规模图引导网络面临着计算挑战，限制了它们在不同应用中的有效性。为了解决这个问题，图形压缩作为一种有希望的技术出现了，它通过构建一个小的合成图来高效地训练图引导网络并保持性能。然而，由于节点之间的拓扑结构，图形压缩仅限于压缩观察到的训练节点及其对应的结构，因此缺乏有效处理未知数据的能力。因此，在推理阶段仍需要原始大图来对归纳节点进行消息传递，导致计算需求巨大。为了解决这个问题，我们提出了映射感知的图形压缩（MCond）方法，明确学习从原始节点到合成节点的一对多节点映射，以无缝地将新节点整合到合成图中。",
    "tldr": "本论文提出了一种映射感知的图形压缩方法（MCond），通过学习节点之间的映射关系，实现了在合成图中高效地处理未知数据的能力。"
}