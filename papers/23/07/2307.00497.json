{
    "title": "Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)",
    "abstract": "Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \\textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i",
    "link": "http://arxiv.org/abs/2307.00497",
    "context": "Title: Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)\nAbstract: Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \\textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i",
    "path": "papers/23/07/2307.00497.json",
    "total_tokens": 883,
    "translated_title": "不要背诵，模仿过去：无需使用记忆的联邦类增量学习",
    "translated_abstract": "深度学习模型在训练新数据时容易忘记过去学习的信息。在联邦学习（FL）的背景下，这个问题变得更加明显，因为数据是分散的，每个用户都会独立地进行更改。连续学习（CL）主要在中心化的环境中研究这种所谓的“灾难性遗忘”现象，其中学习者可以直接访问完整的训练数据集。然而，将CL技术应用于FL并不直接，因为涉及到隐私问题和资源限制。本文提出了一个框架，用于联邦类增量学习，该框架利用生成模型从过去的分布中合成样本，而不是存储部分过去的数据。然后，客户端可以利用生成模型在本地缓解灾难性遗忘。生成模型通过在每个任务结束时使用无数据方法在服务器上进行训练，而不请求来自客户端的数据。",
    "tldr": "该论文提出了一个无需使用记忆的联邦类增量学习框架，通过生成模型合成过去分布的样本，从而缓解联邦学习中的灾难性遗忘问题。",
    "en_tdlr": "This paper presents a framework for federated class incremental learning without episodic memory, which utilizes a generative model to synthesize samples from past distributions and mitigate catastrophic forgetting in federated learning."
}