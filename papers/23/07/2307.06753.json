{
    "title": "Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent. (arXiv:2307.06753v1 [cs.LG])",
    "abstract": "The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.  In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorc",
    "link": "http://arxiv.org/abs/2307.06753",
    "context": "Title: Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent. (arXiv:2307.06753v1 [cs.LG])\nAbstract: The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.  In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorc",
    "path": "papers/23/07/2307.06753.json",
    "total_tokens": 916,
    "translated_title": "用梯度下降学习高斯混合模型的Cramer距离",
    "translated_abstract": "高斯混合模型的学习在机器学习中起着重要作用。高斯混合模型以其表达力和可解释性而闻名，广泛应用于统计学、计算机视觉和分布式强化学习等领域。然而，目前为止，很少有已知算法可以拟合或学习这些模型，其中一些包括期望最大化算法和分割Wasserstein距离。与梯度下降相兼容的算法更少，这是神经网络的常见学习过程。在本文中，我们推导了一维情况下两个高斯混合模型的闭式公式，然后提出了一种称为Sliced Cramer 2距离的距离函数，用于学习一般的多元高斯混合模型。我们的方法比许多先前方法具有几个优点。首先，在一维情况下具有闭式表达式，并且可以使用常见的机器学习库（例如PyTorch）进行易于计算和实现的操作。",
    "tldr": "本文提出了一种用于学习高斯混合模型的Cramer距离，该距离函数在多元情况下具有闭式表达式，并且易于计算和实现，并在梯度下降算法中有效。",
    "en_tdlr": "This paper proposes a Cramer distance for learning Gaussian Mixture Models, which has a closed-form expression and is easy to compute and implement in the multivariate case, and is effective in gradient descent algorithms."
}