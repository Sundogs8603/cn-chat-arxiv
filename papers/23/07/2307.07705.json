{
    "title": "CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])",
    "abstract": "Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named \"CPET\". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating",
    "link": "http://arxiv.org/abs/2307.07705",
    "context": "Title: CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])\nAbstract: Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named \"CPET\". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating",
    "path": "papers/23/07/2307.07705.json",
    "total_tokens": 885,
    "translated_title": "CPET: 高效压缩大型语言模型的参数优化",
    "translated_abstract": "近年来，参数有效调整（PET）因为在调整相对较少的参数（PET模块）的同时仍能激活大型语言模型（LLM）的足够知识以用于下游任务而得到广泛研究。此外，当PET用于为多个任务提供服务时，可以在冻结的LLM上构建不同的任务特定PET模块，避免冗余LLM部署。虽然PET显著降低了调优和部署LLM的成本，但其推理仍然受到LLM计算瓶颈的影响。为了解决上述问题，我们提出了一种基于压缩LLM的有效PET框架，称为“CPET”。在CPET中，我们评估了主流LLM压缩技术对PET性能的影响，然后引入了知识继承和恢复策略来恢复由这些压缩技术引起的知识丢失。我们的实验结果表明，由于CPET的恢复策略，合作",
    "tldr": "CPET提出了一种基于压缩LLM的有效参数优化框架，通过引入知识继承和恢复策略，解决了在参数有效调整中压缩LLM的推理计算瓶颈问题。",
    "en_tdlr": "CPET proposes an effective parameter optimization framework based on compressed LLMs, addressing the computational bottleneck issue of inference in parameter-efficient tuning by introducing knowledge inheritance and recovery strategies."
}