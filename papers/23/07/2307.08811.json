{
    "title": "DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])",
    "abstract": "Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte",
    "link": "http://arxiv.org/abs/2307.08811",
    "context": "Title: DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])\nAbstract: Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte",
    "path": "papers/23/07/2307.08811.json",
    "total_tokens": 901,
    "translated_title": "DeepMem: 将机器学习模型用作存储通道及其（误用）应用",
    "translated_abstract": "机器学习（ML）模型为了支持通用性和避免过拟合而过度参数化。之前的研究表明，这些额外的参数既可以用于恶意目的（例如，在经过训练的模型中隐藏一个模型），也可以用于有益目的（例如，给模型加上水印）。在本文中，我们提出了一个新的信息论视角；我们将ML模型视为一个存储通道，其容量随着过度参数化而增加。具体而言，我们考虑一个发送方，在训练时将任意信息嵌入模型中，接收方可以通过对部署模型的黑盒访问来提取信息。我们根据可用参数的数量推导出通道容量的上界。然后，我们探索了黑盒写入和读取原语，允许攻击者：（i）通过在发射机端扩充训练数据的方式以优化地将数据存储在模型中，以及（ii）通过查询模型来读取数据。",
    "tldr": "本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。",
    "en_tdlr": "This paper presents a novel perspective of treating machine learning models as storage channels, increasing their capacity through overparameterization. By embedding information during training and utilizing black-box access, data can be stored and extracted from the model."
}