{
    "title": "Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])",
    "abstract": "The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor",
    "link": "http://arxiv.org/abs/2307.15034",
    "context": "Title: Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])\nAbstract: The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor",
    "path": "papers/23/07/2307.15034.json",
    "total_tokens": 883,
    "translated_title": "通过混合精度加速傅里叶神经算子",
    "translated_abstract": "傅里叶神经算子（FNO）是一种强大的技术，用于学习偏微分方程（PDE）解算器的代理映射。对于许多现实世界的应用，通常需要高分辨率的数据点，训练时间和内存使用量都是重要瓶颈。虽然对于标准神经网络有混合精度训练技术，但那些只适用于有限维度上的实值数据类型，因此不能直接应用于在复值（傅里叶）域和函数空间中重要操作的FNO。另一方面，由于傅里叶变换本身就是一次近似（由于离散化误差的存在），我们不需要以完全精度执行操作。在这项工作中，我们（i）对使用全精度和混合精度训练的FNO进行内存和运行时间剖析，（ii）对混合精度训练的数值稳定性进行研究，以及（iii）设计了一种训练过程，大大缩短了训练时间和内存使用率。",
    "tldr": "通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。",
    "en_tdlr": "Speed up the runtime and memory usage of Fourier neural operators (FNO) using mixed-precision training, improving training efficiency."
}