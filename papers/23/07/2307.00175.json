{
    "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])",
    "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
    "link": "http://arxiv.org/abs/2307.00175",
    "context": "Title: Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])\nAbstract: We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
    "path": "papers/23/07/2307.00175.json",
    "total_tokens": 956,
    "translated_title": "语言模型仍然没有谎言探测器：探究经验和概念上的障碍",
    "translated_abstract": "本文讨论了大型语言模型（LLM）是否具有信念以及如何衡量它们的问题。首先，我们评估了Azaria和Mitchell（2023）以及Burns等人（2022）提出的两种现有方法，结果表明这些方法在基本方面无法推广。随后我们认为，即使LLM具有信念，这些方法也不太可能在概念上成功。因此，现在仍然没有针对LLM的谎言探测器。在描述了我们的实证结果后，我们退后一步，思考在首次中我们是否应该期待LLM具有类似信念的东西。我们考虑了一些旨在证明LLM不能有信念的最新论证，展示了这些论证是误导性的。我们对围绕LLM中信念的问题的问题提出了更有成效的框架，并强调了该问题的经验性质。最后，我们提出了一些未来工作的具体路径建议。",
    "tldr": "本文讨论了大型语言模型是否具有信念以及如何衡量它们的问题，并通过实证结果和对最新论证的分析，指出现在仍然没有针对大型语言模型的谎言探测器。",
    "en_tdlr": "This paper discusses the question of whether large language models (LLMs) have beliefs and how to measure them. It presents empirical results showing the limitations of existing approaches and argues that even if LLMs have beliefs, current methods are unlikely to be successful. Therefore, there is still no lie detector for LLMs. It also analyzes recent arguments against LLMs having beliefs and suggests a more productive framework for approaching this issue."
}