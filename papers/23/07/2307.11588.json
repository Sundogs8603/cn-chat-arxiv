{
    "title": "Transferability of Convolutional Neural Networks in Stationary Learning Tasks. (arXiv:2307.11588v1 [cs.LG])",
    "abstract": "Recent advances in hardware and big data acquisition have accelerated the development of deep learning techniques. For an extended period of time, increasing the model complexity has led to performance improvements for various tasks. However, this trend is becoming unsustainable and there is a need for alternative, computationally lighter methods. In this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. To accomplish this we investigate the properties of CNNs for tasks where the underlying signals are stationary. We show that a CNN trained on small windows of such signals achieves a nearly performance on much larger windows without retraining. This claim is supported by our theoretical analysis, which provides a bound on the performance degradation. Additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. Our results show that the",
    "link": "http://arxiv.org/abs/2307.11588",
    "context": "Title: Transferability of Convolutional Neural Networks in Stationary Learning Tasks. (arXiv:2307.11588v1 [cs.LG])\nAbstract: Recent advances in hardware and big data acquisition have accelerated the development of deep learning techniques. For an extended period of time, increasing the model complexity has led to performance improvements for various tasks. However, this trend is becoming unsustainable and there is a need for alternative, computationally lighter methods. In this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. To accomplish this we investigate the properties of CNNs for tasks where the underlying signals are stationary. We show that a CNN trained on small windows of such signals achieves a nearly performance on much larger windows without retraining. This claim is supported by our theoretical analysis, which provides a bound on the performance degradation. Additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. Our results show that the",
    "path": "papers/23/07/2307.11588.json",
    "total_tokens": 921,
    "translated_title": "卷积神经网络在固定学习任务中的可迁移性",
    "translated_abstract": "硬件和大规模数据采集的最新进展加速了深度学习技术的发展。在长时间的研究中，增加模型复杂性通常可以提升各种任务的性能。然而，这种趋势正在变得不可持续，需要寻找计算上更轻量的替代方法。本文介绍了一种用于大规模空间问题的卷积神经网络(CNNs)高效训练的新框架。为了实现这一目标，我们研究了CNNs在底层信号为固定的任务中的性质。我们表明，在固定信号的小窗口上训练的CNN可以在不重新训练的情况下，在更大的窗口上取得接近的性能。我们的理论分析支持了这一观点，并给出了性能降低的界限。此外，我们还对两个任务进行了彻底的实验分析：多目标跟踪和按需移动基础设施。我们的结果显示...",
    "tldr": "本文提出了一种用于大规模空间问题的卷积神经网络(CNNs)高效训练的新框架，通过研究CNNs在底层信号为固定的任务中的性质，发现在固定信号的小窗口上训练的CNN可以在更大的窗口上取得接近的性能，无需重新训练。",
    "en_tdlr": "This paper introduces a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. It demonstrates that a CNN trained on small windows of stationary signals achieves similar performance on larger windows without retraining."
}