{
    "title": "Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])",
    "abstract": "This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.",
    "link": "http://arxiv.org/abs/2307.15331",
    "context": "Title: Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])\nAbstract: This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.",
    "path": "papers/23/07/2307.15331.json",
    "total_tokens": 990,
    "translated_title": "使用预训练语言模型进行立场识别的教程：BERT微调和提示大型语言模型",
    "translated_abstract": "本文提供了两个独立的教程，介绍了使用BERT微调和提示大型语言模型（LLMs）在Twitter数据上进行立场识别。第一个教程解释了BERT的架构和分词，指导用户通过使用HuggingFace transformers训练、调优和评估标准和领域特定的BERT模型。第二个教程侧重于构建提示和少样本示例，从ChatGPT和开源FLAN-T5中引出立场而无需进行微调。采用了各种提示策略，并使用混淆矩阵和宏F1分数进行评估。这些教程提供了代码、可视化和洞察力，揭示了少样本ChatGPT和FLAN-T5的优势，它们胜过了微调的BERT。通过以易于理解、实践为导向的方式同时涵盖模型微调和提示技术，这些教程使学习者能够获得对立场检测的尖端方法的实际经验。",
    "tldr": "这篇论文提供了两个教程，介绍了使用BERT微调和提示大型语言模型进行Twitter立场识别的方法。教程通过实例代码和可视化分析，展示了少样本ChatGPT和FLAN-T5的优势，同时提供了对BERT模型的训练和评估的指导。这些教程使学习者能够掌握运用先进方法进行立场识别的实践经验。",
    "en_tdlr": "This paper provides two tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The tutorials showcase the advantages of few-shot ChatGPT and FLAN-T5 over fine-tuned BERT models through code examples and visualizations, while also providing guidance on training and evaluating BERT models. These tutorials enable learners to gain practical experience in applying advanced methods for stance detection."
}