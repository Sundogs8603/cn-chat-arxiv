{
    "title": "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])",
    "abstract": "Data-poisoning based backdoor attacks aim to insert backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing attack methods mainly focus on designing triggers or fusion strategies between triggers and benign samples. However, they often randomly select samples to be poisoned, disregarding the varying importance of each poisoning sample in terms of backdoor injection. A recent selection strategy filters a fixed-size poisoning sample pool by recording forgetting events, but it fails to consider the remaining samples outside the pool from a global perspective. Moreover, computing forgetting events requires significant additional computing resources. Therefore, how to efficiently and effectively select poisoning samples from the entire dataset is an urgent problem in backdoor attacks.To address it, firstly, we introduce a poisoning mask into the regular backdoor training loss. We suppose that a backdoored model training w",
    "link": "http://arxiv.org/abs/2307.07328",
    "context": "Title: Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])\nAbstract: Data-poisoning based backdoor attacks aim to insert backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing attack methods mainly focus on designing triggers or fusion strategies between triggers and benign samples. However, they often randomly select samples to be poisoned, disregarding the varying importance of each poisoning sample in terms of backdoor injection. A recent selection strategy filters a fixed-size poisoning sample pool by recording forgetting events, but it fails to consider the remaining samples outside the pool from a global perspective. Moreover, computing forgetting events requires significant additional computing resources. Therefore, how to efficiently and effectively select poisoning samples from the entire dataset is an urgent problem in backdoor attacks.To address it, firstly, we introduce a poisoning mask into the regular backdoor training loss. We suppose that a backdoored model training w",
    "path": "papers/23/07/2307.07328.json",
    "total_tokens": 872,
    "translated_title": "用可学习的污染样本选择策略增强后门攻击",
    "translated_abstract": "基于数据污染的后门攻击旨在通过操纵训练数据集来将后门插入模型中，而不控制目标模型的训练过程。现有的攻击方法主要集中于设计触发器或触发器与良性样本之间的融合策略。然而，它们往往随机选择要污染的样本，忽视了每个污染样本在后门注入方面的不同重要性。最近的选择策略通过记录遗忘事件来过滤大小固定的污染样本池，但它未能从全局角度考虑池外的剩余样本。此外，计算遗忘事件需要额外的计算资源。因此，如何从整个数据集中高效有效地选择污染样本是后门攻击中的一个迫切问题。为了解决这个问题，首先我们在常规的后门训练损失中引入了一个污染掩码。",
    "tldr": "该论文介绍了一种增强后门攻击的方法，通过引入可学习的污染样本选择策略来有效地选择污染样本。这解决了后门攻击中要从整个数据集中选择污染样本的问题。",
    "en_tdlr": "This paper presents an approach to enhance backdoor attacks by using a learnable poisoning sample selection strategy to effectively choose poisoning samples. It addresses the problem of selecting poisoning samples from the entire dataset in backdoor attacks."
}