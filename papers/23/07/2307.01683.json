{
    "title": "Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])",
    "abstract": "In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as w",
    "link": "http://arxiv.org/abs/2307.01683",
    "context": "Title: Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])\nAbstract: In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as w",
    "path": "papers/23/07/2307.01683.json",
    "total_tokens": 828,
    "translated_title": "使用局部重新参数化技巧学习离散权重和激活",
    "translated_abstract": "在计算机视觉和机器学习中，一个关键的挑战是降低神经网络推理的计算和内存需求。一个常见的解决方案是使用二值化。通过对网络权重和激活进行二值化，可以通过用更快的位运算替代计算复杂的浮点操作来显著降低计算复杂度。这导致了一个更高效的神经网络推理，可以部署在资源有限的设备上。在这项工作中，我们扩展了之前使用局部重新参数化技巧训练具有离散权重的网络的方法，以允许离散激活。原始方法优化了离散权重的分布，并使用中心极限定理将预激活近似为连续的高斯分布。在这里，我们展示了概率建模也可以有效地训练具有离散激活的网络。",
    "tldr": "本论文研究了使用局部重新参数化技巧学习离散权重和激活的方法，通过二值化网络来降低计算复杂度，提高神经网络推理效率。"
}