{
    "title": "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])",
    "abstract": "Artificial intelligence is gaining traction in more ways than ever before. The popularity of language models and AI-based businesses has soared since ChatGPT was made available to the general public via OpenAI. It is becoming increasingly common for people to use ChatGPT both professionally and personally. Considering the widespread use of ChatGPT and the reliance people place on it, this study determined how reliable ChatGPT can be for answering complex medical and clinical questions. Harvard University gross anatomy along with the United States Medical Licensing Examination (USMLE) questionnaire were used to accomplish the objective. The paper evaluated the obtained results using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation between format and prompt. Furthermore, the physician adjudicators independently rated the outcome's accuracy, concordance, and insight. As a result of the analysis, ChatGPT-generated answers were found to be more context-oriented and rep",
    "link": "http://arxiv.org/abs/2307.00112",
    "context": "Title: Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])\nAbstract: Artificial intelligence is gaining traction in more ways than ever before. The popularity of language models and AI-based businesses has soared since ChatGPT was made available to the general public via OpenAI. It is becoming increasingly common for people to use ChatGPT both professionally and personally. Considering the widespread use of ChatGPT and the reliance people place on it, this study determined how reliable ChatGPT can be for answering complex medical and clinical questions. Harvard University gross anatomy along with the United States Medical Licensing Examination (USMLE) questionnaire were used to accomplish the objective. The paper evaluated the obtained results using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation between format and prompt. Furthermore, the physician adjudicators independently rated the outcome's accuracy, concordance, and insight. As a result of the analysis, ChatGPT-generated answers were found to be more context-oriented and rep",
    "path": "papers/23/07/2307.00112.json",
    "total_tokens": 887,
    "translated_title": "ChatGPT在USMLE上的表现：为AI辅助医学教育开启大型语言模型的潜力",
    "translated_abstract": "人工智能正在以前所未有的方式获得越来越多的关注。自从OpenAI发布ChatGPT以来，语言模型和基于AI的业务的流行度大幅增长。人们在职业和个人生活中越来越普遍地使用ChatGPT。考虑到ChatGPT的广泛使用和人们对其的依赖，本研究旨在确定ChatGPT在回答复杂医学和临床问题上的可靠性。该研究使用哈佛大学解剖学知识和美国医学执照考试（USMLE）问卷来实现目标。本文使用双因素方差分析和事后分析评估了获得的结果。两者都显示出格式和提示之间的系统协变。此外，医生评估者还对结果的准确性、一致性和洞察力进行了独立评价。分析结果发现，ChatGPT生成的答案更加注重上下文并且具有较高的可靠性。",
    "tldr": "这项研究评估了ChatGPT在回答复杂医学问题上的可靠性，发现其生成的答案更加注重上下文并具有较高的可靠性。",
    "en_tdlr": "This study evaluated the reliability of ChatGPT in answering complex medical questions and found that its generated answers are more context-oriented and highly reliable."
}