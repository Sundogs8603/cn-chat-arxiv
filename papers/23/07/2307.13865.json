{
    "title": "Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])",
    "abstract": "In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of archi",
    "link": "http://arxiv.org/abs/2307.13865",
    "context": "Title: Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])\nAbstract: In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of archi",
    "path": "papers/23/07/2307.13865.json",
    "total_tokens": 941,
    "translated_title": "预训练的深度2.5D模型用于视网膜OCT的高效预测建模",
    "translated_abstract": "在医学影像领域，3D深度学习模型在构建强大的疾病进展预测模型中起着至关重要的作用。然而，这些模型的大小在计算资源和数据要求方面带来了重大挑战。此外，实现高质量的3D模型的预训练更加困难。为了解决这些问题，混合2.5D方法为利用二维模型高效地处理三维体积数据提供了有效的解决方案。结合二维和三维技术为优化性能并最小化内存要求提供了一条有希望的途径。在本文中，我们探讨了基于卷积神经网络（CNNs）、长短期记忆（LSTM）和Transformer的2.5D架构。此外，利用最近的非对比性预训练方法在二维上的优势，我们进一步提高了2.5D技术的性能和数据效率。我们展示了这种架构的有效性。",
    "tldr": "本文介绍了预训练的深度2.5D模型，用于高效预测视网膜OCT的疾病进展。采用混合2.5D方法结合二维和三维技术，有效优化了性能和内存要求。"
}