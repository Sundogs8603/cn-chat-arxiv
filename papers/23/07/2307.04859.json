{
    "title": "Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])",
    "abstract": "The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar tha",
    "link": "http://arxiv.org/abs/2307.04859",
    "context": "Title: Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])\nAbstract: The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar tha",
    "path": "papers/23/07/2307.04859.json",
    "total_tokens": 932,
    "translated_title": "使用文本到图像扩散模型生成可关节化的3D头像",
    "translated_abstract": "生成多样化的3D可关节化头像对于增强现实、电影制作和教育等众多应用至关重要。最近关于文本引导的3D物体生成的研究展示了很大的潜力来满足这些需求。这些方法直接利用预训练的2D文本到图像扩散模型生成3D多视角一致的通用物体辐射场。然而，由于缺乏几何和纹理先验知识，这些方法对生成的3D物体的控制能力有限，导致难以操作在特定领域内，比如人脸头像。在这项工作中，我们开发了一种新的方法来解决这个限制，实现文本引导的3D头像生成。我们的框架直接在一个可关节化的3D可变形模型（3DMM）的几何和纹理上操作，并引入了更新几何和纹理的新型优化过程，同时保持2D和3D面部特征的对齐。结果是一个3D头像。",
    "tldr": "本研究提出了一种文本引导的3D头像生成方法，通过直接在可关节化的3D模型上操作几何和纹理，并引入了新的优化过程，实现了对生成头像的精确控制。",
    "en_tdlr": "This paper proposes a text-guided approach for generating articulated 3D head avatars. The method operates directly on the geometry and texture of a manipulable 3D model, introducing novel optimization procedures for precise control."
}