{
    "title": "Predicting Code Coverage without Execution. (arXiv:2307.13383v1 [cs.SE])",
    "abstract": "Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from",
    "link": "http://arxiv.org/abs/2307.13383",
    "context": "Title: Predicting Code Coverage without Execution. (arXiv:2307.13383v1 [cs.SE])\nAbstract: Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from",
    "path": "papers/23/07/2307.13383.json",
    "total_tokens": 806,
    "translated_title": "无需执行预测代码覆盖率",
    "translated_abstract": "代码覆盖率是一种广泛应用的衡量程序元素执行情况的度量指标，包括语句或分支的执行情况。计算代码覆盖率需要消耗大量资源，需要构建和执行代码，并且还需要额外的开销进行仪器化。此外，计算任何代码片段的覆盖率需要整个程序的上下文。使用机器学习来分摊这个昂贵的过程可以降低代码覆盖率的成本，只需要源代码上下文，而代码覆盖率预测任务可以成为评估模型理解代码能力的新颖基准。我们提出了一个称为大型语言模型（LLMs）代码覆盖率预测的新颖基准任务。我们通过确定给定测试用例和输入的哪些方法行被执行来形式化评估LLMs在理解代码执行方面的能力。我们整理并发布了一个名为COVERAGEEVAL的数据集，通过执行测试和代码来获取数据。",
    "tldr": "该论文提出了一种无需执行代码即可预测代码覆盖率的方法，并建立了一个基准任务来评估语言模型理解代码能力。"
}