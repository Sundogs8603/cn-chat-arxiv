{
    "title": "Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])",
    "abstract": "Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our",
    "link": "http://arxiv.org/abs/2307.08863",
    "context": "Title: Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])\nAbstract: Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our",
    "path": "papers/23/07/2307.08863.json",
    "total_tokens": 885,
    "translated_title": "元价值学习：一种带有学习意识的学习通用框架",
    "translated_abstract": "多智能体系统中的梯度学习很困难，因为梯度来自于一个一阶模型，不考虑智能体学习过程之间的相互作用。我们扩展了LOLA的思想，并开发了一种完全通用的基于价值的优化方法。核心思想是一个称为元价值的函数，它在联合策略空间的每个点上，为每个智能体给出其未来优化步骤中目标的折扣总和。我们认为，元价值的梯度比原始目标的梯度更可靠的改进方向，因为元价值来自对优化效果的经验观察。我们展示了如何通过训练神经网络来近似元价值，以沿着智能体沿着元价值梯度的优化轨迹进行TD误差最小化。我们分析了我们方法的行为。",
    "tldr": "元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。"
}