{
    "title": "Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])",
    "abstract": "Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti",
    "link": "http://arxiv.org/abs/2307.10802",
    "context": "Title: Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])\nAbstract: Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti",
    "path": "papers/23/07/2307.10802.json",
    "total_tokens": 882,
    "translated_title": "Meta-Transformer: 一个统一的多模态学习框架",
    "translated_abstract": "多模态学习旨在构建能够处理和关联多种模态的信息的模型。尽管在这个领域已经有多年的发展，但由于不同模态之间的固有差距，设计一个用于处理各种模态的统一网络仍然具有挑战性。在这项工作中，我们提出了一个名为Meta-Transformer的框架，该框架利用一个冻结的编码器在没有成对多模态训练数据的情况下进行多模态感知。在Meta-Transformer中，来自各种模态的原始输入数据被映射到一个共享的标记空间中，使得后续的编码器可以提取输入数据的高级语义特征。由三个主要组件组成：一个统一的数据标记器，一个模态共享的编码器和用于下游任务的特定任务头，Meta-Transformer是第一个在12种模态上进行统一学习的框架。",
    "tldr": "Meta-Transformer是一个统一的多模态学习框架，利用一个冻结的编码器进行多模态感知，在没有成对多模态训练数据的情况下可以处理各种模态，并且能够提取输入数据的高级语义特征。",
    "en_tdlr": "Meta-Transformer is a unified framework for multimodal learning that leverages a frozen encoder for multimodal perception, can handle various modalities without paired multimodal training data, and can extract high-level semantic features of the input data."
}