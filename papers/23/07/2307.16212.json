{
    "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty. (arXiv:2307.16212v1 [cs.LG])",
    "abstract": "In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium e",
    "link": "http://arxiv.org/abs/2307.16212",
    "context": "Title: Robust Multi-Agent Reinforcement Learning with State Uncertainty. (arXiv:2307.16212v1 [cs.LG])\nAbstract: In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium e",
    "path": "papers/23/07/2307.16212.json",
    "total_tokens": 922,
    "translated_title": "具有状态不确定性的鲁棒多智能体强化学习",
    "translated_abstract": "在真实世界的多智能体强化学习（MARL）应用中，由于测量不准确或恶意攻击等原因，智能体可能无法获得完美的状态信息，这给智能体的策略的鲁棒性带来了挑战。虽然鲁棒性在MARL部署中变得越来越重要，但很少有先前的工作研究了MARL中的状态不确定性，无论是在问题的表述还是算法设计方面。受到这个鲁棒性问题及缺乏相应研究的启发，我们在本工作中研究了具有状态不确定性的MARL问题。我们首次将该问题建模为具有状态扰动对手的马尔可夫博弈（MG-SPA），通过引入一组状态扰动对手到马尔可夫博弈中。然后，我们引入鲁棒均衡（RE）作为MG-SPA的解决概念。我们进行了对MG-SPA的基本分析，例如给出了满足鲁棒均衡的条件。",
    "tldr": "本论文研究了在多智能体强化学习中状态不确定性的问题。通过引入状态扰动对手，将该问题建模为马尔可夫博弈，并提出了鲁棒均衡作为解决概念。"
}