{
    "title": "Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])",
    "abstract": "The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to d",
    "link": "http://arxiv.org/abs/2307.06962",
    "context": "Title: Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])\nAbstract: The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to d",
    "path": "papers/23/07/2307.06962.json",
    "total_tokens": 832,
    "translated_title": "复制就是你所需的。",
    "translated_abstract": "主导的文本生成模型通过顺序选择来自固定词汇表的单词来组成输出。在本文中，我们将文本生成定义为从现有文本集合中逐步复制文本片段（例如单词或短语）。我们使用高效的向量搜索工具包计算有意义的文本片段的上下文表示并对其进行索引。然后，文本生成的任务被分解为一系列的复制和粘贴操作：在每个时间步骤，我们从文本集合中寻找合适的文本片段，而不是从独立的词汇表中选择。在标准语言建模基准测试（WikiText-103）上的实验表明，我们的方法在自动和人工评估中都实现了更好的生成质量。此外，由于减少了解码步骤，我们的方法的推理效率与基于标记的自回归模型相当。我们还展示了我们的方法通过简单地切换到不同领域，可以实现有效的领域自适应。",
    "tldr": "本文将文本生成定义为从现有文本集合中逐步复制文本片段，并通过复制和粘贴操作来实现生成，相比传统的顺序选择单词生成的模型，在自动和人工评估中取得了更好的生成质量，并且推理效率与基于标记的自回归模型相当。"
}