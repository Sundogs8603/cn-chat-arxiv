{
    "title": "Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])",
    "abstract": "This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t",
    "link": "http://arxiv.org/abs/2307.02509",
    "context": "Title: Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])\nAbstract: This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t",
    "path": "papers/23/07/2307.02509.json",
    "total_tokens": 902,
    "translated_title": "Wasserstein自编码合并树（和持续图）",
    "translated_abstract": "本文提出了一种计算框架，用于合并树的Wasserstein自编码(MT-WAE)，这是将传统的自编码神经网络架构扩展到合并树的Wasserstein度量空间的一种新颖方法。与操作向量化数据的传统自编码器不同，我们的公式在网络的每一层明确地操作合并树的关联度量空间，从而获得更高的准确性和可解释性。我们的新颖神经网络方法可以解释为前期线性尝试[65]的非线性推广。它也很容易扩展到持续图。对公共连锁反应的广泛实验表明我们算法的效率，MT-WAE的计算平均时间仅为几分钟。我们将我们的贡献的实用性通过两个应用来展示，这些应用是基于以前关于合并树编码的工作[65]进行调整得到的。首先，我们将MT-WAE应用于数据压缩，并可靠地压缩合并树。",
    "tldr": "本文提出了一种计算框架，用于将传统的自编码神经网络扩展到合并树的Wasserstein度量空间。算法在准确性和可解释性方面表现出"
}