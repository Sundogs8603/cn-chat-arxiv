{
    "title": "An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2307.04504v2 [math.OC] UPDATED)",
    "abstract": "We study the complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\\delta^{-1}\\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\\delta,\\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned ",
    "link": "http://arxiv.org/abs/2307.04504",
    "context": "Title: An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2307.04504v2 [math.OC] UPDATED)\nAbstract: We study the complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\\delta^{-1}\\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\\delta,\\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned ",
    "path": "papers/23/07/2307.04504.json",
    "total_tokens": 961,
    "translated_title": "零阶非光滑非凸随机优化算法的最优维度依赖性",
    "translated_abstract": "我们研究了使用仅有嘈杂函数评估来产生Lipschitz目标的$(\\delta, \\epsilon)$-稳定点的复杂度，其中目标可能既不光滑也不凸。最近的研究提出了几种解决这个任务的随机零阶算法，所有这些算法都受到了$\\Omega(d^{3/2})$维度依赖性的困扰，其中$d$是问题的维度，这被推测为最优。我们通过提供一个更快的算法来驳斥这个猜想，该算法的复杂度为$O(d\\delta^{-1}\\epsilon^{-3})$，这是关于$d$的最优（在数值常数上），对于精度参数$\\delta, \\epsilon$也是最优的，从而解决了Lin等人留下的一个开放问题（NeurIPS'22）。此外，我们算法实现的收敛速度对于光滑目标也是最优的，证明在非凸随机零阶设置中，非光滑优化与光滑优化一样容易。我们提供了实现上述优化的算法。",
    "tldr": "提出了一个维度依赖优化度为$O(d\\delta^{-1}\\epsilon^{-3})$的最优算法，并证明了非凸随机零阶设置中非光滑优化与光滑优化的一样容易。",
    "en_tdlr": "We propose an algorithm with optimal dimension-dependence complexity $O(d\\delta^{-1}\\epsilon^{-3})$ and prove that nonsmooth optimization is as easy as smooth optimization in the nonconvex stochastic zero-order setting."
}