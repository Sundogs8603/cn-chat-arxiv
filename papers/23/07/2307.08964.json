{
    "title": "Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])",
    "abstract": "Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\\circ \\mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace",
    "link": "http://arxiv.org/abs/2307.08964",
    "context": "Title: Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])\nAbstract: Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\\circ \\mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace",
    "path": "papers/23/07/2307.08964.json",
    "total_tokens": 912,
    "translated_title": "景观替代品：在部分信息下学习数学优化的决策损失",
    "translated_abstract": "最近的学习集成优化工作在优化问题只有部分可观测或通用优化器在无专家调优的情况下表现不佳的情况下显示出了希望。通过学习一个优化器$ \\mathbf{g} $来解决这些具有挑战性的问题，通过利用过去的经验，可以显著加速优化过程。优化器可以通过已知最优解的监督或通过优化复合函数$ f\\circ \\mathbf{g} $的隐式方式进行训练。隐式方法可能不需要最优解作为标签，并且能够处理问题的不确定性；然而，由于在训练和测试过程中频繁调用优化器$ \\mathbf{g} $，因此训练和部署缓慢。对于组合求解器，由于$ \\mathbf{g} $的稀疏梯度，训练进一步受到挑战。为了解决这些问题，我们提出使用平滑可学习的景观替代品$ M $作为一种替代方法。",
    "tldr": "本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。"
}