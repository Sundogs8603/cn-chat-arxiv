{
    "title": "E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])",
    "abstract": "As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive. Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning. To address this challenge, we propose an Effective and Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency. Empirical results demonstrate that our approach outperforms several state-o",
    "link": "http://arxiv.org/abs/2307.13770",
    "context": "Title: E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])\nAbstract: As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive. Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning. To address this challenge, we propose an Effective and Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency. Empirical results demonstrate that our approach outperforms several state-o",
    "path": "papers/23/07/2307.13770.json",
    "total_tokens": 861,
    "translated_title": "E^2VPT: 一种有效高效的视觉提示调整方法",
    "translated_abstract": "随着基于Transformer的模型规模的不断增长，对这些大规模预训练视觉模型进行新任务的微调变得越来越参数密集。为了减少微调过程中可调参数的数量，开发了一种参数高效学习的方法。尽管这些方法表现出了很好的结果，但与完全微调相比仍存在显著的性能差距。为了解决这个挑战，我们提出了一种Effective and Efficient Visual Prompt Tuning（E^2VPT）方法，用于大规模基于Transformer的模型调整。具体而言，我们引入了一组可学习的键值提示和视觉提示到自注意力和输入层，分别改善模型微调的效果。此外，我们设计了一种提示剪枝过程，系统地剪枝低重要性的提示，同时保持模型性能，从而大大提高了模型的效率。实证结果表明，我们的方法优于几种目前最先进的方法。",
    "tldr": "E^2VPT是一种有效和高效的大规模Transformer模型适应方法，通过引入可学习的键值和视觉提示，以及提示剪枝过程来改善模型微调的效果并提高模型的效率。",
    "en_tdlr": "E^2VPT is an effective and efficient approach for large-scale Transformer model adaptation. It improves model fine-tuning by introducing learnable key-value and visual prompts, and enhances efficiency through a prompt pruning procedure."
}