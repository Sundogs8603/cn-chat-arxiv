{
    "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])",
    "abstract": "Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\\widetilde{O}(\\sqrt{\\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\\Omega(\\sqrt{\\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev",
    "link": "http://arxiv.org/abs/2307.02842",
    "context": "Title: Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])\nAbstract: Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\\widetilde{O}(\\sqrt{\\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\\Omega(\\sqrt{\\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev",
    "path": "papers/23/07/2307.02842.json",
    "total_tokens": 1065,
    "translated_title": "可证明高效的迭代CVaR强化学习与函数逼近",
    "translated_abstract": "风险敏感的强化学习旨在优化平衡期望奖励和风险的策略。本文研究了一种新颖的风险敏感强化学习形式，采用迭代条件风险价值（CVaR）目标以线性和一般的函数逼近方法。这种名为带有函数逼近的ICVaR-RL的新形式，为每个决策步骤提供了一种可靠的安全保证方式。对于采用线性函数逼近的ICVaR-RL，我们提出了一个计算效率高的算法ICVaR-L，该算法的后悔度为$\\widetilde{O}(\\sqrt{\\alpha^{-(H+1)}(d^2H^4+dH^6)K})$，其中$\\alpha$是风险水平，$d$是状态行动特征的维度，$H$是每个episode的长度，$K$是episode的数量。我们还建立了一个相匹配的下界$\\Omega(\\sqrt{\\alpha^{-(H-1)}d^2K})$，以验证ICVaR-L在$d$和$K$方面的最优性。对于采用一般函数逼近的ICVaR-RL，我们提出了算法ICVaR-G，它实现了...",
    "tldr": "这篇论文研究了一种新颖的风险敏感强化学习方法，通过迭代条件风险价值目标以及线性和一般函数逼近方法，实现了安全性保证，并提出了高效的算法。通过对于不同逼近方法的实验结果，验证了算法的有效性和优越性。",
    "en_tdlr": "This paper investigates a novel risk-sensitive RL method that balances expected reward and risk through an Iterated Conditional Value-at-Risk objective and linear or general function approximation. The proposed algorithms provide safety guarantees and achieve computational efficiency. Experimental results validate the effectiveness and superiority of the algorithms under different approximation methods."
}