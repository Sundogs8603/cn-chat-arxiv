{
    "title": "Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])",
    "abstract": "In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing",
    "link": "http://arxiv.org/abs/2307.15429",
    "context": "Title: Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])\nAbstract: In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing",
    "path": "papers/23/07/2307.15429.json",
    "total_tokens": 965,
    "translated_title": "可改进的多任务学习中的差距平衡",
    "translated_abstract": "在多任务学习中，梯度平衡近期比损失平衡更吸引研究兴趣，因为它通常能带来更好的性能。然而，损失平衡比梯度平衡更高效，因此在多任务学习中仍然值得进一步探索。注意先前的研究通常忽略了在多个任务之间存在可改进差距的事实，其中每个任务的可改进差距定义为当前训练进度与期望的最终训练进度之间的距离。因此，在损失平衡之后，性能不平衡在许多情况下仍然存在。在本文中，我们基于损失平衡框架提出了两种新的可改进差距平衡算法（IGB）用于多任务学习：一种采用简单的启发式方法，另一种（首次）采用深度强化学习用于多任务学习。特别地，两种算法都选择动态分配任务权重来进行可改进差距平衡。",
    "tldr": "本文提出了两种新颖的可改进差距平衡算法（IGB）用于多任务学习，一种采用简单的启发式方法，另一种首次采用深度强化学习方法。这两种算法通过动态分配任务权重来实现可改进差距平衡，解决了损失平衡之后仍然存在的性能不平衡问题。"
}