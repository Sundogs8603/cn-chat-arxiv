{
    "title": "Minibatching Offers Improved Generalization Performance for Second Order Optimizers. (arXiv:2307.11684v1 [cs.LG])",
    "abstract": "Training deep neural networks (DNNs) used in modern machine learning is computationally expensive. Machine learning scientists, therefore, rely on stochastic first-order methods for training, coupled with significant hand-tuning, to obtain good performance. To better understand performance variability of different stochastic algorithms, including second-order methods, we conduct an empirical study that treats performance as a response variable across multiple training sessions of the same model. Using 2-factor Analysis of Variance (ANOVA) with interactions, we show that batch size used during training has a statistically significant effect on the peak accuracy of the methods, and that full batch largely performed the worst. In addition, we found that second-order optimizers (SOOs) generally exhibited significantly lower variance at specific batch sizes, suggesting they may require less hyperparameter tuning, leading to a reduced overall time to solution for model training.",
    "link": "http://arxiv.org/abs/2307.11684",
    "context": "Title: Minibatching Offers Improved Generalization Performance for Second Order Optimizers. (arXiv:2307.11684v1 [cs.LG])\nAbstract: Training deep neural networks (DNNs) used in modern machine learning is computationally expensive. Machine learning scientists, therefore, rely on stochastic first-order methods for training, coupled with significant hand-tuning, to obtain good performance. To better understand performance variability of different stochastic algorithms, including second-order methods, we conduct an empirical study that treats performance as a response variable across multiple training sessions of the same model. Using 2-factor Analysis of Variance (ANOVA) with interactions, we show that batch size used during training has a statistically significant effect on the peak accuracy of the methods, and that full batch largely performed the worst. In addition, we found that second-order optimizers (SOOs) generally exhibited significantly lower variance at specific batch sizes, suggesting they may require less hyperparameter tuning, leading to a reduced overall time to solution for model training.",
    "path": "papers/23/07/2307.11684.json",
    "total_tokens": 890,
    "translated_title": "小批量化对于二阶优化算法提供了改善的泛化性能",
    "translated_abstract": "训练现代机器学习中使用的深度神经网络(DNN)非常耗费计算资源。因此，机器学习科学家通常依赖于随机一阶方法进行训练，并进行大量手工调整，以获得良好的性能。为了更好地理解不同随机算法（包括二阶方法）的性能变异性，我们进行了一项经验性研究，将性能视为同一模型的多个训练会话中的响应变量。使用二因素方差分析(ANOVA)与交互作用，我们表明训练过程中使用的批次大小对方法的峰值准确性有显著影响，并且完整批次通常表现最差。此外，我们发现二阶优化器(SOOs)在特定批次大小上通常表现出较低的方差，这表明它们可能需要较少的超参数调整，从而减少了模型训练的总体时间解决方案。",
    "tldr": "本研究通过实证研究发现，在训练过程中使用的批次大小对于二阶优化算法的性能具有显著影响，使用小批量化可以提供改善的泛化性能，并减少超参数调整的需求。"
}