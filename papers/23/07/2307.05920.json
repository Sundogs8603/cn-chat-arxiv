{
    "title": "Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])",
    "abstract": "Contrastive language-image Pre-training (CLIP) [13] can leverage large datasets of unlabeled Image-Text pairs, which have demonstrated impressive performance in various downstream tasks. Given that annotating medical data is time-consuming and laborious, Image-Text Pre-training has promising applications in exploiting large-scale medical image and radiology report datasets. However, medical Image-Text Pre-training faces several challenges, as follows: (1) Due to privacy concerns, the amount of available medical data is relatively small compared to natural data, leading to weaker generalization ability of the model. (2) Medical images are highly similar with only fine-grained differences in subtleties, resulting in a large number of false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt usually differs from the natural medical image report, Subtle changes in wording can lead to significant differences in performance. In this paper, we propose a unified Image-Tex",
    "link": "http://arxiv.org/abs/2307.05920",
    "context": "Title: Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])\nAbstract: Contrastive language-image Pre-training (CLIP) [13] can leverage large datasets of unlabeled Image-Text pairs, which have demonstrated impressive performance in various downstream tasks. Given that annotating medical data is time-consuming and laborious, Image-Text Pre-training has promising applications in exploiting large-scale medical image and radiology report datasets. However, medical Image-Text Pre-training faces several challenges, as follows: (1) Due to privacy concerns, the amount of available medical data is relatively small compared to natural data, leading to weaker generalization ability of the model. (2) Medical images are highly similar with only fine-grained differences in subtleties, resulting in a large number of false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt usually differs from the natural medical image report, Subtle changes in wording can lead to significant differences in performance. In this paper, we propose a unified Image-Tex",
    "path": "papers/23/07/2307.05920.json",
    "total_tokens": 938,
    "translated_title": "带有持续提示的统一医学图像-文本-标签对比学习",
    "translated_abstract": "对比性语言-图像预训练（CLIP）可以利用大规模未标记的图像-文本对数据集，在各种下游任务中展现出了令人印象深刻的性能。鉴于医学数据的注释是耗时且费力的，图像-文本预训练在利用大规模医学影像和放射学报告数据集方面具有很大的应用前景。然而，医学图像-文本预训练面临以下几个挑战：（1）由于隐私问题，可用的医学数据相对较少，与自然数据相比，模型的泛化能力较弱。（2）医学图像之间非常相似，细微差别很多，导致比较学习中有大量的假阴性样本对。（3）手工制作的提示通常与自然的医学图像报告不同，措辞上的细微变化可能导致性能的显著差异。本文提出了一种统一的图像-文本-标签对比学习方法，通过持续提示策略，解决了上述挑战。",
    "tldr": "本论文提出了一种统一的医学图像-文本-标签对比学习方法，通过持续提示策略来解决医学图像-文本预训练中的隐私、样本差异和提示差异等挑战。",
    "en_tdlr": "This paper proposes a unified medical image-text-label contrastive learning method that addresses challenges in medical image-text pre-training, including privacy concerns, sample differences, and prompt discrepancies, through continuous prompt strategy."
}