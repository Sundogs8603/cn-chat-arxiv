{
    "title": "Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])",
    "abstract": "Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl",
    "link": "http://arxiv.org/abs/2307.08809",
    "context": "Title: Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])\nAbstract: Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl",
    "path": "papers/23/07/2307.08809.json",
    "total_tokens": 918,
    "translated_title": "本地或全局：基于有限标签的联邦学习中的选择性知识同化",
    "translated_abstract": "许多现有的联邦学习方法假设客户端具有完全标记的数据，而在实际情况下，由于标记过程的昂贵和费力，客户端只有有限的标签。客户端有限的标记本地数据常常导致它们的本地模型对其更大的无标签本地数据具有较差的泛化能力，例如与无标签数据存在类分布不匹配的情况。因此，客户端可能会选择从跨客户端训练的全局模型中受益，以利用他们的无标签数据，但由于客户端之间存在数据异质性，这也变得困难。在我们的工作中，我们提出了FedLabel，客户端根据哪个模型对数据更具专业知识选择本地或全局模型来伪标记其无标签数据。我们进一步通过全局-本地一致性正则化来利用本地和全局模型的知识，当它们对无标签数据具有相同的伪标签时，最小化两个模型的输出之间的差异。",
    "tldr": "本论文提出了FedLabel方法，在联邦学习中，客户端根据数据的专业性选择本地或全局模型对无标签数据进行伪标记，并通过全局-本地一致性正则化来利用本地和全局模型的知识。",
    "en_tdlr": "This paper proposes the FedLabel method, which allows clients in federated learning to selectively choose between local and global models to pseudo-label their unlabeled data based on expertise, and utilizes global-local consistency regularization to leverage knowledge from both models."
}