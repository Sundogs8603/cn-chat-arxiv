{
    "title": "CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])",
    "abstract": "The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch",
    "link": "http://arxiv.org/abs/2307.15663",
    "context": "Title: CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])\nAbstract: The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch",
    "path": "papers/23/07/2307.15663.json",
    "total_tokens": 838,
    "translated_title": "CoRe优化器：机器学习的一体化解决方案",
    "translated_abstract": "优化算法及其超参数在机器学习应用中会显著影响训练速度和模型准确度。理想优化器的愿望清单包括快速、平滑地收敛到低误差、低计算需求和通用适用性。我们最近引入的持续弹性（CoRe）优化器在训练终身机器学习潜力方面比其他最先进的一阶梯度优化器表现出更好的性能。在这项工作中，我们对CoRe优化器进行了与其他九种优化算法的广泛性能对比，包括Adam优化器和弹性反向传播（RPROP）。我们分析了不同超参数的影响，并提供了通用适用的值。CoRe优化器在每个研究应用中都取得了最佳或竞争性的性能，只需要更改一个超参数，具体取决于小批量",
    "tldr": "CoRe优化器是一种高性能的机器学习优化器，具有快速、平滑收敛、低计算需求和通用适用性的特点，在训练终身机器学习潜力方面表现出优势。"
}