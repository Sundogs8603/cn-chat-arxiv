{
    "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)",
    "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.",
    "link": "http://arxiv.org/abs/2307.02477",
    "context": "Title: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)\nAbstract: The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.",
    "path": "papers/23/07/2307.02477.json",
    "total_tokens": 891,
    "translated_title": "推理还是背诵？通过反事实任务探索语言模型的能力和限制",
    "translated_abstract": "最近语言模型在各种任务上的出色表现表明它们具备一定程度的抽象推理能力。这些能力是通用且可转移的，还是专门针对预训练过程中遇到的特定任务？为了分开这些效果，我们提出了一个评估框架，基于“反事实”任务变种，这些变种与支撑标准任务的默认假设有所偏离。在一套包含11个任务的实验中，我们观察到反事实变种的非平凡性能，但与默认条件相比，性能显著而持续地下降。这表明当前的语言模型可能在一定程度上具备抽象任务求解能力，但它们通常也依赖于狭窄、难以转移的任务求解过程。这些结果促使我们对语言模型性能进行更加谨慎的解释，以区分这些行为方面。",
    "tldr": "通过反事实任务的研究，我们发现当前的语言模型具备一定的抽象推理能力，但它们在任务求解过程中往往也依赖于狭窄、难以转移的过程，这对语言模型的性能解释和理解有着重要的启示。",
    "en_tdlr": "Through the study of counterfactual tasks, we found that current language models possess a certain degree of abstract reasoning skills, but they often rely on narrow, non-transferable procedures for task-solving, which has important implications for interpreting and understanding the performance of language models."
}