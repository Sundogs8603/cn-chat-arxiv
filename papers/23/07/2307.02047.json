{
    "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization. (arXiv:2307.02047v2 [cs.CL] UPDATED)",
    "abstract": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of ",
    "link": "http://arxiv.org/abs/2307.02047",
    "context": "Title: CAME: Confidence-guided Adaptive Memory Efficient Optimization. (arXiv:2307.02047v2 [cs.CL] UPDATED)\nAbstract: Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of ",
    "path": "papers/23/07/2307.02047.json",
    "total_tokens": 908,
    "translated_title": "CAME: 靠自信指导的自适应内存高效优化",
    "translated_abstract": "自适应梯度方法，如Adam和LAMB，在大规模语言模型的训练中表现出色。然而，适应性需要维护每个参数梯度的二阶矩估计，这带来了高额的额外内存开销。为了解决这个问题，提出了一些内存高效的优化器（如Adafactor）来大幅减少辅助内存的使用，但会降低性能。本文首先研究了一种基于自信度的策略来减少现有内存高效优化器的不稳定性。基于这一策略，我们提出了CAME，以同时实现两个目标：快速收敛，与传统的自适应方法相似，以及低内存使用率，与内存高效方法相似。大量实验证明了CAME在各种自然语言处理任务（例如BERT和GPT-2训练）中的训练稳定性和优越性能。值得注意的是，在大批量的BERT预训练中，CAME比其他方法训练速度更快，并且具有相似的性能。",
    "tldr": "CAME是一种通过自信指导的策略来实现快速收敛并降低内存使用的自适应内存高效优化方法，在各种自然语言处理任务中表现出稳定性和优越性能。",
    "en_tdlr": "CAME is an adaptive memory efficient optimization method that achieves fast convergence and low memory usage through a confidence-guided strategy, demonstrating stability and superior performance in various natural language processing tasks."
}