{
    "title": "KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])",
    "abstract": "In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.",
    "link": "http://arxiv.org/abs/2307.07409",
    "context": "Title: KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])\nAbstract: In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.",
    "path": "papers/23/07/2307.07409.json",
    "total_tokens": 905,
    "translated_title": "KU-DMIS-MSRA在RadSum23中的预训练视觉语言模型用于放射学报告摘要",
    "translated_abstract": "本文介绍了CheXOFA，一种用于胸部X射线领域的新型预训练视觉语言模型(VLM)。我们的模型首先在一般领域的多模态数据集上进行预训练，然后再转移到胸部X射线领域。在一个著名的VLM中，我们将各种特定领域的任务统一为一个简单的序列到序列的模式。这使得模型能够从有限的领域资源中有效地学习所需的知识和技能。通过在BioNLP共享任务提供的基准数据集上展示出卓越的性能，我们的模型受益于跨多个任务和领域的训练。通过集成和事实校准等微妙的技巧，我们的系统在RadSum23的隐藏测试集上取得了第一名。",
    "tldr": "本文介绍了一种新型的预训练视觉语言模型CheXOFA，通过在一般领域的训练数据上进行预训练，然后转移到胸部X射线领域，该模型能够有效地学习所需的知识和技能，并在放射学报告摘要任务上取得了卓越的性能，获得了RadSum23测试集的第一名。",
    "en_tdlr": "This paper introduces a new pre-trained vision-language model called CheXOFA, which is initially trained on general domain data and then transferred to the chest X-ray domain. The model effectively learns the required knowledge and skills and achieves superior performance in radiology report summarization, ranking first in the RadSum23 leaderboard."
}