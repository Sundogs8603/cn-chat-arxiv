{
    "title": "DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection. (arXiv:2307.06534v1 [cs.LG])",
    "abstract": "Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accu",
    "link": "http://arxiv.org/abs/2307.06534",
    "context": "Title: DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection. (arXiv:2307.06534v1 [cs.LG])\nAbstract: Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accu",
    "path": "papers/23/07/2307.06534.json",
    "total_tokens": 864,
    "translated_title": "DSV:自监督离群点模型选择的对齐验证损失",
    "translated_abstract": "自监督学习（SSL）通过生成内部监督信号已被证明在解决各种问题上有效。无监督异常检测面临获取真实标签的高成本，因此可以从SSL中大大受益。然而，最近的文献表明，调整数据增强函数的超参数对基于SSL的异常检测（SSAD）的成功至关重要，但是目前还没有系统的方法。在这项工作中，我们提出了DSV（不一致性和可分离性验证），这是一种无监督的验证损失，用于选择具有有效参数的高性能检测模型。 DSV通过替代损失函数捕捉数据增强函数与异常生成机制之间的对齐，这些损失函数分别近似了测试数据的不一致性和可分离性。因此，通过DSV进行评估可以选择一个具有更好对齐性的有效SSAD模型，从而实现高检测准确性。",
    "tldr": "DSV提出了一种无监督的验证损失函数用于选择有效的自监督离群点模型，通过捕捉数据增强函数与异常生成机制之间的对齐性，提高了检测准确性。",
    "en_tdlr": "DSV proposes an unsupervised validation loss function for selecting effective self-supervised outlier models, which improves the detection accuracy by capturing the alignment between the data augmentation function and the anomaly-generating mechanism."
}