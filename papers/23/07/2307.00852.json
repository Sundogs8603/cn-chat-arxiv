{
    "title": "VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder",
    "abstract": "arXiv:2307.00852v2 Announce Type: replace  Abstract: The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support",
    "link": "https://arxiv.org/abs/2307.00852",
    "context": "Title: VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder\nAbstract: arXiv:2307.00852v2 Announce Type: replace  Abstract: The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support",
    "path": "papers/23/07/2307.00852.json",
    "total_tokens": 816,
    "translated_title": "通过最大化变分互信息的自编码器改进生成多样性的VOLTA",
    "translated_abstract": "自然语言生成领域得益于Transformer模型取得了巨大成功。虽然它们实现了最先进的生成质量，但往往忽视了生成多样性。先前尝试解决这一问题的方法要么容量较低，要么结构过于复杂。一些最近的方法采用VAE框架增强多样性，但它们的潜在变量完全依赖于输入上下文，限制了潜在空间的探索。在本文中，我们介绍了VOLTA，通过更有效的基于交叉注意力的连接将Transformer与VAE联系起来，从传统的嵌入连接或求和中脱颖而出，提升了生成多样性。此外，我们提议整合InfoGAN风格的潜在编码以实现输入独立的变化性，进一步使生成多样化。此外，我们的框架除了支持现有的连续输入外，还支持离散输入。",
    "tldr": "VOLTA通过Transformer与VAE框架的更有效连接，InfoGAN风格潜在编码以及支持离散输入，提升了生成多样性",
    "en_tdlr": "VOLTA improves generative diversity by bridging Transformer with VAE via a more effective connection, integrating InfoGAN-style latent codes, and accommodating discrete inputs."
}