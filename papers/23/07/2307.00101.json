{
    "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.",
    "link": "http://arxiv.org/abs/2307.00101",
    "context": "Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.",
    "path": "papers/23/07/2307.00101.json",
    "total_tokens": 872,
    "translated_title": "同志人群首先是人：解构大型语言模型中的性别认同刻板印象",
    "translated_abstract": "大型语言模型（LLMs）主要在经过最小化处理的网络文本上进行训练，这些文本展现了创建该内容的人们所持有的各种社会偏见。因此，LLMs生成的文本可能无意中将刻板印象传递给边缘化群体，如LGBTQIA+社群。本文对LLMs生成描述具有不同性别认同的人的文本进行了比较研究。使用regard分数分析文本中的偏见显示出存在对同志人群的可测量偏见。然后，我们展示了一种基于SHAP分析的思维链触发的事后方法可以增加句子的regard，这代表了解决此类情况下LLMs输出偏见的一个有希望的方法。",
    "tldr": "本文通过比较研究了大型语言模型（LLMs）在生成描述不同性别认同的人的文本时产生的偏见问题，发现存在对同志人群的偏见。研究者提出了一种基于SHAP分析的思维链触发的事后方法，可以增加句子的regard，为去除LLMs输出偏见提供了一个有希望的途径。"
}