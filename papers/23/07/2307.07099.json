{
    "title": "Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])",
    "abstract": "In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.",
    "link": "http://arxiv.org/abs/2307.07099",
    "context": "Title: Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])\nAbstract: In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.",
    "path": "papers/23/07/2307.07099.json",
    "total_tokens": 926,
    "translated_title": "通过基于LLM的属性操作生成高效训练数据",
    "translated_abstract": "本文提出了一种新颖的方法，链式思维属性操作（CoTAM），通过从大型语言模型（LLMs）中精心制作的数据来引导少样本学习。主要思想是仅对任务目标属性进行更改并创建数据。受到面部属性操作的启发，我们的方法利用LLMs来操作任务特定属性并以受控的方式重构新的句子，从而生成标签交换数据。我们采用链式思维分解和重构来适应LLMs，而不是传统的潜在表示控制方法。在文本分类和其他任务上进行了广泛的实验结果验证了CoTAM相对于其他具有相同数量训练样本的基于LLMs的文本生成方法的优势。分析结果可视化了CoTAM的属性操作效果，并展示了在更少监督的情况下通过LLM引导学习的潜力。",
    "tldr": "本文提出了一种通过大型语言模型（LLMs）生成精心制作的训练数据来引导少样本学习的方法。通过利用LLMs操作任务特定属性并重构新的句子，我们实现了标签交换数据的生成，与其他基于LLMs的文本生成方法相比具有更好的效果。同时，研究结果还显示了通过LLM引导学习的潜力，即使在更少的监督情况下也能取得良好的表现。",
    "en_tdlr": "This paper proposes a method for guiding few-shot learning by generating carefully crafted training data using Large Language Models (LLMs). The method leverages LLMs to manipulate task-specific attributes and reconstruct new sentences, resulting in improved performance compared to other LLM-based text generation methods. The study also highlights the potential of LLM-guided learning, even with limited supervision."
}