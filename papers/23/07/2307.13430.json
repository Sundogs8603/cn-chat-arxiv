{
    "title": "Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])",
    "abstract": "The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo",
    "link": "http://arxiv.org/abs/2307.13430",
    "context": "Title: Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])\nAbstract: The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo",
    "path": "papers/23/07/2307.13430.json",
    "total_tokens": 839,
    "translated_title": "在分布式随机组合极小化优化中实现线性加速",
    "translated_abstract": "近年来，随机组合极小化问题引起了广泛关注，因为它涵盖了许多新兴的机器学习模型。与此同时，由于分布式数据的出现，需要在分散设置下优化这种问题。然而，损失函数中的组合结构给设计高效的分散式优化算法带来了独特的挑战。我们的研究表明，由于对内层函数的共识误差较大，标准的传递策略无法在分散式组合极小化问题中实现线性加速。为解决这个问题，我们开发了一种新颖的带有动量的分散式随机组合梯度下降算法，以减小内层函数的共识误差。因此，我们的理论结果表明，它可以实现与工作者数量成线性加速。我们相信这种新颖的算法能够在分布式环境中为组合极小化问题提供线性加速。",
    "tldr": "本论文提出了一种新颖的算法，可在分布式环境中实现线性加速，以解决组合极小化问题中内层函数的共识误差问题。"
}