{
    "title": "Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])",
    "abstract": "Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.",
    "link": "http://arxiv.org/abs/2307.07477",
    "context": "Title: Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])\nAbstract: Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.",
    "path": "papers/23/07/2307.07477.json",
    "total_tokens": 785,
    "translated_title": "使用私人联邦学习进行语言模型训练的人口扩展",
    "translated_abstract": "结合差分隐私的联邦学习为分布式设备提供带有正式隐私保证的机器学习训练。当设备数量庞大时，联邦学习与差分隐私的结合能够及时生成性能良好的模型。然而，对于设备数量较少的应用，由于差分隐私噪声与设备数量成反比，模型效用下降，同时由于需要等待来自较小设备池的足够客户端可用，训练延迟也增加。因此，本文提出基于领域适应技术扩展人口，以加快训练并改善使用较少设备进行训练时的最终模型质量。我们通过实验证明，我们的技术可以使实际语言建模数据集的效用提高13%至30%。",
    "tldr": "本文提出了一种使用领域适应技术扩展人口的方法，以加速训练并提高使用较少设备进行训练时的模型质量。",
    "en_tdlr": "This paper proposes a method to expand the population using domain adaptation techniques to speed up training and improve the model quality when training with a smaller number of devices."
}