{
    "title": "RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot. (arXiv:2307.00595v2 [cs.RO] UPDATED)",
    "abstract": "A key challenge in robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent research in one-shot imitation learning has shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improving task and motion planning. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the re",
    "link": "http://arxiv.org/abs/2307.00595",
    "context": "Title: RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot. (arXiv:2307.00595v2 [cs.RO] UPDATED)\nAbstract: A key challenge in robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent research in one-shot imitation learning has shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improving task and motion planning. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the re",
    "path": "papers/23/07/2307.00595.json",
    "total_tokens": 989,
    "translated_title": "RH20T: 一种用于单次学习多样技能的综合机器人数据集",
    "translated_abstract": "在开放领域的机器人操作中，如何获取多样化且具有一般化能力的机器人技能是一个关键挑战。最近的单次模仿学习研究表明，基于示范，将训练好的策略转移到新任务上具有潜力。这种特性有助于使机器人获得新的技能并改进任务和动作规划。然而，由于训练数据集的限制，目前社区的关注点主要集中在简单的情况，如推动或拾取放置任务，仅依靠视觉指导。实际上，存在许多复杂的技能，其中一些甚至可能需要视觉和触觉感知来解决。本文旨在解锁代理商运用多模态感知推广到数百种现实世界技能的潜力。为实现这一目标，我们收集了一个数据集，其中包括超过11万个跨多种技能、环境、机器人和相机视角的接触丰富的机器人操纵序列。",
    "tldr": "本文提出了一个用于单次学习多样技能的综合机器人数据集RH20T。该数据集包含超过11万个接触丰富的机器人操纵序列，涵盖了多种技能、环境、机器人和相机视角。这个数据集的目标是使机器人能够具备广泛的一般化能力，包括视觉和触觉感知。",
    "en_tdlr": "This paper presents a comprehensive robotic dataset called RH20T for learning diverse skills in one-shot. The dataset contains over 110,000 contact-rich robot manipulation sequences, covering various skills, environments, robots, and camera viewpoints. The goal of this dataset is to enable robots to acquire generalizable skills, including both visual and tactile perception."
}