{
    "title": "Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])",
    "abstract": "Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an",
    "link": "http://arxiv.org/abs/2307.16120",
    "context": "Title: Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])\nAbstract: Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an",
    "path": "papers/23/07/2307.16120.json",
    "total_tokens": 944,
    "translated_title": "深度展开网络与循环动量加速用于非线性反问题",
    "translated_abstract": "结合基于模型的迭代算法和数据驱动的深度学习解决方案，深度展开网络(DuNets)已成为解决逆向成像问题的流行工具。虽然DuNets已成功应用于许多线性逆向问题，但非线性问题往往会影响方法的性能。受优化算法中常用的动量加速技术启发，我们提出了一种循环动量加速(RMA)框架，该框架使用长短期记忆循环神经网络(LSTM-RNN)来模拟动量加速过程。RMA模块利用LSTM-RNN学习和保留先前梯度的知识能力。我们将RMA应用于两种流行的DuNets——学习的近端梯度下降(LPGD)和学习的原始-对偶(LPD)方法，分别得到LPGD-RMA和LPD-RMA。我们在两个非线性逆向问题上提供了实验结果：非线性去卷积问题、",
    "tldr": "本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。"
}