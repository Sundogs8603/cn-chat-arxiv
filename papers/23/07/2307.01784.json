{
    "title": "The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])",
    "abstract": "Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., \"but\") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the dis",
    "link": "http://arxiv.org/abs/2307.01784",
    "context": "Title: The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])\nAbstract: Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., \"but\") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the dis",
    "path": "papers/23/07/2307.01784.json",
    "total_tokens": 860,
    "translated_title": "一个思想的内在情感",
    "translated_abstract": "基于Transformer的大型语言模型能够生成高度逼真的文本。它们能够表达并至少暗示出一系列情感和色彩，从明显的价值和唤起到微妙的决心和赞赏。我们首次探索了这些表示及其如何用于理解单个句子内部的情感运作。我们训练了从增长长度的前缀中应用到LLM的隐藏表示的句子的最终情感的分布的定量预测器。在展示了价值、决心、赞赏、焦虑和烦恼的分布预测器是良好校准的基础上，我们提供了使用这些预测器分析句子的示例，例如，展示了即使是普通的连接词（例如，“但是”）也可以极大地改变话语的情感轨迹。然后，我们展示了如何利用这些预测器来利用分布表示。",
    "tldr": "这篇论文探索了基于Transformer的大型语言模型中句子内部情感表示的方法，训练了预测器来分析句子的情感分布，并展示了即使是普通的连接词也可以显著改变话语的情感轨迹。",
    "en_tdlr": "This paper explores the representation of inner sentiments in sentences using Transformer-based large-scale language models. It trains predictors to analyze the distribution of emotions in sentences and demonstrates how even ordinary conjunctions can significantly alter the emotional trajectory of utterances."
}