{
    "title": "Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])",
    "abstract": "There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and ",
    "link": "http://arxiv.org/abs/2307.01453",
    "context": "Title: Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])\nAbstract: There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and ",
    "path": "papers/23/07/2307.01453.json",
    "total_tokens": 833,
    "translated_title": "多样的检索增强上下文学习用于对话状态跟踪",
    "translated_abstract": "由于收集和注释面向任务对话的成本较高，对于零样本和少样本学习的对话状态跟踪 (DST) 引起了重大兴趣。最近的研究表明，在上下文学习中，只需要很少的数据和零个参数更新，甚至在少样本设置中优于训练方法 (Hu等，2022)。我们提出了RefPyDST，它通过三个改进推动了对DST的上下文学习的最新进展。首先，我们将DST形式化为Python编程任务，明确地将语言指代建模为Python中的变量引用。其次，由于上下文学习高度依赖上下文示例，我们提出了一种检索多样的相关示例以提高性能的方法。最后，在解码过程中引入了一种新颖的重新权重方法，考虑了竞争表面形式的概率，并产生了更准确的对话状态预测结果。我们使用MultiWOZ和进行了评估",
    "tldr": "该论文提出了RefPyDST，通过在对话状态跟踪中的上下文学习中引入Python编程任务、多样性检索相关示例和重新权重解码方法的改进，取得了更好的性能。"
}