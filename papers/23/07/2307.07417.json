{
    "title": "RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])",
    "abstract": "Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \\textbf{Ro}bust \\textbf{P}rompt-based \\textbf{D}ata \\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe",
    "link": "http://arxiv.org/abs/2307.07417",
    "context": "Title: RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])\nAbstract: Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \\textbf{Ro}bust \\textbf{P}rompt-based \\textbf{D}ata \\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe",
    "path": "papers/23/07/2307.07417.json",
    "total_tokens": 894,
    "translated_title": "RoPDA：用于低资源命名实体识别的鲁棒基于提示的数据增强",
    "translated_abstract": "数据增强在低资源NER任务中被广泛使用以解决数据稀缺的问题。然而，先前的数据增强方法存在破坏句法结构、标记-标签不匹配和对外部知识或手动工作的需求的缺点。为了解决这些问题，我们提出了RoPDA: 一种用于低资源NER的鲁棒基于提示的数据增强方法。基于预训练语言模型（PLMs）和连续提示，RoPDA通过五个基本的增强操作进行实体增强和上下文增强，生成标签翻转和保留标签的样本。为了优化增强样本的利用，我们提出了两种技术：自一致性过滤和混合。前者有效地消除低质量样本，后者防止直接利用标签翻转样本导致性能下降。在三个基准测试中进行了大量实验...",
    "tldr": "RoPDA是一种用于低资源NER的数据增强方法，通过基于预训练语言模型和连续提示进行实体和上下文增强，并提出了自一致性过滤和混合技术以优化增强样本的利用。",
    "en_tdlr": "RoPDA is a data augmentation method for low-resource NER that performs entity and context augmentation based on pre-trained language models and continuous prompts. It introduces self-consistency filtering and mixup techniques to optimize the utilization of augmented samples."
}