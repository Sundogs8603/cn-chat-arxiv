{
    "title": "Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])",
    "abstract": "Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated. To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively. In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight. Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions. Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts.",
    "link": "http://arxiv.org/abs/2307.06097",
    "context": "Title: Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])\nAbstract: Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated. To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively. In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight. Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions. Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts.",
    "path": "papers/23/07/2307.06097.json",
    "total_tokens": 903,
    "translated_title": "使用图神经网络学习随机动力系统作为一种隐式正则化方法",
    "translated_abstract": "提出了随机Gumbel图网络来学习高维时间序列，其中观测到的维度通常具有空间相关性。为此，通过学习随机微分方程的漂移项和扩散项，分别捕捉观测到的随机性和空间相关性，并采用Gumbel矩阵嵌入来表示。特别地，这种新颖的框架使我们能够研究S-GGNs中噪声项的隐式正则化效果。通过推导权重的小邻域中两个相应损失函数的差异，我们为提出的S-GGNs提供了理论保证。然后，我们使用Kuramoto模型生成数据，比较两个损失函数的Hessian矩阵的谱密度。实验结果表明，与现有技术相比，S-GGNs具有较好的收敛性、鲁棒性和泛化能力。",
    "tldr": "本论文提出了一种使用图神经网络学习随机动力系统的隐式正则化方法，通过学习随机微分方程的漂移项和扩散项来捕捉时间序列中的观测到的随机性和空间相关性。实验结果表明，该方法在收敛性、鲁棒性和泛化能力方面表现出优越性。",
    "en_tdlr": "This paper proposes an implicit regularization method using graph neural networks to learn stochastic dynamical systems, capturing the observed randomness and spatial correlations in high-dimensional time series. Experimental results demonstrate superior convergence, robustness, and generalization compared to state-of-the-art methods."
}