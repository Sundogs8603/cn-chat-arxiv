{
    "title": "Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors. (arXiv:2307.04415v1 [eess.SY])",
    "abstract": "Due to the increasing complexity of technical systems, accurate first principle models can often not be obtained. Supervised machine learning can mitigate this issue by inferring models from measurement data. Gaussian process regression is particularly well suited for this purpose due to its high data-efficiency and its explicit uncertainty representation, which allows the derivation of prediction error bounds. These error bounds have been exploited to show tracking accuracy guarantees for a variety of control approaches, but their direct dependency on the training data is generally unclear. We address this issue by deriving a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on the prediction error bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show to achieve vanishing tracking error with increasi",
    "link": "http://arxiv.org/abs/2307.04415",
    "context": "Title: Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors. (arXiv:2307.04415v1 [eess.SY])\nAbstract: Due to the increasing complexity of technical systems, accurate first principle models can often not be obtained. Supervised machine learning can mitigate this issue by inferring models from measurement data. Gaussian process regression is particularly well suited for this purpose due to its high data-efficiency and its explicit uncertainty representation, which allows the derivation of prediction error bounds. These error bounds have been exploited to show tracking accuracy guarantees for a variety of control approaches, but their direct dependency on the training data is generally unclear. We address this issue by deriving a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on the prediction error bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show to achieve vanishing tracking error with increasi",
    "path": "papers/23/07/2307.04415.json",
    "total_tokens": 910,
    "translated_title": "基于时序高斯过程的学习控制，实现消失的跟踪误差",
    "translated_abstract": "随着技术系统的复杂性增加，往往无法获得准确的第一原理模型。通过监督机器学习可以从测量数据中推断模型，高斯过程回归特别适用于此目的，因为它具有高的数据效率和明确的不确定性表示，可以推导出预测误差边界。这些误差边界已被用于展示不同控制方法的跟踪精度保证，但其直接依赖于训练数据通常不明确。我们通过推导基于贝叶斯的高斯过程回归预测误差边界来解决此问题，并展示其随着一种基于内核的数据密度度量的增长而减小。基于预测误差边界，我们证明了学习到的高斯过程模型作为未知非线性的反馈补偿，可以实现时变的跟踪精度保证，并展示了随着时间增加而消失的跟踪误差。",
    "tldr": "本研究提出了一种基于时序高斯过程的学习控制方法，通过推导预测误差边界以及一种基于内核的数据密度度量，实现了时变的跟踪精度保证，并展示了跟踪误差的消失。"
}