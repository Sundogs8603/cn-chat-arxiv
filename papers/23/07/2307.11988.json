{
    "title": "Sparse then Prune: Toward Efficient Vision Transformers. (arXiv:2307.11988v1 [cs.CV])",
    "abstract": "The Vision Transformer architecture is a deep learning model inspired by the success of the Transformer model in Natural Language Processing. However, the self-attention mechanism, large number of parameters, and the requirement for a substantial amount of training data still make Vision Transformers computationally burdensome. In this research, we investigate the possibility of applying Sparse Regularization to Vision Transformers and the impact of Pruning, either after Sparse Regularization or without it, on the trade-off between performance and efficiency. To accomplish this, we apply Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100 datasets. The training process for the Vision Transformer model consists of two parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results show that when testing with CIFAR-100 an",
    "link": "http://arxiv.org/abs/2307.11988",
    "context": "Title: Sparse then Prune: Toward Efficient Vision Transformers. (arXiv:2307.11988v1 [cs.CV])\nAbstract: The Vision Transformer architecture is a deep learning model inspired by the success of the Transformer model in Natural Language Processing. However, the self-attention mechanism, large number of parameters, and the requirement for a substantial amount of training data still make Vision Transformers computationally burdensome. In this research, we investigate the possibility of applying Sparse Regularization to Vision Transformers and the impact of Pruning, either after Sparse Regularization or without it, on the trade-off between performance and efficiency. To accomplish this, we apply Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100 datasets. The training process for the Vision Transformer model consists of two parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results show that when testing with CIFAR-100 an",
    "path": "papers/23/07/2307.11988.json",
    "total_tokens": 938,
    "translated_title": "稀疏化再剪枝：向高效的视觉Transformer模型过渡",
    "translated_abstract": "视觉Transformer模型是受到自然语言处理中Transformer模型成功启发的深度学习模型。然而，自注意机制、大量参数以及对大量训练数据的需求仍使得视觉Transformer计算上的负担较重。本研究探讨了在视觉Transformer上应用稀疏正则化的可能性，并研究了在性能和效率之间的权衡上，无论是在稀疏正则化之后还是之前进行剪枝的影响。为了实现这一目标，我们将稀疏正则化和剪枝方法应用于用于CIFAR-10、CIFAR-100和ImageNet-100数据集的图像分类任务中的视觉Transformer模型。视觉Transformer模型的训练过程包括两个部分：预训练和微调。预训练使用ImageNet21K数据，然后进行20个epoch的微调。结果表明，当在CIFAR-100上进行测试时，最佳效果是通过在稀疏正则化之后进行剪枝得到的，可以在不损失太多性能的情况下显著减少模型的计算负担。",
    "tldr": "本研究探讨了在视觉Transformer模型上应用稀疏正则化和剪枝的方法，结果表明在稀疏正则化之后进行剪枝可以显著减少模型计算负担而不损失太多性能。",
    "en_tdlr": "This research investigates the application of sparse regularization and pruning methods on the Vision Transformer model, and finds that pruning after sparse regularization significantly reduces computational burden without sacrificing too much performance."
}