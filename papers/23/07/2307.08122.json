{
    "title": "Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)",
    "abstract": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.",
    "link": "http://arxiv.org/abs/2307.08122",
    "context": "Title: Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)\nAbstract: We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.",
    "path": "papers/23/07/2307.08122.json",
    "total_tokens": 885,
    "translated_title": "切线变换器用于组合、隐私和去除",
    "translated_abstract": "我们引入了切线关注微调（TAFT）方法，该方法通过在预训练初始化点周围计算一阶泰勒展开来获得线性化变压器进行微调。我们展示了从线性化得到的雅可比矩阵-向量积可以在单个前向传递中高效计算，将训练和推断成本降低到与原始非线性模型相同数量级，并且使用相同数量的参数。此外，我们还展示了当应用于各种下游视觉分类任务时，通过TAFT进行微调的结果切线变换器可以与对原始非线性网络进行微调相当。由于切线变换器对于新的权值是线性的，并且结果微调损失是凸的，我们展示了相比于非线性微调，TAFT在模型组合、并行训练、机器去除和差分隐私方面具有几个优势。",
    "tldr": "我们引入了一种切线注意微调方法（TAFT），通过线性化变压器的一阶泰勒展开来进行微调。该方法具有与原始非线性网络相当的性能，并在模型组合、并行训练、机器去除和差分隐私方面具有优势。",
    "en_tdlr": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. TAFT performs comparably with the original non-linear network and has advantages in model composition, parallel training, machine unlearning, and differential privacy."
}