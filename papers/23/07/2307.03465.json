{
    "title": "TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])",
    "abstract": "The AllInOne training paradigm squeezes a wide range of tasks into a unified model in a multi-task learning manner. However, optimization in multi-task learning is more challenge than single-task learning, as the gradient norm from different tasks may vary greatly, making the backbone overly biased towards one specific task. To address this issue, we propose the task-level backbone-oriented gradient clip paradigm, compared with the vanilla gradient clip method, it has two points of emphasis:1) gradient clip is performed independently for each task. 2) backbone gradients generated from each task are rescaled to the same norm scale. Based on the experimental results, we argue that the task-level backbone-oriented gradient clip paradigm can relieve the gradient bias problem to some extent. We also propose a novel multi-branch data augmentation strategy where conflict augmentations are placed in different branches. Our approach has been shown to be effective and finally achieve 1st place i",
    "link": "http://arxiv.org/abs/2307.03465",
    "context": "Title: TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])\nAbstract: The AllInOne training paradigm squeezes a wide range of tasks into a unified model in a multi-task learning manner. However, optimization in multi-task learning is more challenge than single-task learning, as the gradient norm from different tasks may vary greatly, making the backbone overly biased towards one specific task. To address this issue, we propose the task-level backbone-oriented gradient clip paradigm, compared with the vanilla gradient clip method, it has two points of emphasis:1) gradient clip is performed independently for each task. 2) backbone gradients generated from each task are rescaled to the same norm scale. Based on the experimental results, we argue that the task-level backbone-oriented gradient clip paradigm can relieve the gradient bias problem to some extent. We also propose a novel multi-branch data augmentation strategy where conflict augmentations are placed in different branches. Our approach has been shown to be effective and finally achieve 1st place i",
    "path": "papers/23/07/2307.03465.json",
    "total_tokens": 899,
    "translated_title": "TBGC: 多任务基础模型学习的任务级骨干导向梯度截断",
    "translated_abstract": "AllInOne训练范式以多任务学习方式将各种任务集中到统一模型中。然而，多任务学习中的优化比单任务学习更具挑战性，因为不同任务的梯度范数可能差异很大，使得骨干过分偏向某个特定任务。为了解决这个问题，我们提出了任务级骨干导向梯度截断范式，与普通的梯度截断方法相比，它有两个重点：1）对每个任务独立进行梯度截断；2）从每个任务生成的骨干梯度重新缩放到相同的范数尺度。根据实验结果，我们认为任务级骨干导向梯度截断范式在一定程度上可以缓解梯度偏向问题。我们还提出了一种新颖的多分支数据增强策略，在不同分支中放置冲突增强。我们的方法被证明是有效的，并最终在比赛中获得第一名。",
    "tldr": "提出了任务级骨干导向梯度截断范式，通过独立的梯度截断和统一的范数缩放，有效缓解了多任务学习中的梯度偏向问题。",
    "en_tdlr": "Introducing the task-level backbone-oriented gradient clip paradigm, with independent gradient clip and uniform norm scaling, effectively mitigates the gradient bias problem in multi-task learning."
}