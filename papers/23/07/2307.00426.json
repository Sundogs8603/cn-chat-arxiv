{
    "title": "Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)",
    "abstract": "Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.",
    "link": "http://arxiv.org/abs/2307.00426",
    "context": "Title: Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)\nAbstract: Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.",
    "path": "papers/23/07/2307.00426.json",
    "total_tokens": 862,
    "translated_title": "基于稀疏感知的深度神经网络泛化理论",
    "translated_abstract": "深度人工神经网络取得了令人惊讶的泛化能力，但其具体机制尚不清楚。本文提出了一种新的方法来分析前向深度ReLU网络的泛化能力，利用隐藏层激活的稀疏程度。通过构建一个考虑每个输入样本减小有效模型大小的框架，我们能够展示出稀疏和泛化之间的根本权衡。重要的是，我们的结果对模型实现的稀疏程度没有强烈的假设，并且改进了最近的基于范数的方法。我们通过数值实例展示了结果，即使在过参数化的模型中，在特定情况下与数据相关的先验结合时也能得到非空界限。",
    "tldr": "本文研究了深度神经网络的泛化能力，提出了一种基于稀疏感知的分析方法。通过考虑隐藏层激活的稀疏程度，我们展示了稀疏和泛化之间的权衡，而且结果对模型的稀疏程度没有强烈的假设，并在特定情况下的数值实验中得到了非空的界限。",
    "en_tdlr": "This paper investigates the generalization ability of deep neural networks and proposes a sparsity-aware analysis approach. By considering the sparsity of hidden layer activations, the paper demonstrates trade-offs between sparsity and generalization. The results do not heavily depend on the sparsity level and are shown to hold even in over-parametrized models with data-dependent priors."
}