{
    "title": "A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])",
    "abstract": "Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and spee",
    "link": "http://arxiv.org/abs/2307.15045",
    "context": "Title: A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])\nAbstract: Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and spee",
    "path": "papers/23/07/2307.15045.json",
    "total_tokens": 848,
    "translated_title": "基于Transformer的阿拉伯离线手写文本识别方法",
    "translated_abstract": "手写识别是模式识别和机器学习领域中具有挑战性和关键性的问题，其应用领域广泛。本文着重研究识别离线阿拉伯手写文本的特定问题。现有的方法通常利用卷积神经网络进行图像特征提取和循环神经网络进行时间建模，使用联结时序分类进行文本生成。然而，由于循环神经网络的序列性质，这些方法缺乏并行化。此外，这些模型无法考虑语言规则，因此需要在后处理阶段使用外部语言模型来提高准确性。为了解决这些问题，我们引入了两种替代架构，即Transformer Transducer和标准的序列到序列Transformer，并比较它们在准确性和速度方面的性能。",
    "tldr": "本文提出了基于Transformer的阿拉伯离线手写文本识别方法，通过引入Transformer Transducer和标准的序列到序列Transformer架构，解决了循环神经网络的并行化和语言规则不考虑的问题，具有较高的准确性和速度。",
    "en_tdlr": "This paper proposes a Transformer-based approach for Arabic offline handwritten text recognition. By introducing the Transformer Transducer and the standard sequence-to-sequence Transformer architectures, it addresses the issues of lack of parallelization in recurrent neural networks and the inability to account for linguistic rules, resulting in higher accuracy and speed."
}