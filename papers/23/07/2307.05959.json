{
    "title": "Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])",
    "abstract": "Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e",
    "link": "http://arxiv.org/abs/2307.05959",
    "context": "Title: Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])\nAbstract: Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e",
    "path": "papers/23/07/2307.05959.json",
    "total_tokens": 892,
    "translated_title": "给机器人以帮助：通过眼手协同的人类视频演示学习通用操作",
    "translated_abstract": "眼手协同摄像头在视觉导向的机器人操作中显示出了更高的样本效率和泛化能力。然而，在机器人模仿中，让人类远程操作员收集大量专家演示对于真实机器人来说仍然很昂贵。另一方面，人类进行任务的视频要便宜得多，因为它们消除了对机器人遥操作专业知识的需求，并且可以在各种场景中快速捕捉。因此，人类视频演示是学习大规模通用机器人操作策略的有希望的数据来源。在这项工作中，我们使用广泛的无标签人类视频演示来增强狭窄的机器人模仿数据集，从而大大提高了眼手协同视觉运动策略的泛化能力。尽管人类和机器人数据之间存在明显的视觉领域差距，但我们的框架不需要使用任何明确的领域适应方法，因为我们利用了部分可观察性。",
    "tldr": "使用无标签的人类视频演示增强了眼手协同的视觉运动策略的泛化能力，而无需昂贵的专家演示数据或领域适应方法。",
    "en_tdlr": "Using unlabeled human video demonstrations enhances the generalization of eye-in-hand visuomotor policies without the need for expensive expert demonstrations or domain adaptation methods."
}