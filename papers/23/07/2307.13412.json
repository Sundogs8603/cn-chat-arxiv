{
    "title": "Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation. (arXiv:2307.13412v1 [cs.LG])",
    "abstract": "The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts t",
    "link": "http://arxiv.org/abs/2307.13412",
    "context": "Title: Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation. (arXiv:2307.13412v1 [cs.LG])\nAbstract: The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts t",
    "path": "papers/23/07/2307.13412.json",
    "total_tokens": 832,
    "translated_title": "用即时生成权重的方法减轻CNN引擎中的内存瓶颈效应",
    "translated_abstract": "卷积神经网络（CNN）在广泛的人工智能任务中取得了前所未有的准确性，这导致它们在移动和嵌入式环境中得到广泛应用。为了实现高性能和低能耗的推理，研究人员在基于FPGA的CNN加速器设计方面投入了大量研究工作。然而，单个计算引擎常常在受内存限制的层上性能下降，并且由于某些层在引擎的固定配置中的次优映射而导致资源利用不足。本文研究了一类引入预卷积阶段在运行时解压缩权重的模型对CNN引擎设计的影响。我们将这些方法称为即时生成权重。本文提出了一种新颖的CNN推理系统unzipFPGA，以解决这些问题。",
    "tldr": "本文研究了用即时生成权重的方法减轻CNN引擎中的内存瓶颈效应，并提出了一种名为unzipFPGA的CNN推理系统。",
    "en_tdlr": "This paper investigates mitigating the memory wall effects in CNN engines by introducing on-the-fly weights generation, and proposes a novel CNN inference system called unzipFPGA."
}