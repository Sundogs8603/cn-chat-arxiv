{
    "title": "Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])",
    "abstract": "\\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \\emph{label} given knowledge of the correct answer \\emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke",
    "link": "http://arxiv.org/abs/2307.09458",
    "context": "Title: Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])\nAbstract: \\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \\emph{label} given knowledge of the correct answer \\emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke",
    "path": "papers/23/07/2307.09458.json",
    "total_tokens": 973,
    "translated_title": "电路分析的可解释性是否具有可扩展性？来自毛丫鼠中多项选择能力的证据。",
    "translated_abstract": "电路分析是一种理解语言模型内部机制的有前途的技术。然而，现有的分析都是在远离最先进技术的小型模型中进行的。为了解决这个问题，我们在70B毛丫鼠模型中进行了一项案例研究，旨在测试电路分析的可扩展性。具体而言，我们研究了多项选择题，调查了毛丫鼠在知道正确答案文本的情况下是否能够识别出正确答案标签。我们发现已有的逻辑层归因、注意力模式可视化和激活修补技术在毛丫鼠模型中具有自然的可扩展性，使我们能够识别和分类一小组“输出节点”（注意力头和多层感知机）。我们进一步研究了“正确字母”类别的注意力头，旨在了解其特征的语义，结果有所不同。对于正常的多项选择问题，我们显著压缩了查询。",
    "tldr": "本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。"
}