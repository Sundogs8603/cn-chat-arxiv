{
    "title": "Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])",
    "abstract": "Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.",
    "link": "http://arxiv.org/abs/2307.01542",
    "context": "Title: Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])\nAbstract: Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.",
    "path": "papers/23/07/2307.01542.json",
    "total_tokens": 795,
    "translated_title": "通过自对比训练减轻开放式生成中对重复的学习偏差",
    "translated_abstract": "尽管在各种生成任务中取得了巨大的进展，但预训练语言模型（如GPT2）仍倾向于使用基于最大化的解码算法生成重复的文本。我们将其对令牌级别重复概率的过度估计归因于学习偏差：语言模型使用MLE损失更快地捕捉到简单的重复模式。我们提出了自对比训练，以惩罚同一模型过早检查点的输出，当它错误地预测重复时，这在两个数据集上显示出有效减轻重复同时保持流畅的效果。此外，我们发现语言模型在预测重复令牌时使用的是更长范围的依赖关系，而非重复令牌则不然，这可能是造成句子级别重复循环的原因。",
    "tldr": "通过自对比训练，我们成功减轻了预训练语言模型在开放式生成中对重复的学习偏差。我们的方法有效地缓解了重复问题，并保持了流畅性。",
    "en_tdlr": "We successfully mitigate the learning bias towards repetition in open-ended generation by self-contrastive training. Our approach effectively alleviates the issue of repetition and maintains fluency."
}