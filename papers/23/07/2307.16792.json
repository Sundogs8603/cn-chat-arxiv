{
    "title": "Classification with Deep Neural Networks and Logistic Loss. (arXiv:2307.16792v1 [stat.ML])",
    "abstract": "Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\\\"older smoothness of the conditional class probability $\\eta$ of data. Moreover, we consider a compositional assumption that requires $\\eta$ to be the composition of several vector-valued functions of which each co",
    "link": "http://arxiv.org/abs/2307.16792",
    "context": "Title: Classification with Deep Neural Networks and Logistic Loss. (arXiv:2307.16792v1 [stat.ML])\nAbstract: Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\\\"older smoothness of the conditional class probability $\\eta$ of data. Moreover, we consider a compositional assumption that requires $\\eta$ to be the composition of several vector-valued functions of which each co",
    "path": "papers/23/07/2307.16792.json",
    "total_tokens": 974,
    "translated_title": "使用深度神经网络和逻辑损失进行分类",
    "translated_abstract": "使用逻辑损失（即交叉熵损失）训练的深度神经网络在各种二分类任务中取得了显著的进展。然而，关于使用深度神经网络和逻辑损失进行二分类的泛化分析仍然很少。逻辑损失的目标函数的无界性是导致推导出令人满意的泛化界限的主要障碍。本文旨在通过建立一种新颖而优雅的oracle型不等式来填补这一空白，该不等式使我们能够处理目标函数的有界性限制，并利用它推导出使用逻辑损失训练的全连接ReLU深度神经网络分类器的收敛速率。特别地，我们仅需要数据的条件类概率$\\eta$的H\\\"older平滑性，就可以获得最优的收敛速率（仅限于对数因子）。此外，我们考虑了一个组合假设，要求$\\eta$是若干向量值函数的复合函数，其中每个向量值函数都是独立的。",
    "tldr": "本文提出了一种新颖的oracle型不等式，通过解决逻辑损失的目标函数无界性限制，推导出使用逻辑损失训练的全连接ReLU深度神经网络分类器的最优收敛速率，仅要求数据的条件类概率具有H\\\"older平滑性，并且考虑了组合假设，使得该方法具有更广泛的适用性。",
    "en_tdlr": "This paper proposes a novel oracle-type inequality, which overcomes the unboundedness restriction of the target function in logistic loss, and derives optimal convergence rates for fully connected ReLU deep neural network classifiers trained with logistic loss. The method requires the conditional class probability to have H\\\"older smoothness and considers a compositional assumption, making it more widely applicable."
}