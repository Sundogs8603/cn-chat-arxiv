{
    "title": "Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])",
    "abstract": "Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten",
    "link": "http://arxiv.org/abs/2307.09943",
    "context": "Title: Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])\nAbstract: Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten",
    "path": "papers/23/07/2307.09943.json",
    "total_tokens": 1022,
    "translated_title": "这里是翻译过的论文标题: 过去曾翻译《Impatient Bandits: Optimizing for the Long-Term Without Delay》",
    "translated_abstract": "这里是翻译过的论文摘要：推荐系统在在线平台上是一个普遍存在的功能。越来越多的情况下，它们明确地被任务为提高用户的长期满意度。在这个背景下，我们研究了一个内容探索任务，将其形式化为一个具有延迟奖励的多臂赌博问题。我们观察到，在选择学习信号时存在明显的权衡：等待完全的奖励可能需要几周时间，这会影响学习发生的速度，而测量短期代理奖励则不完美地反映了实际的长期目标。我们通过两个步骤来解决这个挑战。首先，我们开发了一个预测延迟奖励的模型，该模型可以整合迄今所获得的所有信息。通过贝叶斯滤波器组合完整的观察结果以及部分（短期或中期）的结果，从而得到概率信念。其次，我们设计了一个利用这个新的预测模型的赌博算法。该算法可以快速学习识别内容。",
    "tldr": "这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。"
}