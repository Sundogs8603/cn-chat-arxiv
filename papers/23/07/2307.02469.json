{
    "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?. (arXiv:2307.02469v2 [cs.CV] UPDATED)",
    "abstract": "Recent advancements in Large Language Models (LLMs) such as GPT4 have displayed exceptional multi-modal capabilities in following open-ended instructions given images. However, the performance of these models heavily relies on design choices such as network structures, training data, and training strategies, and these choices have not been extensively discussed in the literature, making it difficult to quantify progress in this field. To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models. We implement over 20 variants with controlled settings. Concretely, for network structures, we compare different LLM backbones and model designs. For training data, we investigate the impact of data and sampling strategies. For instructions, we explore the influence of diversified prompts on the instruction-following ability of the trained models. For benchmarks, we contribute the first, to our best knowledge, compreh",
    "link": "http://arxiv.org/abs/2307.02469",
    "context": "Title: What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?. (arXiv:2307.02469v2 [cs.CV] UPDATED)\nAbstract: Recent advancements in Large Language Models (LLMs) such as GPT4 have displayed exceptional multi-modal capabilities in following open-ended instructions given images. However, the performance of these models heavily relies on design choices such as network structures, training data, and training strategies, and these choices have not been extensively discussed in the literature, making it difficult to quantify progress in this field. To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models. We implement over 20 variants with controlled settings. Concretely, for network structures, we compare different LLM backbones and model designs. For training data, we investigate the impact of data and sampling strategies. For instructions, we explore the influence of diversified prompts on the instruction-following ability of the trained models. For benchmarks, we contribute the first, to our best knowledge, compreh",
    "path": "papers/23/07/2307.02469.json",
    "total_tokens": 933,
    "translated_title": "用多模态输入训练GPT4风格的语言模型有哪些重要问题？",
    "translated_abstract": "最近，关于大型语言模型（LLM）如GPT4的进展显示出在根据图像遵循开放式指令方面具有出色的多模态能力。然而，这些模型的性能很大程度上依赖于网络结构、训练数据和训练策略等设计选择，并且这些选择在文献中尚未得到广泛讨论，这使得在这个领域中很难量化进展。为了解决这个问题，本文对训练这种模型进行了系统全面的定量和定性研究。我们实现了20多个带有控制设置的变体。具体而言，对于网络结构，我们比较了不同的LLM骨干和模型设计。对于训练数据，我们研究了数据和采样策略的影响。对于指令，我们探讨了多样化提示对所训练模型的指令遵循能力的影响。对于基准测试，我们贡献了第一个在我们的最佳知识范围内对此进行全面评估的工作。",
    "tldr": "本论文对训练GPT4风格的语言模型进行了系统综合研究，定量和定性分析了网络结构、训练数据和训练策略等设计选择对模型性能的影响，并提供了相关基准测试。"
}