{
    "title": "The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])",
    "abstract": "Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati",
    "link": "http://arxiv.org/abs/2307.14502",
    "context": "Title: The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])\nAbstract: Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati",
    "path": "papers/23/07/2307.14502.json",
    "total_tokens": 908,
    "translated_title": "通过使用自监督语音表示损失函数研究口语对语音增强的影响",
    "translated_abstract": "最近在语音增强领域的研究中，使用自监督语音表示（SSSRs）作为特征转换在损失函数中。然而，在之前的工作中，很少注意到用于训练自监督表示的音频语言与用于训练语音增强系统的语言之间的关系。使用将自监督表示与用于训练语音增强系统的嘈杂数据的语言完全匹配的损失函数训练的增强模型表现比不完全匹配的模型更好。这可能导致增强系统具有特定语言的特性，因此对未见语言的泛化能力不好，而使用传统的频谱图或时间域损失函数训练的模型则不会出现这个问题。本研究在多种不同语言上训练和测试了语音增强模型，使用了经过不同语言组合训练的自监督表示。",
    "tldr": "本文研究了在语音增强中，使用与嘈杂数据语言完全匹配的自监督语音表示损失函数训练的模型的性能更好。与传统的频谱图或时间域损失函数相比，这些增强模型具有特定语言的特性。",
    "en_tdlr": "This paper investigates the performance of speech enhancement models trained using a self-supervised speech representation loss function that matches the language of the noisy data used for training. These models show better performance than those trained using traditional spectrogram or time domain loss functions, and exhibit language-specific characteristics."
}