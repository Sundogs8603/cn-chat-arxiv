{
    "title": "Submodular Reinforcement Learning. (arXiv:2307.13372v1 [cs.LG])",
    "abstract": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal",
    "link": "http://arxiv.org/abs/2307.13372",
    "context": "Title: Submodular Reinforcement Learning. (arXiv:2307.13372v1 [cs.LG])\nAbstract: In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal",
    "path": "papers/23/07/2307.13372.json",
    "total_tokens": 875,
    "translated_title": "子模块强化学习",
    "translated_abstract": "在强化学习中，状态的奖励通常被认为是可加的，并且根据马尔可夫假设，它们与之前访问的状态$\\textit{独立}$。在许多重要应用中，如覆盖控制、实验设计和信息路径规划，奖励自然具有递减回报，即其价值随之前访问过的相似状态的增加而减小。为了解决这个问题，我们提出了$\\textit{子模块强化学习}$ (SubRL) ，这一范式旨在通过子模块集合函数来建模递减回报，从而优化更一般的非可加奖励（历史相关）。然而，不幸的是，即使在表格设置中，我们证明了得到的优化问题很难近似解决。另一方面，受经典子模块优化中贪婪算法的成功启发，我们提出了SubPO，一种用于SubRL的简单基于策略梯度的算法，通过贪婪地最大化边际来处理非可加奖励。",
    "tldr": "子模块强化学习(SubRL)是一种用于优化非可加奖励的范式，通过子模块集合函数来建模递减回报。这篇论文提出了SubRL的简单策略梯度算法SubPO，可以用于处理这种类型的奖励。"
}