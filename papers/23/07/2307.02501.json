{
    "title": "Generalization Guarantees via Algorithm-dependent Rademacher Complexity. (arXiv:2307.02501v1 [stat.ML])",
    "abstract": "Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient ",
    "link": "http://arxiv.org/abs/2307.02501",
    "context": "Title: Generalization Guarantees via Algorithm-dependent Rademacher Complexity. (arXiv:2307.02501v1 [stat.ML])\nAbstract: Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient ",
    "path": "papers/23/07/2307.02501.json",
    "total_tokens": 893,
    "translated_title": "通过算法相关的Rademacher复杂度实现泛化保证",
    "translated_abstract": "现代机器学习算法的泛化行为需要算法和数据相关的泛化界限来解释。在这个背景下，存在着涉及(各种形式的)互信息的信息论泛化界限，以及基于假设集稳定性的界限。我们提出了一个在技术上不同但在概念上相关的复杂度度量，来控制泛化错误，即算法和数据相关的假设类的经验Rademacher复杂度。结合Rademacher复杂度的标准属性和该类的便捷结构，我们能够：(i)基于有限分形维度获得新的界限，这些界限将前人工作中的分形维度界限从连续的假设类推广到有限假设类，并且避免了之前工作中需要的互信息项；(ii)大大简化了最近提出的针对随机梯度下降的无维度泛化界限的证明。",
    "tldr": "提出了一种通过算法和数据相关的假设类的经验Rademacher复杂度来控制泛化错误的方法，基于有限分形维度获得了新的界限，并简化了对随机梯度下降的无维度泛化界限的证明。",
    "en_tdlr": "A new method is proposed to control generalization error using the empirical Rademacher complexity of algorithm- and data-dependent hypothesis class. Novel bounds based on finite fractal dimension are obtained, avoiding a mutual information term, and the proof of dimension-independent generalization bounds for stochastic gradient descent is greatly simplified."
}