{
    "title": "From Adaptive Query Release to Machine Unlearning. (arXiv:2307.11228v1 [cs.LG])",
    "abstract": "We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\\rho>0$, our results yield an unlearning algorithm with excess population risk of $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\rho}\\big)$ with unlearning query (gradient) complexity $\\tilde O(\\rho \\cdot \\text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\big(\\frac{\\sqrt{d}}{n\\rho}",
    "link": "http://arxiv.org/abs/2307.11228",
    "context": "Title: From Adaptive Query Release to Machine Unlearning. (arXiv:2307.11228v1 [cs.LG])\nAbstract: We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\\rho>0$, our results yield an unlearning algorithm with excess population risk of $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\rho}\\big)$ with unlearning query (gradient) complexity $\\tilde O(\\rho \\cdot \\text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\big(\\frac{\\sqrt{d}}{n\\rho}",
    "path": "papers/23/07/2307.11228.json",
    "total_tokens": 939,
    "translated_title": "从自适应查询释放到机器取消学习",
    "translated_abstract": "我们将机器取消学习问题形式化为设计高效取消学习算法来对应从结构化查询类中选择自适应查询的学习算法。我们给出了线性和前缀和查询类的高效取消学习算法。作为应用，我们展示了在许多问题中，特别是随机凸优化（SCO）中的取消学习可以通过上述方法来减少，从而改善问题的保证。特别地，对于平滑的Lipschitz损失和任意的$\\rho>0$，我们的结果给出了一个取消学习算法，其超出总体风险为$\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\rho}\\big)$，取消学习查询（梯度）复杂性为$\\tilde O(\\rho \\cdot \\text{重新训练复杂性})$，其中$d$是模型的维度，$n$是初始样本数。对于非平滑的Lipschitz损失，我们给出了一个取消学习算法，其超出总体风险为$\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\big(\\frac{\\sqrt{d}}{n\\rho}$",
    "tldr": "该论文将机器取消学习问题形式化为设计高效的取消学习算法，给出了线性和前缀和查询类的高效取消学习算法，以及应用于随机凸优化问题的改进保证。",
    "en_tdlr": "This paper formalizes the problem of machine unlearning and provides efficient unlearning algorithms for linear and prefix-sum query classes, with improved guarantees for stochastic convex optimization problems."
}