{
    "title": "LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])",
    "abstract": "Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the to",
    "link": "http://arxiv.org/abs/2307.02912",
    "context": "Title: LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])\nAbstract: Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the to",
    "path": "papers/23/07/2307.02912.json",
    "total_tokens": 948,
    "translated_title": "LEA: 使用词汇注意偏差提高对打字错误的句子相似性鲁棒性",
    "translated_abstract": "文本噪音，如打字错误或缩写，是一个众所周知的问题，会对大多数下游任务中的纯变压器模型造成惩罚。我们展示了这也适用于句子相似性，这是多个领域中的一个基本任务，比如匹配、检索或释义。可以使用交叉编码器来处理句子相似性，其中两个句子在输入中连接，使模型能够利用它们之间的相互关系。之前解决噪音问题的工作主要依赖于数据增强策略，展示了在处理与训练样本相似的损坏样本时性能有所提升。然而，所有这些方法仍然受到打字错误引起的标记分布偏移的影响。在这项工作中，我们提出使用一种新颖的词汇感知注意模块（LEA）来解决文本噪音问题，该模块在两个句子中的词之间引入了词汇相似性。通过使用原始文本相似性，我们的方法避免了token分布偏移的问题。",
    "tldr": "本论文提出了LEA模块，用于提高对打字错误的句子相似性鲁棒性。该模块通过引入词汇相似性来解决文本噪音问题，并避免了打字错误导致的标记分布偏移。",
    "en_tdlr": "This paper proposes a LEA module to improve the robustness of sentence similarity to typos. By incorporating lexical similarities between words, the module tackles textual noise and avoids token distribution shift caused by typos."
}