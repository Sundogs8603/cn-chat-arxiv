{
    "title": "GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])",
    "abstract": "Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.",
    "link": "http://arxiv.org/abs/2307.04099",
    "context": "Title: GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])\nAbstract: Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.",
    "path": "papers/23/07/2307.04099.json",
    "total_tokens": 956,
    "translated_title": "GNP攻击: 通过梯度范数惩罚实现可转移的对抗样本",
    "translated_abstract": "具有良好可转移性的对抗样本使得针对不同目标模型的黑盒攻击成为可能，而不需要关于目标模型的内部知识。先前的方法往往生成具有很少或没有可转移性的对抗样本；也就是说，它们容易过拟合于源模型的特定架构和特征表示，生成的对抗样本几乎对目标黑盒模型没有作用。在本文中，我们提出了一种新颖的方法，利用梯度范数惩罚（GNP）增强对抗样本的可转移性。它驱使损失函数优化过程收敛到损失函数空间中的一个平坦区域的局部最优点。通过攻击11个最先进的深度学习模型和6种高级防御方法，我们经验证明，GNP在生成具有高转移性的对抗样本方面非常有效。我们还证明，它非常灵活，可以轻松与其他基于梯度的方法结合，以实现更强大的基于转移的攻击。",
    "tldr": "本文提出了一种通过梯度范数惩罚实现可转移的对抗样本的方法，通过攻击多个深度学习模型和防御方法的实验证明，该方法非常有效。同时，该方法还可以与其他梯度方法结合使用，以实现更强大的攻击。",
    "en_tdlr": "This paper proposes a method for generating transferable adversarial examples using gradient norm penalty. Empirical results show that the method is highly effective in attacking multiple deep learning models and defense methods. Additionally, the method can be easily integrated with other gradient-based methods for stronger attacks."
}