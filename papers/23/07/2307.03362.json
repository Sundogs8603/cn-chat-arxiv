{
    "title": "Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agents' Beliefs about Plans. (arXiv:2307.03362v1 [cs.AI])",
    "abstract": "When agents collaborate on a task, it is important that they have some shared mental model of the task routines -- the set of feasible plans towards achieving the goals. However, in reality, situations often arise that such a shared mental model cannot be guaranteed, such as in ad-hoc teams where agents may follow different conventions or when contingent constraints arise that only some agents are aware of. Previous work on human-robot teaming has assumed that the team has a set of shared routines, which breaks down in these situations. In this work, we leverage epistemic logic to enable agents to understand the discrepancy in each other's beliefs about feasible plans and dynamically plan their actions to adapt or communicate to resolve the discrepancy. We propose a formalism that extends conditional doxastic logic to describe knowledge bases in order to explicitly represent agents' nested beliefs on the feasible plans and state of execution. We provide an online execution algorithm ba",
    "link": "http://arxiv.org/abs/2307.03362",
    "context": "Title: Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agents' Beliefs about Plans. (arXiv:2307.03362v1 [cs.AI])\nAbstract: When agents collaborate on a task, it is important that they have some shared mental model of the task routines -- the set of feasible plans towards achieving the goals. However, in reality, situations often arise that such a shared mental model cannot be guaranteed, such as in ad-hoc teams where agents may follow different conventions or when contingent constraints arise that only some agents are aware of. Previous work on human-robot teaming has assumed that the team has a set of shared routines, which breaks down in these situations. In this work, we leverage epistemic logic to enable agents to understand the discrepancy in each other's beliefs about feasible plans and dynamically plan their actions to adapt or communicate to resolve the discrepancy. We propose a formalism that extends conditional doxastic logic to describe knowledge bases in order to explicitly represent agents' nested beliefs on the feasible plans and state of execution. We provide an online execution algorithm ba",
    "path": "papers/23/07/2307.03362.json",
    "total_tokens": 893,
    "translated_title": "人机合作中的适应性和沟通：处理代理关于计划的信念差异",
    "translated_abstract": "当代理共同合作完成任务时，拥有一些共享的任务流程的心理模型非常重要，这些流程是实现目标的可行计划的集合。然而，实际情况经常出现这样的情况，即无法保证存在这样的共享心理模型，比如在临时团队中代理可能遵循不同的约定，或者当出现只有部分代理知晓的临时约束时。以前关于人机合作的研究假设团队拥有一组共享的流程，而在这些情况下这种假设不成立。在这项工作中，我们利用认知逻辑使代理能够理解彼此关于可行计划的信念差异，并动态规划他们的行动来适应或沟通以解决差异。我们提出了一个扩展了条件信念逻辑的形式，以描述知识库，从而明确表示代理对可行计划和执行状态的嵌套信念。我们提供了一个在线执行算法。",
    "tldr": "本文研究人机合作中的适应性和沟通，以处理代理之间关于计划的信念差异，通过利用认知逻辑来理解和解决这些差异。",
    "en_tdlr": "This paper investigates adaptation and communication in human-robot teaming to handle discrepancies in agents' beliefs about plans, using epistemic logic to understand and resolve these differences."
}