{
    "title": "MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])",
    "abstract": "Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.",
    "link": "http://arxiv.org/abs/2307.09361",
    "context": "Title: MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])\nAbstract: Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.",
    "path": "papers/23/07/2307.09361.json",
    "total_tokens": 909,
    "translated_title": "MOCA: 自监督学习通过预测掩码式在线码本分配实现表示学习",
    "translated_abstract": "自监督学习可以用于缓解Vision Transformer网络对大型全注释数据集的贪婪需求。不同类别的自监督学习提供了具有良好语境推理属性的表示，例如使用掩码图像建模策略，或者对图像扰动具有不变性的表示，例如使用对比方法。在这项工作中，我们提出了一种单阶段、独立的方法MOCA，使用基于高级特征（而不是像素级细节）定义的新型掩码和预测目标来统一这两种期望的属性。此外，我们展示了如何以协同和计算高效的方式有效地应用这两种学习范式。通过这样做，我们在低样本设置上实现了新的最先进结果，并且在各种评估协议中取得了强大的实验结果，其训练速度至少比之前的方法快3倍。",
    "tldr": "MOCA是一种自监督学习方法，通过预测掩码式在线码本分配来实现表示学习。它同时具备良好的语境推理属性和对图像扰动的不变性，并在低样本设置和各种评估协议中取得了最新的最先进结果，训练速度比之前的方法快3倍以上。",
    "en_tdlr": "MOCA is a self-supervised learning method that achieves representation learning by predicting masked online codebook assignments. It combines good contextual reasoning properties and invariance to image perturbations, achieving state-of-the-art results in low-shot settings and various evaluation protocols while being at least 3 times faster in training compared to prior methods."
}