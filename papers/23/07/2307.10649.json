{
    "title": "An Adaptive Dual-level Reinforcement Learning Approach for Optimal Trade Execution. (arXiv:2307.10649v1 [q-fin.CP])",
    "abstract": "The purpose of this research is to devise a tactic that can closely track the daily cumulative volume-weighted average price (VWAP) using reinforcement learning. Previous studies often choose a relatively short trading horizon to implement their models, making it difficult to accurately track the daily cumulative VWAP since the variations of financial data are often insignificant within the short trading horizon. In this paper, we aim to develop a strategy that can accurately track the daily cumulative VWAP while minimizing the deviation from the VWAP. We propose a method that leverages the U-shaped pattern of intraday stock trade volumes and use Proximal Policy Optimization (PPO) as the learning algorithm. Our method follows a dual-level approach: a Transformer model that captures the overall(global) distribution of daily volumes in a U-shape, and a LSTM model that handles the distribution of orders within smaller(local) time intervals. The results from our experiments suggest that th",
    "link": "http://arxiv.org/abs/2307.10649",
    "context": "Title: An Adaptive Dual-level Reinforcement Learning Approach for Optimal Trade Execution. (arXiv:2307.10649v1 [q-fin.CP])\nAbstract: The purpose of this research is to devise a tactic that can closely track the daily cumulative volume-weighted average price (VWAP) using reinforcement learning. Previous studies often choose a relatively short trading horizon to implement their models, making it difficult to accurately track the daily cumulative VWAP since the variations of financial data are often insignificant within the short trading horizon. In this paper, we aim to develop a strategy that can accurately track the daily cumulative VWAP while minimizing the deviation from the VWAP. We propose a method that leverages the U-shaped pattern of intraday stock trade volumes and use Proximal Policy Optimization (PPO) as the learning algorithm. Our method follows a dual-level approach: a Transformer model that captures the overall(global) distribution of daily volumes in a U-shape, and a LSTM model that handles the distribution of orders within smaller(local) time intervals. The results from our experiments suggest that th",
    "path": "papers/23/07/2307.10649.json",
    "total_tokens": 917,
    "translated_title": "一种适应性的双层强化学习方法用于最优交易执行",
    "translated_abstract": "本研究的目的是设计一种可以利用强化学习紧密追踪每日累积成交量加权平均价格(VWAP)的策略。以往的研究通常选择相对较短的交易时间段来实施他们的模型，使得准确追踪每日累积VWAP变得困难，因为金融数据的变动在短时间段内往往不显著。在本文中，我们旨在开发一种能够准确追踪每日累积VWAP并最小化与VWAP的偏差的策略。我们提出了一种利用股票交易量的U形模式并使用Proximal Policy Optimization(PPO)作为学习算法的方法。我们的方法采用了双层方法：一个Transformer模型捕捉每日交易量在全局上呈U形分布的总体分布，一个LSTM模型处理较小时段内订单的分布。我们实验的结果表明，我们的方法可以有效追踪VWAP并减小与之偏差。",
    "tldr": "本研究提出一种双层强化学习方法，利用U形模式的股票交易量和Proximal Policy Optimization算法，可以准确追踪并最小化与每日累积VWAP的偏差。"
}