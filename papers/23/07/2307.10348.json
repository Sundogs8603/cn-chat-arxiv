{
    "title": "Code Detection for Hardware Acceleration Using Large Language Models. (arXiv:2307.10348v1 [cs.SE])",
    "abstract": "Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.  This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.  Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection ",
    "link": "http://arxiv.org/abs/2307.10348",
    "context": "Title: Code Detection for Hardware Acceleration Using Large Language Models. (arXiv:2307.10348v1 [cs.SE])\nAbstract: Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.  This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.  Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection ",
    "path": "papers/23/07/2307.10348.json",
    "total_tokens": 972,
    "translated_title": "使用大型语言模型进行硬件加速的代码检测",
    "translated_abstract": "大型语言模型(LLMs)已被广泛应用于许多任务中，通常超过了最先进的方法。虽然人们已经广泛研究了它们在代码生成方面的有效性（例如AlphaCode），但它们在代码检测方面的潜力尚未被探索。本研究首次分析了使用LLMs进行代码检测的情况。我们的研究考察了包括矩阵乘法、卷积和快速傅里叶变换在内的关键核心代码，这些代码是用C/C++实现的。我们提出了一种初步的简单提示和一种用于代码检测的新型提示策略。结果显示，传统的提示策略在精确性方面表现出色，但准确性较差（分别为GEMM、卷积和FFT的68.8%、22.3%和79.2%），因为存在大量的误报。我们的新型提示策略显著减少了误报，从而取得了出色的整体准确性（分别为91.1%、97.9%和99.7%）。这些结果对现有的最先进的代码检测方法提出了重大挑战。",
    "tldr": "本研究利用大型语言模型(LLMs)进行代码检测，并提出了一种新的提示策略，显著降低了误报，实现了出色的整体准确性，对现有的最先进的代码检测方法提出了重大挑战。",
    "en_tdlr": "This study explores the use of large language models (LLMs) for code detection and proposes a novel prompting strategy that significantly reduces false positives, achieving excellent overall accuracy. These results pose a considerable challenge to existing state-of-the-art code detection methods."
}