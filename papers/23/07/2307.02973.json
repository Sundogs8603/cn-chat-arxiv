{
    "title": "Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])",
    "abstract": "Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p",
    "link": "http://arxiv.org/abs/2307.02973",
    "context": "Title: Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])\nAbstract: Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p",
    "path": "papers/23/07/2307.02973.json",
    "total_tokens": 848,
    "translated_title": "修剪与量化：哪个更好？",
    "translated_abstract": "神经网络修剪和量化技术几乎和神经网络本身一样古老。然而，迄今为止只有两者之间的临时比较发表过。本文旨在回答哪个更好：神经网络量化还是修剪？通过回答这个问题，我们希望为神经网络硬件的设计决策提供信息。我们对压缩深度神经网络的这两种技术进行了全面比较。首先，我们对一般数据分布的期望量化和修剪误差进行了分析比较。然后，我们提供了在训练好的网络中每层修剪和量化误差的下界，并将其与优化后的经验误差进行了比较。最后，我们对3个任务上的8个大规模模型进行了广泛的实验比较。我们的结果表明，在大多数情况下，量化优于修剪。只有在一些极高压缩比的情况下，修剪表现出更好的效果。",
    "tldr": "本文比较了神经网络量化和修剪这两种压缩深度神经网络的技术，结果表明在大多数情况下，量化优于修剪。",
    "en_tdlr": "This paper compares the techniques of neural network quantization and pruning for compressing deep neural networks, and finds that in most cases, quantization outperforms pruning."
}