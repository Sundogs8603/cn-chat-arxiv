{
    "title": "Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])",
    "abstract": "Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of informa",
    "link": "http://arxiv.org/abs/2307.09388",
    "context": "Title: Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])\nAbstract: Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of informa",
    "path": "papers/23/07/2307.09388.json",
    "total_tokens": 923,
    "translated_title": "在非平稳环境中具有高成本特征的在线学习",
    "translated_abstract": "在顺序决策问题中，最大化长期回报是主要目标。大多数现有方法假设附加信息是免费提供的，使得学习代理能够在做决策之前观察到所有特征状态。然而，在现实世界的问题中，收集有益信息往往是昂贵的。这意味着，除了个体奖励之外，学习特征状态的观察对于改善决策策略也是必要的。在非平稳环境中，奖励和成本分布随时间发生突变，这进一步加剧了双重学习问题。为了解决上述双重学习问题，我们扩展了上下文乐观设置，并允许代理观察特征状态的子集。目标是最大化长期平均收益，即平均累积回报和平均支付成本之差。因此，代理面临着在最小化信息成本和最大化收益之间的权衡。",
    "tldr": "该论文提出了一种在非平稳环境中进行在线学习的方法，其中收集有益信息是昂贵的。研究扩展了上下文乐观设置，允许代理观察特征状态的子集，以在最小化信息成本和最大化收益之间进行权衡。"
}