{
    "title": "On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])",
    "abstract": "Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions t",
    "link": "http://arxiv.org/abs/2307.15546",
    "context": "Title: On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])\nAbstract: Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions t",
    "path": "papers/23/07/2307.15546.json",
    "total_tokens": 899,
    "translated_title": "关于神经抽象的效率和精确性之间的权衡",
    "translated_abstract": "神经抽象最近被引入作为复杂非线性动力模型的形式近似。它们包括一个神经ODE和抽象神经网络与具体动力模型之间误差的证明上界。到目前为止，神经抽象仅以全$ReLU$激活函数组成的神经网络形式得到，导致具有分段仿射动力学的神经ODE模型，可以等效地解释为线性混合自动机。在这项工作中，我们观察到抽象的效用取决于它的使用：某些情况可能需要容易分析的粗略抽象，而其他情况可能需要更复杂的精细抽象。因此，我们考虑替代形状的神经抽象，即分段常数或非线性非多项式（具体来说，通过sigmoidal激活函数获得）。我们采用正式的归纳综合程序来生成神经抽象。",
    "tldr": "本研究探讨了神经抽象的效率和精确性之间的权衡问题，研究发现抽象的用途取决于具体场景，请求简单的粗略抽象同样会有其用途，而对于更复杂",
    "en_tdlr": "This study investigates the trade-off between efficiency and precision of neural abstractions, and finds that the utility of an abstraction depends on its use. Coarse abstractions may be sufficient for some scenarios, while more complex and refined abstractions are required for others."
}