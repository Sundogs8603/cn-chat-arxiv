{
    "title": "Weighted variation spaces and approximation by shallow ReLU networks. (arXiv:2307.15772v1 [stat.ML])",
    "abstract": "We investigate the approximation of functions $f$ on a bounded domain $\\Omega\\subset \\mathbb{R}^d$ by the outputs of single-hidden-layer ReLU neural networks of width $n$. This form of nonlinear $n$-term dictionary approximation has been intensely studied since it is the simplest case of neural network approximation (NNA). There are several celebrated approximation results for this form of NNA that introduce novel model classes of functions on $\\Omega$ whose approximation rates avoid the curse of dimensionality. These novel classes include Barron classes, and classes based on sparsity or variation such as the Radon-domain BV classes.  The present paper is concerned with the definition of these novel model classes on domains $\\Omega$. The current definition of these model classes does not depend on the domain $\\Omega$. A new and more proper definition of model classes on domains is given by introducing the concept of weighted variation spaces. These new model classes are intrinsic to th",
    "link": "http://arxiv.org/abs/2307.15772",
    "context": "Title: Weighted variation spaces and approximation by shallow ReLU networks. (arXiv:2307.15772v1 [stat.ML])\nAbstract: We investigate the approximation of functions $f$ on a bounded domain $\\Omega\\subset \\mathbb{R}^d$ by the outputs of single-hidden-layer ReLU neural networks of width $n$. This form of nonlinear $n$-term dictionary approximation has been intensely studied since it is the simplest case of neural network approximation (NNA). There are several celebrated approximation results for this form of NNA that introduce novel model classes of functions on $\\Omega$ whose approximation rates avoid the curse of dimensionality. These novel classes include Barron classes, and classes based on sparsity or variation such as the Radon-domain BV classes.  The present paper is concerned with the definition of these novel model classes on domains $\\Omega$. The current definition of these model classes does not depend on the domain $\\Omega$. A new and more proper definition of model classes on domains is given by introducing the concept of weighted variation spaces. These new model classes are intrinsic to th",
    "path": "papers/23/07/2307.15772.json",
    "total_tokens": 853,
    "translated_title": "加权变差空间与浅层ReLU网络的逼近",
    "translated_abstract": "本文研究了在有界域Ω⊂Rd上，通过宽度为n的单隐藏层ReLU神经网络的输出来逼近函数f的情况。这种非线性的n项字典逼近已经得到广泛研究，因为它是神经网络逼近(NNA)的最简单情况。对于这种NNA形式，有几个著名的逼近结果，引入了在Ω上的函数的新型模型类，其逼近速率避免了维数灾难。这些新型模型类包括Barron类和基于稀疏性或变差的类，例如Radon域BV类。本文关注于在域Ω上定义这些新型模型类。当前这些模型类的定义不依赖于域Ω。通过引入加权变差空间的概念，给出了关于域的更恰当的模型类定义。这些新型模型类与域本身相关。",
    "tldr": "本文研究了在有界域上通过单隐藏层ReLU网络逼近函数的问题，介绍了新的模型类定义加权变差空间，该定义与域本身相关。",
    "en_tdlr": "This paper investigates the approximation of functions on a bounded domain by single-hidden-layer ReLU neural networks and proposes a new model class definition called weighted variation spaces, which is intrinsic to the domain itself."
}