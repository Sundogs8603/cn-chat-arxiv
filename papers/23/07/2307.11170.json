{
    "title": "UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition. (arXiv:2307.11170v1 [cs.CL])",
    "abstract": "Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be comb",
    "link": "http://arxiv.org/abs/2307.11170",
    "context": "Title: UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition. (arXiv:2307.11170v1 [cs.CL])\nAbstract: Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be comb",
    "path": "papers/23/07/2307.11170.json",
    "total_tokens": 827,
    "translated_title": "UMLS-KGI-BERT：基于转化器的生物医学实体识别中的数据中心知识整合",
    "translated_abstract": "在应用领域的自然语言处理中，预训练的转化器语言模型（LMs）已成为主导范式。这些模型在信息提取、问答、情感分析、文档分类等任务上取得了最先进的性能。在生物医学领域，已经取得了重要进展，将该范式适应于需要结合领域特定知识以及语言的统计建模的NLP任务。具体而言，该领域的研究重点是如何最好地构建LMs，既考虑医学文本中令牌分布的模式，又考虑UMLS等术语资源中包含的丰富结构化信息。本研究提出了一种数据中心范式，通过从UMLS中提取文本序列来丰富生物医学转换器编码器LMs的语言表示。这使得可以应用基于图的学习目标。",
    "tldr": "这项工作提出了一种数据中心范式，通过从UMLS中提取文本序列来丰富生物医学转换器编码器LMs的语言表示。"
}