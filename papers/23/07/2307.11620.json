{
    "title": "Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning wi",
    "link": "http://arxiv.org/abs/2307.11620",
    "context": "Title: Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning wi",
    "path": "papers/23/07/2307.11620.json",
    "total_tokens": 954,
    "translated_title": "离线多智能体强化学习与隐式全局到局部价值正则化",
    "translated_abstract": "近年来，离线强化学习（RL）由于其在无需环境交互的离线数据集上学习策略的能力而受到了广泛关注。尽管在单智能体环境中取得了一些成功，但离线多智能体强化学习（MARL）仍然是一个挑战。庞大的联合状态-动作空间和紧密耦合的多智能体行为为离线策略优化增加了额外的复杂性。大多数现有的离线MARL研究仅在单个智能体上应用与离线数据相关的正则化，而未完全考虑全局层面上的多智能体系统。在这项工作中，我们提出了OMIGA，一种新的离线多智能体强化学习算法，具有隐式全局到局部价值正则化。OMIGA提供了一个原则性的框架，将全局层面的价值正则化转化为等效的隐式局部价值正则化，并同时实现样本内学习，从而优雅地桥接了多智能体价值分解和策略学习。",
    "tldr": "这项工作提出了一种名为OMIGA的离线多智能体强化学习算法，通过隐式的全局到局部价值正则化，解决了离线多智能体强化学习中的挑战，并优雅地桥接了多智能体价值分解和策略学习。",
    "en_tdlr": "This work presents a novel offline multi-agent reinforcement learning algorithm called OMIGA, which tackles the challenges of offline MARL through implicit global-to-local value regularization and elegantly bridges multi-agent value decomposition and policy learning."
}