{
    "title": "Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])",
    "abstract": "Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne",
    "link": "http://arxiv.org/abs/2307.02632",
    "context": "Title: Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])\nAbstract: Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne",
    "path": "papers/23/07/2307.02632.json",
    "total_tokens": 856,
    "translated_title": "Q-Learning 的稳定性通过设计和乐观性",
    "translated_abstract": "自从20世纪80年代Chris Watkins的论文中介绍以来，Q-learning已成为强化学习工具包中的重要组成部分。本文部分是关于随机逼近和Q-learning的教程，提供了关于INFORMS APS发布的第一届应用概率信托全体大会的详细信息。该论文还提出了确保这些算法的稳定性和可能加速收敛的新方法，以及其他设置中的随机逼近。两个贡献是全新的：1. Q-learning在线性函数逼近下的稳定性一直是一个有待研究的话题。结果表明，通过适当的乐观训练和修改后的Gibbs策略，可以存在满足投影Bellman方程的解，并且该算法是稳定的（参数估计有界）。收敛性仍然是众多待研究的问题之一。2. 新的优化方法在小批量执行中改善了逼近算法的迭代速度。",
    "tldr": "本文主要介绍了Q-learning在强化学习中的重要性，以及使用乐观性训练和修改后的策略解决Q-learning的稳定性问题和算法收敛加速问题的方法。"
}