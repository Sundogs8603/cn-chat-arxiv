{
    "title": "VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])",
    "abstract": "Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. O",
    "link": "http://arxiv.org/abs/2307.05356",
    "context": "Title: VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])\nAbstract: Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. O",
    "path": "papers/23/07/2307.05356.json",
    "total_tokens": 918,
    "translated_title": "VisText：一个丰富语义的图表标题评测基准",
    "translated_abstract": "描述或解释图表的标题有助于提高对图表数据的回忆和理解，并为视觉障碍人士提供更易接触的媒介。然而，当前自动生成这类标题的方法难以表达图表的感知或认知特征（如复杂的趋势和模式）。为此，我们介绍了VisText：一个由12,441个图表和标题对组成的数据集，描述了图表的构造，报告了关键统计数据，并识别了感知和认知现象。在VisText中，一个图表有三种表示形式：光栅化图像、支持数据表格和场景图——类似于Web页面的文档对象模型（DOM）的图表可视元素的分层表示。为了评估VisText的影响，我们在图表标题生成任务上对最先进的语言模型进行了微调，并应用了预处理来产生传达语义内容变化的标题。",
    "tldr": "本研究介绍了VisText，一个丰富语义的图表标题评测基准，该数据集包含了12,441个图表和标题对，描述了图表的构造、报告了关键统计数据，并识别了感知和认知现象。通过在图表标题生成任务上微调语言模型并应用预处理，我们评估了VisText的影响。",
    "en_tdlr": "This study introduces VisText, a benchmark for semantically rich chart captioning. The dataset consists of 12,441 pairs of charts and captions that describe the construction of the charts, report key statistics, and identify perceptual and cognitive phenomena. By fine-tuning language models and applying prefix-tuning, the impact of VisText is evaluated in the generation of captions for charts."
}