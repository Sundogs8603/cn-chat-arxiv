{
    "title": "Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])",
    "abstract": "We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric",
    "link": "http://arxiv.org/abs/2307.10710",
    "context": "Title: Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])\nAbstract: We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric",
    "path": "papers/23/07/2307.10710.json",
    "total_tokens": 954,
    "translated_title": "重参数化策略学习用于多模态轨迹优化",
    "translated_abstract": "我们研究了在高维连续动作空间中为增强学习 (RL) 参数化策略的挑战。我们的目标是开发一种多模态策略，克服了常用的高斯参数化的局限性。为了实现这一目标，我们提出了一个原则性的框架，将连续RL策略建模为最优轨迹的生成模型。通过将策略条件于潜变量，我们导出了一个新颖的变分上界作为优化目标，从而促进了对环境的探索。然后，我们提出了一种实用的基于模型的RL方法，称为重参数化策略梯度(RPG)，它利用了多模态策略参数化和学得的世界模型以实现强大的探索能力和高效的数据利用率。实证结果表明，我们的方法可以帮助代理在具有密集奖励的任务中避免局部最优，并通过整合物体为中心的编码，解决具有挑战性的稀疏奖励环境的问题。",
    "tldr": "本研究针对高维连续动作空间中的增强学习，提出了一种多模态策略参数化的重参数化框架，并基于此框架提出了一种重参数化策略梯度方法，能够在任务中避免局部最优，解决稀疏奖励环境中的挑战。",
    "en_tdlr": "This study proposes a reparameterized framework for reinforcement learning in high-dimensional continuous action spaces, with a focus on multimodal policy parameterization. A practical model-based RL method is introduced, which leverages this framework to overcome local optima and solve challenges in sparse-reward environments."
}