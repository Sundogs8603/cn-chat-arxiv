{
    "title": "qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])",
    "abstract": "We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We p",
    "link": "http://arxiv.org/abs/2307.09025",
    "context": "Title: qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])\nAbstract: We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We p",
    "path": "papers/23/07/2307.09025.json",
    "total_tokens": 911,
    "translated_title": "qecGPT：使用生成式预训练转换器对量子纠错码进行解码",
    "translated_abstract": "我们提出了一个用生成建模解码量子纠错码的通用框架。该模型利用自回归神经网络，特别是使用Transformer学习逻辑运算符和综合的联合概率。该训练是无监督的，不需要标注的训练数据，并且被称为预训练。在预训练之后，模型可以高效地计算给定综合的逻辑运算符的可能性，使用最大似然解码。它可以直接生成最可能的逻辑运算符，计算复杂度为$\\mathcal O(2k)$，其中$k$为逻辑量子比特的数量，这比常规的最大似然解码算法$\\mathcal O(4^k)$更优。基于预训练模型，我们进一步提出了通过直接采样稳定子算符来更准确地获得给定综合的逻辑运算符可能性的改进方法。",
    "tldr": "qecGPT是一个通用框架，用于用生成模型解码量子纠错码。该模型利用Transformers学习逻辑运算符和综合的联合概率，在无监督预训练后可以高效计算和生成最可能的逻辑运算符，比传统方法更快更准确。"
}