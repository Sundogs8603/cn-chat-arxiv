{
    "title": "Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])",
    "abstract": "Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re",
    "link": "http://arxiv.org/abs/2307.01482",
    "context": "Title: Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])\nAbstract: Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re",
    "path": "papers/23/07/2307.01482.json",
    "total_tokens": 931,
    "translated_title": "Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列",
    "translated_abstract": "建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。",
    "tldr": "提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。",
    "en_tdlr": "A compact neural network prediction model was proposed, which is defined by node identification, dense encoder-decoder, and message-passing layer, without complex sequential modules. The model demonstrated effectiveness and efficiency in empirical evaluation."
}