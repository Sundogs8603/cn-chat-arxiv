{
    "title": "Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])",
    "abstract": "In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - ",
    "link": "http://arxiv.org/abs/2307.08913",
    "context": "Title: Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])\nAbstract: In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - ",
    "path": "papers/23/07/2307.08913.json",
    "total_tokens": 870,
    "translated_title": "自监督学习中投影头的稀疏性研究",
    "translated_abstract": "最近几年，自监督学习（SSL）已成为从无标签数据中提取有价值表示的一种有希望的方法。其中一种成功的SSL方法是对比学习，其旨在将正样本聚集在一起，将负样本推开。许多当前的对比学习方法都使用参数化的投影头。通过实证分析和理论探索，我们对投影头的内部机制及其与维度折叠现象的关系进行了深入研究。我们的研究结果表明，投影头通过在一个投影子空间中执行对比损失，提升表示的质量。因此，我们提出一个假设，即在最小化一个小批量数据的对比损失时，只有一部分特征是必要的。理论分析进一步表明，稀疏的投影头可以增强泛化性能，因此我们引入SparseHead这一方法。",
    "tldr": "该论文研究了自监督学习中投影头的稀疏性，发现通过在投影子空间中执行对比损失可以提升表示的质量，建议只有一部分特征是必要的，而稀疏的投影头可以增强模型的泛化性能。",
    "en_tdlr": "This paper investigates the sparsity of the projection head in self-supervised learning and finds that performing contrastive loss in a projected subspace enhances representation quality. The authors suggest that only a subset of features is necessary and introduce SparseHead as a way to enhance model generalization."
}