{
    "title": "Improving Automatic Parallel Training via Balanced Memory Workload Optimization",
    "abstract": "arXiv:2307.02031v2 Announce Type: replace  Abstract: Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to imp",
    "link": "https://arxiv.org/abs/2307.02031",
    "context": "Title: Improving Automatic Parallel Training via Balanced Memory Workload Optimization\nAbstract: arXiv:2307.02031v2 Announce Type: replace  Abstract: Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to imp",
    "path": "papers/23/07/2307.02031.json",
    "total_tokens": 843,
    "translated_title": "通过平衡内存工作负载优化改进自动并行训练",
    "translated_abstract": "Transformer模型已成为实现各种应用领域最先进性能的领先方法，为高级大规模深度学习(DL)模型奠定了基础。然而，由于并行选项的丰富性，跨多个GPU有效训练这些模型仍然是一个复杂的挑战。现有的DL系统要么需要手动设计分布式训练计划，要么将并行组合限制在约束的搜索空间中。在本文中，我们提出了Galvatron-BMW，一个集成多个主流并行维度并自动确定最有效混合并行策略的新型系统框架。为了有效地遍历这个庞大的搜索空间，我们采用了一个基于直观见解的决策树方法进行分解和修剪。我们进一步利用动态规划搜索算法推出最优计划。",
    "tldr": "本文提出了Galvatron-BMW，一个新的系统框架，通过平衡内存工作负载优化，集成多个并行维度并自动识别最有效的混合并行策略，通过决策树方法和动态规划搜索算法来有效地处理复杂的训练挑战。",
    "en_tdlr": "This paper presents Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy, effectively addressing the complex training challenge through a decision tree approach and dynamic programming search algorithm."
}