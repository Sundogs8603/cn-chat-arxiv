{
    "title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])",
    "abstract": "Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different \"views\" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promi",
    "link": "http://arxiv.org/abs/2307.07951",
    "context": "Title: MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])\nAbstract: Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different \"views\" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promi",
    "path": "papers/23/07/2307.07951.json",
    "total_tokens": 840,
    "translated_title": "MinT: 通过多视角微调提升数学推理的泛化性能",
    "translated_abstract": "对于相对较小的语言模型（LM），在数学推理中进行推理仍然是一个重大挑战。许多当前方法专注于在数学推理中专门化LM，并且过度依赖于强大但低效的大型LM（LLM）所提供的知识蒸馏。在这项工作中，我们探索了一种避免过度依赖LLM教师的新思路，引入了一种利用具有不同标注风格的现有数学问题数据集的多视角微调方法。我们的方法将不同的标注格式视为不同的“视图”，并在模型训练中利用它们。通过将不同的指令附加到输入问题上，模型可以学习以灵活的方式生成不同格式的解决方案。实验证明，我们的策略使LLaMA-7B模型能够超越利用知识蒸馏的先前方法以及精心建立的基准。此外，所提出的方法使模型取得了活跃",
    "tldr": "MinT通过多视角微调方法，利用不同标注风格的数学问题数据集提升了数学推理中小型语言模型的泛化能力。",
    "en_tdlr": "MinT improves the generalization ability of small language models in mathematical reasoning by utilizing multi-view fine-tuning with diverse annotation styles."
}