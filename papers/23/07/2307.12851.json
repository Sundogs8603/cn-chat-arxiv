{
    "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization",
    "abstract": "arXiv:2307.12851v2 Announce Type: replace  Abstract: This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\\mathcal{O}(\\frac{1}{t",
    "link": "https://arxiv.org/abs/2307.12851",
    "context": "Title: Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization\nAbstract: arXiv:2307.12851v2 Announce Type: replace  Abstract: This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\\mathcal{O}(\\frac{1}{t",
    "path": "papers/23/07/2307.12851.json",
    "total_tokens": 927,
    "translated_title": "具有小初始化的两层ReLU网络中的早期神经元对齐",
    "translated_abstract": "本文研究了使用梯度流和小初始化对双层ReLU网络进行二元分类训练的问题。我们考虑一个带有良好分离的输入向量的训练数据集：任何具有相同标签的输入数据对正相关，具有不同标签的输入数据对负相关。我们的分析显示，在训练的早期阶段，第一层中的神经元试图与第二层上的权重对应的正数据或负数据对齐。对神经元方向动态的仔细分析使我们能够提供一个$\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$的时间上界，这是为了让所有神经元与输入数据良好对齐所需的时间，其中$n$是数据点的个数，$\\mu$衡量数据分离的程度。在早期对齐阶段之后，损失以$\\mathcal{O}(\\frac{1}{t})$的速度收敛到零。",
    "tldr": "神经元在两层ReLU网络中早期阶段会尝试与输入数据对齐，对齐时间上界为$\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$，在对齐阶段过后损失收敛速度为$\\mathcal{O}(\\frac{1}{t})$",
    "en_tdlr": "Neurons in the early phase of a two-layer ReLU network align with input data, with an alignment time bound of $\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$, and the loss converges at $\\mathcal{O}(\\frac{1}{t})$ rate after the alignment phase."
}