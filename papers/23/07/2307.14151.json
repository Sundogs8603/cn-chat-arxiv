{
    "title": "Learning Disentangled Discrete Representations. (arXiv:2307.14151v1 [cs.LG])",
    "abstract": "Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.",
    "link": "http://arxiv.org/abs/2307.14151",
    "context": "Title: Learning Disentangled Discrete Representations. (arXiv:2307.14151v1 [cs.LG])\nAbstract: Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.",
    "path": "papers/23/07/2307.14151.json",
    "total_tokens": 828,
    "translated_title": "学习解离的离散表示",
    "translated_abstract": "最近在图像生成、基于模型的增强学习和文本到图像生成方面取得了成功，这些都证明了离散潜在表示的经验优势，尽管其背后的原因尚不清楚。我们通过将标准的高斯变分自动编码器（VAE）替换为定制的分类变分自动编码器，探索了离散潜在空间和解离表示之间的关系。我们显示分类分布的底层网格结构减轻了与多变量高斯分布相关的旋转不变性问题，作为解离表示的高效归纳先验。我们提供了分析和实证结果，证明了离散VAE在学习解离表示方面的优势。此外，我们引入了第一个支持解离表示的无监督模型选择策略。",
    "tldr": "通过替换标准的高斯变分自动编码器，使用定制的分类变分自动编码器，我们发现离散潜在空间的底层网格结构可以有效缓解解离表示中的旋转不变性问题，并提供了优化解离表示的无监督模型选择策略。"
}