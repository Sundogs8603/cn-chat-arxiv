{
    "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])",
    "abstract": "This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.",
    "link": "http://arxiv.org/abs/2307.13304",
    "context": "Title: QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])\nAbstract: This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.",
    "path": "papers/23/07/2307.13304.json",
    "total_tokens": 965,
    "translated_title": "QuIP：具有保证的大型语言模型的2比特量化",
    "translated_abstract": "本研究探讨了大型语言模型（LLMs）中的训练后参数量化。我们介绍了一种新的基于无关处理（QuIP）的量化方法，该方法基于以下见解：量化从不相关的权重和 Hessian 矩阵中收益，即通过准确地将它们舍入为与坐标轴不对齐的方向，使得获取重要的量化结果。QuIP 包含两个步骤：（1）最小化二次近似目标的自适应舍入过程；（2）通过与随机正交矩阵相乘来确保权重和 Hessian 无关的高效预处理和后处理。我们通过第一次针对 LLM 规模的量化算法进行了理论分析，并且证明我们的理论也适用于现有方法 OPTQ。经验证实，我们的无关预处理改善了现有的多个量化算法，并首次实现了仅使用每个权重2比特的大型语言模型量化方法。",
    "tldr": "本文提出了一种新的基于无关处理的大型语言模型（LLMs）参数量化方法QuIP，通过使权重和Hessian矩阵与坐标轴不对齐，实现了准确的量化结果。经过经验实验，我们发现我们的方法改善了现有的量化算法，并且首次在仅使用两比特的情况下获得了可行的LLM量化结果。"
}