{
    "title": "Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows. (arXiv:2307.00144v1 [cs.LG])",
    "abstract": "Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This \"implicit bias\" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of \"conservation laws\", which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms (implemented in SageMath) to",
    "link": "http://arxiv.org/abs/2307.00144",
    "context": "Title: Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows. (arXiv:2307.00144v1 [cs.LG])\nAbstract: Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This \"implicit bias\" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of \"conservation laws\", which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms (implemented in SageMath) to",
    "path": "papers/23/07/2307.00144.json",
    "total_tokens": 938,
    "translated_title": "遵守法律并遵循流程：梯度流的守恒定律",
    "translated_abstract": "理解梯度下降动力学的几何特性是解密非常大的机器学习模型最近成功的关键因素。一个引人注目的观察是，超参数调节的模型保留了一些优化初始化的特性。这种“隐式偏差”被认为是训练模型具有有利特性并能解释其良好泛化特性的原因。本文的目的有三个。首先，我们严格介绍了“守恒定律”的定义和基本性质，这些守恒定律是在给定模型（例如具有给定架构的ReLU网络）的梯度流中独立保持的最大量。不论使用任何训练数据和任何损失函数。然后，我们解释如何通过对模型的雅可比矩阵生成的李代数进行有限维代数运算，找到这些数量的确切数量。最后，我们提供了在SageMath中实现的算法。",
    "tldr": "本文通过定义和研究梯度流中的守恒定律，以及在模型的雅可比矩阵生成的李代数上进行有限维代数运算，揭示了超参数调节的模型保留了一些优化初始化的特性，这可能解释了训练模型具有良好泛化特性的原因。"
}