{
    "title": "Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning. (arXiv:2307.13415v1 [eess.SY])",
    "abstract": "Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL fra",
    "link": "http://arxiv.org/abs/2307.13415",
    "context": "Title: Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning. (arXiv:2307.13415v1 [eess.SY])\nAbstract: Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL fra",
    "path": "papers/23/07/2307.13415.json",
    "total_tokens": 961,
    "translated_title": "通过分层强化学习实现通信高效的URLLC服务编排",
    "translated_abstract": "超可靠低延迟通信（URLLC）服务被视为实现对5G中具有严格可靠性和延迟要求的应用场景的关键。一种实现URLLC服务的方法是利用强化学习（RL）来高效分配无线资源。然而，使用传统的RL方法，决策变量（尽管部署在不同的网络层）通常在同一个控制循环中进行优化，导致控制循环的延迟存在显著的实际限制，以及过多的信令和能量消耗。在本文中，我们提出了一个多智能体分层强化学习（HRL）框架，可以实现具有不同控制循环时间尺度的多级策略。具有更快控制循环的智能体部署在靠近基站的位置，而具有较慢控制循环的智能体则部署在边缘或靠近核心网络的位置，为低级动作提供高级指导。在一个来自现有技术的用例中，借助我们的HRL框架，我们减少了控制循环的延迟，信令和能量消耗。",
    "tldr": "本文提出了一个分层强化学习框架，通过不同控制循环时间尺度的多级策略，实现了高效的URLLC服务编排，减少了控制循环的延迟，信令和能量消耗。",
    "en_tdlr": "This paper proposes a hierarchical reinforcement learning framework that achieves efficient URLLC service orchestrations by implementing multi-level policies with different control loop timescales, reducing the control loop delay, signaling, and energy consumption."
}