{
    "title": "Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])",
    "abstract": "In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th",
    "link": "http://arxiv.org/abs/2307.06620",
    "context": "Title: Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])\nAbstract: In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th",
    "path": "papers/23/07/2307.06620.json",
    "total_tokens": 868,
    "translated_title": "在线分布式学习与量化有限时间协作",
    "translated_abstract": "本文研究在线分布式学习问题。在线分布式学习是指在分布式数据源上训练学习模型的过程。在我们的设置中，一组代理需要合作地从流数据中训练学习模型。与联邦学习不同，所提出的方法不依赖于中央服务器，而仅依靠代理之间的点对点通信。该方法经常用于数据由于隐私、安全或成本原因不能移动到集中位置的场景。为了克服缺少中央服务器的问题，我们提出了一个分布式算法，该算法依赖于一个量化的、有限时间的协作协议来聚合本地训练的模型。此外，我们的算法允许在本地训练过程中使用随机梯度。随机梯度是使用本地训练数据的随机抽样子集计算的，这使得所提出的算法更加高效和可扩展。",
    "tldr": "本文研究了一种在线分布式学习问题，提出了一种依靠量化、有限时间协作协议的分布式算法来聚合本地训练的模型，并允许使用随机梯度来提高效率和可扩展性。",
    "en_tdlr": "This paper studies online distributed learning problems and proposes a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate locally trained models, allowing the use of stochastic gradients to improve efficiency and scalability."
}