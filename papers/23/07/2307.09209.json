{
    "title": "Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])",
    "abstract": "We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \\textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.",
    "link": "http://arxiv.org/abs/2307.09209",
    "context": "Title: Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])\nAbstract: We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \\textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.",
    "path": "papers/23/07/2307.09209.json",
    "total_tokens": 911,
    "translated_title": "自动化的残障主义：探索情感分析和毒性检测模型中的明显残障偏见",
    "translated_abstract": "我们分析情感分析和毒性检测模型，以检测对残障人士的明显偏见。我们采用扰动敏感性分析的偏见识别框架，研究社交媒体平台上与残障人士相关的对话，特别是Twitter和Reddit，在真实社交环境中了解残障偏见是如何传播的。然后，我们创建了“情感中的偏见识别测试”（BITS）语料库，以量化任何情感分析和毒性检测模型中的明显残障偏见。我们的研究使用BITS揭示了四种开放的AIaaS（AI即服务）情感分析工具（TextBlob，VADER，Google Cloud Natural Language API，DistilBERT）和两种毒性检测模型（两个版本的Toxic-BERT）中存在显着的偏见。",
    "tldr": "该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。",
    "en_tdlr": "This paper examines sentiment analysis and toxicity detection models to detect explicit bias against people with disability. The study analyzes conversations on social media platforms and creates a test corpus to quantify the bias. The findings indicate significant bias in the analyzed models."
}