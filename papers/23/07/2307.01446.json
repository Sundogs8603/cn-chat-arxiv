{
    "title": "On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])",
    "abstract": "Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine",
    "link": "http://arxiv.org/abs/2307.01446",
    "context": "Title: On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])\nAbstract: Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine",
    "path": "papers/23/07/2307.01446.json",
    "total_tokens": 1012,
    "translated_title": "关于条件和组合语言模型可微提示的论文",
    "translated_abstract": "提示已被证明是一种有效的方法，用于使预训练语言模型（PLM）在下游任务中表现出色。提示可以由人工设计的词序列或学习得到的连续嵌入来表示。在这项工作中，我们研究了条件和组合的可微提示。我们提出了一个新模型，Prompt Production System（PRopS），它学习将任务说明或输入元数据转化为连续的提示，从而激发PLM产生任务特定的输出。我们的模型使用基于我们对于产品系统的神经形式化的模块化网络结构，这使得模型能够学习离散规则——神经函数，这些函数学习专门将特定的提示输入模式转化为特定的输出，使其适用于组合式迁移学习和少样本学习。我们进行了广泛的实证和理论分析，并展示了PRopS始终超越其他PLM适应技术，并且通常改进了完全微调的方法。",
    "tldr": "本论文研究了条件和组合的可微提示方法，提出了Prompt Production System（PRopS）模型，通过将任务说明或输入元数据转化为连续的提示，使预训练语言模型（PLM）能够生成任务特定的输出。该模型利用了神经网络结构和离散规则的学习，适用于组合式迁移学习和少样本学习。实证和理论分析表明，PRopS在PLM适应中始终优于其他技术，并且通常改进了完全微调的方法。",
    "en_tdlr": "This paper investigates conditional and compositional differentiable prompting methods and proposes the PRopS model, which transforms task instructions or input metadata into continuous prompts to elicit task-specific outputs from a pretrained language model (PLM). The model utilizes a modular network structure and learning of discrete rules, making it suitable for compositional transfer learning and few-shot learning. Empirical and theoretical analysis demonstrate that PRopS consistently outperforms other PLM adaptation techniques and often improves upon fully fine-tuning."
}