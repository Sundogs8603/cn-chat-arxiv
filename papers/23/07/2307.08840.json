{
    "title": "Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])",
    "abstract": "Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th",
    "link": "http://arxiv.org/abs/2307.08840",
    "context": "Title: Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])\nAbstract: Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th",
    "path": "papers/23/07/2307.08840.json",
    "total_tokens": 921,
    "translated_title": "使用基于贝叶斯安全策略学习的机会约束优化方法：在越南战争期间的军事安全评估中的应用",
    "translated_abstract": "在高风险决策场景，如刑事司法、医学和公共政策中，常常使用算法和数据驱动的决策和建议。本研究探讨了在越南战争期间，是否有可能改进一种安全评估算法，并使用在其引入后立即测量到的结果进行研究。这个实证应用提出了在高风险算法决策中经常遇到的几个方法学挑战。首先，在实施新算法之前，至关重要的是对更糟糕的结果风险进行表征和控制。其次，现有算法是确定性的，学习新算法需要透明的外推。第三，现有算法涉及常见但难以优化的离散决策表。为了应对这些挑战，我们引入了平均条件风险（ACRisk），首先量化了产生较差结果风险的方法。",
    "tldr": "本研究提出了使用基于贝叶斯安全策略学习的机会约束优化方法，在越南战争期间的军事安全评估中进行实证研究。研究结果对于解决高风险算法决策中的挑战具有重要意义。",
    "en_tdlr": "This study proposes a Bayesian safe policy learning approach with chance constrained optimization for empirical research in military security assessment during the Vietnam War, aiming to address challenges in high-stakes algorithmic decision-making and provide important insights for risk characterization and control."
}