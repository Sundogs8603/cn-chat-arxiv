{
    "title": "Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search. (arXiv:2307.15621v1 [cs.LG])",
    "abstract": "In this work, we show that simultaneously training and mixing neural networks is a promising way to conduct Neural Architecture Search (NAS). For hyperparameter optimization, reusing the partially trained weights allows for efficient search, as was previously demonstrated by the Population Based Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where architectures are improved during training by replacing poorly-performing networks in a population with the result of mixing well-performing ones and inheriting the weights using the shrink-perturb technique. After PBT-NAS terminates, the created networks can be directly used without retraining. PBT-NAS is highly parallelizable and effective: on challenging tasks (image generation and reinforcement learning) PBT-NAS achieves superior performance compared to baselines (random search and mutation-based PBT).",
    "link": "http://arxiv.org/abs/2307.15621",
    "context": "Title: Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search. (arXiv:2307.15621v1 [cs.LG])\nAbstract: In this work, we show that simultaneously training and mixing neural networks is a promising way to conduct Neural Architecture Search (NAS). For hyperparameter optimization, reusing the partially trained weights allows for efficient search, as was previously demonstrated by the Population Based Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where architectures are improved during training by replacing poorly-performing networks in a population with the result of mixing well-performing ones and inheriting the weights using the shrink-perturb technique. After PBT-NAS terminates, the created networks can be directly used without retraining. PBT-NAS is highly parallelizable and effective: on challenging tasks (image generation and reinforcement learning) PBT-NAS achieves superior performance compared to baselines (random search and mutation-based PBT).",
    "path": "papers/23/07/2307.15621.json",
    "total_tokens": 899,
    "translated_title": "使用收缩扰动技术改进种群训练过程中的架构混合，提高神经架构搜索效果的研究",
    "translated_abstract": "本文展示了同时训练和混合神经网络是进行神经架构搜索（NAS）的一种有前景的方法。对于超参数优化，通过重复使用部分训练好的权重，可以实现高效的搜索，这在种群训练（PBT）算法中已经有所展示。我们提出了PBT-NAS，这是一种将种群训练（PBT）算法适应到NAS的方法，通过将表现差的网络替换为混合表现好的网络，并使用收缩扰动技术继承权重来改进架构。在PBT-NAS结束后，创建的网络可以直接使用而无需重新训练。PBT-NAS具有高度可并行化和有效性，对于挑战性的任务（图像生成和强化学习）来说，PBT-NAS相比于基准（随机搜索和基于突变的PBT）实现了更优秀的性能。",
    "tldr": "本文展示了一种名为PBT-NAS的方法，通过同时训练和混合神经网络，并使用收缩扰动技术改进架构，以实现有效的神经架构搜索。PBT-NAS在图像生成和强化学习等挑战性任务中表现出优越的性能。",
    "en_tdlr": "This paper presents a method called PBT-NAS, which achieves effective neural architecture search by simultaneously training and mixing neural networks and improving architectures using the shrink-perturb technique. PBT-NAS achieves superior performance in challenging tasks such as image generation and reinforcement learning."
}