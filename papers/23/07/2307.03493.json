{
    "title": "ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])",
    "abstract": "Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu",
    "link": "http://arxiv.org/abs/2307.03493",
    "context": "Title: ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])\nAbstract: Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu",
    "path": "papers/23/07/2307.03493.json",
    "total_tokens": 852,
    "translated_title": "ITA:一种基于量化的Transformer的能效高的Attention和Softmax加速器",
    "translated_abstract": "Transformer网络已经成为自然语言处理任务的最先进方法，并在计算机视觉和音频处理等其他领域受到欢迎。然而，Transformer模型的高算术强度、大内存需求和复杂数据流依赖导致了其有效硬件加速面临新的挑战。在这项工作中，我们提出了ITA，一种针对嵌入式系统上高效推理的Transformer和相关模型的新型加速器架构，通过利用8位量化和仅基于整数值的创新Softmax实现。通过在流模式下实时计算，我们的Softmax实现最大程度地减少了数据移动和能量消耗。ITA在能效方面与最先进的Transformer加速器保持竞争力，达到了16.9 TOPS/W，同时在面积效率方面以5.93 TOPS/mm$^2$的成绩超越了它们，在22纳米完全耗尽的硅上。",
    "tldr": "ITA是一种基于量化的Transformer的能效高的Attention和Softmax加速器。通过利用8位量化和仅基于整数值的创新Softmax实现，ITA实现了高能效的推理，在16.9 TOPS/W的能效上超过了最先进的Transformer加速器，并在5.93 TOPS/mm$^2$的面积效率上超越了它们。"
}