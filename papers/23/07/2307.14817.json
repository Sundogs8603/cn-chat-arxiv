{
    "title": "Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])",
    "abstract": "In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models' ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust cla",
    "link": "http://arxiv.org/abs/2307.14817",
    "context": "Title: Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])\nAbstract: In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models' ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust cla",
    "path": "papers/23/07/2307.14817.json",
    "total_tokens": 872,
    "translated_title": "参考生成模型：它们是否经得起时间的考验？",
    "translated_abstract": "近年来，许多自然语言处理研究仅关注性能的提高。本文从语言和科学角度出发，针对自然语言处理中生成指代表达的任务进行了研究。我们以GREC为案例研究，GREC是十多年前关于该主题的一系列英语共享任务的综合集合。我们研究了如果我们以更现实的数据集和更高级的方法进行评估，模型的表现会如何。我们使用不同的评估指标和特征选择实验来测试模型。我们得出结论，由于语料库和评估指标的选择对结果产生了很大影响，GREC不再被视为可靠评估模型模拟人类参考生成能力的工具。我们的结果还表明，与传统的机器学习模型相比，预训练语言模型对语料库的选择不太依赖，因此具有更强的鲁棒性。",
    "tldr": "本研究通过分析目前的自然语言处理研究中的参考生成模型，发现GREC不再是一个可靠的评估工具，并证明了预训练语言模型在此任务中具有较强的鲁棒性。",
    "en_tdlr": "This study analyzes the current models of reference production in natural language processing and finds that GREC is no longer a reliable evaluation tool. It also demonstrates that pre-trained language models exhibit stronger robustness in this task."
}