{
    "title": "Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])",
    "abstract": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, ",
    "link": "http://arxiv.org/abs/2307.03381",
    "context": "Title: Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])\nAbstract: Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, ",
    "path": "papers/23/07/2307.03381.json",
    "total_tokens": 864,
    "translated_title": "向小型Transformer模型教授算术",
    "translated_abstract": "大型语言模型如GPT-4，当在大量文本数据上进行训练时，即使这些任务并未直接编码在无监督的下一个标记预测目标中，也展现出了在通用任务（如基本算术）上的新兴能力。本研究调查了如何让从随机初始化训练的小型transformers模型，通过下一个标记预测目标高效学习加法、乘法和诸如平方根等基本算术运算。我们首先证明传统训练数据对于算术学习来说并不是最有效的，通过简单的格式变化可以显著提高准确性。这导致了训练数据规模的尖锐相变，其中一些情况可以通过与低秩矩阵补全的联系来解释。在此基础上，我们在包括中间步骤结果的链式思维样式数据上进行训练。即使完全无先验训练，模型仍然可以学习算术运算。",
    "tldr": "本研究研究了如何通过训练小型Transformer模型在没有先验训练的情况下高效学习基本算术运算，并提出了一种通过格式变化和使用链式思维样式数据来提高准确性的方法。",
    "en_tdlr": "This study investigates how small transformers can efficiently learn basic arithmetic operations without pretraining, and proposes a method to improve accuracy through formatting changes and using chain-of-thought style data."
}