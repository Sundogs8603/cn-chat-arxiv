{
    "title": "Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])",
    "abstract": "We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.",
    "link": "http://arxiv.org/abs/2307.10504",
    "context": "Title: Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])\nAbstract: We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.",
    "path": "papers/23/07/2307.10504.json",
    "total_tokens": 937,
    "translated_title": "在图像表示中识别可解释子空间",
    "translated_abstract": "我们提出了一种解释图像表示特征的可解释性框架——FALCON（Automatic Feature Explanation using Contrasting Concepts）。对于目标特征，FALCON使用一个大型的字幕数据集（如LAION-400m）和一个预训练的视觉语言模型（如CLIP）对其高度激活的裁剪图像进行字幕生成。每个字幕中的单词都经过评分和排序，从而得到了与目标特征密切相关的少数共享的、人类可理解的概念。FALCON还使用低激活的（对立的）图像应用对比解释，以消除虚假概念。尽管许多现有方法独立解释特征，但我们观察到在最先进的自监督和监督模型中，不到20%的表示空间可以通过单个特征进行解释。我们展示了当一组特征一起研究时，更大的空间中的特征变得更易解释，并可以通过FALCON得到高阶评分概念的解释。",
    "tldr": "FALCON是一个解释图像表示特征的框架，可以通过使用字幕数据集和视觉语言模型来生成人类可理解的概念，并通过对比解释消除虚假概念。在较大的空间中，特征通过研究组合可以更易解释和高阶评分概念的解释。",
    "en_tdlr": "FALCON is a framework for interpreting image representation features, which generates human-understandable concepts using captioning datasets and vision-language models, and eliminates spurious concepts through contrastive interpretation. Features in larger spaces become more interpretable and can be explained with high-order scoring concepts through group study."
}