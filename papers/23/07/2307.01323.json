{
    "title": "Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])",
    "abstract": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
    "link": "http://arxiv.org/abs/2307.01323",
    "context": "Title: Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])\nAbstract: Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
    "path": "papers/23/07/2307.01323.json",
    "total_tokens": 890,
    "translated_title": "向语义丰富化的有效语音表示方法迈进",
    "translated_abstract": "在过去几年中，自监督学习的语音表示在解决口语理解任务时已成为传统表面表示的有效替代品。与此同时，利用大规模文本数据训练的多语言模型被引入以编码语言无关的语义信息。最近，SAMU-XLSR方法提出了一种利用这种文本模型从而使多语言语音表示增加语言无关语义的方法。本研究针对具有挑战性的口语理解任务并考虑计算成本，通过专注于对下游任务的少量转录数据的特定领域语义丰富化，对SAMU-XLSR模型进行了研究。此外，我们展示了在资源有限的语言可移植性方面，使用同领域的法语和意大利语基准的好处，还探索了丰富的SAMU-XLSR的跨领域能力。",
    "tldr": "这项研究通过在具有挑战性的口语理解任务上专注于少量转录数据的特定领域语义丰富化，对SAMU-XLSR模型进行了改进，同时还探索了在资源有限的语言可移植性方面的优势和丰富的SAMU-XLSR的跨领域能力。",
    "en_tdlr": "This study focuses on the specific in-domain semantic enrichment of the SAMU-XLSR model by utilizing a small amount of transcribed data from a challenging Spoken Language Understanding task. It also explores the advantages of using same-domain benchmarks for low-resource language portability and the cross-domain capacities of the enriched SAMU-XLSR."
}