{
    "title": "MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)",
    "abstract": "Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. ",
    "link": "http://arxiv.org/abs/2307.02227",
    "context": "Title: MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)\nAbstract: Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. ",
    "path": "papers/23/07/2307.02227.json",
    "total_tokens": 850,
    "translated_title": "MAE-DFER：高效的面具自编码器用于自监督动态面部表情识别",
    "translated_abstract": "动态面部表情识别（DFER）对于智能和移情的机器发展至关重要。本文受到最近面具自编码器（例如VideoMAE）的前所未有的成功启发，提出了一种新颖的自监督方法MAE-DFER，该方法利用大规模的自监督预训练和丰富的未标记数据大力推进DFER的发展。由于VideoMAE中使用的Vanilla Vision Transformer（ViT）在微调过程中需要大量计算，因此MAE-DFER开发了一种高效的局部-全局交互Transformer（LGI-Former）作为编码器。此外，在VideoMAE的独立外观内容重构基础上，MAE-DFER还引入了显式的时间面部运动建模，以鼓励LGI-Former挖掘静态外观和动态运动信息。",
    "tldr": "本研究提出了一种高效的自监督方法MAE-DFER，通过大规模自监督预训练和显式时间面部运动建模，推进了动态面部表情识别的发展。",
    "en_tdlr": "This paper proposes an efficient self-supervised method called MAE-DFER, which advances the development of dynamic facial expression recognition through large-scale self-supervised pre-training and explicit temporal facial motion modeling."
}