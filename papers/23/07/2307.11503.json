{
    "title": "General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])",
    "abstract": "Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.",
    "link": "http://arxiv.org/abs/2307.11503",
    "context": "Title: General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])\nAbstract: Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.",
    "path": "papers/23/07/2307.11503.json",
    "total_tokens": 787,
    "translated_title": "在协变量偏移自适应中的一般正则化方法",
    "translated_abstract": "样本重加权是纠正在再生核希尔伯特空间(RKHS)中由未来数据分布与训练数据分布不同引起的最小二乘学习算法错误的最常用方法之一。在实际情况中，样本权重是由未来数据分布对训练数据分布的估计Radon-Nikod\\'ym导数的值确定的。本研究回顾了在RKHS中重新加权核回归的已知误差界限，并通过组合得到新的结果。我们在弱平滑条件下表明，为了实现与标准监督学习中数据分布差异相同精度的样本数目要比现有的分析证明的少。",
    "tldr": "本文研究了协变量偏移自适应中的一般正则化方法，并通过组合已有结果得到了新的结果。在弱平滑条件下证明了实现与标准监督学习中相同精度所需的样本量要比现有分析证明的少。",
    "en_tdlr": "This paper investigates general regularization methods in covariate shift adaptation and obtains novel results through combination of existing analyses. It shows that under weak smoothness conditions, fewer samples are needed to achieve the same level of accuracy as in standard supervised learning with no differences in data distributions, compared to existing analyses."
}