{
    "title": "DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])",
    "abstract": "Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima",
    "link": "http://arxiv.org/abs/2307.07652",
    "context": "Title: DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])\nAbstract: Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima",
    "path": "papers/23/07/2307.07652.json",
    "total_tokens": 867,
    "translated_title": "DIGEST: 快速和通信高效的分散学习与本地更新",
    "translated_abstract": "两种广泛考虑的分散学习算法是Gossip和基于随机游走的学习。Gossip算法（同步和异步版本）存在较高的通信成本，而基于随机游走的学习则会增加收敛时间。在本文中，我们设计了一种快速和通信有效的异步分散学习机制DIGEST，利用了Gossip和随机游走的思想，并专注于随机梯度下降（SGD）。DIGEST是一个基于本地SGD算法的异步分散算法，它最初是为通信高效的集中式学习而设计的。我们设计了单流和多流的DIGEST，当流的数量增加时通信开销可能会增加，并且有一种收敛和通信开销的权衡可以利用。我们分析了单流和多流DIGEST的收敛性，并证明了两种算法都接近最优解。",
    "tldr": "本文提出了一种名为DIGEST的快速和通信高效的异步分散学习机制，通过结合Gossip和随机游走的思想，并专注于随机梯度下降（SGD），实现了在分散学习中较低的通信成本和较快的收敛时间。",
    "en_tdlr": "This paper proposes a fast and communication-efficient asynchronous decentralized learning mechanism called DIGEST, which combines the ideas of Gossip and random walk and focuses on stochastic gradient descent (SGD), achieving lower communication cost and faster convergence time in decentralized learning."
}