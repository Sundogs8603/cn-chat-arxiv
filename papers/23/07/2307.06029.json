{
    "title": "Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])",
    "abstract": "Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.",
    "link": "http://arxiv.org/abs/2307.06029",
    "context": "Title: Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])\nAbstract: Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.",
    "path": "papers/23/07/2307.06029.json",
    "total_tokens": 880,
    "translated_title": "通过记忆增强的适配器实现可插拔的神经机器翻译模型",
    "translated_abstract": "尽管神经机器翻译（NMT）模型在普通领域表现出色，但是控制其生成行为以满足不同用户需求仍然具有一定挑战性。鉴于每个用户需求都需要从头开始学习新模型的高昂训练成本和数据稀缺的挑战，我们提出了一种记忆增强适配器，以可插拔的方式引导预训练的NMT模型。具体而言，我们基于用户提供的文本样本构建了一个多粒度记忆，并提出了一种新的适配器架构来结合模型表示和检索结果。同时，我们提出了一种使用记忆丢弃的训练策略，以减少NMT模型和记忆之间的虚假依赖关系。我们在风格和领域特定实验中验证了我们的方法，结果表明，我们的方法可以胜过几个代表性的可插拔基准模型。",
    "tldr": "通过记忆增强的适配器，我们提出了一种可插拔的方法来控制神经机器翻译模型的生成行为。实验证明，我们的方法可以胜过几个代表性的可插拔基准模型。",
    "en_tdlr": "We propose a pluggable approach to control the generation behavior of neural machine translation (NMT) models using memory-augmented adapters. Experimental results demonstrate that our method outperforms several representative pluggable baselines."
}