{
    "title": "Attention over pre-trained Sentence Embeddings for Long Document Classification. (arXiv:2307.09084v1 [cs.CL])",
    "abstract": "Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the",
    "link": "http://arxiv.org/abs/2307.09084",
    "context": "Title: Attention over pre-trained Sentence Embeddings for Long Document Classification. (arXiv:2307.09084v1 [cs.CL])\nAbstract: Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the",
    "path": "papers/23/07/2307.09084.json",
    "total_tokens": 808,
    "translated_title": "基于预训练句子嵌入的长文本分类中的注意力机制",
    "translated_abstract": "尽管transformer模型是大多数NLP任务中的默认模型，但由于其关于令牌数量的二次注意力复杂度，它们通常被限制在短序列中。为了解决这个问题，已经进行了一些尝试，包括通过减少自注意力计算的成本，通过对较小的序列建模并通过递归机制或使用新的transformer模型进行结合。在本文中，我们建议利用预训练的句子嵌入来从各个句子的语义有意义的嵌入开始，然后通过一个随文档长度线性扩展的小注意力层将它们组合起来。我们在三个标准的文档分类数据集上报告了这种简单架构的结果。与使用标准微调的当前最先进模型相比，该方法取得了竞争性的结果（即使在这种配置下没有明确的最佳模型）。我们还展示了通过该方法可实现的最佳性能。",
    "tldr": "利用预训练句子嵌入和小注意力层进行长文本分类，取得了竞争性的结果。",
    "en_tdlr": "Achieved competitive results in long document classification by utilizing pre-trained sentence embeddings and a small attention layer."
}