{
    "title": "On Neural Network approximation of ideal adversarial attack and convergence of adversarial training. (arXiv:2307.16099v1 [cs.LG])",
    "abstract": "Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\\\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.",
    "link": "http://arxiv.org/abs/2307.16099",
    "context": "Title: On Neural Network approximation of ideal adversarial attack and convergence of adversarial training. (arXiv:2307.16099v1 [cs.LG])\nAbstract: Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\\\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.",
    "path": "papers/23/07/2307.16099.json",
    "total_tokens": 888,
    "translated_title": "关于神经网络近似理想对抗攻击和对抗训练收敛性的论文翻译",
    "translated_abstract": "对抗攻击通常是通过对输入数据和模型进行基于梯度的操作来实现的，这导致每次生成攻击时都需要进行大量的计算。在这项工作中，我们将对抗攻击表示为可训练的函数的思想更加巩固，而无需进一步计算梯度。我们首先激发出在适当条件下，理论上的最佳攻击可以表示为光滑的分段函数（分段H\\\"older函数）。然后我们通过神经网络得到了这些函数的近似结果。随后，我们通过神经网络模拟理想的攻击过程，并将对抗训练化简为进攻网络和防守模型（防守网络）之间的数学博弈。在这样的设置中，我们还得到了对抗训练的样本大小$n$对于对抗损失的收敛速度。",
    "tldr": "这项研究通过使用神经网络对理想的对抗攻击进行近似表示，并将对抗训练转化为进攻网络和防守网络之间的数学博弈，同时给出了对抗训练在样本大小$n$下的收敛速度。"
}