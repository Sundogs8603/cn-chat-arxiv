{
    "title": "Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])",
    "abstract": "The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien",
    "link": "http://arxiv.org/abs/2307.06093",
    "context": "Title: Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])\nAbstract: The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien",
    "path": "papers/23/07/2307.06093.json",
    "total_tokens": 935,
    "translated_title": "在线 Laplace 模型选择的再探讨",
    "translated_abstract": "Laplace 近似为神经网络提供了一个封闭形式的模型选择目标。在贝叶斯深度学习领域，将神经网络参数与超参数（如权重衰减强度）一起进行优化的在线变体方法再次引起了人们的关注。然而，这些方法违反了 Laplace 方法的一个关键假设，即近似是围绕损失的模态进行的，这就对它们的合理性提出了质疑。本研究重新推导了在线 Laplace 方法，展示了它们针对 Laplace 证据的一个修正模态的变分上界，从而避免了对平稳性的假设。在线 Laplace 方法及其修正模态的对应点满足两个条件：1. 神经网络参数是一个最大后验概率，满足 Laplace 方法的假设；2. 超参数最大化 Laplace 证据，从而促使在线方法的应用。我们通过使用全批量梯度的在线算法演示了这些最优点在实践中的近似程度。",
    "tldr": "本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。"
}