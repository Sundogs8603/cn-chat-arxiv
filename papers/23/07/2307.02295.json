{
    "title": "Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])",
    "abstract": "We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two ",
    "link": "http://arxiv.org/abs/2307.02295",
    "context": "Title: Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])\nAbstract: We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two ",
    "path": "papers/23/07/2307.02295.json",
    "total_tokens": 1046,
    "translated_title": "元学习对抗波段算法",
    "translated_abstract": "我们研究具有波段反馈的在线元学习，目标是在多个任务之间改善性能，如果它们根据某个自然的相似性度量是相似的。作为针对敌对的在线部分信息设置的首个目标，我们设计了元算法，将外层学习器结合在一起，同时为两种重要情况调整内部学习器的初始化和其他超参数：多臂赌博机（MAB）和赌博线性优化（BLO）。对于MAB，元学习器使用Tsallis-熵的泛化Exp3的初始化和设置超参数，如果后见之高峰的熵小，则任务平均遗憾改善。对于BLO，我们学会了使用自协调障碍正则化器初始化和调整在线镜像下降（OMD），表明任务平均遗憾与其引起的动作空间相关的度量直接变化。我们的保证基于证明无正规化跟随者与两个…",
    "tldr": "本论文研究了具有波段反馈的在线元学习，并设计了用于多臂赌博机和赌博线性优化的元算法。对于多臂赌博机，算法使用了Tsallis-熵的泛化Exp3，并且任务平均遗憾会随着最优解的熵的减小而改善。对于赌博线性优化，算法使用了自协调障碍正则化器初始化和调整在线镜像下降，并且任务平均遗憾与动作空间相关的度量直接变化。",
    "en_tdlr": "This paper studies online meta-learning with bandit feedback and designs meta-algorithms for multi-armed bandits and bandit linear optimization. For multi-armed bandits, the algorithm uses a generalized version of Exp3 with Tsallis-entropy and the task-averaged regret improves with a decrease in the entropy of the optima. For bandit linear optimization, the algorithm uses self-concordant barrier regularizers to initialize and tune online mirror descent, and the task-averaged regret varies directly with an action space-dependent measure."
}