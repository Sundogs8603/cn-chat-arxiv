{
    "title": "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user prefe",
    "link": "http://arxiv.org/abs/2307.15020",
    "context": "Title: SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user prefe",
    "path": "papers/23/07/2307.15020.json",
    "total_tokens": 909,
    "translated_title": "SuperCLUE:一个全面的中文大型语言模型基准测试",
    "translated_abstract": "大型语言模型(LLMs)已经显示出将其整合到人们日常生活中的潜力。因此，用户偏好是评估LLMs在实际场景中表现的最关键标准。然而，现有的基准主要集中在使用多选题来衡量模型的准确性，这限制了对它们在实际应用中能力的理解。我们通过提出一个全面的中文基准测试SuperCLUE来填补这个空白，该基准测试以另一个流行的中文LLM基准测试CLUE命名。SuperCLUE包括三个子任务：来自一个LLM对战平台(CArena)的实际用户查询和评级，有单个和多轮对话的开放性问题(OPEN)，以及与开放性单轮问题相同茎的封闭性问题(CLOSE)。我们的研究表明，封闭性问题上的准确性不足以反映在开放性问题上实现的人类偏好。同时，它们可以互补地预测实际用户偏好。",
    "tldr": "提出了一个全面的中文大型语言模型基准测试SuperCLUE，包括实际用户的查询和评级、开放性问题以及封闭性问题，该基准测试填补了目前对模型在实际应用中能力理解的空白。",
    "en_tdlr": "SuperCLUE, a comprehensive Chinese large language model benchmark, fills the gap in understanding the capabilities of models in real applications by including actual user queries and ratings, open-ended questions, and closed-ended questions. It highlights that accuracy on closed-ended questions is insufficient to reflect user preferences achieved on open-ended ones."
}