{
    "title": "MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])",
    "abstract": "Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time",
    "link": "http://arxiv.org/abs/2307.11285",
    "context": "Title: MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])\nAbstract: Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time",
    "path": "papers/23/07/2307.11285.json",
    "total_tokens": 878,
    "translated_title": "MAS：面向资源高效的联邦多任务学习",
    "translated_abstract": "联邦学习是一种新兴的分布式机器学习方法，可以使分布式边缘设备进行模型训练。然而，多个同时进行的联邦学习任务可能会过载资源受限的设备。在这项工作中，我们提出了第一个有效协调和训练多个同时进行的联邦学习任务的联邦学习系统。我们首先形式化了训练同时进行的联邦学习任务的问题。然后，我们提出了新的方法MAS（Merge and Split），以优化训练多个同时进行的联邦学习任务的性能。MAS首先将联邦学习任务合并为一个具有多任务体系结构的整体联邦学习任务。在训练几轮后，MAS通过使用整体训练期间测量的任务之间的亲和性将整体联邦学习任务分割为两个或更多个联邦学习任务。然后，基于整体训练的模型参数，继续训练每个分割的联邦学习任务。大量实验证明，MAS在减少训练时间的同时优于其他方法。",
    "tldr": "本研究提出了第一个有效协调和训练多个同时进行的联邦学习任务的联邦学习系统MAS，通过合并和分割联邦学习任务来优化性能，实验证明其在减少训练时间的同时超过其他方法。"
}