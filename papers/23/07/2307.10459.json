{
    "title": "A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])",
    "abstract": "A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa",
    "link": "http://arxiv.org/abs/2307.10459",
    "context": "Title: A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])\nAbstract: A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa",
    "path": "papers/23/07/2307.10459.json",
    "total_tokens": 916,
    "translated_title": "一种实现具有硬约束输出的神经网络的计算简单方法",
    "translated_abstract": "提出了一种在神经网络输出值上施加硬凸约束的计算简单方法。该方法的关键思想是通过将网络的隐藏参数向量映射到一个点，确保它在由一组约束定义的可行集内。映射是通过具有输出约束的附加神经网络层实现的。将该方法简单地扩展到不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况。在所提出的方法框架中，可以简单地实现对输出的约束投影方法。展示了如何将不同类型的约束引入到所提出的方法中，包括线性和二次约束、等式约束和动态约束，以及边界形式的约束。该方法的一个重要特点是它的计算简单性。",
    "tldr": "提出了一种计算简单的方法来实现具有硬约束输出的神经网络。该方法通过映射隐藏参数向量到一个符合约束集的点实现约束，并通过附加的神经网络层来进行映射。该方法还可以处理不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况，并且可以处理不同类型的约束，包括线性和二次约束、等式约束和动态约束。",
    "en_tdlr": "A computationally simple method is proposed for implementing neural networks with output hard constraints. The method maps the hidden parameters of the network to a point inside the feasible set defined by the constraints, using an additional neural network layer. It can handle joint constraints depending on inputs and different types of constraints, including linear and quadratic constraints, equality constraints, and dynamic constraints."
}