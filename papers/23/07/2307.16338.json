{
    "title": "Distractor generation for multiple-choice questions with predictive prompting and large language models. (arXiv:2307.16338v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach",
    "link": "http://arxiv.org/abs/2307.16338",
    "context": "Title: Distractor generation for multiple-choice questions with predictive prompting and large language models. (arXiv:2307.16338v1 [cs.CL])\nAbstract: Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach",
    "path": "papers/23/07/2307.16338.json",
    "total_tokens": 907,
    "translated_title": "利用预测提示和大型语言模型生成多项选择题的干扰项",
    "translated_abstract": "大型语言模型（LLM）如ChatGPT在各种任务中表现出色，并且吸引了研究人员和实践者的广泛关注。然而，在教育背景下，我们仍然观察到使用LLM为多项选择题（MCQ）生成干扰项（即可信但不正确的答案）存在性能差距。在本研究中，我们提出了一种策略，通过使用从题库中自动检索的问题项作为上下文示例，引导LLM（如ChatGPT）生成相关的干扰项。我们使用现有测试集的定量评估和人类专家（教师）的质量注释来评估我们基于LLM的解决方案。我们发现，平均而言，向教师展示的53％生成的干扰项被评为高质量，即适合立即使用，优于最先进的模型。我们还展示了我们方法的收益。",
    "tldr": "本研究利用预测提示和大型语言模型，通过引导它们生成干扰项来填补教育背景下多项选择题中生成干扰项的性能差距。实验证明，这种方法在质量上超过了现有模型，并获得了高质量的干扰项。",
    "en_tdlr": "This study addresses the performance gap in generating distractors for multiple-choice questions in an educational context by utilizing predictive prompting and large language models. The proposed approach outperforms existing models and generates high-quality distractors."
}