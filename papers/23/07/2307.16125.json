{
    "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])",
    "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, elim",
    "link": "http://arxiv.org/abs/2307.16125",
    "context": "Title: SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])\nAbstract: Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, elim",
    "path": "papers/23/07/2307.16125.json",
    "total_tokens": 926,
    "translated_title": "SEED-Bench: 用生成式理解对多模态LLMs进行基准测试",
    "translated_abstract": "近期基于强大的大语言模型（LLMs），生成式多模态大语言模型（MLLMs）作为一个关键的研究领域受到了广泛关注，展示出了在理解和生成方面的卓越能力。在这项工作中，我们通过引入一个名为SEED-Bench的基准测试，解决了对MLLMs中生成式理解的评估问题，这是对生成式模型全面评估的一个初步步骤。SEED-Bench包括19K个准确的人工注释的多项选择题（比现有基准测试大6倍），涵盖了包括图像和视频模态在内的12个评估维度的理解能力。我们开发了一个先进的流程来生成针对特定评估维度的多项选择题，整合了自动筛选和手动验证过程。通过人工注释获得地面实况选项的多项选择题能够客观高效地评估模型的性能，",
    "tldr": "这项工作引入了一个名为SEED-Bench的基准测试，用于评估生成式多模态大语言模型的理解能力。SEED-Bench包括19K个多项选择题，涵盖了图像和视频模态等12个评估维度。通过人工注释提供的正确选项，能够客观高效地评估模型的性能。",
    "en_tdlr": "This work introduces a benchmark called SEED-Bench for evaluating the generative comprehension of multimodal large language models. SEED-Bench includes 19K multiple-choice questions spanning 12 evaluation dimensions, such as image and video modalities. The use of groundtruth options derived from human annotations enables an objective and efficient assessment of model performance."
}