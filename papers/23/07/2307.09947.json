{
    "title": "U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])",
    "abstract": "Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving. In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings. In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE). Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. Consequently, we contribute to the devel",
    "link": "http://arxiv.org/abs/2307.09947",
    "context": "Title: U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])\nAbstract: Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving. In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings. In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE). Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. Consequently, we contribute to the devel",
    "path": "papers/23/07/2307.09947.json",
    "total_tokens": 905,
    "translated_title": "U-CE: 不确定性感知的语义分割交叉熵",
    "translated_abstract": "深度神经网络在各种任务中表现出色，但它们的鲁棒性、可靠性和过度自信的倾向为在自动驾驶等安全关键应用中部署提出了挑战。因此，量化模型预测中固有的不确定性是解决这些缺点的一种有前途的努力。在这项工作中，我们提出了一种新颖的不确定性感知型交叉熵损失函数（U-CE），通过像素级加权将动态预测不确定性纳入训练过程中的著名交叉熵损失函数（CE）。通过广泛的实验，我们证明了U-CE在两个基准数据集Cityscapes和ACDC上与常见的ResNet-18和ResNet-101两种主干架构相比，具有优势。通过使用U-CE，我们能够训练出不仅在分割性能上有所提升，而且在训练后提供有意义的不确定性的模型。因此，我们对不确定性感知的语义分割交叉熵的研究做出了贡献。",
    "tldr": "本文提出了一种新的不确定性感知型交叉熵损失函数（U-CE），通过像素级加权将动态预测不确定性纳入训练过程中的著名交叉熵损失函数，通过实验证明了U-CE对于语义分割在性能和提供有意义的不确定性方面的改进。"
}