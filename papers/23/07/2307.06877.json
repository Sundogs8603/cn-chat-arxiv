{
    "title": "The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])",
    "abstract": "The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement.",
    "link": "http://arxiv.org/abs/2307.06877",
    "context": "Title: The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])\nAbstract: The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement.",
    "path": "papers/23/07/2307.06877.json",
    "total_tokens": 780,
    "translated_title": "非平稳强化学习的复杂性",
    "translated_abstract": "非平稳强化学习的问题被认为是强化学习应用中的一个重要挑战。我们证明了最坏情况下的复杂性结果，我们认为这恰好捕捉到了这个挑战：修改强化学习问题中一个状态-动作对的概率或奖励，需要花费几乎与状态数目一样多的时间来及时更新值函数，除非强指数时间假设(SETH)是错误的；SETH是P≠NP猜想的广泛接受的加强版。需要注意的是，目前强化学习应用中的状态数目通常是天文数字级别的。相反，我们还展示了仅仅\"添加\"一个新的状态-动作对要容易得多。",
    "tldr": "强化学习中的非平稳学习是一个重要挑战，我们证明了在修改概率或奖励时需要花费大量的时间来保持值函数的最新状态，并且这个挑战与状态数目密切相关。",
    "en_tdlr": "Non-stationary reinforcement learning is an important challenge, and we prove that modifying probabilities or rewards requires a significant amount of time to keep the value function updated, which is closely related to the number of states."
}