{
    "title": "Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])",
    "abstract": "Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.",
    "link": "http://arxiv.org/abs/2307.06794",
    "context": "Title: Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])\nAbstract: Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.",
    "path": "papers/23/07/2307.06794.json",
    "total_tokens": 856,
    "translated_title": "使用大规模语言模型的否定式互补常识",
    "translated_abstract": "更大的语言模型，如GPT-3，在许多任务中表现出色。然而，我们证明，非常规问题可能会使模型失去警觉。本文主要关注在常识情景中寻找否定式互补问题的答案。我们阐述了这类问题对模型响应的不利影响。我们提出了一种模型无关的方法来提高在否定式互补情景中的性能。我们的方法在从GPT-3进行少样本生成方面表现优于（超过11个点），更重要的是，强调了研究大规模语言模型在否定式互补问题中的响应的重要性。代码、数据和实验可在以下链接找到：https://github.com/navidre/negated_complementary_commonsense。",
    "tldr": "本论文研究了使用大规模语言模型解决否定式互补问题的性能。作者发现，这种类型的问题会对模型的响应产生负面影响，并提出了一种模型无关的方法来改善性能。实验证明，该方法在少样本生成方面优于GPT-3，并强调了研究大规模语言模型在否定式互补问题中的重要性。",
    "en_tdlr": "This paper investigates the performance of using large language models to solve negated complementary questions. The authors find that this type of questions has a negative impact on the model's responses and propose a model-agnostic method to improve performance. Experiments show that the method outperforms GPT-3 in few-shot generation and highlights the importance of studying large language models in negated complementary questions."
}