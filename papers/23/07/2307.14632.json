{
    "title": "Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])",
    "abstract": "In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust ",
    "link": "http://arxiv.org/abs/2307.14632",
    "context": "Title: Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])\nAbstract: In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust ",
    "path": "papers/23/07/2307.14632.json",
    "total_tokens": 931,
    "translated_title": "基于度量的上下文学习：文本简化案例研究",
    "translated_abstract": "对大型语言模型进行上下文学习（ICL）在许多自然语言处理任务中已被证明是一种强大的方法。然而，确定选择ICL示例的最佳方法并不容易，因为结果可以因使用的示例的质量、数量和顺序而变化很大。在本文中，我们进行了一个关于文本简化（TS）的案例研究，以探讨如何选择最佳和最健壮的ICL示例。我们提出了一种基于常用的TS度量（如SARI、压缩比例和BERT-Precision）进行选择的基于度量的上下文学习（MBL）方法。通过在标准的TS基准（如TurkCorpus和ASSET）上使用各种规模的GPT模型进行广泛的实验，我们表明在较大的模型（如GPT-175B）上选择的示例通过顶级SARI得分表现最佳，而压缩比例通常在较小的模型（如GPT-13B和GPT-6.7B）上表现更好。此外，我们证明MBL通常具有良好的鲁棒性。",
    "tldr": "本文针对文本简化进行了一个案例研究，提出了一种基于度量的上下文学习（MBL）方法，通过选择具有顶级SARI得分的示例，可以在较大型的GPT模型上获得最佳表现。而在较小型的模型上，通过选择具有较高压缩比例的示例表现更好。"
}