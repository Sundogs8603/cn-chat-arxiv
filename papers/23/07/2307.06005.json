{
    "title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])",
    "abstract": "Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand",
    "link": "http://arxiv.org/abs/2307.06005",
    "context": "Title: DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])\nAbstract: Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand",
    "path": "papers/23/07/2307.06005.json",
    "total_tokens": 897,
    "translated_title": "DDNAS: 离散化可微分神经架构搜索用于文本分类",
    "translated_abstract": "神经架构搜索（NAS）在学习文本表示方面展现出了很好的能力。然而，现有的基于文本的NAS既未对架构进行可学习的融合以优化，也未对文本输入背后的潜在层级分类进行编码。本文提出了一种新颖的NAS方法，即Discretized Differentiable Neural Architecture Search (DDNAS)，用于文本表示学习和分类。通过架构表示的连续松弛，DDNAS可以使用梯度下降来进行搜索优化。我们还提出了一种新颖的离散化层，通过最大化互信息将其施加于每个搜索节点上，以对文本表示中的潜在层级分类进行建模。在八个不同的真实数据集上进行的大量实验表明，DDNAS始终能够优于最先进的NAS方法。尽管DDNAS仅依赖于卷积，池化和无操作这三个基本操作，作为候选操作。",
    "tldr": "这篇论文提出了一种名为DDNAS的离散化可微分神经架构搜索方法，用于文本分类。通过使用连续松弛的架构表示和互信息最大化的离散化层，DDNAS在文本表示学习和分类任务中表现优于其他NAS方法。",
    "en_tdlr": "This paper presents a novel Discretized Differentiable Neural Architecture Search (DDNAS) method for text classification. By using continuous relaxation of architecture representation and a novel discretization layer based on mutual information maximization, DDNAS outperforms other NAS methods in text representation learning and classification tasks."
}