{
    "title": "Tangent Model Composition for Ensembling and Continual Fine-tuning. (arXiv:2307.08114v2 [cs.LG] UPDATED)",
    "abstract": "Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- o",
    "link": "http://arxiv.org/abs/2307.08114",
    "context": "Title: Tangent Model Composition for Ensembling and Continual Fine-tuning. (arXiv:2307.08114v2 [cs.LG] UPDATED)\nAbstract: Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- o",
    "path": "papers/23/07/2307.08114.json",
    "total_tokens": 1035,
    "translated_title": "切线模型组合用于集成和持续微调",
    "translated_abstract": "切线模型组合 (TMC) 是一种将独立微调的组成模型结合在预训练点周围的方法。组成模型是与预训练模型相关的切线向量，可以通过加法、缩放或减法来支持增量学习、集成或取消学习。在推理时，组成模型通过标量组合的方式进行组合，将集成的成本降低到单个模型的水平。与非线性微调模型进行集成相比，TMC提高了4.2%的准确率，并将推理成本降低了2.5倍至10倍，与组成模型的数量呈线性增长。每个组成模型可以以零成本忘记，对推理结果没有剩余影响。当用于持续微调时，TMC不受顺序偏差的限制，并且可以在联合数据上并行执行。在任务增量、类别增量和数据增量设置中，TMC几乎在每个方案上均优于最近发表的持续微调方法。",
    "tldr": "切线模型组合 (TMC) 是一种将独立微调的模型结合的方法，可以用于增量学习、集成和取消学习。通过标量组合方式进行组合，提高了准确率并降低了推理成本。该方法可以零成本忘记组成模型，不受顺序偏差的限制，并能在联合数据上并行执行。在任务增量、类别增量和数据增量设置中，TMC几乎在每个方案上均优于最近发表的持续微调方法。",
    "en_tdlr": "Tangent Model Composition (TMC) is a method for combining independently fine-tuned models for incremental learning, ensembling, and unlearning. By using scalar combination, TMC improves accuracy and reduces inference cost. It allows forgetting component models at zero cost, is not constrained by sequential bias, and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods in various settings."
}