{
    "title": "Accelerating Inexact HyperGradient Descent for Bilevel Optimization. (arXiv:2307.00126v1 [math.OC])",
    "abstract": "We present a method for solving general nonconvex-strongly-convex bilevel optimization problems. Our method -- the \\emph{Restarted Accelerated HyperGradient Descent} (\\texttt{RAHGD}) method -- finds an $\\epsilon$-first-order stationary point of the objective with $\\tilde{\\mathcal{O}}(\\kappa^{3.25}\\epsilon^{-1.75})$ oracle complexity, where $\\kappa$ is the condition number of the lower-level objective and $\\epsilon$ is the desired accuracy. We also propose a perturbed variant of \\texttt{RAHGD} for finding an $\\big(\\epsilon,\\mathcal{O}(\\kappa^{2.5}\\sqrt{\\epsilon}\\,)\\big)$-second-order stationary point within the same order of oracle complexity. Our results achieve the best-known theoretical guarantees for finding stationary points in bilevel optimization and also improve upon the existing upper complexity bound for finding second-order stationary points in nonconvex-strongly-concave minimax optimization problems, setting a new state-of-the-art benchmark. Empirical studies are conducted t",
    "link": "http://arxiv.org/abs/2307.00126",
    "context": "Title: Accelerating Inexact HyperGradient Descent for Bilevel Optimization. (arXiv:2307.00126v1 [math.OC])\nAbstract: We present a method for solving general nonconvex-strongly-convex bilevel optimization problems. Our method -- the \\emph{Restarted Accelerated HyperGradient Descent} (\\texttt{RAHGD}) method -- finds an $\\epsilon$-first-order stationary point of the objective with $\\tilde{\\mathcal{O}}(\\kappa^{3.25}\\epsilon^{-1.75})$ oracle complexity, where $\\kappa$ is the condition number of the lower-level objective and $\\epsilon$ is the desired accuracy. We also propose a perturbed variant of \\texttt{RAHGD} for finding an $\\big(\\epsilon,\\mathcal{O}(\\kappa^{2.5}\\sqrt{\\epsilon}\\,)\\big)$-second-order stationary point within the same order of oracle complexity. Our results achieve the best-known theoretical guarantees for finding stationary points in bilevel optimization and also improve upon the existing upper complexity bound for finding second-order stationary points in nonconvex-strongly-concave minimax optimization problems, setting a new state-of-the-art benchmark. Empirical studies are conducted t",
    "path": "papers/23/07/2307.00126.json",
    "total_tokens": 941,
    "translated_title": "加速非精确超梯度下降用于双层优化",
    "translated_abstract": "我们提出了一种解决一般非凸-凸双层优化问题的方法。我们的方法——\"重新启动的加速超梯度下降\" (RAHGD) 方法——可以找到一个 $\\epsilon$-一阶稳定点，其 oracle 复杂度为 $\\tilde{\\mathcal{O}}(\\kappa^{3.25}\\epsilon^{-1.75})$，其中 $\\kappa$ 是下层目标的条件数，$\\epsilon$ 是期望精度。我们还提出了 RAHGD 的扰动变体，用于在相同的 oracle 复杂度下找到一个 $\\big(\\epsilon,\\mathcal{O}(\\kappa^{2.5}\\sqrt{\\epsilon}\\,)\\big)$-二阶稳定点。我们的结果在双层优化中实现了已知最好的理论保证，并且改进了现有凸-凹极小极大优化问题中找到二阶稳定点的上界复杂度，为最新的基准设置了一个新的最佳状态。我们进行了实证研究。",
    "tldr": "提出一种加速非精确超梯度下降的方法用于双层优化，可以在较低的复杂度下找到一阶和二阶稳定点，成为双层优化和凸-凹极小极大优化问题中的最新最佳状态。",
    "en_tdlr": "This paper proposes an accelerated inexact hypergradient descent method for bilevel optimization, achieving state-of-the-art results in finding first-order and second-order stationary points with lower complexity in both bilevel optimization and convex-strongly-concave minimax optimization problems."
}