{
    "title": "A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])",
    "abstract": "Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review",
    "link": "http://arxiv.org/abs/2307.07982",
    "context": "Title: A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])\nAbstract: Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review",
    "path": "papers/23/07/2307.07982.json",
    "total_tokens": 757,
    "translated_title": "优化Transformer推理技术的综述",
    "translated_abstract": "近年来，Transformer神经网络的性能和应用范围取得了显著的增长。包括BERT、GPT和ViT在内的Transformer网络家族，在自然语言处理（NLP）和计算机视觉（CV）领域展现了效果。ChatGPT等基于Transformer的网络也影响了普通人的生活。然而，追求高预测性能导致了Transformer的内存和计算消耗的指数增长。研究人员提出了在抽象级别的Transformer推理优化技术。本文对Transformer网络推理阶段的优化技术进行了全面的调查。我们在算法层面上调查了知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计等技术。",
    "tldr": "本文综述了优化Transformer推理的技术，包括知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计。",
    "en_tdlr": "This paper provides a comprehensive survey of techniques for optimizing the inference phase of transformer networks, including knowledge distillation, pruning, quantization, neural architecture search, and lightweight network design."
}