{
    "title": "Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])",
    "abstract": "Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t",
    "link": "http://arxiv.org/abs/2307.00268",
    "context": "Title: Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])\nAbstract: Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t",
    "path": "papers/23/07/2307.00268.json",
    "total_tokens": 1061,
    "translated_title": "明里暗中：利用差分隐私噪音的抵御恶意攻击的本地中毒攻击方法在多智能体强化学习中的应用",
    "translated_abstract": "最近，差分隐私（DP）被引入到合作多智能体强化学习（CMARL）中，以保护智能体在知识共享过程中免受对手的推断攻击。然而，我们认为由DP机制引入的噪音可能会在CMARL中的私有知识共享过程中意外地产生一种新的中毒威胁，这在文献中尚未得到研究。为了解决这个问题，我们提出了一种自适应的、利用隐私的、抵御逃避攻击的本地中毒攻击方法（PeLPA），利用了DP噪音的特性，绕过异常检测系统，并阻碍CMARL模型的最优收敛。我们在不同的环境中对我们提出的PeLPA攻击进行了严格的评估，包括非对抗和多对抗环境。我们的研究结果表明，在中等规模环境中，攻击者比例为20%和40%的PeLPA攻击可能导致平均步数的增加。",
    "tldr": "本文介绍了一种利用差分隐私噪音的本地中毒攻击方法（PeLPA）以绕过异常检测系统，并针对合作多智能体强化学习（CMARL）中的私有知识共享过程中的中毒威胁。研究结果表明，在不同环境下，PeLPA攻击能够显著增加平均步数。",
    "en_tdlr": "This paper presents an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that utilizes differential privacy noise to bypass anomaly detection systems, targeting the poisoning threat in private knowledge sharing during cooperative multiagent reinforcement learning (CMARL). The research findings demonstrate that PeLPA attack can significantly increase the average steps in different environments."
}