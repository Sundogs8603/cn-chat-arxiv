{
    "title": "A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])",
    "abstract": "Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.",
    "link": "http://arxiv.org/abs/2307.05704",
    "context": "Title: A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])\nAbstract: Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.",
    "path": "papers/23/07/2307.05704.json",
    "total_tokens": 809,
    "translated_title": "无监督表示学习的因果排序先验方法",
    "translated_abstract": "无监督表示学习依赖于对潜变量的独立性假设。然而，因果表示学习（CRL）认为数据集中的变异因素实际上是因果相关的。允许潜变量由于因果关系而相关性更加真实和可泛化。到目前为止，可证明可识别的方法依赖于：辅助信息、弱标签，以及干预或甚至对照数据。受到功能因果模型的因果发现的启发，我们提出了一种完全无监督的表示学习方法，考虑了具有潜在加性噪声模型（ANM）的数据生成过程。通过基于潜在分布的海森矩阵的损失函数，我们鼓励潜在空间遵循因果排序。",
    "tldr": "该论文提出了一种无监督表示学习的方法，通过考虑具有潜在加性噪声模型的数据生成过程，以及基于潜在分布的海森矩阵的损失函数，鼓励潜在空间遵循因果排序。",
    "en_tdlr": "This paper proposes a method for unsupervised representation learning that encourages a latent space to follow a causal ordering by considering a data generation process with a latent additive noise model and using a loss function based on the Hessian of the latent distribution."
}