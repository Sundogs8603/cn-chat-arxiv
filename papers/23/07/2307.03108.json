{
    "title": "How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])",
    "abstract": "Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener",
    "link": "http://arxiv.org/abs/2307.03108",
    "context": "Title: How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])\nAbstract: Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener",
    "path": "papers/23/07/2307.03108.json",
    "total_tokens": 968,
    "translated_title": "如何检测文本到图像扩散模型中的未授权数据使用",
    "translated_abstract": "最近的文本到图像扩散模型在生成高质量图像方面表现出令人惊讶的性能。然而，对于训练过程中的未授权数据使用引起了关注。一个例子是当模型训练者收集了一个特定艺术家创建的一系列图像，并试图训练一个能够生成类似图像的模型，而没有获得艺术家的许可。为了解决这个问题，我们提出了一种方法，通过将注入的记忆化内容植入保护数据集上训练的文本到图像扩散模型中，来检测此类未授权数据使用。具体地，我们通过在图像上添加独特的内容，例如对人类视觉不可察觉但能够被扩散模型捕捉和记忆的隐秘图像包装函数，来修改受保护的图像数据集。通过分析模型是否对注入的内容进行记忆化，我们可以判断模型是否存在这一记忆（即是否存在生成类似图像的能力）。",
    "tldr": "本文提出了一种方法，通过在训练的文本到图像扩散模型中植入注入的记忆化内容，来检测未授权数据使用。该方法修改了受保护的图像数据集，添加了对人眼不可察觉但模型可以捕捉和记忆的内容，通过分析模型对注入内容的记忆来判断模型是否存在生成类似图像的能力。",
    "en_tdlr": "This paper proposes a method to detect unauthorized data usage in text-to-image diffusion models by planting injected memorization into the trained models. The method modifies the protected image dataset by adding imperceptible content that can be captured and memorized by the models, and analyzes the models' memorization of the injected content to determine if they can generate similar images."
}