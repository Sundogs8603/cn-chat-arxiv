{
    "title": "Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])",
    "abstract": "With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t",
    "link": "http://arxiv.org/abs/2307.15007",
    "context": "Title: Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])\nAbstract: With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t",
    "path": "papers/23/07/2307.15007.json",
    "total_tokens": 882,
    "translated_title": "可验证的特征归因：后期解释性和内在可解释性的桥梁",
    "translated_abstract": "随着机器学习模型在各种实际应用中的部署增加，研究人员和从业者一直强调对模型行为的解释需求。为此，在以前的文献中概述了两种广泛的策略来解释模型。后期解释方法通过突出显示对模型预测至关重要的特征来解释复杂的黑盒模型的行为；然而，之前的工作表明这些解释可能不忠实，更令人担忧的是我们无法验证它们。另一方面，内在可解释模型通过将解释性信息明确编码到模型架构中来规避这些问题，这意味着它们的解释自然忠实且可验证，但由于其有限的表达能力，它们通常表现出较差的预测性能。在这项工作中，我们旨在搭建一座桥梁，将后期解释性和内在可解释性相结合。",
    "tldr": "本文旨在搭建一座桥梁，将后期解释性和内在可解释性相结合，以解释复杂的黑盒模型的行为，并解决解释的忠实性和可验证性之间的问题。",
    "en_tdlr": "This paper aims to bridge the gap between post hoc explainability and inherent interpretability by combining them, in order to explain the behavior of complex black-box models and address the issues of fidelity and verifiability in explanations."
}