{
    "title": "The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])",
    "abstract": "Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.",
    "link": "http://arxiv.org/abs/2307.15196",
    "context": "Title: The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])\nAbstract: Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.",
    "path": "papers/23/07/2307.15196.json",
    "total_tokens": 858,
    "translated_title": "小学习率随机梯度下降中动量的边际价值",
    "translated_abstract": "在没有随机梯度噪声的强凸环境中，动量已被证明能加速梯度下降的收敛。在随机优化中，如训练神经网络，有传言认为动量可以通过减小随机梯度更新的方差来帮助深度学习优化，但之前的理论分析并没有发现动量可以提供任何可证实的加速。本文的理论结果阐明了在学习率较小且梯度噪声是主要不稳定源的随机环境中动量的作用，表明使用和不使用动量的随机梯度下降在短期和长期时间段内表现相似。实验证明，在实际训练中，动量在优化和泛化方面确实有局限的益处，特别是在学习率不是很大的情况下，包括在ImageNet上从头训练小至中等批次大小的模型和在下游任务上微调语言模型。",
    "tldr": "本文通过理论分析和实验证明，在小学习率和梯度噪声是主要不稳定源的随机环境中，动量在深度学习优化中的边际价值是有限的。",
    "en_tdlr": "This paper theoretically analyzes and experimentally demonstrates that momentum has limited marginal value in deep learning optimization, specifically in stochastic environments with small learning rates and gradient noise as the dominant source of instability."
}