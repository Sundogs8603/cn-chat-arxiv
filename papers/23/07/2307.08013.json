{
    "title": "Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])",
    "abstract": "Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars",
    "link": "http://arxiv.org/abs/2307.08013",
    "context": "Title: Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])\nAbstract: Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars",
    "path": "papers/23/07/2307.08013.json",
    "total_tokens": 942,
    "translated_title": "重新审视隐式模型：在视觉任务中权重绑定模型的稀疏度权衡能力",
    "translated_abstract": "隐式模型如深度平衡模型（Deep Equilibrium Models, DEQs）因其能够用优雅的解决方案和恒定的内存占用训练无限层模型而引起了研究者的广泛关注。然而，尽管已经做出了几次尝试，这些方法仍然受到模型低效和优化不稳定性的严重限制。此外，对于视觉任务，缺乏相关方法的公平基准评估。在本研究中，我们重新审视了隐式模型的发展，并将其追溯到原始的权重绑定模型。令人惊讶的是，我们观察到与DEQ变体相比，权重绑定模型在视觉任务上更加有效、稳定和高效。通过这些简单而清晰的权重绑定模型，我们进一步研究了这种模型容量的基本限制，并提出了使用不同稀疏掩模来提高模型容量的方法。最后，我们为从业人员提供了关于深度、宽度和稀疏度的设计指南。",
    "tldr": "本研究重新审视了隐式模型，并发现权重绑定模型在视觉任务中比DEQ变体更有效、稳定和高效。通过使用不同的稀疏掩模，我们提出了提高模型容量的方法。",
    "en_tdlr": "This study revisits implicit models and finds that weight-tied models are more effective, stable, and efficient compared to DEQ variants for vision tasks. By using distinct sparse masks, the model capacity can be improved."
}