{
    "title": "Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])",
    "abstract": "We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.",
    "link": "http://arxiv.org/abs/2307.11166",
    "context": "Title: Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])\nAbstract: We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.",
    "path": "papers/23/07/2307.11166.json",
    "total_tokens": 864,
    "translated_title": "在MuJoCo环境中探索离散和连续控制任务的强化学习方法",
    "translated_abstract": "我们利用快速的物理模拟器MuJoCo在连续控制环境中运行任务，并揭示每个任务的观测空间、动作空间、奖励等详细信息。通过比较离散化方法中的Q学习和SARSA，将值基方法应用于连续控制任务，并使用它们作为基准，逐步向最先进的深度策略梯度方法DDPG过渡。在大量的episode中，Q学习的得分超过了SARSA，但DDPG在少量的episode中表现优于两者。最后，我们还通过调整模型超参数，期望在更少的时间和资源消耗下获得更好的性能。我们预期新设计的DDPG将大幅提高性能，然而在只有少数episode之后，我们已经能够达到不错的平均奖励。我们期望在足够的时间和计算资源下进一步提高性能。",
    "tldr": "该论文研究了在MuJoCo环境中离散和连续控制任务的强化学习方法，通过比较不同算法，发现DDPG在少量episode中表现优于其他方法。",
    "en_tdlr": "This paper explores reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. By comparing different algorithms, the study finds that DDPG performs better than other methods in a small number of episodes."
}