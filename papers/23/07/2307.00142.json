{
    "title": "BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])",
    "abstract": "Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w",
    "link": "http://arxiv.org/abs/2307.00142",
    "context": "Title: BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])\nAbstract: Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w",
    "path": "papers/23/07/2307.00142.json",
    "total_tokens": 923,
    "translated_title": "BuildingsBench：一个包含900K座建筑物的大规模数据集和短期负荷预测基准",
    "translated_abstract": "针对短期负荷预测(STLF)中缺乏开放、大规模、高建筑多样性数据集的问题，本文提出了BuildingsBench，包括1)包含900K个模拟建筑的大规模数据集Buildings-900K，以模拟美国的建筑库存，以及2)拥有来自7个开放数据集的超过1900个真实住宅和商业建筑物的评估平台。BuildingsBench为两个未被充分探索的任务提供了基准：零-shot STLF，其中预训练模型在未见过的建筑上进行评估而无需微调；以及迁移学习，其中预训练模型在目标建筑上进行微调。本次基准分析的主要发现是，经过合成预训练的模型意外地具有良好的泛化能力。",
    "tldr": "本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。",
    "en_tdlr": "This paper presents BuildingsBench, a large-scale dataset of 900K buildings, aimed at addressing the lack of data in short-term load forecasting. Through this dataset, we conducted benchmark evaluations for two tasks and found that synthetically pretrained models have good generalization capabilities."
}