{
    "title": "Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v1 [cs.CL])",
    "abstract": "The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5% F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task and pave the way for further research on extracting relational knowledge from latent language representations.",
    "link": "http://arxiv.org/abs/2307.03122",
    "context": "Title: Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v1 [cs.CL])\nAbstract: The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5% F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task and pave the way for further research on extracting relational knowledge from latent language representations.",
    "path": "papers/23/07/2307.03122.json",
    "total_tokens": 945,
    "translated_title": "从语言模型中提取多值关系",
    "translated_abstract": "广泛使用预训练语言模型（LMs）的潜在语言表示表明它们是一种有前景的结构化知识来源。然而，现有方法仅关注每个主题-关系对中的单个对象，尽管通常有多个对象是正确的。为了克服这个限制，我们分析这些表示以了解它们产生多对象关系知识的潜力。我们将该问题制定为一个排名-选择任务。对于排名候选对象，我们评估现有的提示技术并提出了融入领域知识的新技术。在选择方法中，我们发现选择具有高于学习到的关系特定阈值的对象可以达到49.5%的F1得分。我们的结果突显了使用LMs进行多值槽位填充任务的困难，并为从潜在语言表示中提取关系知识的进一步研究铺平了道路。",
    "tldr": "该论文研究了从预训练语言模型中提取多值关系的问题，并通过排名和选择任务的方法解决了这个问题。结果表明，选择具有特定关系阈值以上的对象可以达到49.5%的F1得分，这对于将语言模型应用于多值槽位填充任务而言是具有挑战性的。该研究为从潜在语言表示中提取关系知识开辟了进一步研究的道路。",
    "en_tdlr": "This paper investigates the problem of extracting multi-valued relations from pre-trained language models and solves it through a rank-then-select task. The results show that selecting objects with a relation-specific threshold above can achieve an F1 score of 49.5%, highlighting the challenge of applying language models to multi-valued slot filling task. This research paves the way for further investigation into extracting relational knowledge from latent language representations."
}