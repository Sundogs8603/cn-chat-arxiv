{
    "title": "Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])",
    "abstract": "Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a",
    "link": "http://arxiv.org/abs/2307.10350",
    "context": "Title: Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])\nAbstract: Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a",
    "path": "papers/23/07/2307.10350.json",
    "total_tokens": 955,
    "translated_title": "改进视觉-语言多模态数据集的图像描述能力",
    "translated_abstract": "大规模网络数据集在CLIP和Flamingo等大型视觉-语言模型的成功中起到了关键作用。然而，原始网络数据存在噪音，现有的降噪方法往往会以数据的多样性为代价。我们的研究聚焦于图像描述的质量作为噪音的一个主要来源，并研究了如何通过生成的描述增加含有含义不明确文本的网络抓取数据点的实用性。通过探索原始模式和生成模式两种不同的混合策略，我们在ImageNet上超过了DataComp基准提出的最佳降噪方法2%，在38个任务中平均提高了4%，给定128M的图像-文本对候选池。我们最好的方法在Flickr和MS-COCO检索上也提升了2倍。然后，我们分析了合成描述为什么是一种有效的文本监督来源。在尝试不同的图像描述模型的同时，我们还证明了模型在标准图像描述基准上的表现（例如，NoCaps CIDEr）并不一定是",
    "tldr": "通过探索混合原始和生成的图像描述的不同策略，我们的方法在ImageNet上超过了当前最佳降噪方法2%，在38个任务中平均提高了4%，我们的最佳方法在Flickr和MS-COCO检索上也提升了2倍。",
    "en_tdlr": "By exploring different strategies for mixing original and generated image captions, our approach outperforms the current best filtering method by 2% on ImageNet and achieves an average improvement of 4% across 38 tasks. Our best method also achieves a 2x improvement in Flickr and MS-COCO retrieval."
}