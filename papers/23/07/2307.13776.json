{
    "title": "Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])",
    "abstract": "In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.",
    "link": "http://arxiv.org/abs/2307.13776",
    "context": "Title: Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])\nAbstract: In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.",
    "path": "papers/23/07/2307.13776.json",
    "total_tokens": 775,
    "translated_title": "通过对齐稀疏上下文化的词表示来解决跨语言词义消歧中的多语言诅咒",
    "translated_abstract": "本文提倡在跨语言零样本词义消歧（WSD）中使用大型预训练单语语言模型，并结合上下文化映射机制。我们还进行了严格的实验，证明了通过字典学习过程获得的稀疏上下文化词表示的有效性。我们的实验结果表明，上述改进使得17种语言的平均F分数有近6.5个百分点的显著提高（从62.0提高到68.5）。我们在https://github.com/begab/sparsity_makes_sense发布了用于复现我们实验的源代码。",
    "tldr": "本文提出了使用大型预训练单语语言模型和上下文化映射机制来解决跨语言词义消歧的多语言诅咒，并通过实验验证了这种方法的有效性。",
    "en_tdlr": "This paper proposes to combat the curse of multilinguality in cross-lingual word sense disambiguation (WSD) by using large pre-trained monolingual language models and a contextualized mapping mechanism. Experimental results demonstrate the effectiveness of this approach with nearly 6.5 points increase in average F-score across 17 typologically diverse languages."
}