{
    "title": "Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)",
    "abstract": "We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont",
    "link": "http://arxiv.org/abs/2307.04721",
    "context": "Title: Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)\nAbstract: We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont",
    "path": "papers/23/07/2307.04721.json",
    "total_tokens": 899,
    "translated_title": "大型语言模型作为通用模式机器",
    "translated_abstract": "我们观察到预训练的大型语言模型（LLMs）能够自动完成复杂的令牌序列，从由概率上下文无关语法（PCFG）随机生成的任意序列，到在Abstraction and Reasoning Corpus（ARC）中发现的更丰富的空间模式，以ASCII艺术的形式提示。令人惊讶的是，即使使用从词汇表中随机抽样的令牌表示序列，模式完成能力也可以部分保留。这些结果表明，在没有任何额外训练的情况下，LLMs可以作为通用序列模型器，通过上下文学习驱动。在这项工作中，我们研究了这些零样本能力如何应用于机器人领域的问题，从对表示随时间变化的状态的数字序列进行外推，以完成简单的运动，到以奖励条件轨迹的最小到最大提示方式，能够发现和表示闭环策略（例如，稳定的控制系统）。",
    "tldr": "预训练的大型语言模型（LLMs）展现出了强大的序列模式完成能力，即使在随机样本的情况下也能保持一定的准确性。这些模型可以用于机器人领域的问题，如动态状态序列预测和自主路径规划。",
    "en_tdlr": "Pre-trained large language models (LLMs) demonstrate powerful sequence pattern completion capabilities, even in the case of random samples. These models can be applied to problems in robotics, such as dynamic state sequence prediction and autonomous path planning."
}