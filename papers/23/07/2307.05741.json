{
    "title": "Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])",
    "abstract": "As the application space of language models continues to evolve, a natural question to ask is how we can quickly adapt models to new tasks. We approach this classic question from a continual learning perspective, in which we aim to continue fine-tuning models trained on past tasks on new tasks, with the goal of \"transferring\" relevant knowledge. However, this strategy also runs the risk of doing more harm than good, i.e., negative transfer. In this paper, we construct a new benchmark of task sequences that target different possible transfer scenarios one might face, such as a sequence of tasks with high potential of positive transfer, high potential for negative transfer, no expected effect, or a mixture of each. An ideal learner should be able to maximally exploit information from all tasks that have any potential for positive transfer, while also avoiding the negative effects of any distracting tasks that may confuse it. We then propose a simple, yet effective, learner that satisfies",
    "link": "http://arxiv.org/abs/2307.05741",
    "context": "Title: Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])\nAbstract: As the application space of language models continues to evolve, a natural question to ask is how we can quickly adapt models to new tasks. We approach this classic question from a continual learning perspective, in which we aim to continue fine-tuning models trained on past tasks on new tasks, with the goal of \"transferring\" relevant knowledge. However, this strategy also runs the risk of doing more harm than good, i.e., negative transfer. In this paper, we construct a new benchmark of task sequences that target different possible transfer scenarios one might face, such as a sequence of tasks with high potential of positive transfer, high potential for negative transfer, no expected effect, or a mixture of each. An ideal learner should be able to maximally exploit information from all tasks that have any potential for positive transfer, while also avoiding the negative effects of any distracting tasks that may confuse it. We then propose a simple, yet effective, learner that satisfies",
    "path": "papers/23/07/2307.05741.json",
    "total_tokens": 921,
    "translated_title": "迈向强大和高效的持续语言学习",
    "translated_abstract": "随着语言模型应用领域的不断发展，一个自然的问题是如何快速适应模型到新的任务。我们从持续学习的角度来研究这个经典问题，我们的目标是在新的任务上继续微调过去任务上训练的模型，以\"传输\"相关知识。然而，这种策略也存在更多害处的风险，即负迁移。在本文中，我们构建了一个新的任务序列基准，针对可能面临的不同传递场景，比如具有积极传递潜力的任务序列、具有负迁移潜力的任务序列、没有预期效果的任务序列或者混合的任务序列。理想的学习者应该能够充分利用所有具有积极传递潜力的任务的信息，同时避免任何可能混淆它的干扰性任务的负面影响。然后，我们提出了一个简单但有效的学习者，满足这些要求。",
    "tldr": "本文研究了如何实现持续的语言学习，并构建了一个任务序列基准用于评估。最终提出了一个简单但有效的学习算法，以最大程度地利用具有积极传递潜力的任务，并避免负迁移的影响。",
    "en_tdlr": "This paper investigates how to achieve continual language learning and constructs a benchmark of task sequences for evaluation. It proposes a simple yet effective learning algorithm to maximize the utilization of tasks with positive transfer potential and avoid the negative effects of transfer."
}