{
    "title": "Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])",
    "abstract": "Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding ",
    "link": "http://arxiv.org/abs/2307.05182",
    "context": "Title: Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])\nAbstract: Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding ",
    "path": "papers/23/07/2307.05182.json",
    "total_tokens": 945,
    "translated_title": "Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery（用于机器人手术中视觉问答定位的共同关注门控视觉-语言嵌入）",
    "translated_abstract": "医学生和初级外科医生在学习手术时通常依赖于高级外科医生和专家回答他们的问题。然而，专家们经常忙于临床和学术工作，没有多少时间提供指导。与此同时，现有的基于深度学习的外科视觉问答系统只能提供简单答案，而没有答案的位置信息。此外，在这类任务中，视觉语言嵌入仍然是一个较少探索的领域。因此，一种外科视觉问答定位系统对于医学生和初级外科医生从录制的手术视频中学习和理解是有帮助的。我们提出了一种适用于外科场景的端到端Transformer与共同关注门控视觉-语言（CAT-ViL）的VQLA方法，它不需要通过检测模型进行特征提取。CAT-ViL嵌入模块的设计旨在融合来自视觉和文本来源的异构特征。",
    "tldr": "这项研究提出了一种用于机器人手术中视觉问答定位的共同关注门控视觉-语言嵌入方法，可以为医学生和初级外科医生提供学习和理解手术视频的帮助。",
    "en_tdlr": "This research proposes a co-attention gated vision-language embedding method for visual question localized-answering in robotic surgery, which can provide assistance for medical students and junior surgeons to learn and understand surgical videos."
}