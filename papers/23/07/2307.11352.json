{
    "title": "Model-based Offline Reinforcement Learning with Count-based Conservatism. (arXiv:2307.11352v1 [cs.LG])",
    "abstract": "In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.",
    "link": "http://arxiv.org/abs/2307.11352",
    "context": "Title: Model-based Offline Reinforcement Learning with Count-based Conservatism. (arXiv:2307.11352v1 [cs.LG])\nAbstract: In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.",
    "path": "papers/23/07/2307.11352.json",
    "total_tokens": 919,
    "translated_title": "基于模型的离线强化学习与基于计数保守性的结合方法",
    "translated_abstract": "本文提出了一种名为$\\texttt{Count-MORL}$的基于模型的离线强化学习方法，它利用了状态动作对的计数估计来量化模型估计误差，据我们所知，这是首个证明了基于计数保守性在基于模型的离线深度强化学习中的有效性的算法。我们首先展示了估计误差与状态动作对的频率成反比的关系。其次，我们证明了在基于计数保守模型下学习的策略提供了接近最优性能的保证。通过大量的数值实验，我们验证了使用哈希编码实现的$\\texttt{Count-MORL}$在D4RL基准数据集上显著优于现有离线强化学习算法。代码可在$\\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$上进行访问。",
    "tldr": "本文提出了一种基于模型的离线强化学习方法$\\texttt{Count-MORL}$，通过利用计数保守性来量化模型估计误差，证明了基于计数保守性在离线深度强化学习中的有效性，并展示了学习到的策略提供了接近最优性能的保证。",
    "en_tdlr": "This paper proposes a model-based offline reinforcement learning method, called Count-MORL, which quantifies model estimation error using count-based conservatism. It demonstrates the effectiveness of count-based conservatism in offline deep reinforcement learning and shows that the learned policy provides near-optimal performance guarantees."
}