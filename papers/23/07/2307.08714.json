{
    "title": "Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages. (arXiv:2307.08714v1 [cs.CL])",
    "abstract": "We propose an efficient modeling framework for cross-lingual named entity recognition in semi-structured text data. Our approach relies on both knowledge distillation and consistency training. The modeling framework leverages knowledge from a large language model (XLMRoBERTa) pre-trained on the source language, with a student-teacher relationship (knowledge distillation). The student model incorporates unsupervised consistency training (with KL divergence loss) on the low-resource target language.  We employ two independent datasets of SMSs in English and Arabic, each carrying semi-structured banking transaction information, and focus on exhibiting the transfer of knowledge from English to Arabic. With access to only 30 labeled samples, our model can generalize the recognition of merchants, amounts, and other fields from English to Arabic. We show that our modeling approach, while efficient, performs best overall when compared to state-of-the-art approaches like DistilBERT pre-trained ",
    "link": "http://arxiv.org/abs/2307.08714",
    "context": "Title: Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages. (arXiv:2307.08714v1 [cs.CL])\nAbstract: We propose an efficient modeling framework for cross-lingual named entity recognition in semi-structured text data. Our approach relies on both knowledge distillation and consistency training. The modeling framework leverages knowledge from a large language model (XLMRoBERTa) pre-trained on the source language, with a student-teacher relationship (knowledge distillation). The student model incorporates unsupervised consistency training (with KL divergence loss) on the low-resource target language.  We employ two independent datasets of SMSs in English and Arabic, each carrying semi-structured banking transaction information, and focus on exhibiting the transfer of knowledge from English to Arabic. With access to only 30 labeled samples, our model can generalize the recognition of merchants, amounts, and other fields from English to Arabic. We show that our modeling approach, while efficient, performs best overall when compared to state-of-the-art approaches like DistilBERT pre-trained ",
    "path": "papers/23/07/2307.08714.json",
    "total_tokens": 924,
    "translated_title": "低资源语言中金融交易数据的跨语言命名实体识别",
    "translated_abstract": "我们提出了一个高效的建模框架，用于跨语言的命名实体识别半结构化文本数据。我们的方法依赖于知识蒸馏和一致性训练。建模框架利用在源语言上预训练的大型语言模型（XLMRoBERTa）的知识，并采用了学生-教师关系（知识蒸馏）。学生模型在低资源目标语言上采用无监督的一致性训练（使用KL散度损失）。我们使用了两个独立的英语和阿拉伯语短信数据集，每个数据集都包含半结构化的银行交易信息，并重点展示从英语到阿拉伯语的知识传递。在仅有30个标记样本的情况下，我们的模型可以从英语泛化到阿拉伯语的识别商家、金额和其他字段。我们展示了我们的建模方法在效率上表现最佳，与DistilBERT等最先进的方法相比。",
    "tldr": "我们提出了一个高效的跨语言命名实体识别的建模框架，利用知识蒸馏和一致性训练方法，在低资源语言中通过从英语到阿拉伯语的知识传递，能够准确识别商家、金额和其他字段。"
}