{
    "title": "Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)",
    "abstract": "Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether an efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity",
    "link": "http://arxiv.org/abs/2307.02472",
    "context": "Title: Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)\nAbstract: Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether an efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity",
    "path": "papers/23/07/2307.02472.json",
    "total_tokens": 925,
    "translated_title": "自然语言证明规划的演绎可加性研究",
    "translated_abstract": "当前设计用于多步骤命题验证的自然语言系统通常分为两个阶段：使用启发式方法检索一组相关的前提陈述（规划），然后使用大型语言模型从这些陈述中生成新的结论（演绎）。规划阶段通常需要昂贵的Transformer操作，并且无法扩展到任意数量的前提陈述。本文研究了是否可以通过与演绎推理兼容的嵌入空间实现高效的规划启发式方法。具体地，我们评估了嵌入空间是否具有我们称之为演绎可加性的特性：前提陈述嵌入的总和应该接近基于这些前提的结论的嵌入。除了来自GPT3的微调嵌入和来自BM25的稀疏嵌入之外，我们还探索了多种现成的密集嵌入源。我们在内在上研究了嵌入模型，评估了演绎可加性的属性是否存在。",
    "tldr": "本论文研究了自然语言证明规划中的演绎可加性，探讨了是否能够通过嵌入空间实现高效的规划启发式方法。研究结果表明，嵌入空间的前提陈述总和接近于基于这些前提的结论嵌入。从而证明了演绎可加性的存在。",
    "en_tdlr": "This paper investigates deductive additivity in natural language proof planning and explores the possibility of efficient planning heuristics using embedding spaces. The study shows that the sum of premise statement embeddings is close to embeddings of conclusions based on those premises, demonstrating the existence of deductive additivity."
}