{
    "title": "U-Calibration: Forecasting for an Unknown Agent. (arXiv:2307.00168v1 [cs.LG])",
    "abstract": "We consider the problem of evaluating forecasts of binary events whose predictions are consumed by rational agents who take an action in response to a prediction, but whose utility is unknown to the forecaster. We show that optimizing forecasts for a single scoring rule (e.g., the Brier score) cannot guarantee low regret for all possible agents. In contrast, forecasts that are well-calibrated guarantee that all agents incur sublinear regret. However, calibration is not a necessary criterion here (it is possible for miscalibrated forecasts to provide good regret guarantees for all possible agents), and calibrated forecasting procedures have provably worse convergence rates than forecasting procedures targeting a single scoring rule.  Motivated by this, we present a new metric for evaluating forecasts that we call U-calibration, equal to the maximal regret of the sequence of forecasts when evaluated under any bounded scoring rule. We show that sublinear U-calibration error is a necessary",
    "link": "http://arxiv.org/abs/2307.00168",
    "context": "Title: U-Calibration: Forecasting for an Unknown Agent. (arXiv:2307.00168v1 [cs.LG])\nAbstract: We consider the problem of evaluating forecasts of binary events whose predictions are consumed by rational agents who take an action in response to a prediction, but whose utility is unknown to the forecaster. We show that optimizing forecasts for a single scoring rule (e.g., the Brier score) cannot guarantee low regret for all possible agents. In contrast, forecasts that are well-calibrated guarantee that all agents incur sublinear regret. However, calibration is not a necessary criterion here (it is possible for miscalibrated forecasts to provide good regret guarantees for all possible agents), and calibrated forecasting procedures have provably worse convergence rates than forecasting procedures targeting a single scoring rule.  Motivated by this, we present a new metric for evaluating forecasts that we call U-calibration, equal to the maximal regret of the sequence of forecasts when evaluated under any bounded scoring rule. We show that sublinear U-calibration error is a necessary",
    "path": "papers/23/07/2307.00168.json",
    "total_tokens": 873,
    "translated_title": "U-Calibration: 针对未知代理的预测",
    "translated_abstract": "我们考虑评估针对由理性代理消费预测并根据预测采取行动的二元事件的预测问题，但是对于预测者来说理性代理的效用是未知的。我们表明，优化单一评分规则（例如Brier得分）的预测无法保证对于所有可能的代理都具有低悔恨值。相反，良好校准的预测保证所有代理都具有次线性的悔恨值。然而，此处校准并不是必要条件（对于校准不良的预测，仍可能为所有可能的代理提供良好的悔恨值保证），而校准的预测程序与针对单一评分规则的预测程序相比具有更差的收敛速度。受此启发，我们提出了一种新的用于评估预测的度量指标，称为U-校准，即在任何有界评分规则下评估时，预测序列的最大悔恨值。我们表明，次线性的U-校准误差是必要的。",
    "tldr": "研究者们提出了一种针对未知代理的预测问题的新度量指标U-校准，该指标能够保证所有代理具有次线性的悔恨值。"
}