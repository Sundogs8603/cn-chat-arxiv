{
    "title": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain. (arXiv:2307.10512v1 [cs.CL])",
    "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.",
    "link": "http://arxiv.org/abs/2307.10512",
    "context": "Title: IvyGPT: InteractiVe Chinese pathwaY language model in medical domain. (arXiv:2307.10512v1 [cs.CL])\nAbstract: General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.",
    "path": "papers/23/07/2307.10512.json",
    "total_tokens": 857,
    "translated_title": "IvyGPT：基于医学领域的互动式中文路径语言模型",
    "translated_abstract": "一般的大型语言模型（LLM）如ChatGPT已取得了显著的成功。然而，由于精度不高和无法提供医疗建议，这些LLM在医学领域并未广泛应用。我们提出了一种基于LLaMA的LLM IvyGPT，它通过高质量的医学问答（QA）示例和人类反馈强化学习（RLHF）进行训练和微调。经过有监督微调后，IvyGPT具有良好的多轮对话能力，但在综合诊断等其他方面不能像医生一样运行。通过RLHF，IvyGPT可以输出更丰富的诊断和治疗答案，更接近于人类。在训练过程中，我们使用QLoRA在少量NVIDIA A100（80GB) GPU上训练了330亿个参数。实验结果表明，IvyGPT在医学GPT模型中表现优于其他模型。",
    "tldr": "IvyGPT是一种基于医学领域的中文互动语言模型，通过使用高质量的医学问答示例和人类反馈强化学习进行训练和微调，它能够输出更丰富的诊断和治疗答案，从而在医学GPT模型中表现出色。"
}