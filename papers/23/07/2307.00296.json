{
    "title": "Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])",
    "abstract": "We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural",
    "link": "http://arxiv.org/abs/2307.00296",
    "context": "Title: Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])\nAbstract: We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural",
    "path": "papers/23/07/2307.00296.json",
    "total_tokens": 984,
    "translated_title": "扩大步长和算子学习的加速原始-对偶方法在非光滑最优控制问题中的应用",
    "translated_abstract": "我们考虑一类具有偏微分方程（PDE）约束的非光滑最优控制问题，由于其非光滑目标函数和离散化后的高维和病态系统，这类问题非常具有挑战性。我们重点研究了原始-对偶方法的应用，该方法可以分别处理不同类型的变量，因此每次迭代的主要计算只需要解决两个PDE。我们的目标是通过使用较大的步长或算子学习技术来加速原始-对偶方法。对于具有较大步长的加速原始-对偶方法，其收敛性仍然可以得到严格证明，同时它以一种简单且普遍的方式数值上加速了原始-对偶方法。对于算子学习加速，我们构建了深度神经网络代理模型来表示涉及的PDE。一旦学习到一个神经算子，解决一个PDE只需要进行神经网络的前向传播。",
    "tldr": "该论文研究了在非光滑最优控制问题中加速原始-对偶方法的两种方法：增大步长和算子学习。研究表明，增大步长的加速原始-对偶方法可以在保证收敛性的同时简单有效地提高计算速度；而算子学习则通过构建神经网络代理模型来加速求解涉及的偏微分方程。",
    "en_tdlr": "This paper investigates two methods, enlarged step sizes and operator learning, to accelerate the primal-dual method for nonsmooth optimal control problems. The research shows that the accelerated primal-dual method with larger step sizes can effectively improve computation speed while maintaining convergence; while the operator learning approach uses neural networks to accelerate the solving of partial differential equations."
}