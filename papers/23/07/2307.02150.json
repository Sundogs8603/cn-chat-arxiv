{
    "title": "Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)",
    "abstract": "Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att",
    "link": "http://arxiv.org/abs/2307.02150",
    "context": "Title: Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)\nAbstract: Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att",
    "path": "papers/23/07/2307.02150.json",
    "total_tokens": 925,
    "translated_title": "跨深度学习架构实现特征归因的协调: 提升解释性和一致性",
    "translated_abstract": "确保机器学习模型的可信度和可解释性对于其在现实世界应用中至关重要。特征归因方法已经引起了许多关注，它们通过将重要性归因给个别输入特征来提供模型预测的局部解释。该研究探讨了特征归因在各种深度学习架构（如卷积神经网络（CNN）和视觉转换器）之间的泛化能力。我们旨在评估将特征归因方法作为未来检测器的可行性，并研究如何在采用不同架构但以相同数据分布训练的多个模型之间协调这些特征。通过探索这种协调，我们旨在开发出更一致和乐观的特征归因理解，提高深度学习模型之间局部解释的一致性。我们的研究结果凸显了实现特征归因协调性的潜力。",
    "tldr": "该论文研究了特征归因方法在不同深度学习架构之间的通用性，并探讨了在多个模型上协调特征归因的可行性。研究结果显示特征归因的协调有助于提高深度学习模型的解释性和一致性。",
    "en_tdlr": "This paper investigates the generalization of feature attribution methods across different deep learning architectures and examines the feasibility of harmonizing feature attributions across multiple models. The findings highlight the potential of coordinated feature attributions to enhance the interpretability and consistency of deep learning models."
}