{
    "title": "Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])",
    "abstract": "Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from \"difficult\" to \"easy\" based on the priorities. Then the p",
    "link": "http://arxiv.org/abs/2307.10845",
    "context": "Title: Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])\nAbstract: Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from \"difficult\" to \"easy\" based on the priorities. Then the p",
    "path": "papers/23/07/2307.10845.json",
    "total_tokens": 953,
    "translated_title": "自适应权重巩固用于持续学习",
    "translated_abstract": "持续学习算法在防止顺序任务学习中的灾难性遗忘方面很受欢迎，它们使得新任务的参数保持接近于先前任务的参数。然而，(1)如果不区分先前学习任务的贡献，新的持续学习器的性能将降低；(2)随着任务数量的增加，计算成本将大大增加，因为大多数现有算法在学习新任务时需要对所有先前任务进行正则化。为了解决以上挑战，我们提出了一种自适应自我学习权重巩固（spWC）框架，通过评估先前任务的区分性贡献，实现稳健的持续学习。具体而言，我们开发了一种自适应正则化方法，通过基于关键绩效指标（即准确度）来衡量难度，以反映过去任务的优先级。在遇到新任务时，根据优先级，对所有先前任务进行从“困难”到“简单”的排序。然后，p",
    "tldr": "提出了一种自适应权重巩固（spWC）框架，通过评估先前任务的区分性贡献，实现稳健的持续学习。具体而言，通过自适应正则化方法，根据关键绩效指标（即准确度）来衡量难度，反映过去任务的优先级，并对先前任务进行排序。"
}