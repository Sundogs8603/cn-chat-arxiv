{
    "title": "Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])",
    "abstract": "Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks",
    "link": "http://arxiv.org/abs/2307.14823",
    "context": "Title: Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])\nAbstract: Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks",
    "path": "papers/23/07/2307.14823.json",
    "total_tokens": 961,
    "translated_title": "残差循环网络中的褪退记忆作为归纳偏差",
    "translated_abstract": "残差连接被提出作为一种基于架构的归纳偏差，以解决反向传播算法训练的前馈和循环神经网络中梯度爆炸和消失的问题，并提高任务性能。然而，我们对残差连接如何影响循环神经网络的动态和褪退记忆属性知之甚少。在这里，我们引入了弱耦合残差循环网络(WCRNNs)，其中残差连接导致了明确定义的李雅普诺夫指数，并允许研究褪退记忆的属性。我们研究了WCRNNs的残差连接如何影响它们的性能、网络动力学和记忆属性在一组基准任务上。我们发现，几种不同形式的残差连接产生了有效的归纳偏差，从而增加了网络的表达能力。特别是，具有以下特点的残差连接：(i) 导致网络动态接近混沌的边缘，(ii) 允许网络在长时间内保持记忆。",
    "tldr": "通过使用残差连接作为归纳偏差，我们提出了一种弱耦合残差循环网络，并研究了其对网络性能、动力学和记忆属性的影响。我们发现，几种不同形式的残差连接可以增加网络的表达能力，并产生有效的归纳偏差。",
    "en_tdlr": "We propose weakly coupled residual recurrent networks (WCRNNs) and investigate how residual connections influence network performance, dynamics, and memory properties. We find that different forms of residual connections can increase network expressivity and result in effective inductive biases."
}