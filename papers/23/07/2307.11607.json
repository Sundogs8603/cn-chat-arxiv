{
    "title": "Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])",
    "abstract": "Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this ou",
    "link": "http://arxiv.org/abs/2307.11607",
    "context": "Title: Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])\nAbstract: Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this ou",
    "path": "papers/23/07/2307.11607.json",
    "total_tokens": 951,
    "translated_title": "利用替代特征选择找到最优的多样特征集",
    "translated_abstract": "特征选择是获取小型、可解释且高精度预测模型的一种常见方法。传统的特征选择方法通常只能得到一个特征集，这在某些场景下可能不足够。例如，用户可能对寻找具有相似预测质量但提供不同数据解释的替代特征集感兴趣。在本文中，我们引入了替代特征选择，并将其形式化为一个优化问题。特别地，我们通过约束定义了替代特征，并使用户可以控制替代的数量和差异性。接下来，我们分析了这个优化问题的复杂性并展示了其NP-hard性质。进一步地，我们讨论了如何将传统的特征选择方法作为目标集成。最后，我们使用30个分类数据集评估了替代特征选择的效果。我们观察到替代特征集确实可能具有较高的预测质量，并分析了几个影响这一结果的因素。",
    "tldr": "本文引入了替代特征选择的概念，将其形式化为优化问题，并通过约束定义了替代特征集，使用户可以控制替代的数量和差异性。我们证明了该问题的NP-hard性，并讨论了如何将传统特征选择方法作为目标集成。实验证明替代特征集确实可以具有高预测质量，同时分析了几个影响因素。"
}