{
    "title": "ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])",
    "abstract": "Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro sco",
    "link": "http://arxiv.org/abs/2307.14912",
    "context": "Title: ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])\nAbstract: Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro sco",
    "path": "papers/23/07/2307.14912.json",
    "total_tokens": 947,
    "translated_title": "ARC-NLP在PAN 2023上的应用：用于触发词检测的层次化长文本分类",
    "translated_abstract": "粉丝小说是一种在已建立的虚构世界中进行的创造性写作形式，已经在网络上拥有大量的追随者。然而，确保参与者的福祉和安全已经成为该社区的一个关键问题。检测可能导致读者情感困扰或创伤的刺激性内容是一个重要挑战。本文描述了我们在PAN CLEF 2023的触发词检测共享任务中的方法，我们希望检测给定粉丝小说文档中的多个触发词。为此，我们构建了一个使用基于Transformer的语言模型的递归层次模型。在我们的方法中，我们首先将长文档拆分为较小的段落，并使用它们来微调Transformer模型。然后，我们从微调的Transformer模型中提取特征嵌入，这些嵌入被用作多标签设置中用于触发词检测的多个LSTM模型的训练的输入。我们的模型达到了F1-macro sco",
    "tldr": "本文介绍了ARC-NLP在PAN 2023的应用，通过构建层次化模型来进行触发词检测，实现了对粉丝小说文档中多个触发词的检测。该模型利用Transformer模型进行微调，并在多标签设置中使用LSTM模型进行训练。",
    "en_tdlr": "This paper presents the application of ARC-NLP at PAN 2023, achieving trigger detection in fanfiction documents by building a hierarchical model. The model fine-tunes a Transformer model and trains LSTM models in a multi-label setting to detect multiple trigger words."
}