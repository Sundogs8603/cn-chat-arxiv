{
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)",
    "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.",
    "link": "http://arxiv.org/abs/2307.00782",
    "context": "Title: ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)\nAbstract: While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.",
    "path": "papers/23/07/2307.00782.json",
    "total_tokens": 978,
    "translated_title": "ContextSpeech：用于段落阅读的富有表现力和高效的文本转语音系统",
    "translated_abstract": "尽管最先进的文本转语音系统可以在句子级别上生成非常高质量的自然语音，但它们在段落/长篇阅读的语音生成方面仍面临巨大挑战。这些不足之处主要是因为：一是忽视了跨句子的上下文信息，二是长篇合成过程中的高计算和内存成本。为了解决这些问题，本研究提出了一种轻量级但有效的文本转语音系统ContextSpeech。具体而言，我们首先设计了一个内存缓存的循环机制，将全局文本和语音上下文融入到句子编码中；然后构建了层次化的文本语义结构，以扩大全局上下文增强的范围；此外，我们还整合了线性化的自注意力机制来提高模型的效率。实验证明，ContextSpeech在段落阅读中显著提升了语音质量和韵律表达能力，同时具备竞争力的模型效率。音频样本可访问链接：https://contextspeech.",
    "tldr": "本研究提出了一种轻量级但有效的文本转语音系统ContextSpeech，通过设计内存缓存的循环机制和构建层次化的文本语义结构，将全局文本和语音上下文融入到句子编码中，以解决段落阅读中的语音生成挑战，并且使用线性化的自注意力机制提高了模型的效率。",
    "en_tdlr": "This study presents a lightweight yet effective text-to-speech system, called ContextSpeech, which tackles the challenges in speech generation for paragraph reading by incorporating global text and speech context, using a memory-cached recurrence mechanism and hierarchically-structured textual semantics, and enhancing model efficiency with linearized self-attention."
}