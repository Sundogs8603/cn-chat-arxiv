{
    "title": "A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System Ranking Consistency and Discriminative Power. (arXiv:2307.02936v1 [cs.IR])",
    "abstract": "Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for offline evaluation metrics. This framework allows information retrieval (IR) researchers to design evaluation metrics through the flexible combination of user browsing models and user gain aggregations. However, the statistical stability of C/W/L/A metrics with different aggregations is not yet investigated. In this study, we investigate the statistical stability of C/W/L/A metrics from the perspective of: (1) the system ranking similarity among aggregations, (2) the system ranking consistency of aggregations and (3) the discriminative power of aggregations. More specifically, we combined various aggregation functions with the browsing model of Precision, Discounted Cumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision (AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of system ranking similarity, system ranking consistency and discriminative power on two offline",
    "link": "http://arxiv.org/abs/2307.02936",
    "context": "Title: A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System Ranking Consistency and Discriminative Power. (arXiv:2307.02936v1 [cs.IR])\nAbstract: Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for offline evaluation metrics. This framework allows information retrieval (IR) researchers to design evaluation metrics through the flexible combination of user browsing models and user gain aggregations. However, the statistical stability of C/W/L/A metrics with different aggregations is not yet investigated. In this study, we investigate the statistical stability of C/W/L/A metrics from the perspective of: (1) the system ranking similarity among aggregations, (2) the system ranking consistency of aggregations and (3) the discriminative power of aggregations. More specifically, we combined various aggregation functions with the browsing model of Precision, Discounted Cumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision (AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of system ranking similarity, system ranking consistency and discriminative power on two offline",
    "path": "papers/23/07/2307.02936.json",
    "total_tokens": 919,
    "translated_title": "C/W/L/A指标的元评估：系统排名相似性，系统排名一致性和区分能力",
    "translated_abstract": "最近，Moffat等人提出了一个名为C/W/L/A的离线评估指标的分析框架。这个框架允许信息检索（IR）研究人员通过灵活组合用户浏览模型和用户收益聚合来设计评估指标。然而，不同聚合方式的C/W/L/A指标的统计稳定性尚未被研究。在本研究中，我们从以下三个方面对C/W/L/A指标的统计稳定性进行了调查：（1）聚合方式之间的系统排名相似性，（2）聚合方式的系统排名一致性，和（3）聚合方式的区分能力。具体而言，我们将不同的聚合函数与Precision、Discounted Cumulative Gain (DCG)、Rank-Biased Precision (RBP)、INST、Average Precision (AP)和Expected Reciprocal Rank (ERR)等浏览模型相结合，通过系统排名相似性、系统排名一致性和区分能力等指标评估它们的性能。",
    "tldr": "本研究通过对不同聚合方式的C/W/L/A指标进行元评估，研究发现它们在系统排名相似性、系统排名一致性和区分能力等方面具有一定的统计稳定性。",
    "en_tdlr": "This study investigates the statistical stability of C/W/L/A metrics with different aggregations. Results show that these metrics exhibit certain degree of stability in terms of system ranking similarity, system ranking consistency, and discriminative power."
}