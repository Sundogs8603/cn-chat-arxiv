{
    "title": "Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution. (arXiv:2307.16422v1 [math.ST])",
    "abstract": "Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.  This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.  We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to g",
    "link": "http://arxiv.org/abs/2307.16422",
    "context": "Title: Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution. (arXiv:2307.16422v1 [math.ST])\nAbstract: Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.  This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.  We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to g",
    "path": "papers/23/07/2307.16422.json",
    "total_tokens": 871,
    "translated_title": "保证从经验分布中最大偏差的最佳生成建模",
    "translated_abstract": "生成建模是一种广泛应用于科学和工业领域的机器学习方法。其主要目标是在给定训练数据的情况下，模拟从未知分布中抽取的新示例，同时确保多样性并避免从训练数据中复制示例。本文提出了关于训练生成模型的理论见解，该模型具有两个属性：（i）将真实数据生成分布与训练数据生成分布替换的误差在样本大小趋近无穷时应最佳收敛于零；（ii）训练数据生成分布应远离复制训练数据中示例的任何分布。我们提供了以有限样本风险界为形式的非渐近结果，量化了这些属性，并取决于相关参数，如样本大小、环境空间的维数和潜空间的维数。我们的结果适用于生成模型的各种应用情况。",
    "tldr": "本文提供了一种理论方法来训练生成模型，确保其在样本大小趋近无穷时与真实数据生成分布的误差收敛为零，并且远离复制训练数据中示例的任何分布。"
}