{
    "title": "Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])",
    "abstract": "Knowledge graph completion (KGC) is the task of inferencing missing facts from any given knowledge graphs (KG). Previous KGC methods typically represent knowledge graph entities and relations as trainable continuous embeddings and fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the missing entities. To achieve this, they either use shallow linear transformations or deep convolutional modules. However, the linear transformations suffer from the expressiveness issue while the deep convolutional modules introduce unnecessary inductive bias, which could potentially degrade the model performance. Thus, we propose a novel Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer first segments the embedding into a sequence of patches and then employs cross-attention modules to allow bi-directional embedding feature interaction between the entities and relations, leading to a bet",
    "link": "http://arxiv.org/abs/2307.05627",
    "context": "Title: Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])\nAbstract: Knowledge graph completion (KGC) is the task of inferencing missing facts from any given knowledge graphs (KG). Previous KGC methods typically represent knowledge graph entities and relations as trainable continuous embeddings and fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the missing entities. To achieve this, they either use shallow linear transformations or deep convolutional modules. However, the linear transformations suffer from the expressiveness issue while the deep convolutional modules introduce unnecessary inductive bias, which could potentially degrade the model performance. Thus, we propose a novel Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer first segments the embedding into a sequence of patches and then employs cross-attention modules to allow bi-directional embedding feature interaction between the entities and relations, leading to a bet",
    "path": "papers/23/07/2307.05627.json",
    "total_tokens": 946,
    "translated_title": "基于Transformer的补丁细化模型，用于知识图谱补全",
    "translated_abstract": "知识图谱补全是从给定的知识图谱中推断缺失事实的任务。先前的方法通常将知识图谱实体和关系表示为可训练的连续嵌入，并将实体$h$（或$t$）和关系$r$的嵌入融合为查询的隐藏表示$(h, r, ?)$（或$(?, r, t$)）以近似缺失实体。为了实现这个目标，他们要么使用浅层线性变换，要么使用深度卷积模块。然而，线性变换存在表达能力问题，而深度卷积模块引入了不必要的归纳偏差，可能降低模型性能。因此，我们提出了一种新颖的基于Transformer的补丁细化模型（PatReFormer）用于知识图谱补全。PatReFormer首先将嵌入分割成一系列补丁，然后使用交叉注意力模块允许实体和关系之间的双向嵌入特征交互，从而获得更好的性能。",
    "tldr": "本文提出了一种基于Transformer的补丁细化模型（PatReFormer）用于知识图谱补全。该模型通过分割嵌入并使用交叉注意力模块来改进实体和关系之间的嵌入特征交互，从而提高了模型性能。",
    "en_tdlr": "This paper proposes a Transformer-based patch refinement model (PatReFormer) for knowledge graph completion. The model improves the interaction of embedding features between entities and relations by segmenting the embeddings and using cross-attention modules, leading to enhanced performance."
}