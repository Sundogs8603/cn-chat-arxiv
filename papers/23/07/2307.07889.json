{
    "title": "Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])",
    "abstract": "Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste",
    "link": "http://arxiv.org/abs/2307.07889",
    "context": "Title: Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])\nAbstract: Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste",
    "path": "papers/23/07/2307.07889.json",
    "total_tokens": 962,
    "translated_title": "通过基于大型语言模型的比较判定进行零样本NLG评估",
    "translated_abstract": "评估自然语言生成（NLG）输出是至关重要但费时费力的。虽然已经提出了各种自动NLG评估方法，但它们通常是特定任务特定领域的，需要针对特定领域和属性进行工程设计。在这项工作中，我们提出了一种使用开源大型语言模型（LLMs）进行零样本NLG评估的稳健方法，采用了配对比较判定的方式。这种方法的动机是，即使作为人类，确定两个选项中哪个更好要比独立客观评分每个选项更容易。我们利用这一观察结果并利用LLMs新兴的能力，通过探测FlanT5，确定两个候选回应中哪一个更好，而不是指定绝对分数。我们的结果表明，比较评估是比绝对评分更有效的方法，使得较小的开源LLMs能够达到与更大的公共访问API相当的性能。我们评估了系统。",
    "tldr": "本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。",
    "en_tdlr": "This research proposes a zero-shot approach to evaluating Natural Language Generation (NLG) outputs using open-source Large Language Models (LLMs) through pairwise comparative judgment. The results show that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs."
}