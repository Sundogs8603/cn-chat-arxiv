{
    "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection. (arXiv:2307.11077v2 [cs.CV] UPDATED)",
    "abstract": "The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised par",
    "link": "http://arxiv.org/abs/2307.11077",
    "context": "Title: AlignDet: Aligning Pre-training and Fine-tuning in Object Detection. (arXiv:2307.11077v2 [cs.CV] UPDATED)\nAbstract: The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised par",
    "path": "papers/23/07/2307.11077.json",
    "total_tokens": 916,
    "translated_title": "AlignDet: 对齐目标检测的预训练与微调",
    "translated_abstract": "大规模预训练后进行微调的范式在各种目标检测算法中被广泛应用。本文揭示了现有实践中预训练和微调过程之间的数据、模型和任务差异，这些差异隐含地限制了检测器的性能、泛化能力和收敛速度。为此，我们提出了AlignDet，一个统一的预训练框架，可适用于各种现有检测器，以减轻这些差异。AlignDet将预训练过程分解为两个阶段，即图像域和框域预训练。图像域预训练优化检测主干以捕捉整体视觉抽象，而框域预训练学习实例级语义和任务感知概念以初始化主干之外的部分。通过融入自监督预训练的主干，我们可以对各种检测器的所有模块进行无监督预训练。",
    "tldr": "本文提出了一个统一的预训练框架AlignDet，通过将预训练过程分解为图像域和框域预训练，可以减轻目标检测中现有实践中的数据、模型和任务差异，提高检测器性能和泛化能力。",
    "en_tdlr": "This paper proposes a unified pre-training framework called AlignDet, which decomposes the pre-training process into image-domain and box-domain pre-training, to alleviate the discrepancies in data, model, and task in existing practices in object detection, and improve detector performance and generalization ability."
}