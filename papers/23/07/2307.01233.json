{
    "title": "RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations. (arXiv:2307.01233v1 [cs.SD])",
    "abstract": "Significant progress has been made in speaker dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Spee",
    "link": "http://arxiv.org/abs/2307.01233",
    "context": "Title: RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations. (arXiv:2307.01233v1 [cs.SD])\nAbstract: Significant progress has been made in speaker dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Spee",
    "path": "papers/23/07/2307.01233.json",
    "total_tokens": 898,
    "translated_title": "RobustL2S: 利用自监督表示的个别讲话者唇语合成的鲁棒性",
    "translated_abstract": "在个别讲话者唇语合成方面取得了显著进展，旨在从无声的说话人面部视频中生成语音。目前最先进的方法主要使用非自回归的序列到序列架构，直接从唇部表示预测mel频谱图或音频波形。我们假设直接mel预测会因训练/模型效率受到语音内容与环境信息和讲话者特征的纠缠而受到限制。为此，我们提出了RobustL2S，一个用于唇语合成的模块化框架。首先，非自回归的序列到序列模型将自监督的视觉特征映射到解耦的语音内容表示。然后，一个声码器将语音特征转换为原始波形。广泛的评估证实了我们的设置的有效性，在无约束的Lip2Wav数据集和约束的GRID和TCD-TIMIT数据集上实现了最先进的性能。",
    "tldr": "RobustL2S是一个模块化框架，利用自监督表示将唇语转换为语音。该方法通过解耦语音内容并对其进行高效地训练，实现了在相关数据集上的最先进性能。",
    "en_tdlr": "RobustL2S is a modularized framework that converts lip movements to speech using self-supervised representations. By disentangling speech content, it achieves state-of-the-art performance on relevant datasets through efficient training."
}