{
    "title": "High Probability Analysis for Non-Convex Stochastic Optimization with Clipping. (arXiv:2307.13680v1 [cs.LG])",
    "abstract": "Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\\alpha$-th moments for some $\\alpha \\in (1, 2]$, which is much weaker than the standard bounded second-moment ass",
    "link": "http://arxiv.org/abs/2307.13680",
    "context": "Title: High Probability Analysis for Non-Convex Stochastic Optimization with Clipping. (arXiv:2307.13680v1 [cs.LG])\nAbstract: Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\\alpha$-th moments for some $\\alpha \\in (1, 2]$, which is much weaker than the standard bounded second-moment ass",
    "path": "papers/23/07/2307.13680.json",
    "total_tokens": 876,
    "translated_title": "对于具有修剪的非凸随机优化的高概率分析",
    "translated_abstract": "梯度修剪是稳定神经网络训练过程的常用技术。越来越多的研究表明，梯度修剪是处理随机优化中出现的重尾行为的一种有前景的技术。虽然梯度修剪很重要，但其理论保证很少。大多数理论保证只提供期望值分析，并且仅关注优化性能。在本文中，我们提供了在非凸设置中的高概率分析，并同时推导了带有梯度修剪的流行随机优化算法的优化界限和泛化界限，包括随机梯度下降及其动量和自适应步长变体。在梯度修剪的情况下，我们研究了一个重尾的假设，即梯度只有对于某些$\\alpha \\in (1, 2]$有界的$\\alpha$-阶矩，这比标准的有界二阶矩假设要弱。",
    "tldr": "本文提供了对于具有修剪的非凸随机优化的高概率分析，同时推导了流行随机优化算法的优化界限和泛化界限，为处理重尾行为提供了理论基础。",
    "en_tdlr": "This paper provides high probability analysis for non-convex stochastic optimization with clipping, deriving optimization and generalization bounds for popular stochastic optimization algorithms. The results offer theoretical foundation for dealing with heavy-tailed behavior."
}