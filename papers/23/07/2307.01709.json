{
    "title": "Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])",
    "abstract": "Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our pr",
    "link": "http://arxiv.org/abs/2307.01709",
    "context": "Title: Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])\nAbstract: Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our pr",
    "path": "papers/23/07/2307.01709.json",
    "total_tokens": 899,
    "translated_title": "通过条件软提示，将结构和文本相结合，实现有效的知识图谱补全",
    "translated_abstract": "知识图谱补全通常需要结构和文本信息的共同作用。预训练的语言模型(PLMs)已被用于学习文本信息，在知识图谱补全任务中通常采用精调模式。然而，精调后的PLMs往往过于注重文本信息，忽视了结构知识。为了解决这个问题，本文提出了CSProm-KG(用于知识图谱补全的条件软提示)，在结构信息和文本知识之间保持平衡。CSProm-KG只调整由实体和关系表示生成的条件软提示参数。我们验证了CSProm-KG在三个流行的静态KGC基准WN18RR、FB15K-237和Wikidata5M，以及两个时态KGC基准ICEWS14和ICEWS05-15上的有效性。CSProm-KG的性能超过了竞争性基准模型，并在这些基准上达到了新的最优水平。我们进行进一步的分析，展示了我们方法的有效性。",
    "tldr": "提出了CSProm-KG方法，通过条件软提示实现结构与文本的有效融合，在知识图谱补全任务中取得了新的最优性能。",
    "en_tdlr": "This paper proposes CSProm-KG, a method that effectively bridges structure and text through conditional soft prompting for knowledge graph completion. It outperforms competitive baseline models and sets new state-of-the-art results on popular benchmarks."
}