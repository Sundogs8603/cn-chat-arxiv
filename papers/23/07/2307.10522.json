{
    "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models. (arXiv:2307.10522v1 [cs.CL])",
    "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
    "link": "http://arxiv.org/abs/2307.10522",
    "context": "Title: Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models. (arXiv:2307.10522v1 [cs.CL])\nAbstract: Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
    "path": "papers/23/07/2307.10522.json",
    "total_tokens": 925,
    "translated_title": "Gender-tuning: 授权微调用于去偏置预训练语言模型的方法",
    "translated_abstract": "最近的研究表明，广泛使用的预训练语言模型（PLMs）从大型非调控的预训练语料库中传播社会偏见。现有的解决方案需要去偏置的训练过程和数据集，这些都需要大量资源和成本。此外，这些方法会损害PLMs在下游任务中的性能。在本研究中，我们提出了Gender-tuning，通过在下游任务的数据集上进行微调来去偏置PLMs。为了实现这个目标，Gender-tuning将掩码语言建模（MLM）的训练目标集成到微调的训练过程中。全面的实验证明，Gender-tuning在PLMs的平均性别偏见得分方面优于现有的基准线，同时仅使用下游任务的数据集来提高PLMs在下游任务中的性能。此外，Gender-tuning是一个可部署的去偏置工具，适用于任何采用原有微调方法的PLM。",
    "tldr": "本研究提出了Gender-tuning方法，通过在下游任务的数据集上进行微调，去偏置预训练语言模型（PLMs），不仅能在PLMs的性别偏见得分方面优于现有方法，而且仅使用下游任务的数据集即可提高PLMs在下游任务中的性能。",
    "en_tdlr": "This paper proposes Gender-tuning, a method that debiases pre-trained language models (PLMs) by fine-tuning on downstream tasks' datasets. It surpasses existing methods in terms of gender bias scores and improves PLMs' performance on downstream tasks solely using the downstream tasks' dataset."
}