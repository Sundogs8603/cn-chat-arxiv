{
    "title": "Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])",
    "abstract": "Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.",
    "link": "http://arxiv.org/abs/2307.05831",
    "context": "Title: Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])\nAbstract: Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.",
    "path": "papers/23/07/2307.05831.json",
    "total_tokens": 961,
    "translated_title": "通过损失函数曲率视角揭示记忆化过程",
    "translated_abstract": "神经网络参数过多，很容易过拟合训练数据。极端情况下，它们可以完全记忆训练集，即使标签是随机的。我们提议使用训练样本周围的损失函数曲率作为记忆化程度的度量，对所有训练轮次进行平均。我们利用这个度量来研究常见图像数据集中不同样本的泛化与记忆化特性。我们可视化具有最高损失曲率的样本，发现它们通常是长尾样本、标签错误或冲突样本。这种分析帮助我们在CIFAR100数据集上发现了一种新的失败模型，即具有不同标签的重复图像。我们还通过随机错误化少量样本的标签来人为地给数据集引入标签错误，并展示了按曲率排序可以高效地识别出标签错误样本的高AUROC值。",
    "tldr": "本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。",
    "en_tdlr": "This study explores the generalization and memorization properties of neural networks by analyzing the curvature of the loss function. Samples with high curvature are found to be long-tailed, mislabeled, or conflicting samples, leading to the discovery of a novel failure model on the CIFAR100 dataset. The study also demonstrates the effectiveness of curvature sorting in identifying mislabeled samples by randomly corrupting a proportion of the dataset."
}