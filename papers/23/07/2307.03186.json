{
    "title": "TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])",
    "abstract": "Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\\textit{principled}$ approach, along with an approximate implementation for $\\textit{dynamically}$ and $\\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves ",
    "link": "http://arxiv.org/abs/2307.03186",
    "context": "Title: TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])\nAbstract: Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\\textit{principled}$ approach, along with an approximate implementation for $\\textit{dynamically}$ and $\\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves ",
    "path": "papers/23/07/2307.03186.json",
    "total_tokens": 903,
    "translated_title": "TGRL:一种用于教师引导强化学习的算法",
    "translated_abstract": "学习奖励(即强化学习或RL)和学习模仿教师(即教师-学生学习)是解决顺序决策问题的两种成熟方法。为了结合这些不同形式学习的优点，通常会训练一个策略来最大化强化学习和教师-学生学习目标的组合。然而，如果没有一个有原则的方法来平衡这些目标，之前的工作使用启发式方法和问题特定的超参数搜索来平衡两个目标。我们提出了一种\"有原则\"的方法，并提出了一种近似实现\"动态\"和\"自动\"平衡何时遵循教师和何时使用奖励。主要思想是通过比较代理的性能与没有教师监督并只从奖励中学习的对照情景来调整教师监督的重要性。如果使用教师监督改善了代理的性能，那么教师监督的重要性就会增加。",
    "tldr": "TGRL是一种用于教师引导强化学习的算法，通过动态和自动平衡何时遵循教师指导和何时使用奖励，教师监督的重要性会根据代理的表现调整。"
}