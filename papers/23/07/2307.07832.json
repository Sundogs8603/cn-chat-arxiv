{
    "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets ",
    "link": "http://arxiv.org/abs/2307.07832",
    "context": "Title: MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets ",
    "path": "papers/23/07/2307.07832.json",
    "total_tokens": 815,
    "translated_title": "MixupExplainer:通过数据增强为图神经网络提供通用解释",
    "translated_abstract": "图神经网络（GNNs）因其学习图结构数据的能力而受到越来越多的关注。然而，它们的预测往往不可解释。已经提出了事后实例级解释方法来理解GNN的预测。这些方法旨在发现解释训练过的GNN预测行为的子结构。本文揭示了现有方法中存在的分布偏移问题，在真实数据集的应用中特别影响解释质量，因为这些数据集具有严格的决策边界。为了解决这个问题，我们引入了一个包括独立于标签的图变量的广义图信息瓶颈（GIB）形式，等价于传统的GIB。受广义GIB的驱动，我们提出了一种图mixup方法，MixupExplainer，具有解决分布偏移问题的理论保证。我们在合成和真实世界数据集上进行了大量实验证明",
    "tldr": "本文提出了一种通用的图神经网络解释方法MixupExplainer，通过引入广义图信息瓶颈（GIB）和图mixup方法来解决现有解释方法中存在的分布偏移问题。"
}