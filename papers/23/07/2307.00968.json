{
    "title": "REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)",
    "abstract": "Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\\underline{R}$epresentative $\\underline{E}$rrors for $\\underline{A}$ctive $\\underline{L}$earning. It identifies minority predictions as \\emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of",
    "link": "http://arxiv.org/abs/2307.00968",
    "context": "Title: REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)\nAbstract: Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\\underline{R}$epresentative $\\underline{E}$rrors for $\\underline{A}$ctive $\\underline{L}$earning. It identifies minority predictions as \\emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of",
    "path": "papers/23/07/2307.00968.json",
    "total_tokens": 890,
    "translated_title": "REAL:一种基于代表性错误驱动的主动学习方法",
    "translated_abstract": "在拥有有限标记预算的情况下，主动学习旨在从未标记的样本中选择最具信息量的实例，以获取标签用于模型训练。为了实现这一目标，主动学习通常根据不确定性和多样性来衡量未标记实例的信息量。然而，它并不考虑具有邻域错误密度的错误实例，这些实例在提高模型性能方面具有巨大潜力。为了解决这个局限性，我们提出了一种名为REAL的新方法，该方法选择具有代表性错误的数据实例用于主动学习。它将少数预测作为聚类中的“伪错误”进行识别，并根据估计的错误密度为该聚类分配自适应的采样预算。在五个文本分类数据集上进行的大量实验证明，REAL在准确率和F1-macro得分方面始终优于所有表现最佳的基准算法。",
    "tldr": "本研究提出了一种名为REAL的新方法，该方法通过选择具有代表性错误的数据实例来改进主动学习。通过考虑错误实例及其邻域中的错误密度，REAL在准确率和F1-macro得分方面优于其他算法。"
}