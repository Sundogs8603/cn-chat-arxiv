{
    "title": "DIP-RL: Demonstration-Inferred Preference Learning in Minecraft. (arXiv:2307.12158v1 [cs.LG])",
    "abstract": "In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspi",
    "link": "http://arxiv.org/abs/2307.12158",
    "context": "Title: DIP-RL: Demonstration-Inferred Preference Learning in Minecraft. (arXiv:2307.12158v1 [cs.LG])\nAbstract: In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspi",
    "path": "papers/23/07/2307.12158.json",
    "total_tokens": 887,
    "translated_title": "DIP-RL：在Minecraft中的演示推导偏好学习",
    "translated_abstract": "在机器学习中的顺序决策过程中，算法代理通过接收奖励信号的反馈来与环境进行交互学习。然而，在许多非结构化的现实世界环境中，这样的奖励信号是未知的，并且人类无法可靠地构建一个正确捕捉所需行为的奖励信号。为了在这样的非结构化和开放的环境中完成任务，我们提出了Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)，这是一种利用人类演示的算法，包括训练自编码器，用演示数据种子强化学习 (RL)训练批次，并推导出偏好以学习引导RL的奖励函数。我们在Minecraft中的砍树任务中评估了DIP-RL。结果表明，该方法能够指导RL代理学习一个反映人类偏好的奖励函数，并且相对于基准模型，DIP-RL表现出了竞争力。",
    "tldr": "DIP-RL是一种利用人类演示的算法，在非结构化和开放的环境中通过多种方式推导偏好并学习奖励函数，其在Minecraft中的砍树任务中表现出了竞争力。"
}