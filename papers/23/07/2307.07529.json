{
    "title": "Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])",
    "abstract": "This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.",
    "link": "http://arxiv.org/abs/2307.07529",
    "context": "Title: Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])\nAbstract: This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.",
    "path": "papers/23/07/2307.07529.json",
    "total_tokens": 860,
    "translated_title": "在有向无环图约束下学习多个协调代理",
    "translated_abstract": "本文提出了一种新颖的多智能体强化学习（MARL）方法，用于在有向无环图（DAG）约束下学习多个协调代理。与现有的MARL方法不同的是，我们的方法明确利用了代理之间的DAG结构，以达到更有效的学习性能。在理论上，我们提出了一种基于合成奖励的MARL模型的新型代理值函数，并证明它作为最优值函数的下界。在计算上，我们提出了一种实际的训练算法，利用领导者代理和奖励生成和分发代理的新概念，引导分解的从属代理在具有DAG约束的环境中更好地探索参数空间。在实证上，我们利用四个DAG环境，包括Intel高产量打包和测试工厂的实际调度，对我们的方法进行了基准测试，并证明它优于其他非DAG方法。",
    "tldr": "本文提出了一种在有向无环图约束下学习多个协调代理的新方法，通过利用DAG结构，提高了学习性能，并在实际环境中的多个任务上取得了优于其他非DAG方法的结果。"
}