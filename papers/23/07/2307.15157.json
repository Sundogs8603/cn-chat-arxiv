{
    "title": "R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])",
    "abstract": "Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep f",
    "link": "http://arxiv.org/abs/2307.15157",
    "context": "Title: R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])\nAbstract: Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep f",
    "path": "papers/23/07/2307.15157.json",
    "total_tokens": 922,
    "translated_title": "R-LPIPS: 一种具有对抗鲁棒性的感知相似度度量",
    "translated_abstract": "相似度度量在计算机视觉中扮演着重要角色，用于捕捉图像的基本语义。最近几年，出现了先进的相似度度量方法，例如学习的感知图像块相似度（LPIPS）。这些度量利用来自训练神经网络的深度特征，并在相对图像相似度评估中展现出与人类感知密切一致的能力。然而，现在已经众所周知，神经网络容易受到对抗性示例的影响，即对人类来说不可见的小扰动，被精心设计用来故意误导模型。因此，LPIPS度量方法也对这些对抗性示例敏感。这种敏感性引入了重大的安全问题，特别是考虑到LPIPS在大规模应用中的广泛采用。本文提出了鲁棒的学习感知图像块相似度（R-LPIPS）度量方法，一种利用对抗性训练的深度特征的新度量方法。",
    "tldr": "该论文提出了一种针对对抗性示例具有鲁棒性的新型感知相似度度量方法R-LPIPS，用于解决在计算机视觉中广泛采用的LPIPS度量方法对对抗性示例的敏感性问题。",
    "en_tdlr": "This paper proposes a new perceptual similarity metric, R-LPIPS, which is robust against adversarial examples, addressing the sensitivity issue of the widely used LPIPS metric in computer vision."
}