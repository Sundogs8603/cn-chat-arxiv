{
    "title": "Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])",
    "abstract": "We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear",
    "link": "http://arxiv.org/abs/2307.14993",
    "context": "Title: Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])\nAbstract: We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear",
    "path": "papers/23/07/2307.14993.json",
    "total_tokens": 873,
    "translated_title": "Thinker: 学习规划和行动",
    "translated_abstract": "我们提出了Thinker算法，一种新颖的方法，使强化学习代理能够自主地与学习的世界模型进行交互并利用其。 Thinker算法通过给环境添加世界模型来改变环境，并引入了用于与世界模型交互的新动作。这些模型交互动作使代理能够通过在选择最终的环境动作之前向世界模型提出备选计划来进行规划。该方法通过使代理自主学习如何进行规划来消除了手工设计的规划算法的需求，并且允许对代理的计划进行易于解释的可视化。我们通过Sokoban游戏和Atari 2600基准测试的实验结果证明了该算法的有效性，其中Thinker算法分别实现了最先进的性能和有竞争力的结果。使用Thinker算法训练的代理的可视化结果表明，它们已经学到了优秀的规划策略。",
    "tldr": "Thinker算法通过引入世界模型和模型交互动作使强化学习代理实现自主规划，消除了手工设计规划算法的需求，并且在Sokoban游戏和Atari 2600基准测试中取得了state-of-the-art的性能。",
    "en_tdlr": "The Thinker algorithm enables reinforcement learning agents to autonomously plan by introducing a world model and model-interaction actions, leading to state-of-the-art performance in Sokoban and Atari 2600 benchmarks while eliminating the need for hand-crafted planning algorithms."
}