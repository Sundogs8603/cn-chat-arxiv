{
    "title": "A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])",
    "abstract": "Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\\alpha/2})$ for some $\\alpha \\in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a \"single-time-scale\" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201",
    "link": "http://arxiv.org/abs/2307.15892",
    "context": "Title: A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])\nAbstract: Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\\alpha/2})$ for some $\\alpha \\in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a \"single-time-scale\" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201",
    "path": "papers/23/07/2307.15892.json",
    "total_tokens": 795,
    "translated_title": "仅使用一个步长的新型梯度时序差分算法：通过$L$-$\\lambda$平滑性进行收敛速率分析",
    "translated_abstract": "梯度时序差分（GTD）算法是第一个具有收敛保证的离策略学习线性函数逼近算法，其复杂度为$O(d)$（$d$是特征数量）。本文提出了一种名为Impression GTD的全新单时间尺度GTD算法，用于最小化期望td更新（NEU）目标，并只有一个步长参数。我们证明这种新算法的收敛速度至少与$O(1/t)$一样快。",
    "tldr": "本论文提出了一种新的梯度时序差分算法，只使用一个步长参数，并证明收敛速度至少为$O(1/t)$。"
}