{
    "title": "Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])",
    "abstract": "Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.",
    "link": "http://arxiv.org/abs/2307.13100",
    "context": "Title: Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])\nAbstract: Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.",
    "path": "papers/23/07/2307.13100.json",
    "total_tokens": 970,
    "translated_title": "标签噪声：对修正的修正",
    "translated_abstract": "在具有标签噪声的数据集上训练神经网络分类器会导致过拟合到噪声标签的风险。为了解决这个问题，研究人员探索了更鲁棒的替代损失函数。然而，许多这些替代方案都是启发式的，并且仍然容易过拟合或欠拟合。在这项工作中，我们提出了一种更直接的方法来解决由标签噪声引起的过拟合问题。我们观察到标签噪声的存在意味着噪声广义风险的下界。基于这个观察，我们建议在训练过程中对经验风险施加一个下界来减轻过拟合。我们的主要贡献是提供了理论结果，为不同的损失函数提供了明确的、易于计算的最小可实现噪声风险的界限。我们通过实验证明，在各种设置下使用这些界限显著提高了鲁棒性，几乎没有额外的计算成本。",
    "tldr": "本研究提出了一种对付标签噪声引起的过拟合的直接方法，通过观察标签噪声存在时噪声广义风险的下界，提出了在训练过程中对经验风险施加下界以减轻过拟合的方法，并提供了明确且易于计算的最小可实现噪声风险界限。",
    "en_tdlr": "This study proposes a direct approach to address overfitting caused by label noise by observing the lower bound on the noisy generalised risk, and imposing a lower bound on the empirical risk during training. It provides theoretical results with explicit and easily computable bounds on the minimum achievable noisy risk for different loss functions, demonstrating enhanced robustness in various settings with almost no additional computational cost."
}