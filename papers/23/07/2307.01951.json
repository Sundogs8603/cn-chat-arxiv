{
    "title": "A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the \"Neural Collapse\" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an \"optimis",
    "link": "http://arxiv.org/abs/2307.01951",
    "context": "Title: A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the \"Neural Collapse\" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an \"optimis",
    "path": "papers/23/07/2307.01951.json",
    "total_tokens": 931,
    "translated_title": "图神经网络中特征演化的神经塌陷视角",
    "translated_abstract": "图神经网络（GNNs）在图结构数据的分类任务中越来越受欢迎。然而，GNNs中图拓扑和特征演化之间的相互作用尚不清楚。本文以基于节点的分类为主题，以随机块模型图上的社区检测为例，通过“神经塌陷”现象来探索特征演化。当训练基于实例的深度分类器（例如图像分类）超过零训练误差点时，神经塌陷表现为最深层特征的类内变异性减少，并且类均值与特定的对称结构更加对齐。我们先从实证研究开始，显示类内变异性的减少在基于节点的分类环境中也普遍存在，但不及基于实例的案例那么明显。然后，我们从理论上研究了这种区别。具体而言，我们证明了即使在不考虑激活，图拓扑信息也能导致特征崩溃。",
    "tldr": "本文以节点分类为例，通过“神经塌陷”现象探索图神经网络中特征演化的机制，并发现即使在节点分类情况下，特征的类内变异性也会减少，但不及基于实例的情况那么显著。"
}