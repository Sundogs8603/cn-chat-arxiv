{
    "title": "KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation",
    "abstract": "In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.",
    "link": "https://arxiv.org/abs/2307.01878",
    "context": "Title: KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation\nAbstract: In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.",
    "path": "papers/23/07/2307.01878.json",
    "total_tokens": 799,
    "translated_title": "KDSTM: 使用知识蒸馏的神经半监督主题建模",
    "translated_abstract": "在文本分类任务中，微调预训练的语言模型（如BERT和GPT-3）可以获得竞争性的准确性；然而，这两种方法都需要在大型文本数据集上进行预训练。相比之下，一般的主题建模方法具有在不需要预训练的情况下分析文档并提取有意义的词汇模式的优势。为了利用主题建模在文本分类任务中的无监督的见解提取，我们开发了一种称为知识蒸馏半监督主题建模（KDSTM）的方法。KDSTM不需要预训练嵌入，只需要少量的标记文档，并且训练效率高，在资源受限的情况下非常理想。在多个数据集上，我们的方法在分类准确性、鲁棒性和效率方面都超过了现有的有监督主题建模方法，并且与最先进的弱监督文本分类方法相比达到了类似的性能。",
    "tldr": "KDSTM是一种使用知识蒸馏的神经半监督主题建模方法，对于文本分类任务，在没有预训练嵌入且资源受限的情况下，能够提供高准确性、鲁棒性和效率。"
}