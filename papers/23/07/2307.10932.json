{
    "title": "Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)",
    "abstract": "The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by differ",
    "link": "http://arxiv.org/abs/2307.10932",
    "context": "Title: Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)\nAbstract: The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by differ",
    "path": "papers/23/07/2307.10932.json",
    "total_tokens": 873,
    "translated_title": "同卵和异卵双胞胎：句子表示的细粒度语义对比学习",
    "translated_abstract": "利用对比学习显著提高了无监督学习句子表示的效果。该方法通过将正样本与锚定样本聚类来创建所需的嵌入空间。然而，仅仅依靠对比目标可能会导致次优结果，因为它无法区分正样本对之间的微小语义变化。具体而言，常见的数据增强技术经常引入语义扭曲，导致正样本对之间存在语义间隔。虽然InfoNCE损失函数忽略了语义间隔，并在训练期间优先考虑正样本对之间的相似性最大化，从而导致训练模型的语义理解能力不敏感。本文介绍了一种新颖的同卵和异卵对比学习（称为IFTCL）框架，能够同时适应不同的正样本对生成方式。",
    "tldr": "本文提出了一种名为同卵和异卵对比学习（IFTCL）框架，通过同时适应不同的正样本对生成方式，在无监督学习句子表示中解决了对比学习中存在的语义扭曲和语义间隔问题。",
    "en_tdlr": "This paper introduces a novel Identical and Fraternal Twins of Contrastive Learning (IFTCL) framework, which addresses the issues of semantic distortion and semantic margin in contrastive learning for unsupervised sentence representation by simultaneously adapting to different positive pair generation methods."
}