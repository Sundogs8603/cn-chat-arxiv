{
    "title": "ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)",
    "abstract": "Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.",
    "link": "http://arxiv.org/abs/2307.12450",
    "context": "Title: ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)\nAbstract: Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.",
    "path": "papers/23/07/2307.12450.json",
    "total_tokens": 945,
    "translated_title": "ProtoFL: 通过原型蒸馏实现无监督的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种提高数据隐私保护的有潜力的方法，特别适用于身份验证系统。然而，有限的通信轮次、稀缺的表示和可扩展性给其部署带来了重大挑战，限制了其发挥全部潜力的能力。在本文中，我们提出了\"ProtoFL\"，基于原型表示蒸馏的无监督联邦学习，以增强全局模型的表示能力并降低通信成本。此外，我们引入了基于正规流的本地单类分类器，以在有限数据下改善性能。我们的研究首次探讨了使用FL来提高单类分类性能。我们在广泛使用的五个基准数据集上进行了大量实验，分别是MNIST、CIFAR-10、CIFAR-100、ImageNet-30和Keystroke-Dynamics，以展示我们提出的框架在文献中的先前方法上具有卓越的性能。",
    "tldr": "本文提出了ProtoFL，一种基于原型蒸馏的无监督联邦学习方法，用于提高全局模型的表示能力并减少通信成本。此外，引入了基于正规流的本地单类分类器以提高有限数据下的性能。在五个基准数据集上的实验证明了该方法相较于先前方法具有超越性能。"
}