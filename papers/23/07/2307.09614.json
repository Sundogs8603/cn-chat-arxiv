{
    "title": "Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])",
    "abstract": "Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.",
    "link": "http://arxiv.org/abs/2307.09614",
    "context": "Title: Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])\nAbstract: Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.",
    "path": "papers/23/07/2307.09614.json",
    "total_tokens": 928,
    "translated_title": "多视角自监督学习用于多变量通道时间序列",
    "translated_abstract": "对多变量生物医学时间序列数据进行标注是一项繁重和昂贵的任务。自监督对比学习通过对未标记数据进行预训练来减少对大型标记数据集的需求。然而，对于多变量时间序列数据，输入通道的集合在不同应用之间通常会有所变化，而大多数现有工作并不允许在具有不同输入通道集合的数据集之间进行迁移学习。我们提出了一种学习一种编码器来分别处理所有输入通道的方法。然后，我们使用传递神经网络在通道之间提取单一表示。我们通过在一个具有六个脑电图通道的数据集上进行预训练，并在一个具有两个不同脑电图通道的数据集上进行微调来展示这种方法的潜力。我们比较了具有传递神经网络和不具有传递神经网络的网络在不同对比损失函数下的性能。我们发现我们的方法结合了TS2Vec损失在大多数设置中的表现优于其他所有方法。",
    "tldr": "本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。"
}