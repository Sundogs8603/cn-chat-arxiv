{
    "title": "A User Study on Explainable Online Reinforcement Learning for Adaptive Systems. (arXiv:2307.04098v1 [cs.SE])",
    "abstract": "Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty. Online RL facilitates learning from actual operational data and thereby leverages feedback only available at runtime. However, Online RL requires the definition of an effective and correct reward function, which quantifies the feedback to the RL algorithm and thereby guides learning. With Deep RL gaining interest, the learned knowledge is no longer explicitly represented, but is represented as a neural network. For a human, it becomes practically impossible to relate the parametrization of the neural network to concrete RL decisions. Deep RL thus essentially appears as a black box, which severely limits the debugging of adaptive systems. We previously introduced the explainable RL technique XRL-DINE, which provides visual insights into why certain decisions were made at important time points. Here, we introduce an empirical user study involving 54 software ",
    "link": "http://arxiv.org/abs/2307.04098",
    "context": "Title: A User Study on Explainable Online Reinforcement Learning for Adaptive Systems. (arXiv:2307.04098v1 [cs.SE])\nAbstract: Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty. Online RL facilitates learning from actual operational data and thereby leverages feedback only available at runtime. However, Online RL requires the definition of an effective and correct reward function, which quantifies the feedback to the RL algorithm and thereby guides learning. With Deep RL gaining interest, the learned knowledge is no longer explicitly represented, but is represented as a neural network. For a human, it becomes practically impossible to relate the parametrization of the neural network to concrete RL decisions. Deep RL thus essentially appears as a black box, which severely limits the debugging of adaptive systems. We previously introduced the explainable RL technique XRL-DINE, which provides visual insights into why certain decisions were made at important time points. Here, we introduce an empirical user study involving 54 software ",
    "path": "papers/23/07/2307.04098.json",
    "total_tokens": 884,
    "translated_title": "一个关于可解释的在线强化学习在自适应系统中应用的用户研究",
    "translated_abstract": "在设计时间的不确定性存在的情况下，在线强化学习（RL）越来越多地被用于实现自适应系统。在线RL利用实际运行数据进行学习，并利用在运行时才能得到的反馈。然而，在线RL需要定义一个有效且正确的奖励函数，来量化对RL算法的反馈并指导学习。随着深度RL引起了人们的关注，学到的知识不再以显式的方式表示，而是以神经网络的形式表示。对于人类来说，将神经网络的参数化与具体的RL决策联系起来几乎是不可能的。因此，深度RL在本质上变成了一个黑盒子，严重限制了自适应系统的调试。我们之前介绍了可解释RL技术XRL-DINE，它提供了对关键时间点的决策原因的可视化洞察。在这里，我们介绍了一个包括54个软件用户的实证用户研究",
    "tldr": "这个研究通过用户实证研究，探讨了可解释的在线强化学习在自适应系统中的应用问题。通过提供可视化的洞察，帮助用户理解关键时间点的决策原因。",
    "en_tdlr": "This research investigates the application of explainable online reinforcement learning in adaptive systems through an empirical user study. By providing visual insights, it helps users understand the reasons behind key decisions at important time points."
}