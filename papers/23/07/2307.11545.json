{
    "title": "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation. (arXiv:2307.11545v1 [cs.CV])",
    "abstract": "Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource savings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an investigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation. Our approach achieves comparable or superior performance with only 1.61\\% to 3.38\\% backbone parameter updates, evaluated on challenging benchmarks. The code is available at \\url{https://github.com/kkakkkka/ETRIS}.",
    "link": "http://arxiv.org/abs/2307.11545",
    "context": "Title: Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation. (arXiv:2307.11545v1 [cs.CV])\nAbstract: Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource savings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an investigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation. Our approach achieves comparable or superior performance with only 1.61\\% to 3.38\\% backbone parameter updates, evaluated on challenging benchmarks. The code is available at \\url{https://github.com/kkakkkka/ETRIS}.",
    "path": "papers/23/07/2307.11545.json",
    "total_tokens": 877,
    "translated_title": "桥接视觉和语言编码器：参数高效的引用图像分割调整",
    "translated_abstract": "参数高效调整 (PET) 在减少参数数量的同时保持性能和提供更好的硬件资源节省方面引起了人们的关注，但很少研究密集预测任务和模态之间的交互。本文探讨了引用图像分割上的高效调整问题。我们提出了一种名为 Bridger 的新型适配器，用于促进跨模态信息交换并将任务特定信息注入预训练模型。我们还为图像分割设计了一个轻量级解码器。我们的方法在挑战性基准测试中通过仅进行1.61％ 至 3.38％ 的主干参数更新，实现了可比或更优的性能。代码可在 \\url{https://github.com/kkakkkka/ETRIS} 获取。",
    "tldr": "本研究针对引用图像分割问题进行了参数高效调整的探索。我们提出了一种名为 Bridger 的适配器，在模型中实现了跨模态信息交换和任务特定信息注入，并设计了一个轻量级的图像分割解码器。通过仅进行1.61％ 至 3.38％ 的主干参数更新，我们的方法在挑战性基准测试中实现了可比或更优的性能。",
    "en_tdlr": "This study investigates parameter-efficient tuning for referring image segmentation. The researchers propose an adapter called Bridger, which facilitates cross-modal information exchange and injects task-specific information into the pre-trained model. They also design a lightweight decoder for image segmentation. With only 1.61% to 3.38% backbone parameter updates, their approach achieves comparable or superior performance on challenging benchmarks."
}