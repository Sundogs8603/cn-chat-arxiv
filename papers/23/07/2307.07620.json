{
    "title": "Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])",
    "abstract": "Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.",
    "link": "http://arxiv.org/abs/2307.07620",
    "context": "Title: Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])\nAbstract: Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.",
    "path": "papers/23/07/2307.07620.json",
    "total_tokens": 822,
    "translated_title": "通过跨批次度量学习实现通用嵌入",
    "translated_abstract": "全局平均汇聚（GAP）是深度度量学习（DML）中常用的组件，用于聚合特征。其有效性通常归因于将每个特征向量视为独立的语义实体，并将GAP视为它们的组合。尽管经过证实，但这种解释在学习可用于表示未见过的类别的通用实体的算法意义上仍不清楚，这是DML的关键目标。为了解决这个问题，我们将GAP定义为可学习原型的凸组合。然后，我们展示了原型学习可以被表达为将线性预测器拟合到一批样本的递归过程。基于这个观点，我们在每次迭代中考虑两个不相交类别的批次，并通过使用适应于另一个批次的原型来规范化学习。我们在4个热门DML基准上验证了我们的方法。",
    "tldr": "通过跨批次度量学习，我们提出了一种基于可学习原型的全局平均汇聚方法，用于学习通用实体以表示未见过的类别。"
}