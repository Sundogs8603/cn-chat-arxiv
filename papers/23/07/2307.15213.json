{
    "title": "PCA, SVD, and Centering of Data. (arXiv:2307.15213v1 [stat.ME])",
    "abstract": "The research detailed in this paper scrutinizes Principal Component Analysis (PCA), a seminal method employed in statistics and machine learning for the purpose of reducing data dimensionality. Singular Value Decomposition (SVD) is often employed as the primary means for computing PCA, a process that indispensably includes the step of centering - the subtraction of the mean location from the data set. In our study, we delve into a detailed exploration of the influence of this critical yet often ignored or downplayed data centering step. Our research meticulously investigates the conditions under which two PCA embeddings, one derived from SVD with centering and the other without, can be viewed as aligned. As part of this exploration, we analyze the relationship between the first singular vector and the mean direction, subsequently linking this observation to the congruity between two SVDs of centered and uncentered matrices. Furthermore, we explore the potential implications arising fro",
    "link": "http://arxiv.org/abs/2307.15213",
    "context": "Title: PCA, SVD, and Centering of Data. (arXiv:2307.15213v1 [stat.ME])\nAbstract: The research detailed in this paper scrutinizes Principal Component Analysis (PCA), a seminal method employed in statistics and machine learning for the purpose of reducing data dimensionality. Singular Value Decomposition (SVD) is often employed as the primary means for computing PCA, a process that indispensably includes the step of centering - the subtraction of the mean location from the data set. In our study, we delve into a detailed exploration of the influence of this critical yet often ignored or downplayed data centering step. Our research meticulously investigates the conditions under which two PCA embeddings, one derived from SVD with centering and the other without, can be viewed as aligned. As part of this exploration, we analyze the relationship between the first singular vector and the mean direction, subsequently linking this observation to the congruity between two SVDs of centered and uncentered matrices. Furthermore, we explore the potential implications arising fro",
    "path": "papers/23/07/2307.15213.json",
    "total_tokens": 898,
    "translated_title": "PCA、SVD和数据居中化",
    "translated_abstract": "本文详细研究了主成分分析（PCA），这是统计学和机器学习中常用的降维方法。奇异值分解（SVD）通常被用作计算PCA的主要方法，这个过程中必不可少地包含了数据居中化的步骤，即从数据集中减去均值位置。在我们的研究中，我们深入探讨了这个关键但常常被忽视或轻视的数据居中化步骤的影响。我们的研究精细地研究了在什么条件下，从带有居中化的SVD和不带居中化的SVD得到的两个PCA嵌入可以看作是对齐的。作为这个探索的一部分，我们分析了第一个奇异向量和均值方向之间的关系，随后将这一观察结果与中心化和非中心化矩阵的两个SVD之间的一致性联系起来。此外，我们还探讨了可能产生的相关影响。",
    "tldr": "本研究详细研究了PCA方法中数据居中化步骤的影响，分析了带居中化和不带居中化的两个PCA嵌入之间的对齐性，并探讨了其与奇异向量以及均值方向之间的关系。",
    "en_tdlr": "This study thoroughly investigates the impact of data centering step in PCA, analyzes the alignment between two PCA embeddings with and without centering, and explores the relationship with the first singular vector and mean direction."
}