{
    "title": "A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])",
    "abstract": "When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.",
    "link": "http://arxiv.org/abs/2307.08767",
    "context": "Title: A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])\nAbstract: When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.",
    "path": "papers/23/07/2307.08767.json",
    "total_tokens": 811,
    "translated_title": "改进语言模型在数学问题上的性能的混合策略",
    "translated_abstract": "在解决数学问题时，大多数语言模型采用采样策略根据条件概率预测下一个词。在数学推理过程中，可能会生成错误的答案。考虑到数学问题是确定性的，我们提出了一种混合策略的探索方法来解决数学问题，利用强化学习。我们提出了一个两级标记探索策略：抽象层以概率采样来决定下一个标记是运算符还是操作数，而第二层则以贪婪方式选择得分最高的下一个标记。我们使用GPT-2模型在GSM8K数据集上测试了我们的方法，并展示了超过2％的性能增益。我们的实现代码可在https://github.com/vividitytech/math_lm_rl找到。",
    "tldr": "本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。",
    "en_tdlr": "This paper proposes a mixed policy exploration approach using reinforcement learning to improve the performance of language models on math problems. By incorporating different exploration strategies at the abstract level and the second level, the method achieves a performance gain of over 2%."
}