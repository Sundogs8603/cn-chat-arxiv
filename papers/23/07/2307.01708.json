{
    "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])",
    "abstract": "We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.",
    "link": "http://arxiv.org/abs/2307.01708",
    "context": "Title: Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])\nAbstract: We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.",
    "path": "papers/23/07/2307.01708.json",
    "total_tokens": 798,
    "translated_title": "风险敏感强化学习的分布模型等效性",
    "translated_abstract": "我们考虑学习用于风险敏感强化学习的模型的问题。我们在理论上证明了适当的价值等价性，这是一种学习模型的方法，可以用于在风险中性的情况下进行最优规划，但在风险敏感的情况下无法进行最优规划。我们利用分布式强化学习引入了两种新的模型等价性概念，其中一个是通用的，可以用于针对任何风险度量进行规划，但是计算复杂；另一个是实际的变体，允许选择可以进行最优规划的风险度量。我们展示了如何使用我们的框架来增强任何基于模型的风险敏感算法，并提供了表格和大规模实验来展示其能力。",
    "tldr": "本文研究了风险敏感强化学习中的分布模型等效性问题。我们提出了两种新的模型等价性概念，并展示了如何将这些概念应用于增强任何基于模型的风险敏感算法。",
    "en_tdlr": "This paper investigates the distributional model equivalence problem in risk-sensitive reinforcement learning. We propose two new notions of model equivalence and demonstrate how to apply them to enhance any model-based risk-sensitive algorithm."
}