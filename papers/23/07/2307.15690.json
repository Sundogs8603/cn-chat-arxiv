{
    "title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware. (arXiv:2307.15690v1 [cs.LG])",
    "abstract": "Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for of",
    "link": "http://arxiv.org/abs/2307.15690",
    "context": "Title: Benchmarking Offline Reinforcement Learning on Real-Robot Hardware. (arXiv:2307.15690v1 [cs.LG])\nAbstract: Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for of",
    "path": "papers/23/07/2307.15690.json",
    "total_tokens": 943,
    "translated_title": "在真实机器人硬件上进行离线强化学习的基准测试",
    "translated_abstract": "从预先记录的数据中学习策略是解决真实世界机器人任务的有希望的方向，因为在线学习通常是不可行的。特别是在灵巧操作领域，目前仍然存在着通用形式的难题。然而，离线强化学习与大型多样数据集的结合有潜力在这个具有挑战性的领域取得突破，类似于近年来在监督学习方面取得的快速进展。为了协调研究社区的努力来解决这个问题，我们提出了一个基准测试，包括：i）从模拟环境中通过强化学习训练能力强大的代理智能体在两个任务上获得的用于离线学习的大量数据集；ii）在真实世界机器人系统和模拟环境上执行学习策略以便进行高效的调试。我们评估了主流的开源离线强化学习算法，并提供了可重复实验的设置。",
    "tldr": "从预先记录的数据中学习策略在解决真实世界机器人任务中具有潜力，该论文提出了一个基准测试，包括大量来自模拟环境的数据集以及在真实机器人系统和模拟环境中执行学习策略的选项，用于推动离线强化学习的研究。"
}