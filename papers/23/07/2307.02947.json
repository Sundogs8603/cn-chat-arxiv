{
    "title": "A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])",
    "abstract": "Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computa",
    "link": "http://arxiv.org/abs/2307.02947",
    "context": "Title: A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])\nAbstract: Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computa",
    "path": "papers/23/07/2307.02947.json",
    "total_tokens": 896,
    "translated_title": "一种从实值观测中进行强化学习的神经形态架构",
    "translated_abstract": "强化学习（RL）为复杂环境下的决策提供了一个强大的框架。然而，以高效且生物启发的方式实现RL仍然是一个挑战。本文提出了一种新颖的脉冲神经网络（SNN）架构，用于解决具有实值观测的RL问题。所提出的模型结合了多层事件驱动聚类，增加了时间差分（TD）误差调制和资格痕迹, 并基于之前的工作进行改进。消融研究证实了这些组成部分对所提出的模型性能的重要影响。在经典RL环境中，我们的网络不断优于表格方法，并成功发现了稳定的控制策略：山车、倒立摆和摆臂。所提出的模型在计算效率上具有吸引力的折中方案。",
    "tldr": "本文提出了一种新颖的神经网络架构，用于从实值观测中进行强化学习。该模型采用了多层事件驱动聚类、时间差分误差调制和资格痕迹等方法，并在经典RL环境中取得了优于表格方法的性能表现。",
    "en_tdlr": "This paper proposes a novel neural network architecture for reinforcement learning from real-valued observations. The model incorporates multi-layered event-based clustering, TD-error modulation, and eligibility traces, and outperforms the tabular approach in classic RL environments."
}