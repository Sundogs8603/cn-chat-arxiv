{
    "title": "Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])",
    "abstract": "Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was c",
    "link": "http://arxiv.org/abs/2307.01288",
    "context": "Title: Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])\nAbstract: Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was c",
    "path": "papers/23/07/2307.01288.json",
    "total_tokens": 848,
    "translated_title": "用共识方式解决可解释机器学习中的分歧问题",
    "translated_abstract": "机器学习模型的价值通常通过其预测的准确性来评估。然而，在某些科学领域中，模型的内部工作方式与其准确性同等重要。为了理解机器学习模型的内部工作原理，解释性算法是首选。然而，尽管有多种算法可供选择，它们在解释模型方面经常存在分歧，导致相互矛盾的解释结果。为了应对这个问题，在模型被解释之后可以应用共识函数。然而，问题并没有完全解决，因为最终结果将取决于选择的共识函数和其他因素。本文评估了六种共识函数用于解释五个机器学习模型。这些模型先前在四个已知内部规则的合成数据集上进行了训练。然后，使用与模型无关的局部和全局可解释性算法对模型进行了解释。最后，进行共识处理。",
    "tldr": "这项研究评估了六种共识函数用于解释五个机器学习模型，并发现了在解释模型方面存在着分歧问题，对于解决这个问题尚需进一步研究。"
}