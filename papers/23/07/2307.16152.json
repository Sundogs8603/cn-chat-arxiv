{
    "title": "Variance Control for Distributional Reinforcement Learning. (arXiv:2307.16152v1 [cs.LG])",
    "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \\emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.",
    "link": "http://arxiv.org/abs/2307.16152",
    "context": "Title: Variance Control for Distributional Reinforcement Learning. (arXiv:2307.16152v1 [cs.LG])\nAbstract: Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \\emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.",
    "path": "papers/23/07/2307.16152.json",
    "total_tokens": 827,
    "translated_title": "分布式强化学习的方差控制",
    "translated_abstract": "尽管在过去的几年中，分布式强化学习（DRL）得到了广泛的研究，但很少有研究探讨了在分布式环境下获得的Q函数估计器的有效性。为了充分了解Q函数的近似误差如何影响整个训练过程，我们进行了一些误差分析，并在理论上展示了如何同时减小误差项的偏差和方差。在这种新的理解下，我们构建了一个新的估计器“Quantiled Expansion Mean”（QEM），并从统计学的角度引入了一个新的DRL算法（QEMRL）。我们在各种Atari和Mujoco基准任务上广泛评估了我们的QEMRL算法，并证明QEMRL在样本效率和收敛性能方面明显优于基线算法。",
    "tldr": "本研究针对分布式强化学习中Q函数估计器的有效性进行了分析，提出了一种新的估计器QEM和DRL算法QEMRL，在Atari和Mujoco基准任务上取得了显著的样本效率和收敛性能的提升。"
}