{
    "title": "Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])",
    "abstract": "In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision tran",
    "link": "http://arxiv.org/abs/2307.13962",
    "context": "Title: Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])\nAbstract: In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision tran",
    "path": "papers/23/07/2307.13962.json",
    "total_tokens": 925,
    "translated_title": "通过隐藏层的线性可分性理解深度神经网络",
    "translated_abstract": "本文通过测量隐藏层输出的线性可分性来研究深度神经网络的特性。具体来说，我们首先提出了基于闵可夫斯基差异的线性可分度量（MD-LSMs）来评估两个点集的线性可分程度。然后，我们证明了隐藏层输出的线性可分性程度与网络训练性能之间存在一种同步性，即如果更新的权重能够提高隐藏层输出的线性可分性程度，那么更新后的网络将实现更好的训练性能，反之亦然。此外，我们还研究了激活函数和网络尺寸（包括宽度和深度）对隐藏层线性可分性的影响。最后，我们进行了数值实验证实了我们的发现对一些流行的深度网络，包括多层感知机（MLP）、卷积神经网络（CNN）、深度置信网络（DBN）、ResNet、VGGNet、AlexNet、vision tran的验证。",
    "tldr": "本文通过测量深度神经网络隐藏层输出的线性可分性来研究其特性，并发现隐藏层输出的线性可分性程度与网络训练性能有同步性，进一步探讨激活函数和网络尺寸对隐藏层线性可分性的影响。"
}