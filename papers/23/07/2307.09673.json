{
    "title": "What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])",
    "abstract": "We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl",
    "link": "http://arxiv.org/abs/2307.09673",
    "context": "Title: What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])\nAbstract: We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl",
    "path": "papers/23/07/2307.09673.json",
    "total_tokens": 797,
    "translated_title": "什么是可解释模型：一项范围审查",
    "translated_abstract": "我们经常在描述基于人工智能（AI）的应用的论文标题中看到可解释这个术语。然而，可解释人工智能（XAI）的文献表明，XAI中的解释是特定应用和领域的，因此在用于解释特定应用问题的模型时需要进行评估。此外，文献揭示了事后方法，特别是特征归因方法的性能存在很大差异，暗示它们并不能成为AI可解释性的解决方案。因此，在使用XAI方法时，应在特定应用中评估其信息输出的质量和适用性。基于这些原因，我们使用了范围审查方法来研究应用AI模型和采用事后解释方法的论文，同时将这些模型称为可解释。",
    "tldr": "这项研究通过范围审查方法调查了应用人工智能模型并采用事后解释方法的论文，探讨了可解释模型这一术语的含义。",
    "en_tdlr": "This study investigates the meaning of explainable models by conducting a scoping review of papers that apply AI models and use post-hoc explanation methods, aiming to understand the term \"explainable\" in the context of AI."
}