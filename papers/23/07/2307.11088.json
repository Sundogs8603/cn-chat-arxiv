{
    "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v2 [cs.CL] UPDATED)",
    "abstract": "Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also ad",
    "link": "http://arxiv.org/abs/2307.11088",
    "context": "Title: L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v2 [cs.CL] UPDATED)\nAbstract: Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also ad",
    "path": "papers/23/07/2307.11088.json",
    "total_tokens": 928,
    "translated_title": "L-Eval：为长文本语言模型引入标准化评估",
    "translated_abstract": "最近，对于扩展指令跟随模型的上下文长度，以便有效处理单回合的长输入（例如，论文总结）和具有更复杂历史的对话，引起了越来越多的关注。虽然像GPT-4和Claude这样的专有模型在处理极长输入方面取得了显著进步，但开放源代码模型仍处于尝试阶段。目前还不清楚扩展上下文是否能够比传统方法（如检索）提供实质性的收益，以及它在实际下游任务中对常规模型的改进程度。为了解决这一挑战，我们提出了为长上下文语言模型引入标准化评估的方案。具体而言，我们开发了L-Eval，其中包含411个长文档和2000多个人工标注的查询-回复对，涵盖法律、金融、学校讲座、长对话、新闻、长篇小说和会议等领域。",
    "tldr": "该论文提出了L-Eval，旨在为长文本语言模型引入标准化评估。通过开发一个包含411个长文档和2000多个人工标注的查询-回复对的数据集，研究探讨了扩展上下文对于处理长输入的实质性收益和改进程度。",
    "en_tdlr": "This paper introduces L-Eval, which aims to institute standardized evaluation for long context language models. By developing a dataset with 411 long documents and over 2000 human-labeled query-response pairs, the study explores the substantial gains and improvements that extending context can offer for handling long inputs."
}