{
    "title": "Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)",
    "abstract": "Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.",
    "link": "http://arxiv.org/abs/2307.04550",
    "context": "Title: Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)\nAbstract: Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.",
    "path": "papers/23/07/2307.04550.json",
    "total_tokens": 748,
    "translated_title": "一次性学习梯度手术在生成模型中的应用",
    "translated_abstract": "最近对“遗忘权”的监管引发了对取消预训练机器学习模型的兴趣。近期的机器学习取消方法通过更新权重来消除样本对权重参数的影响，以逼近一种直接但昂贵的重新训练方法。本文介绍了一种简单且有效的方法，用于在深度生成模型中消除数据的影响。受多任务学习的启发，我们提出了一种使用梯度操作来调整样本之间影响的方法，通过将梯度投影到保留梯度的法平面上进行规范化。我们的方法不依赖于移除样本的统计数据，优于现有基准，并首次在取消生成模型方面提供了理论分析。",
    "tldr": "本文介绍了一种针对生成模型的一次性学习梯度手术方法，通过操纵梯度来消除数据对模型的影响，并在理论上进行了分析。"
}