{
    "title": "ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])",
    "abstract": "Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral image from a low-resolution observation. However, the prevailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interaction information between spectral features. This results in inadequate utilization of spectral information and artifacts after upsampling. To address this issue, we propose ESSAformer, an ESSA attention-embedded Transformer network for single-HSI-SR with an iterative refining structure. Specifically, we first introduce a robust and spectral-friendly similarity metric, \\ie, the spectral correlation coefficient of the spectrum (SCC), to replace the original attention matrix and incorporates inductive biases into the model to facilitate training. Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce atte",
    "link": "http://arxiv.org/abs/2307.14010",
    "context": "Title: ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])\nAbstract: Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral image from a low-resolution observation. However, the prevailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interaction information between spectral features. This results in inadequate utilization of spectral information and artifacts after upsampling. To address this issue, we propose ESSAformer, an ESSA attention-embedded Transformer network for single-HSI-SR with an iterative refining structure. Specifically, we first introduce a robust and spectral-friendly similarity metric, \\ie, the spectral correlation coefficient of the spectrum (SCC), to replace the original attention matrix and incorporates inductive biases into the model to facilitate training. Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce atte",
    "path": "papers/23/07/2307.14010.json",
    "total_tokens": 916,
    "translated_title": "ESSAformer: 高效超光谱图像超分辨率变换器",
    "translated_abstract": "单一超光谱图像超分辨率（Single-HSI-SR）旨在从低分辨率观测中恢复出高分辨率超光谱图像。然而，目前的基于卷积神经网络的方法在构建长程依赖性和捕捉光谱特征之间的交互信息方面存在局限性。这导致光谱信息的利用不足并且在放大后产生伪影。为了解决这个问题，我们提出了ESSAformer，一种嵌入ESSA注意力的变换器网络，用于单一超光谱图像超分辨率，并具有迭代优化结构。具体而言，我们首先引入了一个稳健且与光谱友好的相似度度量，即谱相关系数（SCC），以取代原始的注意力矩阵，并将归纳偏置引入模型以促进训练。在此基础上，我们进一步利用具有理论支持的可核化注意力技术形成一种新的高效SCC核自注意力（ESSA），并减少注意力计算的复杂度。",
    "tldr": "ESSAformer是一种用于单一超光谱图像超分辨率的高效Transformer网络，通过引入稳健的相似度度量和核化注意力技术，解决了CNN-based方法在光谱信息利用和伪影问题上的限制。",
    "en_tdlr": "ESSAformer is an efficient Transformer network for single hyperspectral image super-resolution, addressing the limitations of CNN-based approaches in utilizing spectral information and reducing artifacts by introducing robust similarity metric and kernelized attention technique."
}