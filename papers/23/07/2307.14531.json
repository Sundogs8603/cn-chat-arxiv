{
    "title": "Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])",
    "abstract": "Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.",
    "link": "http://arxiv.org/abs/2307.14531",
    "context": "Title: Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])\nAbstract: Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.",
    "path": "papers/23/07/2307.14531.json",
    "total_tokens": 812,
    "translated_title": "通过修改核谱来控制宽神经网络的归纳偏差",
    "translated_abstract": "宽神经网络在学习特定函数方面存在偏差，影响梯度下降的收敛速度和有限训练时间内可达到的函数。因此，迫切需要一种可以根据任务修改这种偏差的方法。为此，我们引入了Modified Spectrum Kernels(MSKs)这一新颖的构造核族，可以用于近似没有已知闭合形式的期望特征值的核。我们利用宽神经网络和神经切向核之间的对偶性，提出了一种预条件梯度下降方法，改变了梯度下降的轨迹。结果是，这使得训练速度在某些情况下呈多项式甚至指数级加速，同时不改变最终解。我们的方法既计算高效又易于实现。",
    "tldr": "本文引入了Modified Spectrum Kernels（MSKs）构造核族，通过预条件梯度下降方法，实现了对宽神经网络归纳偏差的控制，并在不改变最终解的情况下，加速了训练速度。",
    "en_tdlr": "This paper introduces Modified Spectrum Kernels (MSKs) to modify the inductive bias of wide neural networks, achieving training speedup without changing the final solution."
}