{
    "title": "Advancing Visual Grounding with Scene Knowledge: Benchmark and Method. (arXiv:2307.11558v1 [cs.CV])",
    "abstract": "Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reasoning abilities over their joint space. However, most existing VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study~\\cite{luo2022goes}, where a simple LSTM-based text encoder without pretraining can achieve state-of-the-art performance on mainstream VG datasets. Therefore, in this paper, we propose a novel benchmark of \\underline{S}cene \\underline{K}nowledge-guided \\underline{V}isual \\underline{G}rounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept t",
    "link": "http://arxiv.org/abs/2307.11558",
    "context": "Title: Advancing Visual Grounding with Scene Knowledge: Benchmark and Method. (arXiv:2307.11558v1 [cs.CV])\nAbstract: Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reasoning abilities over their joint space. However, most existing VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study~\\cite{luo2022goes}, where a simple LSTM-based text encoder without pretraining can achieve state-of-the-art performance on mainstream VG datasets. Therefore, in this paper, we propose a novel benchmark of \\underline{S}cene \\underline{K}nowledge-guided \\underline{V}isual \\underline{G}rounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept t",
    "path": "papers/23/07/2307.11558.json",
    "total_tokens": 938,
    "translated_title": "通过场景知识推进视觉对齐：基准与方法",
    "translated_abstract": "视觉对齐旨在建立视觉和语言之间的细粒度对齐。在理想情况下，它可以成为对视觉和语言模型进行评估的基准，以评估它们对图像和文本的理解以及它们在联合空间上的推理能力。然而，大多数现有的视觉对齐数据集是使用简单的描述文本构建的，这些文本不需要对图像和文本进行足够的推理。在最近的一项研究中~\\cite{luo2022goes}，已经证明了一个简单的基于LSTM的文本编码器在主流视觉对齐数据集上可以取得最先进的性能，而无需预训练。因此，在本文中，我们提出了一种新的\\textbf{S}cene \\textbf{K}nowledge-guided \\textbf{V}isual \\textbf{G}rounding (SK-VG)的基准，其中图像内容和引用表达不足以对齐目标对象，迫使模型对长篇场景知识进行推理能力。为了完成这个任务，我们提出了两种方法来接受t",
    "tldr": "本文提出了一种基于场景知识的视觉对齐新基准SK-VG，通过迫使模型具备对长篇场景知识进行推理的能力，对图像和文本进行细粒度对齐。",
    "en_tdlr": "This paper proposes a novel benchmark called SK-VG for visual grounding, which relies on scene knowledge to establish fine-grained alignment between images and texts by forcing models to reason over long-form scene knowledge."
}