{
    "title": "Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain. (arXiv:2307.11769v1 [cs.CL])",
    "abstract": "Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by \"chatting\" with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our find",
    "link": "http://arxiv.org/abs/2307.11769",
    "context": "Title: Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain. (arXiv:2307.11769v1 [cs.CL])\nAbstract: Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by \"chatting\" with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our find",
    "path": "papers/23/07/2307.11769.json",
    "total_tokens": 945,
    "translated_title": "大型语言模型中的领域知识蒸馏：自动驾驶领域的实证研究",
    "translated_abstract": "工程知识化（或专家）系统需要大量手动工作和领域知识。由于大型语言模型（LLM）使用大量跨领域知识进行训练，因此可以自动化此类工程流程。本文提出了一种基于提示工程和LLM ChatGPT的领域知识蒸馏的实证自动化和半自动化框架，并在自动驾驶领域进行了实证评估并呈现了关键观察结果。在我们的实现中，我们通过与ChatGPT“聊天”来构建领域知识本体论。关键发现是，虽然完全自动化的领域本体构建是可能的，但人类监督和早期干预通常可以提高效率和输出质量，因为它们减少了响应随机性和蝴蝶效应的影响。因此，我们还开发了一个基于网络的蒸馏助手，以在运行时进行监督和灵活干预。",
    "tldr": "本文通过使用大型语言模型（LLM）自动化和半自动化的方法，在自动驾驶领域进行了领域知识蒸馏的实证研究。他们发现，尽管完全自动化的领域本体构建是可行的，但人类监督和早期干预通常可以提高效率和输出质量。",
    "en_tdlr": "This paper presents an empirical study on domain knowledge distillation in the autonomous driving domain using large language models (LLMs). The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality."
}