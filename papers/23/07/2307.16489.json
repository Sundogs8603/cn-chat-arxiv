{
    "title": "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models. (arXiv:2307.16489v2 [cs.CV] UPDATED)",
    "abstract": "The rise in popularity of text-to-image generative artificial intelligence (AI) has attracted widespread public interest. We demonstrate that this technology can be attacked to generate content that subtly manipulates its users. We propose a Backdoor Attack on text-to-image Generative Models (BAGM), which upon triggering, infuses the generated images with manipulative details that are naturally blended in the content. Our attack is the first to target three popular text-to-image generative models across three stages of the generative process by modifying the behaviour of the embedded tokenizer, the language model or the image generative model. Based on the penetration level, BAGM takes the form of a suite of attacks that are referred to as surface, shallow and deep attacks in this article. Given the existing gap within this domain, we also contribute a comprehensive set of quantitative metrics designed specifically for assessing the effectiveness of backdoor attacks on text-to-image mo",
    "link": "http://arxiv.org/abs/2307.16489",
    "context": "Title: BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models. (arXiv:2307.16489v2 [cs.CV] UPDATED)\nAbstract: The rise in popularity of text-to-image generative artificial intelligence (AI) has attracted widespread public interest. We demonstrate that this technology can be attacked to generate content that subtly manipulates its users. We propose a Backdoor Attack on text-to-image Generative Models (BAGM), which upon triggering, infuses the generated images with manipulative details that are naturally blended in the content. Our attack is the first to target three popular text-to-image generative models across three stages of the generative process by modifying the behaviour of the embedded tokenizer, the language model or the image generative model. Based on the penetration level, BAGM takes the form of a suite of attacks that are referred to as surface, shallow and deep attacks in this article. Given the existing gap within this domain, we also contribute a comprehensive set of quantitative metrics designed specifically for assessing the effectiveness of backdoor attacks on text-to-image mo",
    "path": "papers/23/07/2307.16489.json",
    "total_tokens": 914,
    "translated_title": "BAGM：一种用于操纵文本到图像生成模型的后门攻击",
    "translated_abstract": "文本到图像生成人工智能（AI）的普及引起了广泛的公众关注。我们证明了这项技术可以被攻击，以生成悄无声息地操纵用户的内容。我们提出了一种针对文本到图像生成模型的后门攻击（BAGM），一旦触发，会向生成的图像中注入自然融入内容的操纵细节。我们的攻击是第一个针对三个流行的文本到图像生成模型在生成过程的三个阶段进行攻击的方法，通过修改嵌入标记器、语言模型或图像生成模型的行为。基于渗透级别，BAGM采用了表面、浅层和深层攻击的一系列攻击形式。鉴于该领域的现有差距，我们还提供了一套全面的定量评估指标，专门用于评估后门攻击对文本到图像模型的影响效果。",
    "tldr": "BAGM是一种后门攻击方法，针对文本到图像生成模型，可以悄无声息地操纵用户生成的图像。该攻击是首个针对三个流行模型不同生成阶段的攻击方法，并提供了一套全面的评估指标。",
    "en_tdlr": "BAGM is a backdoor attack method that manipulates text-to-image generative models to generate subtly manipulative images without users' awareness. It is the first attack that targets three popular models across different stages of the generative process and provides a comprehensive set of evaluation metrics."
}