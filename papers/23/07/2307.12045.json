{
    "title": "Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.12045v1 [cs.CV])",
    "abstract": "The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligni",
    "link": "http://arxiv.org/abs/2307.12045",
    "context": "Title: Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.12045v1 [cs.CV])\nAbstract: The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligni",
    "path": "papers/23/07/2307.12045.json",
    "total_tokens": 992,
    "translated_title": "重新审视用于机器人手术视觉问题定位回答的蒸馏连续学习方法",
    "translated_abstract": "视觉问题定位回答（VQLA）系统可以作为手术教育中的知识助手。除了提供基于文本的答案外，VQLA系统还可以高亮感兴趣的区域，以提高手术场景的理解能力。然而，深度神经网络（DNNs）在学习新知识时容易发生灾难性遗忘。具体来说，当DNNs在增量类别或任务上学习时，其对旧任务的性能会大幅下降。此外，由于医疗数据隐私和许可问题，更新连续学习（CL）模型时往往难以访问旧数据。因此，我们开发了一个非示例连续手术VQLA框架，以在顺序学习范式中探索和平衡DNNs的刚性和可塑性之间的权衡。我们重新审视了CL任务中的蒸馏损失，并提出了刚性-可塑性感知蒸馏（RP-Dist）和自校准异质蒸馏（SH-Dist）来保留旧知识。",
    "tldr": "本文研究了用于机器人手术中视觉问题定位回答的蒸馏连续学习方法。通过重新审视蒸馏损失，提出了刚性-可塑性感知蒸馏和自校准异质蒸馏来保留旧知识。",
    "en_tdlr": "This paper explores the use of distillation for continual learning in visual question localized-answering in robotic surgery. By revisiting the distillation loss and proposing rigidity-plasticity-aware distillation and self-calibrated heterogeneous distillation, old knowledge can be preserved."
}