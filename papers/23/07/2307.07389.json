{
    "title": "Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])",
    "abstract": "The sparsity of Deep Neural Networks is well investigated to maximize the performance and reduce the size of overparameterized networks as possible. Existing methods focus on pruning parameters in the training process by using thresholds and metrics. Meanwhile, feature similarity between different layers has not been discussed sufficiently before, which could be rigorously proved to be highly correlated to the network sparsity in this paper. Inspired by interlayer feature similarity in overparameterized models, we investigate the intrinsic link between network sparsity and interlayer feature similarity. Specifically, we prove that reducing interlayer feature similarity based on Centered Kernel Alignment (CKA) improves the sparsity of the network by using information bottleneck theory. Applying such theory, we propose a plug-and-play CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR, which utilizes CKA to reduce feature similarity between layers and increase n",
    "link": "http://arxiv.org/abs/2307.07389",
    "context": "Title: Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])\nAbstract: The sparsity of Deep Neural Networks is well investigated to maximize the performance and reduce the size of overparameterized networks as possible. Existing methods focus on pruning parameters in the training process by using thresholds and metrics. Meanwhile, feature similarity between different layers has not been discussed sufficiently before, which could be rigorously proved to be highly correlated to the network sparsity in this paper. Inspired by interlayer feature similarity in overparameterized models, we investigate the intrinsic link between network sparsity and interlayer feature similarity. Specifically, we prove that reducing interlayer feature similarity based on Centered Kernel Alignment (CKA) improves the sparsity of the network by using information bottleneck theory. Applying such theory, we propose a plug-and-play CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR, which utilizes CKA to reduce feature similarity between layers and increase n",
    "path": "papers/23/07/2307.07389.json",
    "total_tokens": 891,
    "translated_title": "学习带有恒等层的稀疏神经网络",
    "translated_abstract": "深度神经网络的稀疏性已经得到了广泛的研究，目的是最大化性能并尽可能减少过参数化网络的大小。现有方法主要集中在使用阈值和指标通过剪枝参数来进行训练过程中的稀疏化。而本文提出了一种新的角度，即不同层之间的特征相似性，该相似性在证明之后被证明与网络稀疏性高度相关。受到超参数化模型中层间特征相似性的启发，我们研究了网络稀疏性与层间特征相似性之间的内在联系。具体而言，我们证明基于中心核对齐（CKA）来减少层间特征相似性可以改善网络的稀疏性，利用信息瓶颈理论来支持这一观点。基于这个理论，我们提出了一种基于CKA的稀疏性正则化方法，称为CKA-SR，它利用CKA来减少层间的特征相似性，从而增加网络的稀疏性。",
    "tldr": "该论文提出了一种基于中心核对齐的稀疏性正则化方法，通过减少不同层之间的特征相似性，可以提高神经网络的稀疏性。",
    "en_tdlr": "This paper proposes a sparsity regularization method based on Centered Kernel Alignment (CKA), which improves the sparsity of neural networks by reducing feature similarity between different layers."
}