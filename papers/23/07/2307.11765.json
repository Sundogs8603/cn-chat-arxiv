{
    "title": "Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model. (arXiv:2307.11765v1 [cs.HC])",
    "abstract": "This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the ",
    "link": "http://arxiv.org/abs/2307.11765",
    "context": "Title: Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model. (arXiv:2307.11765v1 [cs.HC])\nAbstract: This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the ",
    "path": "papers/23/07/2307.11765.json",
    "total_tokens": 965,
    "translated_title": "使用调取心智模型的方式来测量对可解释人工智能辅助决策的感知信任度",
    "translated_abstract": "该实证研究提出了一种新的方法来测量用户对可解释人工智能模型的感知信任度。利用模糊认知图来获取用户的心智模型。首先，我们采用一个可解释的机器学习模型将疑似COVID-19患者分类为阳性或阴性。然后，医疗专家根据他们的专业知识以及可解释人工智能模型提供的预测和解释进行诊断决策任务。为了评估解释对感知信任度的影响，医疗专家通过调查对解释满意度进行评分。然后，将其作为模糊认知图的概念，以确定它们对彼此以及最终对感知信任度的影响。此外，为了考虑医疗专家的主观性，使用模糊语言变量来确定影响的强度。在模糊认知图达到稳定状态后，将获得一个量化值来衡量感知信任度。",
    "tldr": "该研究提出了一种通过调取用户的心智模型来测量可解释人工智能模型的感知信任度的方法。通过使用模糊认知图来评估解释对感知信任度的影响，从而为解释人工智能决策提供参考和改进方向。",
    "en_tdlr": "This study proposes a methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model by eliciting their mental models. It utilizes Fuzzy Cognitive Maps (FCMs) to evaluate the impact of explanations on perceived trust and provides insights for improving XAI decision-making."
}