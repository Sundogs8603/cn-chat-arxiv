{
    "title": "Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])",
    "abstract": "With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \\textbf{S}ynt\\textbf{H}es\\textbf{I}zed \\textbf{P}rompts~(\\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and sy",
    "link": "http://arxiv.org/abs/2307.07397",
    "context": "Title: Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])\nAbstract: With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \\textbf{S}ynt\\textbf{H}es\\textbf{I}zed \\textbf{P}rompts~(\\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and sy",
    "path": "papers/23/07/2307.07397.json",
    "total_tokens": 906,
    "translated_title": "通过生成的提示改进CLIP的零样本泛化能力",
    "translated_abstract": "随着人们对CLIP等预训练视觉语言模型的兴趣日益增长，最近的研究重点在于将这些模型适应到下游任务上。尽管取得了有希望的结果，但大多数现有方法都需要所有类别的标记数据，而在真实世界的应用中可能不具备这种条件，因为存在长尾分布和Zipf定律。例如，某些类别可能完全缺乏标记数据，如新兴概念。为了解决这个问题，我们提出了一种名为SYNTHESIZED PROMPTS (SHIP) 的即插即用的生成方法，以改进现有的微调方法。具体而言，我们采用变分自动编码器引入一个生成器，通过将合成的提示和相应的类别名称输入到CLIP的文本编码器中来重构视觉特征。通过这种方式，我们可以轻松获取剩余仅有标签的类别的生成特征。然后，我们采用现成的方法对CLIP进行微调，将标记和合成的特征进行组合。",
    "tldr": "我们提出了一种名为SHIP的生成方法，通过使用合成的提示和类别名称来改进CLIP的微调方法，从而解决了长尾分布和缺乏标记数据的问题。",
    "en_tdlr": "We propose a generative approach called SHIP to improve CLIP's fine-tuning method by using synthesized prompts and class names, addressing the issues of long-tail distribution and lack of labeled data."
}