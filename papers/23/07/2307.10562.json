{
    "title": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])",
    "abstract": "Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or diff",
    "link": "http://arxiv.org/abs/2307.10562",
    "context": "Title: Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])\nAbstract: Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or diff",
    "path": "papers/23/07/2307.10562.json",
    "total_tokens": 881,
    "translated_title": "共享对抗性遗忘：通过遗忘共享对抗样本来减轻后门攻击",
    "translated_abstract": "后门攻击是对机器学习模型的严重安全威胁，敌对方可以向训练集中注入有毒样本，导致一个预测受特定触发器激活的有毒样本到特定目标类的有门模型，而在无害样本上表现正常。本文通过建立后门风险和对抗风险之间的联系，推导出一种新的后门风险上界，主要捕捉了后门模型与纯净模型之间共享的对抗样本的风险。这个上界进一步提出了一种利用对抗训练技术减轻后门攻击的新的双层优化问题。为了解决这个问题，我们提出了共享对抗性遗忘（SAU）。具体来说，SAU首先生成共享对抗样本，然后通过遗忘这些生成的共享对抗样本，使其能够被纯净模型正确分类和/或区分。",
    "tldr": "本文提出了一种使用共享对抗样本来纯化有门模型的方法，并解决了一个新的双层优化问题，从而减轻后门攻击。",
    "en_tdlr": "This paper proposes a method to purify backdoored models using shared adversarial examples and solves a novel bi-level optimization problem to mitigate backdoor attacks."
}