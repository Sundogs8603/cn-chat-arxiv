{
    "title": "Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])",
    "abstract": "This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\\mathtt{ReLU}$, $\\mathtt{LeakyReLU}$, $\\mathtt{ReLU}^2$, $\\mathtt{ELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, $\\mathtt{Mish}$, $\\mathtt{Sigmoid}$, $\\mathtt{Tanh}$, $\\mathtt{Arctan}$, $\\mathtt{Softsign}$, $\\mathtt{dSiLU}$, and $\\mathtt{SRS}$. We demonstrate that for any activation function $\\varrho\\in \\mathscr{A}$, a $\\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.",
    "link": "http://arxiv.org/abs/2307.06555",
    "context": "Title: Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])\nAbstract: This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\\mathtt{ReLU}$, $\\mathtt{LeakyReLU}$, $\\mathtt{ReLU}^2$, $\\mathtt{ELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, $\\mathtt{Mish}$, $\\mathtt{Sigmoid}$, $\\mathtt{Tanh}$, $\\mathtt{Arctan}$, $\\mathtt{Softsign}$, $\\mathtt{dSiLU}$, and $\\mathtt{SRS}$. We demonstrate that for any activation function $\\varrho\\in \\mathscr{A}$, a $\\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.",
    "path": "papers/23/07/2307.06555.json",
    "total_tokens": 977,
    "translated_title": "深度网络逼近：从ReLU到多种激活函数",
    "translated_abstract": "本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。",
    "tldr": "本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。"
}