{
    "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech. (arXiv:2307.00024v1 [eess.AS])",
    "abstract": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
    "link": "http://arxiv.org/abs/2307.00024",
    "context": "Title: EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech. (arXiv:2307.00024v1 [eess.AS])\nAbstract: State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
    "path": "papers/23/07/2307.00024.json",
    "total_tokens": 957,
    "translated_title": "EmoSpeech: 将FastSpeech2引导至情感文本到语音的研究",
    "translated_abstract": "当前最先进的语音合成模型致力于实现与人类声音尽可能接近的效果。因此，情感建模是文本到语音（TTS）研究中的重要一环。在我们的工作中，我们选择了FastSpeech2作为起点，并提出了一系列修改来合成情感语音。根据自动和人工评估，我们的模型EmoSpeech在生成的语音中无论是MOS得分还是情感识别准确度都超过了现有模型。我们为FastSpeech2架构中每个扩展提供了详细的消融研究，形成了EmoSpeech。文本中情感的不均衡分布对于更好的合成语音和音调感知至关重要。我们的模型包含了一种条件机制，通过允许情感以不同的强度水平对每个音素进行贡献，有效处理了这个问题。人工评估表明，我们提出的修改生成了具有更高MOS和情感表达的音频。",
    "tldr": "本研究提出了一种名为EmoSpeech的模型，通过对FastSpeech2进行一系列修改，实现了情感语音的合成。根据评估结果，EmoSpeech在生成的语音中具有更高的MOS得分和情感识别准确度。模型引入了条件机制，可以有效处理文本中情感不均衡分布的问题，生成具有更高MOS和情感表达的音频。"
}