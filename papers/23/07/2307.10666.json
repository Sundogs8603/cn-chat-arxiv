{
    "title": "A Dataset and Strong Baselines for Classification of Czech News Texts. (arXiv:2307.10666v1 [cs.CL])",
    "abstract": "Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source. As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models. We define four classification tasks: news source, news category, inferred author's gender, and day of the week. To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models. Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.",
    "link": "http://arxiv.org/abs/2307.10666",
    "context": "Title: A Dataset and Strong Baselines for Classification of Czech News Texts. (arXiv:2307.10666v1 [cs.CL])\nAbstract: Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source. As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models. We define four classification tasks: news source, news category, inferred author's gender, and day of the week. To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models. Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.",
    "path": "papers/23/07/2307.10666.json",
    "total_tokens": 975,
    "translated_title": "《捷克新闻文本分类的数据集和强基线》",
    "translated_abstract": "捷克自然语言处理的预训练模型通常在纯语言任务（词性标注、句法分析、命名实体识别）和相对简单的分类任务（情感分类、来自单一新闻源的文章分类）上进行评估。作为替代，我们介绍了CZEch~NEws~Classification~dataset（CZE-NEC），这是一个最大的捷克分类数据集，包含来自各种来源、跨越二十多年的新闻文章，可以更严格地评估这种模型。我们定义了四个分类任务：新闻源、新闻类别、推断作者的性别和星期几。为了验证任务的难度，我们进行了人工评估，结果显示，人类的表现落后于基于预训练变换器模型构建的强大机器学习基线。此外，我们还展示了语言特定的预训练编码器分析优于选定的商业可用的大规模生成语言模型。",
    "tldr": "该论文介绍了捷克新闻文本分类的数据集和基线模型。通过引入CZEch~NEws~Classification~dataset（CZE-NEC），这个跨越20多年的最大捷克新闻分类数据集，可以更严格地评估捷克自然语言处理的预训练模型。实验证明，基于预训练变换器模型的机器学习基线表现优于人类，并且语言特定的预训练编码器分析也超过了商业模型。"
}