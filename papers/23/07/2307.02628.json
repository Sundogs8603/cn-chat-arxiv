{
    "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. (arXiv:2307.02628v1 [cs.CL])",
    "abstract": "Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, they incur high computation cost and latency resulting from the autoregressive token-by-token generation. To address this issue, several approaches have been proposed to reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference, they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prio",
    "link": "http://arxiv.org/abs/2307.02628",
    "context": "Title: SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. (arXiv:2307.02628v1 [cs.CL])\nAbstract: Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, they incur high computation cost and latency resulting from the autoregressive token-by-token generation. To address this issue, several approaches have been proposed to reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference, they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prio",
    "path": "papers/23/07/2307.02628.json",
    "total_tokens": 869,
    "translated_title": "SkipDecode：用于高效LLM推理的自回归跳跃解码方法（arXiv:2307.02628v1 [cs.CL]）",
    "translated_abstract": "自回归的大型语言模型（LLM）在各种自然语言生成任务中取得了显著的进展。然而，由于逐个生成标记的自回归方式，它们产生了高计算成本和延迟。为了解决这个问题，已经提出了几种方法来使用早期退出策略来降低计算成本。这些策略能够在不对每个标记应用完整计算图的情况下加快文本生成速度。虽然现有的标记级早期退出方法在在线推理中显示出了有希望的结果，但它们不能轻易应用于批量推理和键值缓存。这是因为它们必须等到批量中的最后一个标记退出后才能停止计算。这严重限制了这种技术的实际应用。在本文中，我们提出了一种简单而有效的标记级早期退出方法SkipDecode，它能够与批量推理和KV缓存无缝配合。它克服了先前的限制，并提出了一种实用的解决方案。",
    "tldr": "本文提出了一种名为SkipDecode的简单而有效的标记级早期退出方法，能够与批量推理和KV缓存无缝配合，解决了传统方法在这些方面的限制。"
}