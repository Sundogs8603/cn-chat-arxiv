{
    "title": "A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])",
    "abstract": "Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.  We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing a",
    "link": "http://arxiv.org/abs/2307.15008",
    "context": "Title: A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])\nAbstract: Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.  We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing a",
    "path": "papers/23/07/2307.15008.json",
    "total_tokens": 985,
    "translated_title": "通过LLM辅助，对AI-Guardian的攻击研究",
    "translated_abstract": "如今，大型语言模型（LLMs）在各种任务上都能够表现出很高的能力。本文研究了一种名为GPT-4的LLM是否能够辅助进行对抗性机器学习研究。以AI-Guardian为案例，我们评估了这个最近在IEEE S&P 2023上发表的针对对抗样本的防御机制的鲁棒性。我们完全破解了这个防御机制：与未防御的基线相比，所提出的方案并没有增加鲁棒性。我们并没有编写攻击该模型的代码，而是指示和引导GPT-4按照我们的指令实施所有攻击算法。这个过程出奇地有效和高效，有时候语言模型在不明确的指令下产生的代码比本文作者还要快。最后，我们讨论了（1）评估中出现的警示信号表明AI-Guardian将被攻破，以及（2）我们在设计和实施攻击方案时的经验。",
    "tldr": "本文研究了LLM是否能辅助进行对抗性机器学习研究，以AI-Guardian为案例评估了其鲁棒性。研究发现，我们成功破解了AI-Guardian的防御机制，并且通过指示和引导GPT-4实施攻击算法的方法非常有效和高效。",
    "en_tdlr": "This paper investigates whether LLMs can assist in adversarial machine learning research, using AI-Guardian as a case study to evaluate its robustness. The study reveals that the defense mechanism of AI-Guardian was completely broken, and the method of instructing and guiding GPT-4 to implement attack algorithms was highly effective and efficient."
}