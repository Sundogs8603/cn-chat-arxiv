{
    "title": "Explaining Competitive-Level Programming Solutions using LLMs. (arXiv:2307.05337v1 [cs.CL])",
    "abstract": "In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to \\textit{<problem, solution>} pairs. We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining solutions. Our explanation generation methodology can generate a structured solution explanation for the problem containing descriptions and analysis. To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively. The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key",
    "link": "http://arxiv.org/abs/2307.05337",
    "context": "Title: Explaining Competitive-Level Programming Solutions using LLMs. (arXiv:2307.05337v1 [cs.CL])\nAbstract: In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to \\textit{<problem, solution>} pairs. We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining solutions. Our explanation generation methodology can generate a structured solution explanation for the problem containing descriptions and analysis. To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively. The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key",
    "path": "papers/23/07/2307.05337.json",
    "total_tokens": 878,
    "translated_title": "通过LLMs解释竞技级别的编程解决方案",
    "translated_abstract": "在本文中，我们将竞技级别的编程问题解决视为推理和代码生成的组合任务。我们提出了一种新颖的方法，可以自动标注自然语言解释和<问题，解决方案>对。我们展示了尽管在解决竞技级别的编程问题方面表现较差，最先进的LLMs在描述和解释解决方案方面表现出很强的能力。我们的解释生成方法可以为包含描述和分析的问题生成结构化的解决方案解释。为了评估注释解释的质量，我们从两个方面进行了评估：1）满足编写oracle解决方案的人类编程专家的要求，2）帮助LLMs更有效地解决问题。在CodeContests数据集上的实验结果表明，尽管LLM GPT3.5和GPT-4在描述解决方案方面的能力相当，但GPT-4在关键细节的理解上更好。",
    "tldr": "本文研究了竞技级别的编程问题解决，提出了一种通过LLMs自动标注解释的方法，并展示了LLMs在描述和解释解决方案方面的强大能力。实验证明GPT-4在关键细节的理解上更好。",
    "en_tdlr": "This paper explores competitive-level programming problem-solving and proposes an automated annotation method using LLMs for generating explanations. It shows that LLMs have strong abilities in describing and explaining solutions. Experimental results demonstrate that GPT-4 has a better understanding of key details."
}