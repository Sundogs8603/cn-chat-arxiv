{
    "title": "The semantic landscape paradigm for neural networks. (arXiv:2307.09550v1 [cs.LG])",
    "abstract": "Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically",
    "link": "http://arxiv.org/abs/2307.09550",
    "context": "Title: The semantic landscape paradigm for neural networks. (arXiv:2307.09550v1 [cs.LG])\nAbstract: Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically",
    "path": "papers/23/07/2307.09550.json",
    "total_tokens": 843,
    "translated_title": "神经网络的语义景观范式",
    "translated_abstract": "深度神经网络展示了一系列令人着迷的现象，从可预测的缩放定律到训练时间、数据集大小和网络大小的不可预测的新能力的出现。对这些现象的分析揭示了这些网络的学习表示中编码的概念和算法的存在。虽然在解释这些观察到的现象方面取得了重要进展，但对于理解、解剖和预测神经网络性能的统一框架尚缺乏。在这里，我们引入了语义景观范式，这是一个概念性和数学框架，描述了神经网络的训练动力学，其中的路径被视为整个网络中学习表示内在的新算法。这种抽象使我们能够以统计物理学中研究过的问题来描述各种神经网络现象。",
    "tldr": "本研究引入了语义景观范式，用于描述神经网络的训练动力学，将其视为在图上的路径，图的节点对应于网络学习表示中的新算法。这种抽象使我们能够以统计物理学中研究过的问题来解释各种神经网络现象。"
}