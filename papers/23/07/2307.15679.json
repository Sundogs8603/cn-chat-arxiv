{
    "title": "Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks. (arXiv:2307.15679v1 [cs.LG])",
    "abstract": "In recurrent neural networks, learning long-term dependency is the main difficulty due to the vanishing and exploding gradient problem. Many researchers are dedicated to solving this issue and they proposed many algorithms. Although these algorithms have achieved great success, understanding how the information decays remains an open problem. In this paper, we study the dynamics of the hidden state in recurrent neural networks. We propose a new perspective to analyze the hidden state space based on an eigen decomposition of the weight matrix. We start the analysis by linear state space model and explain the function of preserving information in activation functions. We provide an explanation for long-term dependency based on the eigen analysis. We also point out the different behavior of eigenvalues for regression tasks and classification tasks. From the observations on well-trained recurrent neural networks, we proposed a new initialization method for recurrent neural networks, which ",
    "link": "http://arxiv.org/abs/2307.15679",
    "context": "Title: Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks. (arXiv:2307.15679v1 [cs.LG])\nAbstract: In recurrent neural networks, learning long-term dependency is the main difficulty due to the vanishing and exploding gradient problem. Many researchers are dedicated to solving this issue and they proposed many algorithms. Although these algorithms have achieved great success, understanding how the information decays remains an open problem. In this paper, we study the dynamics of the hidden state in recurrent neural networks. We propose a new perspective to analyze the hidden state space based on an eigen decomposition of the weight matrix. We start the analysis by linear state space model and explain the function of preserving information in activation functions. We provide an explanation for long-term dependency based on the eigen analysis. We also point out the different behavior of eigenvalues for regression tasks and classification tasks. From the observations on well-trained recurrent neural networks, we proposed a new initialization method for recurrent neural networks, which ",
    "path": "papers/23/07/2307.15679.json",
    "total_tokens": 875,
    "translated_title": "递归神经网络的动态分析和特征值初始化器",
    "translated_abstract": "在递归神经网络中，由于梯度消失和梯度爆炸问题，学习长期依赖性是主要难点。许多研究人员致力于解决这个问题，并提出了许多算法。尽管这些算法取得了很大的成功，但对信息衰减的理解仍然是一个开放的问题。本文研究了递归神经网络中的隐藏状态动态。我们提出了一种基于权重矩阵的特征值分解的隐藏状态空间分析新视角。我们从线性状态空间模型开始分析，并解释了激活函数中信息保留的功能。我们基于特征值分析提供了长期依赖性的解释。我们还指出了回归任务和分类任务特征值行为的差异。通过对训练良好的递归神经网络的观察，我们提出了一种递归神经网络的新初始化方法。",
    "tldr": "本文研究了递归神经网络中的隐藏状态动态，并提出了一种新的角度来分析隐藏状态空间。基于特征值分析，我们提供了长期依赖性的解释，并在此基础上提出了递归神经网络的新初始化方法。"
}