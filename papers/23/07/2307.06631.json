{
    "title": "Frameless Graph Knowledge Distillation. (arXiv:2307.06631v1 [cs.LG])",
    "abstract": "Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy. Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models. However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model. In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating t",
    "link": "http://arxiv.org/abs/2307.06631",
    "context": "Title: Frameless Graph Knowledge Distillation. (arXiv:2307.06631v1 [cs.LG])\nAbstract: Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy. Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models. However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model. In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating t",
    "path": "papers/23/07/2307.06631.json",
    "total_tokens": 898,
    "translated_title": "无框图知识蒸馏",
    "translated_abstract": "知识蒸馏（Knowledge Distillation，KD）已显示出在将复杂的教师模型的知识传输到简单的学生模型中具有巨大潜力，从而可以高效地完成繁重的学习任务而不失去太多的预测准确性。最近，许多研究尝试将KD机制应用于图表示学习模型，如图神经网络（Graph Neural Networks，GNNs），以通过学生模型加速模型的推断速度。然而，许多现有的基于KD的GNNs在学生模型中利用MLP作为通用逼近器来模仿教师模型的流程，而不考虑教师模型的图知识。在这项工作中，我们提供了一个基于多尺度GNNs的KD框架，即图框架（graph framelet），并证明通过充分利用图框架分解提供的多尺度图知识，学生模型能够适应同质和异质图，并具有缓解的潜力。",
    "tldr": "这里是针对图知识蒸馏的无框架KD框架，通过充分利用图框架分解提供的多尺度图知识，学生模型能够适应不同类型的图，并具有缓解的潜力。",
    "en_tdlr": "This is a frameless Knowledge Distillation (KD) framework for graph representation learning, which utilizes graph framelet decomposition to effectively transfer multi-scale graph knowledge and enable the student model to adapt to different types of graphs with potential for alleviating the learning burden."
}