{
    "title": "Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])",
    "abstract": "This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.",
    "link": "http://arxiv.org/abs/2307.14373",
    "context": "Title: Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])\nAbstract: This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.",
    "path": "papers/23/07/2307.14373.json",
    "total_tokens": 735,
    "translated_title": "可由宽度无限、成本有限的浅层ReLU神经网络表示的分段线性函数",
    "translated_abstract": "本文分析了使用修正线性单元（ReLU）作为激活函数的宽度无限、成本有限的浅层神经网络对连续的分段线性函数的表示。通过其积分表示，可以通过相应的有限符号度量在适当参数空间上识别出浅层神经网络。我们将这些度量映射到参数空间上的度量，并将参数空间中的点双射到函数定义域中的超平面。我们证明了Ongie等人的猜想，即每个可以由这种宽度无限神经网络表示的连续分段线性函数也可以由有限宽度的浅层ReLU神经网络表示。",
    "tldr": "本文研究了使用无限宽度、有限成本的浅层ReLU神经网络以及修正线性单元作为激活函数来表示连续分段线性函数。通过将度量从参数空间映射到函数定义域中的超平面，证明了这种神经网络可以表示任意分段线性函数。"
}