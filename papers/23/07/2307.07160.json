{
    "title": "Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])",
    "abstract": "We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).",
    "link": "http://arxiv.org/abs/2307.07160",
    "context": "Title: Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])\nAbstract: We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).",
    "path": "papers/23/07/2307.07160.json",
    "total_tokens": 908,
    "translated_title": "不要随机遮盖：通过遮盖领域内关键词进行有效的领域自适应预训练",
    "translated_abstract": "我们提出了一种新颖的领域无关的领域内预训练方法，介于通用预训练和微调之间。我们的方法选择性地遮盖领域内的关键词，即提供目标领域的紧凑表示的单词。我们使用KeyBERT (Grootendorst, 2020)来识别这些关键词。我们使用六种不同的设置对我们的方法进行评估：三个数据集与两个不同的预训练语言模型（PLMs）相结合。我们的结果表明，使用我们的领域内预训练策略微调的PLMs优于使用随机遮盖的领域内预训练的PLMs，并且优于遵循常见的预训练然后微调范式的PLMs。此外，识别领域内关键词的开销是合理的，例如，对于BERT Large (Devlin et al., 2019)来说，是预训练时间的7-15%（两个epoch）。",
    "tldr": "本研究提出了一种通过遮盖领域内关键词进行的领域自适应预训练方法，实验结果表明该方法优于使用随机遮盖的领域内预训练和常见的预训练然后微调范式。",
    "en_tdlr": "This study proposes a domain-adaptive pre-training method by selectively masking in-domain keywords, which outperforms random masking and the common pre-train-then-fine-tune paradigm."
}