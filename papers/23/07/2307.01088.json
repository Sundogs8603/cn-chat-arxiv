{
    "title": "Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG])",
    "abstract": "Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.",
    "link": "http://arxiv.org/abs/2307.01088",
    "context": "Title: Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG])\nAbstract: Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.",
    "path": "papers/23/07/2307.01088.json",
    "total_tokens": 862,
    "translated_title": "在分布转移和长尾数据下，对现代视觉架构进行合规预测的经验证实",
    "translated_abstract": "合规预测已经成为一种可靠地为深度学习模型提供不确定性估计和安全保证的方法。然而，它的性能已知在分布转移和长尾类别分布下会下降，而这在现实世界的应用中经常存在。在本文中，我们对这些情况下的几种事后和基于训练的合规预测方法进行了性能表征，并首次在大规模数据集和模型上进行了实证评估。我们发现在许多合规方法和神经网络家族中，性能在分布转移下违反安全保证时大大下降。同样，在长尾设置中，我们发现许多类别的保证经常被违反。了解这些方法的局限性对于在现实世界和安全关键应用中部署是必要的。",
    "tldr": "本文在大规模数据集和模型上首次对分布转移和长尾类别分布下的合规预测方法进行了实证评估。研究发现，这些方法在分布转移和长尾设置下的性能大大下降，对于在现实世界和安全关键应用中的部署具有重要的局限性。",
    "en_tdlr": "This paper provides an empirical evaluation of conformal prediction methods under distribution shift and long-tailed class distributions on large-scale datasets and models. The study demonstrates that the performance of these methods greatly degrades under distribution shifts and long-tailed settings, highlighting their limitations in real-world and safety-critical applications."
}