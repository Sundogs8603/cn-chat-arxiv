{
    "title": "BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v1 [cs.CL])",
    "abstract": "Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhan",
    "link": "http://arxiv.org/abs/2307.03131",
    "context": "Title: BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v1 [cs.CL])\nAbstract: Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhan",
    "path": "papers/23/07/2307.03131.json",
    "total_tokens": 986,
    "translated_title": "BLEURT具有通用翻译能力：基于最小风险训练的自动度量分析",
    "translated_abstract": "自动度量在机器翻译中起着关键作用。尽管n-gram度量广泛应用，但最近出现了基于预训练模型的度量的发展潮流，重点在于测量句子语义。然而，这些神经度量虽然与人工评估相关性更高，但常常被认为是带有潜在偏见且难以检测的黑盒子。本研究从训练机器翻译系统的指导角度，系统分析和比较了多种主流和前沿的自动度量。通过最小风险训练（MRT），我们发现某些度量存在鲁棒性缺陷，例如BLEURT和BARTScore中存在通用对抗翻译现象。深入分析表明，这些鲁棒性缺陷主要有两个原因：训练数据集中的分布偏差和度量范式的倾向。通过引入标记级约束，我们增强了度量的鲁棒性。",
    "tldr": "本研究通过最小风险训练系统性地分析和比较了各种自动度量，并发现了BLEURT和BARTScore等度量中存在的通用对抗翻译现象。研究结果表明，这些鲁棒性缺陷主要由训练数据集中的分布偏差和度量范式的倾向引起。通过引入标记级约束，可以提高度量的鲁棒性。",
    "en_tdlr": "This study systematically analyzes and compares various automatic metrics and identifies the presence of universal adversarial translations in BLEURT and BARTScore. The research highlights the causes of these robustness deficits as distribution biases in the training datasets and the tendency of the metric paradigm. By incorporating token-level constraints, the robustness of the metrics can be enhanced."
}