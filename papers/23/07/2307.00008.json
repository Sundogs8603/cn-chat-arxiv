{
    "title": "Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])",
    "abstract": "The current era of natural language processing (NLP) has been defined by the prominence of pre-trained language models since the advent of BERT. A feature of BERT and models with similar architecture is the objective of masked language modeling, in which part of the input is intentionally masked and the model is trained to predict this piece of masked information. Data augmentation is a data-driven technique widely used in machine learning, including research areas like computer vision and natural language processing, to improve model performance by artificially augmenting the training data set by designated techniques. Masked language models (MLM), an essential training feature of BERT, have introduced a novel approach to perform effective pre-training on Transformer based models in natural language processing tasks. Recent studies have utilized masked language model to generate artificially augmented data for NLP downstream tasks. The experimental results show that Mask based data au",
    "link": "http://arxiv.org/abs/2307.00008",
    "context": "Title: Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])\nAbstract: The current era of natural language processing (NLP) has been defined by the prominence of pre-trained language models since the advent of BERT. A feature of BERT and models with similar architecture is the objective of masked language modeling, in which part of the input is intentionally masked and the model is trained to predict this piece of masked information. Data augmentation is a data-driven technique widely used in machine learning, including research areas like computer vision and natural language processing, to improve model performance by artificially augmenting the training data set by designated techniques. Masked language models (MLM), an essential training feature of BERT, have introduced a novel approach to perform effective pre-training on Transformer based models in natural language processing tasks. Recent studies have utilized masked language model to generate artificially augmented data for NLP downstream tasks. The experimental results show that Mask based data au",
    "path": "papers/23/07/2307.00008.json",
    "total_tokens": 811,
    "translated_title": "探索基于掩码的数据生成在语言模型中的应用",
    "translated_abstract": "当BERT问世以来，当前自然语言处理（NLP）的时代已经被预训练语言模型的重要性所定义。BERT和类似结构的模型的一个特点是掩码语言建模的目标，其中部分输入被有意地掩盖，模型被训练以预测这部分被掩码的信息。数据增强是一种数据驱动的技术，广泛应用于机器学习，包括计算机视觉和自然语言处理等研究领域，通过指定的技术人工增加训练数据集以改善模型性能。掩码语言模型（MLM）是BERT的一个重要训练特点，它为基于Transformer的模型在自然语言处理任务中提供了有效的预训练方法。最近的研究利用掩码语言模型生成人工增强数据用于NLP下游任务。实验结果表明，基于掩码的数据增强可以提高模型性能。",
    "tldr": "本研究探索了基于掩码的数据生成在语言模型中的应用，结果表明该方法可以提高模型性能。"
}