{
    "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])",
    "abstract": "As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.",
    "link": "http://arxiv.org/abs/2307.10472",
    "context": "Title: Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])\nAbstract: As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.",
    "path": "papers/23/07/2307.10472.json",
    "total_tokens": 813,
    "translated_title": "通过提示，指令微调的语言模型能否识别社会偏见？",
    "translated_abstract": "随着语言模型应用的广度和深度不断扩展，构建有效的框架来衡量和减轻这些模型学习或继承的社会偏见变得越来越重要。本文提出了一种评估指令微调语言模型通过零样本提示（包括思维链提示）识别偏见能力的方法。在LLaMA及其两个指令微调版本中，Alpaca 7B在偏见识别任务中表现最好，准确率达56.7%。我们还展示了扩大语言模型大小和数据多样性可以进一步提高性能。这是我们偏见缓解框架的第一部分，正在进行的工作。我们将根据获得的更多结果不断更新本文。",
    "tldr": "本文介绍了一种通过零样本提示评估指令微调语言模型识别偏见能力的方法，展示了Alpaca 7B在偏见识别任务中的最佳性能，并提出扩大模型大小和数据多样性可进一步提高性能。",
    "en_tdlr": "This paper presents a method for evaluating the bias identification ability of instruction fine-tuned language models through zero-shot prompting. It demonstrates the superior performance of Alpaca 7B on the bias identification task and suggests that scaling up model size and data diversity can further improve performance."
}