{
    "title": "Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains. (arXiv:2307.13012v1 [cs.SD])",
    "abstract": "Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be",
    "link": "http://arxiv.org/abs/2307.13012",
    "context": "Title: Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains. (arXiv:2307.13012v1 [cs.SD])\nAbstract: Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be",
    "path": "papers/23/07/2307.13012.json",
    "total_tokens": 984,
    "translated_title": "联合语音和重叠检测：多个音频设置和语音领域的基准",
    "translated_abstract": "语音活动检测和重叠讲话检测是说话者分割的关键预处理任务。最终的分段性能非常依赖于这些子任务的鲁棒性。最近的研究表明，可以使用多类别分类模型来联合训练语音活动检测和重叠讲话检测。然而，这些研究通常局限于特定的语音领域，缺乏系统的泛化能力信息。本文提出了一个全面而新颖的不同语音活动检测和重叠讲话检测模型的基准，涵盖了多个音频设置（单声道/多声道）和语音领域（例如媒体、会议等）。我们的2/3类系统将时域卷积神经网络与适应音频设置的语音表征相结合，优于最先进的结果。我们展示了这两个任务的联合训练在F1得分方面与两个专用的语音活动检测和重叠讲话检测系统具有类似的性能，同时降低了训练成本。这种独特的架构也可以",
    "tldr": "本文提出了一个全新的基准，对不同的语音活动检测和重叠讲话检测模型进行了评估，涉及多个音频设置和语音领域。我们的系统在F1得分方面表现出色，并且通过联合训练这两个任务，可以降低训练成本。"
}