{
    "title": "To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?. (arXiv:2307.06708v1 [cs.CL])",
    "abstract": "Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the $\\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would you share your instant messages for $\\varepsilon$ of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\\varepsilon$ thresholds would lead l",
    "link": "http://arxiv.org/abs/2307.06708",
    "context": "Title: To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?. (arXiv:2307.06708v1 [cs.CL])\nAbstract: Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the $\\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would you share your instant messages for $\\varepsilon$ of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\\varepsilon$ thresholds would lead l",
    "path": "papers/23/07/2307.06708.json",
    "total_tokens": 920,
    "translated_title": "是否分享？给予差分隐私的自然语言处理系统敏感数据的普通人接受什么风险？",
    "translated_abstract": "尽管NLP社区已经采用中心差分隐私作为保护隐私的模型训练或数据共享的首选框架，但决定性的关键参数——控制隐私保护强度的隐私预算ε的选择和解释仍然相当随意。我们认为确定ε值不应该仅由研究人员或系统开发者决定，还必须考虑那些共享他们潜在敏感数据的人。换句话说：你愿意为ε值为10而分享你的即时消息吗？我们通过设计、实施和进行行为实验(311名普通参与者)来填补这一研究空白，研究人们在不确定决策环境下面对威胁隐私的情境时的行为。通过将风险感知框架化为两个现实的NLP场景，并使用情节行为研究，我们能够确定哪些ε阈值将导致共享行为的转变。",
    "tldr": "这项研究旨在探索普通人在面临隐私威胁情境时的决策行为，以及他们愿意为给予差分隐私的自然语言处理系统提供敏感数据所承担的风险。",
    "en_tdlr": "This study aims to explore the decision-making behavior of laypeople in the face of privacy-threatening situations and the risks they are willing to accept to provide sensitive data to differentially-private NLP systems."
}