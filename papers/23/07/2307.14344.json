{
    "title": "Strictly Low Rank Constraint Optimization -- An Asymptotically $\\mathcal{O}(\\frac{1}{t^2})$ Method. (arXiv:2307.14344v1 [math.OC])",
    "abstract": "We study a class of non-convex and non-smooth problems with \\textit{rank} regularization to promote sparsity in optimal solution. We propose to apply the proximal gradient descent method to solve the problem and accelerate the process with a novel support set projection operation on the singular values of the intermediate update. We show that our algorithms are able to achieve a convergence rate of $O(\\frac{1}{t^2})$, which is exactly same as Nesterov's optimal convergence rate for first-order methods on smooth and convex problems. Strict sparsity can be expected and the support set of singular values during each update is monotonically shrinking, which to our best knowledge, is novel in momentum-based algorithms.",
    "link": "http://arxiv.org/abs/2307.14344",
    "context": "Title: Strictly Low Rank Constraint Optimization -- An Asymptotically $\\mathcal{O}(\\frac{1}{t^2})$ Method. (arXiv:2307.14344v1 [math.OC])\nAbstract: We study a class of non-convex and non-smooth problems with \\textit{rank} regularization to promote sparsity in optimal solution. We propose to apply the proximal gradient descent method to solve the problem and accelerate the process with a novel support set projection operation on the singular values of the intermediate update. We show that our algorithms are able to achieve a convergence rate of $O(\\frac{1}{t^2})$, which is exactly same as Nesterov's optimal convergence rate for first-order methods on smooth and convex problems. Strict sparsity can be expected and the support set of singular values during each update is monotonically shrinking, which to our best knowledge, is novel in momentum-based algorithms.",
    "path": "papers/23/07/2307.14344.json",
    "total_tokens": 916,
    "translated_title": "严格低秩约束优化--一种渐进为$\\mathcal{O}(\\frac{1}{t^2})$的方法",
    "translated_abstract": "我们研究了一类具有秩正则化的非凸和非光滑问题，以促进最优解的稀疏性。我们提出应用近端梯度下降方法来解决这个问题，并通过对中间更新的奇异值进行新颖的支持集投影操作来加速过程。我们证明了我们的算法能够实现$O(\\frac{1}{t^2})$的收敛速度，这与Nesterov在平滑凸问题上的一阶方法的最优收敛速度完全相同。可以期望出现严格的稀疏性，并且在每次更新中奇异值的支持集是单调缩小的，这在基于动量的算法中是新颖的，据我们所知。",
    "tldr": "该论文介绍了一种严格低秩约束优化方法，通过秩正则化促进最优解的稀疏性。作者提出了一种近端梯度下降方法，并结合奇异值的支持集投影操作加速了优化过程。他们证明了算法收敛速度为$\\mathcal{O}(\\frac{1}{t^2})$，并且支持集在每次更新时单调缩小，这在基于动量的算法中是新颖的。",
    "en_tdlr": "This paper introduces a strictly low rank constraint optimization method that promotes sparsity in the optimal solution through rank regularization. The authors propose a proximal gradient descent method and accelerate the process with a novel support set projection operation. They prove that the algorithm achieves a convergence rate of $\\mathcal{O}(\\frac{1}{t^2})$, and the support set monotonically shrinks during each update, which is novel in momentum-based algorithms."
}