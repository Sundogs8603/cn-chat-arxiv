{
    "title": "Debiased Pairwise Learning from Positive-Unlabeled Implicit Feedback. (arXiv:2307.15973v1 [cs.IR])",
    "abstract": "Learning contrastive representations from pairwise comparisons has achieved remarkable success in various fields, such as natural language processing, computer vision, and information retrieval. Collaborative filtering algorithms based on pairwise learning also rooted in this paradigm. A significant concern is the absence of labels for negative instances in implicit feedback data, which often results in the random selected negative instances contains false negatives and inevitably, biased embeddings. To address this issue, we introduce a novel correction method for sampling bias that yields a modified loss for pairwise learning called debiased pairwise loss (DPL). The key idea underlying DPL is to correct the biased probability estimates that result from false negatives, thereby correcting the gradients to approximate those of fully supervised data. The implementation of DPL only requires a small modification of the codes. Experimental studies on five public datasets validate the effec",
    "link": "http://arxiv.org/abs/2307.15973",
    "context": "Title: Debiased Pairwise Learning from Positive-Unlabeled Implicit Feedback. (arXiv:2307.15973v1 [cs.IR])\nAbstract: Learning contrastive representations from pairwise comparisons has achieved remarkable success in various fields, such as natural language processing, computer vision, and information retrieval. Collaborative filtering algorithms based on pairwise learning also rooted in this paradigm. A significant concern is the absence of labels for negative instances in implicit feedback data, which often results in the random selected negative instances contains false negatives and inevitably, biased embeddings. To address this issue, we introduce a novel correction method for sampling bias that yields a modified loss for pairwise learning called debiased pairwise loss (DPL). The key idea underlying DPL is to correct the biased probability estimates that result from false negatives, thereby correcting the gradients to approximate those of fully supervised data. The implementation of DPL only requires a small modification of the codes. Experimental studies on five public datasets validate the effec",
    "path": "papers/23/07/2307.15973.json",
    "total_tokens": 860,
    "translated_title": "从正负隐式反馈中去偏移的成对学习",
    "translated_abstract": "从成对比较中学习对比表示在自然语言处理、计算机视觉和信息检索等领域取得了显著的成功。基于成对学习的协同过滤算法也源于这种范式。一个重要的问题是隐式反馈数据中缺乏负实例的标签，这经常导致随机选择的负实例包含伪负例，必然产生偏向性嵌入。为了解决这个问题，我们引入了一种新颖的纠正方法，用于解决偏移引起的样本采样偏差，从而得到了一个修改后的成对学习损失，称为去偏移成对损失（DPL）。DPL的关键思想是纠正由于伪负例导致的偏向性概率估计，从而纠正梯度以逼近全监督数据的梯度。DPL的实现只需要对代码进行小的修改。在五个公开数据集上的实验证明了DPL的有效性。",
    "tldr": "本论文提出了一种新的校正方法，用于解决隐式反馈数据中缺乏负实例的标签的问题，在成对学习中引入了去偏移成对损失（DPL）来纠正由于伪负例导致的偏向性概率估计，从而纠正梯度以逼近全监督数据的梯度。"
}