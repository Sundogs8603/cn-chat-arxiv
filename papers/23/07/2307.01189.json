{
    "title": "Trainable Transformer in Transformer",
    "abstract": "Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat",
    "link": "https://arxiv.org/abs/2307.01189",
    "context": "Title: Trainable Transformer in Transformer\nAbstract: Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat",
    "path": "papers/23/07/2307.01189.json",
    "total_tokens": 845,
    "translated_title": "可训练的Transformer in Transformer",
    "translated_abstract": "最近的研究将大型预训练语言模型中的上下文学习能力归因于在推理过程中隐式模拟和微调内部模型（如线性或2层MLP）。然而，这种构造需要大量的内存开销，使得模拟更复杂的内部模型变得困难。在这项工作中，我们提出了一种高效的构造方式，称为Transformer in Transformer（简称TinT），它允许一个Transformer在推理过程中模拟和微调复杂的内部模型（如预训练语言模型）。特别是，我们引入了创新的近似技术，使得一个拥有不到20亿参数的TinT模型能够在单次前向传递中模拟和微调一个拥有1.25亿参数的Transformer模型。TinT适用于许多常见的Transformer变体，其设计思路还改进了Transformer中简单模型的效率。我们进行了端到端实验来验证...",
    "tldr": "这篇论文介绍了一种名为Transformer in Transformer (TinT)的高效构造方式，它可以让Transformer在推理过程中模拟和微调复杂的内部模型，同时使用创新的近似技术大幅减少了模型参数和内存开销。"
}