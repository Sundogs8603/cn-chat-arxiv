{
    "title": "Monadic Deep Learning. (arXiv:2307.12187v1 [cs.PL])",
    "abstract": "The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.  We solved this problem in DeepLearning.scala 2. Our contributions are:  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.  2. We designed a set of monads and monad transformers, whic",
    "link": "http://arxiv.org/abs/2307.12187",
    "context": "Title: Monadic Deep Learning. (arXiv:2307.12187v1 [cs.PL])\nAbstract: The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.  We solved this problem in DeepLearning.scala 2. Our contributions are:  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.  2. We designed a set of monads and monad transformers, whic",
    "path": "papers/23/07/2307.12187.json",
    "total_tokens": 790,
    "translated_title": "单子化深度学习",
    "translated_abstract": "Java和Scala社区建立了一个非常成功的大数据生态系统。然而，大部分在其上运行的神经网络是用动态类型编程语言建模的。这些动态类型的深度学习框架将神经网络视为包含许多可训练变量的可微分表达式，并在训练时对这些表达式进行自动求导。直到2019年，静态类型语言的学习框架没有提供传统框架的表达能力。除非创建大量样板代码进行硬编码的反向传播，否则用户无法使用自定义算法。我们在DeepLearning.scala 2中解决了这个问题。我们的贡献是：1.我们发现了一种新的方法，可以对包含多个可训练变量的静态类型函数进行反向模式的自动求导，并且可以自由地与元语言互操作。2.我们设计了一组单子和单子变换器，",
    "tldr": "DeepLearning.scala 2解决了在静态类型语言中使用神经网络进行训练时的自动求导问题，并提供了一组单子和单子变换器。"
}