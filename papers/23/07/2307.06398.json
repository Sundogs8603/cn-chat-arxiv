{
    "title": "Trainability, Expressivity and Interpretability in Gated Neural ODEs. (arXiv:2307.06398v1 [cs.LG])",
    "abstract": "Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (nODEs) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend nODEs by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural ODEs (gnODEs). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gnODEs to learn (approximate) continuous attractors. We further show how reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attrac",
    "link": "http://arxiv.org/abs/2307.06398",
    "context": "Title: Trainability, Expressivity and Interpretability in Gated Neural ODEs. (arXiv:2307.06398v1 [cs.LG])\nAbstract: Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (nODEs) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend nODEs by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural ODEs (gnODEs). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gnODEs to learn (approximate) continuous attractors. We further show how reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attrac",
    "path": "papers/23/07/2307.06398.json",
    "total_tokens": 1034,
    "translated_title": "训练性、表达性和解释性在门控神经ODE中的应用",
    "translated_abstract": "理解生物和人工神经网络中的动态是机器学习和神经科学中一个重要的开放问题。特别是，涉及到复杂存储和检索的计算对于这些网络来说是一个很大的挑战，不容易实现或学习。最近，一类由神经常微分方程（nODEs）描述的模型已经成为一种强大的动态神经网络模型，能够捕捉到复杂的动态。在这里，我们通过引入门控交互作用，赋予nODEs自适应时间尺度，进一步扩展了nODEs模型，我们称之为门控神经ODEs（gnODEs）。通过使用需要对连续量进行记忆的任务，我们展示了gnODEs的归纳偏差学习（近似）连续吸引子。我们进一步展示了降维的gnODEs如何保持其建模能力，同时大大提高了可解释性，甚至允许对学习到的吸引子结构进行显性可视化。",
    "tldr": "本研究扩展了神经常微分方程（nODEs）模型，将其赋予自适应时间尺度，并称之为门控神经ODEs（gnODEs）。通过在连续记忆任务中展示，我们发现gnODEs具有学习（近似）连续吸引子的归纳偏差学习能力。此外，我们还展示了降维后的gnODEs在提高可解释性的同时保持了建模能力，并可以对学习到的吸引子结构进行显性可视化。",
    "en_tdlr": "This study extends neural ordinary differential equations (nODEs) by endowing them with adaptive timescales using gating interactions, resulting in gated neural ODEs (gnODEs). The gnODEs demonstrate an inductive bias to learn (approximate) continuous attractors in a continuous memory task. Additionally, reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability and enabling explicit visualization of learned attractor structures."
}