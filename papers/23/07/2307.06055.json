{
    "title": "Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])",
    "abstract": "Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.",
    "link": "http://arxiv.org/abs/2307.06055",
    "context": "Title: Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])\nAbstract: Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.",
    "path": "papers/23/07/2307.06055.json",
    "total_tokens": 855,
    "translated_title": "深度贝叶斯分类的函数空间正则化",
    "translated_abstract": "贝叶斯深度学习方法假设模型参数为潜在随机变量，并推断后验分布以量化不确定性，增加安全性和可信度，并防止过于自信和不可预测的行为。然而，权重空间先验是特定于模型的，可能难以解释和难以指定。相反，我们在预测空间中应用Dirichlet先验，并执行近似函数空间变分推断。为此，我们将随机神经网络分类器的传统分类预测解释为来自隐式Dirichlet分布的样本。通过调整推断，可以将相同的函数空间先验与不同的模型结合在一起，而不影响模型的架构或大小。我们通过玩具实验说明了这种先验的灵活性和功效，并通过大规模图像分类实验展示了可扩展性、改进的不确定性量化和对抗性鲁棒性。",
    "tldr": "本研究提出了一种函数空间正则化方法来增加深度贝叶斯分类模型的不确定性量化和对抗性鲁棒性。该方法使用Dirichlet先验在预测空间中进行变分推断，并能与不同模型相结合而不影响模型的架构大小。",
    "en_tdlr": "This paper proposes a function-space regularization method to enhance uncertainty quantification and adversarial robustness in deep Bayesian classification models. The method utilizes a Dirichlet prior in the predictive space for variational inference, and can be combined with different models without affecting their architecture or size."
}