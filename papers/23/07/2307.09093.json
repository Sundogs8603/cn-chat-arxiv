{
    "title": "Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])",
    "abstract": "Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utiliz",
    "link": "http://arxiv.org/abs/2307.09093",
    "context": "Title: Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])\nAbstract: Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utiliz",
    "path": "papers/23/07/2307.09093.json",
    "total_tokens": 895,
    "translated_title": "非平稳延迟组合半强化学习在因果相关回报中的应用",
    "translated_abstract": "在不确定性下的顺序决策中，常常存在长时间的反馈延迟。这种延迟会降低学习代理在长期中识别出一组具有最优总回报的臂的性能。在具有结构依赖的非平稳环境中，这个问题变得极具挑战性。因此，除了适应延迟和环境变化外，学习因果关系可以减轻反馈延迟对决策过程的不利影响。我们将所描述的情景形式化为一个具有因果相关回报的非平稳和延迟的组合半强化学习问题。我们通过一个定向图在一个固定的结构方程模型中建模因果关系。学习代理最大化长期平均回报，该回报定义为基础臂的回报的线性函数。我们开发了一种策略，该策略从延迟的反馈中学习结构依赖关系，并利用这些信息进行决策。",
    "tldr": "这篇论文研究了在非平稳环境中具有结构依赖关系的组合半强化学习问题，提出了一种从延迟的反馈中学习因果关系并做出决策的策略。",
    "en_tdlr": "This paper investigates the combinatorial semi-bandit problem with causal related rewards in a non-stationary environment. It proposes a policy that learns causal relations from delayed feedback and makes decisions based on them."
}