{
    "title": "Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise. (arXiv:2307.08438v1 [cs.LG])",
    "abstract": "We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces with Random Classification Noise under the Gaussian distribution. We establish nearly-matching algorithmic and Statistical Query (SQ) lower bound results revealing a surprising information-computation gap for this basic problem. Specifically, the sample complexity of this learning problem is $\\widetilde{\\Theta}(d/\\epsilon)$, where $d$ is the dimension and $\\epsilon$ is the excess error. Our positive result is a computationally efficient learning algorithm with sample complexity $\\tilde{O}(d/\\epsilon + d/(\\max\\{p, \\epsilon\\})^2)$, where $p$ quantifies the bias of the target halfspace. On the lower bound side, we show that any efficient SQ algorithm (or low-degree test) for the problem requires sample complexity at least $\\Omega(d^{1/2}/(\\max\\{p, \\epsilon\\})^2)$. Our lower bound suggests that this quadratic dependence on $1/\\epsilon$ is inherent for efficient algorithms.",
    "link": "http://arxiv.org/abs/2307.08438",
    "context": "Title: Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise. (arXiv:2307.08438v1 [cs.LG])\nAbstract: We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces with Random Classification Noise under the Gaussian distribution. We establish nearly-matching algorithmic and Statistical Query (SQ) lower bound results revealing a surprising information-computation gap for this basic problem. Specifically, the sample complexity of this learning problem is $\\widetilde{\\Theta}(d/\\epsilon)$, where $d$ is the dimension and $\\epsilon$ is the excess error. Our positive result is a computationally efficient learning algorithm with sample complexity $\\tilde{O}(d/\\epsilon + d/(\\max\\{p, \\epsilon\\})^2)$, where $p$ quantifies the bias of the target halfspace. On the lower bound side, we show that any efficient SQ algorithm (or low-degree test) for the problem requires sample complexity at least $\\Omega(d^{1/2}/(\\max\\{p, \\epsilon\\})^2)$. Our lower bound suggests that this quadratic dependence on $1/\\epsilon$ is inherent for efficient algorithms.",
    "path": "papers/23/07/2307.08438.json",
    "total_tokens": 1001,
    "translated_title": "学习具有随机分类噪声的高斯半空间的近似最优界限",
    "translated_abstract": "我们研究了在高斯分布下学习一般（不一定是齐次的）具有随机分类噪声的半空间的问题。我们建立了一致的算法性和统计查询（SQ）下界结果，揭示了这个基本问题中令人惊讶的信息计算间隙。具体而言，这个学习问题的样本复杂度是$\\widetilde{\\Theta}(d/\\epsilon)$，其中$d$是维度，$\\epsilon$是过量误差。我们的正面结果是一个计算上高效的学习算法，样本复杂度为$\\tilde{O}(d/\\epsilon + d/(\\max\\{p, \\epsilon\\})^2)$，其中$p$量化了目标半空间的偏差。在下界方面，我们表明任何有效的SQ算法（或低次检验）对于该问题至少需要样本复杂度为$\\Omega(d^{1/2}/(\\max\\{p, \\epsilon\\})^2)$。我们的下界结果表明，这种对$1/\\epsilon$的二次依赖性在有效算法中是固有的。",
    "tldr": "该论文研究了在高斯分布下学习具有随机分类噪声的一般半空间的问题，提出了一个近乎最优的算法和统计查询的下界，发现了该问题中的信息计算间隙，并给出了具有两项平方依赖的样本复杂度的计算上高效的学习算法。",
    "en_tdlr": "This paper investigates the problem of learning general halfspaces with random classification noise under the Gaussian distribution. It provides nearly optimal algorithmic and statistical query lower bound results, revealing an information-computation gap for this problem. The paper presents a computationally efficient learning algorithm with quadratic dependence on the sample complexity and establishes a lower bound for any efficient algorithm that also exhibits quadratic dependence."
}