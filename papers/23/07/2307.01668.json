{
    "title": "Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])",
    "abstract": "Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data",
    "link": "http://arxiv.org/abs/2307.01668",
    "context": "Title: Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])\nAbstract: Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data",
    "path": "papers/23/07/2307.01668.json",
    "total_tokens": 910,
    "translated_title": "使用扩散对比发散训练能量模型",
    "translated_abstract": "能量模型（EBM）广泛应用于生成建模。传统的对比发散（CD）训练目标需要使用马尔可夫链蒙特卡罗方法（MCMCs）从EBM中采样，这导致了计算负担和CD有效性之间的不可调和的折衷。MCMCs的收敛需要大量计算资源，而短期运行的MCMC会引入难以处理的额外参数梯度项。本文提出了扩散对比发散（DCD）系列的一般解释，将CD视为DCD的一种特殊情况，并通过在CD中使用不同于Langevin动力学的EBM参数自由扩散过程，提出了一种更有效的发散方法。我们展示了DCD比CD更加计算高效，并且不受非可忽略梯度项的限制。我们进行了大量实验，包括合成数据和实际应用场景的验证。",
    "tldr": "本文提出了一种使用扩散对比发散（DCD）训练能量模型（EBM）的方法，相较于传统的对比发散（CD），DCD在计算效率上更高，并且不受非可忽略梯度项的限制。",
    "en_tdlr": "This paper proposes a method for training energy-based models (EBMs) using diffusion contrastive divergences (DCD). Compared to the traditional contrastive divergence (CD), DCD is more computationally efficient and not limited by the presence of non-negligible gradient terms."
}