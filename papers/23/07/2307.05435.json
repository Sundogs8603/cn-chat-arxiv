{
    "title": "One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])",
    "abstract": "Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \\choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with",
    "link": "http://arxiv.org/abs/2307.05435",
    "context": "Title: One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])\nAbstract: Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \\choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with",
    "path": "papers/23/07/2307.05435.json",
    "total_tokens": 897,
    "translated_title": "One-Versus-Others Attention: 可扩展的多模态集成",
    "translated_abstract": "随着多模态学习模型在问题回答和自动驾驶等各种任务上超越单模态方法，多模态学习模型变得日益重要。尽管多模态学习的重要性，现有的工作仅关注于自然语言处理应用，其中模态数通常少于四个（音频、视频、文本、图像）。然而，在其他领域，如医疗领域，数据输入可能包括X射线、PET扫描、MRI、遗传筛查、临床笔记等，这就需要高效而准确的信息融合。许多最先进的模型依赖于两两跨模态注意力，但对于超过三个模态的应用，这种方法不会很好地扩展。对于$n$个模态，计算注意力将导致$n \\choose 2$的复杂度，可能需要大量的计算资源。为了解决这个问题，我们提出了一种新的领域中立的注意力机制，即一对多（OvO）注意力，该机制随着模态数量线性扩展。",
    "tldr": "提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。"
}