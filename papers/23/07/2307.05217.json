{
    "title": "Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])",
    "abstract": "Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.",
    "link": "http://arxiv.org/abs/2307.05217",
    "context": "Title: Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])\nAbstract: Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.",
    "path": "papers/23/07/2307.05217.json",
    "total_tokens": 828,
    "translated_title": "基于同质性的有监督注意力图神经网络",
    "translated_abstract": "图神经网络已经成为处理图上学习问题的标准方法。在不同变种的图神经网络中，图注意力网络（GATs）被成功应用于不同的任务。在GAT模型中，每个节点使用注意力机制为其邻居分配重要性分数。然而，类似于其他图神经网络，GAT聚合来自属于不同类别的节点的信息，因此产生的节点表示在不同类别方面不够明确，这可能会影响它们的性能。在这项工作中，为了缓解这个问题，我们提出了一种新的技术，可以将其纳入到任何图注意力模型中，以鼓励共享相同类别标签的节点之间获得更高的注意力分数。我们在几个节点分类数据集上评估了所提出的方法，并展示了与标准基线模型相比的性能提升。",
    "tldr": "本文提出了一种新技术，可以在任何图注意力模型中应用，以鼓励共享相同类别标签的节点获得更高的注意力分数，并在多个节点分类数据集上展示了比标准基线模型更高的性能。",
    "en_tdlr": "This paper proposes a new technique that can be applied to any graph attention model to encourage nodes with the same class label to have higher attention scores, and demonstrates improved performance over standard baseline models on multiple node classification datasets."
}