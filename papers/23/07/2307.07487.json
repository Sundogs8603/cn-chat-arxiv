{
    "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])",
    "abstract": "In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significan",
    "link": "http://arxiv.org/abs/2307.07487",
    "context": "Title: DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])\nAbstract: In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significan",
    "path": "papers/23/07/2307.07487.json",
    "total_tokens": 931,
    "translated_title": "DreamTeacher: 使用深度生成模型对图像骨干进行预训练的自监督特征表示学习框架",
    "translated_abstract": "在这项工作中，我们介绍了一个自监督特征表示学习框架DreamTeacher，它利用生成网络来预训练下游图像骨干。我们提出从训练好的生成模型中提取知识，并将其蒸馏到已经为特定感知任务进行了优化的标准图像骨干中。我们研究了两种知识蒸馏方法：1）将学习到的生成特征蒸馏到目标图像骨干上，作为预训练这些骨干的替代方法，而不是使用大规模标注数据集（如ImageNet）进行预训练；2）将生成网络得到的标签通过任务头蒸馏到目标骨干的逻辑层上。我们对多个生成模型、密集预测基准和几种预训练方法进行了详细分析。我们实验证明，我们的DreamTeacher在各方面显著优于现有的自监督特征表示学习方法。使用DreamTeacher进行无监督的ImageNet预训练可以显著提高模型性能。",
    "tldr": "DreamTeacher是一个自监督特征表示学习框架，通过利用生成网络预训练图像骨干，然后将生成模型的知识蒸馏到目标骨干中，取得了显著的性能提升。",
    "en_tdlr": "DreamTeacher is a self-supervised feature representation learning framework that pretrains image backbones using generative networks and distills the knowledge from the trained generative model into target backbones, leading to significant performance improvement."
}