{
    "title": "Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])",
    "abstract": "We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the data set consists of unlabeled data along with a smaller set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution. Additionally, our model is capable of recovering the latent subspace representation of data. Moreover, we establish that the model generates a new population that moves closer to a user-specified target reward value, where the optimality gap aligns with the off-policy bandit regret in the feature subspace. Th",
    "link": "http://arxiv.org/abs/2307.07055",
    "context": "Title: Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])\nAbstract: We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the data set consists of unlabeled data along with a smaller set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution. Additionally, our model is capable of recovering the latent subspace representation of data. Moreover, we establish that the model generates a new population that moves closer to a user-specified target reward value, where the optimality gap aligns with the off-policy bandit regret in the feature subspace. Th",
    "path": "papers/23/07/2307.07055.json",
    "total_tokens": 901,
    "translated_title": "有奖导向的条件扩散：可证明的分布估计和奖励改进",
    "translated_abstract": "本文探讨了通过条件扩散模型进行有奖导向生成的方法和理论。有奖导向生成旨在生成具有由奖励函数衡量的所需特性的样本，广泛应用于生成式人工智能、强化学习和计算生物学领域。我们考虑了数据集包含未标记数据和一小部分具有带噪声奖励标签的数据的常见学习场景。我们的方法利用在较小数据集上学习到的奖励函数作为伪标签生成器。从理论上讲，我们展示了这个有导向的生成器能够有效地学习和从奖励条件下的数据分布中进行采样。此外，我们的模型能够恢复数据的潜在子空间表示。此外，我们还建立了模型生成一个新的群体，该群体靠近用户指定的目标奖励值，其中最优性差距与特征子空间中的离线策略强盗遗憾对齐。",
    "tldr": "使用条件扩散模型进行有奖导向生成，能够有效学习和从奖励条件下的数据分布中进行采样，同时恢复数据的潜在子空间表示，并且生成新的群体靠近用户指定的目标奖励值。"
}