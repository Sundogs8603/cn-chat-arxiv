{
    "title": "Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])",
    "abstract": "In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \\url{https://github.com",
    "link": "http://arxiv.org/abs/2307.14959",
    "context": "Title: Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])\nAbstract: In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \\url{https://github.com",
    "path": "papers/23/07/2307.14959.json",
    "total_tokens": 1010,
    "translated_title": "通过自监督先验在高度不平衡的医学图像分类中实现联邦模型聚合",
    "translated_abstract": "在医学领域，联邦学习通常处理高度不平衡的数据集，包括皮肤病变和胃肠图像。现有的联邦方法在高度不平衡的数据集上主要关注优化全局模型，而没有考虑到由于不同人群、发现和扫描仪导致的医学图像中可能出现的类内变化。在本文中，我们利用公开可用的自监督辅助网络研究了客户间的类内变化。具体而言，我们发现在每个客户端上局部使用共享的辅助预训练模型（如MoCo-V2）可以得到一致的发散测量结果。基于这些发现，我们提出了一个通过自监督先验（MAS）引导全局模型优化的动态平衡模型聚合方法。Fed-MAS可以与不同的局部学习方法结合使用，实现对高度鲁棒和无偏全局模型的有效聚合。",
    "tldr": "本文研究了在高度不平衡的医学图像分类中通过自监督先验实现联邦模型聚合。根据利用公开可用的自监督辅助网络发现，在每个客户端上局部使用共享的预训练模型可以得到一致的发散测量结果。基于这些发现，提出了一个通过自监督先验引导全局模型优化的动态平衡模型聚合方法，用于实现高度鲁棒和无偏的全局模型。",
    "en_tdlr": "This paper explores federated model aggregation via self-supervised priors for highly imbalanced medical image classification. By utilizing publicly available self-supervised auxiliary networks, the authors find that using a shared auxiliary pre-trained model locally on each client yields consistent divergence measurements. Based on these findings, they propose a dynamic balanced model aggregation method, MAS, to guide global model optimization, leading to a highly robust and unbiased global model."
}