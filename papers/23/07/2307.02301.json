{
    "title": "Sumformer: Universal Approximation for Efficient Transformers. (arXiv:2307.02301v1 [cs.LG])",
    "abstract": "Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.",
    "link": "http://arxiv.org/abs/2307.02301",
    "context": "Title: Sumformer: Universal Approximation for Efficient Transformers. (arXiv:2307.02301v1 [cs.LG])\nAbstract: Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.",
    "path": "papers/23/07/2307.02301.json",
    "total_tokens": 840,
    "translated_title": "Sumformer:高效Transformer的通用逼近",
    "translated_abstract": "随着Transformer的引入，自然语言处理（NLP）取得了显著进展。ChatGPT是其中最著名的例子，即使在研究社区之外，也改变了人们对AI可能性的看法。然而，除了令人印象深刻的性能之外，Transformer相对于序列长度的二次时间和空间复杂度限制了处理长序列的能力。尽管高效Transformer架构（如Linformer和Performer）以线性复杂度出现作为有希望的解决方案，但它们的理论理解仍然有限。在本文中，我们引入了Sumformer，一种新颖且简单的架构，能够通用逼近等变序列到序列的函数。我们使用Sumformer给出了Linformer和Performer的第一个通用逼近结果。此外，我们还推导了一个新的Transformer证明，显示只需要一个注意力层就足以进行通用逼近。",
    "tldr": "Sumformer是一种新颖且简单的架构，可以高效地逼近Transformer。通过Sumformer，我们首次给出了Linformer和Performer的通用逼近结果，并推导出一个新的证明，证明只需要一个注意力层就足以进行Transformer的通用逼近。",
    "en_tdlr": "Sumformer is a novel and simple architecture that efficiently approximates Transformers. Through Sumformer, we provide the first universal approximation results for Linformer and Performer, and derive a new proof showing that just one attention layer is sufficient for universal approximation in Transformers."
}