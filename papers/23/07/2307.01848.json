{
    "title": "Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV] CROSS LISTED)",
    "abstract": "Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-traine",
    "link": "http://arxiv.org/abs/2307.01848",
    "context": "Title: Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV] CROSS LISTED)\nAbstract: Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-traine",
    "path": "papers/23/07/2307.01848.json",
    "total_tokens": 945,
    "translated_title": "带有巨型语言模型的具身任务规划",
    "translated_abstract": "为了使机器人能够在通用环境中成功完成复杂的人类指令，为具身智能体提供常识是非常重要的。最近的巨型语言模型（LLM）可以在复杂任务的计划生成中嵌入丰富的语义知识，然而它们缺乏关于真实世界的信息，通常会产生不可行的动作序列。在本文中，我们提出了一个具身任务规划代理（TaPA），用于基于物理场景约束的具身任务规划，其中代理根据场景中已存在的对象通过将LLM与视觉感知模型对齐来生成可执行的计划。具体而言，我们首先构建了一个包含室内场景、指令和行动计划三元组的多模态数据集，其中我们为GPT-3.5提供了设计的提示信息和场景中已存在的对象列表，以生成大量的指令和相应的计划行动。生成的数据用于预训练的基础上进行具身计划的调优。",
    "tldr": "本文提出了一个带有巨型语言模型的具身任务规划代理（TaPA），通过将LLM与视觉感知模型对齐，根据场景中已存在的对象生成可执行的计划。通过构建多模态数据集和利用GPT-3.5生成的数据对预训练模型进行具身计划的调优。",
    "en_tdlr": "This paper proposes an embodied task planning agent (TaPA) with large language models (LLM), which generates executable plans based on existing objects in the scene by aligning LLMs with visual perception models. By constructing a multimodal dataset and leveraging generated data from GPT-3.5, the pre-trained model is fine-tuned for embodied planning."
}