{
    "title": "Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])",
    "abstract": "In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective",
    "link": "http://arxiv.org/abs/2307.08794",
    "context": "Title: Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])\nAbstract: In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective",
    "path": "papers/23/07/2307.08794.json",
    "total_tokens": 997,
    "translated_title": "多时间尺度多智能体强化学习中的非平稳策略学习",
    "translated_abstract": "在多时间尺度多智能体强化学习中，智能体在不同的时间尺度上进行交互。一般来说，对于受多时间尺度引起的时间依赖行为，策略是非平稳的。学习非平稳策略是具有挑战性的，往往需要复杂或低效的算法。由于现实世界复杂系统中控制问题的普遍存在，我们引入了一个简单的框架来学习多时间尺度多智能体强化学习中的非平稳策略。我们的方法利用关于智能体时间尺度的可用信息来定义周期性时间编码。具体而言，我们从理论上证明了通过周期性多智能体策略可以学习多时间尺度引入的非平稳性的效果。为了学习这样的策略，我们提出了一个策略梯度算法，该算法使用相位函数化的神经网络来参数化演员和评论家，为周期性提供归纳偏置。",
    "tldr": "这篇论文提出了一个简单的框架，用于在多时间尺度多智能体强化学习中学习非平稳策略。他们利用智能体时间尺度的信息定义了周期性时间编码，并通过周期性多智能体策略学习多时间尺度引入的非平稳性的效果。他们还提出了一个使用神经网络的策略梯度算法来学习这些策略。",
    "en_tdlr": "This paper proposes a simple framework for learning non-stationary policies in multi-timescale multi-agent reinforcement learning. They define a periodic time encoding using information about agent timescales and learn the effects of non-stationarity introduced by multiple timescales through periodic multi-agent policies. They also propose a policy gradient algorithm using neural networks to learn these policies."
}