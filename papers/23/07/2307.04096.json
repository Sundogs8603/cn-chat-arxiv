{
    "title": "Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing. (arXiv:2307.04096v1 [cs.CL])",
    "abstract": "Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods, however, exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation ",
    "link": "http://arxiv.org/abs/2307.04096",
    "context": "Title: Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing. (arXiv:2307.04096v1 [cs.CL])\nAbstract: Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods, however, exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation ",
    "path": "papers/23/07/2307.04096.json",
    "total_tokens": 997,
    "translated_title": "跨语义解析的最优输运后验对齐",
    "translated_abstract": "跨语义解析通过从高资源语言（例如英语）转移解析能力到训练数据稀缺的低资源语言。以往的研究主要考虑了银标准数据增强或零样本方法，然而利用少样本黄金数据的方法相对未被探索。我们提出了一种新的方法，通过显式地最小化概率潜变量之间的跨语义差异来进行跨语义解析。我们展示了这种直接引导如何在使用较少的样例和较少的训练下改善自然语言解析。我们在两个数据集（MTOP和MultiATIS++SQL）上评估了我们的方法，在少样本跨语言制度下取得了最先进的结果。消融研究进一步揭示了我们的方法即使在没有并行输入翻译的情况下也能提高性能。此外，我们还证明了我们的模型更好地捕捉跨语言结构在潜空间中以改善语义表示。",
    "tldr": "该论文提出了一种新的跨语义解析方法，利用最优输运来显式地最小化概率潜变量之间的跨语义差异。通过这种直接引导，可以使用较少的样例和训练来改善自然语言解析效果，并且在少样本跨语言制度下取得了最先进的结果。同时，该方法在没有并行输入翻译的情况下也能提高性能，并且更好地捕捉跨语言结构以改善语义表示。",
    "en_tdlr": "This paper proposes a new approach for cross-lingual semantic parsing by minimizing cross-lingual divergence between latent variables using Optimal Transport. The method improves parsing from natural languages using fewer examples and establishes state-of-the-art results under a few-shot cross-lingual regime. It also demonstrates performance improvement without parallel input translations and better capture of cross-lingual structure for improved semantic representation."
}