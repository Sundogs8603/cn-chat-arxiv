{
    "title": "FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])",
    "abstract": "We present FinTree, Financial Dataset Pretrain Transformer Encoder for Relation Extraction. Utilizing an encoder language model, we further pretrain FinTree on the financial dataset, adapting the model in financial domain tasks. FinTree stands out with its novel structure that predicts a masked token instead of the conventional [CLS] token, inspired by the Pattern Exploiting Training methodology. This structure allows for more accurate relation predictions between two given entities. The model is trained with a unique input pattern to provide contextual and positional information about the entities of interest, and a post-processing step ensures accurate predictions in line with the entity types. Our experiments demonstrate that FinTree outperforms on the REFinD, a large-scale financial relation extraction dataset. The code and pretrained models are available at https://github.com/HJ-Ok/FinTree.",
    "link": "http://arxiv.org/abs/2307.13900",
    "context": "Title: FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])\nAbstract: We present FinTree, Financial Dataset Pretrain Transformer Encoder for Relation Extraction. Utilizing an encoder language model, we further pretrain FinTree on the financial dataset, adapting the model in financial domain tasks. FinTree stands out with its novel structure that predicts a masked token instead of the conventional [CLS] token, inspired by the Pattern Exploiting Training methodology. This structure allows for more accurate relation predictions between two given entities. The model is trained with a unique input pattern to provide contextual and positional information about the entities of interest, and a post-processing step ensures accurate predictions in line with the entity types. Our experiments demonstrate that FinTree outperforms on the REFinD, a large-scale financial relation extraction dataset. The code and pretrained models are available at https://github.com/HJ-Ok/FinTree.",
    "path": "papers/23/07/2307.13900.json",
    "total_tokens": 862,
    "translated_title": "FinTree：用于关系抽取的金融数据集预训练变换器编码器",
    "translated_abstract": "我们提出了FinTree，使用金融数据集预训练的变换器编码器，用于关系抽取。利用编码器语言模型，在金融领域任务中进一步预训练FinTree。FinTree以其预测掩码标记而不是传统的[CLS]标记的新颖结构而脱颖而出，受到模式利用训练方法的启发。这种结构可以更准确地预测两个给定实体之间的关系。该模型通过一种独特的输入模式进行训练，以提供有关所关注实体的上下文和位置信息，并通过后处理步骤确保与实体类型一致的准确预测。我们的实验证明FinTree在大规模金融关系抽取数据集REFinD上的表现优于其他模型。代码和预训练模型可在https://github.com/HJ-Ok/FinTree上获取。",
    "tldr": "FinTree是一种用于关系抽取的金融数据集预训练变换器编码器，其采用了预测掩码标记的新颖结构，通过训练提供上下文和位置信息，并在大规模金融关系抽取数据集上表现出色。",
    "en_tdlr": "FinTree is a financial dataset pretrain transformer encoder for relation extraction. It introduces a novel structure that predicts a masked token instead of the conventional [CLS] token, providing more accurate relation predictions. The model outperforms on a large-scale financial relation extraction dataset."
}