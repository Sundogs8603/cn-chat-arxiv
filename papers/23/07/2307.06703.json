{
    "title": "Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues. (arXiv:2307.06703v1 [cs.CL])",
    "abstract": "Answer selection in open-domain dialogues aims to select an accurate answer from candidates. Recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.",
    "link": "http://arxiv.org/abs/2307.06703",
    "context": "Title: Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues. (arXiv:2307.06703v1 [cs.CL])\nAbstract: Answer selection in open-domain dialogues aims to select an accurate answer from candidates. Recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.",
    "path": "papers/23/07/2307.06703.json",
    "total_tokens": 954,
    "translated_title": "意图校准的自我训练用于开放领域对话中的答案选择",
    "translated_abstract": "开放领域对话中的答案选择旨在从候选答案中选择准确的答案。最近答案选择模型的成功依赖于使用大量标记数据进行训练。然而，收集大规模标记数据是一项耗时且费力的工作。在本文中，我们引入了预测的意图标签以校准自我训练范式中的答案标签。具体而言，我们提出了意图校准的自我训练（ICAST）来通过意图校准的答案选择范式提高伪答案标签的质量，其中我们使用伪意图标签来帮助改进伪答案标签。我们在两个基准数据集上进行了大量实验，涉及开放领域对话。实验结果显示，在只有1％、5％和10％标记数据的情况下，ICAST始终优于基线。特别是与仅有5％标记数据的最强基线相比，它在两个数据集上将F1得分分别提高了2.06％和1.00％。",
    "tldr": "这项研究介绍了一种名为意图校准的自我训练方法，用于改善开放领域对话中答案选择的质量。通过使用意图标签来校准答案标签，作者通过实验证明了这种方法在两个基准数据集上的效果优于基线模型。",
    "en_tdlr": "This paper presents intent-calibrated self-training (ICAST) method for improving answer selection quality in open-domain dialogues. By using intent labels to calibrate answer labels, the authors demonstrated through experiments that ICAST outperforms baselines on two benchmark datasets."
}