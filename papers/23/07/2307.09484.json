{
    "title": "MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])",
    "abstract": "Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures",
    "link": "http://arxiv.org/abs/2307.09484",
    "context": "Title: MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])\nAbstract: Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures",
    "path": "papers/23/07/2307.09484.json",
    "total_tokens": 830,
    "translated_title": "MolFM:一种多模态分子基础模型",
    "translated_abstract": "分子知识存在于三种不同的信息来源模式中：分子结构、生物医学文献和知识库。有效整合来自这些模态的分子知识对促进生物医学研究至关重要。然而，现有的多模态分子基础模型在捕捉分子结构和文本之间的复杂关联方面存在局限性，更重要的是，它们中的任何一个都没有尝试利用从知识图谱中获得的丰富分子专业知识。在本研究中，我们介绍了MolFM，一种多模态分子基础模型，旨在促进从分子结构、生物医学文本和知识图谱中进行联合表示学习。我们提出了分子结构中的原子、分子实体的邻居和语义相关文本之间的跨模态关注，以促进跨模态理解。我们提供了理论分析，表明我们的跨模态预训练捕捉到了分子结构、文本和知识图谱之间的复杂关系。",
    "tldr": "MolFM是一种多模态分子基础模型，通过跨模态关注实现了分子结构、文本和知识图谱之间的联合表示学习。",
    "en_tdlr": "MolFM is a multimodal molecular foundation model that enables joint representation learning from molecular structures, biomedical texts, and knowledge graphs through cross-modal attention."
}