{
    "title": "Generative Visual Question Answering. (arXiv:2307.10405v1 [cs.CV])",
    "abstract": "Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The current models lack temporal generalization which enables models to adapt to changes in future data. This paper discusses a viable approach to creating an advanced Visual Question Answering (VQA) model which can produce successful results on temporal generalization. We propose a new data set, GenVQA, utilizing images and captions from the VQAv2 and MS-COCO dataset to generate new images through stable diffusion. This augmented dataset is then used to test a combination of seven baseline and cutting edge VQA models. Performance evaluation focuses on questions mirroring the original VQAv2 dataset, with the answers having been adjusted to the new images. This paper's purpose is to investigate the robustness of several successful VQA models to assess their performance on future da",
    "link": "http://arxiv.org/abs/2307.10405",
    "context": "Title: Generative Visual Question Answering. (arXiv:2307.10405v1 [cs.CV])\nAbstract: Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The current models lack temporal generalization which enables models to adapt to changes in future data. This paper discusses a viable approach to creating an advanced Visual Question Answering (VQA) model which can produce successful results on temporal generalization. We propose a new data set, GenVQA, utilizing images and captions from the VQAv2 and MS-COCO dataset to generate new images through stable diffusion. This augmented dataset is then used to test a combination of seven baseline and cutting edge VQA models. Performance evaluation focuses on questions mirroring the original VQAv2 dataset, with the answers having been adjusted to the new images. This paper's purpose is to investigate the robustness of several successful VQA models to assess their performance on future da",
    "path": "papers/23/07/2307.10405.json",
    "total_tokens": 910,
    "translated_title": "生成视觉问答",
    "translated_abstract": "在深度学习中，涉及视觉和语言的多模态任务越来越受欢迎，并导致开发出可以超越其训练数据范围的新模型。当前模型缺乏时间上的泛化能力，无法适应未来数据的变化。本文讨论了一种可行的方法，可以创建一个先进的视觉问答（VQA）模型，能在时间上进行泛化，并取得成功的结果。我们提出了一个新的数据集，GenVQA，利用VQAv2和MS-COCO数据集中的图像和标题通过稳定的扩散生成新的图像。然后使用这个增广的数据集来测试七种基线和尖端的VQA模型的组合。性能评估主要关注问题与原始VQAv2数据集相似的问题，答案已经根据新的图像进行了调整。本文的目的是调查几个成功的VQA模型的鲁棒性，以评估它们在未来数据上的表现。",
    "tldr": "本文研究了一种先进的生成视觉问答（VQA）模型，通过利用新数据集GenVQA，该数据集通过稳定扩散生成新的图像，并使用了七种不同的VQA模型。研究结果表明，这些模型在未来数据上表现出较好的适应性和鲁棒性。",
    "en_tdlr": "This paper investigates an advanced Generative Visual Question Answering (VQA) model, utilizing a new dataset GenVQA that generates new images through stable diffusion, and evaluates the performance of seven different VQA models. The results show that these models demonstrate good adaptability and robustness on future data."
}