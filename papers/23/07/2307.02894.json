{
    "title": "Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge. (arXiv:2307.02894v1 [cs.LG])",
    "abstract": "Mixed-precision quantization, where a deep neural network's layers are quantized to different precisions, offers the opportunity to optimize the trade-offs between model size, latency, and statistical accuracy beyond what can be achieved with homogeneous-bit-width quantization. To navigate the intractable search space of mixed-precision configurations for a given network, this paper proposes a hybrid search methodology. It consists of a hardware-agnostic differentiable search algorithm followed by a hardware-aware heuristic optimization to find mixed-precision configurations latency-optimized for a specific hardware target. We evaluate our algorithm on MobileNetV1 and MobileNetV2 and deploy the resulting networks on a family of multi-core RISC-V microcontroller platforms with different hardware characteristics. We achieve up to 28.6% reduction of end-to-end latency compared to an 8-bit model at a negligible accuracy drop from a full-precision baseline on the 1000-class ImageNet dataset",
    "link": "http://arxiv.org/abs/2307.02894",
    "context": "Title: Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge. (arXiv:2307.02894v1 [cs.LG])\nAbstract: Mixed-precision quantization, where a deep neural network's layers are quantized to different precisions, offers the opportunity to optimize the trade-offs between model size, latency, and statistical accuracy beyond what can be achieved with homogeneous-bit-width quantization. To navigate the intractable search space of mixed-precision configurations for a given network, this paper proposes a hybrid search methodology. It consists of a hardware-agnostic differentiable search algorithm followed by a hardware-aware heuristic optimization to find mixed-precision configurations latency-optimized for a specific hardware target. We evaluate our algorithm on MobileNetV1 and MobileNetV2 and deploy the resulting networks on a family of multi-core RISC-V microcontroller platforms with different hardware characteristics. We achieve up to 28.6% reduction of end-to-end latency compared to an 8-bit model at a negligible accuracy drop from a full-precision baseline on the 1000-class ImageNet dataset",
    "path": "papers/23/07/2307.02894.json",
    "total_tokens": 924,
    "translated_title": "自由位：在边缘上优化混合精度量化的神经网络延迟",
    "translated_abstract": "混合精度量化，即将深度神经网络的层量化为不同的精度，为优化模型大小、延迟和统计准确性之间的权衡提供了机会，超越了同质位宽量化所能实现的。为了在给定网络的混合精度配置的难以处理的搜索空间中进行导航，本文提出了一种混合搜索方法。它由一种硬件不可知的可微分搜索算法和一种硬件感知的启发式优化组成，以找到针对特定硬件目标优化延迟的混合精度配置。我们在MobileNetV1和MobileNetV2上评估了我们的算法，并将结果网络部署在不同硬件特性的多核RISC-V微控制器平台系列上。与8位模型相比，在1000类ImageNet数据集上，与全精度基准相比，我们实现了高达28.6％的端到端延迟降低，准确性几乎没有下降",
    "tldr": "本论文提出了一种混合搜索方法，通过硬件不可知的可微分搜索算法和硬件感知的启发式优化，可以优化混合精度配置对特定硬件目标的延迟。在MobileNetV1和MobileNetV2上的实验结果表明，在1000类ImageNet数据集上相比于8位模型，在保证几乎没有准确性下降的情况下，能够实现高达28.6％的端到端延迟降低"
}