{
    "title": "Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])",
    "abstract": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.",
    "link": "http://arxiv.org/abs/2307.05228",
    "context": "Title: Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])\nAbstract: Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.",
    "path": "papers/23/07/2307.05228.json",
    "total_tokens": 788,
    "translated_title": "属性控制的对话引导",
    "translated_abstract": "为了适应下游任务，提示调整已成为一种越来越受欢迎的参数高效方法。然而，离散提示和连续提示都假设任务中的所有数据样本使用相同的固定提示，忽略了某些任务（如开放域对话生成）中输入的巨大变化。本文提出了一种新颖的、基于实例级控制代码的对话引导算法。具体来说，我们基于实例级控制代码而不是对话历史生成提示，以探索实例特定的提示对于控制对话生成的影响。在流行的开放域对话数据集上进行的实验，在自动指标和人工评估方面都证明我们的方法优于提示基线，并且与仅使用总参数的5%-6%的微调相媲美。",
    "tldr": "本文提出了一种新的基于实例级控制代码的对话引导算法，用于探索实例特定的提示对于控制对话生成的影响。实验结果表明，该方法优于提示基线，并且与仅使用总参数的微调相媲美。",
    "en_tdlr": "This paper introduces a novel dialogue prompting algorithm based on instance-specific control code to explore the impact of instance-specific prompts on controlled dialogue generation. Experimental results demonstrate that the method outperforms baseline prompting approaches and is comparable to fine-tuning with only a small fraction of total parameters."
}