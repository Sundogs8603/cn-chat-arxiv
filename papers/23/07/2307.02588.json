{
    "title": "TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])",
    "abstract": "Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti",
    "link": "http://arxiv.org/abs/2307.02588",
    "context": "Title: TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])\nAbstract: Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti",
    "path": "papers/23/07/2307.02588.json",
    "total_tokens": 844,
    "translated_title": "TransformerG2G：使用Transformer进行自适应时间步长的学习时态图嵌入",
    "translated_abstract": "动态图嵌入已成为处理不同时间图分析任务（如链路预测、节点分类、推荐系统、异常检测和图生成）的一种非常有效的技术，广泛应用于各种应用领域。这些时态图展现了异质的瞬时动态、不同的时间间隔以及在演化过程中高度变化的节点特征。因此，将历史图上的长程依赖融入到学习时态动态的过程中至关重要。本文提出了一个带有不确定性量化的图嵌入模型TransformerG2G，通过利用先进的Transformer编码器从当前状态（$t$）和之前的上下文（时间戳[$t-1, t-l$]，$l$是上下文的长度）中首先学习中间节点表示。此外，我们采用两个投影层来生成每个节点的低维多元高斯分布，作为其潜在嵌入。",
    "tldr": "TransformerG2G是一种使用Transformer进行自适应时间步长的图嵌入模型，通过学习历史上的长程依赖关系，准确地捕捉时态图的动态特征。"
}