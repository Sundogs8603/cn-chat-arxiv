{
    "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])",
    "abstract": "In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to p",
    "link": "http://arxiv.org/abs/2307.03692",
    "context": "Title: Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])\nAbstract: In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to p",
    "path": "papers/23/07/2307.03692.json",
    "total_tokens": 893,
    "translated_title": "成为自学者：引入最小指令调整的早停标准",
    "translated_abstract": "在本文中，我们引入了指令跟随得分（IFS），一种检测语言模型遵循指令能力的度量标准。该度量标准具有双重目的。首先，IFS可以用于区分基础模型和指令模型。我们对公开可用的基础模型和指令模型进行了基准测试，并显示出良好格式化响应与部分和完整句子的比例可以作为这两种模型类别之间的有效衡量指标。其次，该度量标准可以用作指令调整的早停标准。我们计算了7B和13B LLaMA模型的有监督微调的IFS，显示模型在训练过程中相对早期就学会了遵循指令，并且进一步微调可能导致基础模型语义的变化。作为语义变化的示例，我们展示了由辅助度量标准ObjecQA定义的模型预测的客观性。我们表明在这种特定情况下，当IFS倾向于p时，语义变化最为剧烈。",
    "tldr": "该论文引入了一个用于检测语言模型遵循指令能力的度量标准，并将其用作早停标准进行指令调整。实验证明模型能够相对早期地学会遵循指令，并且进一步微调可能导致语义变化。"
}