{
    "title": "Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)",
    "abstract": "While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net pr",
    "link": "http://arxiv.org/abs/2307.12344",
    "context": "Title: Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)\nAbstract: While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net pr",
    "path": "papers/23/07/2307.12344.json",
    "total_tokens": 940,
    "translated_title": "错误的原因而正确的：可解释的机器学习技术能够检测到虚假相关性吗？",
    "translated_abstract": "虽然深度神经网络模型可以提供无与伦比的分类性能，但它们很容易学习到数据中的虚假相关性。如果测试数据与训练数据来自相同的分布，那么这种对混淆信息的依赖会很难通过性能指标来检测出来。可解释的机器学习方法，如事后解释或本质上可解释的分类器，承诺可以识别出错误的模型推理。然而，目前对于这些技术是否真的能够做到这一点存在着一些混合的证据。本文提出了一种严格的评估策略，以评估解释技术正确识别虚假相关性的能力。使用这种策略，我们评估了五种事后解释技术和一种本质上可解释的方法对于在胸透诊断任务中检测三种人为添加的混淆因子的能力。我们发现事后解释技术SHAP以及本质上可解释的Attri-Net方法可以有效地检测出虚假相关性。",
    "tldr": "本研究在胸透诊断任务中评估了五种事后解释技术和一种本质上可解释的方法对于检测虚假相关性的能力，发现SHAP和Attri-Net可以有效地检测出这种类型的依赖关系。",
    "en_tdlr": "This study evaluates five post-hoc explanation techniques and one inherently interpretable method for their ability to detect spurious correlations in a chest x-ray diagnosis task and finds that SHAP and Attri-Net are effective in detecting such dependencies."
}