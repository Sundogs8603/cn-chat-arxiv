{
    "title": "Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])",
    "abstract": "Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi",
    "link": "http://arxiv.org/abs/2307.05639",
    "context": "Title: Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])\nAbstract: Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi",
    "path": "papers/23/07/2307.05639.json",
    "total_tokens": 854,
    "translated_title": "使用高斯径向基函数神经网络学习活跃子空间并发现重要特征",
    "translated_abstract": "提供一个既能达到强大预测性能，又能被人类解释的模型是机器学习研究中最困难的挑战之一，由于这两个目标的冲突性。为解决这个挑战，我们提出了一种修改的径向基函数神经网络模型，通过为其高斯核添加可学习的精度矩阵。我们展示了训练完成后可以从精度矩阵的谱中提取宝贵的信息。特别是，特征向量解释了模型最敏感的方向，揭示了活跃子空间，并提出了用于监督降维的潜在应用。同时，特征向量凸显了输入和潜在变量之间的绝对变化关系，从而使我们能够基于其对预测的重要性提取输入变量的排序。",
    "tldr": "本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。",
    "en_tdlr": "This paper proposes a modified Gaussian Radial Basis Function Neural Network model with a learnable precision matrix, which extracts valuable information from the spectrum of the precision matrix, including the direction of the active subspace and the ranking of input variable importance."
}