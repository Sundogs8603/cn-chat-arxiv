{
    "title": "Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ",
    "link": "http://arxiv.org/abs/2307.04679",
    "context": "Title: Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)\nAbstract: In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ",
    "path": "papers/23/07/2307.04679.json",
    "total_tokens": 944,
    "translated_title": "一阶方法在具有通用预测器的统计学习中的泛化误差分析",
    "translated_abstract": "本文提供了一个新的框架，用于分析在统计学习中使用一阶优化算法时，当梯度只能通过预测器给出的部分观测来访问时的泛化误差。我们的分析依赖于梯度对数据样本的规则性，并且可以推导出多个学习问题的泛化误差的紧密匹配的上界和下界，包括监督学习、迁移学习、鲁棒学习、分布式学习和使用梯度量化的通信效率学习。这些结果适用于光滑且强凸的优化问题，以及满足Polyak-Lojasiewicz假设的光滑非凸优化问题。特别地，我们的上界和下界依赖于一个扩展了条件标准差概念的新量，它衡量了通过访问预测器可以近似梯度的程度。",
    "tldr": "本文提出了一种新的框架来分析使用一阶优化算法进行统计学习时的泛化误差，该框架适用于多个学习问题，并且可以推导出紧密匹配的上界和下界。这些结果适用于光滑、强凸和满足Polyak-Lojasiewicz假设的优化问题。"
}