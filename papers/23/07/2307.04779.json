{
    "title": "Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational Inference. (arXiv:2307.04779v1 [stat.ML])",
    "abstract": "We provide a rigorous analysis of training by variational inference (VI) of Bayesian neural networks in the two-layer and infinite-width case. We consider a regression problem with a regularized evidence lower bound (ELBO) which is decomposed into the expected log-likelihood of the data and the Kullback-Leibler (KL) divergence between the a priori distribution and the variational posterior. With an appropriate weighting of the KL, we prove a law of large numbers for three different training schemes: (i) the idealized case with exact estimation of a multiple Gaussian integral from the reparametrization trick, (ii) a minibatch scheme using Monte Carlo sampling, commonly known as Bayes by Backprop, and (iii) a new and computationally cheaper algorithm which we introduce as Minimal VI. An important result is that all methods converge to the same mean-field limit. Finally, we illustrate our results numerically and discuss the need for the derivation of a central limit theorem.",
    "link": "http://arxiv.org/abs/2307.04779",
    "context": "Title: Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational Inference. (arXiv:2307.04779v1 [stat.ML])\nAbstract: We provide a rigorous analysis of training by variational inference (VI) of Bayesian neural networks in the two-layer and infinite-width case. We consider a regression problem with a regularized evidence lower bound (ELBO) which is decomposed into the expected log-likelihood of the data and the Kullback-Leibler (KL) divergence between the a priori distribution and the variational posterior. With an appropriate weighting of the KL, we prove a law of large numbers for three different training schemes: (i) the idealized case with exact estimation of a multiple Gaussian integral from the reparametrization trick, (ii) a minibatch scheme using Monte Carlo sampling, commonly known as Bayes by Backprop, and (iii) a new and computationally cheaper algorithm which we introduce as Minimal VI. An important result is that all methods converge to the same mean-field limit. Finally, we illustrate our results numerically and discuss the need for the derivation of a central limit theorem.",
    "path": "papers/23/07/2307.04779.json",
    "total_tokens": 928,
    "translated_title": "使用变分推断训练的贝叶斯两层神经网络的大数定律",
    "translated_abstract": "我们对使用变分推断训练的贝叶斯神经网络在两层和无限宽度情况下进行了严格的分析。我们考虑了一个带有正则化证据下界（ELBO）的回归问题，它被分解为数据的期望对数似然和先验分布与变分后验之间的Kullback-Leibler（KL）散度。通过适当加权KL，我们证明了三种不同的训练方案的大数定律：（i）理想情况下，通过重新参数化技巧准确估计多元高斯积分，（ii）使用蒙特卡洛采样的小批量方案，通常被称为Bayes by Backprop，（iii）一种新的、计算成本更低的算法，我们将其称为Minimal VI。一个重要的结果是所有方法都收敛到相同的均场极限。最后，我们通过数值实验验证了我们的结果，并讨论了中心极限定理的推导需求。",
    "tldr": "该论文针对贝叶斯神经网络在两层和无限宽度情况下使用变分推断进行训练提供了严格的分析，并证明了三种不同的训练方案的大数定律。这些方法都收敛到相同的均场极限。",
    "en_tdlr": "This paper rigorously analyzes the training of Bayesian neural networks using variational inference in the case of two layers and infinite width, and proves the law of large numbers for three different training schemes. All methods converge to the same mean-field limit."
}