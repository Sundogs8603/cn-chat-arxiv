{
    "title": "inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])",
    "abstract": "The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th",
    "link": "http://arxiv.org/abs/2307.03854",
    "context": "Title: inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])\nAbstract: The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th",
    "path": "papers/23/07/2307.03854.json",
    "total_tokens": 904,
    "translated_title": "inTformer: 一种基于时间嵌入的关注机制Transformer用于使用连接车辆数据的路口事故可能性预测",
    "translated_abstract": "实时事故可能性预测模型是主动交通安全管理系统的关键组成部分。多年来，许多研究尝试构建事故可能性预测模型，以提高交通安全，但主要集中在高速公路上。在大多数现有研究中，研究人员主要采用基于深度学习的框架来识别事故潜在风险。最近，Transformer已经成为一种潜在的深度神经网络，其基本原理是通过注意力机制来进行操作。Transformer在功能上比现有的深度学习模型（如LSTM，CNN等）具有几个优势。首先，Transformer可以轻松处理数据序列中的长期依赖性。其次，Transformer可以在训练期间并行处理数据序列中的所有元素。最后，Transformer不存在梯度消失的问题。认识到Transformer的巨大潜力，",
    "tldr": "inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。"
}