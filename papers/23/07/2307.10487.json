{
    "title": "Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground tr",
    "link": "http://arxiv.org/abs/2307.10487",
    "context": "Title: Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground tr",
    "path": "papers/23/07/2307.10487.json",
    "total_tokens": 903,
    "translated_title": "恶意注释下的物体检测后门攻击",
    "translated_abstract": "深度神经网络（DNN）在物体检测任务中取得了前所未有的成功。然而，也发现DNN对多种攻击，包括后门攻击，是脆弱的。通过这种攻击，攻击者成功地将隐藏的后门嵌入到DNN中，使得模型在良性数据样本上表现正常，但在预定义触发器出现时给出攻击者指定的判断。尽管已经在图像分类上尝试了大量后门攻击，但对物体检测任务的后门攻击研究尚未得到适当的调查和探索。由于物体检测已被应用于多个安全敏感应用程序的重要模块，如自动驾驶，对物体检测的后门攻击可能造成更严重的威胁。受基于深度学习的目标检测器的固有属性启发，我们提出了一种简单而有效的物体检测后门攻击方法，而不修改地面真伪标签。",
    "tldr": "本文提出了一种在物体检测中进行后门攻击的方法，通过嵌入隐藏的后门，使得模型在正常数据上表现正常，在触发器出现时给出攻击者指定的判断。这对于安全敏感应用如自动驾驶具有严重威胁。",
    "en_tdlr": "This paper proposes a method for conducting backdoor attacks on object detection by embedding hidden backdoors, making the model behave normally on clean data and give attacker-specified judgments on trigger occurrences. This poses a serious threat to security-sensitive applications such as autonomous driving."
}