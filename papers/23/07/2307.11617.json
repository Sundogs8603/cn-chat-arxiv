{
    "title": "Robust Fully-Asynchronous Methods for Distributed Training over General Architecture. (arXiv:2307.11617v1 [cs.DC])",
    "abstract": "Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly",
    "link": "http://arxiv.org/abs/2307.11617",
    "context": "Title: Robust Fully-Asynchronous Methods for Distributed Training over General Architecture. (arXiv:2307.11617v1 [cs.DC])\nAbstract: Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly",
    "path": "papers/23/07/2307.11617.json",
    "total_tokens": 947,
    "translated_title": "强健的全异步方法用于通用架构上的分布式训练",
    "translated_abstract": "分布式机器学习问题中完美的同步是低效甚至不可能的，由于延迟、数据丢失和延迟较大的设备。我们提出了一种强健的全异步随机梯度跟踪方法（R-FAST），其中每个设备以自己的速度进行本地计算和通信，而无需任何形式的同步。与现有的异步分布式算法不同，R-FAST可以通过采用基于设计良好的辅助变量来跟踪和缓冲整体梯度向量的鲁棒梯度跟踪策略，消除设备间数据异构性的影响，并允许数据包丢失。更重要的是，所提出的方法利用两个生成树图进行通信，只要两者共享至少一个共同的根节点，就能实现灵活的通信架构设计。我们证明了R-FAST对于平滑和强凸问题，收敛到最优解的期望邻域，并具有几何收敛率。",
    "tldr": "该论文提出了一种强健的全异步方法（R-FAST），在分布式机器学习中解决了完美同步的低效性和不可能性问题，通过采用鲁棒的梯度跟踪策略和灵活的通信架构，消除了数据异构性和数据包丢失的影响，并实现了期望的邻域收敛。",
    "en_tdlr": "This paper proposes a robust fully-asynchronous method (R-FAST) that addresses the inefficiency and impossibility of perfect synchronization in distributed machine learning. By employing a robust gradient tracking strategy and flexible communication architecture, R-FAST eliminates the impact of data heterogeneity and packet losses, achieving convergence in the expected neighborhood."
}