{
    "title": "An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])",
    "abstract": "Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.",
    "link": "http://arxiv.org/abs/2307.08873",
    "context": "Title: An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])\nAbstract: Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.",
    "path": "papers/23/07/2307.08873.json",
    "total_tokens": 918,
    "translated_title": "一种风险厌恶策略梯度的方差替代：基尼离差",
    "translated_abstract": "在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。",
    "tldr": "本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。",
    "en_tdlr": "This research proposes an alternative method for risk-averse policy gradient by using Gini deviation as a substitute for variance, mitigating the limitations of variance-based methods and achieving high return and low risk in empirical evaluation."
}