{
    "title": "Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow. (arXiv:2307.01879v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\\min_G \\max_D E(G, D)$ in GANs. To address this issue, we a",
    "link": "http://arxiv.org/abs/2307.01879",
    "context": "Title: Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow. (arXiv:2307.01879v2 [cs.LG] UPDATED)\nAbstract: In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\\min_G \\max_D E(G, D)$ in GANs. To address this issue, we a",
    "path": "papers/23/07/2307.01879.json",
    "total_tokens": 967,
    "translated_title": "基于粒子距离的GAN稳定性分析框架与Wasserstein渐变流",
    "translated_abstract": "本文研究了生成网络的训练过程，该网络将粒子距离作为目标函数，例如MMD GAN，Cramer GAN和EIEG GAN。然而，这些GAN往往存在训练不稳定的问题。本文从概率密度动态的角度分析了这些GAN的训练过程的稳定性。我们将GAN中的判别器D看作是一个特征转换映射，它将高维数据映射到一个特征空间，而生成器G则将随机变量映射到类似于真实数据的样本。基于这个视角，我们可以通过概率密度函数的Wasserstein渐变流来进行GAN的稳定性分析。我们发现，由于GAN中$\\min_G \\max_D E(G, D)$的公式，判别器的训练过程通常是不稳定的。为了解决这个问题，我们提出了一个稳定的训练方法。",
    "tldr": "本文提出了一个基于粒子距离的GAN稳定性分析框架，并使用Wasserstein渐变流对GAN的训练过程进行稳定性分析。研究发现，由于GAN的目标函数形式的原因，判别器的训练过程通常是不稳定的。为了解决这个问题，文中提出了一个稳定的训练方法。",
    "en_tdlr": "This paper presents a stability analysis framework for particle-based distance GANs and analyzes their training process using Wasserstein gradient flow. The research finds that the training of the discriminator in GANs is often unstable due to the formulation of the objective function. A stable training method is proposed to address this issue."
}