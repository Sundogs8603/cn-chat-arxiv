{
    "title": "CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization. (arXiv:2307.02716v1 [cs.CL])",
    "abstract": "Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies th",
    "link": "http://arxiv.org/abs/2307.02716",
    "context": "Title: CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization. (arXiv:2307.02716v1 [cs.CL])\nAbstract: Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies th",
    "path": "papers/23/07/2307.02716.json",
    "total_tokens": 868,
    "translated_title": "CFSum: 一种用于多模态摘要的粗到细贡献网络",
    "translated_abstract": "多模态摘要通常存在视觉模态贡献不明确的问题。现有的多模态摘要方法都集中在设计不同模态的融合方法，而忽视了视觉模态有用的自适应条件。因此，我们提出了一种新颖的粗到细贡献网络用于多模态摘要（CFSum），以考虑图像在摘要中的不同贡献。首先，为了消除无用图像的干扰，我们提出了一个预过滤模块来舍弃无用图像。其次，为了准确使用有用图像，我们提出了两个层次的视觉补充模块，词级和短语级。具体而言，计算图像贡献并用于引导文本和视觉模态的注意力。实验结果表明，CFSum在标准基准上明显优于多个强基准。此外，分析验证了提出的方法的有效性。",
    "tldr": "CFSum是一个用于多模态摘要的粗到细贡献网络，能够准确计算和利用图像在摘要中的不同贡献，实验结果显示其优于其他基准方法。",
    "en_tdlr": "CFSum is a coarse-to-fine contribution network for multimodal summarization that accurately calculates and utilizes the different contributions of images in the summary. Experimental results show its superiority over other baselines."
}