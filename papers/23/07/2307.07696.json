{
    "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])",
    "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot pla",
    "link": "http://arxiv.org/abs/2307.07696",
    "context": "Title: Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])\nAbstract: While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot pla",
    "path": "papers/23/07/2307.07696.json",
    "total_tokens": 951,
    "translated_title": "将大型语言模型与逻辑编程相结合，实现文本的强大和通用推理",
    "translated_abstract": "尽管像GPT-3这样的大型语言模型在鲁棒性和通用性方面表现出色，但它们的推理能力还不能与针对特定自然语言推理问题训练的最佳模型相竞争。本研究观察到，大型语言模型可以作为一种高效的少示例语义解析器。它可以将自然语言句子转换为逻辑形式，作为答案集程序的输入，该程序是一种基于逻辑的声明性知识表示形式。这种组合结果形成了一个强大而通用的系统，可以处理多个问答任务，无需为每个新任务重新训练。它只需要少量示例来指导LLM适应特定任务，以及可应用于多个任务的可重用ASP知识模块。我们证明这种方法在几个NLP基准测试中取得了最先进的性能，包括bAbI、StepGame、CLUTRR和gSCAN。此外，它还成功解决了机器人规划问题。",
    "tldr": "这项研究将大型语言模型与逻辑编程相结合，通过将自然语言句子转换为逻辑形式，使模型能够进行强大且通用的推理。该方法只需要少量的示例和可重用的知识模块，即可在多个问答任务中取得最先进的性能。",
    "en_tdlr": "This study combines large language models with logic programming to enable robust and general reasoning from text. By converting natural language sentences into a logical form, the model achieves state-of-the-art performance on multiple question-answering tasks with only a few examples and reusable knowledge modules."
}