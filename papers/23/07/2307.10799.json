{
    "title": "Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])",
    "abstract": "Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization. There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do. In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively. Sta",
    "link": "http://arxiv.org/abs/2307.10799",
    "context": "Title: Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])\nAbstract: Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization. There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do. In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively. Sta",
    "path": "papers/23/07/2307.10799.json",
    "total_tokens": 1014,
    "translated_title": "Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])",
    "translated_abstract": "尽管序列到序列模型在广泛的应用中取得了成功，但其构建的解决方案被认为在组合泛化方面不如人类。越来越多的证据表明，阻碍组合泛化的一个原因是编码器和解码器最上层的表示被纠缠在一起。换句话说，序列的句法和语义表示被不适当地扭曲了。然而，大多数以前的研究主要集中于增强基于令牌的语义信息，以缓解表示纠缠问题，而不是像人类那样适当地组合和使用序列的句法和语义表示。此外，我们从近期关于训练更深Transformer的研究的角度解释了为什么纠缠问题存在，主要是由于“浅层”残差连接和其简单的单步操作导致无法有效地融合前面层的信息。",
    "tldr": "该论文提出了一种层级表示融合的方法，以提升序列到序列模型在组合泛化方面的表现。之前的研究主要关注增强基于令牌的语义信息，而本文提出了在人类那样适当地组合和使用序列的句法和语义表示的方法。此外，从近期的关于训练更深Transformer的研究结果来看，纠缠问题主要是由于残差连接的“浅层”和简单的单步操作导致不能有效地融合前面层的信息。",
    "en_tdlr": "The paper proposes a layer-wise representation fusion method to improve the compositional generalization of sequence-to-sequence models. Previous studies mainly focused on enhancing token-level semantic information, while this paper introduces a method to appropriately combine and use the syntactic and semantic representations of sequences, similar to how humans do. Additionally, recent research on training deeper Transformers shows that the entanglement problem is mainly due to the \"shallow\" residual connections and simple one-step operations, which fail to effectively fuse information from previous layers."
}