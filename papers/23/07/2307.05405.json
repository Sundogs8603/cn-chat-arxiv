{
    "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])",
    "abstract": "Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adap",
    "link": "http://arxiv.org/abs/2307.05405",
    "context": "Title: Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])\nAbstract: Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adap",
    "path": "papers/23/07/2307.05405.json",
    "total_tokens": 942,
    "translated_title": "提升自适应学习分数来增加交互式强化学习的反馈效率",
    "translated_abstract": "交互式强化学习在学习复杂的机器人任务方面显示出潜力。然而，由于需要大量的交互反馈，这一过程可能需要人工参与。本文提出了一种新的方法，利用人类提供的分数而不是成对偏好，来提高交互式强化学习的反馈效率。我们的关键洞察是，分数可以产生比成对偏好更多的数据。具体而言，在稀疏奖励环境中，我们要求教师与代理交互评分全面的轨迹来训练行为策略。为了避免人类给出的不稳定分数对训练过程产生负面影响，我们提出了一种自适应学习方案。这使得学习范式对于不完美或不可靠的分数不敏感。我们对机器人运动和操作任务进行了广泛评估。结果表明，所提出的方法可以通过自适应学习分数有效地学习到接近最优的策略。",
    "tldr": "本文提出一种利用人类提供的分数来改进交互式强化学习的反馈效率的方法。通过使用分数代替成对偏好，我们可以获得更多的数据。我们通过对机器人任务的评估表明，该方法能够通过自适应学习分数有效地学习到接近最优的策略。",
    "en_tdlr": "This paper presents a method that improves the feedback efficiency of interactive reinforcement learning by using scores provided by humans. By using scores instead of pairwise preferences, more data can be obtained. The proposed method has been evaluated on robotic tasks and shown to efficiently learn near-optimal policies through adaptive learning from scores."
}