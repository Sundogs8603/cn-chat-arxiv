{
    "title": "Learning Mixtures of Gaussians Using the DDPM Objective. (arXiv:2307.01178v1 [cs.DS])",
    "abstract": "Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.  In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\\text{poly}(d)$-separated centers. 2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\\Omega(\\sqrt{\\log(\\min(K,d))})$-separated centers. A key ingredient in our proofs is a new ",
    "link": "http://arxiv.org/abs/2307.01178",
    "context": "Title: Learning Mixtures of Gaussians Using the DDPM Objective. (arXiv:2307.01178v1 [cs.DS])\nAbstract: Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.  In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\\text{poly}(d)$-separated centers. 2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\\Omega(\\sqrt{\\log(\\min(K,d))})$-separated centers. A key ingredient in our proofs is a new ",
    "path": "papers/23/07/2307.01178.json",
    "total_tokens": 969,
    "translated_title": "使用DDPM目标函数学习高斯混合模型",
    "translated_abstract": "最近的研究表明，扩散模型可以学习几乎任何分布，前提是我们能够进行评分估计。然而，在什么设置下可以进行评分估计，以及何时可以实际上证明基于梯度的算法能够成功，这仍然不十分清楚。在这项工作中，我们首次在这些方面提供了可证明有效的结果，研究的是最基本的分布族之一，即高斯混合模型。我们证明了在以下两种设置下，通过梯度下降对去噪扩散概率模型（DDPM）目标进行训练可以高效地恢复混合模型的真实参数：1）我们证明了在随机初始化的情况下，梯度下降可以学习具有$d$维度和$1/\\text{poly}(d)$-分隔中心的两个球面高斯混合模型。2）我们证明了在带有热启动的情况下，梯度下降可以学习具有$\\Omega(\\sqrt{\\log(\\min(K,d))})$-分隔中心的$K$个球面高斯混合模型。我们证明的一个关键因素是一个新的...",
    "tldr": "本文针对高斯混合模型这一基础分布族提供了首个可证明高效的结果，通过梯度下降对去噪扩散概率模型（DDPM）目标进行训练可以有效地恢复混合模型的参数。",
    "en_tdlr": "This paper presents the first provably efficient results for learning Gaussian mixture models using gradient descent on the denoising diffusion probabilistic model (DDPM) objective. It shows that the ground truth parameters of the mixture model can be efficiently recovered in two different settings."
}