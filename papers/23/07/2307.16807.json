{
    "title": "On the use of associative memory in Hopfield networks designed to solve propositional satisfiability problems. (arXiv:2307.16807v2 [nlin.AO] UPDATED)",
    "abstract": "Hopfield networks are an attractive choice for solving many types of computational problems because they provide a biologically plausible mechanism. The Self-Optimization (SO) model adds to the Hopfield network by using a biologically founded Hebbian learning rule, in combination with repeated network resets to arbitrary initial states, for optimizing its own behavior towards some desirable goal state encoded in the network. In order to better understand that process, we demonstrate first that the SO model can solve concrete combinatorial problems in SAT form, using two examples of the Liars problem and the map coloring problem. In addition, we show how under some conditions critical information might get lost forever with the learned network producing seemingly optimal solutions that are in fact inappropriate for the problem it was tasked to solve. What appears to be an undesirable side-effect of the SO model, can provide insight into its process for solving intractable problems.",
    "link": "http://arxiv.org/abs/2307.16807",
    "context": "Title: On the use of associative memory in Hopfield networks designed to solve propositional satisfiability problems. (arXiv:2307.16807v2 [nlin.AO] UPDATED)\nAbstract: Hopfield networks are an attractive choice for solving many types of computational problems because they provide a biologically plausible mechanism. The Self-Optimization (SO) model adds to the Hopfield network by using a biologically founded Hebbian learning rule, in combination with repeated network resets to arbitrary initial states, for optimizing its own behavior towards some desirable goal state encoded in the network. In order to better understand that process, we demonstrate first that the SO model can solve concrete combinatorial problems in SAT form, using two examples of the Liars problem and the map coloring problem. In addition, we show how under some conditions critical information might get lost forever with the learned network producing seemingly optimal solutions that are in fact inappropriate for the problem it was tasked to solve. What appears to be an undesirable side-effect of the SO model, can provide insight into its process for solving intractable problems.",
    "path": "papers/23/07/2307.16807.json",
    "total_tokens": 971,
    "translated_title": "关于在解决命题可满足性问题的Hopfield网络中使用关联记忆的研究",
    "translated_abstract": "Hopfield网络由于提供了一种生物学上可行的机制，因此在解决许多类型的计算问题时是一个有吸引力的选择。自我优化（SO）模型通过使用基于生物学原理的赫布学习规则和重复的网络重置到任意初始状态，来优化网络行为以达到网络中编码的某个希望的目标状态。为了更好地理解该过程，我们首先证明了SO模型可以通过使用Liars问题和地图着色问题的两个例子来解决具体的组合问题。此外，我们展示了在某些条件下关键信息可能永远丢失，从而使得学习网络产生看似最优解但实际上对所要解决的问题不合适。这种SO模型的副作用看似不好，却可以对其解决难以处理的问题的过程提供洞察力。",
    "tldr": "该论文研究了在解决命题可满足性问题的Hopfield网络中使用关联记忆的方法。通过自我优化模型，网络可以解决具体的组合问题。然而，研究还发现在某些情况下，关键信息可能会永久丢失，导致网络产生看似最优但实际上不适用的解决方案。这一发现对理解网络解决难以处理问题的过程很有启发。"
}