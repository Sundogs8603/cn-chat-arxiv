{
    "title": "What can we learn from Data Leakage and Unlearning for Law?. (arXiv:2307.10476v1 [cs.CR])",
    "abstract": "Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference. A company can train an LLM on its domain-customized data which can potentially also include their users' PII. In order to comply with privacy laws such as the \"right to be forgotten\", the data points of users that are most vulnerable to extraction could be deleted. We find that once the most vulnerable points are deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to understanding memorization for fine-tuned models. In this work, we also show that not only do fine-tuned models leak their training data but they also leak the pre-training data (and PII) memorized during the pre-training phase. The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-tuned models can pos",
    "link": "http://arxiv.org/abs/2307.10476",
    "context": "Title: What can we learn from Data Leakage and Unlearning for Law?. (arXiv:2307.10476v1 [cs.CR])\nAbstract: Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference. A company can train an LLM on its domain-customized data which can potentially also include their users' PII. In order to comply with privacy laws such as the \"right to be forgotten\", the data points of users that are most vulnerable to extraction could be deleted. We find that once the most vulnerable points are deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to understanding memorization for fine-tuned models. In this work, we also show that not only do fine-tuned models leak their training data but they also leak the pre-training data (and PII) memorized during the pre-training phase. The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-tuned models can pos",
    "path": "papers/23/07/2307.10476.json",
    "total_tokens": 846,
    "translated_title": "数据泄露和遗忘对法律的启示",
    "translated_abstract": "大型语言模型存在隐私问题，会在推理过程中泄露训练数据（包括个人身份信息如电子邮件和电话号码）。为了遵守隐私法律，可以删除最容易被提取的用户数据。然而我们发现，一旦删除了最容易被提取的数据，一组新的数据点就会变得容易被提取。目前对于精调模型的记忆性尚未引起足够的关注。在这项研究中，我们还展示了精调模型不仅泄露其训练数据，而且还会泄露在预训练阶段记忆的预训练数据（以及个人身份信息）。",
    "tldr": "本研究发现，大型语言模型存在泄露训练数据的问题，删除最容易被提取的用户数据后，新的数据点仍然容易被提取。精调模型不仅泄露训练数据，还会泄露预训练阶段记忆的预训练数据和个人身份信息。"
}