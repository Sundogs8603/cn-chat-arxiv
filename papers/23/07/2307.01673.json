{
    "title": "Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])",
    "abstract": "Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional",
    "link": "http://arxiv.org/abs/2307.01673",
    "context": "Title: Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])\nAbstract: Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional",
    "path": "papers/23/07/2307.01673.json",
    "total_tokens": 990,
    "translated_title": "无条件语音合成中的生成对抗网络中的解耦技术",
    "translated_abstract": "我们是否能够开发一个模型，能够直接从潜在空间合成逼真的语音，而无需显式条件？尽管过去十年中进行了几次尝试，之前的对抗性和扩散性方法仍然难以实现，即使在小字典数据集上也是如此。为了解决这个问题，我们提出了AudioStyleGAN(ASGAN)——一种用于无条件语音合成的生成对抗网络，旨在学习一个解耦潜在空间。在StyleGAN系列图像合成模型的基础上构建ASGAN，它将采样噪声映射到一个解耦潜在向量，然后将其映射到一个音频特征序列，以在每一层中抑制信号混叠。为了成功训练ASGAN，我们引入了一些新技术，包括对自适应鉴别器增强的修改，使其以概率方式跳过鉴别器更新。我们将其应用于小字典的谷歌语音命令数字数据集上，在该数据集上实现了无条件语音合成方面的最新成果。",
    "tldr": "在无条件语音合成中，我们提出了一种名为ASGAN的解耦生成对抗网络模型，该模型借鉴了StyleGAN图像合成模型，并引入了一些新技术。通过学习解耦的潜在空间，ASGAN能够从潜在空间中合成逼真的语音，即使在小字典数据集上也能取得最新成果。",
    "en_tdlr": "In unconditional speech synthesis, we propose a disentangled generative adversarial network called ASGAN, which builds upon the StyleGAN image synthesis model and introduces new techniques. By learning a disentangled latent space, ASGAN is able to synthesize realistic speech directly from a latent space, even on small-vocabulary datasets, achieving state-of-the-art results."
}