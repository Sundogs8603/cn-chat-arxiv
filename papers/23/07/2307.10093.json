{
    "title": "Revisiting invariances and introducing priors in Gromov-Wasserstein distances. (arXiv:2307.10093v1 [cs.LG])",
    "abstract": "Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.",
    "link": "http://arxiv.org/abs/2307.10093",
    "context": "Title: Revisiting invariances and introducing priors in Gromov-Wasserstein distances. (arXiv:2307.10093v1 [cs.LG])\nAbstract: Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.",
    "path": "papers/23/07/2307.10093.json",
    "total_tokens": 928,
    "translated_title": "重新审视不变性并引入先验知识在Gromov-Wasserstein距离中",
    "translated_abstract": "由于其能够比较度量空间中的测度并且对等度变换具有不变性，Gromov-Wasserstein距离在机器学习中有很多应用。然而，在某些应用中，这种不变性可能过于灵活而不可取。此外，Gromov-Wasserstein距离仅考虑输入数据集中的成对样本相似性，而忽略原始特征表示。我们提出了一种新的基于最优传输的距离，称为增强的Gromov-Wasserstein，它允许对变换的刚度有一定控制。它还结合了特征对齐，使我们能够更好地利用输入数据上的先验知识以提高性能。我们提出了对所提出的度量的理论洞察力。然后，我们展示了它在单细胞多组学对齐任务和机器学习中的迁移学习场景中的实用性。",
    "tldr": "本文提出了一种新的基于最优传输的距离，增强的Gromov-Wasserstein，它在Gromov-Wasserstein距离的基础上引入了对变换刚度的控制和特征对齐，并应用于单细胞多组学和迁移学习任务中，展示了其在机器学习中的实用性和改进性能。"
}