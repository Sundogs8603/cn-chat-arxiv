{
    "title": "Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])",
    "abstract": "Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.",
    "link": "http://arxiv.org/abs/2307.11288",
    "context": "Title: Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])\nAbstract: Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.",
    "path": "papers/23/07/2307.11288.json",
    "total_tokens": 784,
    "translated_title": "基于内核的离线背景双向竞标者算法",
    "translated_abstract": "基于偏好的反馈在许多应用中非常重要，这些应用中无法直接评估奖励函数。在人们对大型语言模型的强化学习中，这是一个显著的最新实例。对于许多这些应用，获取人类反馈的成本可能相当高甚至不可行。在这项工作中，我们利用一个事实，即代理通常可以选择获得人类反馈的上下文，以最高效地确定一个良好策略，并引入了离线背景双向竞标者设置。我们为这个设置提供了一个上界置信区间样式的算法，并证明了一个遗憾上界。我们还通过经验证实这种方法胜过使用均匀采样上下文的类似策略。",
    "tldr": "本论文提出了基于内核的离线背景双向竞标者算法，用于解决基于偏好反馈的问题，并证明了算法的遗憾界。通过实验证明该方法优于使用均匀采样上下文的相似策略。",
    "en_tdlr": "This paper proposes a kernelized offline contextual dueling bandit algorithm for preference-based feedback and proves a regret bound for the algorithm. Empirical results confirm that this method outperforms a similar strategy that uses uniformly sampled contexts."
}