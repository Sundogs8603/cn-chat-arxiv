{
    "title": "Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])",
    "abstract": "Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.",
    "link": "http://arxiv.org/abs/2307.07380",
    "context": "Title: Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])\nAbstract: Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.",
    "path": "papers/23/07/2307.07380.json",
    "total_tokens": 843,
    "translated_title": "句子嵌入的构成对比学习",
    "translated_abstract": "自然语言的向量表示在搜索应用中非常普遍。最近，提出了基于对比学习的各种方法，用于从无标签数据中学习文本表示；通过最大化相同文本的微小扰动嵌入之间的对齐，并鼓励嵌入在更广泛的语料库中的均匀分布。与此不同的是，我们提出了最大化文本和其词组成分的对齐。我们考虑了这一目标的几个实现方式，并详细说明了每种情况下对表示的影响。在语义文本相似性任务上的实验结果显示，与最先进的方法相比，我们的方法在提高基线水平方面有所改进。此外，这项工作是第一个在不产生辅助训练目标或额外网络参数成本的情况下取得这样的改进。",
    "tldr": "该论文提出了一种句子嵌入的构成对比学习方法，通过最大化文本和其词组成分的对齐，实现了从无标签数据中学习文本表示的目标，并在语义文本相似性任务上取得了与最先进方法可比较的改进，而无需额外的训练目标或网络参数。",
    "en_tdlr": "This paper introduces a composition-contrastive learning method for sentence embeddings, which maximizes alignment between texts and their phrasal constituents, realizing the goal of learning textual representations from unlabelled data. It achieves comparable improvements in semantic textual similarity tasks without incurring costs in auxiliary training objectives or additional network parameters."
}