{
    "title": "Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])",
    "abstract": "The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned \"time-frequency\" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that impro",
    "link": "http://arxiv.org/abs/2307.07325",
    "context": "Title: Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])\nAbstract: The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned \"time-frequency\" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that impro",
    "path": "papers/23/07/2307.07325.json",
    "total_tokens": 896,
    "translated_title": "无资源语音应用的隐藏单元聚类表示学习",
    "translated_abstract": "无文本资源下的语音表示学习是许多低资源语音应用中一个非常感兴趣的领域。本文介绍了一种使用隐藏单元聚类（HUC）框架从原始音频中进行自监督表示学习的方法。模型的输入包括被窗口化并经过1-D卷积层处理的音频样本。从卷积神经网络（CNN）模块学习到的\"时频\"表示经过长短期记忆（LSTM）层进一步处理，为每个窗口化片段生成上下文向量表示。使用隐藏单元聚类框架对这些表示进行分类，将其归类为少量类似音素的单元，用于训练模型学习语义丰富的语音表示。目标包括每个音频片段的类似音素伪标签，这些标签是使用迭代k-means算法生成的。我们探索了一些可以提高模型性能的技术。",
    "tldr": "本文提出了一种使用隐藏单元聚类框架进行自监督表示学习的方法，用于无文本资源下的语音表示学习。模型使用卷积神经网络和长短期记忆层对音频样本进行处理并生成上下文向量表示，通过隐藏单元聚类框架将这些表示归类为少量类似音素的单元，用于学习语义丰富的语音表示。研究结果表明这种方法可以提高模型性能。"
}