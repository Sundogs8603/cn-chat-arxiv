{
    "title": "Exploring Transformer Extrapolation. (arXiv:2307.10156v1 [cs.CL])",
    "abstract": "Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on",
    "link": "http://arxiv.org/abs/2307.10156",
    "context": "Title: Exploring Transformer Extrapolation. (arXiv:2307.10156v1 [cs.CL])\nAbstract: Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on",
    "path": "papers/23/07/2307.10156.json",
    "total_tokens": 876,
    "translated_title": "探索Transformer外推",
    "translated_abstract": "长度外推近期引起了相当大的关注，因为它允许transformers在训练中使用的序列长度之外进行测试。先前的研究表明，通过使用精心设计的相对位置编码(RPEs)可以实现这一属性。虽然这些方法在各种文集上表现良好，但对于长度外推的条件尚未得到研究。本文试图通过彻底的数学和实证分析确定哪种类型的RPEs可以实现长度外推。我们发现只要对应于RPE的指数收敛的序列，transformer一定具有这个属性。从这些条件中导出了两种实践方法，并在各种文集上进行了语言建模任务的研究。作为条件衍生的额外好处，我们推导出了一种新的理论感受野(TRF)，可以在不进行任何训练步骤的情况下测量RPE的感受野。进行了大量的实验。",
    "tldr": "本文通过数学和实证分析，发现只要RPE的指数序列收敛，Transformer就具有长度外推的能力。从中导出了两种实践方法，并提出了一种新的理论感受野(TRF)来测量RPE的感受野。"
}