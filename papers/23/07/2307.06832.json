{
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring. (arXiv:2307.06832v1 [eess.AS])",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
    "link": "http://arxiv.org/abs/2307.06832",
    "context": "Title: Personalization for BERT-based Discriminative Speech Recognition Rescoring. (arXiv:2307.06832v1 [eess.AS])\nAbstract: Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
    "path": "papers/23/07/2307.06832.json",
    "total_tokens": 961,
    "translated_title": "基于BERT的区分性语音识别候选修正的个性化方法",
    "translated_abstract": "在端到端语音识别中，个性化内容的识别仍然是一个挑战。我们探索了三种利用个性化内容在神经修正步骤中提高识别准确性的新方法：词表、提示和基于交叉注意力的编码器-解码器模型。我们使用内部的去标识化英语(美国)交互数据，补充个性化命名实体，以比较这些方法。在一个包含个性化命名实体的测试集上，我们展示了每种方法相对于神经修正基线可以将词错误率提高超过10%。我们还展示了在这个测试集上，自然语言提示可以提高词错误率7%，而不需要任何训练，并且在泛化方面只有微小的损失。总体而言，词表的表现最好，词错误率提高了10%，在一个通用测试集上还提高了1%。",
    "tldr": "本论文提出了三种利用个性化内容提高端到端语音识别准确性的方法：词表、提示和基于交叉注意力的编码器-解码器模型。实验证明，这些方法都显著提高了词错误率，并且自然语言提示不需要训练即可改善准确率。其中，词表表现最佳，提高了10%的词错误率，同时在通用测试集上也有1%的提升。",
    "en_tdlr": "This paper proposes three methods for improving end-to-end speech recognition accuracy using personalized content: gazetteers, prompting, and a cross-attention based encoder-decoder model. The experiments show that these methods significantly improve word error rate, and natural language prompts can improve accuracy without requiring training. Among them, gazetteers perform the best with a 10% improvement in word error rate and a 1% improvement on a general test set."
}