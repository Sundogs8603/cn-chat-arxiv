{
    "title": "Towards Cross-Table Masked Pretraining for Web Data Mining",
    "abstract": "Tabular data pervades the landscape of the World Wide Web, playing a foundational role in the digital architecture that underpins online information. Given the recent influence of large-scale pretrained models like ChatGPT and SAM across various domains, exploring the application of pretraining techniques for mining tabular data on the web has emerged as a highly promising research direction. Indeed, there have been some recent works around this topic where most (if not all) of them are limited in the scope of a fixed-schema/single table. Due to the scale of the dataset and the parameter size of the prior models, we believe that we have not reached the ''BERT moment'' for the ubiquitous tabular data. The development on this line significantly lags behind the counterpart research domains such as natural language processing. In this work, we first identify the crucial challenges behind tabular data pretraining, particularly overcoming the cross-table hurdle. As a pioneering endeavor, thi",
    "link": "https://arxiv.org/abs/2307.04308",
    "context": "Title: Towards Cross-Table Masked Pretraining for Web Data Mining\nAbstract: Tabular data pervades the landscape of the World Wide Web, playing a foundational role in the digital architecture that underpins online information. Given the recent influence of large-scale pretrained models like ChatGPT and SAM across various domains, exploring the application of pretraining techniques for mining tabular data on the web has emerged as a highly promising research direction. Indeed, there have been some recent works around this topic where most (if not all) of them are limited in the scope of a fixed-schema/single table. Due to the scale of the dataset and the parameter size of the prior models, we believe that we have not reached the ''BERT moment'' for the ubiquitous tabular data. The development on this line significantly lags behind the counterpart research domains such as natural language processing. In this work, we first identify the crucial challenges behind tabular data pretraining, particularly overcoming the cross-table hurdle. As a pioneering endeavor, thi",
    "path": "papers/23/07/2307.04308.json",
    "total_tokens": 892,
    "translated_title": "面向网络数据挖掘的跨表掩码预训练研究",
    "translated_abstract": "表格数据在全球网络中广泛存在，在支撑在线信息的数字架构中起着基础性作用。鉴于ChatGPT和SAM等大规模预训练模型在各个领域的影响力，探索将预训练技术应用于网络表格数据挖掘已成为一个极具潜力的研究方向。尽管最近有一些关于这个主题的研究，但大多数（如果不是全部）都局限于固定模式/单表的范围。由于数据集的规模和先前模型的参数大小，我们认为在普遍存在的表格数据中还没有达到“BERT时刻”。这一领域的发展明显滞后于自然语言处理等相应研究领域。在这项工作中，我们首先确定了表格数据预训练背后的关键挑战，特别是克服跨表障碍。作为一项开创性的努力，本研究展示了通过跨表预训练来挖掘网络表格数据的潜力。",
    "tldr": "本论文旨在研究面向网络数据挖掘的跨表掩码预训练方法，解决了表格数据预训练中的关键挑战，显示出对挖掘网络表格数据的潜力。"
}