{
    "title": "Cross-Modal Concept Learning and Inference for Vision-Language Models. (arXiv:2307.15460v1 [cs.CV])",
    "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of image",
    "link": "http://arxiv.org/abs/2307.15460",
    "context": "Title: Cross-Modal Concept Learning and Inference for Vision-Language Models. (arXiv:2307.15460v1 [cs.CV])\nAbstract: Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of image",
    "path": "papers/23/07/2307.15460.json",
    "total_tokens": 903,
    "translated_title": "跨模态概念学习和推理用于视觉-语言模型",
    "translated_abstract": "大规模预训练的视觉-语言模型（VLMs），如CLIP，在各种下游任务中取得了显著的成功，通过微调建立了文本和图像之间的相关性。在现有的微调方法中，将特定类别的文本描述与整个图像进行匹配。我们认识到整个图像匹配并不有效，因为同一类别的图像通常包含一组不同的语义对象，而对象又包含一组语义部分或概念。个别的语义部分或概念可能出现在不同类别的图像样本中。为了解决这个问题，在本文中，我们开发了一种新的方法，称为跨模态概念学习和推理（CCLI）。利用CLIP的强大的文本-图像相关性能力，我们的方法使用一组语义文本概念从图像中自动学习一组独特的视觉概念。基于这些视觉概念，我们构建了图像的判别式表示。",
    "tldr": "本论文提出了一种跨模态概念学习和推理（CCLI）方法，通过利用CLIP的文本-图像相关性能力，从图像中自动学习一组独特的视觉概念，并利用这些概念构建了图像的判别式表示。"
}