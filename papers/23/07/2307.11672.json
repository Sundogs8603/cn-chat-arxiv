{
    "title": "Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])",
    "abstract": "Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, ",
    "link": "http://arxiv.org/abs/2307.11672",
    "context": "Title: Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])\nAbstract: Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, ",
    "path": "papers/23/07/2307.11672.json",
    "total_tokens": 886,
    "translated_title": "快速自适应测试防御与稳健特征",
    "translated_abstract": "自适应的测试防御被用来提高深度神经网络对抗性样本的鲁棒性。然而，现有方法由于对模型参数或输入进行额外的优化导致推理时间大幅增加。在本工作中，我们提出了一种新颖的自适应测试防御策略，它可以与任何现有（稳健的）训练过程轻松集成，并且无需额外的测试时间计算。基于我们提出的特征鲁棒性的概念，关键思想是将训练好的模型投影到最稳健的特征空间，从而降低对非稳健方向的对抗攻击的脆弱性。我们在广义可加性模型和使用神经切向核函数（NTK）等价法证明了特征矩阵的顶层特征空间更加稳健。我们在CIFAR-10和CIFAR-100数据集上进行了大量实验，用于几个稳健性基准测试。",
    "tldr": "本文提出了一种快速适应的测试防御策略，通过投影训练好的模型到最稳健的特征空间，降低了对抗攻击的脆弱性，无需额外的测试时间计算。"
}