{
    "title": "ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])",
    "abstract": "The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \\textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \\textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.",
    "link": "http://arxiv.org/abs/2307.01387",
    "context": "Title: ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])\nAbstract: The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \\textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \\textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.",
    "path": "papers/23/07/2307.01387.json",
    "total_tokens": 968,
    "translated_title": "ALBERTI,一种用于诗歌分析的多语言领域特定语言模型.",
    "translated_abstract": "诗歌的计算分析受到自动分析和扫描工具的稀缺性的限制。在多语言环境中，由于仅存在单个语言的扫描和韵律系统，因此比较性研究非常具有挑战性和耗时。在这项工作中，我们提出了\\textsc{Alberti}，这是第一个用于诗歌的多语言预训练大型语言模型。通过领域特定的预训练(DSP)，我们在12种语言的1,200万行诗歌语料库上对多语言BERT进行了进一步训练。我们在两个结构性诗歌任务上评估了其性能: 西班牙诗歌篇章类型分类以及西班牙语、英语和德语的韵律模式预测。在这两种情况下，\\textsc{Alberti}优于多语言BERT和其他相似规模的基于转换器的模型，甚至在与基于规则的系统进行比较时，对于德语实现了最新的成果，展示了领域特定语言模型在诗歌领域的可行性和有效性。",
    "tldr": "本研究提出了一种名为ALBERTI的多语言预训练语言模型，通过领域特定预训练，在12种语言的1200万行诗歌上进行训练。在结构性诗歌任务上的性能评估显示，ALBERTI优于多语言BERT和其他类似模型，并且在德语任务上实现了最新的成果，证明了领域特定语言模型在诗歌领域的可行性和有效性。"
}