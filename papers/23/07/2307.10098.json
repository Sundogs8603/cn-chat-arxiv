{
    "title": "Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])",
    "abstract": "Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the b",
    "link": "http://arxiv.org/abs/2307.10098",
    "context": "Title: Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])\nAbstract: Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the b",
    "path": "papers/23/07/2307.10098.json",
    "total_tokens": 894,
    "translated_title": "渐进稀疏化用于Transformer模型的遮罩微调",
    "translated_abstract": "预训练的自监督语言模型的微调被广泛应用于向下游任务的迁移学习。微调可通过冻结预训练网络的梯度并只更新新添加的分类层的梯度，或通过对所有参数进行梯度更新来实现。渐进解冻在训练过程中逐渐解冻整个层的梯度，以在存储和训练速度与泛化性能之间进行权衡，这是一种有效的策略。然而，目前还不清楚渐进解冻整个训练是否是最优选择，相比之下，稀疏变体的渐进解冻可能可以提高微调性能。在本文中，我们提出了随机屏蔽梯度来正则化预训练语言模型，从而改善整体微调性能。我们介绍了GradDrop及其变体，一类梯度稀疏化方法，在训练过程中对梯度进行屏蔽。",
    "tldr": "本研究提出了一种使用渐进稀疏化方法对预训练语言模型进行正则化，以改善微调性能。GradDrop及其变体通过在训练过程中随机屏蔽梯度，有效地进行梯度稀疏化。"
}