{
    "title": "Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])",
    "abstract": "In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text",
    "link": "http://arxiv.org/abs/2307.07312",
    "context": "Title: Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])\nAbstract: In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text",
    "path": "papers/23/07/2307.07312.json",
    "total_tokens": 897,
    "translated_title": "使用大型语言模型从知识图生成零-shot自然语言生成",
    "translated_abstract": "在任何使用结构化知识图（KG）数据作为其底层知识表示的系统中，KG到文本生成是将图数据的部分转化为人类可理解的文本的有用工具。最近的工作表明，使用大量文本数据进行预训练的模型即使在特定图到文本任务的相对小的训练集上也能表现出良好的性能。在本文中，我们在这个概念的基础上利用大型语言模型执行零-shot生成，仅仅根据模型对三元组结构的理解进行生成。我们展示了ChatGPT在WebNLG 2020挑战赛的某些指标上实现了接近最新技术水平的性能，但在其他指标上落后。此外，我们比较了事实、反事实和虚构陈述，并展示了LLM已经对其解析的数据有关的知识与输出文本质量之间的显著关联。",
    "tldr": "本论文通过使用大型语言模型，实现了基于图数据的零-shot自然语言生成。实验结果表明，该方法在部分指标上接近最新技术水平，在事实、反事实和虚构陈述的对比中也有显著的关联。"
}