{
    "title": "A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v1 [cs.CL])",
    "abstract": "Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across $2$ biomedical NLP tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs",
    "link": "http://arxiv.org/abs/2307.11254",
    "context": "Title: A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v1 [cs.CL])\nAbstract: Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across $2$ biomedical NLP tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs",
    "path": "papers/23/07/2307.11254.json",
    "total_tokens": 985,
    "translated_title": "对生物医学自然语言处理中的联邦学习进行系统评估",
    "translated_abstract": "语言模型（LM）如BERT和GPT已经改变了自然语言处理（NLP）。然而，隐私敏感的领域，特别是医疗领域，由于有限的数据访问和由《健康保险便携性和责任法案》（HIPPA）和《通用数据保护条例》（GDPR）等法规的隐私约束，面临着训练LM的挑战。联邦学习（FL）提供了一种分散的解决方案，既能够实现协同学习，又能够确保数据隐私的保护。在本研究中，我们对医学中的FL进行了系统评估，涵盖了六个生物医学NLP任务，使用了八个语料库和六个LM。我们的结果表明：1）FL模型始终优于单个客户端数据训练的LM，并且有时能够与使用汇总数据训练的模型匹配；2）在总数据量固定的情况下，使用更多客户端进行FL训练的LM表现出较差的性能，但基于预训练的转换器模型表现出更强的鲁棒性；3）LM们",
    "tldr": "本研究对医学领域中的联邦学习在生物医学自然语言处理中的应用进行了系统评估，结果显示联邦学习模型优于单独训练的模型，并且在考虑数据隐私的情况下仍能取得良好的效果。",
    "en_tdlr": "This study systematically evaluates the application of federated learning in biomedical natural language processing in the medical field, showing that federated learning models outperform individually trained models and achieve good results while considering data privacy."
}