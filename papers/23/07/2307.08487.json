{
    "title": "Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models. (arXiv:2307.08487v2 [cs.CL] UPDATED)",
    "abstract": "Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation fram",
    "link": "http://arxiv.org/abs/2307.08487",
    "context": "Title: Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models. (arXiv:2307.08487v2 [cs.CL] UPDATED)\nAbstract: Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation fram",
    "path": "papers/23/07/2307.08487.json",
    "total_tokens": 973,
    "translated_title": "潜在越狱：评估大型语言模型的文本安全性和输出鲁棒性的测试套件",
    "translated_abstract": "已经有大量的研究致力于确保大型语言模型（LLMs）与人类价值观相一致并生成安全文本。然而，对某些主题的过度关注可能会损害模型在遵循指令方面的鲁棒性，从而影响其在完成任务方面的整体表现。以往用于越狱LLMs的基准主要关注评估模型的安全性，而没有考虑其鲁棒性。在本文中，我们提出了一个评估LLMs安全性和鲁棒性的基准，强调需要一个平衡的方法。为了全面研究文本安全性和输出鲁棒性，我们引入了一个潜在越狱提示数据集，每个数据集中都包含恶意指令嵌入。具体而言，我们指导模型完成常规任务，例如翻译，其中待翻译的文本包含恶意指令。为了进一步分析安全性和鲁棒性，我们设计了一个分层注释框架。",
    "tldr": "这篇论文提出了一个评估大型语言模型安全性和鲁棒性的基准测试套件，强调了平衡的方法。通过引入含有恶意指令的潜在越狱提示数据集，并设计分层注释框架，全面研究了文本安全性和输出鲁棒性。"
}