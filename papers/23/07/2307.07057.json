{
    "title": "Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])",
    "abstract": "We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as",
    "link": "http://arxiv.org/abs/2307.07057",
    "context": "Title: Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])\nAbstract: We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as",
    "path": "papers/23/07/2307.07057.json",
    "total_tokens": 988,
    "translated_title": "利用预训练的ASR编码器实现高效且有效的端到端语音意图分类和槽位填充",
    "translated_abstract": "本文研究了语音意图分类和槽位填充（SICSF），提出使用在语音识别（ASR）上预训练的编码器来初始化端到端（E2E）Conformer-Transformer模型。我们在SLURP数据集上实现了新的最优结果，意图准确率达到90.14%，SLURP-F1为82.27%。我们将我们的模型与自监督学习（SSL）预训练的编码器进行比较，并表明相对于SSL，ASR预训练在SICSF上更加有效。为了探索参数效率，我们冻结了编码器并添加了Adapter模块，结果表明只有使用ASR预训练的编码器才能实现参数效率，而SSL编码器需要完全微调才能达到可比较的结果。此外，我们对端到端模型与级联模型（ASR+NLU）进行了深入比较，并表明除非提供了oracle ASR模型，否则端到端模型优于级联模型。最后，我们的模型是第一个能够达到与预先训练的ASR模型相同性能的端到端模型。",
    "tldr": "本文通过利用预训练的ASR编码器来初始化一个端到端模型，在语音意图分类和槽位填充任务上取得了新的最优结果。与自监督学习相比，使用ASR预训练的编码器在效果上更好，并且能够实现参数效率。此外，与级联模型相比，端到端模型在大部分情况下都更好，除非提供了oracle ASR模型。"
}