{
    "title": "Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training. (arXiv:2307.04042v1 [stat.ML])",
    "abstract": "We show the sup-norm convergence of deep neural network estimators with a novel adversarial training scheme. For the nonparametric regression problem, it has been shown that an estimator using deep neural networks can achieve better performances in the sense of the $L2$-norm. In contrast, it is difficult for the neural estimator with least-squares to achieve the sup-norm convergence, due to the deep structure of neural network models. In this study, we develop an adversarial training scheme and investigate the sup-norm convergence of deep neural network estimators. First, we find that ordinary adversarial training makes neural estimators inconsistent. Second, we show that a deep neural network estimator achieves the optimal rate in the sup-norm sense by the proposed adversarial training with correction. We extend our adversarial training to general setups of a loss function and a data-generating function. Our experiments support the theoretical findings.",
    "link": "http://arxiv.org/abs/2307.04042",
    "context": "Title: Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training. (arXiv:2307.04042v1 [stat.ML])\nAbstract: We show the sup-norm convergence of deep neural network estimators with a novel adversarial training scheme. For the nonparametric regression problem, it has been shown that an estimator using deep neural networks can achieve better performances in the sense of the $L2$-norm. In contrast, it is difficult for the neural estimator with least-squares to achieve the sup-norm convergence, due to the deep structure of neural network models. In this study, we develop an adversarial training scheme and investigate the sup-norm convergence of deep neural network estimators. First, we find that ordinary adversarial training makes neural estimators inconsistent. Second, we show that a deep neural network estimator achieves the optimal rate in the sup-norm sense by the proposed adversarial training with correction. We extend our adversarial training to general setups of a loss function and a data-generating function. Our experiments support the theoretical findings.",
    "path": "papers/23/07/2307.04042.json",
    "total_tokens": 933,
    "translated_title": "使用对抗训练的深度神经网络估计器在非参数回归中的超范数收敛性",
    "translated_abstract": "我们展示了使用一种新颖的对抗训练方案的深度神经网络估计器的超范数收敛性。针对非参数回归问题，已经证明使用深度神经网络的估计器在$L2$-范数意义下可以获得更好的性能。相比之下，由于神经网络模型的深度结构，使用最小二乘法的神经估计器很难达到超范数收敛。在本研究中，我们发展了一种对抗训练方案，并研究了深度神经网络估计器的超范数收敛性。首先，我们发现普通的对抗训练使得神经估计器不一致。其次，我们展示了通过所提出的带修正的对抗训练，深度神经网络估计器在超范数意义下达到最优速率。我们将我们的对抗训练扩展到了一般的损失函数和数据生成函数的设置。我们的实验支持了理论发现。",
    "tldr": "我们展示了使用对抗训练的深度神经网络估计器在非参数回归中的超范数收敛性。我们发现普通的对抗训练使得神经估计器不一致，但通过所提出的带修正的对抗训练，深度神经网络估计器在超范数意义下达到最优速率。我们的实验证实了这些理论发现。"
}