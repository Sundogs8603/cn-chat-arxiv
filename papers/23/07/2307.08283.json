{
    "title": "Complexity Matters: Rethinking the Latent Space for Generative Modeling. (arXiv:2307.08283v1 [cs.LG])",
    "abstract": "In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel \"distance\" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propo",
    "link": "http://arxiv.org/abs/2307.08283",
    "context": "Title: Complexity Matters: Rethinking the Latent Space for Generative Modeling. (arXiv:2307.08283v1 [cs.LG])\nAbstract: In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel \"distance\" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propo",
    "path": "papers/23/07/2307.08283.json",
    "total_tokens": 891,
    "translated_title": "复杂性至关重要：重新思考生成建模的潜在空间",
    "translated_abstract": "在生成建模中，许多成功的方法利用低维潜在空间，例如，稳定扩散模型通过编码器引导的潜在空间生成图像，并通过配对的解码器进行生成。尽管潜在空间的选择在实践中非常重要，但确定最优选择和识别过程仍不清楚。在本研究中，我们旨在从模型复杂性的角度重新思考潜在空间，来揭示这个未被充分探索的话题。我们的调查从经典的生成对抗网络（GANs）开始。受到GAN训练目标的启发，我们提出了一种新的潜在与数据分布之间的“距离”，其最小化与生成器的复杂性最小化相一致。这个距离的最小化者被描述为能够最有效地利用生成器容量的最佳数据相关的潜在。然后，我们考虑通过编码器网络对这样的潜在分布进行参数化，并提出了一个方法...",
    "tldr": "本研究从模型复杂性的角度重新思考生成建模的潜在空间，提出了一种新的潜在与数据分布之间的“距离”，并通过该距离的最小化来优化生成器的复杂性。",
    "en_tdlr": "This research rethinks the latent space for generative modeling from the perspective of model complexity, proposing a novel \"distance\" between the latent and data distributions and optimizing the generator complexity by minimizing this distance."
}