{
    "title": "Multi-Grained Multimodal Interaction Network for Entity Linking. (arXiv:2307.09721v1 [cs.AI])",
    "abstract": "Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction u",
    "link": "http://arxiv.org/abs/2307.09721",
    "context": "Title: Multi-Grained Multimodal Interaction Network for Entity Linking. (arXiv:2307.09721v1 [cs.AI])\nAbstract: Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction u",
    "path": "papers/23/07/2307.09721.json",
    "total_tokens": 909,
    "translated_title": "多粒度多模态交互网络用于实体链接",
    "translated_abstract": "近年来，多模态实体链接（MEL）任务在解决多模态知识图谱中的含糊提及方面吸引了广泛关注。尽管已经做出了大量努力来探索多个模态之间的互补效应，但它们可能未能充分吸收简写文本上下文和隐含视觉指示的综合表达。更糟糕的是，不可避免的噪声数据可能导致学习过程中不同模态的一致性不足，严重降低性能。为了解决以上问题，本文提出了一个新颖的多粒度多模态交互网络(MIMIC)框架用于解决MEL任务。具体而言，首先将提及和实体的统一输入由文本/视觉编码器分别进行编码，以提取全局描述特征和局部详细特征。然后，为每个提及-实体对派生相似性匹配分数，我们设计了三种交互方法。",
    "tldr": "这篇论文提出了一个名为MIMIC的多粒度多模态交互网络框架，用于解决多模态实体链接任务。该框架解决了多模态之间信息互补不充分和噪声数据导致的性能下降问题。"
}