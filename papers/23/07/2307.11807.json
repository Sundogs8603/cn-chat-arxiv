{
    "title": "Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks. (arXiv:2307.11807v1 [cs.LG])",
    "abstract": "Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show t",
    "link": "http://arxiv.org/abs/2307.11807",
    "context": "Title: Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks. (arXiv:2307.11807v1 [cs.LG])\nAbstract: Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show t",
    "path": "papers/23/07/2307.11807.json",
    "total_tokens": 861,
    "translated_title": "以本地核归一化为机制的过度参数化卷积神经网络中的特征学习",
    "translated_abstract": "特征学习是深度神经网络自动从原始数据中学习相关特征的能力，是其解决复杂任务的优秀能力的基础。然而，在全连接或卷积架构中，特征学习似乎以不同的方式实现。经验证据显示，无限宽度极限下的全连接神经网络最终优于有限宽度的对应网络。由于描述无限宽度网络的核在训练过程中不会改变，所以在深度全连接架构中出现的任何形式的特征学习对于改善泛化没有太大帮助。另一方面，具有卷积层的最先进架构在有限宽度范围内实现了最佳性能，这表明在这种情况下出现了一种有效的特征学习形式。在这项工作中，我们提出了一个简单的理论框架，以解释这些差异，以及在一个隐藏层网络中的理由。",
    "tldr": "本文提出了一种以本地核归一化为机制的理论框架，解释了全连接和卷积神经网络在特征学习方面的差异。",
    "en_tdlr": "This paper presents a theoretical framework based on local kernel renormalization to explain the differences in feature learning between fully-connected and convolutional neural networks."
}