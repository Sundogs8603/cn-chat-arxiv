{
    "title": "Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])",
    "abstract": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score ",
    "link": "http://arxiv.org/abs/2307.06908",
    "context": "Title: Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])\nAbstract: Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score ",
    "path": "papers/23/07/2307.06908.json",
    "total_tokens": 1093,
    "translated_title": "生成用于语言模型事实性评估的基准数据集",
    "translated_abstract": "在将语言模型（LM）部署到特定领域之前，衡量其在该领域中生成事实错误信息的倾向很重要。现有的事实生成评估方法集中于从LM自身中采样的事实，因此无法控制评估事实的集合，并且可能低估了罕见和不太可能的事实。我们提出了FACTOR：通过语料库变换进行事实评估的方法，这是一种可扩展的方法来评估LM的事实性。FACTOR会自动将感兴趣的事实语料库转化为一个基准数据集，评估LM根据语料库生成真实事实的倾向与生成类似但不正确的陈述的能力。我们使用我们的框架创建了两个基准数据集：Wiki-FACTOR和News-FACTOR。我们的实验结果表明：（i）我们的基准数据集分数随模型大小增加而增加，并且当LM与检索方法结合使用时，性能得到改善；（ii）基准数据集分数与困惑度之间存在相关性，但这两个指标在模型排序上并不总是一致；以及（iii）当困惑度和基准数据集分数发生冲突时，基准数据集分数更能准确反映LM的事实性能。",
    "tldr": "该论文提出了一个名为FACTOR的方法，用于生成用于语言模型事实性评估的基准数据集。通过自动转换事实语料库，评估语言模型根据语料库生成真实事实的倾向与生成不正确陈述的能力。实验结果表明，该基准数据集的分数随模型大小增加而增加，在LM与检索方法结合时性能得到改善。困惑度和基准数据集分数之间存在相关性，但不总是一致。"
}