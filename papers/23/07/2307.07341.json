{
    "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting. (arXiv:2307.07341v1 [cs.IR])",
    "abstract": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.  To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN",
    "link": "http://arxiv.org/abs/2307.07341",
    "context": "Title: PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting. (arXiv:2307.07341v1 [cs.IR])\nAbstract: Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.  To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN",
    "path": "papers/23/07/2307.07341.json",
    "total_tokens": 999,
    "translated_title": "PiTL: 通过提示进行弱监督视觉-语言预训练进行跨模态检索",
    "translated_abstract": "视觉-语言预训练（VLP）已经证明可以很好地在各种视觉-语言下游任务中推广VLP模型，特别是用于跨模态检索。然而，它依赖于大量的图像-文本对，这需要繁琐和昂贵的策划。相反，弱监督VLP（W-VLP）利用预训练的对象检测器（OD）从图像中生成的对象标签来探索方法。然而，它们仍然需要配对的信息，即图像和对象级注释，作为训练OD的监督。为了进一步减少监督的数量，我们提出了在大语言模型（LLM）中提示知识来描述图像的\"PiTL\"。具体来说，给定一个图像的类别标签，例如炼油厂，由LLM提取的知识，例如炼油厂可以带有大型储罐、管道等，用作语言方面的对应物。这些知识补充了最有可能出现在场景中的实体之间的共同关系。",
    "tldr": "本文提出了一个名为PiTL的方法，通过从大语言模型中提取的知识来描述图像，以实现弱监督视觉-语言预训练进行跨模态检索。这种方法可以减少对图像-文本对的需求，并且可以使用对象检测器生成的对象标签作为监督。",
    "en_tdlr": "This paper proposes a method called PiTL, which utilizes knowledge extracted from large language models (LLMs) to describe images in order to achieve weakly-supervised vision-language pre-training for cross-modal retrieval. This approach reduces the need for image-text pairs and can use object labels generated by an object detector as supervision."
}