{
    "title": "ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])",
    "abstract": "The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we",
    "link": "http://arxiv.org/abs/2304.13140",
    "context": "Title: ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])\nAbstract: The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we",
    "path": "papers/23/04/2304.13140.json",
    "total_tokens": 900,
    "translated_title": "ESimCSE无监督对比学习联合UDA半监督学习用于大标签系统文本分类模型",
    "translated_abstract": "在自然语言处理任务中，文本分类面临的挑战包括多个标签系统、数据分布不均匀和高噪声。为了解决这些问题，本文通过使用联合训练技术，将ESimCSE无监督比较学习和UDA半监督比较学习模型相结合。ESimCSE模型利用无标签数据高效地学习文本向量表示，从而实现更好的分类结果；而UDA则通过半监督学习方法使用无标签数据进行训练，提高模型的预测性能和稳定性，并进一步提高模型的泛化能力。此外，模型训练过程中采用了对抗训练技术FGM和PGD，以提高模型的鲁棒性和可靠性。实验结果表明，与基线模型相比，在公共数据集Ruesters上有8%和10%的准确率提高。",
    "tldr": "本文提出ESimCSE无监督比较学习和UDA半监督比较学习模型相结合，通过联合训练技术解决了大标签系统文本分类的多个问题，并在公共数据集上实现了准确率提高。",
    "en_tdlr": "This paper proposes a combined model of ESimCSE unsupervised contrastive learning and UDA semi-supervised learning to address multiple challenges in text classification with large tag systems, and achieves accuracy improvement on a public dataset through joint training techniques."
}