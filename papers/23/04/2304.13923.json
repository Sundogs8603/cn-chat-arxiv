{
    "title": "Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])",
    "abstract": "With recent progress in large-scale vision and language representation learning, Vision Language Pretraining (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these pre-training models still do not take advantage of world knowledge, which is implicit in multi-modal data but comprises abundant and complementary information. In this work, we propose a REtrieval-based knowledge Augmented Vision Language Pre-training model (REAVL), which retrieves world knowledge from knowledge graphs (KGs) and incorporates them in vision-language pre-training. REAVL has two core components: a knowledge retriever that retrieves knowledge given multi-modal data, and a knowledge-augmented model that fuses multi-modal data and knowledge. By novelly unifying four knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of multi-modal data and knowledge by fusing explicit knowledge with vision-language pairs for masked multi-modal dat",
    "link": "http://arxiv.org/abs/2304.13923",
    "context": "Title: Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])\nAbstract: With recent progress in large-scale vision and language representation learning, Vision Language Pretraining (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these pre-training models still do not take advantage of world knowledge, which is implicit in multi-modal data but comprises abundant and complementary information. In this work, we propose a REtrieval-based knowledge Augmented Vision Language Pre-training model (REAVL), which retrieves world knowledge from knowledge graphs (KGs) and incorporates them in vision-language pre-training. REAVL has two core components: a knowledge retriever that retrieves knowledge given multi-modal data, and a knowledge-augmented model that fuses multi-modal data and knowledge. By novelly unifying four knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of multi-modal data and knowledge by fusing explicit knowledge with vision-language pairs for masked multi-modal dat",
    "path": "papers/23/04/2304.13923.json",
    "total_tokens": 826,
    "translated_title": "基于检索的知识增强视觉语言预训练模型",
    "translated_abstract": "随着大规模视觉和语言表示学习的进展，视觉语言预训练(VLP)模型在各种多模态下游任务中取得了可喜的进展。然而，这些预训练模型仍未利用世界知识，世界知识隐含在多模态数据中，但包含丰富和互补的信息。在本工作中，我们提出了一种REtrieval-based knowledge Augmented Vision Language Pre-training (REAVL)模型，它从知识图谱(KGs)中检索世界知识，并将其融入视觉语言预训练中。",
    "tldr": "本文提出了一种基于检索的知识增强视觉语言预训练模型，将从知识图谱中检索到的世界知识融入视觉语言预训练中，将明确的知识与视觉语言对融合，通过四个知识感知的自监督任务推动多模态数据和知识的相互整合。",
    "en_tdlr": "This work proposes a retrieval-based knowledge augmented vision language pre-training model, which retrieves world knowledge from knowledge graphs and incorporates them into pre-training by fusing explicit knowledge with vision-language pairs via four knowledge-aware self-supervised tasks to promote the mutual integration of multi-modal data and knowledge."
}