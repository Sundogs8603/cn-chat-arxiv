{
    "title": "Approximation of Nonlinear Functionals Using Deep ReLU Networks. (arXiv:2304.04443v1 [stat.ML])",
    "abstract": "In recent years, functional neural networks have been proposed and studied in order to approximate nonlinear continuous functionals defined on $L^p([-1, 1]^s)$ for integers $s\\ge1$ and $1\\le p<\\infty$. However, their theoretical properties are largely unknown beyond universality of approximation or the existing analysis does not apply to the rectified linear unit (ReLU) activation function. To fill in this void, we investigate here the approximation power of functional deep neural networks associated with the ReLU activation function by constructing a continuous piecewise linear interpolation under a simple triangulation. In addition, we establish rates of approximation of the proposed functional deep ReLU networks under mild regularity conditions. Finally, our study may also shed some light on the understanding of functional data learning algorithms.",
    "link": "http://arxiv.org/abs/2304.04443",
    "context": "Title: Approximation of Nonlinear Functionals Using Deep ReLU Networks. (arXiv:2304.04443v1 [stat.ML])\nAbstract: In recent years, functional neural networks have been proposed and studied in order to approximate nonlinear continuous functionals defined on $L^p([-1, 1]^s)$ for integers $s\\ge1$ and $1\\le p<\\infty$. However, their theoretical properties are largely unknown beyond universality of approximation or the existing analysis does not apply to the rectified linear unit (ReLU) activation function. To fill in this void, we investigate here the approximation power of functional deep neural networks associated with the ReLU activation function by constructing a continuous piecewise linear interpolation under a simple triangulation. In addition, we establish rates of approximation of the proposed functional deep ReLU networks under mild regularity conditions. Finally, our study may also shed some light on the understanding of functional data learning algorithms.",
    "path": "papers/23/04/2304.04443.json",
    "total_tokens": 815,
    "translated_title": "使用深度ReLU网络逼近非线性函数的研究",
    "translated_abstract": "最近几年，人们提出并研究了函数神经网络，以逼近在L^p([-1,1]^s)上定义的非线性连续函数，其中s≥1，1≤p<∞。然而，除逼近的普遍性外，它们的理论性质大多数还是未知的，或者现有的分析不适用于修正的线性单元（ReLU）激活函数。为填补这个空白，我们在此通过在简单的三角剖分下构建连续分段线性插值，研究了与ReLU激活函数相关的函数深度神经网络的逼近能力。此外，我们还在温和的正则条件下建立了所提出的函数深度ReLU网络的逼近速率。最后，我们的研究也可能有助于理解函数数据学习算法。",
    "tldr": "本文研究了使用深度ReLU网络逼近非线性函数的理论性质，并使用简单的三角剖分下构建了连续分段线性插值，实现了函数深度神经网络的逼近能力。",
    "en_tdlr": "This paper investigates the theoretical properties of using deep ReLU networks to approximate nonlinear functions, and constructs a continuous piecewise linear interpolation under a simple triangulation to achieve the approximation power of functional deep neural networks associated with ReLU activation function."
}