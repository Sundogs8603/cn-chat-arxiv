{
    "title": "Wavelets Beat Monkeys at Adversarial Robustness. (arXiv:2304.09403v1 [cs.LG])",
    "abstract": "Research on improving the robustness of neural networks to adversarial noise - imperceptible malicious perturbations of the data - has received significant attention. The currently uncontested state-of-the-art defense to obtain robust deep neural networks is Adversarial Training (AT), but it consumes significantly more resources compared to standard training and trades off accuracy for robustness. An inspiring recent work [Dapello et al.] aims to bring neurobiological tools to the question: How can we develop Neural Nets that robustly generalize like human vision? [Dapello et al.] design a network structure with a neural hidden first layer that mimics the primate primary visual cortex (V1), followed by a back-end structure adapted from current CNN vision models. It seems to achieve non-trivial adversarial robustness on standard vision benchmarks when tested on small perturbations. Here we revisit this biologically inspired work, and ask whether a principled parameter-free representatio",
    "link": "http://arxiv.org/abs/2304.09403",
    "context": "Title: Wavelets Beat Monkeys at Adversarial Robustness. (arXiv:2304.09403v1 [cs.LG])\nAbstract: Research on improving the robustness of neural networks to adversarial noise - imperceptible malicious perturbations of the data - has received significant attention. The currently uncontested state-of-the-art defense to obtain robust deep neural networks is Adversarial Training (AT), but it consumes significantly more resources compared to standard training and trades off accuracy for robustness. An inspiring recent work [Dapello et al.] aims to bring neurobiological tools to the question: How can we develop Neural Nets that robustly generalize like human vision? [Dapello et al.] design a network structure with a neural hidden first layer that mimics the primate primary visual cortex (V1), followed by a back-end structure adapted from current CNN vision models. It seems to achieve non-trivial adversarial robustness on standard vision benchmarks when tested on small perturbations. Here we revisit this biologically inspired work, and ask whether a principled parameter-free representatio",
    "path": "papers/23/04/2304.09403.json",
    "total_tokens": 1054,
    "translated_title": "小波胜过了猴子——对抗鲁棒性的研究",
    "translated_abstract": "研究人员一直致力于提高神经网络对对抗性噪声的鲁棒性——即对数据中微不可见的恶意扰动的抵御能力。目前，获取强大神经网络鲁棒性的无可争议的最先进防御策略是对抗性训练(AT)，但它比标准训练耗费更多资源，同时在鲁棒性和准确性之间有所折衷。最近的一项鼓舞人心的工作[Dapello等人]旨在将神经生物学工具应用于问题：我们如何开发能够像人类视觉一样鲁棒地概括的神经网络？[Dapello等人]设计了一个神经隐藏第一层的网络结构，该层模仿了灵长类动物的视觉皮层(V1)，其后是从当前CNN视觉模型调整而来的后端结构。当在小扰动条件下在标准视觉基准测试中测试时，似乎实现了非平凡的对抗鲁棒性。在这里，我们重新审视这项受生物启发的研究，并问一个问题：是否存在一个基于原则的无参数表示方法?",
    "tldr": "该研究旨在探讨如何开发出能够像人类视觉一样鲁棒地概括的神经网络，提高对抗性噪声的鲁棒性，并提出了一种模仿灵长类动物视觉皮层(V1)的神经网络模型，其在小扰动下具有非平凡的对抗鲁棒性，该模型使用小波方法。",
    "en_tdlr": "This research aims to explore how to develop neural networks that generalize robustly like human vision, improve the robustness of neural networks to adversarial noise, and propose a neural network model that mimics the primate primary visual cortex (V1). The model achieved non-trivial adversarial robustness on standard vision benchmarks under small perturbations, using wavelet methods."
}