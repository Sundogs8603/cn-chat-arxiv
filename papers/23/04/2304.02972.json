{
    "title": "Training a Two Layer ReLU Network Analytically. (arXiv:2304.02972v1 [cs.LG])",
    "abstract": "Neural networks are usually trained with different variants of gradient descent based optimization algorithms such as stochastic gradient descent or the Adam optimizer. Recent theoretical work states that the critical points (where the gradient of the loss is zero) of two-layer ReLU networks with the square loss are not all local minima. However, in this work we will explore an algorithm for training two-layer neural networks with ReLU-like activation and the square loss that alternatively finds the critical points of the loss function analytically for one layer while keeping the other layer and the neuron activation pattern fixed. Experiments indicate that this simple algorithm can find deeper optima than Stochastic Gradient Descent or the Adam optimizer, obtaining significantly smaller training loss values on four out of the five real datasets evaluated. Moreover, the method is faster than the gradient descent methods and has virtually no tuning parameters.",
    "link": "http://arxiv.org/abs/2304.02972",
    "context": "Title: Training a Two Layer ReLU Network Analytically. (arXiv:2304.02972v1 [cs.LG])\nAbstract: Neural networks are usually trained with different variants of gradient descent based optimization algorithms such as stochastic gradient descent or the Adam optimizer. Recent theoretical work states that the critical points (where the gradient of the loss is zero) of two-layer ReLU networks with the square loss are not all local minima. However, in this work we will explore an algorithm for training two-layer neural networks with ReLU-like activation and the square loss that alternatively finds the critical points of the loss function analytically for one layer while keeping the other layer and the neuron activation pattern fixed. Experiments indicate that this simple algorithm can find deeper optima than Stochastic Gradient Descent or the Adam optimizer, obtaining significantly smaller training loss values on four out of the five real datasets evaluated. Moreover, the method is faster than the gradient descent methods and has virtually no tuning parameters.",
    "path": "papers/23/04/2304.02972.json",
    "total_tokens": 921,
    "translated_title": "解析训练双层ReLU网络",
    "translated_abstract": "神经网络通常使用各种梯度下降的优化算法进行训练，如随机梯度下降或Adam优化器。最近的理论研究表明，双层ReLU网络的临界点（损失梯度为零的点）不都是局部最小值。然而，在本研究中，我们将探讨一种使用ReLU激活的双层神经网络和平方损失的算法，该算法交替地在一个层的情况下解析地找到损失函数的临界点，同时保持另一个层和神经元激活模式不变。实验表明，这个简单的算法比随机梯度下降或Adam优化器能够找到更深的最小值，在评估的五个真实数据集中有四个获得了显著更小的训练损失值。而且，该方法比梯度下降方法更快，几乎没有调参参数。",
    "tldr": "本研究探讨了一种算法，可以使用解析的方法训练双层ReLU网络，相比随机梯度下降和Adam优化器能够找到更深的最小值，在四个真实数据集中获得了显著更小的训练损失值，同时该方法速度更快，调参参数更少。",
    "en_tdlr": "This study explores an algorithm for training two-layer ReLU networks using an analytical approach that finds critical points of the loss function for one layer while keeping the other layer and neuron activation pattern fixed. Compared to stochastic gradient descent and Adam optimizer, this algorithm finds deeper optima and achieves significantly smaller training loss values on four out of the five evaluated datasets, with faster speed and fewer tuning parameters."
}