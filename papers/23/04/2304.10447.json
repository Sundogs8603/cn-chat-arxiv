{
    "title": "Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health. (arXiv:2304.10447v1 [cs.CL])",
    "abstract": "Pretrained language models have been used in various natural language processing applications. In the mental health domain, domain-specific language models are pretrained and released, which facilitates the early detection of mental health conditions. Social posts, e.g., on Reddit, are usually long documents. However, there are no domain-specific pretrained models for long-sequence modeling in the mental health domain. This paper conducts domain-specific continued pretraining to capture the long context for mental health. Specifically, we train and release MentalXLNet and MentalLongformer based on XLNet and Longformer. We evaluate the mental health classification performance and the long-range ability of these two domain-specific pretrained models. Our models are released in HuggingFace.",
    "link": "http://arxiv.org/abs/2304.10447",
    "context": "Title: Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health. (arXiv:2304.10447v1 [cs.CL])\nAbstract: Pretrained language models have been used in various natural language processing applications. In the mental health domain, domain-specific language models are pretrained and released, which facilitates the early detection of mental health conditions. Social posts, e.g., on Reddit, are usually long documents. However, there are no domain-specific pretrained models for long-sequence modeling in the mental health domain. This paper conducts domain-specific continued pretraining to capture the long context for mental health. Specifically, we train and release MentalXLNet and MentalLongformer based on XLNet and Longformer. We evaluate the mental health classification performance and the long-range ability of these two domain-specific pretrained models. Our models are released in HuggingFace.",
    "path": "papers/23/04/2304.10447.json",
    "total_tokens": 847,
    "translated_title": "面向心理健康的语言模型继续预训练以捕获长上下文",
    "translated_abstract": "预训练语言模型已被用于各种自然语言处理应用。在心理健康领域，预训练的领域特定语言模型被释放，有助于早期发现心理健康问题。社交媒体帖子（例如 Reddit）通常都很长。然而，在心理健康领域中并没有面向长序列的领域特定预训练模型。本文进行了面向心理健康的继续预训练，以捕捉长上下文。具体而言，我们基于 XLNet 和 Longformer 训练并发布了 MentalXLNet 和 MentalLongformer。我们评估了这两个领域特定预训练模型的心理健康分类性能和长距离建模能力。我们的模型已在 HuggingFace 上发布。",
    "tldr": "本文在心理健康领域进行了继续预训练以捕捉长上下文。作者基于XLNet和Longformer训练和发布了MentalXLNet和MentalLongformer模型，并对其进行了性能评估和能力测试。这些模型对于早期心理健康问题的检测非常有用。",
    "en_tdlr": "This paper conducts continued pretraining in mental health domain to capture long context. The authors train and release MentalXLNet and MentalLongformer models based on XLNet and Longformer, and evaluate their classification performance and long-range ability. These models are useful for early detection of mental health problems."
}