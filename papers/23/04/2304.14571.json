{
    "title": "DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation. (arXiv:2304.14571v1 [cs.CV])",
    "abstract": "Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-traine",
    "link": "http://arxiv.org/abs/2304.14571",
    "context": "Title: DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation. (arXiv:2304.14571v1 [cs.CV])\nAbstract: Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-traine",
    "path": "papers/23/04/2304.14571.json",
    "total_tokens": 1020,
    "translated_title": "DIAMANT: 基于双图像注意力映射编码器的医学图像分割",
    "translated_abstract": "虽然纯Transformer架构在许多计算机视觉任务中表现出了很好的性能，但许多由CNN和Transformer块组成的混合模型被引入以适应更专业的任务。然而，尽管相对于CNNs，在医学图像分割中，纯Transformer和混合Transformer架构的性能都有所提升，但它们的高训练成本和复杂性使得在真实场景中使用它们变得具有挑战性。在这项工作中，我们提出了基于纯卷积层的简单架构，并表明仅利用从自监督预训练视觉Transformer网络（例如DINO）获得的注意力映射可视化就可以以更少的计算成本胜过复杂的Transformer架构。所提出的架构由两个编码器分支组成，其中原始图像作为一个分支的输入，而来自预训练视觉Transformer网络中多个自注意力头的注意力映射可视化作为另一个分支的输入。",
    "tldr": "该论文提出了一种基于双图像注意力映射编码器的医学图像分割架构，仅利用从自监督预训练视觉Transformer网络获得的注意力映射可视化，就可以以更少的计算成本胜过复杂的Transformer架构，这在多个医疗图像数据集上得到验证。",
    "en_tdlr": "The paper proposes a medical image segmentation architecture based on dual image-attention map encoders. By taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network, the proposed architecture outperforms complex transformer-based networks with much less computation costs. The experimental results on multiple medical image datasets confirm the effectiveness and efficiency of the proposed DIAMANT architecture."
}