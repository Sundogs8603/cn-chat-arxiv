{
    "title": "SwiftTron: An Efficient Hardware Accelerator for Quantized Transformers. (arXiv:2304.03986v1 [cs.LG])",
    "abstract": "Transformers' compute-intensive operations pose enormous challenges for their deployment in resource-constrained EdgeAI / tinyML devices. As an established neural network compression technique, quantization reduces the hardware computational and memory resources. In particular, fixed-point quantization is desirable to ease the computations using lightweight blocks, like adders and multipliers, of the underlying hardware. However, deploying fully-quantized Transformers on existing general-purpose hardware, generic AI accelerators, or specialized architectures for Transformers with floating-point units might be infeasible and/or inefficient.  Towards this, we propose SwiftTron, an efficient specialized hardware accelerator designed for Quantized Transformers. SwiftTron supports the execution of different types of Transformers' operations (like Attention, Softmax, GELU, and Layer Normalization) and accounts for diverse scaling factors to perform correct computations. We synthesize the com",
    "link": "http://arxiv.org/abs/2304.03986",
    "context": "Title: SwiftTron: An Efficient Hardware Accelerator for Quantized Transformers. (arXiv:2304.03986v1 [cs.LG])\nAbstract: Transformers' compute-intensive operations pose enormous challenges for their deployment in resource-constrained EdgeAI / tinyML devices. As an established neural network compression technique, quantization reduces the hardware computational and memory resources. In particular, fixed-point quantization is desirable to ease the computations using lightweight blocks, like adders and multipliers, of the underlying hardware. However, deploying fully-quantized Transformers on existing general-purpose hardware, generic AI accelerators, or specialized architectures for Transformers with floating-point units might be infeasible and/or inefficient.  Towards this, we propose SwiftTron, an efficient specialized hardware accelerator designed for Quantized Transformers. SwiftTron supports the execution of different types of Transformers' operations (like Attention, Softmax, GELU, and Layer Normalization) and accounts for diverse scaling factors to perform correct computations. We synthesize the com",
    "path": "papers/23/04/2304.03986.json",
    "total_tokens": 818,
    "tldr": "本文提出了SwiftTron，一种专门为量化Transformer设计的高效硬件加速器，能够执行各种类型的Transformer操作，并考虑了各种缩放因子以进行正确的计算。"
}