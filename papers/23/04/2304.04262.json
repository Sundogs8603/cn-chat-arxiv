{
    "title": "A Comprehensive Survey on Knowledge Distillation of Diffusion Models. (arXiv:2304.04262v1 [cs.LG])",
    "abstract": "Diffusion Models (DMs), also referred to as score-based diffusion models, utilize neural networks to specify score functions. Unlike most other probabilistic models, DMs directly model the score functions, which makes them more flexible to parametrize and potentially highly expressive for probabilistic modeling. DMs can learn fine-grained knowledge, i.e., marginal score functions, of the underlying distribution. Therefore, a crucial research direction is to explore how to distill the knowledge of DMs and fully utilize their potential. Our objective is to provide a comprehensible overview of the modern approaches for distilling DMs, starting with an introduction to DMs and a discussion of the challenges involved in distilling them into neural vector fields. We also provide an overview of the existing works on distilling DMs into both stochastic and deterministic implicit generators. Finally, we review the accelerated diffusion sampling algorithms as a training-free method for distillati",
    "link": "http://arxiv.org/abs/2304.04262",
    "context": "Title: A Comprehensive Survey on Knowledge Distillation of Diffusion Models. (arXiv:2304.04262v1 [cs.LG])\nAbstract: Diffusion Models (DMs), also referred to as score-based diffusion models, utilize neural networks to specify score functions. Unlike most other probabilistic models, DMs directly model the score functions, which makes them more flexible to parametrize and potentially highly expressive for probabilistic modeling. DMs can learn fine-grained knowledge, i.e., marginal score functions, of the underlying distribution. Therefore, a crucial research direction is to explore how to distill the knowledge of DMs and fully utilize their potential. Our objective is to provide a comprehensible overview of the modern approaches for distilling DMs, starting with an introduction to DMs and a discussion of the challenges involved in distilling them into neural vector fields. We also provide an overview of the existing works on distilling DMs into both stochastic and deterministic implicit generators. Finally, we review the accelerated diffusion sampling algorithms as a training-free method for distillati",
    "path": "papers/23/04/2304.04262.json",
    "total_tokens": 848,
    "translated_title": "一篇关于知识蒸馏扩散模型的综合概述",
    "translated_abstract": "扩散模型是一种利用神经网络规定评分函数的概率模型，直接对评分函数进行建模，可用于精细知识的学习和潜在的概率建模。本文旨在提供现代扩散模型知识蒸馏方法的综合概述，从扩散模型介绍和知识蒸馏面临的挑战入手，概述现有方法将扩散模型蒸馏至隐式生成函数的研究，最后回顾一种不需要训练的加速扩散采样算法作为知识蒸馏的一种方法。",
    "tldr": "扩散模型是一种直接建模评分函数的概率模型，在实现概率建模上更具可塑性和表达能力。本文综述了扩散模型知识蒸馏的现代方法和挑战，包括将扩散模型蒸馏至隐式生成函数的方法以及训练-free 的加速扩散采样算法。"
}