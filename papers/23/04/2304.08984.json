{
    "title": "Robustness of Visual Explanations to Common Data Augmentation. (arXiv:2304.08984v1 [cs.CV])",
    "abstract": "As the use of deep neural networks continues to grow, understanding their behaviour has become more crucial than ever. Post-hoc explainability methods are a potential solution, but their reliability is being called into question. Our research investigates the response of post-hoc visual explanations to naturally occurring transformations, often referred to as augmentations. We anticipate explanations to be invariant under certain transformations, such as changes to the colour map while responding in an equivariant manner to transformations like translation, object scaling, and rotation. We have found remarkable differences in robustness depending on the type of transformation, with some explainability methods (such as LRP composites and Guided Backprop) being more stable than others. We also explore the role of training with data augmentation. We provide evidence that explanations are typically less robust to augmentation than classification performance, regardless of whether data augm",
    "link": "http://arxiv.org/abs/2304.08984",
    "context": "Title: Robustness of Visual Explanations to Common Data Augmentation. (arXiv:2304.08984v1 [cs.CV])\nAbstract: As the use of deep neural networks continues to grow, understanding their behaviour has become more crucial than ever. Post-hoc explainability methods are a potential solution, but their reliability is being called into question. Our research investigates the response of post-hoc visual explanations to naturally occurring transformations, often referred to as augmentations. We anticipate explanations to be invariant under certain transformations, such as changes to the colour map while responding in an equivariant manner to transformations like translation, object scaling, and rotation. We have found remarkable differences in robustness depending on the type of transformation, with some explainability methods (such as LRP composites and Guided Backprop) being more stable than others. We also explore the role of training with data augmentation. We provide evidence that explanations are typically less robust to augmentation than classification performance, regardless of whether data augm",
    "path": "papers/23/04/2304.08984.json",
    "total_tokens": 915,
    "translated_title": "深度神经网络可视化解释对数据增强的稳健性研究",
    "translated_abstract": "随着深度神经网络的广泛应用，了解其行为变得比以往任何时候都更为重要。后续可解释性方法是潜在的解决方案，但它们的可靠性受到质疑。我们的研究探究后续可视化解释对自然发生的转换（通常称为增强）的响应。我们预计解释在某些转换下是不变的，例如更改颜色映射，同时对于像平移、对象缩放和旋转这样的转换则响应变换。我们发现，稳健性的不同程度存在显着差异，某些解释方法（例如LRP复合物和Guided Backprop）比其他方法更稳定。我们还探讨了使用数据增强进行培训的作用。我们提供证据表明，解释通常相较分类性能而言对增强的鲁棒性较差，无论数据增强是否明确地包括在训练过程中。",
    "tldr": "本文研究了深度神经网络可视化解释对自然发生的转换（增强）的响应，发现不同解释方法的稳定性存在显着差异，证明解释相较于分类性能更容易受到增强的影响。",
    "en_tdlr": "This paper investigates the response of post-hoc visual explanations to natural occurring transformations, commonly referred to as augmentations, in deep neural networks. The study found significant differences in stability between explanation methods and evidence that explanations are typically less robust to augmentation than classification performance."
}