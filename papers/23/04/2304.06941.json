{
    "title": "AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks. (arXiv:2304.06941v1 [cs.LG])",
    "abstract": "Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA me",
    "link": "http://arxiv.org/abs/2304.06941",
    "context": "Title: AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks. (arXiv:2304.06941v1 [cs.LG])\nAbstract: Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA me",
    "path": "papers/23/04/2304.06941.json",
    "total_tokens": 951,
    "translated_title": "AUTOSPARSE:实现神经网络自动稀疏训练的方法研究",
    "translated_abstract": "稀疏训练是减少训练神经网络计算成本的一个有前途的途径。最近的研究提出了使用可学习阈值的修剪方法，以有效地探索模型中内在稀疏性的不均匀分布。本文提出了梯度退火法（GA），其中掩码权重的梯度按非线性方式缩小。 GA在不需要额外稀疏诱导正则化的情况下提供了一种优美的权衡稀疏性和准确性的方法。我们将GA与最新的可学习修剪方法相结合，创建了一种称为AutoSparse的自动稀疏训练算法，它在ImageNet-1K上的稀疏ResNet50和MobileNetV1上实现了更好的准确性和/或训练/推理FLOPS减少，例如AutoSparse在80％的稀疏下，ResNet50在ImageNet上实现了（2倍，7倍）的（训练，推理）FLOPS减少。最后，AutoSparse在创新性稀疏领域表现优异。",
    "tldr": "本文提出了一种叫做AutoSparse的自动稀疏训练算法，其中包含梯度退火法来权衡稀疏和准确性，在ResNet50和MobileNetV1上表现出更好的准确性和较少的计算资源需求。"
}