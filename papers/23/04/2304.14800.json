{
    "title": "Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation. (arXiv:2304.14800v1 [cs.CV])",
    "abstract": "3D point cloud semantic segmentation is one of the fundamental tasks for environmental understanding. Although significant progress has been made in recent years, the performance of classes with few examples or few points is still far from satisfactory. In this paper, we propose a novel multi-to-single knowledge distillation framework for the 3D point cloud semantic segmentation task to boost the performance of those hard classes. Instead of fusing all the points of multi-scans directly, only the instances that belong to the previously defined hard classes are fused. To effectively and sufficiently distill valuable knowledge from multi-scans, we leverage a multilevel distillation framework, i.e., feature representation distillation, logit distillation, and affinity distillation. We further develop a novel instance-aware affinity distillation algorithm for capturing high-level structural knowledge to enhance the distillation efficacy for hard classes. Finally, we conduct experiments on ",
    "link": "http://arxiv.org/abs/2304.14800",
    "context": "Title: Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation. (arXiv:2304.14800v1 [cs.CV])\nAbstract: 3D point cloud semantic segmentation is one of the fundamental tasks for environmental understanding. Although significant progress has been made in recent years, the performance of classes with few examples or few points is still far from satisfactory. In this paper, we propose a novel multi-to-single knowledge distillation framework for the 3D point cloud semantic segmentation task to boost the performance of those hard classes. Instead of fusing all the points of multi-scans directly, only the instances that belong to the previously defined hard classes are fused. To effectively and sufficiently distill valuable knowledge from multi-scans, we leverage a multilevel distillation framework, i.e., feature representation distillation, logit distillation, and affinity distillation. We further develop a novel instance-aware affinity distillation algorithm for capturing high-level structural knowledge to enhance the distillation efficacy for hard classes. Finally, we conduct experiments on ",
    "path": "papers/23/04/2304.14800.json",
    "total_tokens": 955,
    "translated_title": "基于多对单的知识蒸馏的点云语义分割",
    "translated_abstract": "三维点云语义分割是环境理解的基本任务之一。尽管近年来已经取得了显著的进展，但是对于样本较少或点数较少的类的性能仍然不够满意。本文提出了一种新颖的基于多对单知识蒸馏框架，来提高那些难分类的类的性能。不同于直接融合多个扫描的所有点，只融合属于之前定义的难分类类别的实例。为了有效和充分地从多个扫描中蒸馏有价值的知识，我们使用了多级蒸馏框架，即特征表征蒸馏、logit蒸馏和构形蒸馏。我们进一步开发了一种新型的实例感知构形蒸馏算法，用于捕捉高层次的结构知识，以增强难分类的蒸馏效能。最后，在官方数据集上对我们的方法进行了实验。",
    "tldr": "本文提出了一个基于多对单知识蒸馏的模型来提高点云语义分割中的分类表现，该模型针对难分类类别只融合它们的实例，并使用多级蒸馏框架和实例感知构形蒸馏算法从多个扫描中蒸馏有价值的知识。"
}