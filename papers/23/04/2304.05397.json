{
    "title": "Accelerating Hybrid Federated Learning Convergence under Partial Participation. (arXiv:2304.05397v1 [cs.DC])",
    "abstract": "Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients' privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process. To address this, we focus on the hybrid FL framework in this paper. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under",
    "link": "http://arxiv.org/abs/2304.05397",
    "context": "Title: Accelerating Hybrid Federated Learning Convergence under Partial Participation. (arXiv:2304.05397v1 [cs.DC])\nAbstract: Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients' privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process. To address this, we focus on the hybrid FL framework in this paper. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under",
    "path": "papers/23/04/2304.05397.json",
    "total_tokens": 1007,
    "translated_title": "部分参与下加速混合联邦学习的收敛",
    "translated_abstract": "近年来，联邦学习(Federated Learning，FL)已成为一种常见的分布式机器学习范式。FL涉及一组有着分散数据的客户端，通过集中服务器的协调合作学习一个公共模型，其目的是通过确保本地数据集永远不会离开客户端，只有服务器进行模型聚合来保护客户端的隐私。然而，在现实场景中，服务器可能能够收集少量数据以近似总体分布，并具有更强的计算能力来执行学习过程。为了解决这个问题，本文聚焦于混合FL框架。在先前混合FL工作中，已经证明了客户端和服务器的交替训练可以增加收敛速度，但是它仅关注客户端完全参与的情况，而忽略了部分参与的负面影响。本文提出了一种自适应采样方法和部分聚合方法来改进混合FL的收敛速度，并对其进行了理论分析。我们提出的方法在减少通信成本的同时不会造成太大精度损失，与传统FL方法相比，我们方法的有效性在实际数据集上得到了证明。",
    "tldr": "本文提出了一种适用于部分参与环境下的混合联邦学习框架，其包括自适应采样方法和部分聚合方法，有助于加快模型的收敛速度，从而减少通信成本且不影响模型精度。",
    "en_tdlr": "This paper proposes a hybrid federated learning framework suitable for partial participation scenarios, which includes an adaptive sampling method and a partial aggregation method to accelerate model convergence, reduce communication costs, and maintain model accuracy."
}