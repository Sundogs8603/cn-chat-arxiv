{
    "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])",
    "abstract": "We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal ",
    "link": "http://arxiv.org/abs/2304.03208",
    "context": "Title: Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])\nAbstract: We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal ",
    "path": "papers/23/04/2304.03208.json",
    "total_tokens": 1036,
    "translated_title": "基于 Cerebras Wafer-Scale Cluster 的开放计算优化语言模型 Cerebras-GPT 的研究",
    "translated_abstract": "本文研究了最近改善大型语言模型的有效预训练和扩展以及开放数据集和工具的研究进展。同时结合这些进展，介绍了一系列从111M到13B参数的开放计算优化语言模型 Cerebras-GPT。我们根据 DeepMind 的 Chinchilla 缩放规则对 Eleuther Pile 数据集进行训练，达到了在给定计算预算下最高精度的高效预训练。我们表征了可预测的幂律缩放规律，并与其他公开可用模型进行比较，展示了 Cerebras-GPT 具有最先进的预训练和下游目标训练效率。我们描述了我们的发现，包括最大更新参数化($\\mu$P)如何进一步提高大型模型扩展的精度和超参数可预测性。我们发布了预训练模型和代码，使本文成为关于在亿级参数规模下比较计算优化语言模型的首个开放可复现的工作。",
    "tldr": "本论文介绍了从111M到13B参数的开放计算优化语言模型 Cerebras-GPT，它采用了高效的预训练方法和缩放规则，具有最先进的训练效率和可预测性，这是一个开放可复现的工作。"
}