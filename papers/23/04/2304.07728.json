{
    "title": "TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation. (arXiv:2304.07728v1 [cs.RO])",
    "abstract": "Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \\textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalit",
    "link": "http://arxiv.org/abs/2304.07728",
    "context": "Title: TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation. (arXiv:2304.07728v1 [cs.RO])\nAbstract: Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \\textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalit",
    "path": "papers/23/04/2304.07728.json",
    "total_tokens": 958,
    "translated_title": "可解释的基于Transformer的LiDAR-惯性融合里程计估计",
    "translated_abstract": "增强里程计估计性能的常用方法是多模态传感器融合，这也是移动机器人的基本模块。然而，在监督式传感器融合里程计估计任务中，如何在不同模态之间执行融合仍然是具有挑战性的问题。一些简单的操作，如逐元素求和和连接，并不具备分配自适应关注权重以有效合并不同模态的能力，这使得获得具有竞争力的里程计结果变得困难。最近，Transformer架构显示出在视觉与语言领域的多模态融合任务中具有潜力。在本文中，我们提出了一种基于Transformer的端到端的激光雷达-惯性融合框架(即TransFusionOdom)来进行里程计估计。多头注意力融合模块展示了同构和异构模态的不同融合方法。所提出的TransFusionOdom模型在KITTI和EuRoC数据集上显示出比现有方法更好的性能。",
    "tldr": "本文提出了一种可解释的基于Transformer的LiDAR-惯性融合里程计估计方法，通过多头注意力融合模块展示了不同融合方法，实验表明该方法在KITTI和EuRoC数据集上表现优于现有方法。",
    "en_tdlr": "This paper proposes an interpretable Transformer-based LiDAR-Inertial fusion framework with a multi-attention fusion module for odometry estimation, which shows improved performance compared to state-of-the-art methods on KITTI and EuRoC datasets."
}