{
    "title": "Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])",
    "abstract": "Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture",
    "link": "http://arxiv.org/abs/2304.05288",
    "context": "Title: Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])\nAbstract: Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture",
    "path": "papers/23/04/2304.05288.json",
    "total_tokens": 858,
    "translated_title": "任务难度感知的增量学习参数分配与正则化",
    "translated_abstract": "参数正则化或分配方法对于克服增量学习中的灾难性遗忘非常有效。然而，它们在序列中均匀解决所有任务，忽略了不同任务的学习难度差异。因此，当学习与已学任务非常不同的新任务时，参数正则化方法会面临显着的遗忘，而参数分配方法在学习简单任务时则面临不必要的参数开销。本文提出了参数分配和正则化策略（PAR），其可以根据学习难度从参数分配和正则化中自适应地选择适当的策略。对于一个已经学习相关任务的模型来说，一个任务会变得容易，反之亦然。本文提出了基于最近原型距离的离散度估计方法，仅使用新任务的特征来度量任务相关性。此外，我们提出了一种时效性相关性感知采样的架构。",
    "tldr": "本文提出了一种任务难度感知的增量学习参数分配与正则化方法，该方法可以根据任务的学习难度自适应地选择适当的分配或正则化策略，以克服在学习不同任务时的挑战。",
    "en_tdlr": "This paper proposes a task difficulty aware parameter allocation and regularization method for lifelong learning, which adaptively selects an appropriate allocation or regularization strategy based on the task's learning difficulty, to overcome the challenges in learning different tasks."
}