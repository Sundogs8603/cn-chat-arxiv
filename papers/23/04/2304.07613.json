{
    "title": "STen: Productive and Efficient Sparsity in PyTorch. (arXiv:2304.07613v1 [cs.LG])",
    "abstract": "As deep learning models grow, sparsity is becoming an increasingly critical component of deep neural networks, enabling improved performance and reduced storage. However, existing frameworks offer poor support for sparsity. Specialized sparsity engines focus exclusively on sparse inference, while general frameworks primarily focus on sparse tensors in classical formats and neglect the broader sparsification pipeline necessary for using sparse models, especially during training. Further, existing frameworks are not easily extensible: adding a new sparse tensor format or operator is challenging and time-consuming. To address this, we propose STen, a sparsity programming model and interface for PyTorch, which incorporates sparsity layouts, operators, and sparsifiers, in an efficient, customizable, and extensible framework that supports virtually all sparsification methods. We demonstrate this by developing a high-performance grouped n:m sparsity layout for CPU inference at moderate sparsi",
    "link": "http://arxiv.org/abs/2304.07613",
    "context": "Title: STen: Productive and Efficient Sparsity in PyTorch. (arXiv:2304.07613v1 [cs.LG])\nAbstract: As deep learning models grow, sparsity is becoming an increasingly critical component of deep neural networks, enabling improved performance and reduced storage. However, existing frameworks offer poor support for sparsity. Specialized sparsity engines focus exclusively on sparse inference, while general frameworks primarily focus on sparse tensors in classical formats and neglect the broader sparsification pipeline necessary for using sparse models, especially during training. Further, existing frameworks are not easily extensible: adding a new sparse tensor format or operator is challenging and time-consuming. To address this, we propose STen, a sparsity programming model and interface for PyTorch, which incorporates sparsity layouts, operators, and sparsifiers, in an efficient, customizable, and extensible framework that supports virtually all sparsification methods. We demonstrate this by developing a high-performance grouped n:m sparsity layout for CPU inference at moderate sparsi",
    "path": "papers/23/04/2304.07613.json",
    "total_tokens": 930,
    "translated_title": "STen: PyTorch中高效的稀疏性支持",
    "translated_abstract": "随着深度学习模型的增长，稀疏性正在成为深度神经网络中越来越关键的组成部分，使性能得到改善并减少存储需求。然而，现有的框架对稀疏性的支持较差。专用的稀疏引擎专注于稀疏推理，而一般框架主要集中在经典格式的稀疏张量上，并忽略了使用稀疏模型所需的更广泛的稀疏化流程，特别是在训练期间。此外，现有框架不容易扩展：添加新的稀疏张量格式或操作是具有挑战性和耗时的。为了解决这个问题，我们提出了STen，这是一个为PyTorch设计的稀疏编程模型和接口，它包括有效、可定制和可扩展的稀疏布局、运算符和稀疏化程序，支持几乎所有稀疏化方法。我们通过开发一个在CPU推理中进行中等稀疏度的高性能分组n:m稀疏布局来演示了这一点。",
    "tldr": "本论文提出了一个作为PyTorch接口的稀疏编程模型STen，它支持几乎所有稀疏化方法，高效、可定制、可扩展且能够在各种场景下进行实现。",
    "en_tdlr": "This paper proposes STen, a sparse programming model and interface for PyTorch, which supports virtually all sparsification methods in an efficient, customizable, and extensible framework that can be implemented in various scenarios."
}