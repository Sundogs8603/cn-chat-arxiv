{
    "title": "Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability. (arXiv:2304.14864v1 [cs.AI])",
    "abstract": "Analysis of how semantic concepts are represented within Convolutional Neural Networks (CNNs) is a widely used approach in Explainable Artificial Intelligence (XAI) for interpreting CNNs. A motivation is the need for transparency in safety-critical AI-based systems, as mandated in various domains like automated driving. However, to use the concept representations for safety-relevant purposes, like inspection or error retrieval, these must be of high quality and, in particular, stable. This paper focuses on two stability goals when working with concept representations in computer vision CNNs: stability of concept retrieval and of concept attribution. The guiding use-case is a post-hoc explainability framework for object detection (OD) CNNs, towards which existing concept analysis (CA) methods are successfully adapted. To address concept retrieval stability, we propose a novel metric that considers both concept separation and consistency, and is agnostic to layer and concept representati",
    "link": "http://arxiv.org/abs/2304.14864",
    "context": "Title: Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability. (arXiv:2304.14864v1 [cs.AI])\nAbstract: Analysis of how semantic concepts are represented within Convolutional Neural Networks (CNNs) is a widely used approach in Explainable Artificial Intelligence (XAI) for interpreting CNNs. A motivation is the need for transparency in safety-critical AI-based systems, as mandated in various domains like automated driving. However, to use the concept representations for safety-relevant purposes, like inspection or error retrieval, these must be of high quality and, in particular, stable. This paper focuses on two stability goals when working with concept representations in computer vision CNNs: stability of concept retrieval and of concept attribution. The guiding use-case is a post-hoc explainability framework for object detection (OD) CNNs, towards which existing concept analysis (CA) methods are successfully adapted. To address concept retrieval stability, we propose a novel metric that considers both concept separation and consistency, and is agnostic to layer and concept representati",
    "path": "papers/23/04/2304.14864.json",
    "total_tokens": 977,
    "translated_title": "评估CNN中语义概念表示的稳定性，以实现强大的可解释性",
    "translated_abstract": "在可解释的人工智能（XAI）中，分析卷积神经网络（CNNs）中语义概念的表示是一种广泛使用的方法。其动机是因为各个领域如自动驾驶等安全关键的基于AI的系统需要透明度。然而，要将这些概念表示用于安全相关目的，例如检查或错误检索，这些表示必须具有高质量，特别是稳定性。本文关注计算机视觉CNN中概念表示的稳定性：概念检索稳定性和概念归属稳定性。以目标检测（OD）CNN的事后可解释性框架为指导目标，成功地将现有的概念分析（CA）方法应用于其上。为了解决概念检索的稳定性问题，我们提出了一种新的度量标准，考虑概念分离和一致性，与层和概念表示无关。",
    "tldr": "本文提出了一种用于评估语义概念表示在CNN中稳定性的方法，为实现强大的可解释性提供了基础。本文关注计算机视觉CNN中概念表示的稳定性：概念检索稳定性和概念归属稳定性。在此基础上，本文提出了一种新的度量标准以解决概念检索稳定性的问题。",
    "en_tdlr": "This paper proposes a method for evaluating the stability of semantic concept representations in CNNs for robust explainability, and focuses on two stability goals: stability of concept retrieval and of concept attribution. A novel metric is proposed to solve the problem of concept retrieval stability. This work provides a foundation for achieving powerful explainability in safety-critical AI-based systems such as automated driving."
}