{
    "title": "Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning. (arXiv:2304.04312v1 [cs.LG])",
    "abstract": "Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on ``benign overfitting'' and ``double descent'' pheno",
    "link": "http://arxiv.org/abs/2304.04312",
    "context": "Title: Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning. (arXiv:2304.04312v1 [cs.LG])\nAbstract: Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on ``benign overfitting'' and ``double descent'' pheno",
    "path": "papers/23/04/2304.04312.json",
    "total_tokens": 961,
    "translated_title": "过拟合元学习的泛化性能的理论表征",
    "translated_abstract": "元学习已经成为一种成功的方法，通过对许多相似的任务进行训练来提高训练性能，特别是在深度神经网络（DNNs）中。然而，对于过度参数化模型（例如DNNs），何时以及为什么可以在元学习中很好地进行泛化的理论理解仍然有限。作为解决这一挑战的初步步骤，本文研究了具有高斯特征的线性回归模型下过拟合元学习的泛化性能。与一些最近的研究相比，我们的框架允许模型参数的数量任意大于地面实况信号中的特征数量，因此自然地捕捉深度元学习实践中的过度参数化制度。我们展示了模型无关元学习（MAML）的过拟合最小 $\\ell_2$范数解可以是有益的，这类似于最近关于“良性过拟合”和“双下降”现象的显着发现。",
    "tldr": "本文研究了过拟合元学习的泛化性能，发现模型无关元学习（MAML）的过拟合最小 $\\ell_2$ 范数解可以是有益的，类似于最近关于“良性过拟合”和“双下降”现象的显着发现。",
    "en_tdlr": "This paper studies the generalization performance of overfitted meta-learning and shows that the overfitted min $\\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on \"benign overfitting\" and \"double descent\" phenomena."
}