{
    "title": "Data-Efficient Image Quality Assessment with Attention-Panel Decoder. (arXiv:2304.04952v1 [cs.CV])",
    "abstract": "Blind Image Quality Assessment (BIQA) is a fundamental task in computer vision, which however remains unresolved due to the complex distortion conditions and diversified image contents. To confront this challenge, we in this paper propose a novel BIQA pipeline based on the Transformer architecture, which achieves an efficient quality-aware feature representation with much fewer data. More specifically, we consider the traditional fine-tuning in BIQA as an interpretation of the pre-trained model. In this way, we further introduce a Transformer decoder to refine the perceptual information of the CLS token from different perspectives. This enables our model to establish the quality-aware feature manifold efficiently while attaining a strong generalization capability. Meanwhile, inspired by the subjective evaluation behaviors of human, we introduce a novel attention panel mechanism, which improves the model performance and reduces the prediction uncertainty simultaneously. The proposed BIQ",
    "link": "http://arxiv.org/abs/2304.04952",
    "context": "Title: Data-Efficient Image Quality Assessment with Attention-Panel Decoder. (arXiv:2304.04952v1 [cs.CV])\nAbstract: Blind Image Quality Assessment (BIQA) is a fundamental task in computer vision, which however remains unresolved due to the complex distortion conditions and diversified image contents. To confront this challenge, we in this paper propose a novel BIQA pipeline based on the Transformer architecture, which achieves an efficient quality-aware feature representation with much fewer data. More specifically, we consider the traditional fine-tuning in BIQA as an interpretation of the pre-trained model. In this way, we further introduce a Transformer decoder to refine the perceptual information of the CLS token from different perspectives. This enables our model to establish the quality-aware feature manifold efficiently while attaining a strong generalization capability. Meanwhile, inspired by the subjective evaluation behaviors of human, we introduce a novel attention panel mechanism, which improves the model performance and reduces the prediction uncertainty simultaneously. The proposed BIQ",
    "path": "papers/23/04/2304.04952.json",
    "total_tokens": 1031,
    "translated_title": "基于Transformer架构的低数据量图像质量评估方法",
    "translated_abstract": "盲目的图像质量评估是计算机视觉中的基本任务，然而由于复杂的失真条件和多样化的图像内容，这个问题一直没有解决。为了应对这一挑战，本文提出了一种基于Transformer架构的新型盲目图像质量评估流程，该流程利用了少量数据实现了高效的质量感知特征表示。具体来说，我们将BIQA中传统的微调解释为预训练模型的一种解释方法，同时引入了一个Transformer解码器来从不同角度完善CLS token的知觉信息。这使得我们的模型能够高效地建立质量感知特征流形并具有强大的泛化能力。同时，受人类主观评估行为的启发，我们引入了一种新的注意力面板机制，该机制同时提高了模型性能并减少了预测的不确定性。",
    "tldr": "本文提出了基于Transformer架构的低数据量图像质量评估方法，通过解释传统的微调为预训练模型的一种解释方法，同时引入Transformer解码器改善知觉信息，建立质量感知特征流形，并引入注意力面板机制改进模型性能，减少预测的不确定性。",
    "en_tdlr": "A Transformer-based pipeline is proposed for blind image quality assessment using less data. The pipeline interprets traditional fine-tuning in BIQA through pre-trained models and introduces a Transformer decoder to refine perceptual information of the CLS token from different perspectives to efficiently establish quality-aware feature manifold with strong generalization capability. A novel attention panel mechanism is introduced to improve model performance and reduce prediction uncertainty simultaneous, inspired by the subjective evaluation behaviors of humans."
}