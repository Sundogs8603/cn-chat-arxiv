{
    "title": "Neuroevolution of Recurrent Architectures on Control Tasks. (arXiv:2304.12431v1 [cs.NE])",
    "abstract": "Modern artificial intelligence works typically train the parameters of fixed-sized deep neural networks using gradient-based optimization techniques. Simple evolutionary algorithms have recently been shown to also be capable of optimizing deep neural network parameters, at times matching the performance of gradient-based techniques, e.g. in reinforcement learning settings. In addition to optimizing network parameters, many evolutionary computation techniques are also capable of progressively constructing network architectures. However, constructing network architectures from elementary evolution rules has not yet been shown to scale to modern reinforcement learning benchmarks. In this paper we therefore propose a new approach in which the architectures of recurrent neural networks dynamically evolve according to a small set of mutation rules. We implement a massively parallel evolutionary algorithm and run experiments on all 19 OpenAI Gym state-based reinforcement learning control task",
    "link": "http://arxiv.org/abs/2304.12431",
    "context": "Title: Neuroevolution of Recurrent Architectures on Control Tasks. (arXiv:2304.12431v1 [cs.NE])\nAbstract: Modern artificial intelligence works typically train the parameters of fixed-sized deep neural networks using gradient-based optimization techniques. Simple evolutionary algorithms have recently been shown to also be capable of optimizing deep neural network parameters, at times matching the performance of gradient-based techniques, e.g. in reinforcement learning settings. In addition to optimizing network parameters, many evolutionary computation techniques are also capable of progressively constructing network architectures. However, constructing network architectures from elementary evolution rules has not yet been shown to scale to modern reinforcement learning benchmarks. In this paper we therefore propose a new approach in which the architectures of recurrent neural networks dynamically evolve according to a small set of mutation rules. We implement a massively parallel evolutionary algorithm and run experiments on all 19 OpenAI Gym state-based reinforcement learning control task",
    "path": "papers/23/04/2304.12431.json",
    "total_tokens": 815,
    "translated_title": "“在控制任务上的递归结构的神经演化。”",
    "translated_abstract": "现代人工智能工作通常使用基于渐变的优化技术来训练固定大小的深层神经网络的参数。最近已经证明，简单的进化算法也可以优化深度神经网络的参数，有时可以匹配基于梯度的技术在强化学习设置中的性能。除了优化网络参数外，许多进化计算技术还能够逐步构建网络架构。然而，从基本进化规则中构建网络体系结构尚未显示出在现代强化学习基准测试中具有可扩展性。因此，在本文中，我们提出了一种新方法，其中递归神经网络的架构根据一小组突变规则动态演变。我们实现了一个大规模并行的进化算法，并在所有19个OpenAI Gym状态基础的强化学习控制任务上进行了实验。",
    "tldr": "本文提出了一种新颖的方法，通过一小组变异规则动态演化递归神经网络的架构，实现了在强化学习控制任务上的神经演化。",
    "en_tdlr": "This paper proposes a novel approach to evolve the architecture of recurrent neural networks dynamically according to a small set of mutation rules. The experiments conducted on all 19 OpenAI Gym state-based reinforcement learning control tasks demonstrate the effectiveness of this method."
}