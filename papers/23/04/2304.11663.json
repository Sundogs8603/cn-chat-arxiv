{
    "title": "Efficient Training of Deep Equilibrium Models. (arXiv:2304.11663v1 [cs.LG])",
    "abstract": "Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.",
    "link": "http://arxiv.org/abs/2304.11663",
    "context": "Title: Efficient Training of Deep Equilibrium Models. (arXiv:2304.11663v1 [cs.LG])\nAbstract: Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.",
    "path": "papers/23/04/2304.11663.json",
    "total_tokens": 807,
    "translated_title": "深度平衡模型的高效训练",
    "translated_abstract": "深度平衡模型（DEQ）在学习数据表示方面已经被证明非常强大。其思想是用隐式的固定点方程替换传统（显式的）前馈神经网络，从而允许解耦前向和后向传递。特别地，通过隐式函数定理，DEQ层的训练变得非常高效。但是，通过DEQ层的反向传播仍需要解决一个昂贵的基于Jacobian的方程。在本文中，我们介绍了一种简单而有效的策略来避免这种计算负担。我们的方法依赖于Broyden方法的Jacobian近似，在前向传递之后计算反向传递中的梯度。实验证明，简单地重复使用这个近似可以显著加速训练，同时不会造成性能降低。",
    "tldr": "本文介绍一种简单而有效的策略来避免深度平衡模型层中反向传播的计算负担。该方法可以显著加速训练，同时不会影响性能。",
    "en_tdlr": "This paper introduces a simple and effective strategy to avoid the computational burden of backpropagation through deep equilibrium model layers. The method significantly speeds up the training without causing any performance degradation by relying on the Jacobian approximation of Broyden's method."
}