{
    "title": "Fairness Uncertainty Quantification: How certain are you that the model is fair?. (arXiv:2304.13950v1 [stat.ML])",
    "abstract": "Fairness-aware machine learning has garnered significant attention in recent years because of extensive use of machine learning in sensitive applications like judiciary systems. Various heuristics, and optimization frameworks have been proposed to enforce fairness in classification \\cite{del2020review} where the later approaches either provides empirical results or provides fairness guarantee for the exact minimizer of the objective function \\cite{celis2019classification}. In modern machine learning, Stochastic Gradient Descent (SGD) type algorithms are almost always used as training algorithms implying that the learned model, and consequently, its fairness properties are random. Hence, especially for crucial applications, it is imperative to construct Confidence Interval (CI) for the fairness of the learned model. In this work we provide CI for test unfairness when a group-fairness-aware, specifically, Disparate Impact (DI), and Disparate Mistreatment (DM) aware linear binary classifi",
    "link": "http://arxiv.org/abs/2304.13950",
    "context": "Title: Fairness Uncertainty Quantification: How certain are you that the model is fair?. (arXiv:2304.13950v1 [stat.ML])\nAbstract: Fairness-aware machine learning has garnered significant attention in recent years because of extensive use of machine learning in sensitive applications like judiciary systems. Various heuristics, and optimization frameworks have been proposed to enforce fairness in classification \\cite{del2020review} where the later approaches either provides empirical results or provides fairness guarantee for the exact minimizer of the objective function \\cite{celis2019classification}. In modern machine learning, Stochastic Gradient Descent (SGD) type algorithms are almost always used as training algorithms implying that the learned model, and consequently, its fairness properties are random. Hence, especially for crucial applications, it is imperative to construct Confidence Interval (CI) for the fairness of the learned model. In this work we provide CI for test unfairness when a group-fairness-aware, specifically, Disparate Impact (DI), and Disparate Mistreatment (DM) aware linear binary classifi",
    "path": "papers/23/04/2304.13950.json",
    "total_tokens": 892,
    "translated_title": "公平性不确定性量化: 您有多大把握模型是公平的?",
    "translated_abstract": "最近几年，由于机器学习在司法系统等敏感应用中的广泛使用，公平感知机器学习引起了广泛关注。提出了各种启发式和优化框架来强制实现分类中的公平性，其中后一种方法要么提供经验结果，要么为目标函数的精确最小化器提供公平性保证。在现代机器学习中，几乎总是使用随机梯度下降（SGD）类型的算法作为训练算法，这意味着学习的模型以及其公平性属性是随机的。因此，尤其是对于关键应用程序，必须构建置信区间（CI）以评估所学模型的公平性。在这项工作中，我们为测试不公平性提供了置信区间（CI），具体而言，是在考虑到群体公平性的前提下，即差异影响（DI）和不公平影响（DM）感知的线性二元分类模型。",
    "tldr": "本篇论文提出了一种针对机器学习模型公平性的不确定性量化方法，针对公平感知模型提供了置信区间（CI）来评估其测试不公平性。"
}