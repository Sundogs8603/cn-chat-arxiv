{
    "title": "Regularization of the policy updates for stabilizing Mean Field Games. (arXiv:2304.01547v1 [cs.AI])",
    "abstract": "This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm \\textit{Mean Field Proximal Policy Optimization (MF-PPO)}, and we empirically show the effectiveness of our method in the OpenSpiel framework.",
    "link": "http://arxiv.org/abs/2304.01547",
    "context": "Title: Regularization of the policy updates for stabilizing Mean Field Games. (arXiv:2304.01547v1 [cs.AI])\nAbstract: This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm \\textit{Mean Field Proximal Policy Optimization (MF-PPO)}, and we empirically show the effectiveness of our method in the OpenSpiel framework.",
    "path": "papers/23/04/2304.01547.json",
    "total_tokens": 872,
    "translated_title": "对策略更新的正则化以稳定均场博弈",
    "translated_abstract": "本文研究非合作多智能体强化学习(MARL)，其中多个智能体在同一环境中相互作用，目标是最大化个体回报。当智能体数量扩大时，由于许多智能体引入的非静止性，会产生挑战。为了解决这个问题，均场博弈（MFG）依靠对称性和同质性假设，近似具有很大群体的博弈。最近，深度强化学习被用于将MFG扩展到具有更多状态的博弈中。目前的方法依赖于平滑技术，如对q值或均场分布更新进行平均。本文提出了一种在均场策略上进行近似更新以稳定学习的不同方法。我们将我们的算法命名为均场近端策略优化（MF-PPO），并在OpenSpiel框架中经验性地展示了我们方法的有效性。",
    "tldr": "本文提出了一种均场近端策略优化算法（MF-PPO），以稳定深度强化学习在均场博弈中的应用，并在OpenSpiel框架中进行了实验验证。",
    "en_tdlr": "This paper proposes a Mean Field Proximal Policy Optimization (MF-PPO) algorithm to stabilize deep reinforcement learning in Mean Field Games, and empirically verifies its effectiveness in the OpenSpiel framework."
}