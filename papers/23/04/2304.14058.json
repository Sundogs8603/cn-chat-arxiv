{
    "title": "A Parameterized Theory of PAC Learning. (arXiv:2304.14058v1 [cs.CC])",
    "abstract": "Probably Approximately Correct (i.e., PAC) learning is a core concept of sample complexity theory, and efficient PAC learnability is often seen as a natural counterpart to the class P in classical computational complexity. But while the nascent theory of parameterized complexity has allowed us to push beyond the P-NP ``dichotomy'' in classical computational complexity and identify the exact boundaries of tractability for numerous problems, there is no analogue in the domain of sample complexity that could push beyond efficient PAC learnability.  As our core contribution, we fill this gap by developing a theory of parameterized PAC learning which allows us to shed new light on several recent PAC learning results that incorporated elements of parameterized complexity. Within the theory, we identify not one but two notions of fixed-parameter learnability that both form distinct counterparts to the class FPT -- the core concept at the center of the parameterized complexity paradigm -- and ",
    "link": "http://arxiv.org/abs/2304.14058",
    "context": "Title: A Parameterized Theory of PAC Learning. (arXiv:2304.14058v1 [cs.CC])\nAbstract: Probably Approximately Correct (i.e., PAC) learning is a core concept of sample complexity theory, and efficient PAC learnability is often seen as a natural counterpart to the class P in classical computational complexity. But while the nascent theory of parameterized complexity has allowed us to push beyond the P-NP ``dichotomy'' in classical computational complexity and identify the exact boundaries of tractability for numerous problems, there is no analogue in the domain of sample complexity that could push beyond efficient PAC learnability.  As our core contribution, we fill this gap by developing a theory of parameterized PAC learning which allows us to shed new light on several recent PAC learning results that incorporated elements of parameterized complexity. Within the theory, we identify not one but two notions of fixed-parameter learnability that both form distinct counterparts to the class FPT -- the core concept at the center of the parameterized complexity paradigm -- and ",
    "path": "papers/23/04/2304.14058.json",
    "total_tokens": 902,
    "translated_title": "PAC学习的参数化理论",
    "translated_abstract": "可能准确(即PAC)学习是样本复杂度理论的核心概念，高效的PAC可学习性经常被视为经典计算复杂度P类的自然对应物。然而，尽管参数化复杂性的崛起使我们能够超越经典计算复杂性的P-NP“二分法”，并确定许多问题的可处理边界，但在样本复杂性领域中没有类似的模型能够超越高效的PAC可学习性。我们的核心贡献是开发了一种参数化PAC学习理论，它使我们能够对几个最近包含参数化复杂性元素的PAC学习结果进行新的解释。在该理论中，我们确定了不止一种固定参数可学习性的概念，这两种概念都是参数化复杂性范式中核心概念FPT的独立对应物，并且允许我们以细粒度的方式分析高效和难以处理的PAC学习之间的界限。",
    "tldr": "本文提出了一种参数化PAC学习理论，确定了两种固定参数可学习性的概念，从而能够以细粒度的方式分析高效和难以处理的PAC学习之间的界限。",
    "en_tdlr": "This paper proposes a theory of parameterized PAC learning, which identifies two notions of fixed-parameter learnability and allows for fine-grained analysis of the boundary between efficient and intractable PAC learning."
}