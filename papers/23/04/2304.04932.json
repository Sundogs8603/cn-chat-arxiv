{
    "title": "Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])",
    "abstract": "Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been \"dequantized\" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa",
    "link": "http://arxiv.org/abs/2304.04932",
    "context": "Title: Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])\nAbstract: Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been \"dequantized\" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa",
    "path": "papers/23/04/2304.04932.json",
    "total_tokens": 977,
    "translated_title": "量子奇异值变换及量子机器学习算法的鲁棒去量子化方法研究",
    "translated_abstract": "在过去的几年中，已经有几种用于解决线性代数问题和特别是量子机器学习问题的量子算法被“去量子化”。这些去量子化结果通常在经典算法通过长度平方采样方法访问数据时成立。本文研究了这些去量子化结果的稳健性。我们引入了近似长度平方采样的概念，其中经典算法只能从接近理想分布的分布中进行采样。虽然量子算法在面对小扰动时本质上是鲁棒的，但当前的去量子化技术并不是。我们的主要技术贡献在于展示了如何将许多随机线性代数技术适应到这种更弱的假设下。然后，我们使用这些技术证明了最近由Chia、Gily\\'en、Li、Lin、Tang和Wang（JACM 2022）提出的低秩去量子化框架和用于spa的去量子化框架。",
    "tldr": "本文研究了量子机器学习算法的鲁棒去量子化方法。我们提出了近似长度平方采样的概念，并展示了如何将随机线性代数技术适应到这种更弱的假设下。我们使用这些技术证明了最近的低秩去量子化框架和spa去量子化框架。",
    "en_tdlr": "This paper investigates robust dequantization methods for quantum machine learning algorithms. The notion of approximate length-squared sampling is introduced and techniques from randomized linear algebra are adapted to work under this weaker assumption. The recent low-rank dequantization framework and the dequantization framework for spa are proven using these techniques."
}