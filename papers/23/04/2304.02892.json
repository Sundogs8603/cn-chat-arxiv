{
    "title": "Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients. (arXiv:2304.02892v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed framework for collaboratively training with privacy guarantees. In real-world scenarios, clients may have Non-IID data (local class imbalance) with poor annotation quality (label noise). The co-existence of label noise and class imbalance in FL's small local datasets renders conventional FL methods and noisy-label learning methods both ineffective. To address the challenges, we propose FedCNI without using an additional clean proxy dataset. It includes a noise-resilient local solver and a robust global aggregator. For the local solver, we design a more robust prototypical noise detector to distinguish noisy samples. Further to reduce the negative impact brought by the noisy samples, we devise a curriculum pseudo labeling method and a denoise Mixup training strategy. For the global aggregator, we propose a switching re-weighted aggregation method tailored to different learning periods. Extensive experiments demonstrate our method can substantiall",
    "link": "http://arxiv.org/abs/2304.02892",
    "context": "Title: Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients. (arXiv:2304.02892v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed framework for collaboratively training with privacy guarantees. In real-world scenarios, clients may have Non-IID data (local class imbalance) with poor annotation quality (label noise). The co-existence of label noise and class imbalance in FL's small local datasets renders conventional FL methods and noisy-label learning methods both ineffective. To address the challenges, we propose FedCNI without using an additional clean proxy dataset. It includes a noise-resilient local solver and a robust global aggregator. For the local solver, we design a more robust prototypical noise detector to distinguish noisy samples. Further to reduce the negative impact brought by the noisy samples, we devise a curriculum pseudo labeling method and a denoise Mixup training strategy. For the global aggregator, we propose a switching re-weighted aggregation method tailored to different learning periods. Extensive experiments demonstrate our method can substantiall",
    "path": "papers/23/04/2304.02892.json",
    "total_tokens": 977,
    "translated_title": "在具有嘈杂和异质客户端的联邦学习中谨慎学习",
    "translated_abstract": "联邦学习是一种分布式的框架，可在保护隐私的情况下进行协作训练。在现实场景中，客户端可能具有非独立同分布数据（本地类别不平衡）和低质量的注释（标签嘈杂）。FL 的小型本地数据集中存在标签噪声和类别不平衡的共存在，使传统 FL 方法和嘈杂标签学习方法均无效。为了解决这些问题，我们提出了 FedCNI，它不使用额外的干净代理数据集。它包括一个鲁棒的全局聚合器和一个抗噪局部求解器。对于局部求解器，我们设计了一个更稳健的样本嘈杂检测器来区分嘈杂样本。为了减少噪声样本带来的负面影响，我们设计了一个课程伪标签方法和一个去噪 Mixup 训练策略。对于全局聚合器，我们提出了一个针对不同学习阶段量身定制的切换加权聚合方法。广泛的实验表明，我们的方法可以显著提高嘈杂标签下的联邦学习的效果。",
    "tldr": "本文提出了一种名为FedCNI的联邦学习方法，该方法包括鲁棒的全局聚合器和抗噪局部求解器，可以有效处理在小型本地数据集中存在的标签噪声和类别不平衡的问题。",
    "en_tdlr": "This paper proposes a federated learning method, named FedCNI, which includes a robust global aggregator and a noise-resilient local solver to handle the challenge of label noise and class imbalance in small local datasets."
}