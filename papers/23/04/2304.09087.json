{
    "title": "MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])",
    "abstract": "Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo",
    "link": "http://arxiv.org/abs/2304.09087",
    "context": "Title: MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])\nAbstract: Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo",
    "path": "papers/23/04/2304.09087.json",
    "total_tokens": 1072,
    "translated_title": "MDDL: 基于强化学习的多通道Feed位置分配框架",
    "translated_abstract": "目前，位置分配系统的主流方法是利用强化学习模型为各通道的物品分配合适的位置，然后混合到Feed中。强化学习模型的训练使用两种数据：策略数据和随机数据。策略数据来自当前在线模型，它受到状态-动作对分布不均衡的困扰，导致训练过程中存在严重的高估问题。另一方面，随机数据提供了更均匀的状态-动作对分布，但在工业场景中很难获取，因为随机探索可能会对平台收入和用户体验产生负面影响。由于这两种数据具有不同的分布，因此设计一种有效的策略来利用两种数据以增强强化学习模型的训练效果已成为一个极具挑战性的问题。本研究提出了一种名为MDDL（多通道深度确定性策略梯度学习）的框架来解决上述问题。我们的框架旨在整合多种策略，以增强位置分配的RL模型训练。实验证明，在线和离线性能方面，MDDL表现优于一些最先进的方法。",
    "tldr": "本研究提出了一种名为MDDL的多通道深度确定性策略梯度学习框架，旨在整合多种策略，以增强位置分配的强化学习模型训练。该框架在在线和离线性能方面表现优于一些最先进的方法。",
    "en_tdlr": "This study proposes a framework named MDDL, a Multi-Channel Deep Deterministic Policy Gradient Learning framework that integrates multiple strategies to enhance the training of RL model for position allocation. The experiment results demonstrate that MDDL outperforms some state-of-the-art approaches in terms of both online and offline performance."
}