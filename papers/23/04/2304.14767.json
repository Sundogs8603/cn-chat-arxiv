{
    "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v1 [cs.CL])",
    "abstract": "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagat",
    "link": "http://arxiv.org/abs/2304.14767",
    "context": "Title: Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v1 [cs.CL])\nAbstract: Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagat",
    "path": "papers/23/04/2304.14767.json",
    "total_tokens": 865,
    "tldr": "通过研究信息流，我们揭示了自回归语言模型内部检索事实关联的机制，其中包括主体位置和关系位置的关键信息点以及用于提取属性的三个步骤。"
}