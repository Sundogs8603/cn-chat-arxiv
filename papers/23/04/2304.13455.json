{
    "title": "From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])",
    "abstract": "Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o",
    "link": "http://arxiv.org/abs/2304.13455",
    "context": "Title: From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])\nAbstract: Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o",
    "path": "papers/23/04/2304.13455.json",
    "total_tokens": 1069,
    "translated_title": "从混沌中迸发出秩序：为物体检测排序事件表示法",
    "translated_abstract": "如今，处理事件的顶尖深度神经网络在使用现成网络之前，首先将其转换为稠密的网格状输入表示。然而，传统上为任务选择适当的表示需要针对每个表示训练一个神经网络，并根据验证分数选择最佳表示，这非常耗时。在这项工作中，我们通过基于原始事件及其表示之间的Gromov-Wasserstein Discrepancy (GWD)选择最佳表示来消除这个瓶颈。它的计算速度大约比训练神经网络快200倍，同时在多个表示、网络骨干和数据集上保持事件表示法任务性能排名的一致性。这意味着找到具有高任务分数的表示相当于找到具有低GWD的表示。我们利用这一观察结果，首次对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示。我们的方法在Moving MNIST和N-Caltech101数据集上都优于最先进的基于事件的对象检测方法，在后者达到了83.0%的1%误报率下的mAP新的最高水平。",
    "tldr": "本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。",
    "en_tdlr": "This paper proposes a method of selecting the best event representation based on Gromov-Wasserstein Discrepancy, which can maintain the consistency of task performance ranking across multiple representations, network backbones, and datasets. Based on this method, the paper performs a hyperparameter search on a large family of event representations to select the most suitable representation for object detection, and achieves better results than state-of-the-art event-based object detection methods."
}