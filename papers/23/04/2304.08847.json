{
    "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])",
    "abstract": "Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack",
    "link": "http://arxiv.org/abs/2304.08847",
    "context": "Title: BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])\nAbstract: Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack",
    "path": "papers/23/04/2304.08847.json",
    "total_tokens": 1214,
    "translated_title": "BadVFL: 竖直联邦学习中的后门攻击",
    "translated_abstract": "联邦学习（FL）使多个参与者可以在不共享其数据的情况下协作地训练机器学习模型；相反，他们在本地训练自己的模型，并将更新发送到中央服务器进行聚合。根据数据在参与者之间的分布方式，FL可以分为水平（HFL）和竖直（VFL）。在VFL中，参与者共享相同的训练实例集，但仅托管整个特征空间的不同和非重叠子集。而在HFL中，每个参与者共享相同的特征集，而训练集被分为本地拥有的训练数据子集。尽管VFL越来越多地应用于金融欺诈检测等应用程序中，但很少有工作分析其安全性。本文重点研究VFL的鲁棒性，特别是后门攻击，其中对手试图在训练过程中操纵聚合模型以触发错误分类。在VFL上执行后门攻击可以创建严重的安全和隐私问题，因为它可以允许攻击者有针对性地控制模型预测的结果。为此，我们提出了一种新的VFL后门攻击框架，称为BadVFL，它可以有效地将后门注入VFL的训练过程中。我们的方法不仅考虑数据的特征，还适应于VFL中使用的不同中心度量。在三个数据集上进行的大量实验证明了我们的方法在保持干净数据的低失真的同时实现了高攻击成功率的有效性。",
    "tldr": "本文聚焦于竖直联邦学习中的后门攻击的鲁棒性问题，提出了一种新的后门攻击框架BadVFL，可以有效地将后门注入VFL的训练过程中，成功率高并且误分类率很低。",
    "en_tdlr": "The paper proposes a novel framework BadVFL for injecting backdoors into the training process of vertical federated learning (VFL), including adaptation to different centrality measures used in VFL for effectively achieving a high attack success rate while maintaining low distortion on clean data."
}