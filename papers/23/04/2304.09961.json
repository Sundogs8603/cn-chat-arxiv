{
    "title": "Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])",
    "abstract": "Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to ",
    "link": "http://arxiv.org/abs/2304.09961",
    "context": "Title: Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])\nAbstract: Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to ",
    "path": "papers/23/04/2304.09961.json",
    "total_tokens": 937,
    "translated_title": "边缘服务器上的深度神经网络调度",
    "translated_abstract": "深度神经网络(DNN)已经广泛用于各种视频分析任务中。这些任务要求实时响应，由于移动设备处理能力有限，支持此类实时分析的常见方法是将处理离线到边缘服务器。本文考察如何加速为多个客户端运行边缘服务器DNN。我们观察到，批处理多个DNN请求可以显著加速处理时间。基于此观察，我们首先设计了一种新的调度算法，以利用运行相同DNN的所有请求的批处理优势。这很有说服力，因为只有少数DNN，许多请求倾向于使用同一个DNN。我们的算法是通用的，可以支持不同的目标，如最小化完成时间或最大化及时率。然后，我们扩展我们的算法以处理使用不同DNN的具有或不具有共享层的请求。最后，我们开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。",
    "tldr": "本论文研究了如何加速为多个客户端运行边缘服务器DNN。批处理多个DNN请求可以显著加速处理时间。研究设计了一种新的调度算法，并开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。",
    "en_tdlr": "This paper examines how to speed up edge server DNN processing for multiple clients, using the observation that batching DNN requests can significantly decrease processing time. A novel scheduling algorithm is designed to exploit this advantage, along with a collaborative approach for multiple edge servers."
}