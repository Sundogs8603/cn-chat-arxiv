{
    "title": "Disentangling Neuron Representations with Concept Vectors. (arXiv:2304.09707v1 [cs.CV])",
    "abstract": "Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.",
    "link": "http://arxiv.org/abs/2304.09707",
    "context": "Title: Disentangling Neuron Representations with Concept Vectors. (arXiv:2304.09707v1 [cs.CV])\nAbstract: Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.",
    "path": "papers/23/04/2304.09707.json",
    "total_tokens": 770,
    "translated_title": "用概念向量解开神经元表示的纠缠",
    "translated_abstract": "机制可解释性旨在通过将神经网络分解为可解释单元来理解模型存储表示的方式。然而，多义神经元的出现使得解释单个神经元变得具有挑战性。这导致了在激活空间中寻找有意义的向量，称为概念向量，而不是单个神经元。本文主要贡献是一种方法，可以将多义神经元解开为封装不同特征的概念向量。我们的方法可以根据用户所需的概念分离级别搜索细粒度概念。分析表明，多义神经元可以解开为由神经元的线性组合组成的方向。我们的评估表明，所找到的概念向量编码了连续的、人类可理解的特征。",
    "tldr": "本文提出了一种方法，可以将多义神经元解开为封装不同特征的概念向量，这些向量编码了连贯的、人类可理解的特征。",
    "en_tdlr": "This paper presents a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features, which encode coherent and human-understandable representations."
}