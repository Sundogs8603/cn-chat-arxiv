{
    "title": "Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])",
    "abstract": "Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.",
    "link": "http://arxiv.org/abs/2304.01597",
    "context": "Title: Unsupervised Improvement of Factual Knowledge in Language Models. (arXiv:2304.01597v1 [cs.CL])\nAbstract: Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.",
    "path": "papers/23/04/2304.01597.json",
    "total_tokens": 782,
    "translated_title": "语言模型中无监督改进事实知识的方法",
    "translated_abstract": "掩盖语言建模（MLM）在预训练大型语言模型中扮演关键角色。但是由于高频词支配了MLM目标，因此不利于学习事实知识。在本文中，我们提出了一种方法，以一种完全无监督的方式影响MLM预训练，从而提高语言模型在各种知识密集型任务中的性能。我们强制语言模型优先考虑信息性单词。实验表明，所提出的方法可以显著提高预训练语言模型在关闭书本的情况下的事实回忆，问答，情感分析和自然语言推理等任务的性能。",
    "tldr": "本文提出了一种无监督的方法来改进预训练的语言模型，从而提高其在各种知识密集型任务中的性能，包括事实回忆，问答，情感分析和自然语言推理等。这种方法可以通过优先考虑信息性单词来影响掩盖语言建模。",
    "en_tdlr": "This paper proposes an unsupervised method to improve the pretraining of language models, which can enhance their performance on various knowledge-intensive tasks, including factual recall, question answering, sentiment analysis, and natural language inference. By prioritizing informative words in masked language modeling, this approach influences language modeling in a fully unsupervised way."
}