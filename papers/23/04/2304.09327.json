{
    "title": "Federated Alternate Training (FAT): Leveraging Unannotated Data Silos in Federated Segmentation for Medical Imaging. (arXiv:2304.09327v1 [cs.CV])",
    "abstract": "Federated Learning (FL) aims to train a machine learning (ML) model in a distributed fashion to strengthen data privacy with limited data migration costs. It is a distributed learning framework naturally suitable for privacy-sensitive medical imaging datasets. However, most current FL-based medical imaging works assume silos have ground truth labels for training. In practice, label acquisition in the medical field is challenging as it often requires extensive labor and time costs. To address this challenge and leverage the unannotated data silos to improve modeling, we propose an alternate training-based framework, Federated Alternate Training (FAT), that alters training between annotated data silos and unannotated data silos. Annotated data silos exploit annotations to learn a reasonable global segmentation model. Meanwhile, unannotated data silos use the global segmentation model as a target model to generate pseudo labels for self-supervised learning. We evaluate the performance of ",
    "link": "http://arxiv.org/abs/2304.09327",
    "context": "Title: Federated Alternate Training (FAT): Leveraging Unannotated Data Silos in Federated Segmentation for Medical Imaging. (arXiv:2304.09327v1 [cs.CV])\nAbstract: Federated Learning (FL) aims to train a machine learning (ML) model in a distributed fashion to strengthen data privacy with limited data migration costs. It is a distributed learning framework naturally suitable for privacy-sensitive medical imaging datasets. However, most current FL-based medical imaging works assume silos have ground truth labels for training. In practice, label acquisition in the medical field is challenging as it often requires extensive labor and time costs. To address this challenge and leverage the unannotated data silos to improve modeling, we propose an alternate training-based framework, Federated Alternate Training (FAT), that alters training between annotated data silos and unannotated data silos. Annotated data silos exploit annotations to learn a reasonable global segmentation model. Meanwhile, unannotated data silos use the global segmentation model as a target model to generate pseudo labels for self-supervised learning. We evaluate the performance of ",
    "path": "papers/23/04/2304.09327.json",
    "total_tokens": 987,
    "translated_title": "联邦交替训练：在联邦分割中利用未注释数据库(arXiv:2304.09327v1 [cs.CV])",
    "translated_abstract": "联邦学习(Fl)旨在分布式地训练机器学习(ML)模型，以加强数据隐私，同时减少数据迁移成本。它是一种适用于隐私敏感的医学图像数据集的分布式学习框架。然而，目前大多数基于FL的医学图像研究都假定数据库具有地面真值标签来进行训练。在实践中，医学领域的标签获取是具有挑战性的，因为它经常需要大量的精力和时间成本。为解决这一挑战并利用未注释数据库来改进建模，我们提出了一种基于替代训练的框架——联邦交替训练(FAT)，它在带注释数据库和未带注释数据库之间交替训练。带注释的数据库利用注释来学习一个合理的全局分割模型。与此同时，未注释的数据库使用全局分割模型作为目标模型，为自我监督学习生成伪标签。我们评估了FAT在数据保护和性能方面的有效性。",
    "tldr": "本文提出了一种联邦交替训练(FAT)框架，它可以在带有和不带有地面真值标签的数据库之间交替训练，利用未标注的数据来辅助模型学习，适用于医学图像领域。"
}