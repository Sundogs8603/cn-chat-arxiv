{
    "title": "Learning to Extrapolate: A Transductive Approach. (arXiv:2304.14329v1 [cs.LG])",
    "abstract": "Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.",
    "link": "http://arxiv.org/abs/2304.14329",
    "context": "Title: Learning to Extrapolate: A Transductive Approach. (arXiv:2304.14329v1 [cs.LG])\nAbstract: Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.",
    "path": "papers/23/04/2304.14329.json",
    "total_tokens": 884,
    "translated_title": "学习外推：一种传导方法",
    "translated_abstract": "机器学习系统，特别是拥有过度参数化的深度神经网络，可以推广到从训练数据相同分布中提取的新测试实例。然而，在支持外的测试点上评估时，它们表现不佳。在这项工作中，我们解决开发机器学习系统的问题，使其保留过度参数化函数逼近器的能力，同时在可能时使推广到支持外的测试点外推。通过注意在某些条件下，“传导”重新参数化可以将支持外的外推问题转换为支持内组合泛化问题的问题，从而实现这一目的。我们提出了一种基于双线性嵌入的简单策略，以实现这种类型的组合泛化，从而在某些条件下解决了支持外推问题。我们将实例化一个适用于各种监督学习和模仿学习任务的简单实用算法。",
    "tldr": "该论文提出了一种解决在支持外进行推广的问题的传导方法，该方法通过“传导”重新参数化将支持外的外推问题转换为支持内组合泛化问题的问题，从而让机器学习系统保留过度参数化函数逼近器的能力，并能够在某些条件下进行外推。",
    "en_tdlr": "This paper proposes a transductive approach to tackle the problem of extrapolating to out-of-support points, which converts the problem into one of within-support combinatorial generalization under certain conditions through reparameterization, allowing machine learning systems to retain their power while enabling extrapolation under certain conditions. The authors instantiate a simple algorithm applicable to various supervised and imitation learning tasks."
}