{
    "title": "Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])",
    "abstract": "Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \\emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \\emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\\Omega(2^N)$ evaluations are typically required in TNLS for \\emph{reaching} the objective reduction in the neighborhood",
    "link": "http://arxiv.org/abs/2304.12875",
    "context": "Title: Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])\nAbstract: Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \\emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \\emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\\Omega(2^N)$ evaluations are typically required in TNLS for \\emph{reaching} the objective reduction in the neighborhood",
    "path": "papers/23/04/2304.12875.json",
    "total_tokens": 1043,
    "translated_title": "交替局部枚举(TnALE): 用较少的评估解决张量网络结构搜索问题",
    "translated_abstract": "张量网络(TN)是机器学习中强大的框架，但选择一个好的TN模型，即TN结构搜索(TN-SS)，是一项具有挑战性和计算密集型的任务。最近的方法TNLS ~ \\cite {li2022permutation} 在这个任务中显示出了有希望的结果，但它的计算效率仍然是无法承受的，需要太多评估目标函数的次数。我们提出了TnALE，一种新的算法，通过局部枚举交替更新每个与结构相关的变量，与TNLS相比，大大减少了评估次数。我们从理论上研究了TNLS和TnALE的下降步骤，证明如果在每个邻域中达到了足够的目标函数降低，那么两种算法都可以实现线性收敛度，直到一个常数。我们还比较了TNLS和TnALE的评估效率，揭示了在TNLS中通常需要Ω(2 ^ N)个评估才能在邻域内达到目标降低。",
    "tldr": "提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。",
    "en_tdlr": "TnALE algorithm is proposed to solve tensor network structure search problem by updating each structure-related variable alternately through local enumeration, significantly reducing the number of function evaluations compared to TNLS. It’s theoretically proved that both TnALE and TNLS can achieve linear convergence up to a constant if there is sufficient reduction of the objective in each neighborhood. Meanwhile, TnALE requires fewer evaluations compared with TNLS."
}