{
    "title": "Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])",
    "abstract": "Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their",
    "link": "http://arxiv.org/abs/2304.06768",
    "context": "Title: Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])\nAbstract: Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their",
    "path": "papers/23/04/2304.06768.json",
    "total_tokens": 858,
    "translated_title": "通过坐标变换改进梯度方法：应用于量子机器学习",
    "translated_abstract": "无论是经典的还是量子的机器学习算法，都大量依赖基于梯度的优化算法，如梯度下降等。总体性能取决于局部最小值和荒原高原的出现，这会减缓计算速度并导致非最优解。实际应用中，这会导致人工智能应用的计算和能源成本激增。本文介绍了一种通用策略，以加速和改善这些方法的总体性能，从而减轻了荒原高原和局部最小值的影响。我们的方法基于坐标变换，有点类似于变分旋转，在参数空间中添加了额外的方向，这些方向取决于成本函数本身，并且允许更有效地探索配置景观。我们已通过增强多种量子机器学习算法来测试我们的方法，得到了非常显着的改进。",
    "tldr": "本文介绍了一种通过坐标变换来加速梯度优化算法、改善荒原高原和局部最小值影响的通用策略，有效提高了多种量子机器学习算法的性能。",
    "en_tdlr": "This paper introduces a generic strategy to accelerate gradient optimization algorithms and alleviate the impact of barren plateaus and local minima by coordinate transformations. Our approach has been benchmarked by boosting a number of quantum machine learning algorithms, achieving significant improvements."
}