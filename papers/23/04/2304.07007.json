{
    "title": "Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])",
    "abstract": "How does one measure \"ability to understand language\"? If it is a person's ability that is being measured, this is a question that almost never poses itself in an unqualified manner: Whatever formal test is applied, it takes place on the background of the person's language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). Computer programs do not have this background. What does that mean for the applicability of formal tests of language understanding? I argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of \"artificial language understanding\". To do such tests systematically, I propose to use \"Dialogue Games\" -- constructed activities that provide a situational embedding for language use. I describe a taxonomy of Dialogue Game types, linked to a model of underlying capabilites that ",
    "link": "http://arxiv.org/abs/2304.07007",
    "context": "Title: Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])\nAbstract: How does one measure \"ability to understand language\"? If it is a person's ability that is being measured, this is a question that almost never poses itself in an unqualified manner: Whatever formal test is applied, it takes place on the background of the person's language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). Computer programs do not have this background. What does that mean for the applicability of formal tests of language understanding? I argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of \"artificial language understanding\". To do such tests systematically, I propose to use \"Dialogue Games\" -- constructed activities that provide a situational embedding for language use. I describe a taxonomy of Dialogue Game types, linked to a model of underlying capabilites that ",
    "path": "papers/23/04/2304.07007.json",
    "total_tokens": 842,
    "translated_title": "基于对话游戏的语言理解基准测量：动机、分类和策略",
    "translated_abstract": "如何测量“理解语言的能力”？如果是衡量一个人的能力，这几乎从未以不合格的方式提出：无论应用何种正式测试，都是在人们日常社交实践的语言使用背景下进行的，所测量的是一种专业的语言理解（例如第二语言或书面技术语言）。计算机程序没有这样的背景。这对于正式的语言理解测试的适用性意味着什么？我认为这些测试需要补充嵌入在实践中的语言使用测试，以得出更全面的“人工语言理解”评估。为了系统地进行这样的测试，我提议使用“对话游戏”，构建提供情境嵌入的活动。我描述了一种与潜在能力模型相关联的对话游戏类型分类。",
    "tldr": "本论文提出通过构建不同类型的对话游戏进行实践性语言使用的测试，以评价计算机程序的“人工语言理解”能力，从而补充形式化语言理解测试的不足之处。",
    "en_tdlr": "This paper proposes using Dialogue Games to complement formal tests of language understanding by providing situational embedding for language use and evaluating computer programs' \"artificial language understanding\" in a more comprehensive way."
}