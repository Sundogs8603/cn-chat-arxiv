{
    "title": "Spectral Gap Regularization of Neural Networks. (arXiv:2304.03096v1 [stat.ML])",
    "abstract": "We introduce Fiedler regularization, a novel approach for regularizing neural networks that utilizes spectral/graphical information. Existing regularization methods often focus on penalizing weights in a global/uniform manner that ignores the connectivity structure of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical motivation for this approach via spectral graph theory. We demonstrate several useful properties of the Fiedler value that make it useful as a regularization tool. We provide an approximate, variational approach for faster computation during training. We provide an alternative formulation of this framework in the form of a structurally weighted $\\text{L}_1$ penalty, thus linking our approach to sparsity induction. We provide uniform generalization error bounds for Fiedler regularization via a Rademacher complexity analysis. We performed experiments on datasets that compare F",
    "link": "http://arxiv.org/abs/2304.03096",
    "context": "Title: Spectral Gap Regularization of Neural Networks. (arXiv:2304.03096v1 [stat.ML])\nAbstract: We introduce Fiedler regularization, a novel approach for regularizing neural networks that utilizes spectral/graphical information. Existing regularization methods often focus on penalizing weights in a global/uniform manner that ignores the connectivity structure of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical motivation for this approach via spectral graph theory. We demonstrate several useful properties of the Fiedler value that make it useful as a regularization tool. We provide an approximate, variational approach for faster computation during training. We provide an alternative formulation of this framework in the form of a structurally weighted $\\text{L}_1$ penalty, thus linking our approach to sparsity induction. We provide uniform generalization error bounds for Fiedler regularization via a Rademacher complexity analysis. We performed experiments on datasets that compare F",
    "path": "papers/23/04/2304.03096.json",
    "total_tokens": 940,
    "translated_title": "神经网络的谱间隙正则化",
    "translated_abstract": "本文引入了Fiedler正则化，这是一种利用谱/图形信息对神经网络进行正则化的新方法。现有的正则化方法常常通过全局/均匀地惩罚权重来实现，忽略了神经网络的连通性结构。我们提出利用神经网络底层图的Fiedler值作为正则化工具。我们通过谱图理论提供了这种方法的理论动机。我们证明了Fiedler值的几个有用属性，使其成为正则化工具。我们提供了一种近似的变分方法，以便在训练期间更快地计算。我们提供了该框架的另一种形式，这是一种结构加权的 $\\text{L}_1$ 惩罚，因此将我们的方法与稀疏感应联系起来。我们通过Rademacher复杂性分析提供了Fiedler正则化的统一泛化误差界限。我们对数据集进行了实验比较。",
    "tldr": "本文介绍了一种利用谱/图形信息进行神经网络正则化的新方法，即Fiedler正则化。通过使用神经网络底层图的Fiedler值作为正则化工具，我们提供了一种结构加权的 $\\text{L}_1$ 惩罚并提供统一泛化误差界限的分析，这使得我们的方法在许多数据集上都取得了良好的性能。",
    "en_tdlr": "This paper presents a novel approach for regularizing neural networks, called Fiedler regularization, which utilizes spectral/graphical information of the underlying graph. By using the Fiedler value of the network's graph, this approach provides a structurally weighted $\\text{L}_1$ penalty and a unified generalization error bound analysis. Experimental results on various datasets demonstrate the effectiveness of this method."
}