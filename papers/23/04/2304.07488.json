{
    "title": "SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training. (arXiv:2304.07488v1 [cs.LG])",
    "abstract": "Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that",
    "link": "http://arxiv.org/abs/2304.07488",
    "context": "Title: SalientGrads: Sparse Models for Communication Efficient and Data Aware Distributed Federated Training. (arXiv:2304.07488v1 [cs.LG])\nAbstract: Federated learning (FL) enables the training of a model leveraging decentralized data in client sites while preserving privacy by not collecting data. However, one of the significant challenges of FL is limited computation and low communication bandwidth in resource limited edge client nodes. To address this, several solutions have been proposed in recent times including transmitting sparse models and learning dynamic masks iteratively, among others. However, many of these methods rely on transmitting the model weights throughout the entire training process as they are based on ad-hoc or random pruning criteria. In this work, we propose Salient Grads, which simplifies the process of sparse training by choosing a data aware subnetwork before training, based on the model-parameter's saliency scores, which is calculated from the local client data. Moreover only highly sparse gradients are transmitted between the server and client models during the training process unlike most methods that",
    "path": "papers/23/04/2304.07488.json",
    "total_tokens": 1092,
    "translated_title": "SalientGrads：面向通信效率和数据感知的分布式联邦学习的稀疏模型",
    "translated_abstract": "联邦学习可以在客户端站点利用分散的数据训练模型，同时通过不收集数据来保护隐私。然而，联邦学习面临的一大挑战是资源有限的边缘客户端节点计算和通信带宽不足。为了解决这个问题，近年来提出了几种解决方案，包括传输稀疏模型和迭代学习动态掩码等。然而，许多这些方法在整个训练过程中都需要传输模型权重，因为它们基于特定的剪枝标准。在本研究中，我们提出了 SalientGrads，它可以在训练之前基于本地客户端数据计算模型参数的显着性分数，从而选择一个数据感知的子网络来简化稀疏训练过程。此外，在训练过程中，服务器和客户端模型之间仅传输高度稀疏的梯度，而不像大多数方法需要传输整个模型。实验结果表明，SalientGrads 可以在减少通信成本高达 90% 和计算成本高达 60% 的同时，实现与最先进的联邦学习方法相当或更好的模型准确度。",
    "tldr": "SalientGrads是一个用于联邦学习的稀疏模型，通过选择一个数据感知的子网络和仅传输高度稀疏的梯度来简化稀疏训练过程，可在减少通信成本高达90%和计算成本高达60%的同时，实现与最先进的联邦学习方法相当或更好的模型准确度。",
    "en_tdlr": "SalientGrads is a sparse model for federated learning, which simplifies the process of sparse training by choosing a data-aware subnetwork before training and transmitting only highly sparse gradients during the training process. It can achieve comparable or better model accuracy to state-of-the-art FL methods while reducing communication costs by up to 90% and computation costs by up to 60%."
}