{
    "title": "Unsupervised Speech Representation Pooling Using Vector Quantization. (arXiv:2304.03940v1 [cs.LG])",
    "abstract": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "link": "http://arxiv.org/abs/2304.03940",
    "context": "Title: Unsupervised Speech Representation Pooling Using Vector Quantization. (arXiv:2304.03940v1 [cs.LG])\nAbstract: With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "path": "papers/23/04/2304.03940.json",
    "total_tokens": 857,
    "translated_title": "无监督语音表示池化的矢量量化方法",
    "translated_abstract": "随着大规模自监督模型生成通用语音表示，将一个模型应用到多个下游任务已成为一种事实标准。然而，池化问题仍然存在；语音表示的长度固有地是可变的。尽管忽略了语音的特性，例如不同长度的音素，但通常使用简单的平均池化方法。因此，我们设计了一种新颖的池化方法，通过矢量量化来压缩声学上相似的表示，与基于注意力的池化方法不同，不需要额外的训练。此外，我们评估了各种无监督池化方法在各种自监督模型上的表现。我们收集了散落在语音和文本领域的不同方法，并在各种任务上进行评估：关键字识别、说话人识别、意图分类和情感识别。最后，我们对我们的方法进行定量和定性分析，将其与有监督的池化方法进行比较。",
    "tldr": "该论文提出了一种新颖的无监督语音表示池化方法，通过矢量量化压缩声学上相似的表示，不需要额外的训练，并在多个下游任务上进行了评估和分析。",
    "en_tdlr": "This paper proposes a novel unsupervised speech representation pooling method which compresses acoustically similar representations via vector quantization without additional training, and evaluates and analyzes it on multiple downstream tasks."
}