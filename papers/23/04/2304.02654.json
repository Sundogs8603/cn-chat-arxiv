{
    "title": "Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks. (arXiv:2304.02654v1 [cs.LG])",
    "abstract": "Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of artificial intelligence tasks. Often consisting of hundreds of millions, if not hundreds of billion parameters, these DNNs are too large to be deployed to, or efficiently run on resource-constrained devices such as mobile phones or IoT microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this paper, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs, ",
    "link": "http://arxiv.org/abs/2304.02654",
    "context": "Title: Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks. (arXiv:2304.02654v1 [cs.LG])\nAbstract: Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of artificial intelligence tasks. Often consisting of hundreds of millions, if not hundreds of billion parameters, these DNNs are too large to be deployed to, or efficiently run on resource-constrained devices such as mobile phones or IoT microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this paper, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs, ",
    "path": "papers/23/04/2304.02654.json",
    "total_tokens": 902,
    "translated_title": "采用两个监督器用于大规模远程深度神经网络的有效使用",
    "translated_abstract": "近几十年来，大规模深度神经网络的出现在各种人工智能任务中取得了人类竞争水平的性能。由于这些深度神经网络通常包含数十亿甚至数百亿参数，因此无法部署到资源受限的设备（如手机或物联网微控制器）上或高效运行。因此，使用大规模深度神经网络的系统需要通过网络调用相应的模型，导致托管和运行大规模远程模型的成本相当高，这些成本通常按使用次数收费。在本文中，我们提出了一种新的架构BiSupervised，它在依赖于大规模远程深度神经网络之前，尝试在小规模本地模型上进行预测，而一个DNN主管监控此预测过程并确定易于进行本地预测的输入。对于这些输入，无需调用远程模型，从而节省成本。",
    "tldr": "本文提出了一种名为BiSupervised的新架构，使用两个监督器在调用大规模远程深度神经网络之前，对小规模本地模型进行预测，通过这种方式避免不必要的网络调用，可以节省成本。",
    "en_tdlr": "This paper proposes a novel architecture called BiSupervised, which adopts two supervisors to make predictions on a small-scale local model before relying on a large remote DNN, in order to avoid unnecessary network calls and save costs."
}