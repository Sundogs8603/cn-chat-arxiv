{
    "title": "Robust nonlinear set-point control with reinforcement learning. (arXiv:2304.10277v1 [eess.SY])",
    "abstract": "There has recently been an increased interest in reinforcement learning for nonlinear control problems. However standard reinforcement learning algorithms can often struggle even on seemingly simple set-point control problems. This paper argues that three ideas can improve reinforcement learning methods even for highly nonlinear set-point control problems: 1) Make use of a prior feedback controller to aid amplitude exploration. 2) Use integrated errors. 3) Train on model ensembles. Together these ideas lead to more efficient training, and a trained set-point controller that is more robust to modelling errors and thus can be directly deployed to real-world nonlinear systems. The claim is supported by experiments with a real-world nonlinear cascaded tank process and a simulated strongly nonlinear pH-control system.",
    "link": "http://arxiv.org/abs/2304.10277",
    "context": "Title: Robust nonlinear set-point control with reinforcement learning. (arXiv:2304.10277v1 [eess.SY])\nAbstract: There has recently been an increased interest in reinforcement learning for nonlinear control problems. However standard reinforcement learning algorithms can often struggle even on seemingly simple set-point control problems. This paper argues that three ideas can improve reinforcement learning methods even for highly nonlinear set-point control problems: 1) Make use of a prior feedback controller to aid amplitude exploration. 2) Use integrated errors. 3) Train on model ensembles. Together these ideas lead to more efficient training, and a trained set-point controller that is more robust to modelling errors and thus can be directly deployed to real-world nonlinear systems. The claim is supported by experiments with a real-world nonlinear cascaded tank process and a simulated strongly nonlinear pH-control system.",
    "path": "papers/23/04/2304.10277.json",
    "total_tokens": 883,
    "translated_title": "基于强化学习的鲁棒非线性设定点控制",
    "translated_abstract": "最近，强化学习在非线性控制问题中受到了越来越多的关注。然而，标准的强化学习算法经常在看似简单的设定点控制问题上遇到困难。本文认为三个思想可改进强化学习方法，即 1）利用先前反馈控制器来帮助振幅探索。2）使用积分误差。3）对模型集合进行训练。这些思想共同导致更高效的训练，以及一个经过训练的设定点控制器，它对建模误差更加鲁棒，因此可以直接应用于真实的非线性系统中。该论文通过实验支持了这一论断，实验使用了真实世界的非线性级联罐进程和模拟的强非线性 pH 控制系统。",
    "tldr": "本文提出了三种思想来改进强化学习方法，即利用先前反馈控制器来帮助振幅探索，使用积分误差，对模型集合进行训练。这些想法可提高训练效率，使得训练出来的设定点控制器更加鲁棒，可以直接用于真实的非线性系统中。",
    "en_tdlr": "This paper proposes three ideas to improve reinforcement learning methods for highly nonlinear set-point control problems: 1) Make use of a prior feedback controller to aid amplitude exploration. 2) Use integrated errors. 3) Train on model ensembles. These ideas can lead to more efficient training and a more robust trained set-point controller that can be deployed to real-world nonlinear systems."
}