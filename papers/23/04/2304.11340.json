{
    "title": "Semantic Specialization for Knowledge-based Word Sense Disambiguation. (arXiv:2304.11340v1 [cs.CL])",
    "abstract": "A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the \\textit{sense} and \\textit{context} embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-",
    "link": "http://arxiv.org/abs/2304.11340",
    "context": "Title: Semantic Specialization for Knowledge-based Word Sense Disambiguation. (arXiv:2304.11340v1 [cs.CL])\nAbstract: A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the \\textit{sense} and \\textit{context} embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-",
    "path": "papers/23/04/2304.11340.json",
    "total_tokens": 863,
    "translated_title": "基于语义专业化的知识驱动词义消歧研究",
    "translated_abstract": "基于预训练语言模型计算“sense”和上下文嵌入向量相似度的方法是目前词义消歧任务的一种有前途的方法。本文提出了一种基于词汇知识的词义消歧语义专业化方法，通过调整上下文嵌入向量，将语义相关的“sense”和上下文彼此加近，将不相关的“sense”彼此远离。采用Attract-Repel优化方式优化sense对，采用自训练的方式优化context-sense对，并控制嵌入向量与原来的差距。该方法在英语、意大利语和中文三种语言的知识驱动词义消歧任务中表现优异，取得了最好的成绩。",
    "tldr": "本文提出了一种基于词汇知识的词义消歧语义专业化方法，通过调整上下文嵌入向量，将语义相关的“sense”和上下文彼此加近，将不相关的“sense”彼此远离。",
    "en_tdlr": "The paper proposes a semantic specialization for knowledge-based Word Sense Disambiguation (WSD) in which contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The proposed method outperformed previous studies that adapt contextualized embeddings and achieved state-of-the-art performance on knowledge-based WSD tasks for three languages."
}