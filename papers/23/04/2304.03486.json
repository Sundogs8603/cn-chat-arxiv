{
    "title": "Can we learn better with hard samples?. (arXiv:2304.03486v1 [cs.CV])",
    "abstract": "In deep learning, mini-batch training is commonly used to optimize network parameters. However, the traditional mini-batch method may not learn the under-represented samples and complex patterns in the data, leading to a longer time for generalization. To address this problem, a variant of the traditional algorithm has been proposed, which trains the network focusing on mini-batches with high loss. The study evaluates the effectiveness of the proposed training using various deep neural networks trained on three benchmark datasets (CIFAR-10, CIFAR-100, and STL-10). The deep neural networks used in the study are ResNet-18, ResNet-50, Efficient Net B4, EfficientNetV2-S, and MobilenetV3-S. The experimental results showed that the proposed method can significantly improve the test accuracy and speed up the convergence compared to the traditional mini-batch training method. Furthermore, we introduce a hyper-parameter delta ({\\delta}) that decides how many mini-batches are considered for trai",
    "link": "http://arxiv.org/abs/2304.03486",
    "context": "Title: Can we learn better with hard samples?. (arXiv:2304.03486v1 [cs.CV])\nAbstract: In deep learning, mini-batch training is commonly used to optimize network parameters. However, the traditional mini-batch method may not learn the under-represented samples and complex patterns in the data, leading to a longer time for generalization. To address this problem, a variant of the traditional algorithm has been proposed, which trains the network focusing on mini-batches with high loss. The study evaluates the effectiveness of the proposed training using various deep neural networks trained on three benchmark datasets (CIFAR-10, CIFAR-100, and STL-10). The deep neural networks used in the study are ResNet-18, ResNet-50, Efficient Net B4, EfficientNetV2-S, and MobilenetV3-S. The experimental results showed that the proposed method can significantly improve the test accuracy and speed up the convergence compared to the traditional mini-batch training method. Furthermore, we introduce a hyper-parameter delta ({\\delta}) that decides how many mini-batches are considered for trai",
    "path": "papers/23/04/2304.03486.json",
    "total_tokens": 913,
    "translated_title": "我们可以通过难样本来更好地学习吗?",
    "translated_abstract": "在深度学习中，常用mini-batch训练优化网络参数。然而，传统的mini-batch方法可能无法学习数据中的次数较少的样本和复杂的模式，导致泛化时间更长。为了解决这个问题，提出了一种改进传统算法的方案，该方案重点训练具有高损失的mini-batch网络。该研究评估了在三个基准数据集（CIFAR-10，CIFAR-100和STL-10）上训练的各种深度神经网络（ResNet-18，ResNet-50，Efficient Net B4，EfficientNetV2-S和MobilenetV3-S）的效果。实验结果表明，与传统的mini-batch训练方法相比，所提出的方法可以显着提高测试准确性并加快收敛速度。此外，我们引入了一个超参数delta ({\\delta})，它决定有多少个mini-batch被考虑用于训练。",
    "tldr": "该研究提出一种改进传统mini-batch算法的方案，该方案重点训练具有高损失的mini-batch网络，其在三个基准数据集上的实验结果表明，该方法可以显着提高测试准确性并加快收敛速度。",
    "en_tdlr": "This study proposes a variant of the traditional mini-batch algorithm, which trains the network focusing on mini-batches with high loss, to improve learning of under-represented samples and complex patterns. Experimental results on three benchmark datasets show significant improvements in test accuracy and convergence speed compared to traditional methods."
}