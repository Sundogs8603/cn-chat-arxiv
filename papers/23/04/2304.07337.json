{
    "title": "Learning to Learn Group Alignment: A Self-Tuning Credo Framework with Multiagent Teams. (arXiv:2304.07337v1 [cs.AI])",
    "abstract": "Mixed incentives among a population with multiagent teams has been shown to have advantages over a fully cooperative system; however, discovering the best mixture of incentives or team structure is a difficult and dynamic problem. We propose a framework where individual learning agents self-regulate their configuration of incentives through various parts of their reward function. This work extends previous work by giving agents the ability to dynamically update their group alignment during learning and by allowing teammates to have different group alignment. Our model builds on ideas from hierarchical reinforcement learning and meta-learning to learn the configuration of a reward function that supports the development of a behavioral policy. We provide preliminary results in a commonly studied multiagent environment and find that agents can achieve better global outcomes by self-tuning their respective group alignment parameters.",
    "link": "http://arxiv.org/abs/2304.07337",
    "context": "Title: Learning to Learn Group Alignment: A Self-Tuning Credo Framework with Multiagent Teams. (arXiv:2304.07337v1 [cs.AI])\nAbstract: Mixed incentives among a population with multiagent teams has been shown to have advantages over a fully cooperative system; however, discovering the best mixture of incentives or team structure is a difficult and dynamic problem. We propose a framework where individual learning agents self-regulate their configuration of incentives through various parts of their reward function. This work extends previous work by giving agents the ability to dynamically update their group alignment during learning and by allowing teammates to have different group alignment. Our model builds on ideas from hierarchical reinforcement learning and meta-learning to learn the configuration of a reward function that supports the development of a behavioral policy. We provide preliminary results in a commonly studied multiagent environment and find that agents can achieve better global outcomes by self-tuning their respective group alignment parameters.",
    "path": "papers/23/04/2304.07337.json",
    "total_tokens": 839,
    "translated_title": "学习群体调整：具有多智能体团队的自适应Credo框架。",
    "translated_abstract": "多个智能体团队中混合激励比完全合作系统具有优势，然而，发现最佳的激励组合或团队结构是一个困难而动态的问题。我们提出了一个框架，其中个体学习智能体通过其奖励函数的不同部分自我调节其激励配置。该模型扩展了以前的工作，让智能体能够在学习过程中动态更新其团队对齐，并允许队友具有不同的团队对齐。我们的模型借鉴了层次强化学习和元学习的思想，以学习支持行为策略发展的奖励函数的配置。我们在一个常见的多智能体环境中提供了初步结果，并发现智能体通过自我调整其各自的组对准参数可以实现更好的全局结果。",
    "tldr": "本研究提出了一个自适应Credo框架，通过元学习和层次强化学习来支持智能体对群体对齐参数进行自我调节，从而实现更好的多智能体团队结果。",
    "en_tdlr": "This study proposes a self-tuning Credo framework that uses meta-learning and hierarchical reinforcement learning to support individual learning agents in self-regulating their configuration of incentives, achieving better global outcomes for multiagent teams."
}