{
    "title": "Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])",
    "abstract": "Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten",
    "link": "http://arxiv.org/abs/2304.12944",
    "context": "Title: Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])\nAbstract: Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten",
    "path": "papers/23/04/2304.12944.json",
    "total_tokens": 903,
    "translated_title": "潜在生成模型中的潜在遍历作为潜在流的潜在路线",
    "translated_abstract": "尽管深度生成模型的最近进展已经取得了显著进展，但它们的潜在空间的基本结构仍然很不好理解，因此执行语义上有意义的潜在遍历的任务仍然是一个开放的研究挑战。大多数之前的工作旨在通过线性建模潜在结构，并找到相应的线性方向，从而产生“解缠”的代数。在这项工作中，我们提议改为使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行。这些潜在景观受到物理学、最优运输和神经科学的启发，被作为物理上现实的偏微分方程来学习，从而允许它们在空间和时间上具有灵活性。为了实现解缠，同时学习了多个势能，并通过分类器进行约束，使其具有明显差异且在语义上自我一致。",
    "tldr": "该论文使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行，以实现解缠，并通过分类器进行约束。",
    "en_tdlr": "This paper proposes to model latent structures with a learned dynamic potential landscape, allowing for semantic traversals as the flow of samples down the landscape's gradient. Multiple potentials are learned and constrained by a classifier to achieve disentanglement."
}