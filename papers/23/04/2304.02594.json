{
    "title": "Competitive plasticity to reduce the energetic costs of learning. (arXiv:2304.02594v1 [cs.NE])",
    "abstract": "The brain is not only constrained by energy needed to fuel computation, but it is also constrained by energy needed to form memories. Experiments have shown that learning simple conditioning tasks already carries a significant metabolic cost. Yet, learning a task like MNIST to 95% accuracy appears to require at least 10^{8} synaptic updates. Therefore the brain has likely evolved to be able to learn using as little energy as possible. We explored the energy required for learning in feedforward neural networks. Based on a parsimonious energy model, we propose two plasticity restricting algorithms that save energy: 1) only modify synapses with large updates, and 2) restrict plasticity to subsets of synapses that form a path through the network. Combining these two methods leads to substantial energy savings while only incurring a small increase in learning time. In biology networks are often much larger than the task requires. In particular in that case, large savings can be achieved. Th",
    "link": "http://arxiv.org/abs/2304.02594",
    "context": "Title: Competitive plasticity to reduce the energetic costs of learning. (arXiv:2304.02594v1 [cs.NE])\nAbstract: The brain is not only constrained by energy needed to fuel computation, but it is also constrained by energy needed to form memories. Experiments have shown that learning simple conditioning tasks already carries a significant metabolic cost. Yet, learning a task like MNIST to 95% accuracy appears to require at least 10^{8} synaptic updates. Therefore the brain has likely evolved to be able to learn using as little energy as possible. We explored the energy required for learning in feedforward neural networks. Based on a parsimonious energy model, we propose two plasticity restricting algorithms that save energy: 1) only modify synapses with large updates, and 2) restrict plasticity to subsets of synapses that form a path through the network. Combining these two methods leads to substantial energy savings while only incurring a small increase in learning time. In biology networks are often much larger than the task requires. In particular in that case, large savings can be achieved. Th",
    "path": "papers/23/04/2304.02594.json",
    "total_tokens": 890,
    "translated_title": "降低学习的能量成本的竞争性可塑性",
    "translated_abstract": "大脑不仅在支持计算方面受能量限制，而且在形成记忆的能量方面也受限。实验表明，学习简单的条件反射任务已经产生了很大的代谢成本。然而，学习像MNIST这样的任务到95％的准确度似乎至少需要10^8个突触更新。因此，大脑可能已经进化出能够以尽可能少的能量学习的能力。我们探讨了前馈神经网络的学习能量需求。基于一个简约的能量模型，我们提议了两个节能的可塑性限制算法：1）仅修改大更新的突触，2）将可塑性限制在形成网络路径的突触子集上。将这两种方法结合起来可以大大节省能量，同时只增加很少的学习时间。在生物网络中，网络通常比任务要求的大得多。特别是在这种情况下，可以实现大幅度的节能。",
    "tldr": "该论文探讨了大脑如何通过竞争性可塑性降低学习的能量成本，并提出了两个省能的可塑性限制算法，结合起来可以大幅降低能量消耗。",
    "en_tdlr": "This paper explores how the brain reduces the energy cost of learning through competitive plasticity, and proposes two energy-saving plasticity restricting algorithms. When combined, these methods can significantly reduce energy consumption."
}