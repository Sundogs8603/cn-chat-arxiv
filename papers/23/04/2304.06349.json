{
    "title": "Neural State-Space Models: Empirical Evaluation of Uncertainty Quantification. (arXiv:2304.06349v1 [cs.LG])",
    "abstract": "Effective quantification of uncertainty is an essential and still missing step towards a greater adoption of deep-learning approaches in different applications, including mission-critical ones. In particular, investigations on the predictive uncertainty of deep-learning models describing non-linear dynamical systems are very limited to date. This paper is aimed at filling this gap and presents preliminary results on uncertainty quantification for system identification with neural state-space models. We frame the learning problem in a Bayesian probabilistic setting and obtain posterior distributions for the neural network's weights and outputs through approximate inference techniques. Based on the posterior, we construct credible intervals on the outputs and define a surprise index which can effectively diagnose usage of the model in a potentially dangerous out-of-distribution regime, where predictions cannot be trusted.",
    "link": "http://arxiv.org/abs/2304.06349",
    "context": "Title: Neural State-Space Models: Empirical Evaluation of Uncertainty Quantification. (arXiv:2304.06349v1 [cs.LG])\nAbstract: Effective quantification of uncertainty is an essential and still missing step towards a greater adoption of deep-learning approaches in different applications, including mission-critical ones. In particular, investigations on the predictive uncertainty of deep-learning models describing non-linear dynamical systems are very limited to date. This paper is aimed at filling this gap and presents preliminary results on uncertainty quantification for system identification with neural state-space models. We frame the learning problem in a Bayesian probabilistic setting and obtain posterior distributions for the neural network's weights and outputs through approximate inference techniques. Based on the posterior, we construct credible intervals on the outputs and define a surprise index which can effectively diagnose usage of the model in a potentially dangerous out-of-distribution regime, where predictions cannot be trusted.",
    "path": "papers/23/04/2304.06349.json",
    "total_tokens": 834,
    "translated_title": "基于神经态空间模型的不确定性量化的实证评估",
    "translated_abstract": "有效量化不确定性是实现深度学习在各种应用中，包括关键任务中更广泛采用的一个关键且仍然缺失的步骤。尤其是描述非线性动态系统的深度学习模型的预测不确定性的研究目前非常有限。本文旨在填补这一空白，并提出了神经态空间模型系统识别中的不确定性量化的初步结果。我们将学习问题框架放在贝叶斯概率设置中，并通过近似推理技术获得神经网络的权重和输出的后验分布。根据后验分布，我们构建输出的可信区间，并定义一种惊异指数，该指数可以有效地诊断模型在潜在危险的分布外情况下的使用情况，这时的预测是不能被信任的。",
    "tldr": "本文以贝叶斯概率设置为框架，提出了神经态空间模型系统识别的不确定性量化方法，给出了后验分布、可信区间及惊异指数，以有效甄别模型在分布外情况下的使用可能带来的危险性。",
    "en_tdlr": "This paper proposes a Bayesian approach to neural state-space models for system identification which provides posterior distributions, credible intervals and a surprise index to effectively diagnose the danger of using the model in potentially dangerous out-of-distribution situations."
}