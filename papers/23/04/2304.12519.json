{
    "title": "RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])",
    "abstract": "Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \\textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \\emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\\ie quality and diversity), \\textsc{RenderDiffusion} can achieve ",
    "link": "http://arxiv.org/abs/2304.12519",
    "context": "Title: RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])\nAbstract: Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \\textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \\emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\\ie quality and diversity), \\textsc{RenderDiffusion} can achieve ",
    "path": "papers/23/04/2304.12519.json",
    "total_tokens": 751,
    "translated_title": "RenderDiffusion: 文本生成作为图像生成",
    "translated_abstract": "扩散模型已成为文本生成的新生成范式。考虑到文本的离散分类特性，在本文中，我们提出了一种新颖的扩散方法——\\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。我们的关键思路是将目标文本呈现为包含视觉语言内容的\"字形图像\"。这样，条件化的文本生成可以被形式化为一个字形图像生成任务，然后自然地将连续扩散模型应用于离散文本。",
    "tldr": "本文提出了一种新的扩散方法——\\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。它将连续扩散模型应用于离散文本并实现了条件文本生成作为字形图像生成问题。",
    "en_tdlr": "This paper proposes a novel diffusion approach, \\textsc{RenderDiffusion}, for text generation via text-guided image generation. By rendering the target text as a glyph image, continuous diffusion models can be applied to discrete texts for conditional text generation."
}