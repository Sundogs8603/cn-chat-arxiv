{
    "title": "Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])",
    "abstract": "Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes",
    "link": "http://arxiv.org/abs/2304.13861",
    "context": "Title: Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])\nAbstract: Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes",
    "path": "papers/23/04/2304.13861.json",
    "total_tokens": 1012,
    "translated_title": "一个提示和几个示例就足够了吗？使用GPT-4进行数据增强在低资源分类任务中",
    "translated_abstract": "在复杂的低资源领域中，获取和注释数据可能是昂贵和耗时的。我们使用GPT-4和ChatGPT通过简单的提示将小型标记数据集扩充为合成数据集，应用于三个不同的分类任务中，复杂程度各异。对于每个任务，我们随机选择了500个文本作为基本样本，生成了5,000个新的合成样本。我们探索了两种增强策略：一种保留原始标签分布，另一种平衡分布。使用逐步变大的训练样本量，我们分别在真实数据和合成数据上训练和评估了一个1.1亿参数的多语言语言模型。我们还在测试集上测试了GPT-4和ChatGPT的零-shot设置。我们发现GPT-4和ChatGPT在所有任务中都具有很强的零-shot性能。我们发现，使用合成样本增强的数据在下游任务中产生了良好的性能，尤其在识别罕见类别等低资源设置方面表现突出。",
    "tldr": "本文使用GPT-4和ChatGPT对低资源分类任务进行数据增强，通过简单的提示将小型标记数据集扩充为合成数据集，在保留原始标签分布或平衡分布的情况下，产生了良好的下游性能。在测试集上，GPT-4和ChatGPT表现出出色的零-shot性能，尤其在低资源设置中能够较好地识别罕见类别。",
    "en_tdlr": "This paper utilizes GPT-4 and ChatGPT for data augmentation in low-resource classification tasks. The augmented data, generated via simple prompts, preserves or balances the original label distribution and results in good downstream performance, particularly in identifying rare classes. GPT-4 and ChatGPT also demonstrate strong zero-shot performance on test sets."
}