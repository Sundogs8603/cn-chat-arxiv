{
    "title": "Pretrained Language Models as Visual Planners for Human Assistance. (arXiv:2304.09179v1 [cs.CV])",
    "abstract": "To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., \"make a shelf\", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as \"sand shelf\", \"paint shelf\", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We dem",
    "link": "http://arxiv.org/abs/2304.09179",
    "context": "Title: Pretrained Language Models as Visual Planners for Human Assistance. (arXiv:2304.09179v1 [cs.CV])\nAbstract: To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., \"make a shelf\", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as \"sand shelf\", \"paint shelf\", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We dem",
    "path": "papers/23/04/2304.09179.json",
    "total_tokens": 1039,
    "translated_title": "预训练语言模型作为人类辅助视觉计划者",
    "translated_abstract": "为了实现多模态AI助手指导用户完成复杂多步骤目标的进展，本研究提出了视觉辅助计划（VPA）的任务。给定自然语言简要描述的目标，例如“制作书架”，以及用户迄今为止的视频进展，VPA的目标是获得一个计划，即一系列行动，如“砂光书架”、“涂漆书架”等，以实现目标。这需要评估用户在未经修剪的视频中的进展，并与底层目标的要求相关联，即行动的相关性和其中的排序依赖关系。因此，这需要处理长时间的视频历史记录和任意复杂的行动依赖性。为了解决这些问题，我们将VPA分解为视频行动分割和预测。我们将预测步骤公式化为多模态序列建模问题，并提出了基于视觉语言模型的计划者（VLaMP），其中利用预训练的LMs作为序列模型。我们在两个数据集（Epic Kitchen和Charades-Ego）上展示了VLaMP的有效性。我们的实验结果表明，VLaMP在准确性、效率和泛化方面优于现有的方法。",
    "tldr": "本研究提出了视觉辅助计划（VPA）的任务，利用预训练语言模型作为序列模型，在视频行动分割和预测方面优于现有的方法，来实现多模态AI助手指导用户完成复杂多步骤目标的进展。"
}