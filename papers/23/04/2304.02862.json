{
    "title": "Learning to Learn with Indispensable Connections. (arXiv:2304.02862v1 [cs.LG])",
    "abstract": "Meta-learning aims to solve unseen tasks with few labelled instances. Nevertheless, despite its effectiveness for quick learning in existing optimization-based methods, it has several flaws. Inconsequential connections are frequently seen during meta-training, which results in an over-parameterized neural network. Because of this, meta-testing observes unnecessary computations and extra memory overhead. To overcome such flaws. We propose a novel meta-learning method called Meta-LTH that includes indispensible (necessary) connections. We applied the lottery ticket hypothesis technique known as magnitude pruning to generate these crucial connections that can effectively solve few-shot learning problem. We aim to perform two things: (a) to find a sub-network capable of more adaptive meta-learning and (b) to learn new low-level features of unseen tasks and recombine those features with the already learned features during the meta-test phase. Experimental results show that our proposed Met-",
    "link": "http://arxiv.org/abs/2304.02862",
    "context": "Title: Learning to Learn with Indispensable Connections. (arXiv:2304.02862v1 [cs.LG])\nAbstract: Meta-learning aims to solve unseen tasks with few labelled instances. Nevertheless, despite its effectiveness for quick learning in existing optimization-based methods, it has several flaws. Inconsequential connections are frequently seen during meta-training, which results in an over-parameterized neural network. Because of this, meta-testing observes unnecessary computations and extra memory overhead. To overcome such flaws. We propose a novel meta-learning method called Meta-LTH that includes indispensible (necessary) connections. We applied the lottery ticket hypothesis technique known as magnitude pruning to generate these crucial connections that can effectively solve few-shot learning problem. We aim to perform two things: (a) to find a sub-network capable of more adaptive meta-learning and (b) to learn new low-level features of unseen tasks and recombine those features with the already learned features during the meta-test phase. Experimental results show that our proposed Met-",
    "path": "papers/23/04/2304.02862.json",
    "total_tokens": 934,
    "translated_title": "学习不可或缺的连接来进行元学习",
    "translated_abstract": "元学习旨在通过少量标记实例来解决未知任务。然而，尽管现有基于优化的方法在快速学习方面非常有效，但它们存在一些缺陷。元训练中经常出现不必要的连接，导致神经网络过度参数化。因此，在元测试过程中，观察到不必要的计算和额外的内存开销。为了克服这些问题，我们提出了一种新的元学习方法，称为Meta-LTH，它包括不可缺少的连接。我们应用了被称为重要性剪枝的彩票假设技术来生成这些关键的连接，可以有效地解决少样本学习问题。我们的目标是实现两件事：(a) 寻找一个更适应元学习的子网络，(b) 在元测试阶段学习未见过任务的新低级特征并将这些特征与已经学习的特征重新组合起来。实验结果表明，我们提出的Meta-LTH方法在标准少样本学习基准测试上优于现有的元学习算法。",
    "tldr": "该论文提出了一种新的元学习方法Meta-LTH，该方法包括不可缺少的连接，通过重要性剪枝技术生成关键的连接，能够有效地解决少样本学习问题，并在标准少样本学习基准测试上优于现有的元学习算法。",
    "en_tdlr": "The paper proposes a novel meta-learning method, Meta-LTH, which includes indispensable connections generated by magnitude pruning technique, effectively solves few-shot learning problem and outperforms existing meta-learning algorithms on standard few-shot learning benchmarks."
}