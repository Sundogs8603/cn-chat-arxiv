{
    "title": "Towards Open-Vocabulary Video Instance Segmentation. (arXiv:2304.01715v1 [cs.CV])",
    "abstract": "Video Instance Segmentation(VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset(LV-VIS), that contains well-annotated objects from 1,212 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Vision-Language Transformer, MindVLT, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive ex",
    "link": "http://arxiv.org/abs/2304.01715",
    "context": "Title: Towards Open-Vocabulary Video Instance Segmentation. (arXiv:2304.01715v1 [cs.CV])\nAbstract: Video Instance Segmentation(VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset(LV-VIS), that contains well-annotated objects from 1,212 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Vision-Language Transformer, MindVLT, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive ex",
    "path": "papers/23/04/2304.01715.json",
    "total_tokens": 961,
    "translated_title": "开放词汇视频实例分割的探索",
    "translated_abstract": "视频实例分割（VIS）旨在从一组封闭的训练类别中对视频中的对象进行分割和分类，缺乏处理真实世界中新类别的泛化能力。为了解决这个问题，本文提出了三个方案。首先，我们引入了开放词汇视频实例分割的新任务，旨在同时从开放集类别中对视频中的对象进行分割、跟踪和分类，包括训练期间未见过的新类别。其次，为了评测开放词汇实例分割，我们收集了包含1,212个不同类别的大规模词汇视频实例分割数据集（LV-VIS），显著超出了现有数据集的类别规模一个数量级以上。第三，我们提出了一种高效的记忆驱动视觉语言变换器MindVLT，以实现近实时端到端的开放词汇视频实例分割。广泛的实验结果表明，我们提出的MindVLT在封闭集和开放集视频实例分割任务上显著优于现有方法。",
    "tldr": "本文提出了新任务--开放词汇视频实例分割，并收集了大规模的LV-VIS数据集，同时提出了高效的MindVLT方法，能够以近实时的速度实现开放集视频实例分割任务，相比现有方法有显著的提升。",
    "en_tdlr": "This paper proposes a novel task of open-vocabulary video instance segmentation, introduces a large vocabulary dataset, and proposes an efficient method called MindVLT which achieves near real-time open-set video instance segmentation, significantly outperforming existing methods on both closed-set and open-set Video Instance Segmentation tasks."
}