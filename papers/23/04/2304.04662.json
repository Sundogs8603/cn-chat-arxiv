{
    "title": "SELFormer: Molecular Representation Learning via SELFIES Language Models. (arXiv:2304.04662v2 [q-bio.QM] UPDATED)",
    "abstract": "Automated computational analysis of the vast chemical space is critical for numerous fields of research such as drug discovery and material science. Representation learning techniques have recently been employed with the primary objective of generating compact and informative numerical expressions of complex data. One approach to efficiently learn molecular representations is processing string-based notations of chemicals via natural language processing (NLP) algorithms. Majority of the methods proposed so far utilize SMILES notations for this purpose; however, SMILES is associated with numerous problems related to validity and robustness, which may prevent the model from effectively uncovering the knowledge hidden in the data. In this study, we propose SELFormer, a transformer architecture-based chemical language model that utilizes a 100% valid, compact and expressive notation, SELFIES, as input, in order to learn flexible and high-quality molecular representations. SELFormer is pre-",
    "link": "http://arxiv.org/abs/2304.04662",
    "context": "Title: SELFormer: Molecular Representation Learning via SELFIES Language Models. (arXiv:2304.04662v2 [q-bio.QM] UPDATED)\nAbstract: Automated computational analysis of the vast chemical space is critical for numerous fields of research such as drug discovery and material science. Representation learning techniques have recently been employed with the primary objective of generating compact and informative numerical expressions of complex data. One approach to efficiently learn molecular representations is processing string-based notations of chemicals via natural language processing (NLP) algorithms. Majority of the methods proposed so far utilize SMILES notations for this purpose; however, SMILES is associated with numerous problems related to validity and robustness, which may prevent the model from effectively uncovering the knowledge hidden in the data. In this study, we propose SELFormer, a transformer architecture-based chemical language model that utilizes a 100% valid, compact and expressive notation, SELFIES, as input, in order to learn flexible and high-quality molecular representations. SELFormer is pre-",
    "path": "papers/23/04/2304.04662.json",
    "total_tokens": 890,
    "translated_title": "SELFormer：利用SELFIES语言模型进行分子表示学习",
    "translated_abstract": "对广阔的化学空间进行自动化的计算分析对于药物发现和材料科学等许多研究领域至关重要。最近，表示学习技术被广泛应用于生成复杂数据的紧凑且信息丰富的数值表达式。一种有效学习分子表示的方法是利用自然语言处理（NLP）算法处理基于字符串的化学标注。目前，大多数方法利用SMILES标注实现此目的; 然而，SMILES标注存在许多与有效性和鲁棒性相关的问题，这可能会阻止模型有效地揭示数据中隐藏的知识。在本研究中，我们提出了SELFormer，一种基于变压器架构的化学语言模型，它使用100％有效，紧凑且表达丰富的符号SELFIES作为输入，以学习灵活且高质量的分子表示。 SELFormer在大型化学数据集上进行预训练，并在多项分子下游任务上进行了评估，表现优于其他最先进的方法。",
    "tldr": "本文提出了一种基于SELFIES语言模型的SELFormer架构，利用该架构可有效学习灵活、高质量的分子表示，相比其他同类方法表现更佳。",
    "en_tdlr": "This paper proposes a SELFormer architecture based on the SELFIES language model, which can effectively learn flexible and high-quality molecular representations, outperforming other similar methods."
}