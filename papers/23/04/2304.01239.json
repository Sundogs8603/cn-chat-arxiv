{
    "title": "Online Distillation with Continual Learning for Cyclic Domain Shifts. (arXiv:2304.01239v1 [cs.CV])",
    "abstract": "In recent years, online distillation has emerged as a powerful technique for adapting real-time deep neural networks on the fly using a slow, but accurate teacher model. However, a major challenge in online distillation is catastrophic forgetting when the domain shifts, which occurs when the student model is updated with data from the new domain and forgets previously learned knowledge. In this paper, we propose a solution to this issue by leveraging the power of continual learning methods to reduce the impact of domain shifts. Specifically, we integrate several state-of-the-art continual learning methods in the context of online distillation and demonstrate their effectiveness in reducing catastrophic forgetting. Furthermore, we provide a detailed analysis of our proposed solution in the case of cyclic domain shifts. Our experimental results demonstrate the efficacy of our approach in improving the robustness and accuracy of online distillation, with potential applications in domains ",
    "link": "http://arxiv.org/abs/2304.01239",
    "context": "Title: Online Distillation with Continual Learning for Cyclic Domain Shifts. (arXiv:2304.01239v1 [cs.CV])\nAbstract: In recent years, online distillation has emerged as a powerful technique for adapting real-time deep neural networks on the fly using a slow, but accurate teacher model. However, a major challenge in online distillation is catastrophic forgetting when the domain shifts, which occurs when the student model is updated with data from the new domain and forgets previously learned knowledge. In this paper, we propose a solution to this issue by leveraging the power of continual learning methods to reduce the impact of domain shifts. Specifically, we integrate several state-of-the-art continual learning methods in the context of online distillation and demonstrate their effectiveness in reducing catastrophic forgetting. Furthermore, we provide a detailed analysis of our proposed solution in the case of cyclic domain shifts. Our experimental results demonstrate the efficacy of our approach in improving the robustness and accuracy of online distillation, with potential applications in domains ",
    "path": "papers/23/04/2304.01239.json",
    "total_tokens": 961,
    "translated_title": "基于循环领域变化的在线知识蒸馏与持续学习",
    "translated_abstract": "近年来，使用缓慢但准确的教师模型进行在线深度神经网络调整的在线知识蒸馏技术已经成为一种强大的技术。然而，在线知识蒸馏的一个重要挑战是当领域发生变化时出现的灾难性遗忘，这是当学生模型使用新域的数据进行更新时遗忘之前学习的知识所导致。在本文中，我们提出了一种解决这个问题的方法，通过利用持续学习方法的优势来降低领域变化的影响。具体而言，我们将几种最先进的持续学习方法集成到在线知识蒸馏的上下文中，并展示了它们在减少灾难性遗忘方面的有效性。此外，我们在循环领域变化的情况下对我们提出的解决方案进行了详细分析。我们的实验结果表明，在提高在线知识蒸馏的鲁棒性和准确性方面，我们的方法非常有效，并且具有潜在的应用价值。",
    "tldr": "本文提出了一种在线知识蒸馏与持续学习相结合的方法，旨在解决领域变化引起的灾难性遗忘问题。实验结果表明这种方法有效地提高了在线知识蒸馏的鲁棒性和准确性，具有潜在的应用价值。",
    "en_tdlr": "This paper proposes a method that combines online distillation and continual learning to solve the problem of catastrophic forgetting caused by domain shifts. Experimental results demonstrate that this method effectively improves the robustness and accuracy of online distillation with potential applications in various domains."
}