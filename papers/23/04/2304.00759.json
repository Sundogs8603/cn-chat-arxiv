{
    "title": "FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line ",
    "link": "http://arxiv.org/abs/2304.00759",
    "context": "Title: FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line ",
    "path": "papers/23/04/2304.00759.json",
    "total_tokens": 987,
    "translated_title": "FedIN：用于模型异构的联邦中间层学习",
    "translated_abstract": "联邦学习（FL）使得边缘设备能够合作训练全局共享模型，同时在本地和私密地保留训练数据。然而，在FL中一个普遍但不切实际的假设是参与边缘设备拥有相同的必需资源并共享相同的全局模型架构。本研究提出了一种名为Federated Intermediate Layers Learning（FedIN）的新型FL方法，支持异构模型而不使用任何公共数据集。FedIN中的训练模型分为三部分，包括提取器、中间层和分类器。提取器和分类器的模型结构在所有设备中都相同，以保持中间层特征的一致性，而中间层的架构可以根据异构设备的资源容量而变化。为了利用特征知识，我们提出了IN训练，以IN标准化为基础训练中间层。在图像分类任务上的实验结果表明了我们方法在精度和效率方面的有效性。",
    "tldr": "FedIN是一种新型的联邦学习方法，支持异构模型，无需公共数据集。在FedIN中，提取器和分类器的模型结构在所有设备中都相同，而中间层的架构可以根据异构设备的资源容量而变化。IN训练可用于利用特征知识，实验结果表明了该方法在图像分类任务上的有效性。",
    "en_tdlr": "FedIN is a novel federated learning method that supports heterogeneous models without the need for a public dataset. In FedIN, the model architecture of the extractor and classifier is the same across all devices while the intermediate layers can vary based on the resources of heterogeneous devices. IN training can be used to exploit knowledge from features, and experimental results demonstrate its effectiveness for image classification tasks in terms of both accuracy and efficiency."
}