{
    "title": "Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])",
    "abstract": "This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach uses the best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them, and feed them to linear classifiers. We introduce two novel features, chordality, and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA in English and Russian, typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LMs during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behavior of monolingual LMs in the acceptability classification task, provide insights into",
    "link": "http://arxiv.org/abs/2304.01680",
    "context": "Title: Can BERT eat RuCoLA? Topological Data Analysis to Explain. (arXiv:2304.01680v1 [cs.CL])\nAbstract: This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach uses the best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them, and feed them to linear classifiers. We introduce two novel features, chordality, and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA in English and Russian, typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LMs during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behavior of monolingual LMs in the acceptability classification task, provide insights into",
    "path": "papers/23/04/2304.01680.json",
    "total_tokens": 1054,
    "translated_title": "BERT可以吞咽RuCoLA吗？拓扑数据分析来解释。",
    "translated_abstract": "本文研究了Transformer语言模型如何擅长Fine-Tune任务捕捉语言特征，并用Topological Data Analysis（TDA）方法从注意力矩阵中构建有向图，从中提取拓扑特征，将其送入线性分类器中。在英语和俄语两个拓扑上不同的数据集CoLA和RuCoLA上进行了实验，提出了多种黑盒内省技术，以便深入了解LM对语言的表示和使用。",
    "tldr": "本文研究了Transformer语言模型Fine-Tune任务中如何捕捉语言特征，并通过Topological Data Analysis方法构建有向图从中提取拓扑特征。实验发现，以弦性和匹配数为新特征的TDA-based分类器优于Fine-Tune基线模型，这些结果有助于理解LMs在可接受性分类任务中的行为。",
    "en_tdlr": "This paper investigates how Transformer language models fine-tuned for acceptability classification capture linguistic features and proposes a method using topological data analysis to extract topological features from attention matrices. Novel features, such as chordality and the matching number, are introduced and shown to outperform fine-tuning baselines. The study provides insights into monolingual LMs behavior in acceptability classification and their representation and use of language."
}