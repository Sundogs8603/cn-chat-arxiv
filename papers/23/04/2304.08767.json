{
    "title": "Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])",
    "abstract": "Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a",
    "link": "http://arxiv.org/abs/2304.08767",
    "context": "Title: Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])\nAbstract: Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a",
    "path": "papers/23/04/2304.08767.json",
    "total_tokens": 916,
    "translated_title": "基于掩码语言模型的文本对抗样本检测",
    "translated_abstract": "对抗攻击是机器学习模型在关键安全应用中可靠部署的严重威胁，稍微修改输入即可误导当前模型进行错误预测。最近，大量研究表明，对抗样本往往偏离正常样本的基础数据流形，而预训练的掩码语言模型可以适应正常的NLP数据流形。为了探索如何将掩码语言模型用于对抗性检测，我们提出了一种新颖的文本对抗例子检测方法，即基于掩码语言模型的检测（MLMD），它可以通过探索掩码语言模型引起的流形变化，在正常样本和对抗样本之间产生明显可区分的信号。MLMD具有即插即用的使用方法（即无需重新训练受害模型）用于对抗性防御，而且不受分类任务、受害模型结构和待防御的数据集的影响。",
    "tldr": "通过探索掩码语言模型引起的流形变化，我们提出了一种插即用的文本对抗例子检测方法，可以在保持对分类任务、模型结构和数据集无依赖的前提下，有效地检测到对抗例子。",
    "en_tdlr": "We proposed a plug and play textual adversarial example detection method, named Masked Language Model-based Detection (MLMD), that can produce distinguishable signals between normal and adversarial examples by exploring the changes in manifolds induced by the masked language model, without being reliant on classification tasks, model architectures, and datasets."
}