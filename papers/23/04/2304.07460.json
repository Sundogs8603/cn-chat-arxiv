{
    "title": "Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha",
    "link": "http://arxiv.org/abs/2304.07460",
    "context": "Title: Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])\nAbstract: Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha",
    "path": "papers/23/04/2304.07460.json",
    "total_tokens": 881,
    "translated_title": "具有内在隐私保护的高效通信和节能无线联邦学习",
    "translated_abstract": "联邦学习（FL）是一种协同学习框架，使边缘设备在保留原始数据的同时协同学习全局模型。虽然FL避免了从本地数据集泄漏直接信息，但仍可能从共享模型推断出敏感信息。为解决FL中的隐私问题，利用差分隐私（DP）机制提供正式的隐私保证。然而，使用无线边缘部署FL时，确保客户端级别的DP面临着重大挑战。在本文中，我们提出了一种名为带稀疏化的私有联邦边缘学习（PFELS）的新型无线FL方案，以提供具有内在信道噪声的客户端级别DP保证，同时降低通信和能量开销并提高模型精度。PFELS的关键思想是使每个设备先压缩其模型更新，然后根据无线信道自适应设计压缩模型更新的发送功率。",
    "tldr": "本文提出了一种名为PFELS的无线FL方案，通过先压缩模型更新再自适应地设计发送功率来提供客户端级别DP保证，并降低通信和能量开销并提高模型精度。",
    "en_tdlr": "This paper proposes a wireless FL scheme called PFELS, which provides client-level DP guarantee by compressing model updates and adaptively designing transmit power, and reduces communication and energy overhead while improving model accuracy."
}