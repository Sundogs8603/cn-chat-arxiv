{
    "title": "Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])",
    "abstract": "Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula",
    "link": "http://arxiv.org/abs/2304.04164",
    "context": "Title: Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])\nAbstract: Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula",
    "path": "papers/23/04/2304.04164.json",
    "total_tokens": 939,
    "translated_title": "基于梯度稀疏化和差分隐私的高效无线联合学习",
    "translated_abstract": "联合学习使分布式客户端在不共享原始数据的情况下协同训练机器学习模型。但是，由于上传模型而泄漏私有信息。此外，随着模型大小的增加，由于有限的传输带宽，训练延迟增加，同时使用差分隐私（DP）保护时模型性能会下降。在本文中，我们提出了一种基于梯度稀疏化和差分隐私的无线联合学习框架，以提高训练效率而不损失收敛性能。具体而言，我们首先设计了一个随机稀疏化算法，在每个客户端的本地训练中保留一部分梯度元素，从而缓解了DP引起的性能下降，并减少了无线信道上传输的参数数量。然后，我们通过建模非凸FL问题分析了所提出算法的收敛度界。接下来，我们提出了一个分布式联合优化问题，使用Alternating Direction Method of Multipliers（ADMM）解决其优化问题。",
    "tldr": "本文提出了一种基于梯度稀疏化和差分隐私的无线联合学习框架，使用随机稀疏化算法缓解DP引起的性能下降，并减少上传的参数数量，提高训练效率而不损失收敛性能。",
    "en_tdlr": "This paper proposes a wireless federated learning framework based on gradient sparsification and differential privacy, which mitigates performance degradation induced by DP and reduces the number of transmitted parameters with a random sparsification algorithm, leading to improved training efficiency without sacrificing convergence performance."
}