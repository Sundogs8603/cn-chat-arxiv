{
    "title": "To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])",
    "abstract": "Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \\textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove",
    "link": "http://arxiv.org/abs/2304.09355",
    "context": "Title: To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])\nAbstract: Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \\textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove",
    "path": "papers/23/04/2304.09355.json",
    "total_tokens": 960,
    "translated_title": "压缩与否——自监督学习与信息论:一篇综述",
    "translated_abstract": "深度神经网络在监督学习任务中表现出了卓越的性能，但需要大量的标注数据。自监督学习提供了一个替代范例，使得模型可以在没有明确标签的情况下学习。信息论在理解和优化深度神经网络方面起着关键作用。特别地，信息瓶颈原则被应用于在监督设置中优化压缩和相关信息保存之间的权衡。然而，自监督学习中的最佳信息目标仍然不清楚。在本文中，我们从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。我们将现有研究融合成一个一致的框架，研究了最近的自监督方法，并确定了研究机会和挑战。此外，我们还讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。",
    "tldr": "本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。",
    "en_tdlr": "This paper provides a review of various self-supervised learning approaches from an information-theoretic standpoint, proposes a unified framework for the self-supervised information-theoretic learning problem, discusses the role of compressibility and compression algorithms in self-supervised learning, and highlights potential future directions."
}