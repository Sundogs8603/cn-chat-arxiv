{
    "title": "Deep reinforcement learning applied to an assembly sequence planning problem with user preferences. (arXiv:2304.06567v1 [cs.LG])",
    "abstract": "Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user's preferences and (2) total assembly time duration. The user's preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios:",
    "link": "http://arxiv.org/abs/2304.06567",
    "context": "Title: Deep reinforcement learning applied to an assembly sequence planning problem with user preferences. (arXiv:2304.06567v1 [cs.LG])\nAbstract: Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user's preferences and (2) total assembly time duration. The user's preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios:",
    "path": "papers/23/04/2304.06567.json",
    "total_tokens": 920,
    "translated_title": "利用深度强化学习解决装配序列规划问题",
    "translated_abstract": "深度强化学习（DRL）在解决复杂制造决策问题方面展示了其潜力，特别是在没有训练数据的情况下随着实际操作而进行系统学习的环境下。本文提出了一种将DRL方法应用于装配序列规划（ASP）的方法。所提出的方法在RL环境中引入了参数行动，以提高训练时间和样本效率，并使用了两种不同的奖励信号：（1）用户的偏好和（2）总装配时间。用户的偏好信号解决了装配过程中人类面临的困难和非人体工效特性，而总装配时间信号则强制执行装配的优化。本文研究了三种最强大的深度RL方法 A2C、DQN 和 Rainbow，并在两个不同场景下进行了研究：",
    "tldr": "本文研究了将深度强化学习应用于装配序列规划问题，引入参数行动以提高训练时间和样本效率，并使用了两种不同的奖励信号。研究结果表明，三种最强大的深度RL方法，A2C、DQN和Rainbow都能够解决这个问题。",
    "en_tdlr": "This paper proposes an approach to implementing deep reinforcement learning (DRL) methods in assembly sequence planning (ASP), introducing parametric actions to improve training time and sample efficiency, and using two different reward signals for user preferences and total assembly time duration. The research shows that the three most powerful deep RL methods, A2C, DQN, and Rainbow, can effectively solve the ASP problem."
}