{
    "title": "Quantum Imitation Learning. (arXiv:2304.02480v1 [quant-ph])",
    "abstract": "Despite remarkable successes in solving various complex decision-making tasks, training an imitation learning (IL) algorithm with deep neural networks (DNNs) suffers from the high computation burden. In this work, we propose quantum imitation learning (QIL) with a hope to utilize quantum advantage to speed up IL. Concretely, we develop two QIL algorithms, quantum behavioural cloning (Q-BC) and quantum generative adversarial imitation learning (Q-GAIL). Q-BC is trained with a negative log-likelihood loss in an off-line manner that suits extensive expert data cases, whereas Q-GAIL works in an inverse reinforcement learning scheme, which is on-line and on-policy that is suitable for limited expert data cases. For both QIL algorithms, we adopt variational quantum circuits (VQCs) in place of DNNs for representing policies, which are modified with data re-uploading and scaling parameters to enhance the expressivity. We first encode classical data into quantum states as inputs, then perform V",
    "link": "http://arxiv.org/abs/2304.02480",
    "context": "Title: Quantum Imitation Learning. (arXiv:2304.02480v1 [quant-ph])\nAbstract: Despite remarkable successes in solving various complex decision-making tasks, training an imitation learning (IL) algorithm with deep neural networks (DNNs) suffers from the high computation burden. In this work, we propose quantum imitation learning (QIL) with a hope to utilize quantum advantage to speed up IL. Concretely, we develop two QIL algorithms, quantum behavioural cloning (Q-BC) and quantum generative adversarial imitation learning (Q-GAIL). Q-BC is trained with a negative log-likelihood loss in an off-line manner that suits extensive expert data cases, whereas Q-GAIL works in an inverse reinforcement learning scheme, which is on-line and on-policy that is suitable for limited expert data cases. For both QIL algorithms, we adopt variational quantum circuits (VQCs) in place of DNNs for representing policies, which are modified with data re-uploading and scaling parameters to enhance the expressivity. We first encode classical data into quantum states as inputs, then perform V",
    "path": "papers/23/04/2304.02480.json",
    "total_tokens": 1149,
    "translated_title": "量子模仿学习",
    "translated_abstract": "尽管在解决各种复杂决策问题方面取得了卓越的成功，但使用深度神经网络（DNN）来训练模仿学习（IL）算法仍然面临着高计算负担。在本文中，我们提出了一种希望利用量子优势加速IL的量子模仿学习（QIL）。具体而言，我们开发了两种QIL算法，量子行为克隆（Q-BC）和量子生成对抗模仿学习（Q-GAIL）。Q-BC采用负对数似然损失以适应广泛的专家数据情况进行离线训练，而Q-GAIL采用逆强化学习方案进行在线和在线策略，适用于有限的专家数据情况。对于两种QIL算法，我们采用变分量子电路（VQC）代替DNN来表示策略，并通过数据重新上传和缩放参数进行修改以增强表现力。我们首先将经典数据编码为量子状态作为输入，然后使用带有噪声模拟器的VQC进行策略优化。我们的数值实验表明，QIL可以实现至少与其经典对应物相当的结果，并且基于VQC的策略表示优于一些现有的量子RL算法。",
    "tldr": "本论文提出了量子模仿学习（QIL）以加速学习，其中通过采用变分量子电路（VQC）代替DNN来表示策略，分别开发了量子行为克隆（Q-BC）和量子生成对抗模仿学习（Q-GAIL）两种QIL算法，实验表明QIL可以实现至少与其经典对应物相当的结果，并且基于VQC的策略表示优于一些现有的量子RL算法。",
    "en_tdlr": "This paper proposes Quantum Imitation Learning (QIL) to speed up the training process, with Variational Quantum Circuits (VQC) used to represent policies instead of DNNs. Two QIL algorithms are developed - Quantum Behavioral Cloning (Q-BC) and Quantum Generative Adversarial Imitation Learning (Q-GAIL). Numerical experiments show that QIL achieves comparable results to its classical counterparts, and the VQC-based policy representation outperforms some existing Quantum RL algorithms."
}