{
    "title": "On the Generalization Error of Meta Learning for the Gibbs Algorithm. (arXiv:2304.14332v1 [cs.LG])",
    "abstract": "We analyze the generalization ability of joint-training meta learning algorithms via the Gibbs algorithm. Our exact characterization of the expected meta generalization error for the meta Gibbs algorithm is based on symmetrized KL information, which measures the dependence between all meta-training datasets and the output parameters, including task-specific and meta parameters. Additionally, we derive an exact characterization of the meta generalization error for the super-task Gibbs algorithm, in terms of conditional symmetrized KL information within the super-sample and super-task framework introduced in Steinke and Zakynthinou (2020) and Hellstrom and Durisi (2022) respectively. Our results also enable us to provide novel distribution-free generalization error upper bounds for these Gibbs algorithms applicable to meta learning.",
    "link": "http://arxiv.org/abs/2304.14332",
    "context": "Title: On the Generalization Error of Meta Learning for the Gibbs Algorithm. (arXiv:2304.14332v1 [cs.LG])\nAbstract: We analyze the generalization ability of joint-training meta learning algorithms via the Gibbs algorithm. Our exact characterization of the expected meta generalization error for the meta Gibbs algorithm is based on symmetrized KL information, which measures the dependence between all meta-training datasets and the output parameters, including task-specific and meta parameters. Additionally, we derive an exact characterization of the meta generalization error for the super-task Gibbs algorithm, in terms of conditional symmetrized KL information within the super-sample and super-task framework introduced in Steinke and Zakynthinou (2020) and Hellstrom and Durisi (2022) respectively. Our results also enable us to provide novel distribution-free generalization error upper bounds for these Gibbs algorithms applicable to meta learning.",
    "path": "papers/23/04/2304.14332.json",
    "total_tokens": 753,
    "translated_title": "关于Gibbs算法元学习的泛化误差分析",
    "translated_abstract": "本文通过Gibbs算法来分析联合训练元学习算法的泛化能力。我们对元Gibbs算法的期望元泛化误差进行了精确的刻画，基于对称化KL信息，该信息度量了所有元训练数据集与输出参数之间的依赖关系，包括任务特定和元参数。此外，我们在Steinke和Zakynthinou (2020)以及Hellstrom 和 Durisi (2022)分别引入的超样本和超任务框架中，利用条件对称KL信息，为超：任务Gibbs算法提供了精确的元泛化误差刻画。我们的结果还使我们能够为这些Gibbs算法提供新的分布无关的泛化误差上界，适用于元学习。",
    "tldr": "本文通过Gibbs算法来分析元学习算法的泛化能力，提供了精确的元泛化误差刻画和新的泛化误差上界。"
}