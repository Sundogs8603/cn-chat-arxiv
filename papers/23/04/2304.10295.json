{
    "title": "Decouple Non-parametric Knowledge Distillation For End-to-end Speech Translation. (arXiv:2304.10295v1 [cs.CL])",
    "abstract": "Existing techniques often attempt to make knowledge transfer from a powerful machine translation (MT) to speech translation (ST) model with some elaborate techniques, which often requires transcription as extra input during training. However, transcriptions are not always available, and how to improve the ST model performance without transcription, i.e., data efficiency, has rarely been studied in the literature. In this paper, we propose Decoupled Non-parametric Knowledge Distillation (DNKD) from data perspective to improve the data efficiency. Our method follows the knowledge distillation paradigm. However, instead of obtaining the teacher distribution from a sophisticated MT model, we construct it from a non-parametric datastore via k-Nearest-Neighbor (kNN) retrieval, which removes the dependence on transcription and MT model. Then we decouple the classic knowledge distillation loss into target and non-target distillation to enhance the effect of the knowledge among non-target logit",
    "link": "http://arxiv.org/abs/2304.10295",
    "context": "Title: Decouple Non-parametric Knowledge Distillation For End-to-end Speech Translation. (arXiv:2304.10295v1 [cs.CL])\nAbstract: Existing techniques often attempt to make knowledge transfer from a powerful machine translation (MT) to speech translation (ST) model with some elaborate techniques, which often requires transcription as extra input during training. However, transcriptions are not always available, and how to improve the ST model performance without transcription, i.e., data efficiency, has rarely been studied in the literature. In this paper, we propose Decoupled Non-parametric Knowledge Distillation (DNKD) from data perspective to improve the data efficiency. Our method follows the knowledge distillation paradigm. However, instead of obtaining the teacher distribution from a sophisticated MT model, we construct it from a non-parametric datastore via k-Nearest-Neighbor (kNN) retrieval, which removes the dependence on transcription and MT model. Then we decouple the classic knowledge distillation loss into target and non-target distillation to enhance the effect of the knowledge among non-target logit",
    "path": "papers/23/04/2304.10295.json",
    "total_tokens": 925,
    "translated_title": "解离非参数知识蒸馏来达到端到端语音翻译",
    "translated_abstract": "现有技术通常尝试通过一些复杂的技术，将强大的机器翻译(MT)向语音翻译(ST)模型进行知识转移，但这往往需要在训练期间作为额外输入来记录。然而，并不总是有转录数据可用，如何在没有转录的情况下提高ST模型的性能，即数据效率，在文献中很少被研究。在本文中，我们从数据角度提出了解离非参数知识蒸馏(DNKD)来提高数据效率。我们的方法遵循知识蒸馏范式。但是，我们构建它的教师分布，而不是从复杂的MT模型中获得它，而是通过kNN检索从非参数数据存储中获得它，这消除了对转录和MT模型的依赖。然后，我们将经典的知识蒸馏损失分为目标和非目标蒸馏，以增强非目标logit之间的知识效果。",
    "tldr": "本文提出了一种新方法，从数据角度提高了语音翻译模型的数据效率，而不需要转录数据。该方法解离了非参数知识蒸馏，通过构建教师分布来达到知识转移的目的。"
}