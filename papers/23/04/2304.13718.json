{
    "title": "Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models. (arXiv:2304.13718v1 [cs.LG])",
    "abstract": "With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their origin",
    "link": "http://arxiv.org/abs/2304.13718",
    "context": "Title: Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models. (arXiv:2304.13718v1 [cs.LG])\nAbstract: With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their origin",
    "path": "papers/23/04/2304.13718.json",
    "total_tokens": 965,
    "translated_title": "稀疏化模型动物园双胞胎：研究稀疏神经网络模型族的行为和鲁棒性",
    "translated_abstract": "随着神经网络（NNs）的规模增长，模型稀疏化以减少模型推理的计算成本和内存需求已经成为研究和生产的关键兴趣。虽然许多稀疏化方法已被提出并成功应用于个体模型，但据我们所知，它们的行为和鲁棒性尚未在大量模型族上进行研究。本文通过将两种流行的稀疏化方法应用于模型族（所谓的模型动物园）上，创建原始动物园的稀疏化版本，来填补这个空白。我们研究了这两种方法在每个动物园中的表现，逐层比较稀疏化，并分析原始族群和稀疏化后族群之间的一致性。我们发现，除了高于80%的较高稀疏化比率之外，两种方法都非常稳健，而幅度修剪能够优于变分丢失。此外，我们发现稀疏化模型与原始模型高度一致。",
    "tldr": "本文研究了两种稀疏化方法在模型族中的表现，并发现幅度修剪方法优于变分丢失方法，除了高于80%的较高稀疏化比率之外，两种方法都非常稳健。",
    "en_tdlr": "This paper investigates the sparsification behavior and robustness of two popular methods on populations of models, and finds that magnitude pruning outperforms variational dropout except for high sparsification ratios above 80%, and both methods are very robust."
}