{
    "title": "Training Large Language Models Efficiently with Sparsity and Dataflow. (arXiv:2304.05511v1 [cs.LG])",
    "abstract": "Large foundation language models have shown their versatility in being able to be adapted to perform a wide variety of downstream tasks, such as text generation, sentiment analysis, semantic search etc. However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts. Furthermore, sparsity drops the operation intensity and introduces irregular memory access patterns that makes it challenging to efficiently utilize compute resources. This paper demonstrates an end-to-end training flow on a large language model - 13 billion GPT - using sparsity and dataflow. The dataflow execution model and architecture enables effi",
    "link": "http://arxiv.org/abs/2304.05511",
    "context": "Title: Training Large Language Models Efficiently with Sparsity and Dataflow. (arXiv:2304.05511v1 [cs.LG])\nAbstract: Large foundation language models have shown their versatility in being able to be adapted to perform a wide variety of downstream tasks, such as text generation, sentiment analysis, semantic search etc. However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts. Furthermore, sparsity drops the operation intensity and introduces irregular memory access patterns that makes it challenging to efficiently utilize compute resources. This paper demonstrates an end-to-end training flow on a large language model - 13 billion GPT - using sparsity and dataflow. The dataflow execution model and architecture enables effi",
    "path": "papers/23/04/2304.05511.json",
    "total_tokens": 760,
    "translated_title": "利用稀疏性和数据流高效训练大型语言模型",
    "translated_abstract": "针对大型基础语言模型的训练需要大量计算资源和机器学习专家的专业知识, 稀疏性技术可以缓解训练中的计算要求，但稀疏性会引入新的挑战。本文提出了一种基于稀疏性和数据流的端到端训练方法，在有效利用计算资源的同时，实现13亿GPT语言模型训练，且稀疏性技术和稠密模型对比具有相当的结果和计算资源和记忆要求的减少。",
    "tldr": "本论文提出了一种利用稀疏性和数据流训练大型语言模型的方法，不仅可以有效利用计算资源，提高计算效率，还能达到与稠密模型类似的结果。",
    "en_tdlr": "This paper presents a method for training large language models efficiently using sparsity and dataflow, achieving comparable results with dense models while reducing compute and memory requirements."
}