{
    "title": "Learning a Universal Human Prior for Dexterous Manipulation from Human Preference. (arXiv:2304.04602v2 [cs.RO] UPDATED)",
    "abstract": "Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Scripting policies from scratch is intractable due to the high-dimensional control space, and training policies with reinforcement learning (RL) and manual reward engineering can also be hard and lead to unnatural motions. Leveraging the recent progress on RL from Human Feedback, we propose a framework that learns a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policies on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. A task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks,",
    "link": "http://arxiv.org/abs/2304.04602",
    "context": "Title: Learning a Universal Human Prior for Dexterous Manipulation from Human Preference. (arXiv:2304.04602v2 [cs.RO] UPDATED)\nAbstract: Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Scripting policies from scratch is intractable due to the high-dimensional control space, and training policies with reinforcement learning (RL) and manual reward engineering can also be hard and lead to unnatural motions. Leveraging the recent progress on RL from Human Feedback, we propose a framework that learns a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policies on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. A task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks,",
    "path": "papers/23/04/2304.04602.json",
    "total_tokens": 948,
    "translated_title": "从人类偏好学习灵巧机器人操作的普适人类先验",
    "translated_abstract": "在机器人手灵巧操作任务中生成类似人类行为是一个巨大的挑战。由于高维的控制空间，从头开始编写策略很难，使用强化学习（RL）和手动奖励设计训练策略也难以实现并导致不自然的动作。借鉴强化学习从人类反馈中的最新进展，我们提出了一个框架，通过直接人类偏好反馈视频来学习一个普适的人类先验，以在仿真中有效调整20个双手机器人操作任务的RL策略，而无需进行任何人类示范。通过生成多样的策略并收集轨迹上的人类偏好，训练了一个任务不可知的奖励模型，然后在微调阶段用于规范策略的行为。我们的方法在各种任务中经验性地展示了机器人手的更类人行为，甚至包括未见过的任务。",
    "tldr": "通过借鉴最新的强化学习从人类反馈中的方法，本论文提出了一种学习普适人类先验的框架，通过直接人类偏好反馈视频来调整机器人在各种任务中的策略。结果表明，该方法能够在仿真中表现出更加类人的行为，甚至在未见过的任务中也能取得良好效果。",
    "en_tdlr": "This paper proposes a framework for learning a universal human prior by leveraging recent advances in reinforcement learning from human feedback. By using direct human preference feedback over videos, the framework efficiently adjusts the robot's policies in various tasks, resulting in more human-like behaviors even in unseen tasks."
}