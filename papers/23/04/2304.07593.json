{
    "title": "Teacher Network Calibration Improves Cross-Quality Knowledge Distillation. (arXiv:2304.07593v1 [cs.CV])",
    "abstract": "We investigate cross-quality knowledge distillation (CQKD), a knowledge distillation method where knowledge from a teacher network trained with full-resolution images is transferred to a student network that takes as input low-resolution images. As image size is a deciding factor for the computational load of computer vision applications, CQKD notably reduces the requirements by only using the student network at inference time. Our experimental results show that CQKD outperforms supervised learning in large-scale image classification problems. We also highlight the importance of calibrating neural networks: we show that with higher temperature smoothing of the teacher's output distribution, the student distribution exhibits a higher entropy, which leads to both, a lower calibration error and a higher network accuracy.",
    "link": "http://arxiv.org/abs/2304.07593",
    "context": "Title: Teacher Network Calibration Improves Cross-Quality Knowledge Distillation. (arXiv:2304.07593v1 [cs.CV])\nAbstract: We investigate cross-quality knowledge distillation (CQKD), a knowledge distillation method where knowledge from a teacher network trained with full-resolution images is transferred to a student network that takes as input low-resolution images. As image size is a deciding factor for the computational load of computer vision applications, CQKD notably reduces the requirements by only using the student network at inference time. Our experimental results show that CQKD outperforms supervised learning in large-scale image classification problems. We also highlight the importance of calibrating neural networks: we show that with higher temperature smoothing of the teacher's output distribution, the student distribution exhibits a higher entropy, which leads to both, a lower calibration error and a higher network accuracy.",
    "path": "papers/23/04/2304.07593.json",
    "total_tokens": 852,
    "translated_title": "教师网络校准在跨质量知识蒸馏中的应用",
    "translated_abstract": "本研究探讨了跨质量知识蒸馏（CQKD），一种知识蒸馏方法，其中从使用高分辨率图像训练的教师网络中获取知识，然后将其转移到仅使用低分辨率图像的学生网络。由于图像大小是影响计算机视觉应用程序计算负荷的重要因素，CQKD通过仅在推断时使用学生网络，显着降低了要求。我们的实验结果表明，CQKD在大规模图像分类问题中优于监督学习。我们还强调了校准神经网络的重要性：我们表明，通过更高的温度平滑教师输出分布，学生分布展现出更高的熵，这既降低了校准误差，也提高了网络精度。",
    "tldr": "本论文研究了跨质量知识蒸馏方法，将使用高分辨率图像训练的教师网络中的知识转移到低分辨率图像的学生网络中，并通过提高温度平滑教师输出分布来优化方法，实现在大规模图像分类问题中的性能优越。",
    "en_tdlr": "This paper investigates the cross-quality knowledge distillation method, which transfers knowledge from a teacher network trained on high-resolution images to a student network that only takes low-resolution images. The paper improves the method by highlighting the importance of calibrating neural networks, achieving superior performance in large-scale image classification problems."
}