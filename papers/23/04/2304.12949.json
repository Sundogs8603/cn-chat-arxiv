{
    "title": "eFAT: Improving the Effectiveness of Fault-Aware Training for Mitigating Permanent Faults in DNN Hardware Accelerators. (arXiv:2304.12949v1 [cs.AR])",
    "abstract": "Fault-Aware Training (FAT) has emerged as a highly effective technique for addressing permanent faults in DNN accelerators, as it offers fault mitigation without significant performance or accuracy loss, specifically at low and moderate fault rates. However, it leads to very high retraining overheads, especially when used for large DNNs designed for complex AI applications. Moreover, as each fabricated chip can have a distinct fault pattern, FAT is required to be performed for each faulty chip individually, considering its unique fault map, which further aggravates the problem. To reduce the overheads of FAT while maintaining its benefits, we propose (1) the concepts of resilience-driven retraining amount selection, and (2) resilience-driven grouping and fusion of multiple fault maps (belonging to different chips) to perform consolidated retraining for a group of faulty chips. To realize these concepts, in this work, we present a novel framework, eFAT, that computes the resilience of a",
    "link": "http://arxiv.org/abs/2304.12949",
    "context": "Title: eFAT: Improving the Effectiveness of Fault-Aware Training for Mitigating Permanent Faults in DNN Hardware Accelerators. (arXiv:2304.12949v1 [cs.AR])\nAbstract: Fault-Aware Training (FAT) has emerged as a highly effective technique for addressing permanent faults in DNN accelerators, as it offers fault mitigation without significant performance or accuracy loss, specifically at low and moderate fault rates. However, it leads to very high retraining overheads, especially when used for large DNNs designed for complex AI applications. Moreover, as each fabricated chip can have a distinct fault pattern, FAT is required to be performed for each faulty chip individually, considering its unique fault map, which further aggravates the problem. To reduce the overheads of FAT while maintaining its benefits, we propose (1) the concepts of resilience-driven retraining amount selection, and (2) resilience-driven grouping and fusion of multiple fault maps (belonging to different chips) to perform consolidated retraining for a group of faulty chips. To realize these concepts, in this work, we present a novel framework, eFAT, that computes the resilience of a",
    "path": "papers/23/04/2304.12949.json",
    "total_tokens": 1176,
    "translated_title": "eFAT：改进DNN硬件加速器中的故障感知训练以提高有效性",
    "translated_abstract": "故障感知训练（FAT）已经成为一种高效应对DNN加速器永久性故障的技术，因为它能够在低和中等故障率下提供故障缓解而不会带来显着的性能或准确性损失。然而，当用于面向复杂AI应用程序的大型DNN时，它会导致非常高的重训练开销。此外，由于每个芯片的故障模式都可能不同，因此需要逐一为每个故障芯片进行FAT，考虑到其独特的故障地图，这进一步加剧了问题。为了减少FAT的开销同时维持其优势，我们提出了（1）韧性驱动的重训练量选择和（2）多个故障地图（属于不同芯片）的韧性驱动的分组和融合，以对一组故障芯片进行合并重训练。为了实现这些概念，我们提出了一种新的框架eFAT，用于计算DNN加速器的韧性并将其映射到相应的重训练量。eFAT还根据故障地图对存在故障的芯片进行分组，并对组进行合并重训练，同时考虑组间的差异。我们在多个DNN加速器上的实验表明，eFAT显著降低了FAT的重训练开销，同时提供类似或更好的容错能力。",
    "tldr": "研究提出了一种新的框架eFAT，用于计算DNN加速器的韧性并将其映射到相应的重训练量，通过分组和融合故障地图减少FAT的开销，同时提高容错能力。",
    "en_tdlr": "The paper proposes a new framework eFAT to compute the resilience of DNN accelerator and map it to corresponding retraining amounts, reduce the overheads of Fault-Aware Training (FAT) by grouping and fusing multiple fault maps and maintain its fault tolerance."
}