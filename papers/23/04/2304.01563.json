{
    "title": "Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])",
    "abstract": "The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA fa",
    "link": "http://arxiv.org/abs/2304.01563",
    "context": "Title: Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])\nAbstract: The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA fa",
    "path": "papers/23/04/2304.01563.json",
    "total_tokens": 782,
    "translated_title": "多模态实体对齐的属性一致知识图谱表示学习",
    "translated_abstract": "多模态实体对齐(MMEA)旨在找到多模态知识图谱(MMKGs)之间所有等价的实体对。本文提出一个新颖的属性一致知识图谱表示学习框架(ACK-MMEA)，通过合并一致的对齐知识来弥补上下文差距问题。通过使用多模态属性统一化构建属性一致的知识图谱(ACKGs)，并在基于关系的图神经网络中融合这些ACKGs以获得聚合的关系表示和稳健的实体表示。",
    "tldr": "本文提出了一个新颖的属性一致知识图谱表示学习框架(ACK-MMEA)，弥补实体在学习过程中特定模态上具有不同数量属性的上下文差距问题，从而提高多模态实体对齐(MMEA)的准确性。",
    "en_tdlr": "The paper proposes a novel attribute-consistent knowledge graph representation learning framework for multi-modal entity alignment (ACK-MMEA) to compensate the contextual gaps problem, aiming to improve the accuracy of MMEA by addressing the issue of entities having a different number of attributes on specific modality during the learning process."
}