{
    "title": "Adversarial Policy Optimization in Deep Reinforcement Learning. (arXiv:2304.14533v1 [cs.LG])",
    "abstract": "The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future r",
    "link": "http://arxiv.org/abs/2304.14533",
    "context": "Title: Adversarial Policy Optimization in Deep Reinforcement Learning. (arXiv:2304.14533v1 [cs.LG])\nAbstract: The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future r",
    "path": "papers/23/04/2304.14533.json",
    "total_tokens": 900,
    "translated_title": "深度强化学习中的对抗策略优化",
    "translated_abstract": "由于神经网络表示的策略可以过度拟合观测中的表面特征，这会妨碍强化学习智能体学习有效的策略。在高维状态下，这个问题变得更加严重，智能体难以学习有用的策略。数据增强可以通过减轻过拟合的影响来提供性能提升。然而，这样的数据增强是一种先验知识，如果在环境中简单地应用它们可能会降低智能体的性能。在本文中，我们提出了一种新的强化学习算法，以减轻上述问题并提高学习策略的效率。我们的方法包括一个博弈理论目标，在这个目标中，扰动网络修改状态，以最大化智能体执行不同动作的概率，同时最小化状态的扭曲。相反，策略网络更新其参数，以最小化扰动效果，同时最大化未来奖励的期望值。",
    "tldr": "本文提出了一种新的强化学习算法，其中一个扰动网络通过最大化智能体执行不同动作的概率，同时最小化状态的扭曲，以减轻数据过拟合的影响。",
    "en_tdlr": "This paper proposes a novel RL algorithm consisting of a perturber network that modifies the state to maximize the agent's probability of taking different actions, while minimizing distortion, and a policy network that minimizes perturbation and maximizes expected future reward, to mitigate overfitting in deep reinforcement learning."
}