{
    "title": "Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings. (arXiv:2304.05989v1 [cs.AI])",
    "abstract": "Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that o",
    "link": "http://arxiv.org/abs/2304.05989",
    "context": "Title: Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings. (arXiv:2304.05989v1 [cs.AI])\nAbstract: Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that o",
    "path": "papers/23/04/2304.05989.json",
    "total_tokens": 906,
    "translated_title": "通过无监督学习图嵌入进行物体无关能力分类",
    "translated_abstract": "获取关于物体交互和能力的知识可以促进场景理解和人机协作任务。在日常生活场景中，人们倾向于根据场景和物体的可用性以多种不同的方式使用物体。针对存在开放互动和物体的类别不确定的情况下，学习物体的能力是一项具有挑战性的任务。本文提出了一种方法来实现具有不确定交互开放集合的类别无关物体的能力分类，通过无监督学习物体交互之间的相似之处从而诱导物体能力簇。采用了一种新颖的深度感知定性空间表示法来构建活动图（AGs），这些图从RGB-D视频中抽象出时空交互的连续表示。然后对这些AGs进行聚类，以获取具有类似能力的一组物体。我们在实际场景中的实验表明，",
    "tldr": "本文通过无监督学习相似物体交互，诱导物体能力簇，使用新颖的深度感知定性空间表示法构建活动图并进行聚类，从而实现具有不确定交互开放集合的类别无关物体的能力分类。",
    "en_tdlr": "This paper proposes a way to achieve affordance categorization for class-agnostic objects with an open set of interactions by learning similarities between object interactions in an unsupervised way, inducing clusters of object affordances, constructing Activity Graphs using a novel depth-informed qualitative spatial representation for spatio-temporal interactions in RGB-D videos, and clustering these graphs to obtain groups of objects with similar affordances."
}