{
    "title": "r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])",
    "abstract": "Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l",
    "link": "http://arxiv.org/abs/2304.05243",
    "context": "Title: r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])\nAbstract: Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l",
    "path": "papers/23/04/2304.05243.json",
    "total_tokens": 926,
    "translated_title": "r-softmax: 具有可控稀疏率的广义Softmax",
    "translated_abstract": "如今，人工神经网络模型在许多领域取得了显著的成果。将模型提供的表示映射到概率分布的函数是深度学习解决方案的不可分割的方面。虽然softmax是机器学习社区中通常接受的概率映射函数，但它不能返回稀疏的输出，并且总是将正概率分散到所有位置。在本文中，我们提出了r-softmax，这是softmax的一种修改，它输出具有可控稀疏度的稀疏概率分布。与现有的稀疏概率映射函数相比，我们提供了一种直观的机制来控制输出稀疏度。我们在几个多标签数据集上展示了r-softmax优于其他稀疏的softmax替代方案，并且与原始的softmax相比具有高竞争力。我们还将r-softmax应用于预训练转换语言模型的自我注意模块中，并展示了它在自然语言处理方面的应用。",
    "tldr": "本文提出了一种新的广义Softmax函数r-softmax，可以输出具有可控稀疏度的概率分布，相较于现有的替代方案效果更好，在多标签数据集上表现突出，在预训练转换语言模型的自我注意模块中具有重要应用。",
    "en_tdlr": "This paper proposes a new generalized Softmax function called r-softmax, which can output probability distributions with controllable sparsity rate. In contrast to other sparse alternatives, r-softmax outperforms them on multi-label datasets and can be applied in the self-attention module of pre-trained transformer language models."
}