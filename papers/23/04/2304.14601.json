{
    "title": "Improve Video Representation with Temporal Adversarial Augmentation. (arXiv:2304.14601v1 [cs.CV])",
    "abstract": "Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We",
    "link": "http://arxiv.org/abs/2304.14601",
    "context": "Title: Improve Video Representation with Temporal Adversarial Augmentation. (arXiv:2304.14601v1 [cs.CV])\nAbstract: Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We",
    "path": "papers/23/04/2304.14601.json",
    "total_tokens": 913,
    "translated_title": "用时序对抗增强技术改进视频表示",
    "translated_abstract": "最近的研究表明，如果以适当的方式使用，对抗增强有助于神经网络的泛化。本文提出了一种使用时间注意力的新型视频增强技术——Temporal Adversarial Augmentation (TA)。与传统的对抗增强不同，TA专为通过最大化时间相关的损失函数来改变神经网络对视频片段的注意分布而设计。我们证明，TA将获得多样化的时间视角，这显著影响神经网络的焦点。使用这些示例进行训练修复了不平衡的时间信息感知缺陷，并增强了抵御时间偏移的能力，最终导致更好的泛化性能。为了利用TA，我们提出了Temporal Video Adversarial Fine-tuning (TAF)框架来改进视频表示。TAF是一种通用的模型无关、可解释性友好的训练策略。",
    "tldr": "本文提出了Temporal Adversarial Augmentation（TA），一种利用时间注意力的视频增强技术，可以通过最大化时间相关的损失函数来改变神经网络对视频片段的注意分布。利用TA，我们提出了Temporal Video Adversarial Fine-tuning（TAF）框架，可以有效地改善视频表示并提高神经网络的泛化能力。",
    "en_tdlr": "This paper proposes Temporal Adversarial Augmentation (TA), a video augmentation technique that utilizes temporal attention to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. Using TA, we introduce Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations and enhancing neural networks' ability to defend against temporal shifts, ultimately leading to better generalization."
}