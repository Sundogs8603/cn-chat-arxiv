{
    "title": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (arXiv:2304.14475v1 [cs.CR])",
    "abstract": "Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that Figure 4 achieves comparable attack performance w",
    "link": "http://arxiv.org/abs/2304.14475",
    "context": "Title: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (arXiv:2304.14475v1 [cs.CR])\nAbstract: Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that Figure 4 achieves comparable attack performance w",
    "path": "papers/23/04/2304.14475.json",
    "total_tokens": 996,
    "translated_title": "ChatGPT作为攻击工具：通过黑盒生成模型触发的隐蔽文本后门攻击",
    "translated_abstract": "文本后门攻击威胁着现有系统的安全性，因为攻击者可以将难以感知的触发器插入输入数据并操纵训练数据集的标签来破坏模型。随着尖端的生成模型（如GPT-4）将重写推向了前所未有的高度，这种攻击变得更加难以检测。本文全面调查了黑盒生成模型作为后门攻击工具的角色，并强调了研究相关防御策略的重要性。我们揭示了所提出的基于生成模型的攻击BGMAttack可以有效地欺骗文本分类器。相较于传统攻击方法，BGMAttack通过利用最先进的生成型号使后门触发器不太显眼。我们在五个数据集上进行了广泛的攻击有效性评估，并辅以三个不同的人类认知评估，发现BGMAttack的表现相当且输入数据没有明显的变形。我们的工作强调了进一步研究和开发对这种隐蔽文本后门攻击的防御机制的必要性。",
    "tldr": "本论文研究了黑盒生成模型作为后门攻击工具的角色以及相应的防御策略。其中，我们发现通过BGMAttack攻击文本分类器可以有效地实现后门攻击，而且输入的数据没有明显变形。",
    "en_tdlr": "This paper investigates the role of black-box generative models as a backdoor attack tool and highlights the importance of researching defense mechanisms. The proposed BGMAttack can effectively deceive textual classifiers with less conspicuous backdoor triggers. Further research is required for developing defenses against such stealthy textual backdoor attacks."
}