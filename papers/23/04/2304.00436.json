{
    "title": "Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])",
    "abstract": "Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model ",
    "link": "http://arxiv.org/abs/2304.00436",
    "context": "Title: Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])\nAbstract: Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model ",
    "path": "papers/23/04/2304.00436.json",
    "total_tokens": 933,
    "translated_title": "通过神经元激活空间的对抗学习，在视觉问答中对实例进行的特洛伊攻击",
    "translated_abstract": "恶意扰动被嵌入输入数据中，称为特洛伊攻击，可以导致神经网络表现异常。然而，在模型微调过程中，即从预训练的大规模模型（如视觉问答）向目标模型转移知识的过程中，特洛伊攻击的影响有所减小。为了减轻特洛伊攻击的影响，我们可以替换和微调预训练模型的多个层。本研究聚焦于样本效率、隐蔽性、多样性和鲁棒性。为了解决这些挑战，我们提出了一个针对实例级的特洛伊攻击，该攻击生成跨输入样本和模态的多样化特洛伊。对抗学习建立了指定扰动层和微调模型的异常行为之间的相关性。我们在VQA-v2数据集上进行了大量实验，并使用多种指标进行了评估。实验结果表明，我们提出的方法可以有效适应微调模型。",
    "tldr": "本研究提出了一种实例级别的特洛伊攻击方法，在视觉问答中通过神经元激活空间的对抗学习生成多样性特洛伊，有效地适应了微调模型。",
    "en_tdlr": "This research proposes an instance-level Trojan attack using adversarial learning in neuron activation space, which generates diverse Trojans across input samples and modalities in visual question answering, effectively adapting to the fine-tuned model."
}