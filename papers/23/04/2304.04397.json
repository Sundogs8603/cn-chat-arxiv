{
    "title": "Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension. (arXiv:2304.04397v1 [cs.DS])",
    "abstract": "Large language models (LLMs) have shown their power in different areas. Attention computation, as an important subroutine of LLMs, has also attracted interests in theory. Recently the static computation and dynamic maintenance of attention matrix has been studied by [Alman and Song 2023] and [Brand, Song and Zhou 2023] from both algorithmic perspective and hardness perspective. In this work, we consider the sparsification of the attention problem. We make one simplification which is the logit matrix is symmetric. Let $n$ denote the length of sentence, let $d$ denote the embedding dimension. Given a matrix $X \\in \\mathbb{R}^{n \\times d}$, suppose $d \\gg n$ and $\\| X X^\\top \\|_{\\infty} < r$ with $r \\in (0,0.1)$, then we aim for finding $Y \\in \\mathbb{R}^{n \\times m}$ (where $m\\ll d$) such that \\begin{align*} \\| D(Y)^{-1} \\exp( Y Y^\\top ) D(X)^{-1} \\exp( X X^\\top) \\|_{\\infty} \\leq O(r) \\end{align*} We provide two results for this problem.  $\\bullet$ Our first result is a randomized algo",
    "link": "http://arxiv.org/abs/2304.04397",
    "context": "Title: Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension. (arXiv:2304.04397v1 [cs.DS])\nAbstract: Large language models (LLMs) have shown their power in different areas. Attention computation, as an important subroutine of LLMs, has also attracted interests in theory. Recently the static computation and dynamic maintenance of attention matrix has been studied by [Alman and Song 2023] and [Brand, Song and Zhou 2023] from both algorithmic perspective and hardness perspective. In this work, we consider the sparsification of the attention problem. We make one simplification which is the logit matrix is symmetric. Let $n$ denote the length of sentence, let $d$ denote the embedding dimension. Given a matrix $X \\in \\mathbb{R}^{n \\times d}$, suppose $d \\gg n$ and $\\| X X^\\top \\|_{\\infty} < r$ with $r \\in (0,0.1)$, then we aim for finding $Y \\in \\mathbb{R}^{n \\times m}$ (where $m\\ll d$) such that \\begin{align*} \\| D(Y)^{-1} \\exp( Y Y^\\top ) D(X)^{-1} \\exp( X X^\\top) \\|_{\\infty} \\leq O(r) \\end{align*} We provide two results for this problem.  $\\bullet$ Our first result is a randomized algo",
    "path": "papers/23/04/2304.04397.json",
    "total_tokens": 1058,
    "translated_title": "针对超参数化特征维度的随机和确定性注意力稀疏化算法",
    "translated_abstract": "大型语言模型在不同领域展示了它们的实力。作为LLMs的一个重要子例程，注意力计算也引起了理论上的兴趣。最近，[Alman and Song 2023]和[Brand，Song and Zhou 2023]从算法和困难的角度研究了注意矩阵的静态计算和动态维护。在这项工作中，我们考虑了注意问题的稀疏化。我们做了一个简化，即logit矩阵是对称的。假设$n$表示句子长度，$d$表示嵌入维度。给定一个矩阵$X \\in \\mathbb{R} ^{n \\times d}$，假设$d \\gg n$且$\\| X X^\\top \\|_{\\infty} < r$，其中$r \\in (0,0.1)$，则我们旨在找到$Y \\in \\mathbb{R}^{n \\times m}$（其中$m\\ll d$）, 使得 $D(Y)^{-1} \\exp(Y Y ^\\top) D(X)^{-1}\\exp(X X ^\\top)$的$\\| \\_\\|_{\\infty}$范数$\\leq O（r）$。我们为这个问题提供了两个结果。$\\bullet$我们的第一个结果是一个随机算法。",
    "tldr": "本论文提出了随机和确定性注意力稀疏化算法，用于处理超参数化特征维度，其目标是找到适当的矩阵Y，使得其满足特定条件，从而帮助解决注意问题。",
    "en_tdlr": "This paper proposes randomized and deterministic attention sparsification algorithms for over-parameterized feature dimensions, aiming to find an appropriate matrix Y that satisfies specific conditions to help solve attention problems."
}