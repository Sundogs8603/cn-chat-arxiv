{
    "title": "Towards Explainable Collaborative Filtering with Taste Clusters Learning. (arXiv:2304.13937v1 [cs.IR])",
    "abstract": "Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements.  In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations ",
    "link": "http://arxiv.org/abs/2304.13937",
    "context": "Title: Towards Explainable Collaborative Filtering with Taste Clusters Learning. (arXiv:2304.13937v1 [cs.IR])\nAbstract: Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements.  In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations ",
    "path": "papers/23/04/2304.13937.json",
    "total_tokens": 996,
    "translated_title": "用品味聚类学习实现可解释性协同过滤",
    "translated_abstract": "协同过滤是推荐系统中广泛使用且有效的技术。近年来，基于潜在嵌入的协同过滤方法（如矩阵分解、神经协同过滤和LightGCN）已经有了显著的进展，以提高准确性。但是，这些模型的可解释性尚未得到充分探索。给推荐模型添加解释性，不仅可以增加人们对决策过程的信任，而且还有多个好处，如为项目推荐提供有说服力的解释、为用户和项目创建明确的文件、为项目制造商提供设计改进的协助。在本文中，我们提出了一种清晰有效的可解释性协同过滤模型，利用可解释的聚类学习来实现两个最苛刻的目标：（1）精确——模型在追求可解释性时不应妥协准确性；（2）自我解释——模型的解释应易于人们理解。引入品味聚类学习来构成用户和项目的解释，并在四个真实数据集上进行实验，结果证实了我们提出的方法在提供人类可理解的解释的同时保证了高准确性。",
    "tldr": "本文提出了一种利用品味聚类学习实现可解释性协同过滤的模型，在保证高准确性的同时为用户和项目提供可解释的聚类解释。",
    "en_tdlr": "This paper proposes an Explainable Collaborative Filtering model that leverages interpretable cluster learning, specifically taste clusters learning, to provide human-understandable explanations while maintaining high accuracy in recommendation."
}