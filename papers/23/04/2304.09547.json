{
    "title": "Graph Exploration for Effective Multi-agent Q-Learning. (arXiv:2304.09547v1 [cs.LG])",
    "abstract": "This paper proposes an exploration technique for multi-agent reinforcement learning (MARL) with graph-based communication among agents. We assume the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighbouring agents collaborate to estimate the uncertainty about the state-action space in order to execute more efficient explorative behaviour. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for continuou",
    "link": "http://arxiv.org/abs/2304.09547",
    "context": "Title: Graph Exploration for Effective Multi-agent Q-Learning. (arXiv:2304.09547v1 [cs.LG])\nAbstract: This paper proposes an exploration technique for multi-agent reinforcement learning (MARL) with graph-based communication among agents. We assume the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighbouring agents collaborate to estimate the uncertainty about the state-action space in order to execute more efficient explorative behaviour. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for continuou",
    "path": "papers/23/04/2304.09547.json",
    "total_tokens": 876,
    "translated_title": "有效的多智能体Q学习的图探索方法",
    "translated_abstract": "本文为基于图通信的多智能体强化学习（MARL）提出了一种探索技术。我们假设智能体收到的个体奖励和其他智能体的动作无关，但策略相互耦合。在所提出的框架中，相邻的智能体合作估计状态行为空间不确定性，以执行更有效的探索行为。与现有方法不同，所提出的算法不需要计数机制，并可以应用于连续状态环境，而无需复杂的转换技术。此外，所提出的方案允许智能体以完全分散的方式进行通信，最小化信息交换。对于连续状态场景，每个智能体只需要交换一个参数向量。离散状态场景的理论结果以及连续状态的实验验证了算法的性能。",
    "tldr": "本文提出了一种基于图通信的多智能体强化学习探索技术，在智能体的协作下估计状态行为空间不确定性，以实现更有效的探索行为，同时也不需要计数机制和复杂的转换技术，该方案允许智能体在完全分散的方式下进行通信。",
    "en_tdlr": "This paper proposes a graph-based exploration technique for multi-agent reinforcement learning, in which neighbouring agents collaborate to estimate the uncertainty about the state-action space and execute more efficient explorative behaviour. The proposed algorithm does not require counting mechanisms or complex conversion techniques, and allows agents to communicate in a fully decentralized manner with minimal information exchange."
}