{
    "title": "A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)",
    "abstract": "Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training c",
    "link": "http://arxiv.org/abs/2304.08109",
    "context": "Title: A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)\nAbstract: Recently, the instruction-tuning of large language models is a crucial area of research in the field of natural language processing. Due to resource and cost limitations, several researchers have employed parameter-efficient tuning techniques, such as LoRA, for instruction tuning, and have obtained encouraging results In comparison to full-parameter fine-tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine-tuning and LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental results show that the selection of the foundational model, training dataset scale, learnable parameter quantity, and model training cost are all important factors. We hope that the experimental conclusions of this paper can provide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training c",
    "path": "papers/23/04/2304.08109.json",
    "total_tokens": 860,
    "translated_title": "基于中文指令数据的全参数和LoRA调参方法在指令遵循大型语言模型上的比较研究",
    "translated_abstract": "近来，大型语言模型的指令调参研究是自然语言处理领域的一个关键研究方向。由于资源与成本限制，一些研究者采用参数效率高的调参技术，例如LoRA，在指令调参中取得了令人鼓舞的结果。本文以LLaMA为基础模型，对全参数调参和LoRA调参方法进行了实验比较。实验结果表明，基础模型的选择、训练数据集规模、可学参数量以及模型训练成本都是重要因素。希望本文的实验结论能为大型语言模型的训练提供启示，特别是在中文领域，并帮助研究人员找到更好的训练成本与性能的平衡策略。",
    "tldr": "采用参数效率高的LoRA调参技术可以在指令调参中取得令人鼓舞的结果，但全参数调参方法在基础模型选择、训练数据集规模、可学参数量等方面也有重要作用。"
}