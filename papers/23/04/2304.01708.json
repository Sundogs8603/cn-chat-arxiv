{
    "title": "Learning and Concentration for High Dimensional Linear Gaussians: an Invariant Subspace Approach. (arXiv:2304.01708v1 [stat.ML])",
    "abstract": "In this work, we study non-asymptotic bounds on correlation between two time realizations of stable linear systems with isotropic Gaussian noise. Consequently, via sampling from a sub-trajectory and using \\emph{Talagrands'} inequality, we show that empirical averages of reward concentrate around steady state (dynamical system mixes to when closed loop system is stable under linear feedback policy ) reward , with high-probability. As opposed to common belief of larger the spectral radius stronger the correlation between samples, \\emph{large discrepancy between algebraic and geometric multiplicity of system eigenvalues leads to large invariant subspaces related to system-transition matrix}; once the system enters the large invariant subspace it will travel away from origin for a while before coming close to a unit ball centered at origin where an isotropic Gaussian noise can with high probability allow it to escape the current invariant subspace it resides in, leading to \\emph{bottleneck",
    "link": "http://arxiv.org/abs/2304.01708",
    "context": "Title: Learning and Concentration for High Dimensional Linear Gaussians: an Invariant Subspace Approach. (arXiv:2304.01708v1 [stat.ML])\nAbstract: In this work, we study non-asymptotic bounds on correlation between two time realizations of stable linear systems with isotropic Gaussian noise. Consequently, via sampling from a sub-trajectory and using \\emph{Talagrands'} inequality, we show that empirical averages of reward concentrate around steady state (dynamical system mixes to when closed loop system is stable under linear feedback policy ) reward , with high-probability. As opposed to common belief of larger the spectral radius stronger the correlation between samples, \\emph{large discrepancy between algebraic and geometric multiplicity of system eigenvalues leads to large invariant subspaces related to system-transition matrix}; once the system enters the large invariant subspace it will travel away from origin for a while before coming close to a unit ball centered at origin where an isotropic Gaussian noise can with high probability allow it to escape the current invariant subspace it resides in, leading to \\emph{bottleneck",
    "path": "papers/23/04/2304.01708.json",
    "total_tokens": 928,
    "tldr": "该论文研究了高维线性高斯系统的学习与集中问题，提出了一种不变子空间方法，并发现代数多重性和几何多重性之间的差异会导致不变子空间的存在，对于系统的学习与集中过程产生影响。",
    "en_tdlr": "This paper studies the learning and concentration problem of high dimensional linear Gaussians and proposes an invariant subspace approach. The paper finds that the difference between the algebraic and geometric multiplicities leads to the existence of invariant subspaces that affect the learning and concentration process."
}