{
    "title": "Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation. (arXiv:2304.13350v1 [cs.AI])",
    "abstract": "In this paper, we define a neuro-symbolic approach to address the task of finding semantically similar clones for the codes of the legacy programming language COBOL, without training data. We define a meta-model that is instantiated to have an Intermediate Representation (IR) in the form of Abstract Syntax Trees (ASTs) common across codes in C and COBOL. We linearize the IRs using Structure Based Traversal (SBT) to create sequential inputs. We further fine-tune UnixCoder, the best-performing model for zero-shot cross-programming language code search, for the Code Cloning task with the SBT IRs of C code-pairs, available in the CodeNet dataset. This allows us to learn latent representations for the IRs of the C codes, which are transferable to the IRs of the COBOL codes. With this fine-tuned UnixCoder, we get a performance improvement of 12.85 MAP@2 over the pre-trained UniXCoder model, in a zero-shot setting, on the COBOL test split synthesized from the CodeNet dataset. This demonstrate",
    "link": "http://arxiv.org/abs/2304.13350",
    "context": "Title: Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation. (arXiv:2304.13350v1 [cs.AI])\nAbstract: In this paper, we define a neuro-symbolic approach to address the task of finding semantically similar clones for the codes of the legacy programming language COBOL, without training data. We define a meta-model that is instantiated to have an Intermediate Representation (IR) in the form of Abstract Syntax Trees (ASTs) common across codes in C and COBOL. We linearize the IRs using Structure Based Traversal (SBT) to create sequential inputs. We further fine-tune UnixCoder, the best-performing model for zero-shot cross-programming language code search, for the Code Cloning task with the SBT IRs of C code-pairs, available in the CodeNet dataset. This allows us to learn latent representations for the IRs of the C codes, which are transferable to the IRs of the COBOL codes. With this fine-tuned UnixCoder, we get a performance improvement of 12.85 MAP@2 over the pre-trained UniXCoder model, in a zero-shot setting, on the COBOL test split synthesized from the CodeNet dataset. This demonstrate",
    "path": "papers/23/04/2304.13350.json",
    "total_tokens": 1226,
    "translated_title": "跨语言中间表示的神经符号式零样本代码克隆",
    "translated_abstract": "本文提出了一种神经符号式的方法来解决COBOL编程语言中的代码语义相似的克隆任务，无需训练数据。我们定义了一个元模型，采用在C和COBOL中的代码都可用的ASTs（抽象语法树）作为中间表示。使用基于结构遍历（SBT）的方法将IRs线性化，以创建顺序输入。进一步对UnixCoder进行微调，在CodeNet数据集上使用SBT IR的C代码对进行零样本跨编程语言代码搜索的任务，以进行代码克隆。使用这个精调的UnixCoder，相对于预先训练的UniXCoder模型，在COBOL测试数据中获得了12.85 MAP @ 2的性能提升。",
    "tldr": "本研究介绍了一种可以跨越多种编程语言，通过神经符号学方法进行零样本跨编程语言代码搜索的中间表示方式，并在COBOL编程语言上进行了验证，使得代码克隆任务的性能获得了12.85 MAP @ 2的提升。",
    "en_tdlr": "This paper proposes a neuro-symbolic approach for finding semantically similar clones for COBOL programming language codes without training data. By defining a meta-model with an Intermediate Representation in the form of Abstract Syntax Trees common across codes in C and COBOL, and using Structure Based Traversal to linearize the IRs and create sequential inputs, the authors fine-tune UnixCoder for the Code Cloning task with the SBT IRs of C code-pairs. The result, with a 12.85 MAP @ 2 performance improvement, demonstrates a successful zero-shot cross-programming language code search method."
}