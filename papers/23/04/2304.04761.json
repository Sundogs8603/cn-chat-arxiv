{
    "title": "Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])",
    "abstract": "Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims",
    "link": "http://arxiv.org/abs/2304.04761",
    "context": "Title: Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])\nAbstract: Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims",
    "path": "papers/23/04/2304.04761.json",
    "total_tokens": 904,
    "translated_title": "将机器学习中的公平性与公共卫生平等联系起来",
    "translated_abstract": "机器学习已成为公共卫生中至关重要的工具，有望提高人口健康、诊断、治疗选择和卫生系统效率。然而，数据和模型设计中的偏见可能导致某些受保护群体的不平等，并放大现有的医疗保健不平等。为了解决这一挑战，本研究总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差。该框架提供了指导，可以将公平性纳入典型的机器学习流程的不同阶段，如数据处理、模型设计、部署和评估。为了说明数据中偏见对机器学习模型的影响，我们提供了例子，展示了系统性偏见如何通过模型预测被放大。这些案例展示了如何使用该框架来预防这些偏见，并强调了在公共卫生领域需要公平和公正的机器学习模型。",
    "tldr": "这篇论文总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差，强调在公共卫生领域需要公平和公正的机器学习模型。",
    "en_tdlr": "This paper summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model to prevent amplification of existing healthcare inequalities for certain protected groups."
}