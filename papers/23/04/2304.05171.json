{
    "title": "Curriculum-Based Imitation of Versatile Skills. (arXiv:2304.05171v1 [cs.LG])",
    "abstract": "Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context",
    "link": "http://arxiv.org/abs/2304.05171",
    "context": "Title: Curriculum-Based Imitation of Versatile Skills. (arXiv:2304.05171v1 [cs.LG])\nAbstract: Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context",
    "path": "papers/23/04/2304.05171.json",
    "total_tokens": 882,
    "translated_title": "基于课程的多技能模仿学习",
    "translated_abstract": "通过模仿学习技能是为机器人提供直观教学的有前途的概念。学习此类技能的一种常见方法是通过最大化给定演示下的似然来学习参数模型。然而，人类演示往往是多模态的，即相同的任务以多种方式解决，这对大多数基于最大似然（ML）目标的模仿学习方法构成了重大挑战。ML目标强制模型涵盖所有数据，防止它在上下文空间中专业化，并可能导致行为空间中的模式平均化，从而导致次优或潜在的灾难性行为。在这里，我们通过引入一个课程，为每个数据点引入一个权重来缓解这些问题，允许模型专门处理它可以代表的数据，同时通过熵奖励来激励模型尽可能涵盖更多的数据。我们将算法扩展到线性专家的混合物，使单个组件可以专门处理局部内容。",
    "tldr": "论文提出了基于课程的多技能模仿学习算法，以专业化处理局部内容，同时通过奖励机制激励尽可能多地涵盖数据。",
    "en_tdlr": "The paper proposes a curriculum-based imitation learning algorithm for versatile skills, aiming to specialize in handling local context while incentivizing the model to cover as much data as possible through an entropy bonus."
}