{
    "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement. (arXiv:2304.03946v1 [cs.DC])",
    "abstract": "With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.  In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and oppo",
    "link": "http://arxiv.org/abs/2304.03946",
    "context": "Title: FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement. (arXiv:2304.03946v1 [cs.DC])\nAbstract: With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.  In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and oppo",
    "path": "papers/23/04/2304.03946.json",
    "total_tokens": 843,
    "translated_title": "FlexMoE：通过动态设备配置扩展大规模稀疏预训练模型训练",
    "translated_abstract": "随着数据量的增加，使用大规模预训练模型以将知识存储到大量模型参数中成为一种趋势。这些模型的训练由大量的密集代数操作组成，需要大量的硬件资源。最近，稀疏门控专家混合（MoE）越来越受欢迎，并在各种下游任务中展示了可观的预训练可扩展性。然而，由于路由不平衡和波动问题，这种稀疏条件计算在实际系统中可能并不如预期那么有效。通常，MoE正在成为数据生命周期中的新的数据分析范例，并面临着以前从未有过的规模、复杂性和颗粒度的独特挑战。",
    "tldr": "本文提出了一个名为FlexMoE的DNN训练框架，通过动态数据流系统性、透明地解决动态数据流带来的低效问题，提高了稀疏门控专家混合的训练效率。"
}