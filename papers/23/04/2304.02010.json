{
    "title": "Multi-Level Contrastive Learning for Dense Prediction Task. (arXiv:2304.02010v1 [cs.CV])",
    "abstract": "In this work, we present Multi-Level Contrastive Learning for Dense Prediction Task (MCL), an efficient self-supervised method for learning region-level feature representation for dense prediction tasks. Our method is motivated by the three key factors in detection: localization, scale consistency and recognition. To explicitly encode absolute position and scale information, we propose a novel pretext task that assembles multi-scale images in a montage manner to mimic multi-object scenarios. Unlike the existing image-level self-supervised methods, our method constructs a multi-level contrastive loss that considers each sub-region of the montage image as a singleton. Our method enables the neural network to learn regional semantic representations for translation and scale consistency while reducing pre-training epochs to the same as supervised pre-training. Extensive experiments demonstrate that MCL consistently outperforms the recent state-of-the-art methods on various datasets with si",
    "link": "http://arxiv.org/abs/2304.02010",
    "context": "Title: Multi-Level Contrastive Learning for Dense Prediction Task. (arXiv:2304.02010v1 [cs.CV])\nAbstract: In this work, we present Multi-Level Contrastive Learning for Dense Prediction Task (MCL), an efficient self-supervised method for learning region-level feature representation for dense prediction tasks. Our method is motivated by the three key factors in detection: localization, scale consistency and recognition. To explicitly encode absolute position and scale information, we propose a novel pretext task that assembles multi-scale images in a montage manner to mimic multi-object scenarios. Unlike the existing image-level self-supervised methods, our method constructs a multi-level contrastive loss that considers each sub-region of the montage image as a singleton. Our method enables the neural network to learn regional semantic representations for translation and scale consistency while reducing pre-training epochs to the same as supervised pre-training. Extensive experiments demonstrate that MCL consistently outperforms the recent state-of-the-art methods on various datasets with si",
    "path": "papers/23/04/2304.02010.json",
    "total_tokens": 934,
    "translated_title": "多层级对比学习在密集预测任务中的应用",
    "translated_abstract": "本文提出了一种称为MCL的自监督方法，用于学习密集预测任务的区域级特征表示。该方法考虑了目标检测的三个关键因素：定位性、尺度一致性和识别性。为了明确地编码绝对位置和尺度信息，我们提出了一种新的预训练任务，将多尺度图像以拼贴的方式组装起来，模拟多目标场景。与现有的图像级自监督方法不同，我们的方法构建了一个多层级对比损失，将拼贴图像的每个子区域视为具有单个样本。我们的方法能够使神经网络学习区域语义表示，从而实现平移和尺度一致性，并将预训练轮数降至与监督预训练相同。大量实验证明，MCL在各种数据集上始终优于最新的最先进方法。",
    "tldr": "本文提出了一种多层级对比学习的自监督方法，用于学习密集预测任务的区域级特征表示。该方法可通过一个新的预训练任务为神经网络学习区域语义表示，从而实现平移和尺度一致性，并将预训练轮数降至与监督预训练相同。该方法已在多个数据集上证明了其优越性能。",
    "en_tdlr": "This paper proposes a multi-level contrastive learning self-supervised method for learning region-level feature representation for dense prediction task. The method enables the neural network to learn regional semantic representations for translation and scale consistency while reducing pre-training epochs to the same as supervised pre-training. The experiments demonstrate that this method outperforms the recent state-of-the-art methods on various datasets."
}