{
    "title": "Analysing Fairness of Privacy-Utility Mobility Models. (arXiv:2304.06469v1 [cs.LG])",
    "abstract": "Preserving the individuals' privacy in sharing spatial-temporal datasets is critical to prevent re-identification attacks based on unique trajectories. Existing privacy techniques tend to propose ideal privacy-utility tradeoffs, however, largely ignore the fairness implications of mobility models and whether such techniques perform equally for different groups of users. The quantification between fairness and privacy-aware models is still unclear and there barely exists any defined sets of metrics for measuring fairness in the spatial-temporal context. In this work, we define a set of fairness metrics designed explicitly for human mobility, based on structural similarity and entropy of the trajectories. Under these definitions, we examine the fairness of two state-of-the-art privacy-preserving models that rely on GAN and representation learning to reduce the re-identification rate of users for data sharing. Our results show that while both models guarantee group fairness in terms of de",
    "link": "http://arxiv.org/abs/2304.06469",
    "context": "Title: Analysing Fairness of Privacy-Utility Mobility Models. (arXiv:2304.06469v1 [cs.LG])\nAbstract: Preserving the individuals' privacy in sharing spatial-temporal datasets is critical to prevent re-identification attacks based on unique trajectories. Existing privacy techniques tend to propose ideal privacy-utility tradeoffs, however, largely ignore the fairness implications of mobility models and whether such techniques perform equally for different groups of users. The quantification between fairness and privacy-aware models is still unclear and there barely exists any defined sets of metrics for measuring fairness in the spatial-temporal context. In this work, we define a set of fairness metrics designed explicitly for human mobility, based on structural similarity and entropy of the trajectories. Under these definitions, we examine the fairness of two state-of-the-art privacy-preserving models that rely on GAN and representation learning to reduce the re-identification rate of users for data sharing. Our results show that while both models guarantee group fairness in terms of de",
    "path": "papers/23/04/2304.06469.json",
    "total_tokens": 952,
    "translated_title": "分析隐私-效用移动模型的公平性",
    "translated_abstract": "在共享时空数据集中，保护个人的隐私对于防止基于唯一轨迹的重新识别攻击至关重要。现有的隐私技术往往提出理想的隐私-效用权衡，但基本忽略了移动模型的公平性影响，以及这些技术对不同用户群体是否同等适用。在时空背景下，公平性与隐私意识模型之间的度量仍然不清晰，并且几乎不存在任何定义的公平度量集。在本文中，我们定义了一组专为人类移动性而设计的公平度量指标，基于轨迹的结构相似性和熵。在这些定义下，我们研究了两种最先进的隐私保护模型，在数据共享中依靠生成对抗网络和表示学习降低用户的重新识别率，检查了其公平性。我们的结果表明，虽然这两种模型都保证在组上公平性方面具有优势，但它们在个别公平性方面存在显著区别。",
    "tldr": "本研究为人类移动性定义了一组公平度量指标，基于轨迹的结构相似性和熵，研究了两种依靠生成对抗网络和表示学习降低用户的重新识别率的隐私保护模型，并探讨了它们的公平性优势和差异。"
}