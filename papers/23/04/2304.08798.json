{
    "title": "Large-scale Dynamic Network Representation via Tensor Ring Decomposition. (arXiv:2304.08798v1 [cs.LG])",
    "abstract": "Large-scale Dynamic Networks (LDNs) are becoming increasingly important in the Internet age, yet the dynamic nature of these networks captures the evolution of the network structure and how edge weights change over time, posing unique challenges for data analysis and modeling. A Latent Factorization of Tensors (LFT) model facilitates efficient representation learning for a LDN. But the existing LFT models are almost based on Canonical Polyadic Factorization (CPF). Therefore, this work proposes a model based on Tensor Ring (TR) decomposition for efficient representation learning for a LDN. Specifically, we incorporate the principle of single latent factor-dependent, non-negative, and multiplicative update (SLF-NMU) into the TR decomposition model, and analyze the particular bias form of TR decomposition. Experimental studies on two real LDNs demonstrate that the propose method achieves higher accuracy than existing models.",
    "link": "http://arxiv.org/abs/2304.08798",
    "context": "Title: Large-scale Dynamic Network Representation via Tensor Ring Decomposition. (arXiv:2304.08798v1 [cs.LG])\nAbstract: Large-scale Dynamic Networks (LDNs) are becoming increasingly important in the Internet age, yet the dynamic nature of these networks captures the evolution of the network structure and how edge weights change over time, posing unique challenges for data analysis and modeling. A Latent Factorization of Tensors (LFT) model facilitates efficient representation learning for a LDN. But the existing LFT models are almost based on Canonical Polyadic Factorization (CPF). Therefore, this work proposes a model based on Tensor Ring (TR) decomposition for efficient representation learning for a LDN. Specifically, we incorporate the principle of single latent factor-dependent, non-negative, and multiplicative update (SLF-NMU) into the TR decomposition model, and analyze the particular bias form of TR decomposition. Experimental studies on two real LDNs demonstrate that the propose method achieves higher accuracy than existing models.",
    "path": "papers/23/04/2304.08798.json",
    "total_tokens": 841,
    "translated_title": "基于张量环分解的大规模动态网络表示",
    "translated_abstract": "在互联网时代，大规模动态网络变得越来越重要。然而，这些网络的动态性质捕捉了网络结构的演化和边权重的随时间变化，因此对数据分析和建模提出了独特的挑战。张量的潜在分解（LFT）模型为LDN的有效表示学习提供了便利。但是，现有的LFT模型几乎都是基于正交多项式分解（CPF）的。因此，本研究提出了一种基于张量环（TR）分解的模型，用于LDN的有效表示学习。具体而言，作者将单个潜在因子依赖性、非负性和乘法更新（SLF-NMU）原则纳入TR分解模型，并分析了TR分解的特殊偏置形式。两个真实LDN的实验研究表明，所提出的方法比现有模型达到了更高的精度。",
    "tldr": "本文提出了一种基于张量环分解的模型，用于大规模动态网络的有效表示学习。在两个真实的LDN实验中，该方法比现有的模型具有更高的准确性。",
    "en_tdlr": "This paper proposes a Tensor Ring (TR) decomposition based model for efficient representation learning of Large-scale Dynamic Networks (LDNs). Experimental results on two real LDNs show that the proposed method achieves higher accuracy than existing models."
}