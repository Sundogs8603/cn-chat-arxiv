{
    "title": "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. (arXiv:2304.00958v2 [cs.CL] UPDATED)",
    "abstract": "In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained.",
    "link": "http://arxiv.org/abs/2304.00958",
    "context": "Title: DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. (arXiv:2304.00958v2 [cs.CL] UPDATED)\nAbstract: In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained.",
    "path": "papers/23/04/2304.00958.json",
    "total_tokens": 1004,
    "translated_title": "DrBERT:用于生物医学和临床领域的健壮的法语预训练模型",
    "translated_abstract": "近年来，预训练语言模型（PLMs）在自然语言处理（NLP）任务中取得了最好的性能。虽然最初的模型是使用通用领域数据训练的，但专用于特定领域的模型已经出现，以更有效地处理特定领域的任务。在本文中，我们提出了对法语生物医学领域中PLMs的原始研究。我们首次比较了在公共网络数据和医疗机构的私有数据上训练的PLMs的性能。我们还评估了不同的学习策略在一组生物医学任务上的效果。特别是，我们展示了我们可以利用已经存在的外语生物医学PLMs，并在我们的目标数据上进一步预训练它。最后，我们发布了第一个用于生物医学领域的法语专用PLMs，称为DrBERT，以及这些模型所训练的最大的医学数据语料库。",
    "tldr": "本文提出了DrBERT，用于生物医学和临床领域的健壮的法语预训练模型，并通过对公共数据和医疗机构的私有数据进行性能比较，证明了在特定领域数据上进一步预训练PLMs可以显著提高性能。最终，发布了首个用于生物医学领域的法语专用PLMs，以及最大的医学数据语料库。",
    "en_tdlr": "This paper proposes DrBERT, a robust pre-trained model in French for biomedical and clinical domains. By comparing PLMs trained on both public and private medical data, the authors show that further pre-training on specific domain data can significantly improve performance. The authors also release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained."
}