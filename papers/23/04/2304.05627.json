{
    "title": "Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation. (arXiv:2304.05627v1 [cs.NE])",
    "abstract": "Spiking neural networks (SNNs) are well known as the brain-inspired models with high computing efficiency, due to a key component that they utilize spikes as information units, close to the biological neural systems. Although spiking based models are energy efficient by taking advantage of discrete spike signals, their performance is limited by current network structures and their training methods. As discrete signals, typical SNNs cannot apply the gradient descent rules directly into parameters adjustment as artificial neural networks (ANNs). Aiming at this limitation, here we propose a novel method of constructing deep SNN models with knowledge distillation (KD) that uses ANN as teacher model and SNN as student model. Through ANN-SNN joint training algorithm, the student SNN model can learn rich feature information from the teacher ANN model through the KD method, yet it avoids training SNN from scratch when communicating with non-differentiable spikes. Our method can not only build ",
    "link": "http://arxiv.org/abs/2304.05627",
    "context": "Title: Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation. (arXiv:2304.05627v1 [cs.NE])\nAbstract: Spiking neural networks (SNNs) are well known as the brain-inspired models with high computing efficiency, due to a key component that they utilize spikes as information units, close to the biological neural systems. Although spiking based models are energy efficient by taking advantage of discrete spike signals, their performance is limited by current network structures and their training methods. As discrete signals, typical SNNs cannot apply the gradient descent rules directly into parameters adjustment as artificial neural networks (ANNs). Aiming at this limitation, here we propose a novel method of constructing deep SNN models with knowledge distillation (KD) that uses ANN as teacher model and SNN as student model. Through ANN-SNN joint training algorithm, the student SNN model can learn rich feature information from the teacher ANN model through the KD method, yet it avoids training SNN from scratch when communicating with non-differentiable spikes. Our method can not only build ",
    "path": "papers/23/04/2304.05627.json",
    "total_tokens": 889,
    "tldr": "本文提出一种使用知识蒸馏方法构建深度脉冲神经网络的新方法，通过人工神经网络作为教师模型和脉冲神经网络作为学生模型，在联合训练中学习丰富的特征信息，避免了原有方法中使用非可微分信号训练SNN的缺陷。"
}