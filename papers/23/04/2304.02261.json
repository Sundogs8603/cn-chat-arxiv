{
    "title": "Optimal Sketching Bounds for Sparse Linear Regression. (arXiv:2304.02261v1 [cs.DS])",
    "abstract": "We study oblivious sketching for $k$-sparse linear regression under various loss functions such as an $\\ell_p$ norm, or from a broad class of hinge-like loss functions, which includes the logistic and ReLU losses. We show that for sparse $\\ell_2$ norm regression, there is a distribution over oblivious sketches with $\\Theta(k\\log(d/k)/\\varepsilon^2)$ rows, which is tight up to a constant factor. This extends to $\\ell_p$ loss with an additional additive $O(k\\log(k/\\varepsilon)/\\varepsilon^2)$ term in the upper bound. This establishes a surprising separation from the related sparse recovery problem, which is an important special case of sparse regression. For this problem, under the $\\ell_2$ norm, we observe an upper bound of $O(k \\log (d)/\\varepsilon + k\\log(k/\\varepsilon)/\\varepsilon^2)$ rows, showing that sparse recovery is strictly easier to sketch than sparse regression. For sparse regression under hinge-like loss functions including sparse logistic and sparse ReLU regression, we giv",
    "link": "http://arxiv.org/abs/2304.02261",
    "context": "Title: Optimal Sketching Bounds for Sparse Linear Regression. (arXiv:2304.02261v1 [cs.DS])\nAbstract: We study oblivious sketching for $k$-sparse linear regression under various loss functions such as an $\\ell_p$ norm, or from a broad class of hinge-like loss functions, which includes the logistic and ReLU losses. We show that for sparse $\\ell_2$ norm regression, there is a distribution over oblivious sketches with $\\Theta(k\\log(d/k)/\\varepsilon^2)$ rows, which is tight up to a constant factor. This extends to $\\ell_p$ loss with an additional additive $O(k\\log(k/\\varepsilon)/\\varepsilon^2)$ term in the upper bound. This establishes a surprising separation from the related sparse recovery problem, which is an important special case of sparse regression. For this problem, under the $\\ell_2$ norm, we observe an upper bound of $O(k \\log (d)/\\varepsilon + k\\log(k/\\varepsilon)/\\varepsilon^2)$ rows, showing that sparse recovery is strictly easier to sketch than sparse regression. For sparse regression under hinge-like loss functions including sparse logistic and sparse ReLU regression, we giv",
    "path": "papers/23/04/2304.02261.json",
    "total_tokens": 992,
    "translated_title": "稀疏线性回归的最优草图界限",
    "translated_abstract": "本文研究了各种损失函数下k-稀疏线性回归的遗忘草图，如l_p范数或广泛的hinge-like损失函数类，其中包括logistic和ReLU损失。我们表明，对于稀疏l_2范数回归，存在一个遗忘草图分布，具有Θ(klog(d/k)/ε^2)排，这是紧的，直到一个常数因子。这扩展到l_p损失，上界还有一个附加的O(klog(k/ε)/ε^2)项。这建立了与相关的稀疏恢复问题的出人意料的分离，这是稀疏回归的一个重要特例。对于这个问题，在l_2范数下，我们观察到一个O(klog(d)/ε+klog(k/ε)/ε^2)行的上界，表明稀疏恢复比稀疏回归更容易草图。对于包括稀疏logistic和稀疏ReLU回归在内的hinge-like损失函数下的稀疏回归，我们给出了与之对应的最优草图界。",
    "tldr": "本文研究稀疏线性回归的最优草图界限，表明稀疏恢复比稀疏回归更容易草图，对于稀疏l_p回归，其上界包括l_2的额外添加项。",
    "en_tdlr": "This paper studies the optimal sketching bounds for sparse linear regression, and shows that sparse recovery is easier to sketch than sparse regression under the l_2 norm. Additionally, an upper bound for sparse l_p regression with an extra term is provided. The results reveal a surprising separation from the related sparse recovery problem."
}