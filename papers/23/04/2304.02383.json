{
    "title": "How good Neural Networks interpretation methods really are? A quantitative benchmark. (arXiv:2304.02383v1 [cs.LG])",
    "abstract": "Saliency Maps (SMs) have been extensively used to interpret deep learning models decision by highlighting the features deemed relevant by the model. They are used on highly nonlinear problems, where linear feature selection (FS) methods fail at highlighting relevant explanatory variables. However, the reliability of gradient-based feature attribution methods such as SM has mostly been only qualitatively (visually) assessed, and quantitative benchmarks are currently missing, partially due to the lack of a definite ground truth on image data. Concerned about the apophenic biases introduced by visual assessment of these methods, in this paper we propose a synthetic quantitative benchmark for Neural Networks (NNs) interpretation methods. For this purpose, we built synthetic datasets with nonlinearly separable classes and increasing number of decoy (random) features, illustrating the challenge of FS in high-dimensional settings. We also compare these methods to conventional approaches such ",
    "link": "http://arxiv.org/abs/2304.02383",
    "context": "Title: How good Neural Networks interpretation methods really are? A quantitative benchmark. (arXiv:2304.02383v1 [cs.LG])\nAbstract: Saliency Maps (SMs) have been extensively used to interpret deep learning models decision by highlighting the features deemed relevant by the model. They are used on highly nonlinear problems, where linear feature selection (FS) methods fail at highlighting relevant explanatory variables. However, the reliability of gradient-based feature attribution methods such as SM has mostly been only qualitatively (visually) assessed, and quantitative benchmarks are currently missing, partially due to the lack of a definite ground truth on image data. Concerned about the apophenic biases introduced by visual assessment of these methods, in this paper we propose a synthetic quantitative benchmark for Neural Networks (NNs) interpretation methods. For this purpose, we built synthetic datasets with nonlinearly separable classes and increasing number of decoy (random) features, illustrating the challenge of FS in high-dimensional settings. We also compare these methods to conventional approaches such ",
    "path": "papers/23/04/2304.02383.json",
    "total_tokens": 864,
    "translated_title": "神经网络解释方法有多好？一个定量基准测试。",
    "translated_abstract": "唯有通过凸显模型认为相关的特征来解释深度学习模型决策的观点显著增加。这些 Saliency Maps（SMs）用于高度非线性问题，超越了线性特征选择 (FS) 方法的局限。然而，SM 等基于梯度的特征归因方法的可靠性主要是通过定性 (视觉上) 评估的，缺乏定量基准测试，部分原因是缺乏图像数据上的定义性标准。本文提出了神经网络 (NNs) 解释方法的合成定量基准测试，并就此目的构建了具有非线性可分离类别和逐渐增加的伪特征数据集。我们也将这些方法与传统的 FS 方法进行了比较。",
    "tldr": "本论文提出了一种定量基准测试方法来评估深度学习模型的解释方法，旨在解决对基于视觉评估方法引入的假阳性偏见的关注。通过构建具有非线性可分离类别和逐渐增加的伪特征数据集的测试，为神经网络解释方法提供了量化标准。",
    "en_tdlr": "This paper proposes a quantitative benchmark to evaluate interpretation methods for deep learning models. It addresses concerns on apophenic biases introduced by visual assessment and provides a synthetic dataset with nonlinearly separable classes and increasing number of decoy features, offering a quantitative standard for neural network interpretation methods."
}