{
    "title": "Semantic Compression With Large Language Models. (arXiv:2304.12512v1 [cs.AI])",
    "abstract": "The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as \"hallucinations\"), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed.  This paper presents three contributions to research on LLMs. First, we present the results from experiments exploring the viability of approximate compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT interfaces.",
    "link": "http://arxiv.org/abs/2304.12512",
    "context": "Title: Semantic Compression With Large Language Models. (arXiv:2304.12512v1 [cs.AI])\nAbstract: The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as \"hallucinations\"), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed.  This paper presents three contributions to research on LLMs. First, we present the results from experiments exploring the viability of approximate compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT interfaces.",
    "path": "papers/23/04/2304.12512.json",
    "total_tokens": 833,
    "translated_title": "基于大型语言模型的语义压缩",
    "translated_abstract": "大型语言模型的兴起正在彻底改变信息检索、问答、摘要和代码生成等任务。然而，除了有时会自信地呈现事实不准确的信息（称为“幻觉”）外，LLMs的输入和输出令牌数量也天生受限，这使它们在处理大量信息或连续流信息的任务上可能不太有效。减小数据的常见方法是通过无损或有损压缩。然而，在某些情况下，如果能够传达所需的语义精度或意图，就不一定需要从原始数据中恢复每个细节 to。本文提出了三个LLMs研究方面的贡献。首先，我们介绍了使用LLMs进行近似压缩的实验结果，重点关注GPT-3.5和GPT-4通过ChatGPT接口。",
    "tldr": "本研究使用大型语言模型（LLMs）进行近似压缩研究。具体实验是以GPT-3.5和GPT-4为基础进行的，旨在探索近似压缩的可行性。",
    "en_tdlr": "This paper explores the viability of approximate compression using large language models (LLMs), specifically GPT-3.5 and GPT-4, and presents three contributions to LLMs research."
}