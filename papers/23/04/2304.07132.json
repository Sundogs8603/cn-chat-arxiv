{
    "title": "Towards Controllable Diffusion Models via Reward-Guided Exploration. (arXiv:2304.07132v1 [cs.LG])",
    "abstract": "By formulating data samples' formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel framework named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum e",
    "link": "http://arxiv.org/abs/2304.07132",
    "context": "Title: Towards Controllable Diffusion Models via Reward-Guided Exploration. (arXiv:2304.07132v1 [cs.LG])\nAbstract: By formulating data samples' formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel framework named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum e",
    "path": "papers/23/04/2304.07132.json",
    "total_tokens": 1041,
    "translated_title": "通过奖励引导探索实现可控扩散模型",
    "translated_abstract": "通过将数据样本的生成形式化为马尔可夫去噪过程，扩散模型在多个任务中实现了最先进的性能。最近，许多扩散模型的变体已被提出，以使生成的样本能够被有效控制。大多数现有方法要么将控制信息作为噪声逼近器的输入（即条件表示），要么在测试阶段引入预先训练的分类器来指导Langevin动力学朝向条件目标。然而，前一种方法仅适用于控制信息可以被表示为条件表示的情况，而后一种方法则需要可微分的预训练辅导分类器。本文提出了一种名为RGDM（奖励引导扩散模型）的新框架，通过强化学习（RL）引导扩散模型的训练阶段。所提出的训练框架将加权对数-似然的目标和最大熵RL目标相结合，从而仅使用非可微分的引导信号即可训练模型。我们的实验表明，RGDM可以生成高质量、细粒度控制的样本，同时实现了与最先进的扩散模型相当的性能。",
    "tldr": "本论文提出了一种新的框架(RGDM)，可以通过强化学习(RL)引导扩散模型的训练阶段，在不需要可微分的引导信号的情况下，实现对生成的样本的有效控制，并生成高质量、细粒度控制的样本。",
    "en_tdlr": "This paper proposes a novel framework (RGDM) that guides the training-phase of diffusion models via reinforcement learning (RL), enabling effective control over generated samples without requiring differentiable guidance signals. High-quality, fine-grained controlled samples can be generated using the proposed method, which achieves comparable performance to state-of-the-art diffusion models."
}