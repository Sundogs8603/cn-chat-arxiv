{
    "title": "Accelerated Doubly Stochastic Gradient Algorithm for Large-scale Empirical Risk Minimization. (arXiv:2304.11665v1 [cs.LG])",
    "abstract": "Nowadays, algorithms with fast convergence, small memory footprints, and low per-iteration complexity are particularly favorable for artificial intelligence applications. In this paper, we propose a doubly stochastic algorithm with a novel accelerating multi-momentum technique to solve large scale empirical risk minimization problem for learning tasks. While enjoying a provably superior convergence rate, in each iteration, such algorithm only accesses a mini batch of samples and meanwhile updates a small block of variable coordinates, which substantially reduces the amount of memory reference when both the massive sample size and ultra-high dimensionality are involved. Empirical studies on huge scale datasets are conducted to illustrate the efficiency of our method in practice.",
    "link": "http://arxiv.org/abs/2304.11665",
    "context": "Title: Accelerated Doubly Stochastic Gradient Algorithm for Large-scale Empirical Risk Minimization. (arXiv:2304.11665v1 [cs.LG])\nAbstract: Nowadays, algorithms with fast convergence, small memory footprints, and low per-iteration complexity are particularly favorable for artificial intelligence applications. In this paper, we propose a doubly stochastic algorithm with a novel accelerating multi-momentum technique to solve large scale empirical risk minimization problem for learning tasks. While enjoying a provably superior convergence rate, in each iteration, such algorithm only accesses a mini batch of samples and meanwhile updates a small block of variable coordinates, which substantially reduces the amount of memory reference when both the massive sample size and ultra-high dimensionality are involved. Empirical studies on huge scale datasets are conducted to illustrate the efficiency of our method in practice.",
    "path": "papers/23/04/2304.11665.json",
    "total_tokens": 794,
    "translated_title": "大规模经验风险最小化加速双随机梯度算法",
    "translated_abstract": "如今，对于人工智能应用来说，具有快速收敛、小内存占用和低每次迭代复杂度的算法特别有利。在本文中，我们提出了一种带有新型加速多动量技术的双随机算法，用于解决大规模经验风险最小化问题。虽然享有可证明的更优收敛速度，但每次迭代中，该算法仅访问一小批样本，同时更新少量变量坐标，大大减少了在涉及大量样本大小和超高维度时的内存引用量。我们进行了针对巨大规模数据集的实证研究，以展示我们的方法在实践中的效率。",
    "tldr": "本文提出了一种针对大规模经验风险最小化问题的加速双随机梯度算法，采用新型加速多动量技术，每次迭代仅访问一小批样本和更新少量变量坐标，具有快速收敛和小内存占用特点。",
    "en_tdlr": "This paper proposes an accelerated doubly stochastic gradient algorithm for large-scale empirical risk minimization problem, which only accesses a mini batch of samples and updates a small block of variable coordinates in each iteration using a novel accelerating multi-momentum technique, and achieves faster convergence and lower memory occupation."
}