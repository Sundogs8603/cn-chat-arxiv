{
    "title": "A vector quantized masked autoencoder for speech emotion recognition. (arXiv:2304.11117v1 [cs.SD])",
    "abstract": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector-quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
    "link": "http://arxiv.org/abs/2304.11117",
    "context": "Title: A vector quantized masked autoencoder for speech emotion recognition. (arXiv:2304.11117v1 [cs.SD])\nAbstract: Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector-quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
    "path": "papers/23/04/2304.11117.json",
    "total_tokens": 864,
    "translated_title": "一种用于语音情感识别的向量量化掩蔽自编码器",
    "translated_abstract": "近年来，深度学习技术的进步使得语音情感识别 (SER) 取得了显著的进展。但标注数据的有限可用性仍然是该领域面临的重要挑战。自监督学习最近已经成为解决这一挑战的一种有前途的解决方案。在本文中，我们提出了向量量化掩蔽自编码器 (VQ-MAE-S)，这是一种自监督模型，它被微调以从语音信号中识别情感。VQ-MAE-S 模型基于运行在向量量化变分自编码器的离散潜在空间中的掩蔽自编码器。实验结果表明，预先在VoxCeleb2数据集上进行了预训练，并在情感语音数据上进行了微调的VQ-MAE-S模型，在SER方面表现优于基于光谱图表示的MAE和其他最先进的方法。",
    "tldr": "本文提出了一种自监督模型 VQ-MAE-S 以识别情感，预训练在 VoxCeleb2 数据集上微调，性能优于其他最先进的方法。",
    "en_tdlr": "This paper proposes a self-supervised model VQ-MAE-S to recognize emotions, pre-trained on VoxCeleb2 dataset and fine-tuned on emotional speech data, and outperforms other state-of-the-art methods in SER."
}