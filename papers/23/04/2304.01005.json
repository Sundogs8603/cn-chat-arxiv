{
    "title": "Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)",
    "abstract": "Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users' sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attack",
    "link": "http://arxiv.org/abs/2304.01005",
    "context": "Title: Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)\nAbstract: Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users' sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attack",
    "path": "papers/23/04/2304.01005.json",
    "total_tokens": 932,
    "translated_title": "基于联邦学习的干净和攻击情景下的多语言表情符号预测",
    "translated_abstract": "联邦学习是机器学习社区中一个日益增长的领域，由于其分散和私密的设计而得到发展。联邦学习中的模型训练分布在多个客户端上，从而提供了大量客户端数据的访问，同时保护了每个客户端数据的隐私性。然后，服务器聚合了在这些多个客户端上进行的训练，而不访问它们的数据，这些数据可能是在任何社交媒体服务和即时通讯平台中广泛使用的表情符号，以表达用户的情感。本文提出了一种基于联邦学习的干净和攻击情境下的多语言表情符号预测。表情符号预测数据从Twitter和SemEval表情符号数据集中获取。使用这些数据来训练和评估不同大小的转换器模型，包括在所有客户端中假定数据干净或在一些客户端中进行标签翻转攻击的稀疏激活转换器。对这些模型的实验结果表明，在干净或攻击情境下，联邦学习可以有效地预测多语言表情符号，同时保持数据隐私。",
    "tldr": "本文提出了一种基于联邦学习的多语言表情符号预测方法，在干净或攻击情境下均有效，同时保护了用户数据隐私。",
    "en_tdlr": "This paper proposes a federated learning-based approach for predicting multilingual emojis in both clean and attack scenarios, which effectively maintains data privacy."
}