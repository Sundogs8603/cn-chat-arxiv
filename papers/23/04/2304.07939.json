{
    "title": "Leveraging sparse and shared feature activations for disentangled representation learning. (arXiv:2304.07939v2 [cs.LG] UPDATED)",
    "abstract": "Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), ",
    "link": "http://arxiv.org/abs/2304.07939",
    "context": "Title: Leveraging sparse and shared feature activations for disentangled representation learning. (arXiv:2304.07939v2 [cs.LG] UPDATED)\nAbstract: Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), ",
    "path": "papers/23/04/2304.07939.json",
    "total_tokens": 886,
    "translated_title": "利用稀疏和共享特征激活进行解缠表示学习",
    "translated_abstract": "迄今为止，恢复高维数据的潜在变化因素一直集中在简单的合成环境中。在大多数基于无监督和弱监督目标的先前研究中，人们忽略了这对真实世界数据的表示学习的积极影响。在本研究中，我们建议利用从多样化监督任务中提取的知识来学习通用的解缠表示。我们假设每个监督任务仅依赖于未知因素的子集，我们将监督多任务模型的特征空间解缠，并在不同任务之间稀疏地激活特征并适当地共享信息。重要的是，我们从未直接观察到变异因素，但在充分性和最小性假设下，访问多个任务足以实现可识别性。我们在六个真实世界分布转移基准以及不同的数据模态（图像，文本）上验证了我们的方法。",
    "tldr": "本文提出了利用从多样化监督任务中提取的知识来学习通用的解缠表示的方法，使监督多任务模型的特征空间得以解缠，适当共享信息，达到可识别性。",
    "en_tdlr": "This work proposes a method of using knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation and achieve identifiability by disentangling the feature space of a supervised multi-task model and appropriately sharing information."
}