{
    "title": "BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer. (arXiv:2304.09649v1 [cs.CL])",
    "abstract": "Retrieval-based language models are increasingly employed in question-answering tasks. These models search in a corpus of documents for relevant information instead of having all factual knowledge stored in its parameters, thereby enhancing efficiency, transparency, and adaptability. We develop the first Norwegian retrieval-based model by adapting the REALM framework and evaluating it on various tasks. After training, we also separate the language model, which we call the reader, from the retriever components, and show that this can be fine-tuned on a range of downstream tasks. Results show that retrieval augmented language modeling improves the reader's performance on extractive question-answering, suggesting that this type of training improves language models' general ability to use context and that this does not happen at the expense of other abilities such as part-of-speech tagging, dependency parsing, named entity recognition, and lemmatization. Code, trained models, and data are ",
    "link": "http://arxiv.org/abs/2304.09649",
    "context": "Title: BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer. (arXiv:2304.09649v1 [cs.CL])\nAbstract: Retrieval-based language models are increasingly employed in question-answering tasks. These models search in a corpus of documents for relevant information instead of having all factual knowledge stored in its parameters, thereby enhancing efficiency, transparency, and adaptability. We develop the first Norwegian retrieval-based model by adapting the REALM framework and evaluating it on various tasks. After training, we also separate the language model, which we call the reader, from the retriever components, and show that this can be fine-tuned on a range of downstream tasks. Results show that retrieval augmented language modeling improves the reader's performance on extractive question-answering, suggesting that this type of training improves language models' general ability to use context and that this does not happen at the expense of other abilities such as part-of-speech tagging, dependency parsing, named entity recognition, and lemmatization. Code, trained models, and data are ",
    "path": "papers/23/04/2304.09649.json",
    "total_tokens": 842,
    "translated_title": "BRENT: 双向检索增强的挪威语Transformer",
    "translated_abstract": "检索式语言模型在问答任务中越来越受到重视。这些模型在语料库中搜索相关信息，而不是将所有事实性知识存储在它的参数中，从而提高了效率、透明度和适应性。我们通过调整REALM框架来开发第一个挪威语检索式模型，并在各种任务上进行了评估。在训练后，我们还将检索组件与语言模型（称为读者）分离，并展示了这可以在各种下游任务中进行微调。结果表明，检索增强的语言建模提高了读者在提取问答方面的表现，这表明这种类型的训练提高了语言模型使用上下文的一般能力，而这并不会牺牲其他能力，例如词性标注、依赖分析、命名实体识别和词形归并。代码、训练模型和数据可在https://github.com/salaniz/BRENT获取。",
    "tldr": "BRENT是第一个使用双向检索提高挪威语的检索式语言模型，可以提高读者在提取问答方面的表现。",
    "en_tdlr": "BRENT is the first Norwegian retrieval-based language model using bidirectional retrieval, and improves the reader's performance on extractive question-answering."
}