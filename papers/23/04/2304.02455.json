{
    "title": "Selecting Features by their Resilience to the Curse of Dimensionality. (arXiv:2304.02455v1 [cs.LG])",
    "abstract": "Real-world datasets are often of high dimension and effected by the curse of dimensionality. This hinders their comprehensibility and interpretability. To reduce the complexity feature selection aims to identify features that are crucial to learn from said data. While measures of relevance and pairwise similarities are commonly used, the curse of dimensionality is rarely incorporated into the process of selecting features. Here we step in with a novel method that identifies the features that allow to discriminate data subsets of different sizes. By adapting recent work on computing intrinsic dimensionalities, our method is able to select the features that can discriminate data and thus weaken the curse of dimensionality. Our experiments show that our method is competitive and commonly outperforms established feature selection methods. Furthermore, we propose an approximation that allows our method to scale to datasets consisting of millions of data points. Our findings suggest that fea",
    "link": "http://arxiv.org/abs/2304.02455",
    "context": "Title: Selecting Features by their Resilience to the Curse of Dimensionality. (arXiv:2304.02455v1 [cs.LG])\nAbstract: Real-world datasets are often of high dimension and effected by the curse of dimensionality. This hinders their comprehensibility and interpretability. To reduce the complexity feature selection aims to identify features that are crucial to learn from said data. While measures of relevance and pairwise similarities are commonly used, the curse of dimensionality is rarely incorporated into the process of selecting features. Here we step in with a novel method that identifies the features that allow to discriminate data subsets of different sizes. By adapting recent work on computing intrinsic dimensionalities, our method is able to select the features that can discriminate data and thus weaken the curse of dimensionality. Our experiments show that our method is competitive and commonly outperforms established feature selection methods. Furthermore, we propose an approximation that allows our method to scale to datasets consisting of millions of data points. Our findings suggest that fea",
    "path": "papers/23/04/2304.02455.json",
    "total_tokens": 969,
    "translated_title": "根据数据的维度鲁棒性选择特征",
    "translated_abstract": "真实世界中的数据集通常维度很高，并受到维度诅咒的影响，这阻碍了它们的可理解性和可解释性。为了降低复杂度，特征选择旨在识别对于学习数据至关重要的特征。虽然相关性和成对相似性的度量通常被使用，但维度诅咒很少被纳入到选择特征的过程中。本文提出了一种新的方法，通过识别能够区分不同大小数据子集的特征来降低维度诅咒。通过调整最近关于计算内在维度的工作，我们的方法能够选择能够区分数据的特征，并因此减弱维度诅咒。我们的实验表明，我们的方法具有竞争力，并通常优于成熟的特征选择方法。此外，我们提出了一个近似方法，使我们的方法可扩展到由数百万数据点组成的数据集。我们的研究结果表明，特征强度方法更加鲁棒，选择的特征更加适用于高维数据集。",
    "tldr": "该研究提出了一种新的特征选择方法，通过识别能够区分不同大小数据子集的特征来降低高维数据的复杂性。实验表明，该方法具有竞争力，并通常优于其他已有的特征选择方法。此外，该方法可扩展到由数百万数据点组成的数据集。",
    "en_tdlr": "This study proposes a novel feature selection method that reduces the complexity of high-dimensional data by identifying the features that can discriminate data subsets of different sizes. The experiments show that the method is competitive and commonly outperforms established feature selection methods. Additionally, the method is scalable to datasets consisting of millions of data points."
}