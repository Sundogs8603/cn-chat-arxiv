{
    "title": "Hear Me Out: A Study on the Use of the Voice Modality for Crowdsourced Relevance Assessments. (arXiv:2304.10881v1 [cs.IR])",
    "abstract": "The creation of relevance assessments by human assessors (often nowadays crowdworkers) is a vital step when building IR test collections. Prior works have investigated assessor quality & behaviour, though into the impact of a document's presentation modality on assessor efficiency and effectiveness. Given the rise of voice-based interfaces, we investigate whether it is feasible for assessors to judge the relevance of text documents via a voice-based interface. We ran a user study (n = 49) on a crowdsourcing platform where participants judged the relevance of short and long documents sampled from the TREC Deep Learning corpus-presented to them either in the text or voice modality. We found that: (i) participants are equally accurate in their judgements across both the text and voice modality; (ii) with increased document length it takes participants significantly longer (for documents of length > 120 words it takes almost twice as much time) to make relevance judgements in the voice con",
    "link": "http://arxiv.org/abs/2304.10881",
    "context": "Title: Hear Me Out: A Study on the Use of the Voice Modality for Crowdsourced Relevance Assessments. (arXiv:2304.10881v1 [cs.IR])\nAbstract: The creation of relevance assessments by human assessors (often nowadays crowdworkers) is a vital step when building IR test collections. Prior works have investigated assessor quality & behaviour, though into the impact of a document's presentation modality on assessor efficiency and effectiveness. Given the rise of voice-based interfaces, we investigate whether it is feasible for assessors to judge the relevance of text documents via a voice-based interface. We ran a user study (n = 49) on a crowdsourcing platform where participants judged the relevance of short and long documents sampled from the TREC Deep Learning corpus-presented to them either in the text or voice modality. We found that: (i) participants are equally accurate in their judgements across both the text and voice modality; (ii) with increased document length it takes participants significantly longer (for documents of length > 120 words it takes almost twice as much time) to make relevance judgements in the voice con",
    "path": "papers/23/04/2304.10881.json",
    "total_tokens": 935,
    "translated_title": "听我说：使用语音调制进行众包相关性评估的研究",
    "translated_abstract": "在构建信息检索测试集合时，人工评估员（现今通常是众包工人）创建相关性评估是至关重要的一步。先前的工作调查了评估员的质量和行为，但并没有研究文档的呈现模式对评估员效率和有效性的影响。鉴于语音界面的普及，我们研究了评估员是否能够通过语音界面判断文本文档的相关性，并在众包平台上对 TREC 深度学习语料库中的短文档和长文档进行了用户研究 (n=49)，向参与者展示了文本和语音模态。我们发现：(i)参与者在文本和语音模态下的判断准确度相同；(ii)随着文档长度的增加，参与者在语音构造下做出相关性判断所需的时间显着增加（对于长度>120个单词的文档，所需时间几乎是文本钟的两倍）。",
    "tldr": "本研究研究了使用语音调制进行众包相关性评估，发现评估员在文本和语音模态下的判断准确度相同，但随着文档长度的增加，在语音构造下做出相关性判断所需的时间显着增加。"
}