{
    "title": "Saddle-to-Saddle Dynamics in Diagonal Linear Networks. (arXiv:2304.00488v2 [cs.LG] UPDATED)",
    "abstract": "In this paper we fully describe the trajectory of gradient flow over diagonal linear networks in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\\ell_1$-norm solution. This saddle-to-saddle dynamics translates to an incremental learning process as each saddle corresponds to the minimiser of the loss constrained to an active set outside of which the coordinates must be zero. We explicitly characterise the visited saddles as well as the jumping times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the heteroclinic transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of acti",
    "link": "http://arxiv.org/abs/2304.00488",
    "context": "Title: Saddle-to-Saddle Dynamics in Diagonal Linear Networks. (arXiv:2304.00488v2 [cs.LG] UPDATED)\nAbstract: In this paper we fully describe the trajectory of gradient flow over diagonal linear networks in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\\ell_1$-norm solution. This saddle-to-saddle dynamics translates to an incremental learning process as each saddle corresponds to the minimiser of the loss constrained to an active set outside of which the coordinates must be zero. We explicitly characterise the visited saddles as well as the jumping times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the heteroclinic transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of acti",
    "path": "papers/23/04/2304.00488.json",
    "total_tokens": 1053,
    "translated_title": "对角线性网络中的鞍点到鞍点动力学",
    "translated_abstract": "在本文中，我们完全描述了在初始化趋近于零的情况下，梯度流在对角线性网络上的轨迹。我们展示了极限流从一个训练损失的鞍点跳到另一个鞍点，直到到达最小的$\\ell_1$-范数解。这种鞍点到鞍点的动力学转化为一个增量式的学习过程，因为每个鞍点对应于在活动集之外坐标必须为零的损失最小化器。我们通过一个递归算法明确刻画了访问过的鞍点以及跳跃时间，这个算法类似于用于计算Lasso路径的LARS算法。我们的证明利用了方便的弧长时间重新参数化，使得我们可以跟踪跳跃之间的异室房转换。我们的分析对数据的要求很低，适用于欠参数化和过参数化的情况，并涵盖了复杂的情况，其中活动数量的单调性问题。",
    "tldr": "本文研究了对角线性网络中的鞍点到鞍点动力学，并展示了在初始化趋近于零的情况下，极限流如何从一个训练损失的鞍点跳到另一个鞍点，直到达到最小的$\\ell_1$-范数解。通过递归算法和弧长时间重新参数化，我们明确刻画了访问过的鞍点和跳跃时间。我们的分析适用于欠参数化和过参数化的情况，也涵盖了活动数量单调性问题的复杂情况。",
    "en_tdlr": "This paper investigates the saddle-to-saddle dynamics in diagonal linear networks and demonstrates how the limiting flow jumps from one saddle of the training loss to another until reaching the minimum $\\ell_1$-norm solution when the initialization approaches zero. The visited saddles and jumping times are explicitly characterized using a recursive algorithm and arc-length time reparametrization. The analysis applies to under and overparametrized settings and covers complex cases with non-monotonicity of the number of active coordinates."
}