{
    "title": "Membrane Potential Distribution Adjustment and Parametric Surrogate Gradient in Spiking Neural Networks. (arXiv:2304.13289v1 [cs.LG])",
    "abstract": "As an emerging network model, spiking neural networks (SNNs) have aroused significant research attentions in recent years. However, the energy-efficient binary spikes do not augur well with gradient descent-based training approaches. Surrogate gradient (SG) strategy is investigated and applied to circumvent this issue and train SNNs from scratch. Due to the lack of well-recognized SG selection rule, most SGs are chosen intuitively. We propose the parametric surrogate gradient (PSG) method to iteratively update SG and eventually determine an optimal surrogate gradient parameter, which calibrates the shape of candidate SGs. In SNNs, neural potential distribution tends to deviate unpredictably due to quantization error. We evaluate such potential shift and propose methodology for potential distribution adjustment (PDA) to minimize the loss of undesired pre-activations. Experimental results demonstrate that the proposed methods can be readily integrated with backpropagation through time (B",
    "link": "http://arxiv.org/abs/2304.13289",
    "context": "Title: Membrane Potential Distribution Adjustment and Parametric Surrogate Gradient in Spiking Neural Networks. (arXiv:2304.13289v1 [cs.LG])\nAbstract: As an emerging network model, spiking neural networks (SNNs) have aroused significant research attentions in recent years. However, the energy-efficient binary spikes do not augur well with gradient descent-based training approaches. Surrogate gradient (SG) strategy is investigated and applied to circumvent this issue and train SNNs from scratch. Due to the lack of well-recognized SG selection rule, most SGs are chosen intuitively. We propose the parametric surrogate gradient (PSG) method to iteratively update SG and eventually determine an optimal surrogate gradient parameter, which calibrates the shape of candidate SGs. In SNNs, neural potential distribution tends to deviate unpredictably due to quantization error. We evaluate such potential shift and propose methodology for potential distribution adjustment (PDA) to minimize the loss of undesired pre-activations. Experimental results demonstrate that the proposed methods can be readily integrated with backpropagation through time (B",
    "path": "papers/23/04/2304.13289.json",
    "total_tokens": 981,
    "translated_title": "膜电位分布调整和参数化代理梯度在尖峰神经网络中的应用",
    "translated_abstract": "作为一种新兴的网络模型，尖峰神经网络（SNN）近年来引起了重大的研究关注。然而，高效能的二进制脉冲信号并不适用于基于梯度下降的训练方法。代理梯度（SG）策略被研究和应用于绕过这个问题并从零开始训练SNN。由于缺乏公认的SG选择规则，大多数SG被直观地选择。我们提出了参数化代理梯度（PSG）方法来迭代地更新SG，并最终确定最佳代理梯度参数，该参数校准了候选SG的形状。在SNN中，由于量化误差，神经电位分布往往会出现不可预测的偏差。我们评估了这种潜在的偏移并提出了潜在分布调整（PDA）的方法来最小化不希望的预激活损失。实验结果表明，所提出的方法可以轻松地与时间反向传播（BPTT）算法集成，并在基准数据集上实现优异的性能。",
    "tldr": "本论文提出了一种参数化代理梯度（PSG）方法和潜在分布调整（PDA）方法，以解决尖峰神经网络（SNN）中梯度下降训练的问题，并在基准数据集上实现了优异的性能。",
    "en_tdlr": "This paper proposes a parametric surrogate gradient (PSG) method and potential distribution adjustment (PDA) method to tackle the problem of gradient descent-based training in spiking neural networks (SNNs), achieving superior performance on benchmark datasets."
}