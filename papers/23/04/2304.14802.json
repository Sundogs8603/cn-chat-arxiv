{
    "title": "ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])",
    "abstract": "Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empiric",
    "link": "http://arxiv.org/abs/2304.14802",
    "context": "Title: ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])\nAbstract: Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empiric",
    "path": "papers/23/04/2304.14802.json",
    "total_tokens": 927,
    "translated_title": "ResiDual：具有双重残差连接的Transformer",
    "translated_abstract": "由于其卓越的性能，Transformer网络已成为许多任务的首选架构。然而，如何最优化地实现Transformer中的残差连接仍存在争议，而这些残差连接对于有效训练是必不可少的。两个广泛使用的变体是Post-Layer-Normalization(Post-LN)和Pre-Layer-Normalization(Pre-LN) Transformers，它们分别在每个残差块的输出之后或输入之前应用层规范化。尽管两种变体都有它们的优点，但也存在严重的局限性：Post-LN会导致梯度消失问题，从而阻碍训练深层Transformer，而Pre-LN会导致表示崩溃问题，限制模型容量。本文提出了一种新颖的Transformer架构ResiDual，具有Pre-Post-LN(PPLN)，它将Post-LN和Pre-LN中的连接融合在一起，继承了它们的优点，同时避免了它们的局限性。我们进行了理论分析和实证评估，表明ResiDual比现有方法优越，尤其是训练非常深的Transformer。",
    "tldr": "本文提出了具有Pre-Post-LN双重残差连接的新型Transformer架构ResiDual，解决了Post-LN和Pre-LN存在的问题，并具有优越的性能表现。"
}