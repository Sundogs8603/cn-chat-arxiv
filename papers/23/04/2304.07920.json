{
    "title": "Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. (arXiv:2304.07920v2 [cs.IR] UPDATED)",
    "abstract": "Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this p",
    "link": "http://arxiv.org/abs/2304.07920",
    "context": "Title: Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. (arXiv:2304.07920v2 [cs.IR] UPDATED)\nAbstract: Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this p",
    "path": "papers/23/04/2304.07920.json",
    "total_tokens": 880,
    "translated_title": "通过离线强化学习实现因果决策Transformer的推荐系统",
    "translated_abstract": "最近，基于强化学习的推荐系统已经变得越来越流行。然而，优化推荐策略的奖励函数的设计往往并不简单。探索用户行为背后的因果关系可以替代奖励函数，指导代理捕捉用户的动态兴趣。此外，由于仿真环境的典型限制（如数据效率），大部分研究无法广泛应用于大规模情境。尽管一些研究尝试将离线数据集转化为仿真器，但数据效率使学习过程变得更加缓慢。由于强化学习的本质是通过相互作用进行学习，它无法在单次交互过程中收集足够的数据进行训练。此外，传统的强化学习算法不像监督学习方法那样具备直接从离线数据集进行学习的牢固能力。",
    "tldr": "本论文提出了一种基于因果决策Transformer和离线强化学习的推荐系统，通过探索用户行为的因果关系，指导代理捕捉动态兴趣，并解决了大规模情境下数据效率低的问题。",
    "en_tdlr": "This paper proposes a recommender system based on causal decision transformer and offline reinforcement learning, which explores the causal relationship of user behavior to guide the agent in capturing dynamic interests and addresses the issue of low data efficiency in large-scale situations."
}