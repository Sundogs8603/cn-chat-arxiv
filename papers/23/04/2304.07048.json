{
    "title": "Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])",
    "abstract": "PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \\emph{Wasserstein PAC-Bayes} framework, briefly introduced in \\cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \\citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.",
    "link": "http://arxiv.org/abs/2304.07048",
    "context": "Title: Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])\nAbstract: PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \\emph{Wasserstein PAC-Bayes} framework, briefly introduced in \\cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \\citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.",
    "path": "papers/23/04/2304.07048.json",
    "total_tokens": 935,
    "translated_title": "Wasserstein PAC-Bayes 学习：泛化与优化之间的桥梁。",
    "translated_abstract": "PAC-Bayes 学习是一种已建立的框架，用于在训练阶段评估学习算法的泛化能力。然而，在训练之前，弄清楚为什么知名算法的输出具有良好的泛化特性而 PAC-Bayes 是否有用仍然具有挑战性。我们通过扩展简要介绍在文献 \\cite{amit2022ipm} 中提出的 \\emph{Wasserstein PAC-Bayes} 框架来积极回答这个问题。我们提供了新的泛化界限，利用损失函数上的几何假设。使用我们的框架，我们在任何训练之前就证明了 \\cite{lambert2022variational} 中算法的输出具有强大的渐近泛化能力。更具体地说，我们展示了如何在泛化框架中将优化结果结合起来，构建了 PAC-Bayes 和优化算法之间的桥梁。",
    "tldr": "本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \\cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。",
    "en_tdlr": "This paper introduces an extended Wasserstein PAC-Bayes framework with new generalization bounds based on geometric assumptions on the loss function. The framework is used to prove the strong asymptotic generalization ability of an algorithm from \\citet{lambert2022variational} before any training, and to build a bridge between PAC-Bayes and optimization algorithms."
}