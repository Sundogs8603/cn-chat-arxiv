{
    "title": "Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. (arXiv:2304.12961v1 [cs.LG])",
    "abstract": "In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrat",
    "link": "http://arxiv.org/abs/2304.12961",
    "context": "Title: Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. (arXiv:2304.12961v1 [cs.LG])\nAbstract: In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrat",
    "path": "papers/23/04/2304.12961.json",
    "total_tokens": 958,
    "translated_title": "变色龙: 适应对等镜像以植入耐用后门来进行联邦学习",
    "translated_abstract": "在联邦学习系统中，分布式客户端上传其本地模型到中心服务器以聚合成全局模型。恶意客户端可以通过上传有毒的本地模型来在全局模型中植入后门，导致具有特定模式的图像被错误分类为某些目标标签。当前攻击植入的后门是不耐用的，一旦攻击者停止模型中毒，便会迅速消失。本文研究了FL后门的耐用性与良性图象和有毒图象之间的关系(即在本地训练期间标签被翻转为目标标签的图像)。具体来说，发现原始图象和有毒图象的目标标签对后门的耐久性有关键影响。因此，我们提出了一种新型攻击，称为\"变色龙\"，它利用对比学习进一步放大这种影响来实现更耐用的后门。广泛的实验证明了我们的攻击在各种FL设置中的有效性。",
    "tldr": "本论文提出了一种新的攻击方法\"变色龙\"，可以在联邦学习中实现更加耐用的后门攻击，通过对提供的良性图像和有毒图像目标标签之间的关系进行对比学习，取得了显著的实验效果。",
    "en_tdlr": "The paper proposes a novel attack method called \"Chameleon\" that can achieve a more durable backdoor attack in federated learning, by amplifying the effects of the relationship between benign and poisoned images' target labels through contrastive learning. The extensive experiments have demonstrated the effectiveness of this attack."
}