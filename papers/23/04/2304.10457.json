{
    "title": "Angle based dynamic learning rate for gradient descent. (arXiv:2304.10457v1 [cs.LG])",
    "abstract": "In our work, we propose a novel yet simple approach to obtain an adaptive learning rate for gradient-based descent methods on classification tasks. Instead of the traditional approach of selecting adaptive learning rates via the decayed expectation of gradient-based terms, we use the angle between the current gradient and the new gradient: this new gradient is computed from the direction orthogonal to the current gradient, which further helps us in determining a better adaptive learning rate based on angle history, thereby, leading to relatively better accuracy compared to the existing state-of-the-art optimizers. On a wide variety of benchmark datasets with prominent image classification architectures such as ResNet, DenseNet, EfficientNet, and VGG, we find that our method leads to the highest accuracy in most of the datasets. Moreover, we prove that our method is convergent.",
    "link": "http://arxiv.org/abs/2304.10457",
    "context": "Title: Angle based dynamic learning rate for gradient descent. (arXiv:2304.10457v1 [cs.LG])\nAbstract: In our work, we propose a novel yet simple approach to obtain an adaptive learning rate for gradient-based descent methods on classification tasks. Instead of the traditional approach of selecting adaptive learning rates via the decayed expectation of gradient-based terms, we use the angle between the current gradient and the new gradient: this new gradient is computed from the direction orthogonal to the current gradient, which further helps us in determining a better adaptive learning rate based on angle history, thereby, leading to relatively better accuracy compared to the existing state-of-the-art optimizers. On a wide variety of benchmark datasets with prominent image classification architectures such as ResNet, DenseNet, EfficientNet, and VGG, we find that our method leads to the highest accuracy in most of the datasets. Moreover, we prove that our method is convergent.",
    "path": "papers/23/04/2304.10457.json",
    "total_tokens": 862,
    "translated_title": "基于角度的梯度下降动态学习率",
    "translated_abstract": "我们提出了一种新颖而简单的方法，用于获取分类任务上基于梯度下降方法的自适应学习率。我们使用当前梯度与新梯度之间的角度来选择自适应学习率，而不是传统的通过基于梯度项期望的衰减来选择自适应学习率的方法。我们使用了在当前梯度垂直方向上计算出来的新梯度，从而帮助我们根据角度历史记录得到更好的自适应学习率，因此与现有最先进的优化器相比，我们的方法具有更高的准确性。在广泛的基准数据集上使用ResNet、DenseNet、EfficientNet和VGG等流行的图像分类架构，我们发现我们的方法在大多数数据集上都获得了最高准确率。此外，我们证明了我们的方法是收敛的。",
    "tldr": "该论文提出了一种基于角度计算的动态学习率方式，用于梯度下降方法中的自适应学习率选择，相较于传统方法，该方法在各个基准数据集上的可以获得更高的准确性，并被证明是可以收敛的。",
    "en_tdlr": "The paper proposes a novel approach for obtaining adaptive learning rate for gradient-based descent methods in classification tasks, by selecting the adaptive learning rate based on the angle between the current gradient and the new gradient. This method proves to achieve higher accuracy compared to traditional methods on various benchmark datasets and is proved to be convergent."
}