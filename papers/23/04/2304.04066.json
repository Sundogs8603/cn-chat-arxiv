{
    "title": "A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])",
    "abstract": "Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide",
    "link": "http://arxiv.org/abs/2304.04066",
    "context": "Title: A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])\nAbstract: Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide",
    "path": "papers/23/04/2304.04066.json",
    "total_tokens": 1023,
    "translated_title": "基于屏障-李亚普诺夫Actor-Critic强化学习方法的安全稳定控制",
    "translated_abstract": "强化学习在视频游戏和机器人等领域展现出惊人的性能。然而，使用强化学习控制现实世界系统时，确保安全和稳定性仍然是一个重大挑战。在本文中，我们首先为强化学习系统提供安全和稳定性的定义，然后将控制屏障函数（CBF）和控制李亚普诺夫函数（CLF）方法与Actor-Critic方法相结合，提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，有助于保持系统的安全和稳定性。在该框架中，基于来自重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。此外，还引入了一个备用控制器，以防RL控制器无法提供稳定控制。",
    "tldr": "本文提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，针对强化学习控制现实世界系统时的安全稳定控制问题提出了一种解决方案。其中，基于重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。",
    "en_tdlr": "This paper proposes a Barrier-Lyapunov Actor-Critic (BLAC) framework to solve the challenge of ensuring safety and stability when using reinforcement learning (RL) to control real-world systems. The framework constructs CBF constraints for safety and CLF constraints for stability based on the sampled data from the replay buffer, and uses the augmented Lagrangian method to update the parameters of the RL-based controller. A backup controller is also introduced in case the RL-based controller cannot provide stable control."
}