{
    "title": "Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])",
    "abstract": "Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person's privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals' data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thu",
    "link": "http://arxiv.org/abs/2304.11336",
    "context": "Title: Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])\nAbstract: Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person's privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals' data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thu",
    "path": "papers/23/04/2304.11336.json",
    "total_tokens": 1145,
    "translated_title": "通过利普希茨正则化变分自编码器生成差分隐私合成数据",
    "translated_abstract": "合成数据被誉为隐私保护数据分析的银弹。如果一条记录不是真实的，那么它怎么会侵犯个人的隐私呢？此外，基于深度学习的生成模型被成功地用于近似表示数据的高维复杂分布，并从学习到的分布中抽取逼真的样本。然而，往往忽视生成模型容易记忆个人训练数据集的许多细节，并且生成的合成数据往往太类似于底层敏感训练数据，从而违反了如医疗保健中所遇到的强隐私法规。差分隐私是确保保护敏感个人数据的国际公认的最先进框架，使得可以公开发布集合统计数据和甚至机器学习模型而不会影响隐私。然而，训练机制往往会在训练过程中添加过多的噪声，因此很难生成高质量的合成数据。在本文中，我们提出了一种通过利普希茨正则化变分自编码器生成差分隐私合成数据的新方法。变分自编码器的编码器被正则化以确保对于每个训练数据的编码表示尽可能地与模型参数分享最少的信息，而生成器则被训练以近似底层数据分布。我们的方法生成的高质量合成数据严格符合差分隐私保证。我们通过实验展示了我们的方法的有效性。",
    "tldr": "本文提出了通过利普希茨正则化变分自编码器生成差分隐私合成数据的新方法。该方法可以生成符合差分隐私保证的高质量合成数据。",
    "en_tdlr": "This paper proposes a new method for generating differentially private synthetic data through the use of a Lipschitz-regularised variational autoencoder. The approach generates high quality synthetic data that strictly complies with differential privacy guarantees."
}