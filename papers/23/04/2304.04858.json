{
    "title": "Simulated Annealing in Early Layers Leads to Better Generalization. (arXiv:2304.04858v1 [cs.LG])",
    "abstract": "Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learning in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers. Essentially, later layers go through the normal gradient descent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin. We further show that, compared to normal training, LLF features, although improving on the target task, degrade",
    "link": "http://arxiv.org/abs/2304.04858",
    "context": "Title: Simulated Annealing in Early Layers Leads to Better Generalization. (arXiv:2304.04858v1 [cs.LG])\nAbstract: Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learning in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers. Essentially, later layers go through the normal gradient descent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin. We further show that, compared to normal training, LLF features, although improving on the target task, degrade",
    "path": "papers/23/04/2304.04858.json",
    "total_tokens": 952,
    "translated_title": "在早期层中使用模拟退火可提高泛化性能",
    "translated_abstract": "最近，引入了一些迭代学习方法来改进泛化性能。这些方法通常依靠更长时间的训练来换取更好的泛化性能。LLF是这一类方法中最先进的方法。它通过定期重新初始化网络的最后几层来增强早期层次的学习。我们在这项工作中的主要创新是在网络的早期层中使用模拟退火(SEAL)代替后面层次的重新初始化。实质上，后面的层通过正常的梯度下降过程，而早期层通过短暂的梯度上升后跟着梯度下降。在流行的Tiny-ImageNet数据集基准测试以及一系列传递学习和少量样本学习任务上进行了广泛的实验，结果显示SEAL比LLF的表现要好得多。我们进一步表明，与正常训练相比，虽然LLF特性能提高目标任务的性能，但会降低原始任务的性能。另一方面，SEAL不仅可提高目标任务的性能，还可改善原始任务的性能。",
    "tldr": "使用模拟退火在早期层次的网络中可以提高泛化性能，并且在Tiny-ImageNet数据集基准测试和一系列传递学习和少量样本学习任务上表现得比LLF更好。"
}