{
    "title": "Explicitly Minimizing the Blur Error of Variational Autoencoders. (arXiv:2304.05939v1 [cs.CV])",
    "abstract": "Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution. We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs.",
    "link": "http://arxiv.org/abs/2304.05939",
    "context": "Title: Explicitly Minimizing the Blur Error of Variational Autoencoders. (arXiv:2304.05939v1 [cs.CV])\nAbstract: Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution. We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs.",
    "path": "papers/23/04/2304.05939.json",
    "total_tokens": 868,
    "translated_title": "显式最小化变分自编码器的模糊误差",
    "translated_abstract": "变分自编码器（VAEs）是强大的生成建模方法，但与它们所训练的图像相比，它们会产生模糊的生成样本和重构图像。已经投入了大量的研究努力来创建更灵活的模型以增加生成能力，但通常灵活性的代价是更高的复杂性和计算成本。几项工作集中在改变证据下界（ELBO）的重构项上，但往往是以损失将样本最大似然的数学联系为代价。在这里，我们提出一种针对VAE的重构项的新公式，特别惩罚生成模糊图像，同时仍然在建模分布下最大化ELBO。我们展示了所提出的损失在三个不同的数据集上的潜力，其中它优于VAE的几个最近提出的重构损失。",
    "tldr": "该论文提出了一种新的变分自编码器的重构项公式，它特别针对生成模糊图像进行惩罚，同时仍然最大化建模分布下的ELBO。 实验证明，该方法在三个数据集上优于VAE的几个最近提出的重构损失。",
    "en_tdlr": "The paper proposes a new reconstruction term for VAEs that penalizes the generation of blurry images while still maximizing ELBO under the modeled distribution. Experiments demonstrate that the proposed method outperforms several recently proposed reconstruction losses for VAEs on three different datasets."
}