{
    "title": "Generative Adversarial Neuroevolution for Control Behaviour Imitation. (arXiv:2304.12432v1 [cs.NE])",
    "abstract": "There is a recent surge in interest for imitation learning, with large human video-game and robotic manipulation datasets being used to train agents on very complex tasks. While deep neuroevolution has recently been shown to match the performance of gradient-based techniques on various reinforcement learning problems, the application of deep neuroevolution techniques to imitation learning remains relatively unexplored. In this work, we propose to explore whether deep neuroevolution can be used for behaviour imitation on popular simulation environments. We introduce a simple co-evolutionary adversarial generation framework, and evaluate its capabilities by evolving standard deep recurrent networks to imitate state-of-the-art pre-trained agents on 8 OpenAI Gym state-based control tasks. Across all tasks, we find the final elite actor agents capable of achieving scores as high as those obtained by the pre-trained agents, all the while closely following their score trajectories. Our result",
    "link": "http://arxiv.org/abs/2304.12432",
    "context": "Title: Generative Adversarial Neuroevolution for Control Behaviour Imitation. (arXiv:2304.12432v1 [cs.NE])\nAbstract: There is a recent surge in interest for imitation learning, with large human video-game and robotic manipulation datasets being used to train agents on very complex tasks. While deep neuroevolution has recently been shown to match the performance of gradient-based techniques on various reinforcement learning problems, the application of deep neuroevolution techniques to imitation learning remains relatively unexplored. In this work, we propose to explore whether deep neuroevolution can be used for behaviour imitation on popular simulation environments. We introduce a simple co-evolutionary adversarial generation framework, and evaluate its capabilities by evolving standard deep recurrent networks to imitate state-of-the-art pre-trained agents on 8 OpenAI Gym state-based control tasks. Across all tasks, we find the final elite actor agents capable of achieving scores as high as those obtained by the pre-trained agents, all the while closely following their score trajectories. Our result",
    "path": "papers/23/04/2304.12432.json",
    "total_tokens": 962,
    "translated_title": "生成对抗神经进化用于控制行为模仿",
    "translated_abstract": "最近对于模仿学习的兴趣急剧增加，大量的人类视频游戏和机器人操作数据集被用来训练代理在非常复杂的任务上。虽然深度神经进化最近已经显示可以匹配基于梯度的技术在各种强化学习问题上的性能，但将深度神经进化技术应用于模仿学习仍相对未被探索。在这项工作中，我们提出探索深度神经进化是否可以用于在流行的仿真环境中进行行为模仿 。我们引入了一个简单的协同进化的对抗生成框架，并通过演化标准深度递归网络来模仿8个OpenAI Gym状态控制任务上的最先进的预训练代理，并评估其能力。在所有任务中，我们发现最终的优秀执行器代理能够达到与预训练代理获得的得分一样高的得分，同时紧密地跟随它们的得分轨迹。我们的结果证明了所提出的方法的有效性。",
    "tldr": "本文研究了使用生成对抗神经进化技术进行行为模仿的方法，在8个OpenAI Gym状态控制任务上，最终的代理能够达到与预训练代理一样高的得分，展示了深度神经进化技术在模仿学习中的潜力。",
    "en_tdlr": "This paper explores the use of generative adversarial neuroevolution for behavior imitation on several OpenAI Gym state-based control tasks. The authors introduce a co-evolutionary adversarial generation framework and show that the final elite actor agents can achieve scores as high as those obtained by the pre-trained agents, demonstrating the potential of this technique in imitation learning."
}