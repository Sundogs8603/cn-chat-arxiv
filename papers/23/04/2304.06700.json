{
    "title": "Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])",
    "abstract": "Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same di",
    "link": "http://arxiv.org/abs/2304.06700",
    "context": "Title: Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])\nAbstract: Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same di",
    "path": "papers/23/04/2304.06700.json",
    "total_tokens": 947,
    "translated_title": "从单视图图像学习可控三维扩散模型",
    "translated_abstract": "最近，扩散模型已经成为2D领域生成建模的事实标准。然而，由于获取三维基准数据进行训练的困难，将扩散模型扩展到三维领域是具有挑战性的。另一方面，将隐式三维表示集成到GANs中的3D GANs在仅训练单视图图像数据集时展示了显着的3D感知生成。然而，3D GANs没有提供精确控制图像合成的简单方法。为了解决这些挑战，我们提出了Control3Diff，这是一种结合了扩散模型和3D GANs优点的三维扩散模型，用于单视图数据集的多功能，可控的三维感知图像合成。Control3Diff明确地建模了潜在的潜在分布（可以是外部输入条件下的潜在分布），从而允许在扩散过程中直接控制。此外，我们的方法通用，可适用于任何类型的控制输入，使我们能够使用相同的基础体系结构对其进行训练。",
    "tldr": "该论文介绍了一种名为Control3Diff的三维扩散模型，结合了扩散模型和3D GANs的优点，可以用于单视图数据集的多功能、可控的三维感知图像合成。",
    "en_tdlr": "This paper presents a 3D diffusion model named Control3Diff, which combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets."
}