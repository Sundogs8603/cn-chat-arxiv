{
    "title": "Contrastive Language, Action, and State Pre-training for Robot Learning. (arXiv:2304.10782v1 [cs.RO])",
    "abstract": "In this paper, we introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning. Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning. Our distributional encoders exhibit superior retrieval and captioning performance on unseen datasets, and the ability to generate meaningful exploratory behaviours from textual com",
    "link": "http://arxiv.org/abs/2304.10782",
    "context": "Title: Contrastive Language, Action, and State Pre-training for Robot Learning. (arXiv:2304.10782v1 [cs.RO])\nAbstract: In this paper, we introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning. Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning. Our distributional encoders exhibit superior retrieval and captioning performance on unseen datasets, and the ability to generate meaningful exploratory behaviours from textual com",
    "path": "papers/23/04/2304.10782.json",
    "total_tokens": 972,
    "translated_title": "机器人学习中的对比语言、动作和状态预训练",
    "translated_abstract": "本文介绍了一种方法，将语言、动作和状态信息统一到共享的嵌入空间中，以促进机器人学习中的一系列下游任务。我们的方法名为对比语言、动作和状态预训练（CLASP），扩展了CLIP公式，结合分布式学习，捕捉行为文本对齐中的固有复杂性和一对多关系。通过为文本和行为编码器提供分布输出，我们的模型有效地将不同的文本命令与单个行为相关联，并反之亦然。我们展示了我们的方法在以下下游任务中的实用性：零样本文本行为检索、为未见机器人行为加标题以及学习一个行为先验知识以进行语言依存的强化学习。我们的分布式编码器在未见数据集上表现出卓越的检索和标题性能，以及从文本命令生成有意义的探索性行为的能力。",
    "tldr": "本文介绍了一种名为CLASP的方法，将语言、动作和状态信息统一到共享的嵌入空间中，用于机器人学习中的一系列下游任务。通过分布式编码器实现了不同的文本命令与单个行为相关联，并反之亦然。模型在零样本文本行为检索、为未见机器人行为加标题以及学习一个行为先验知识以进行语言依存的强化学习方面表现出卓越的性能。"
}