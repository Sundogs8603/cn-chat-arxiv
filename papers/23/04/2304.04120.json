{
    "title": "Surrogate Lagrangian Relaxation: A Path To Retrain-free Deep Neural Network Pruning. (arXiv:2304.04120v1 [cs.NE])",
    "abstract": "Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline significantly increases the overall training time. In this paper, we develop a systematic weight-pruning optimization approach based on Surrogate Lagrangian relaxation, which is tailored to overcome difficulties caused by the discrete nature of the weight-pruning problem. We prove that our method ensures fast convergence of the model compression problem, and the convergence of the SLR is accelerated by using quadratic penalties. Model parameters obtained by SLR during the training phase are much closer to their optimal values as compared to those obtained by other state-of-the-art methods. We evaluate our method on image classification tasks using CIFAR-10 and ImageNet with state-of-the-art MLP-Mixer, Swin Transformer, and VGG-16, ResNet-18, ResNet-50 and ResNet-110, MobileNetV2. We also evaluate object detection and segmentation tasks",
    "link": "http://arxiv.org/abs/2304.04120",
    "context": "Title: Surrogate Lagrangian Relaxation: A Path To Retrain-free Deep Neural Network Pruning. (arXiv:2304.04120v1 [cs.NE])\nAbstract: Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline significantly increases the overall training time. In this paper, we develop a systematic weight-pruning optimization approach based on Surrogate Lagrangian relaxation, which is tailored to overcome difficulties caused by the discrete nature of the weight-pruning problem. We prove that our method ensures fast convergence of the model compression problem, and the convergence of the SLR is accelerated by using quadratic penalties. Model parameters obtained by SLR during the training phase are much closer to their optimal values as compared to those obtained by other state-of-the-art methods. We evaluate our method on image classification tasks using CIFAR-10 and ImageNet with state-of-the-art MLP-Mixer, Swin Transformer, and VGG-16, ResNet-18, ResNet-50 and ResNet-110, MobileNetV2. We also evaluate object detection and segmentation tasks",
    "path": "papers/23/04/2304.04120.json",
    "total_tokens": 949,
    "translated_title": "替代拉格朗日松弛：一种不需要重新训练的深度神经网络剪枝方法",
    "translated_abstract": "剪枝是一种常用的技术，用于减少深度神经网络的计算成本和模型大小。然而，典型的三阶段管道显著增加了总体训练时间。本文提出了一种基于替代拉格朗日松弛的系统权重剪枝优化方法，特别针对权重剪枝问题的离散性而设计。我们证明了我们的方法确保了模型压缩问题的快速收敛，并且通过使用二次惩罚来加速SLR的收敛。与其他最先进的方法相比，SLR在训练阶段得到的模型参数距离其最优值更近。我们使用CIFAR-10和ImageNet数据集评估了我们的方法，并与MLP-Mixer、Swin Transformer、VGG-16、ResNet-18、ResNet-50、ResNet-110和MobileNetV2等最先进的模型进行了比较。我们还评估了目标检测和分割任务。",
    "tldr": "本文提出了一种基于替代拉格朗日松弛的系统权重剪枝优化方法，可加快模型剪枝问题的收敛速度，并在深度神经网络上得到了显著效果，无需重新训练。",
    "en_tdlr": "This paper proposes a systematic weight-pruning optimization approach using Surrogate Lagrangian relaxation, tailored to overcome the difficulties caused by the discrete nature of weight-pruning problem. The method ensures fast convergence of model compression problem and obtains significantly closer model parameters to their optimal values compared to state-of-the-art methods, without the need for retraining. It is evaluated on various tasks with deep neural networks and achieves significant results."
}