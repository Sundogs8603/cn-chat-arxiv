{
    "title": "Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])",
    "abstract": "Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding: video-t",
    "link": "http://arxiv.org/abs/2304.06708",
    "context": "Title: Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])\nAbstract: Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding: video-t",
    "path": "papers/23/04/2304.06708.json",
    "total_tokens": 968,
    "translated_title": "动词行动：改进视频语言模型中的动词理解",
    "translated_abstract": "理解动词对于模型化人与物体在空间和时间上如何相互作用以及与环境相互作用至关重要。最近，基于CLIP的最先进的视频语言模型在动词理解方面受限，且严重依赖名词，限制了它们在需要动作和时间理解的实际视频应用中的性能。在本文中，我们通过提出一种新的动词聚焦对比（VFC）框架，改善基于CLIP的视频语言模型的动词理解能力。该框架由两个主要组成部分组成：（1）利用预训练的大型语言模型（LLM）创建跨模态对比学习的硬负例，以及通过校准策略平衡正负对中概念的出现来平衡正负对；（2）执行细粒度的动词短语对齐损失。我们的方法在三个聚焦于动词理解的下游任务的零样本表现方面实现了最先进的结果：视频t...",
    "tldr": "本文提出了一个新的动词聚焦对比框架，通过利用预训练的大型语言模型和执行细粒度的动词短语对齐损失来改善基于CLIP的视频语言模型的动词理解能力，实现了在三个聚焦于动词理解的下游任务的零样本性能最先进的结果。",
    "en_tdlr": "The paper proposes a new Verb-Focused Contrastive (VFC) framework to improve verb understanding of CLIP-based video-language models by leveraging pretrained large language models and having a fine-grained verb phrase alignment loss. The proposed method achieves state-of-the-art zero-shot performance on three downstream tasks that focus on verb understanding."
}