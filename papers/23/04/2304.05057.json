{
    "title": "SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction",
    "abstract": "Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration have shown remarkable success in providing high-quality reconstruction. However, as the number of cascades increases, the improvements in reconstruction tend to become marginal, indicating possible excess model capacity. Knowledge distillation (KD) is an emerging technique to compress these models, in which a trained deep teacher network is used to distill knowledge to a smaller student network such that the student learns to mimic the behavior of the teacher. Most KD methods focus on effectively training the student with a pre-trained teacher unaware of the student model. We propose SFT-KD-Recon, a student-friendly teacher training approach along with the student as a prior step to KD to make the teacher aware of the structure and capacity of the student and enable aligning the representations of the teacher with the student. In SFT, the teacher is jointly trained with the unfolded branch configurations of t",
    "link": "https://arxiv.org/abs/2304.05057",
    "context": "Title: SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction\nAbstract: Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration have shown remarkable success in providing high-quality reconstruction. However, as the number of cascades increases, the improvements in reconstruction tend to become marginal, indicating possible excess model capacity. Knowledge distillation (KD) is an emerging technique to compress these models, in which a trained deep teacher network is used to distill knowledge to a smaller student network such that the student learns to mimic the behavior of the teacher. Most KD methods focus on effectively training the student with a pre-trained teacher unaware of the student model. We propose SFT-KD-Recon, a student-friendly teacher training approach along with the student as a prior step to KD to make the teacher aware of the structure and capacity of the student and enable aligning the representations of the teacher with the student. In SFT, the teacher is jointly trained with the unfolded branch configurations of t",
    "path": "papers/23/04/2304.05057.json",
    "total_tokens": 891,
    "translated_title": "SFT-KD-Recon:学习MRI重建中学生友好的知识蒸馏教师",
    "translated_abstract": "磁共振成像(MRI)加速的深级联架构在提供高质量重建方面取得了显著成功。然而，随着级联数量的增加，重建的改进往往变得边际，这可能表明存在过多的模型容量。知识蒸馏(KD)是一种新兴的压缩这些模型的技术，其中使用训练有素的深度教师网络将知识蒸馏给较小的学生网络，使学生学习模仿教师的行为。大多数KD方法侧重于有效地训练不知道学生模型的预训练教师。我们提出了SFT-KD-Recon，一种学生友好的教师训练方法，以及学生作为KD的先决步骤，使教师了解学生的结构和能力，并使教师的表示与学生的表示相一致。在SFT中，教师与展开的支路配置同时训练。",
    "tldr": "SFT-KD-Recon是一种学生友好的教师训练策略，使教师了解学生的结构和能力，并将教师的表示与学生的表示相一致，以实现MRI重建中的知识蒸馏。"
}