{
    "title": "On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])",
    "abstract": "In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better ",
    "link": "http://arxiv.org/abs/2304.03216",
    "context": "Title: On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])\nAbstract: In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better ",
    "path": "papers/23/04/2304.03216.json",
    "total_tokens": 872,
    "translated_title": "关于多语言神经机器翻译的Pareto前沿研究",
    "translated_abstract": "本研究探讨了在多语言神经机器翻译中，给定方向的泛化性能如何随其采样比例的变化而变化。通过训练200多个具有不同模型大小、方向和总任务数量的多语言模型，我们发现在训练语料库存在数据不平衡时，标量化导致了一个多任务权衡前沿，该前沿偏离了传统的Pareto前沿。基于我们的观察，我们提出了双重幂律来预测MNMT中独特的性能权衡前沿，该方法在各种语言、数据充足性和任务数量方面都很鲁棒。最后，我们将MNMT中的样本比例选择问题建模为基于双重幂律的优化问题，取得了更好的结果。",
    "tldr": "本研究针对多语言神经机器翻译的数据不平衡问题，提出双重幂律方法用于预测独特的性能权衡前沿，并建立基于该方法的样本比例选择优化问题，取得更好的结果。",
    "en_tdlr": "This paper studies the generalization performance of a given direction in Multilingual Neural Machine Translation (MNMT) and proposes the Double Power Law method to predict the unique performance trade-off front, which is robust across various languages, data adequacy and number of tasks. The method is applied to sample ratio selection optimization problem and achieves better results."
}