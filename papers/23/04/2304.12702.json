{
    "title": "A Static Pruning Study on Sparse Neural Retrievers. (arXiv:2304.12702v1 [cs.IR])",
    "abstract": "Sparse neural retrievers, such as DeepImpact, uniCOIL and SPLADE, have been introduced recently as an efficient and effective way to perform retrieval with inverted indexes. They aim to learn term importance and, in some cases, document expansions, to provide a more effective document ranking compared to traditional bag-of-words retrieval models such as BM25. However, these sparse neural retrievers have been shown to increase the computational costs and latency of query processing compared to their classical counterparts. To mitigate this, we apply a well-known family of techniques for boosting the efficiency of query processing over inverted indexes: static pruning. We experiment with three static pruning strategies, namely document-centric, term-centric and agnostic pruning, and we assess, over diverse datasets, that these techniques still work with sparse neural retrievers. In particular, static pruning achieves $2\\times$ speedup with negligible effectiveness loss ($\\leq 2\\%$ drop) ",
    "link": "http://arxiv.org/abs/2304.12702",
    "context": "Title: A Static Pruning Study on Sparse Neural Retrievers. (arXiv:2304.12702v1 [cs.IR])\nAbstract: Sparse neural retrievers, such as DeepImpact, uniCOIL and SPLADE, have been introduced recently as an efficient and effective way to perform retrieval with inverted indexes. They aim to learn term importance and, in some cases, document expansions, to provide a more effective document ranking compared to traditional bag-of-words retrieval models such as BM25. However, these sparse neural retrievers have been shown to increase the computational costs and latency of query processing compared to their classical counterparts. To mitigate this, we apply a well-known family of techniques for boosting the efficiency of query processing over inverted indexes: static pruning. We experiment with three static pruning strategies, namely document-centric, term-centric and agnostic pruning, and we assess, over diverse datasets, that these techniques still work with sparse neural retrievers. In particular, static pruning achieves $2\\times$ speedup with negligible effectiveness loss ($\\leq 2\\%$ drop) ",
    "path": "papers/23/04/2304.12702.json",
    "total_tokens": 970,
    "translated_title": "稀疏神经信息检索器的静态修剪研究",
    "translated_abstract": "近期提出了一种称为DeepImpact、uniCOIL和SPLADE的稀疏神经信息检索器，它们是一种有效且高效的通过倒排索引进行信息检索的方法，旨在通过学习术语重要性和文档扩展来提供比传统的基于词袋的信息检索模型（例如BM25）更为有效的文档排名。然而，与传统的信息检索器相比，这些稀疏神经信息检索器已被证明会增加计算成本和查询处理的延迟。为了缓解这一问题，我们应用了一种旨在提高倒排索引查询处理效率的著名技术家族：静态修剪。我们尝试了三种静态修剪策略，即面向文档、面向术语和不可知修剪，并在不同数据集上进行了评估，结果表明这些技术仍然适用于稀疏神经信息检索器。特别地，静态修剪实现了2倍的速度提升，且效果损失极小（≤ 2%）。",
    "tldr": "本论文研究了稀疏神经信息检索器的静态修剪方法，结果表明在不同数据集上，采用面向文档、面向术语和不可知三种策略的静态修剪仍然适用于稀疏神经信息检索器，能够实现2倍速度提升，且效果损失极小（≤ 2%）。",
    "en_tdlr": "This paper studies the static pruning methods for sparse neural retrievers and shows that they are still applicable to achieve significant speedup with negligible effectiveness loss when using document-centric, term-centric and agnostic pruning strategies on diverse datasets."
}