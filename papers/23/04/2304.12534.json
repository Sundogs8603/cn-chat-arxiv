{
    "title": "Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])",
    "abstract": "In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th",
    "link": "http://arxiv.org/abs/2304.12534",
    "context": "Title: Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])\nAbstract: In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th",
    "path": "papers/23/04/2304.12534.json",
    "total_tokens": 874,
    "translated_title": "通过随机行走随机交替方向乘法算法推动个性化联邦学习",
    "translated_abstract": "在本研究中，我们探讨了在现实世界中实现联邦学习（FL）时存在的障碍，其中不能维护中央服务器与所有客户端之间的一致连接，并且数据分布是异构的。为了解决这些挑战，我们专注于动态联邦学习，其中服务器在相邻客户端组之间移动以学习本地模型。具体来说，我们提出了一种新算法，即随机行走随机交替方向乘法算法（RWSADMM），只要有足够数量的连接客户端用于模型训练，就能适应动态和即席网络条件。在RWSADMM中，服务器随机向一组客户端行走。它基于硬不等式约束形成相邻客户端之间的局部近似，而不是采用一致更新来解决数据异构性。我们提出的方法是收敛的，可以降低通信成本，通过减少训练时间提高可扩展性。",
    "tldr": "本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。",
    "en_tdlr": "This research proposes a new algorithm, RWSADMM, to address the challenges of data heterogeneity and high communication costs in dynamic federated learning, enhancing scalability."
}