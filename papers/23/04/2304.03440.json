{
    "title": "Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts. (arXiv:2304.03440v1 [cs.LG])",
    "abstract": "Distribution shifts are problems where the distribution of data changes between training and testing, which can significantly degrade the performance of a model deployed in the real world. Recent studies suggest that one reason for the degradation is a type of overfitting, and that proper regularization can mitigate the degradation, especially when using highly representative models such as neural networks. In this paper, we propose a new regularization using the supervised contrastive learning to prevent such overfitting and to train models that do not degrade their performance under the distribution shifts. We extend the cosine similarity in contrastive loss to a more general similarity measure and propose to use different parameters in the measure when comparing a sample to a positive or negative example, which is analytically shown to act as a kind of margin in contrastive loss. Experiments on benchmark datasets that emulate distribution shifts, including subpopulation shift and do",
    "link": "http://arxiv.org/abs/2304.03440",
    "context": "Title: Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts. (arXiv:2304.03440v1 [cs.LG])\nAbstract: Distribution shifts are problems where the distribution of data changes between training and testing, which can significantly degrade the performance of a model deployed in the real world. Recent studies suggest that one reason for the degradation is a type of overfitting, and that proper regularization can mitigate the degradation, especially when using highly representative models such as neural networks. In this paper, we propose a new regularization using the supervised contrastive learning to prevent such overfitting and to train models that do not degrade their performance under the distribution shifts. We extend the cosine similarity in contrastive loss to a more general similarity measure and propose to use different parameters in the measure when comparing a sample to a positive or negative example, which is analytically shown to act as a kind of margin in contrastive loss. Experiments on benchmark datasets that emulate distribution shifts, including subpopulation shift and do",
    "path": "papers/23/04/2304.03440.json",
    "total_tokens": 840,
    "translated_title": "带有异构相似性的监督对比学习用于分布偏移问题",
    "translated_abstract": "数据的分布在训练和测试时发生变化会导致分布偏移问题，进而严重影响模型在实际应用中的性能表现。近期研究表明，过拟合是其原因之一，合适的正则化可以缓解这种影响，尤其适用于使用神经网络等高度具有代表性的模型。本文提出了一种新的监督对比学习方法，通过该方法可以防止过拟合，训练模型避免在分布偏移下性能退化。作者将对比损失中的余弦相似性扩展为更通用的相似性度量，并建议在比较样本与正样本或负样本时使用不同的参数，在理论上这一建议被证明可以作为对比损失中的一种边缘效应。实验在模拟分布偏移的基准数据集上进行，包括子种群偏移和...（原文未完成）",
    "tldr": "本文提出了一种带有异构相似性的新的监督对比学习方法，用于解决分布偏移问题，防止过拟合影响模型性能。",
    "en_tdlr": "This paper proposes a new supervised contrastive learning method with heterogeneous similarity to address distribution shift problems and prevent overfitting from degrading model performance."
}