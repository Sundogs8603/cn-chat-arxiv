{
    "title": "DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables. (arXiv:2304.09049v1 [cs.LG])",
    "abstract": "A lot of recent progress has been made in ultra low-bit quantization, promising significant improvements in latency, memory footprint and energy consumption on edge devices. Quantization methods such as Learned Step Size Quantization can achieve model accuracy that is comparable to full-precision floating-point baselines even with sub-byte quantization. However, it is extremely challenging to deploy these ultra low-bit quantized models on mainstream CPU devices because commodity SIMD (Single Instruction, Multiple Data) hardware typically supports no less than 8-bit precision. To overcome this limitation, we propose DeepGEMM, a lookup table based approach for the execution of ultra low-precision convolutional neural networks on SIMD hardware. The proposed method precomputes all possible products of weights and activations, stores them in a lookup table, and efficiently accesses them at inference time to avoid costly multiply-accumulate operations. Our 2-bit implementation outperforms co",
    "link": "http://arxiv.org/abs/2304.09049",
    "context": "Title: DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables. (arXiv:2304.09049v1 [cs.LG])\nAbstract: A lot of recent progress has been made in ultra low-bit quantization, promising significant improvements in latency, memory footprint and energy consumption on edge devices. Quantization methods such as Learned Step Size Quantization can achieve model accuracy that is comparable to full-precision floating-point baselines even with sub-byte quantization. However, it is extremely challenging to deploy these ultra low-bit quantized models on mainstream CPU devices because commodity SIMD (Single Instruction, Multiple Data) hardware typically supports no less than 8-bit precision. To overcome this limitation, we propose DeepGEMM, a lookup table based approach for the execution of ultra low-precision convolutional neural networks on SIMD hardware. The proposed method precomputes all possible products of weights and activations, stores them in a lookup table, and efficiently accesses them at inference time to avoid costly multiply-accumulate operations. Our 2-bit implementation outperforms co",
    "path": "papers/23/04/2304.09049.json",
    "total_tokens": 856,
    "translated_title": "DeepGEMM：使用查找表在CPU体系结构上加速超低精度推断",
    "translated_abstract": "近年来，超低比特量化取得了很多进展，承诺在边缘设备上显着提高延迟、内存占用和能量消耗。学习的步长量化等量化方法即使使用子字节量化也可以实现模型精度与全精度浮点基线相媲美。但是，要在主流CPU设备上部署这些超低比特量化模型非常具有挑战性，因为通用SIMD（单指令流多数据流）硬件通常支持不低于8位精度。为了克服这个限制，我们提出了基于查找表的DeepGEMM方法，用于在SIMD硬件上执行超低精度卷积神经网络。所提出的方法预先计算权重和激活的所有可能的乘积，将它们存储在查找表中，并在推断时高效地访问它们，以避免昂贵的乘加运算。我们的2位实现优于联",
    "tldr": "DeepGEMM提出一种使用查找表在CPU体系结构上加速超低精度推断的方法，用于执行超低精度卷积神经网络。",
    "en_tdlr": "DeepGEMM proposes a lookup table based method for executing ultra low-precision convolutional neural networks on CPU architectures, to accelerate the inference process."
}