{
    "title": "STIR: Siamese Transformer for Image Retrieval Postprocessing. (arXiv:2304.13393v1 [cs.IR])",
    "abstract": "Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the ",
    "link": "http://arxiv.org/abs/2304.13393",
    "context": "Title: STIR: Siamese Transformer for Image Retrieval Postprocessing. (arXiv:2304.13393v1 [cs.IR])\nAbstract: Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the ",
    "path": "papers/23/04/2304.13393.json",
    "total_tokens": 970,
    "translated_title": "STIR：用于图像检索后处理的Siamese Transformer（arXiv：2304.13393v1 [cs.IR]）",
    "translated_abstract": "当前，图像检索的度量学习方法通常基于学习具有信息的潜在表示空间，其中简单的方法如余弦距离将表现良好。最近的最先进方法（如HypViT）转向更复杂的嵌入空间，可能会产生更好的结果，但更难以扩展到生产环境中。在这项工作中，我们首先构建了一个基于三元组损失的简单模型，具有硬负例挖掘，性能达到了最先进水平，但没有这些缺点。其次，我们引入了一种新颖的图像检索后处理方法，称为用于图像检索的Siamese Transformer（STIR），可在单个前向传递中重新排列多个顶部输出。与先前提出的重排变压器不同，STIR不依赖于全局/局部特征提取，而是借助注意机制直接在像素级别比较查询图像和检索到的候选图像。由此得出的方法定义了一个新的最先进水平。",
    "tldr": "这项工作提出了两部分内容。首先，他们构建了一个基于三元组损失的简单模型，性能达到了最先进水平，但没有复杂模型的缩放问题。其次，他们提出了一种新颖的后处理方法STIR，可在单个前向传递中重新排列多个顶部输出，而不依赖于全局/局部特征提取。",
    "en_tdlr": "This paper proposes two contributions, first, they construct a simpler model based on triplet loss that performs at the state of the art level without the scaling issue of complex models. Second, they introduce a novel postprocessing approach, STIR, which can rerank top outputs in a single forward pass, without relying on global/local feature extraction."
}