{
    "title": "Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])",
    "abstract": "Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.",
    "link": "http://arxiv.org/abs/2304.09869",
    "context": "Title: Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])\nAbstract: Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.",
    "path": "papers/23/04/2304.09869.json",
    "total_tokens": 862,
    "translated_title": "进化约束强化学习策略",
    "translated_abstract": "进化算法已被用于演化出一组执行者，以产生多样化的体验来训练强化学习智能体，从而解决时间信用分配问题并提高探索效率。然而，当将这种方法应用于解决约束问题时，很难平衡奖励和约束违规之间的权衡。本文提出了一种新颖的进化约束强化学习（ECRL）算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中进行的广泛实验表明，我们的ECRL相比最先进的算法取得了优异的性能。消融分析表明引入随机排名和约束缓冲区的优势。",
    "tldr": "本文提出了一种新颖的进化约束强化学习算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中取得优异性能。",
    "en_tdlr": "This paper proposes a novel evolutionary constrained reinforcement learning algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and restricts the policy's behavior by maintaining a set of Lagrange relaxation coefficients and a constraint buffer, achieving outstanding performance on robotic control benchmarks."
}