{
    "title": "LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])",
    "abstract": "This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.",
    "link": "http://arxiv.org/abs/2304.11406",
    "context": "Title: LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])\nAbstract: This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.",
    "path": "papers/23/04/2304.11406.json",
    "total_tokens": 810,
    "translated_title": "LaMP：当大型语言模型遇见个性化",
    "translated_abstract": "本文强调在当前自然语言理解和生成领域的个性化的重要性，并介绍了LaMP基准——用于训练和评估生成个性化输出的语言模型的新典范。LaMP提供了一个全面的评估框架，具有多样化的语言任务和每个用户的多个条目，包括三个分类任务和四个文本生成任务的七个个性化任务。我们还提出了一种检索增强方法，可从用户配置文件中检索个性化项目，构建大型语言模型的个性化提示。我们的基线零-shot和微调模型的结果表明，利用个人资料扩展的LM优于不考虑个人资料信息的对应模型。",
    "tldr": "本论文强调了当前自然语言处理领域中个性化的重要性，并提出了LaMP（一种用于训练和评估大型语言模型的新的个性化基准），并针对大型语言模型的生成任务，设计了七项个性化任务以及一种检索增强方法，结果表明在利用用户配置文件扩展大型语言模型的基础上，其生成结果明显优于传统方法。",
    "en_tdlr": "This paper emphasizes the importance of personalization in natural language understanding and generation and presents LaMP, a novel benchmark for training and evaluating language models for personalized output. The paper introduces seven personalized tasks and a retrieval augmentation approach for constructing personalized prompts. Results show that models utilizing profile augmentation outperform those that do not consider personal information."
}