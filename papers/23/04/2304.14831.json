{
    "title": "Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v1 [cs.LG])",
    "abstract": "Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named \\emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods wher",
    "link": "http://arxiv.org/abs/2304.14831",
    "context": "Title: Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v1 [cs.LG])\nAbstract: Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named \\emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods wher",
    "path": "papers/23/04/2304.14831.json",
    "total_tokens": 899,
    "translated_title": "从限制性反馈中提高模型性能",
    "translated_abstract": "许多机器学习应用程序面临这样一种情况：模型提供者需要进一步改进先前训练的模型以满足本地用户的特定需求。如果可以将目标数据传递给模型，那么这个问题就转化为标准的模型调整范例。然而，在许多实际应用中，目标数据并不共享给模型提供者，而只是一些关于模型的评估可供访问。本文提出了一个名为EXPECTED（Earning eXtra PerformancE from restriCTive feEDdbacks）的挑战，正式描述了这种形式的模型调整问题，允许模型提供者通过来自本地用户（或一组用户）的反馈多次访问候选模型的操作性能。模型提供者的目标是通过利用反馈最终向本地用户提供令人满意的模型。与现有的模型调整方法不同，EXPECTED在不依赖于目标数据的情况下实现了模型优化。",
    "tldr": "本文提出了一个名为EXPECTED的挑战，解决模型调整问题，模型提供者可以通过来自本地用户的反馈多次访问候选模型的操作性能，从而优化模型，同时不需要依赖目标数据。",
    "en_tdlr": "This paper proposes the EXPECTED challenge to solve the problem of model tuning when target data is not shared with the model provider, where the provider can access the operational performance of the candidate model multiple times via feedback from local user(s), achieving model optimization without relying on target data."
}