{
    "title": "LayerNAS: Neural Architecture Search in Polynomial Complexity. (arXiv:2304.11517v1 [cs.LG])",
    "abstract": "Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial.  For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \\cdot |\\mathbb{S}| \\cdot L) $, where $H$ is a constant set in LayerNAS.  Our experiments show that LayerNAS is able to consistently discover superior models",
    "link": "http://arxiv.org/abs/2304.11517",
    "context": "Title: LayerNAS: Neural Architecture Search in Polynomial Complexity. (arXiv:2304.11517v1 [cs.LG])\nAbstract: Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial.  For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \\cdot |\\mathbb{S}| \\cdot L) $, where $H$ is a constant set in LayerNAS.  Our experiments show that LayerNAS is able to consistently discover superior models",
    "path": "papers/23/04/2304.11517.json",
    "total_tokens": 869,
    "translated_title": "LayerNAS：多目标神经架构搜索的多项式复杂度方法",
    "translated_abstract": "神经架构搜索（NAS）已成为发现有效模型架构的流行方法，尤其是对于目标硬件而言。因此，在约束条件下找到最佳架构的NAS方法至关重要。在本文中，我们提出了LayerNAS，将多目标NAS的挑战转化为组合优化问题，有效地将搜索复杂度限制为多项式。对于具有$L$层的模型架构，我们为每一层执行逐层搜索，从一个搜索选项集$\\mathbb{S}$中进行选择。LayerNAS根据一个目标，例如模型大小或延迟，对模型候选进行分组，并基于另一个目标搜索最佳模型，从而分割了搜索的成本和奖励元素。这种方法将搜索复杂度限制在$O(H \\cdot |\\mathbb{S}| \\cdot L)$，其中$H$是在LayerNAS中设定的常数。我们的实验表明，LayerNAS能够持续发现优越的模型。",
    "tldr": "LayerNAS提出了一种多项式复杂度的神经架构搜索方法，将搜索分为多个目标，并将搜索成本和奖励元素分开，能够快速有效地发现优越模型。",
    "en_tdlr": "LayerNAS proposes a polynomial complexity method for multi-objective neural architecture search, which separates the cost and reward elements of the search and splits the search into multiple objectives, enabling the rapid and effective discovery of superior models."
}