{
    "title": "Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11. (arXiv:2304.12982v1 [cs.CL])",
    "abstract": "With increasing demand for and adoption of virtual assistants, recent work has investigated ways to accelerate bot schema design through the automatic induction of intents or the induction of slots and dialogue states. However, a lack of dedicated benchmarks and standardized evaluation has made progress difficult to track and comparisons between systems difficult to make. This challenge track, held as part of the Eleventh Dialog Systems Technology Challenge, introduces a benchmark that aims to evaluate methods for the automatic induction of customer intents in a realistic setting of customer service interactions between human agents and customers. We propose two subtasks for progressively tackling the automatic induction of intents and corresponding evaluation methodologies. We then present three datasets suitable for evaluating the tasks and propose simple baselines. Finally, we summarize the submissions and results of the challenge track, for which we received submissions from 34 tea",
    "link": "http://arxiv.org/abs/2304.12982",
    "context": "Title: Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11. (arXiv:2304.12982v1 [cs.CL])\nAbstract: With increasing demand for and adoption of virtual assistants, recent work has investigated ways to accelerate bot schema design through the automatic induction of intents or the induction of slots and dialogue states. However, a lack of dedicated benchmarks and standardized evaluation has made progress difficult to track and comparisons between systems difficult to make. This challenge track, held as part of the Eleventh Dialog Systems Technology Challenge, introduces a benchmark that aims to evaluate methods for the automatic induction of customer intents in a realistic setting of customer service interactions between human agents and customers. We propose two subtasks for progressively tackling the automatic induction of intents and corresponding evaluation methodologies. We then present three datasets suitable for evaluating the tasks and propose simple baselines. Finally, we summarize the submissions and results of the challenge track, for which we received submissions from 34 tea",
    "path": "papers/23/04/2304.12982.json",
    "total_tokens": 895,
    "translated_title": "面向DSTC11任务导向式对话跟踪的会话意图诱导",
    "translated_abstract": "随着虚拟助手的需求和普及增加，近年来的一些工作研究了通过自动诱导意图或诱导槽和对话状态来加速机器人架构设计的方法。然而，缺乏专用基准和标准化评估使得进展难以跟踪，系统之间的比较也难以进行。本次挑战赛提出了一个基准来评估在人类代理和客户之间的客户服务交互的真实环境中自动诱导客户意图方法的研究。我们提出了两个子任务来逐步解决自动诱导意图和相应评估方法。我们提供了三个适合进行任务评估的数据集，并提出了简单的基线。最后，我们总结了挑战赛的提交和结果，共收到了来自34个团队的提交。",
    "tldr": "本论文介绍了DSTC11任务导向式对话跟踪的会话意图诱导，提出了两个子任务和三个数据集，并给出了简单的基线，用以评估方法的研究。其中旨在在客户服务交互的真实环境中自动诱导客户意图并对其进行评估。"
}