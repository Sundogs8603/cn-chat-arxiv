{
    "title": "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])",
    "abstract": "This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \\begin{align*}  \\frac{SAH^3}{\\varepsilon^2} \\text{ sample episodes (up to log factor)} \\end{align*} without guidance of the reward information, our algorithm is able to find $\\varepsilon$-optimal policies for all these reward functions, provided that $\\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\\frac{S^2AH^3}{\\varepsilon^2}$ episodes (up to log f",
    "link": "http://arxiv.org/abs/2304.07278",
    "context": "Title: Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])\nAbstract: This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \\begin{align*}  \\frac{SAH^3}{\\varepsilon^2} \\text{ sample episodes (up to log factor)} \\end{align*} without guidance of the reward information, our algorithm is able to find $\\varepsilon$-optimal policies for all these reward functions, provided that $\\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\\frac{S^2AH^3}{\\varepsilon^2}$ episodes (up to log f",
    "path": "papers/23/04/2304.07278.json",
    "total_tokens": 831,
    "translated_title": "强化学习中的极小极大最优无关奖励探索",
    "translated_abstract": "本文研究了强化学习中的无关奖励探索，设计了一种算法来改进现有技术。研究了具有S个状态，A个动作和有限时间水平H的非平稳马尔科夫决策过程，并收集了一定数量的无引导奖励信息的样本集，在保证收集的数量满足多项式级别时，算法能够发现所有这些奖励函数的最小值，实现了可证明的极小极大最优探索方案。",
    "tldr": "本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。",
    "en_tdlr": "This paper proposes an algorithm for reward-agnostic exploration in reinforcement learning, which can find the minimum value of all given reward functions while the learner is unaware of the reward information. The algorithm achieves provable minimax optimality and requires a polynomial number of sample episodes."
}