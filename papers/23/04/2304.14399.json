{
    "title": "We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])",
    "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value ",
    "link": "http://arxiv.org/abs/2304.14399",
    "context": "Title: We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])\nAbstract: Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value ",
    "path": "papers/23/04/2304.14399.json",
    "total_tokens": 967,
    "translated_title": "我们担心语言模型无法建模歧义",
    "translated_abstract": "歧义是自然语言的内在特征。管理歧义是人类语言理解的关键部分，允许我们作为沟通者预期到误解，并作为听众修正我们的解释。随着语言模型（LM）越来越多地被用作对话界面和写作助手，处理含糊不清的语言对它们的成功至关重要。我们通过与另一句子的蕴含关系来描述句子中的歧义，并收集了一组包含1,645个不同类型歧义的语言学注释示例的基准数据AmbiEnt。我们设计了一系列基于AmbiEnt的测试，呈现了对预训练语言模型识别歧义和分离可能含义的第一次评估。我们发现这个任务仍然非常具有挑战性，包括最近发布的GPT-4。人类评估中，GPT-4产生的消歧被认为仅有32%是正确的，而我们的数据集中消歧的正确率为90%。最后，为了阐明我们研究的价值，",
    "tldr": "本研究旨在探讨语言模型处理歧义的问题。我们通过AmbiEnt基准数据集设计了一系列测试来评估GPT-4和其他预训练语言模型对歧义的识别和分离能力。结果表明，这是一个极具挑战性的任务，目前的语言模型还无法准确处理歧义。",
    "en_tdlr": "This paper investigates the problem of ambiguity in language models, and presents a benchmark dataset called AmbiEnt to evaluate the ability of pretrained LMs, including GPT-4, to recognize and disentangle ambiguity. The results show that the task remains extremely challenging and current LMs struggle to accurately handle ambiguity."
}