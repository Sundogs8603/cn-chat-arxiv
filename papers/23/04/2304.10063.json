{
    "title": "Understanding Accelerated Gradient Methods: Lyapunov Analyses and Hamiltonian Assisted Interpretations. (arXiv:2304.10063v1 [math.OC])",
    "abstract": "We formulate two classes of first-order algorithms more general than previously studied for minimizing smooth and strongly convex or, respectively, smooth and convex functions. We establish sufficient conditions, via new discrete Lyapunov analyses, for achieving accelerated convergence rates which match Nesterov's methods in the strongly and general convex settings. Next, we study the convergence of limiting ordinary differential equations (ODEs) and point out currently notable gaps between the convergence properties of the corresponding algorithms and ODEs. Finally, we propose a novel class of discrete algorithms, called the Hamiltonian assisted gradient method, directly based on a Hamiltonian function and several interpretable operations, and then demonstrate meaningful and unified interpretations of our acceleration conditions.",
    "link": "http://arxiv.org/abs/2304.10063",
    "context": "Title: Understanding Accelerated Gradient Methods: Lyapunov Analyses and Hamiltonian Assisted Interpretations. (arXiv:2304.10063v1 [math.OC])\nAbstract: We formulate two classes of first-order algorithms more general than previously studied for minimizing smooth and strongly convex or, respectively, smooth and convex functions. We establish sufficient conditions, via new discrete Lyapunov analyses, for achieving accelerated convergence rates which match Nesterov's methods in the strongly and general convex settings. Next, we study the convergence of limiting ordinary differential equations (ODEs) and point out currently notable gaps between the convergence properties of the corresponding algorithms and ODEs. Finally, we propose a novel class of discrete algorithms, called the Hamiltonian assisted gradient method, directly based on a Hamiltonian function and several interpretable operations, and then demonstrate meaningful and unified interpretations of our acceleration conditions.",
    "path": "papers/23/04/2304.10063.json",
    "total_tokens": 841,
    "translated_title": "理解加速梯度方法：李亚普诺夫分析和哈密顿助理解释",
    "translated_abstract": "我们提出了两类比之前研究的更加广泛的一阶算法，用于最小化平滑且强凸或平滑凸函数。我们通过新的离散李亚普诺夫分析建立足够条件，使其达到加速收敛速度，这与在强和一般的凸设置中与Nesterov的方法相匹配。接下来，我们研究了极限普通微分方程的收敛性，并指出了相应算法和微分方程的收敛性质之间目前明显的差距。最后，我们提出了一种新型的离散算法——哈密顿助理梯度方法——该方法直接基于一个哈密顿函数和多个可解释的操作，然后演示了我们加速条件的有意义和统一的解释。",
    "tldr": "本文提出了两类更广泛的一阶算法，通过新的离散李亚普诺夫分析建立足够条件，实现了与Nesterov方法类似的加速收敛速度。此外，还提出了哈密顿助理梯度方法来解释加速条件的意义。",
    "en_tdlr": "This paper proposes two more general classes of first-order algorithms for minimizing smooth and convex functions, with accelerated convergence rates achieved through new discrete Lyapunov analyses. It also introduces the Hamiltonian assisted gradient method to provide meaning and unified interpretation of the acceleration conditions."
}