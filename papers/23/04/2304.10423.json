{
    "title": "Fully Autonomous Programming with Large Language Models. (arXiv:2304.10423v1 [cs.SE])",
    "abstract": "Current approaches to program synthesis with Large Language Models (LLMs) exhibit a \"near miss syndrome\": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generati",
    "link": "http://arxiv.org/abs/2304.10423",
    "context": "Title: Fully Autonomous Programming with Large Language Models. (arXiv:2304.10423v1 [cs.SE])\nAbstract: Current approaches to program synthesis with Large Language Models (LLMs) exhibit a \"near miss syndrome\": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generati",
    "path": "papers/23/04/2304.10423.json",
    "total_tokens": 865,
    "translated_title": "基于大语言模型的完全自主编程",
    "translated_abstract": "目前基于大语言模型（LLMs）的程序综合方法存在“几乎成功综合”的问题：它们生成的程序在语义上类似于正确答案（通过文本相似性度量或人工评估来衡量），但由于输入输出格式错误等细微差异，对于单元测试而言的准确性很低甚至为零。这需要一种称为Synthesize、Execute、Debug（SED）的方法，首先生成解决方案的草稿，然后进行程序修复阶段以解决未通过的测试。为了有效地将这种方法应用于指令驱动的LLMs，需要确定哪些提示作为LLMs的指令表现最佳，并在修复未成功的程序和用新生成的程序替换它们之间取得平衡。我们通过比较以替换为重点、以修复为重点和混合调试策略以及不同的基于模板和基于模型的提示生成方法来实证探讨这些权衡。",
    "tldr": "本文讨论了基于大语言模型的完全自主编程方法，提出了Synthesize、Execute、Debug（SED）方法来解决程序生成的低准确度问题，并比较了不同的调试策略和提示生成方法。",
    "en_tdlr": "This paper discusses the fully autonomous programming approach based on Large Language Models (LLMs), proposes a Synthesize, Execute, Debug (SED) method to address the low accuracy problem of generated programs, and compares different debugging strategies and prompt generation methods."
}