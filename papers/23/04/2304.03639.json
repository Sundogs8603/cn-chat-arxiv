{
    "title": "Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])",
    "abstract": "Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of ",
    "link": "http://arxiv.org/abs/2304.03639",
    "context": "Title: Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])\nAbstract: Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of ",
    "path": "papers/23/04/2304.03639.json",
    "total_tokens": 874,
    "translated_title": "线性递归网络在长序列计数问题上的理论与实验困境",
    "translated_abstract": "先前的研究已经证明具有无限激活函数的RNN有计数的能力。然而，RNN的有效训练往往困难，通常无法学习准确的计数行为。本文通过研究最简单的线性单元RNN来解决这个问题。我们对线性RNN进行了理论分析，并确定了模型展现精确计数行为的条件。我们证明这些条件是必要且充分的。我们还使用涉及类似Dyck-1平衡符号的任务在两个不同的设置下进行了实证分析。我们发现线性RNN通常无法在标准方法训练下满足计数行为的必要且充分条件。我们研究了不同的训练序列长度和利用不同的目标类别对模型行为在训练期间和计数能力的影响。",
    "tldr": "本文研究了最简单的线性单元RNN在长序列计数问题上的极限，理论上使用的条件具有充分必要性；实验数据表明，通过标准的方法，该网络通常无法实现准确的计数行为。",
    "en_tdlr": "This paper studied the theoretical and practical limitations of linear single-cell RNN on long sequence counting problem. The necessary and sufficient conditions for counting behavior were identified and proved. The empirical analysis showed that the network generally cannot achieve accurate counting behavior through standard methods."
}