{
    "title": "A Brief Review of Explainable Artificial Intelligence in Healthcare. (arXiv:2304.01543v1 [cs.AI])",
    "abstract": "XAI refers to the techniques and methods for building AI applications which assist end users to interpret output and predictions of AI models. Black box AI applications in high-stakes decision-making situations, such as medical domain have increased the demand for transparency and explainability since wrong predictions may have severe consequences. Model explainability and interpretability are vital successful deployment of AI models in healthcare practices. AI applications' underlying reasoning needs to be transparent to clinicians in order to gain their trust. This paper presents a systematic review of XAI aspects and challenges in the healthcare domain. The primary goals of this study are to review various XAI methods, their challenges, and related machine learning models in healthcare. The methods are discussed under six categories: Features-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric methods. Most importantly, th",
    "link": "http://arxiv.org/abs/2304.01543",
    "context": "Title: A Brief Review of Explainable Artificial Intelligence in Healthcare. (arXiv:2304.01543v1 [cs.AI])\nAbstract: XAI refers to the techniques and methods for building AI applications which assist end users to interpret output and predictions of AI models. Black box AI applications in high-stakes decision-making situations, such as medical domain have increased the demand for transparency and explainability since wrong predictions may have severe consequences. Model explainability and interpretability are vital successful deployment of AI models in healthcare practices. AI applications' underlying reasoning needs to be transparent to clinicians in order to gain their trust. This paper presents a systematic review of XAI aspects and challenges in the healthcare domain. The primary goals of this study are to review various XAI methods, their challenges, and related machine learning models in healthcare. The methods are discussed under six categories: Features-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric methods. Most importantly, th",
    "path": "papers/23/04/2304.01543.json",
    "total_tokens": 877,
    "translated_title": "解析可解释人工智能在医疗保健领域的简要评论",
    "translated_abstract": "XAI是指构建AI应用程序的技术和方法，可以帮助最终用户解释AI模型的输出和预测。在高风险决策情境中（如医学领域）使用黑盒AI应用程序增加了透明性和可解释性的需求，因为错误的预测可能会产生严重后果。模型的可解释性对于在医疗保健实践中成功部署AI模型至关重要。需要让临床医生透明地了解AI应用程序的基本推理过程以获得他们的信任。本文系统地评估了可解释人工智能在医疗保健领域中的方方面面和挑战。本研究的主要目标是回顾各种XAI方法、其挑战以及医疗保健中的相关机器学习模型。这些方法分为六类：特征导向方法、全局方法、概念模型、代理模型、本地像素方法和以人为中心的方法。",
    "tldr": "这篇文章系统评估了可解释人工智能在医疗保健领域的各种挑战和方法，旨在提高AI模型的可解释性，使其对临床医生更加透明可信。",
    "en_tdlr": "This paper presents a systematic review of various challenges and methods of explainable artificial intelligence in healthcare, with the aim of increasing the transparency and trustworthiness of AI models to clinicians."
}