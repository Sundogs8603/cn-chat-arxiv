{
    "title": "BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])",
    "abstract": "Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.",
    "link": "http://arxiv.org/abs/2304.05219",
    "context": "Title: BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])\nAbstract: Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.",
    "path": "papers/23/04/2304.05219.json",
    "total_tokens": 789,
    "translated_title": "BanditQ - 在敌对环境中保证用户每次奖励的无悔学习",
    "translated_abstract": "经典的在线预测算法如Hedge在设计上具有不公平性，因为它们尝试尽可能多地玩最具回报的臂而忽略次优臂，以实现亚线性遗憾。本文考虑在具有对所有臂累积奖励速率下界的敌对设置中，以公平的在线预测问题。通过将基本排队论与在线学习相结合，我们提出了一种名为BanditQ的新的在线预测策略，它在全信息设置下实现了目标速率约束，并实现了$O(T^{3/4})$的遗憾。BanditQ的设计和分析涉及潜在函数方法的新颖应用，并具有独立的兴趣。",
    "tldr": "提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。",
    "en_tdlr": "A new online prediction policy called BanditQ is proposed to achieve target rate constraints in a fair manner in an adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms, achieving a regret of $O(T^{3/4})$ in the full-information setting."
}