{
    "title": "Solving Tensor Low Cycle Rank Approximation. (arXiv:2304.06594v1 [cs.DS])",
    "abstract": "Large language models have become ubiquitous in modern life, finding applications in various domains such as natural language processing, language translation, and speech recognition. Recently, a breakthrough work [Zhao, Panigrahi, Ge, and Arora Arxiv 2023] explains the attention model from probabilistic context-free grammar (PCFG). One of the central computation task for computing probability in PCFG is formulating a particular tensor low rank approximation problem, we can call it tensor cycle rank. Given an $n \\times n \\times n$ third order tensor $A$, we say that $A$ has cycle rank-$k$ if there exists three $n \\times k^2$ size matrices $U , V$, and $W$ such that for each entry in each \\begin{align*} A_{a,b,c} = \\sum_{i=1}^k \\sum_{j=1}^k \\sum_{l=1}^k U_{a,i+k(j-1)} \\otimes V_{b, j + k(l-1)} \\otimes W_{c, l + k(i-1) } \\end{align*} for all $a \\in [n], b \\in [n], c \\in [n]$. For the tensor classical rank, tucker rank and train rank, it has been well studied in [Song, Woodruff, Zhong SOD",
    "link": "http://arxiv.org/abs/2304.06594",
    "context": "Title: Solving Tensor Low Cycle Rank Approximation. (arXiv:2304.06594v1 [cs.DS])\nAbstract: Large language models have become ubiquitous in modern life, finding applications in various domains such as natural language processing, language translation, and speech recognition. Recently, a breakthrough work [Zhao, Panigrahi, Ge, and Arora Arxiv 2023] explains the attention model from probabilistic context-free grammar (PCFG). One of the central computation task for computing probability in PCFG is formulating a particular tensor low rank approximation problem, we can call it tensor cycle rank. Given an $n \\times n \\times n$ third order tensor $A$, we say that $A$ has cycle rank-$k$ if there exists three $n \\times k^2$ size matrices $U , V$, and $W$ such that for each entry in each \\begin{align*} A_{a,b,c} = \\sum_{i=1}^k \\sum_{j=1}^k \\sum_{l=1}^k U_{a,i+k(j-1)} \\otimes V_{b, j + k(l-1)} \\otimes W_{c, l + k(i-1) } \\end{align*} for all $a \\in [n], b \\in [n], c \\in [n]$. For the tensor classical rank, tucker rank and train rank, it has been well studied in [Song, Woodruff, Zhong SOD",
    "path": "papers/23/04/2304.06594.json",
    "total_tokens": 1045,
    "translated_title": "解决张量低周期秩近似问题",
    "translated_abstract": "大型语言模型已经成为现代生活中普遍存在的事物，在自然语言处理、语言翻译和语音识别等各个领域都有着广泛的应用。最近的一项突破性工作[Zhao, Panigrahi, Ge, and Arora Arxiv 2023]从概率上下文无关语法(PCFG)的角度解释了注意力模型。在计算PCFG概率的核心计算任务中，需要解决一个特定的张量低周期秩近似问题，我们称之为张量周期秩。给定一个$n\\times n\\times n$的三阶张量$A$，如果存在三个$n\\times k^2$大小的矩阵$U,V,W$，满足对于每个条目中的每个$A_{a,b,c}$，都有\\begin{align*} A_{a,b,c} = \\sum_{i=1}^k \\sum_{j=1}^k \\sum_{l=1}^k U_{a,i+k(j-1)} \\otimes V_{b, j + k(l-1)} \\otimes W_{c, l + k(i-1) } \\end{align*},对于所有的$a \\in [n], b \\in [n], c \\in [n]$，则称$A$具有周期秩-$k$。对于张量经典秩、Tucker秩和Train秩等问题已经被广泛研究[SOD",
    "tldr": "该论文介绍了如何解决张量低周期秩近似问题，这是自然语言处理、语言翻译和语音识别中重要的计算任务。"
}