{
    "title": "Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v1 [cs.LG])",
    "abstract": "The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.",
    "link": "http://arxiv.org/abs/2304.08309",
    "context": "Title: Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v1 [cs.LG])\nAbstract: The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.",
    "path": "papers/23/04/2304.08309.json",
    "total_tokens": 876,
    "translated_title": "Bayesian Optimization中线性化Laplace的优势和局限性",
    "translated_abstract": "线性化Laplace逼近(LLA)已被证明在构建贝叶斯神经网络时有效且高效。它在理论上具有吸引力，因为它可以被看作是具有高斯过程后验的最大后验预测函数最大化的神经网络的平均函数，并且由经验神经曲面核诱导的协方差函数。然而，尽管已经研究过其在图像分类等大规模任务中的效果，但在诸如Bayesian optimization这样的序列决策问题中尚未对其进行研究，其中高斯过程是默认的代理模型，具有简单的平均函数和核函数，例如径向基函数。在本文中，我们研究了LLA在Bayesian optimization中的有用性和灵活性，并强调其强大的性能。但是，我们还提出了可能出现的一些问题和一个LLA可能存在的问题，即当搜索空间是无界的时候。",
    "tldr": "本论文研究了在线性化Laplace逼近(LLA)在Bayesian optimization中的应用。虽然LLA在构建贝叶斯神经网络时已被证明具有效性和高效性，但是在序列决策问题中，需要考虑其可能的局限性。",
    "en_tdlr": "This paper studies the application of Linearized-Laplace approximation (LLA) in Bayesian optimization, highlighting its strong performance and flexibility. However, the potential pitfalls and limitations need to be considered, especially in sequential decision-making problems."
}