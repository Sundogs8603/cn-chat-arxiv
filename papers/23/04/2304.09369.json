{
    "title": "ContraCluster: Learning to Classify without Labels by Contrastive Self-Supervision and Prototype-Based Semi-Supervision. (arXiv:2304.09369v1 [cs.CV])",
    "abstract": "The recent advances in representation learning inspire us to take on the challenging problem of unsupervised image classification tasks in a principled way. We propose ContraCluster, an unsupervised image classification method that combines clustering with the power of contrastive self-supervised learning. ContraCluster consists of three stages: (1) contrastive self-supervised pre-training (CPT), (2) contrastive prototype sampling (CPS), and (3) prototype-based semi-supervised fine-tuning (PB-SFT). CPS can select highly accurate, categorically prototypical images in an embedding space learned by contrastive learning. We use sampled prototypes as noisy labeled data to perform semi-supervised fine-tuning (PB-SFT), leveraging small prototypes and large unlabeled data to further enhance the accuracy. We demonstrate empirically that ContraCluster achieves new state-of-the-art results for standard benchmark datasets including CIFAR-10, STL-10, and ImageNet-10. For example, ContraCluster achi",
    "link": "http://arxiv.org/abs/2304.09369",
    "context": "Title: ContraCluster: Learning to Classify without Labels by Contrastive Self-Supervision and Prototype-Based Semi-Supervision. (arXiv:2304.09369v1 [cs.CV])\nAbstract: The recent advances in representation learning inspire us to take on the challenging problem of unsupervised image classification tasks in a principled way. We propose ContraCluster, an unsupervised image classification method that combines clustering with the power of contrastive self-supervised learning. ContraCluster consists of three stages: (1) contrastive self-supervised pre-training (CPT), (2) contrastive prototype sampling (CPS), and (3) prototype-based semi-supervised fine-tuning (PB-SFT). CPS can select highly accurate, categorically prototypical images in an embedding space learned by contrastive learning. We use sampled prototypes as noisy labeled data to perform semi-supervised fine-tuning (PB-SFT), leveraging small prototypes and large unlabeled data to further enhance the accuracy. We demonstrate empirically that ContraCluster achieves new state-of-the-art results for standard benchmark datasets including CIFAR-10, STL-10, and ImageNet-10. For example, ContraCluster achi",
    "path": "papers/23/04/2304.09369.json",
    "total_tokens": 1037,
    "translated_title": "ContraCluster：通过对比自监督和基于原型的半监督学习学习无标签分类",
    "translated_abstract": "最近表示学习的进展启发我们采用一种有原则的方式应对无监督的图像分类任务。我们提出了ContraCluster，这是一种无监督的图像分类方法，将聚类与对比自监督学习的能力相结合。ContraCluster包括三个阶段：(1) 对比自监督预训练(CPT)，(2) 对比原型采样(CPS)，(3) 基于原型的半监督微调(PB-SFT)。CPS可以在由对比学习学习的嵌入空间中选择高准确性的类别原型图像。我们使用采样的原型作为带噪声的标记数据来执行半监督微调(PB-SFT)，利用小原型和大规模未标记数据进一步提高准确性。我们凭经验证明，ContraCluster在标准基准数据集包括CIFAR-10，STL-10和ImageNet-10中实现了新的最先进结果。例如，ContraCluster在仅有10%标记数据的情况下实现了87.9％的CIFAR-10准确率，超过了先前最先进技术水平3.8％。",
    "tldr": "ContraCluster是一种无监督的图像分类方法，结合了聚类和对比自监督学习。ContraCluster使用对比原型采样和基于原型的半监督微调来提高准确性，并且在CIFAR-10等标准基准数据集上实现了新的最先进结果。"
}