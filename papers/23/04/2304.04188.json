{
    "title": "HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural Representations via Knowledge Distillation. (arXiv:2304.04188v1 [cs.GR])",
    "abstract": "Implicit Neural Representations (INRs) have recently exhibited immense potential in the field of scientific visualization for both data generation and visualization tasks. However, these representations often consist of large multi-layer perceptrons (MLPs), necessitating millions of operations for a single forward pass, consequently hindering interactive visual exploration. While reducing the size of the MLPs and employing efficient parametric encoding schemes can alleviate this issue, it compromises generalizability for unseen parameters, rendering it unsuitable for tasks such as temporal super-resolution. In this paper, we introduce HyperINR, a novel hypernetwork architecture capable of directly predicting the weights for a compact INR. By harnessing an ensemble of multiresolution hash encoding units in unison, the resulting INR attains state-of-the-art inference performance (up to 100x higher inference bandwidth) and can support interactive photo-realistic volume visualization. Addi",
    "link": "http://arxiv.org/abs/2304.04188",
    "context": "Title: HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural Representations via Knowledge Distillation. (arXiv:2304.04188v1 [cs.GR])\nAbstract: Implicit Neural Representations (INRs) have recently exhibited immense potential in the field of scientific visualization for both data generation and visualization tasks. However, these representations often consist of large multi-layer perceptrons (MLPs), necessitating millions of operations for a single forward pass, consequently hindering interactive visual exploration. While reducing the size of the MLPs and employing efficient parametric encoding schemes can alleviate this issue, it compromises generalizability for unseen parameters, rendering it unsuitable for tasks such as temporal super-resolution. In this paper, we introduce HyperINR, a novel hypernetwork architecture capable of directly predicting the weights for a compact INR. By harnessing an ensemble of multiresolution hash encoding units in unison, the resulting INR attains state-of-the-art inference performance (up to 100x higher inference bandwidth) and can support interactive photo-realistic volume visualization. Addi",
    "path": "papers/23/04/2304.04188.json",
    "total_tokens": 940,
    "translated_title": "基于知识蒸馏的超网络HyperINR：高效快速的隐式神经表示",
    "translated_abstract": "隐式神经表示（INRs）在科学可视化领域的数据生成和可视化任务中表现出巨大的潜力。然而，这些表示通常由大型多层感知器（MLPs）组成，对于单次前传需要数百万操作，从而阻碍交互式视觉探索。本文提出了HyperINR，一种新型超网络架构，能够直接预测紧凑INR的权重，通过同步利用多分辨率哈希编码单元的集合，从而使得结果INR具有最先进的推理性能(最高可达100倍的推理带宽)，并且能够支持交互式的照片级体积可视化。此外，还提出了一种基于知识蒸馏的训练框架，使得HyperINR即便在存在未见过的参数变化的情况下也能良好地进行泛化。",
    "tldr": "本文提出了一种基于知识蒸馏的超网络架构HyperINR，能够高效快速地构建出最先进推理性能的隐式神经表示（INR），并且支持交互式的照片级体积可视化。",
    "en_tdlr": "This paper proposes a novel hypernetwork architecture, called HyperINR, based on knowledge distillation, capable of constructing state-of-the-art implicit neural representations (INRs) with high inference performance and interactive photorealistic volume visualization. It also introduces a training framework that enables HyperINR to generalize well even with unseen parametric variations."
}