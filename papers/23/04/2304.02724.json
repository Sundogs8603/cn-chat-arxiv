{
    "title": "Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound. (arXiv:2304.02724v1 [cs.CV])",
    "abstract": "Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study i",
    "link": "http://arxiv.org/abs/2304.02724",
    "context": "Title: Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound. (arXiv:2304.02724v1 [cs.CV])\nAbstract: Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study i",
    "path": "papers/23/04/2304.02724.json",
    "total_tokens": 1006,
    "translated_title": "探索自监督预训练策略在M-模式肺部超声波中检测缺失肺滑动性的实用性。",
    "translated_abstract": "自监督预训练已被证实可以提高医学影像中监督学习任务的性能。本研究调查了在进行监督微调之前自监督预训练的实用性，用于M-模式肺部超声波图像中的肺滑动分类任务。我们提出了一种新的配对关系，将从同一B-模式图像构建的M-模式图像进行了配对，并研究了特定于M-模式肺部超声波的数据增强程序的实用性。结果表明，自监督预训练比完全监督更能提高性能，尤其适用于未使用ImageNet预训练权重的特征提取器。此外，我们观察到包括大量未标记数据会导致在外部验证数据集上提高性能，凸显了自监督性的价值，以提高自动超声解释的通用性。作者最好的知识，本研究是第一个探索使用自监督预训练来解决M-模式肺部超声图像分类问题的工作。",
    "tldr": "本研究探索了在M-模式肺部超声图像中使用自监督预训练来提高缺失肺滑动检测的分类性能。结果表明自监督预训练可以显著提高性能，并且使用大量未标记数据可以提高模型的通用性和外部验证性能。",
    "en_tdlr": "This study investigated the utility of self-supervised pretraining for improving the classification performance of absent lung sliding detection in M-mode lung ultrasound images. Results showed that self-supervised pretraining can significantly improve performance, and incorporating a large amount of unlabelled data can enhance model generalizability and external validation performance. This is the first work exploring the use of self-supervised pretraining for addressing the classification problem in M-mode lung ultrasound images."
}