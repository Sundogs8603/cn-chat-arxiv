{
    "title": "Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study. (arXiv:2304.10909v1 [cs.LG])",
    "abstract": "Medical coding is the task of assigning medical codes to clinical free-text documentation. Healthcare professionals manually assign such codes to track patient diagnoses and treatments. Automated medical coding can considerably alleviate this administrative burden. In this paper, we reproduce, compare, and analyze state-of-the-art automated medical coding machine learning models. We show that several models underperform due to weak configurations, poorly sampled train-test splits, and insufficient evaluation. In previous work, the macro F1 score has been calculated sub-optimally, and our correction doubles it. We contribute a revised model comparison using stratified sampling and identical experimental setups, including hyperparameters and decision boundary tuning. We analyze prediction errors to validate and falsify assumptions of previous works. The analysis confirms that all models struggle with rare codes, while long documents only have a negligible impact. Finally, we present the ",
    "link": "http://arxiv.org/abs/2304.10909",
    "context": "Title: Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study. (arXiv:2304.10909v1 [cs.LG])\nAbstract: Medical coding is the task of assigning medical codes to clinical free-text documentation. Healthcare professionals manually assign such codes to track patient diagnoses and treatments. Automated medical coding can considerably alleviate this administrative burden. In this paper, we reproduce, compare, and analyze state-of-the-art automated medical coding machine learning models. We show that several models underperform due to weak configurations, poorly sampled train-test splits, and insufficient evaluation. In previous work, the macro F1 score has been calculated sub-optimally, and our correction doubles it. We contribute a revised model comparison using stratified sampling and identical experimental setups, including hyperparameters and decision boundary tuning. We analyze prediction errors to validate and falsify assumptions of previous works. The analysis confirms that all models struggle with rare codes, while long documents only have a negligible impact. Finally, we present the ",
    "path": "papers/23/04/2304.10909.json",
    "total_tokens": 999,
    "translated_title": "MIMIC-III和MIMIC-IV上的自动化医疗编码：一项关键回顾和可复制性研究",
    "translated_abstract": "医疗编码是将医学代码分配给临床自由文档的任务。医疗专业人士手动分配这些代码以跟踪患者的诊断和治疗。自动化医疗编码可以极大地减轻这种行政负担。本文重现、比较和分析了最先进的自动化医疗编码机器学习模型。我们显示出多个模型表现不佳，原因是配置弱、训练-测试拆分样本不足以及评估不充分。在以往的工作中，宏平均F1分数被计算出亚优的结果，并且我们的修正使其翻倍。我们采用分层抽样和相同的实验设置进行了修订的模型比较，包括超参数和决策边界调整。我们分析预测误差来验证和证伪以前的工作假设。分析证实，所有模型都难以处理稀有的代码，而长文档仅对结果有微不足道的影响。最后，我们提出了一种基于流行病学采样的改进，该方法可以更好地评估系统的性能，并公开了我们的代码和数据集。",
    "tldr": "本文探究了在MIMIC-III和MIMIC-IV上进行自动化医疗编码的最新机器学习模型，并发现了这些模型的局限性和不足之处。我们提出了一种改进方法，该方法可以更好地评估系统性能，并公开了我们的代码和数据集。",
    "en_tdlr": "This paper explores the latest machine learning models for automated medical coding on MIMIC-III and MIMIC-IV, identifying limitations and shortcomings of these models. The authors propose an improved method for evaluating system performance and openly share their code and dataset."
}