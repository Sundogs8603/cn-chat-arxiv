{
    "title": "Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator. (arXiv:2304.04911v1 [cs.LG])",
    "abstract": "Many state-of-the art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for 12 hours (overnight) without an operator present with",
    "link": "http://arxiv.org/abs/2304.04911",
    "context": "Title: Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator. (arXiv:2304.04911v1 [cs.LG])\nAbstract: Many state-of-the art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for 12 hours (overnight) without an operator present with",
    "path": "papers/23/04/2304.04911.json",
    "total_tokens": 918,
    "translated_title": "实时无模型深度强化学习用于系列弹性执行器力控制",
    "translated_abstract": "系列弹性执行器(SEAs)是许多最先进的机器人应用中使用的一种闭环力控制技术，可实现诸如行走、举起和操作等复杂任务。模型自由PID控制方法更容易受到SEAs非线性的不稳定性影响，而级联模型基础鲁棒控制器可以消除这些影响以实现稳定的力控制。然而，这些模型基础方法需要详细调查以准确表征系统。深度强化学习(DRL)已被证明是一种有效的连续控制任务无模型方法，其中仅有少量工作涉及硬件学习。本文描述了在SEAs摆系统硬件上使用Proximal Policy Optimization（PPO）算法对DRL策略进行训练过程，以跟踪力控制轨迹从0.05 ~ 0.35 Hz，振幅为50 N。开发和使用安全机制来训练策略12小时(过夜)而无需操作员的存在。",
    "tldr": "本研究论文探讨了一种实时无模型深度强化学习技术用于系列弹性执行器的力量控制，证明了这种技术在连续控制任务中的有效性和在硬件学习中的应用价值。",
    "en_tdlr": "This paper explores a real-time model-free deep reinforcement learning technique for force control of series elastic actuators. It demonstrates the effectiveness of this technique in continuous control tasks and its value in hardware learning."
}