{
    "title": "Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])",
    "abstract": "We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto",
    "link": "http://arxiv.org/abs/2304.13013",
    "context": "Title: Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])\nAbstract: We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto",
    "path": "papers/23/04/2304.13013.json",
    "total_tokens": 858,
    "translated_title": "大规模视觉语言模型的稳定和低精度训练",
    "translated_abstract": "我们介绍了新的方法，用于加速和稳定大语言-视觉模型的训练。为加速训练，我们引入了SwitchBack，这是一种线性层用于int8量化训练，其提供了13-25％的速度提升，而与1B参数CLIP ViT-Huge的bfloat16训练的性能相匹配，在目前为止是最大的int8训练。我们的重点是int8，因为GPU支持float8很少，虽然我们也通过模拟分析了float8训练。为了稳定训练，我们分析了损失峰值，并发现它们在二次梯度估计器的AdamW second moment之后1-8次迭代中一致发生低估。因此，我们推荐使用AdamW-Adafacto方法。",
    "tldr": "该研究介绍了用于大规模视觉语言模型稳定和低精度训练的新方法，包括SwitchBack和AdamW-Adafacto方法。这些方法提高了训练速度和稳定性。",
    "en_tdlr": "This study introduces new methods for stable and low-precision training for large-scale vision-language models, including SwitchBack and the AdamW-Adafacto approach. These methods improve training speed and stability."
}