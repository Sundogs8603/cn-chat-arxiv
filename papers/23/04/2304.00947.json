{
    "title": "RePAST: Relative Pose Attention Scene Representation Transformer. (arXiv:2304.00947v2 [cs.CV] UPDATED)",
    "abstract": "The Scene Representation Transformer (SRT) is a recent method to render novel views at interactive rates. Since SRT uses camera poses with respect to an arbitrarily chosen reference camera, it is not invariant to the order of the input views. As a result, SRT is not directly applicable to large-scale scenes where the reference frame would need to be changed regularly. In this work, we propose Relative Pose Attention SRT (RePAST): Instead of fixing a reference frame at the input, we inject pairwise relative camera pose information directly into the attention mechanism of the Transformers. This leads to a model that is by definition invariant to the choice of any global reference frame, while still retaining the full capabilities of the original method. Empirical results show that adding this invariance to the model does not lead to a loss in quality. We believe that this is a step towards applying fully latent transformer-based rendering methods to large-scale scenes.",
    "link": "http://arxiv.org/abs/2304.00947",
    "context": "Title: RePAST: Relative Pose Attention Scene Representation Transformer. (arXiv:2304.00947v2 [cs.CV] UPDATED)\nAbstract: The Scene Representation Transformer (SRT) is a recent method to render novel views at interactive rates. Since SRT uses camera poses with respect to an arbitrarily chosen reference camera, it is not invariant to the order of the input views. As a result, SRT is not directly applicable to large-scale scenes where the reference frame would need to be changed regularly. In this work, we propose Relative Pose Attention SRT (RePAST): Instead of fixing a reference frame at the input, we inject pairwise relative camera pose information directly into the attention mechanism of the Transformers. This leads to a model that is by definition invariant to the choice of any global reference frame, while still retaining the full capabilities of the original method. Empirical results show that adding this invariance to the model does not lead to a loss in quality. We believe that this is a step towards applying fully latent transformer-based rendering methods to large-scale scenes.",
    "path": "papers/23/04/2304.00947.json",
    "total_tokens": 917,
    "translated_title": "RePAST：相对位姿注意力场景表示变换器",
    "translated_abstract": "场景表示变换器（SRT）是一种最近的方法，可以以交互速率渲染新视图。由于SRT使用相对于任意选择的参考摄像机的相机姿态，因此它对输入视图的顺序不变。因此，SRT不直接适用于需要定期更改参考帧的大规模场景。在这项工作中，我们提出了相对姿态注意力SRT（RePAST）：我们将成对的相对相机姿态信息直接注入转换器的注意机制中，而不是在输入时固定一个参考帧。这导致了一个模型，其定义不变于任何全局参考帧的选择，同时仍保留原始方法的全部功能。实证结果表明，将这种不变性添加到模型中并不会导致质量下降。我们认为这是向应用完全潜在的基于Transformer的渲染方法于大规模场景迈出的一步。",
    "tldr": "RePAST是一种相对位姿注意力场景表示变换器，其将成对的相对相机姿态信息直接注入转换器的注意机制中，不需要固定参考帧，同时保留了原始方法的全部功能，加入这种不变性并不会导致质量下降，可以应用于大规模场景。",
    "en_tdlr": "RePAST is a relative pose attention scene representation transformer that injects pairwise relative camera pose information directly into the attention mechanism of the transformers, making it invariant to the choice of any global reference frame and retaining the full capabilities of the original method. The empirical results show that the invariance added to the model does not lead to any loss in quality, making it applicable to large-scale scenes."
}