{
    "title": "Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])",
    "abstract": "We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\\sqrt{\\mu^*(1-\\mu^*) K T \\ln K} + K \\ln T)$, where $\\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.",
    "link": "http://arxiv.org/abs/2304.14989",
    "context": "Title: Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])\nAbstract: We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\\sqrt{\\mu^*(1-\\mu^*) K T \\ln K} + K \\ln T)$, where $\\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.",
    "path": "papers/23/04/2304.14989.json",
    "total_tokens": 834,
    "translated_title": "Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用",
    "translated_abstract": "本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\\sqrt{\\mu^*(1-\\mu^*) K T \\ln K} + K \\ln T)$，其中$\\mu^*$是最优臂的期望奖励，$T$是时段长度。",
    "tldr": "本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。",
    "en_tdlr": "This paper proposes a new algorithm called Kullback-Leibler Maillard Sampling (KL-MS) for multi-armed bandits with bounded rewards, which is an extension of Maillard sampling in KL space and has good asymptotic performance in Bernoulli rewards."
}