{
    "title": "Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data. (arXiv:2304.02080v1 [cs.CV])",
    "abstract": "Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual back",
    "link": "http://arxiv.org/abs/2304.02080",
    "context": "Title: Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data. (arXiv:2304.02080v1 [cs.CV])\nAbstract: Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual back",
    "path": "papers/23/04/2304.02080.json",
    "total_tokens": 751,
    "translated_title": "无需对齐视频和文本数据，可扩展准确的自监督多模态表示学习",
    "translated_abstract": "弱监督数据集的扩展在图形文本领域已被证明非常有效，并为大多数最新的计算机视觉和多模态神经网络做出了贡献。然而，现有的大规模视频文本数据集和挖掘技术存在多个限制，如对齐数据的稀缺性、数据缺乏多样性以及对齐数据的采集难度。本文展示了最新的图像字幕技术如何使我们能够在没有任何并行视频文本数据的情况下预训练高质量的视频模型。",
    "tldr": "本文介绍了一种自监督的多模态表示学习方法，可以在不需要对齐视频和文本数据的情况下扩展准确的数据集。",
    "en_tdlr": "This paper introduces a self-supervised multimodal representation learning method that can scale up weakly-supervised datasets without the need for aligned video and text data."
}