{
    "title": "A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])",
    "abstract": "In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \\textit{privacy leakage} cannot be ignored in the presence of \\textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \\textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \\textit{oracle} that ha",
    "link": "http://arxiv.org/abs/2304.05836",
    "context": "Title: A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])\nAbstract: In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \\textit{privacy leakage} cannot be ignored in the presence of \\textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \\textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \\textit{oracle} that ha",
    "path": "papers/23/04/2304.05836.json",
    "total_tokens": 1219,
    "translated_title": "一种联邦学习的博弈论框架",
    "translated_abstract": "在联邦学习中，良性参与者旨在协同优化全局模型。然而，在存在半诚实的对手时，\\textit{隐私泄漏}的风险是不可忽视的。现有研究要么专注于设计保护机制，要么专注于发明攻击机制。虽然保护者与攻击者之间的斗争似乎永无止境，但我们关心一个关键问题：是否可能事先预防潜在的攻击？为了解决这个问题，我们提出了一个博弈论框架，同时考虑FL保护者和攻击者的相应收益，其中包括计算成本、FL模型效用和隐私泄漏风险。我们将此游戏称为联邦学习安全博弈（FLSG），在其中保护者和攻击者都不知道所有参与者的收益。为了处理这种情况固有的\\textit{不完全信息}，我们建议将FLSG与一个\\textit{oracle}相关联，该oracle具有所有参与者的收益知识。我们分析了在各种效用函数和攻击模型组合下FLSG的纳什均衡存在性和唯一性。此外，我们提出了一个实用算法来近似oracle并保持隐私。实验结果说明了我们的算法在预防和检测现实世界中的FL场景中的攻击方面的有效性。",
    "tldr": "本文提出了一个名为联邦学习安全博弈（FLSG）的博弈论框架，该框架同时考虑到联邦学习的保护者和攻击者的收益，包括计算成本、FL模型效用和隐私泄漏风险，并提出了一个实用算法来近似oracle并保持隐私。研究表明该算法对于预防和检测现实世界中的联邦学习攻击具有有效性。",
    "en_tdlr": "This paper proposes a game-theoretic framework named Federated Learning Security Game (FLSG) for preventing potential attacks in advance in federated learning, which considers the payoffs of both defenders and attackers including computational costs, FL model utilities, and privacy leakage risks. The paper also presents a practical algorithm to approximate the oracle while maintaining privacy and experimentally demonstrates the effectiveness of the proposed algorithm in preventing and detecting attacks in a real-world FL scenario."
}