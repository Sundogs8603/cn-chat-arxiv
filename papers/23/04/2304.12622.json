{
    "title": "Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures. (arXiv:2304.12622v1 [cs.CV])",
    "abstract": "Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which",
    "link": "http://arxiv.org/abs/2304.12622",
    "context": "Title: Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures. (arXiv:2304.12622v1 [cs.CV])\nAbstract: Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which",
    "path": "papers/23/04/2304.12622.json",
    "total_tokens": 692,
    "translated_title": "剪枝视觉模型中的偏差问题：深入分析与对策",
    "translated_abstract": "剪枝神经网络是一种常用的模型压缩方法，但最近的研究表明，剪枝可能会引起或加剧压缩模型输出的偏差。本文系统地研究和描述了这种现象在计算机视觉中的行为，并提出了一些易于使用的标准来帮助减少偏差。",
    "tldr": "本文针对计算机视觉中常用的剪枝神经网络方法，系统分析了其可能引发的偏差问题，提出了可以降低偏差的标准。",
    "en_tdlr": "This paper presents a systematic investigation of the phenomenon of induced bias in pruned convolutional neural networks for computer vision. The study proposes easy-to-use criteria to mitigate the bias issue."
}