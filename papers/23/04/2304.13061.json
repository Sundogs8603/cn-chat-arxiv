{
    "title": "iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])",
    "abstract": "In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var",
    "link": "http://arxiv.org/abs/2304.13061",
    "context": "Title: iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])\nAbstract: In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var",
    "path": "papers/23/04/2304.13061.json",
    "total_tokens": 931,
    "translated_title": "iMixer: 分层Hopfield网络暗示了可逆、隐式和迭代的MLP-Mixer",
    "translated_abstract": "在过去的几年中，Transformer在计算机视觉领域的成功促使寻找可以与之竞争的许多替代模型，如MLP-Mixer。尽管这些模型的引入偏差较弱，但它们的表现可与研究较多的卷积神经网络相媲美。最近对现代Hopfield网络的研究表明了某些基于能量的关联记忆模型与Transformer或MLP-Mixer之间的对应关系，并揭示了Transformer类型架构设计的理论背景。在本文中，我们将该对应关系推广到最近引入的分层Hopfield网络，并找到了iMixer，这是MLP-Mixer模型的新的概括。与普通的前馈神经网络不同，iMixer涉及从输出侧向输入侧传播的MLP层。我们将该模块特征化为可逆、隐式和迭代混合模块的一个例子。我们通过各种任务评估了模型的性能。",
    "tldr": "本文推广了 Hopkins field 分层网络，并介绍了 iMixer，MLP-Mixer 模型的新概括，不同于普通的前馈网络，iMixer 涉及到从输出到输入的传播的 MLP 层，被特征化为一个可逆、隐式、迭代的 mixing block。"
}