{
    "title": "The XAISuite framework and the implications of explanatory system dissonance. (arXiv:2304.08499v1 [cs.LG])",
    "abstract": "Explanatory systems make machine learning models more transparent. However, they are often inconsistent. In order to quantify and isolate possible scenarios leading to this discrepancy, this paper compares two explanatory systems, SHAP and LIME, based on the correlation of their respective importance scores using 14 machine learning models (7 regression and 7 classification) and 4 tabular datasets (2 regression and 2 classification). We make two novel findings. Firstly, the magnitude of importance is not significant in explanation consistency. The correlations between SHAP and LIME importance scores for the most important features may or may not be more variable than the correlation between SHAP and LIME importance scores averaged across all features. Secondly, the similarity between SHAP and LIME importance scores cannot predict model accuracy. In the process of our research, we construct an open-source library, XAISuite, that unifies the process of training and explaining models. Fin",
    "link": "http://arxiv.org/abs/2304.08499",
    "context": "Title: The XAISuite framework and the implications of explanatory system dissonance. (arXiv:2304.08499v1 [cs.LG])\nAbstract: Explanatory systems make machine learning models more transparent. However, they are often inconsistent. In order to quantify and isolate possible scenarios leading to this discrepancy, this paper compares two explanatory systems, SHAP and LIME, based on the correlation of their respective importance scores using 14 machine learning models (7 regression and 7 classification) and 4 tabular datasets (2 regression and 2 classification). We make two novel findings. Firstly, the magnitude of importance is not significant in explanation consistency. The correlations between SHAP and LIME importance scores for the most important features may or may not be more variable than the correlation between SHAP and LIME importance scores averaged across all features. Secondly, the similarity between SHAP and LIME importance scores cannot predict model accuracy. In the process of our research, we construct an open-source library, XAISuite, that unifies the process of training and explaining models. Fin",
    "path": "papers/23/04/2304.08499.json",
    "total_tokens": 961,
    "translated_abstract": "解释系统使得机器学习模型更加透明，但它们常常不一致。本文针对14个机器学习模型（7个回归和7个分类）和4个表格数据集（2个回归和2个分类），比较了两种解释系统SHAP和LIME的重要性分数之间的相关性，以量化和分离可能导致差异的情况。我们得出了两个新发现。首先，重要性的大小不会对解释的一致性造成显着影响。SHAP和LIME重要性分数之间的相关性可能更可变，也可能比所有特征的SHAP和LIME重要性分数平均相关性更稳定。其次，SHAP和LIME重要性分数之间的相似性不能预测模型的准确性。在研究过程中，我们构建了一个开源库，XAISuite，统一了模型的训练和解释过程。",
    "tldr": "本文比较了两种解释系统SHAP和LIME之间的相关性，发现重要性的大小对解释的一致性没有显著影响，SHAP和LIME重要性分数之间的相关性可能更可变，也可能比所有特征的SHAP和LIME重要性分数平均相关性更稳定，并且SHAP和LIME重要性分数之间的相似性不能预测模型的准确性。",
    "en_tdlr": "This paper compares the correlation between the importance scores of two explanatory systems, SHAP and LIME, and finds that the magnitude of importance does not significantly affect explanation consistency. The correlations between SHAP and LIME importance scores for the most important features may or may not be more variable than the correlation between SHAP and LIME importance scores averaged across all features. The similarity between SHAP and LIME importance scores cannot predict model accuracy."
}