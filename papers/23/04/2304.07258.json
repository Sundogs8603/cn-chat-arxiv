{
    "title": "Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games. (arXiv:2304.07258v1 [cs.CL])",
    "abstract": "Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure--e.g. in dialogue there are many valid responses for a given context. However, only some of these responses will be desirable in our downstream task. This raises the question of how we should train the model such that it can emulate the desirable behaviours, but not the undesirable ones. Current approaches train in a one-to-one setup--only a single target response is given for a single dialogue context--leading to models only learning to predict the average response, while ignoring the full range of possible responses. Using text-based games as a testbed, our approach, PASA, uses discrete latent variables to capture the range of different behaviours represented in our larger pre-training dataset. We then use knowledge distillation to distil the posterior probability distribution into a",
    "link": "http://arxiv.org/abs/2304.07258",
    "context": "Title: Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games. (arXiv:2304.07258v1 [cs.CL])\nAbstract: Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure--e.g. in dialogue there are many valid responses for a given context. However, only some of these responses will be desirable in our downstream task. This raises the question of how we should train the model such that it can emulate the desirable behaviours, but not the undesirable ones. Current approaches train in a one-to-one setup--only a single target response is given for a single dialogue context--leading to models only learning to predict the average response, while ignoring the full range of possible responses. Using text-based games as a testbed, our approach, PASA, uses discrete latent variables to capture the range of different behaviours represented in our larger pre-training dataset. We then use knowledge distillation to distil the posterior probability distribution into a",
    "path": "papers/23/04/2304.07258.json",
    "total_tokens": 937,
    "translated_abstract": "在NLP领域中，预先在大规模自我监督语料库上训练的语言模型，然后进行特定任务的微调已成为主流模式。这些预训练数据集通常具有一对多的结构——例如，在对话中，对于给定的背景，有许多合适的回复。但是，只有其中一些回复是我们下游任务中需要的。这就引发了一个问题：如何训练模型使其可以模拟所需的行为，而不是不需要的行为。当前的方法是在一对一的设置下进行训练——只为一个对话背景提供一个目标回应——导致模型只学习预测平均回应，而忽略了所有可能的回应范围。我们的方法PASA使用离散潜变量来捕捉较大预训练数据集中的不同行为范围，使用知识蒸馏将后验概率分布转化为在测试环境下的扩展响应的离散分布。我们在文本游戏中进行的实验表明，该方法在多个任务上都可以带来很好的效果。",
    "tldr": "该论文利用离散潜变量和知识蒸馏，解决了NLP中处理一对多关系的问题，并在文本游戏实验中取得良好效果。",
    "en_tdlr": "This paper proposes a method called PASA, which uses discrete latent variables and knowledge distillation to address the issue of one-to-many relations in NLP. The experiments conducted on text-based games demonstrate the effectiveness of the approach in dealing with multiple tasks."
}