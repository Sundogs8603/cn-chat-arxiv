{
    "title": "An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])",
    "abstract": "Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. ",
    "link": "http://arxiv.org/abs/2304.09388",
    "context": "Title: An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])\nAbstract: Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. ",
    "path": "papers/23/04/2304.09388.json",
    "total_tokens": 1055,
    "translated_title": "利用知识蒸馏压缩多语言神经机器翻译模型的实证研究",
    "translated_abstract": "知识蒸馏是一种压缩神经模型的方法。然而，尽管MNMT（多语言神经机器翻译）的普及和优越性，但从大型MNMT模型中提取知识的研究实际上并不存在。本文填补了这一空白，提出了一种利用知识蒸馏压缩MNMT模型的实证研究。我们以印地语到英语的翻译为案例研究，并证明了常用的语言无关和语言感知的蒸馏方法可以使模型压缩4-5倍，但性能下降多达3.5 BLEU。为了缓解这一问题，我们进行了多个设计上的实验，包括深层模型和浅层模型、参数共享、多阶段训练和适配器等。我们观察到，深层紧凑模型往往与浅层非紧凑模型一样好，将蒸馏模型在高质量子集上微调可以稍微提高翻译质量。",
    "tldr": "本文研究了利用知识蒸馏方法压缩多语言神经机器翻译模型的实证效果，并以印地语到英语的翻译为案例展示了蒸馏方法对模型大小和性能的影响。研究发现，深层紧凑模型往往与浅层非紧凑模型一样好，将蒸馏模型在高质量子集上微调可以提高翻译质量。",
    "en_tdlr": "This paper investigates the effectiveness of using knowledge distillation to compress multilingual neural machine translation models and demonstrates the impact of distillation methods on model size and performance in the context of Indic to English translation. The study finds that deeper compact models can perform as well as shallower non-compact ones and fine-tuning a distilled model on a high-quality subset can slightly boost translation quality."
}