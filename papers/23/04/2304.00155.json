{
    "title": "Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])",
    "abstract": "We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\\tilde{O}(LX\\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of \"optimism in the face of uncertainty\", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared",
    "link": "http://arxiv.org/abs/2304.00155",
    "context": "Title: Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])\nAbstract: We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\\tilde{O}(LX\\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of \"optimism in the face of uncertainty\", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared",
    "path": "papers/23/04/2304.00155.json",
    "total_tokens": 838,
    "translated_title": "使用线性规划在马尔可夫决策过程上进行在线强化学习",
    "translated_abstract": "本文考虑了具有未知转移矩阵和固定但未知分布的随机奖励的情况下，马尔可夫决策过程中的在线强化学习。学习者旨在通过与环境交互来学习最优策略并在有限的时间内最小化他们的遗憾。我们设计了一种简单而高效的模型算法，通过保持过渡和奖励函数的置信区间并使用占用度量将在线MDP与线性规划相连接，实现了$\\tilde{O}(LX\\sqrt{TA})$的高概率遗憾界。它比现有的使用类似置信区间框架的算法实现了更紧的遗憾界并改善了计算效率。",
    "tldr": "本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。",
    "en_tdlr": "This paper proposes a simple and efficient algorithm for online MDP learning under unknown transition matrices and fixed but unknown distributions, achieving tighter regret bounds compared to existing algorithms using similar confidence set frameworks and improving computational efficiency."
}