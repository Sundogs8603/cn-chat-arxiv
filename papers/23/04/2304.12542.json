{
    "title": "Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion. (arXiv:2304.12542v1 [cs.CV])",
    "abstract": "Depth completion and object detection are two crucial tasks often used for aerial 3D mapping, path planning, and collision avoidance of Uncrewed Aerial Vehicles (UAVs). Common solutions include using measurements from a LiDAR sensor; however, the generated point cloud is often sparse and irregular and limits the system's capabilities in 3D rendering and safety-critical decision-making. To mitigate this challenge, information from other sensors on the UAV (viz., a camera used for object detection) is utilized to help the depth completion process generate denser 3D models. Performing both aerial depth completion and object detection tasks while fusing the data from the two sensors poses a challenge to resource efficiency. We address this challenge by proposing a novel approach to jointly execute the two tasks in a single pass. The proposed method is based on an encoder-focused multi-task learning model that exposes the two tasks to jointly learned features. We demonstrate how semantic ex",
    "link": "http://arxiv.org/abs/2304.12542",
    "context": "Title: Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion. (arXiv:2304.12542v1 [cs.CV])\nAbstract: Depth completion and object detection are two crucial tasks often used for aerial 3D mapping, path planning, and collision avoidance of Uncrewed Aerial Vehicles (UAVs). Common solutions include using measurements from a LiDAR sensor; however, the generated point cloud is often sparse and irregular and limits the system's capabilities in 3D rendering and safety-critical decision-making. To mitigate this challenge, information from other sensors on the UAV (viz., a camera used for object detection) is utilized to help the depth completion process generate denser 3D models. Performing both aerial depth completion and object detection tasks while fusing the data from the two sensors poses a challenge to resource efficiency. We address this challenge by proposing a novel approach to jointly execute the two tasks in a single pass. The proposed method is based on an encoder-focused multi-task learning model that exposes the two tasks to jointly learned features. We demonstrate how semantic ex",
    "path": "papers/23/04/2304.12542.json",
    "total_tokens": 996,
    "translated_title": "物体语义信息赋予我们所需的深度: 多任务方法解决航空深度补全问题",
    "translated_abstract": "深度补全和目标检测是航空三维建图、路径规划和无人机避障等领域中经常使用的两个关键任务。常见方法使用来自LiDAR传感器的测量数据，但生成的点云通常是稀疏和不规则的，限制了系统在三维渲染和安全决策方面的性能。为了解决这个挑战，利用无人机上其他传感器（如用于目标检测的摄像头）的信息来帮助深度补全过程生成更密集的三维模型。同时执行航空深度补全和目标检测任务并融合两个传感器的数据对资源利用效率提出了挑战。我们通过提出一种新颖的方法来解决这个挑战，该方法采用基于编码器的多任务学习模型，将两个任务暴露给共同学习的特征。我们展示了如何从目标检测中提取语义信息改进航空深度补全结果，在流行的KITTI深度补全基准上实现了最先进的性能。",
    "tldr": "该论文提出了一种利用对象检测中的语义信息来提高航空深度补全效果的方法，通过基于编码器的多任务学习模型将两个任务在一个模型中执行一次，实现了与KITTI深度补全基准的最先进性能。",
    "en_tdlr": "This paper proposes a method to improve the performance of aerial depth completion by using semantic information from object detection. The method jointly executes both tasks in a single pass using an encoder-focused multi-task learning model, achieving state-of-the-art performance on the KITTI depth completion benchmark."
}