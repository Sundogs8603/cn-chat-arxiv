{
    "title": "XAI-based Comparison of Input Representations for Audio Event Classification. (arXiv:2304.14019v1 [cs.SD])",
    "abstract": "Deep neural networks are a promising tool for Audio Event Classification. In contrast to other data like natural images, there are many sensible and non-obvious representations for audio data, which could serve as input to these models. Due to their black-box nature, the effect of different input representations has so far mostly been investigated by measuring classification performance. In this work, we leverage eXplainable AI (XAI), to understand the underlying classification strategies of models trained on different input representations. Specifically, we compare two model architectures with regard to relevant input features used for Audio Event Detection: one directly processes the signal as the raw waveform, and the other takes in its time-frequency spectrogram representation. We show how relevance heatmaps obtained via \"Siren\"{Layer-wise Relevance Propagation} uncover representation-dependent decision strategies. With these insights, we can make a well-informed decision about the",
    "link": "http://arxiv.org/abs/2304.14019",
    "context": "Title: XAI-based Comparison of Input Representations for Audio Event Classification. (arXiv:2304.14019v1 [cs.SD])\nAbstract: Deep neural networks are a promising tool for Audio Event Classification. In contrast to other data like natural images, there are many sensible and non-obvious representations for audio data, which could serve as input to these models. Due to their black-box nature, the effect of different input representations has so far mostly been investigated by measuring classification performance. In this work, we leverage eXplainable AI (XAI), to understand the underlying classification strategies of models trained on different input representations. Specifically, we compare two model architectures with regard to relevant input features used for Audio Event Detection: one directly processes the signal as the raw waveform, and the other takes in its time-frequency spectrogram representation. We show how relevance heatmaps obtained via \"Siren\"{Layer-wise Relevance Propagation} uncover representation-dependent decision strategies. With these insights, we can make a well-informed decision about the",
    "path": "papers/23/04/2304.14019.json",
    "total_tokens": 1146,
    "translated_title": "基于XAI的音频事件分类输入表示法比较",
    "translated_abstract": "深度神经网络是音频事件分类的有前途的工具。相对于自然图像等其他数据，音频数据有许多明智和非显而易见的表示法，这些表示法可以作为这些模型的输入。在本文中，我们利用可解释人工智能（XAI）来了解不同输入表示法训练的模型所使用的基本分类策略。具体而言，我们比较两种模型体系结构，以了解与音频事件检测相关的输入特征。我们展示了通过“Siren”（层面相关传递）获得的相关热图如何揭示表示法相关的决策策略。通过这些深入的洞察，我们能够作出明智的决策。",
    "tldr": "本文比较了两种不同的音频事件分类输入表示法的模型，即原始波形信号和时间-频率谱，利用可解释人工智能（XAI）揭示了不同表示法的决策策略，为正确的决策提供了深入的见解。",
    "en_tdlr": "This paper compares two models of different audio event classification input representations, namely raw waveform signals and time-frequency spectrogram representations, and uses eXplainable AI (XAI) to reveal the decision strategies of different representations, providing insights for making informed decisions."
}