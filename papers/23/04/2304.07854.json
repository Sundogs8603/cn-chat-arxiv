{
    "title": "Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation. (arXiv:2304.07854v1 [cs.CL])",
    "abstract": "Recently, significant public efforts have been directed towards developing low-cost models with capabilities akin to ChatGPT, thereby fostering the growth of open-source conversational models. However, there remains a scarcity of comprehensive and in-depth evaluations of these models' performance. In this study, we examine the influence of training data factors, including quantity, quality, and linguistic distribution, on model performance. Our analysis is grounded in several publicly accessible, high-quality instruction datasets, as well as our own Chinese multi-turn conversations. We assess various models using a evaluation set of 1,000 samples, encompassing nine real-world scenarios. Our goal is to supplement manual evaluations with quantitative analyses, offering valuable insights for the continued advancement of open-source chat models. Furthermore, to enhance the performance and training and inference efficiency of models in the Chinese domain, we extend the vocabulary of LLaMA -",
    "link": "http://arxiv.org/abs/2304.07854",
    "context": "Title: Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation. (arXiv:2304.07854v1 [cs.CL])\nAbstract: Recently, significant public efforts have been directed towards developing low-cost models with capabilities akin to ChatGPT, thereby fostering the growth of open-source conversational models. However, there remains a scarcity of comprehensive and in-depth evaluations of these models' performance. In this study, we examine the influence of training data factors, including quantity, quality, and linguistic distribution, on model performance. Our analysis is grounded in several publicly accessible, high-quality instruction datasets, as well as our own Chinese multi-turn conversations. We assess various models using a evaluation set of 1,000 samples, encompassing nine real-world scenarios. Our goal is to supplement manual evaluations with quantitative analyses, offering valuable insights for the continued advancement of open-source chat models. Furthermore, to enhance the performance and training and inference efficiency of models in the Chinese domain, we extend the vocabulary of LLaMA -",
    "path": "papers/23/04/2304.07854.json",
    "total_tokens": 857,
    "tldr": "本研究探究训练数据和评估对中文指令跟随语言模型的影响，并采用定量分析为开源会话模型提供见解和帮助。",
    "en_tdlr": "This study investigates the impact of training data and evaluation on Chinese instruction following language models. By using quantitative analysis, it provides insights and assistance for the continued development of open-source chat models in the Chinese domain."
}