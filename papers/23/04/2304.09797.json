{
    "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v1 [cs.CL])",
    "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted an extensive and comprehensive evaluation to demonstrate the effectiveness of the proposed method. Our experimental results on six benchmarks show that combining CoT and self-consistency with PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davin",
    "link": "http://arxiv.org/abs/2304.09797",
    "context": "Title: Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v1 [cs.CL])\nAbstract: The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted an extensive and comprehensive evaluation to demonstrate the effectiveness of the proposed method. Our experimental results on six benchmarks show that combining CoT and self-consistency with PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davin",
    "path": "papers/23/04/2304.09797.json",
    "total_tokens": 860,
    "tldr": "本文提出了一种新的渐进提示提示法（PHP）来提高大型语言模型的推理能力，通过使用先前生成的答案作为提示来逐步指向正确答案，能够与最先进的技术相结合以进一步提高性能。"
}