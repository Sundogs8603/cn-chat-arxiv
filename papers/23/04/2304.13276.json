{
    "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])",
    "abstract": "Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.  In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learni",
    "link": "http://arxiv.org/abs/2304.13276",
    "context": "Title: The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])\nAbstract: Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.  In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learni",
    "path": "papers/23/04/2304.13276.json",
    "total_tokens": 776,
    "translated_title": "基于Softmax回归的上下文学习和权重调整的关系",
    "translated_abstract": "大型语言模型因其在自然语言处理中的出色表现而闻名，适用于许多与人类生活或工作相关的任务。Transformer架构中的注意机制是LLMs的一个关键组件，因为它允许模型有选择地关注特定的输入部分。softmax单元作为注意机制的关键部分，规范化了注意得分。因此，LLMs在各种NLP任务中的表现很大程度上取决于softmax单元与注意机制发挥的关键作用。最近，许多工作[RTH+22，ASA+22，GTLV22，ONR+22]研究了上下文学习。",
    "tldr": "本论文讨论了大型语言模型中关键组件注意机制的 softmax 单元以及上下文学习，探究了在上下文学习中softmax单元的权重调整。",
    "en_tdlr": "This paper discusses the relationship between the key component attention mechanism with softmax unit and in-context learning in large language models, exploring the weight shifting of softmax unit in in-context learning."
}