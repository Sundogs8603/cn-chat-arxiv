{
    "title": "Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])",
    "abstract": "Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o",
    "link": "http://arxiv.org/abs/2304.05216",
    "context": "Title: Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])\nAbstract: Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o",
    "path": "papers/23/04/2304.05216.json",
    "total_tokens": 909,
    "translated_title": "面向预训练代码模型有效微调：实验研究及其拓展",
    "translated_abstract": "近年来，在许多软件测试和分析任务中，对预训练代码模型进行微调（如CodeBERT）以适应下游任务取得了巨大成功。虽然有效且流行，但微调预训练参数会导致大量计算成本。在本文中，我们进行了广泛的实验研究，探索微调期间每层预训练表示和编码的代码知识发生了什么。然后，我们基于上述发现提出了有效的微调大型预训练代码模型的替代方案。我们的实验研究表明：（1）源代码的词汇、语法和结构性质分别编码在较低、中间和较高的层中，而语义属性跨越整个模型。（2）微调过程保留了大部分代码属性。具体而言，较低和中间层捕获的基本代码属性在微调期间仍然保留。此外，我们发现...",
    "tldr": "本文研究了对预训练代码模型的微调，探索了各层预训练表示和编码的代码知识，提出了有效的微调方案。实验发现微调过程可以保留大部分代码属性，基本代码属性由较低和中间层捕获。",
    "en_tdlr": "This paper studies fine-tuning pre-trained code models and proposes an efficient alternative based on an extensive experimental study exploring the layer-wise pre-trained representations and encoded code knowledge. The study shows that fine-tuning preserves most code properties, especially those captured by lower and intermediate layers."
}