{
    "title": "PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])",
    "abstract": "Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr",
    "link": "http://arxiv.org/abs/2304.13639",
    "context": "Title: PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])\nAbstract: Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr",
    "path": "papers/23/04/2304.13639.json",
    "total_tokens": 918,
    "translated_title": "PVP: 预训练的视觉参数高效微调",
    "translated_abstract": "大规模预训练变换器在各种计算机视觉任务中取得了显著的成功。然而，由于其高计算和存储成本，在下游任务中完全微调这些模型仍然面临着极大的挑战。我们首次通过经验探究发现，大多数PETuning方法仍需要大量的下游任务训练数据才能取得良好的结果。为了克服这个问题，我们提出了PVP，利用预训练模型的统计信息，利用无监督聚类方法初始化提示模块。PVP可在各种图像分类和少样本学习基准上仅使用少量标记数据（例如每类一到两个示例）实现具有竞争力的性能。",
    "tldr": "本文提出了一种名为PVP的新方法，通过利用预训练模型的统计信息和无监督聚类方法初始化提示模块，可以在极少的标记数据（每类一到两个示例）的情况下获得具有竞争力的性能。",
    "en_tdlr": "This paper proposes a new method called PVP, which initializes prompt modules using the statistics of pre-trained models and an unsupervised clustering method. With only a small amount of labeled data (e.g., one or two examples per class), PVP achieves competitive performance on various image classification and few-shot learning benchmarks."
}