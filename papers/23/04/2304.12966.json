{
    "title": "Towards Theoretical Understanding of Inverse Reinforcement Learning. (arXiv:2304.12966v1 [cs.LG])",
    "abstract": "Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order ${\\Omega}\\Bigl( \\frac{H^3SA}{",
    "link": "http://arxiv.org/abs/2304.12966",
    "context": "Title: Towards Theoretical Understanding of Inverse Reinforcement Learning. (arXiv:2304.12966v1 [cs.LG])\nAbstract: Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order ${\\Omega}\\Bigl( \\frac{H^3SA}{",
    "path": "papers/23/04/2304.12966.json",
    "total_tokens": 978,
    "translated_title": "逆强化学习的理论研究进展",
    "translated_abstract": "逆强化学习（IRL）是一系列用于恢复一个能够解释专家代理演示的行为的奖励函数的强大算法。IRL的一个已知限制是在选择奖励函数时存在歧义，由于存在可以解释观察到的行为的多个奖励函数。最近，通过将IRL表述为估计可行奖励集的问题，即与专家行为兼容的奖励范围的问题，已经绕过了这种限制。在本文中，我们在具有生成模型的有限时间问题的IRL理论上迈出了一步。我们首先正式介绍了估计可行奖励集的问题、相应的PAC要求，并讨论了特定类别奖励的属性。接着，我们为估计可行奖励集的问题提出了第一个样本复杂度的极小极大下界，其为${\\Omega}\\Bigl( \\frac{H^3SA}{...",
    "tldr": "逆强化学习是一类用于恢复能够解释专家代理演示的行为的奖励函数的强大算法，然而，由于存在多个可解释观察行为的奖励函数歧义，采用可行奖励集的方式绕过这种限制。该论文在有限时间问题的IRL理论方面进行了探究，提出第一个样本复杂度的极小极大下界。",
    "en_tdlr": "Inverse Reinforcement Learning is a powerful algorithm for recovering a reward function that can explain the behaviour of an expert agent, but suffers from multiple rewards explaining the same behaviour. This paper makes progress towards understanding the theory of IRL in finite-horizon problems with a generative model, by introducing the problem of estimating the feasible reward set and providing the first minimax lower bound on the sample complexity for this problem."
}