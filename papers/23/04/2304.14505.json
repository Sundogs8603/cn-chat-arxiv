{
    "title": "Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])",
    "abstract": "A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method ",
    "link": "http://arxiv.org/abs/2304.14505",
    "context": "Title: Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])\nAbstract: A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method ",
    "path": "papers/23/04/2304.14505.json",
    "total_tokens": 848,
    "translated_title": "基于Transformer的可解释多模态数据融合用于皮肤病分类",
    "translated_abstract": "当今许多深度学习（DL）研究主要集中在提高定量指标方面，而忽略了其他因素。在人类中心的应用领域，如皮肤病分类在皮肤科中，仍处于其初级阶段的DL驱动的临床决策支持系统，由于其决策过程的透明度有限。此外，缺乏能够解释训练的DL算法行为的程序几乎没有得到临床医师的信任。为诊断皮肤病变，皮肤科医生依靠疾病的视觉评估和患者病史收集的数据。处理多模态数据的数据驱动算法受限于卷积结构所需的特征级和决策级融合程序的分离。为解决这个问题，我们通过基于Transformer的架构的注意机制实现单阶段多模态数据融合，以帮助诊断皮肤疾病。",
    "tldr": "本文提出了一种基于Transformer的可解释多模态数据融合算法，用于帮助皮肤疾病的诊断。",
    "en_tdlr": "This paper proposes a transformer-based interpretable multi-modal data fusion algorithm for aiding in the diagnosis of skin diseases."
}