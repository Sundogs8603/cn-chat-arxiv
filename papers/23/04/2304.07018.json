{
    "title": "DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution. (arXiv:2304.07018v1 [cs.CV])",
    "abstract": "Efficient deep learning-based approaches have achieved remarkable performance in single image super-resolution. However, recent studies on efficient super-resolution have mainly focused on reducing the number of parameters and floating-point operations through various network designs. Although these methods can decrease the number of parameters and floating-point operations, they may not necessarily reduce actual running time. To address this issue, we propose a novel multi-stage lightweight network boosting method, which can enable lightweight networks to achieve outstanding performance. Specifically, we leverage enhanced high-resolution output as additional supervision to improve the learning ability of lightweight student networks. Upon convergence of the student network, we further simplify our network structure to a more lightweight level using reparameterization techniques and iterative network pruning. Meanwhile, we adopt an effective lightweight network training strategy that c",
    "link": "http://arxiv.org/abs/2304.07018",
    "context": "Title: DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution. (arXiv:2304.07018v1 [cs.CV])\nAbstract: Efficient deep learning-based approaches have achieved remarkable performance in single image super-resolution. However, recent studies on efficient super-resolution have mainly focused on reducing the number of parameters and floating-point operations through various network designs. Although these methods can decrease the number of parameters and floating-point operations, they may not necessarily reduce actual running time. To address this issue, we propose a novel multi-stage lightweight network boosting method, which can enable lightweight networks to achieve outstanding performance. Specifically, we leverage enhanced high-resolution output as additional supervision to improve the learning ability of lightweight student networks. Upon convergence of the student network, we further simplify our network structure to a more lightweight level using reparameterization techniques and iterative network pruning. Meanwhile, we adopt an effective lightweight network training strategy that c",
    "path": "papers/23/04/2304.07018.json",
    "total_tokens": 942,
    "translated_title": "DIPNet：图像超分辨率的效率蒸馏与迭代修剪",
    "translated_abstract": "现有基于深度学习的高效单幅图像超分辨率方法在性能上取得了显著的成果。然而，近期关于高效超分辨率的研究主要集中在通过各种网络设计减少参数数量和浮点运算。虽然这些方法可以降低参数数量和浮点运算，但并不一定缩短实际运行时间。为了解决这个问题，我们提出了一种新的多阶段轻量级网络增强方法，可以使轻量级网络实现出色的性能。具体地，我们利用增强的高分辨率输出作为额外的监督，提高轻量级学生网络的学习能力。在学生网络收敛后，我们进一步利用重新参数化技术和迭代网络修剪简化网络结构到更轻量级的水平。同时，我们采用有效的轻量级网络训练策略，使我们的方法不仅可以保证网络效率，同时也能在实现高质量超分辨率方面取得优异的性能。",
    "tldr": "该论文提出了一种名为DIPNet的新型多阶段轻量级网络增强方法，通过增强的高分辨率输出作为额外监督，提高轻量级学生网络的学习能力，并进一步采用重新参数化技术和迭代网络修剪等简化网络结构的方法，以实现高质量超分辨率的有效网络训练。"
}