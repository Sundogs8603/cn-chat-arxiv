{
    "title": "Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])",
    "abstract": "Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art results in various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multiand cross-lingual tasks ",
    "link": "http://arxiv.org/abs/2304.14796",
    "context": "Title: Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])\nAbstract: Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art results in various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multiand cross-lingual tasks ",
    "path": "papers/23/04/2304.14796.json",
    "total_tokens": 920,
    "translated_title": "最好的多语言文档嵌入是否仅基于句子嵌入？",
    "translated_abstract": "在现代自然语言处理中，文本数据的密集向量表征至关重要。从原始文本估计的词嵌入和句子嵌入是在多种需要语义理解的任务中实现最新成果的关键。然而，由于计算需求和缺乏适当的数据，获取文档级别的嵌入是具有挑战性的。相反，大多数方法退而使用基于句子表示的文档嵌入计算。尽管存在一些用于完全编码文档的体系结构和模型，但它们通常仅限于英语和其他几种高资源语言。在本文中，我们基于预训练的多语言模型LASER、LaBSE和Sentence BERT，系统比较从句子中产生文档级表示的方法。我们比较输入令牌数截断、句子平均以及一些简单的窗口方法，在三个多语言和跨语言任务中进行比较。",
    "tldr": "本文系统比较了从句子级别嵌入中产生文档级嵌入的方法，基于预训练的多语言模型LASER、LaBSE和Sentence BERT。我们着重比较了输入令牌数截断、句子平均以及一些简单的窗口方法，对三个多语言和跨语言任务表现进行了比较。",
    "en_tdlr": "This paper provides a systematic comparison of methods to produce document-level embeddings based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models, focusing on input token truncation, sentence averaging, and simple windowing. Results are evaluated on three multi and cross-lingual tasks."
}