{
    "title": "Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])",
    "abstract": "Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside ",
    "link": "http://arxiv.org/abs/2304.08845",
    "context": "Title: Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])\nAbstract: Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside ",
    "path": "papers/23/04/2304.08845.json",
    "total_tokens": 939,
    "translated_title": "可行性策略迭代",
    "translated_abstract": "安全强化学习旨在在安全约束下解决最优控制问题。现有的 $\\textit{直接}$ 安全强化学习方法会在整个学习过程中一直使用原始约束。它们或者缺乏策略迭代期间的理论保证，或者遭遇不可行性问题。为了解决这个问题，我们提出了一个叫做可行性策略迭代（FPI）的 $\\textit{间接}$ 安全强化学习方法，它使用最后一个策略的可行域来迭代地限制当前策略。可行域由一个叫做约束衰减函数（CDF）的可行性函数表示。FPI 的核心是一个叫做可行性策略改进的区域性策略更新规则，它在可行域内最大化回报，在可行域外最小化 CDF。这个更新规则总是可行的，并确保可行域单调地扩展，状态值函数单调地增长。",
    "tldr": "可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF).",
    "en_tdlr": "Feasible Policy Iteration (FPI) is an indirect safe reinforcement learning method, which iteratively constrains the current policy using the feasible region of the last policy. The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the feasibility function called constraint decay function (CDF) inside the feasible region and minimizes the CDF outside the feasible region."
}