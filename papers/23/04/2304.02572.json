{
    "title": "Optimism Based Exploration in Large-Scale Recommender Systems. (arXiv:2304.02572v1 [cs.IR])",
    "abstract": "Bandit learning algorithms have been an increasingly popular design choice for recommender systems. Despite the strong interest in bandit learning from the community, there remains multiple bottlenecks that prevent many bandit learning approaches from productionalization. Two of the most important bottlenecks are scaling to multi-task and A/B testing. Classic bandit algorithms, especially those leveraging contextual information, often requires reward for uncertainty estimation, which hinders their adoptions in multi-task recommender systems. Moreover, different from supervised learning algorithms, bandit learning algorithms emphasize greatly on the data collection process through their explorative nature. Such explorative behavior induces unfair evaluation for bandit learning agents in a classic A/B test setting. In this work, we present a novel design of production bandit learning life-cycle for recommender systems, along with a novel set of metrics to measure their efficiency in user",
    "link": "http://arxiv.org/abs/2304.02572",
    "context": "Title: Optimism Based Exploration in Large-Scale Recommender Systems. (arXiv:2304.02572v1 [cs.IR])\nAbstract: Bandit learning algorithms have been an increasingly popular design choice for recommender systems. Despite the strong interest in bandit learning from the community, there remains multiple bottlenecks that prevent many bandit learning approaches from productionalization. Two of the most important bottlenecks are scaling to multi-task and A/B testing. Classic bandit algorithms, especially those leveraging contextual information, often requires reward for uncertainty estimation, which hinders their adoptions in multi-task recommender systems. Moreover, different from supervised learning algorithms, bandit learning algorithms emphasize greatly on the data collection process through their explorative nature. Such explorative behavior induces unfair evaluation for bandit learning agents in a classic A/B test setting. In this work, we present a novel design of production bandit learning life-cycle for recommender systems, along with a novel set of metrics to measure their efficiency in user",
    "path": "papers/23/04/2304.02572.json",
    "total_tokens": 842,
    "tldr": "本文探讨了基于乐观主义探索的优化在推荐系统中的应用，解决了多任务和A/B测试等瓶颈。",
    "en_tdlr": "This paper explores the application of optimism-based exploration for optimization in recommender systems, addressing bottlenecks such as multi-tasking and A/B testing."
}