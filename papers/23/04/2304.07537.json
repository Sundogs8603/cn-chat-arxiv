{
    "title": "Gradient-less Federated Gradient Boosting Trees with Learnable Learning Rates. (arXiv:2304.07537v1 [cs.LG])",
    "abstract": "The privacy-sensitive nature of decentralized datasets and the robustness of eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train XGBoost in the context of federated learning (FL). Existing works on federated XGBoost in the horizontal setting rely on the sharing of gradients, which induce per-node level communication frequency and serious privacy concerns. To alleviate these problems, we develop an innovative framework for horizontal federated XGBoost which does not depend on the sharing of gradients and simultaneously boosts privacy and communication efficiency by making the learning rates of the aggregated tree ensembles learnable. We conduct extensive evaluations on various classification and regression datasets, showing our approach achieves performance comparable to the state-of-the-art method and effectively improves communication efficiency by lowering both communication rounds and communication overhead by factors ranging from 25x to 700x.",
    "link": "http://arxiv.org/abs/2304.07537",
    "context": "Title: Gradient-less Federated Gradient Boosting Trees with Learnable Learning Rates. (arXiv:2304.07537v1 [cs.LG])\nAbstract: The privacy-sensitive nature of decentralized datasets and the robustness of eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train XGBoost in the context of federated learning (FL). Existing works on federated XGBoost in the horizontal setting rely on the sharing of gradients, which induce per-node level communication frequency and serious privacy concerns. To alleviate these problems, we develop an innovative framework for horizontal federated XGBoost which does not depend on the sharing of gradients and simultaneously boosts privacy and communication efficiency by making the learning rates of the aggregated tree ensembles learnable. We conduct extensive evaluations on various classification and regression datasets, showing our approach achieves performance comparable to the state-of-the-art method and effectively improves communication efficiency by lowering both communication rounds and communication overhead by factors ranging from 25x to 700x.",
    "path": "papers/23/04/2304.07537.json",
    "total_tokens": 863,
    "translated_title": "可学习学习率的无梯度联邦梯度提升决策树",
    "tldr": "该论文提出了一种无梯度的联邦梯度提升框架，通过可学习的学习率，提高巨大数据集训练的通信效率，同时保护数据隐私。",
    "en_tdlr": "This paper proposes a gradient-less federated gradient boosting framework with learnable learning rates, which improves communication efficiency and protects data privacy by avoiding gradient sharing in horizontal federated learning. The performance is comparable to state-of-the-art methods with reduced communication rounds and overhead."
}