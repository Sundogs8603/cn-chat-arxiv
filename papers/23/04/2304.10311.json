{
    "title": "Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining. (arXiv:2304.10311v1 [cs.MM])",
    "abstract": "Investments in movie production are associated with a high level of risk as movie revenues have long-tailed and bimodal distributions. Accurate prediction of box-office revenue may mitigate the uncertainty and encourage investment. However, learning effective representations for actors, directors, and user-generated content-related keywords remains a challenging open problem. In this work, we investigate the effects of self-supervised pretraining and propose visual grounding of content keywords in objects from movie posters as a pertaining objective. Experiments on a large dataset of 35,794 movies demonstrate significant benefits of self-supervised training and visual grounding. In particular, visual grounding pretraining substantially improves learning on movies with content keywords and achieves 14.5% relative performance gains compared to a finetuned BERT model with identical architecture.",
    "link": "http://arxiv.org/abs/2304.10311",
    "context": "Title: Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining. (arXiv:2304.10311v1 [cs.MM])\nAbstract: Investments in movie production are associated with a high level of risk as movie revenues have long-tailed and bimodal distributions. Accurate prediction of box-office revenue may mitigate the uncertainty and encourage investment. However, learning effective representations for actors, directors, and user-generated content-related keywords remains a challenging open problem. In this work, we investigate the effects of self-supervised pretraining and propose visual grounding of content keywords in objects from movie posters as a pertaining objective. Experiments on a large dataset of 35,794 movies demonstrate significant benefits of self-supervised training and visual grounding. In particular, visual grounding pretraining substantially improves learning on movies with content keywords and achieves 14.5% relative performance gains compared to a finetuned BERT model with identical architecture.",
    "path": "papers/23/04/2304.10311.json",
    "total_tokens": 827,
    "translated_title": "采用自监督与视觉相关的预训练方法进行电影票房预测",
    "translated_abstract": "电影投资风险较高，因为电影票房收入呈现长尾和双峰分布。准确预测电影票房收入可以减轻不确定性并鼓励投资。然而，学习对演员、导演和用户生成内容相关关键词的有效表示仍然是一个具有挑战性的开放问题。在本研究中，我们调查了自监督预训练的效果，并提出了从电影海报中的对象开始实现视觉相关关键词作为预训练目标。在一个包含35,794部电影的大数据集上的实验表明，自监督训练和视觉相关关键词的预训练都会带来显著的好处。特别是，在具有内容关键词的电影上进行视觉相关的预训练，相对于具有相同架构的微调BERT模型，可以实现14.5%的性能提高。",
    "tldr": "本研究采用自监督和视觉相关的预训练方法对电影票房进行预测，对具有内容关键词的电影表现出了显著的性能提高。",
    "en_tdlr": "This study proposes self-supervised and visually grounded pretraining methods for movie box office prediction, demonstrating significant performance gains, particularly in movies with content keywords."
}