{
    "title": "Adaptive Consensus Optimization Method for GANs. (arXiv:2304.10317v1 [cs.LG])",
    "abstract": "We propose a second order gradient based method with ADAM and RMSprop for the training of generative adversarial networks. The proposed method is fastest to obtain similar accuracy when compared to prominent second order methods. Unlike state-of-the-art recent methods, it does not require solving a linear system, or it does not require additional mixed second derivative terms. We derive the fixed point iteration corresponding to proposed method, and show that the proposed method is convergent. The proposed method produces better or comparable inception scores, and comparable quality of images compared to other recently proposed state-of-the-art second order methods. Compared to first order methods such as ADAM, it produces significantly better inception scores. The proposed method is compared and validated on popular datasets such as FFHQ, LSUN, CIFAR10, MNIST, and Fashion MNIST for image generation tasks\\footnote{Accepted in IJCNN 2023}. Codes: \\url{https://github.com/misterpawan/acom",
    "link": "http://arxiv.org/abs/2304.10317",
    "context": "Title: Adaptive Consensus Optimization Method for GANs. (arXiv:2304.10317v1 [cs.LG])\nAbstract: We propose a second order gradient based method with ADAM and RMSprop for the training of generative adversarial networks. The proposed method is fastest to obtain similar accuracy when compared to prominent second order methods. Unlike state-of-the-art recent methods, it does not require solving a linear system, or it does not require additional mixed second derivative terms. We derive the fixed point iteration corresponding to proposed method, and show that the proposed method is convergent. The proposed method produces better or comparable inception scores, and comparable quality of images compared to other recently proposed state-of-the-art second order methods. Compared to first order methods such as ADAM, it produces significantly better inception scores. The proposed method is compared and validated on popular datasets such as FFHQ, LSUN, CIFAR10, MNIST, and Fashion MNIST for image generation tasks\\footnote{Accepted in IJCNN 2023}. Codes: \\url{https://github.com/misterpawan/acom",
    "path": "papers/23/04/2304.10317.json",
    "total_tokens": 978,
    "translated_title": "GAN的自适应共识优化方法",
    "translated_abstract": "本文提出了一种基于ADAM和RMSprop的二阶梯度优化方法，用于训练生成对抗网络。该方法在获得相似精度时比其他著名的二阶方法更快。与最先进的方法不同，它不需要解决线性系统，也不需要额外的混合二阶导数项。我们推导出了对应于所提出的方法的固定点迭代，并证明了所提出的方法的收敛性。与其他最新提出的最先进的二阶方法相比，所提出的方法产生了更好或可比的Inception分数，以及与其他方法相当的图像质量。与一阶方法（如ADAM）相比，它产生了显著更好的Inception分数。该方法在流行的图像生成任务数据集，如FFHQ、LSUN、CIFAR10、MNIST和Fashion MNIST上进行了比较和验证\\footnote{已被IJCNN 2023接受}。代码：\\url{https://github.com/misterpawan/acom}",
    "tldr": "本文提出一种基于ADAM和RMSprop的二阶梯度优化方法，用于训练GAN，在不需要解决线性系统或混合二阶导数项的情况下取得与其他方法相比更快的相似精度，产生了更好或可比的Inception分数和图像质量，并与一阶方法相比产生了显著更好的Inception分数。",
    "en_tdlr": "This paper proposes a second-order gradient-based optimization method with ADAM and RMSprop for training GANs, which achieves similar accuracy faster without requiring the solution of a linear system or additional mixed second derivative terms. This method produces better or comparable inception scores and image quality compared to other state-of-the-art second-order methods, and significantly better inception scores than first-order methods like ADAM. It is validated on popular datasets and has been accepted in IJCNN 2023."
}