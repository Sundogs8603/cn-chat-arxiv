{
    "title": "Explainable AI And Visual Reasoning: Insights From Radiology. (arXiv:2304.03318v1 [cs.HC])",
    "abstract": "Why do explainable AI (XAI) explanations in radiology, despite their promise of transparency, still fail to gain human trust? Current XAI approaches provide justification for predictions, however, these do not meet practitioners' needs. These XAI explanations lack intuitive coverage of the evidentiary basis for a given classification, posing a significant barrier to adoption. We posit that XAI explanations that mirror human processes of reasoning and justification with evidence may be more useful and trustworthy than traditional visual explanations like heat maps. Using a radiology case study, we demonstrate how radiology practitioners get other practitioners to see a diagnostic conclusion's validity. Machine-learned classifications lack this evidentiary grounding and consequently fail to elicit trust and adoption by potential users. Insights from this study may generalize to guiding principles for human-centered explanation design based on human reasoning and justification of evidence",
    "link": "http://arxiv.org/abs/2304.03318",
    "context": "Title: Explainable AI And Visual Reasoning: Insights From Radiology. (arXiv:2304.03318v1 [cs.HC])\nAbstract: Why do explainable AI (XAI) explanations in radiology, despite their promise of transparency, still fail to gain human trust? Current XAI approaches provide justification for predictions, however, these do not meet practitioners' needs. These XAI explanations lack intuitive coverage of the evidentiary basis for a given classification, posing a significant barrier to adoption. We posit that XAI explanations that mirror human processes of reasoning and justification with evidence may be more useful and trustworthy than traditional visual explanations like heat maps. Using a radiology case study, we demonstrate how radiology practitioners get other practitioners to see a diagnostic conclusion's validity. Machine-learned classifications lack this evidentiary grounding and consequently fail to elicit trust and adoption by potential users. Insights from this study may generalize to guiding principles for human-centered explanation design based on human reasoning and justification of evidence",
    "path": "papers/23/04/2304.03318.json",
    "total_tokens": 862,
    "translated_title": "可解释人工智能和视觉推理：来自放射学的见解",
    "translated_abstract": "为什么可解释人工智能（XAI）在放射学中的解释，尽管承诺透明度，仍然无法获得人类的信任？当前的XAI方法提供了有关预测的证明，但这些方法不能满足从业者的需求。这些XAI解释缺乏直观的证据基础覆盖，这是采用的一个重要障碍。我们认为，与传统的热图等视觉解释不同，模仿人类推理和证明的XAI解释可能比较有用和可靠。通过放射学案例研究，我们演示了放射学从业者如何让其他从业者看到诊断结论的有效性。机器学习的分类缺乏这种证据基础，因此未能引起潜在用户的信任和采用。本研究的洞见可能适用于基于人类推理和证明的面向人类解释设计的指导原则。",
    "tldr": "本研究探讨了当前可解释人工智能在放射学应用中的难点，提出了一种基于人类推理和证明的解释设计方法，并通过放射学案例，证明了这种方法可行。",
    "en_tdlr": "This study explores challenges in applying explainable AI in radiology and proposes an explanation design approach based on human reasoning and justification. Through a radiology case, the study demonstrates the feasibility of this approach."
}