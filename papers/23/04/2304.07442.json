{
    "title": "Learning To Optimize Quantum Neural Network Without Gradients. (arXiv:2304.07442v1 [quant-ph])",
    "abstract": "Quantum Machine Learning is an emerging sub-field in machine learning where one of the goals is to perform pattern recognition tasks by encoding data into quantum states. This extension from classical to quantum domain has been made possible due to the development of hybrid quantum-classical algorithms that allow a parameterized quantum circuit to be optimized using gradient based algorithms that run on a classical computer. The similarities in training of these hybrid algorithms and classical neural networks has further led to the development of Quantum Neural Networks (QNNs). However, in the current training regime for QNNs, the gradients w.r.t objective function have to be computed on the quantum device. This computation is highly non-scalable and is affected by hardware and sampling noise present in the current generation of quantum hardware. In this paper, we propose a training algorithm that does not rely on gradient information. Specifically, we introduce a novel meta-optimizati",
    "link": "http://arxiv.org/abs/2304.07442",
    "context": "Title: Learning To Optimize Quantum Neural Network Without Gradients. (arXiv:2304.07442v1 [quant-ph])\nAbstract: Quantum Machine Learning is an emerging sub-field in machine learning where one of the goals is to perform pattern recognition tasks by encoding data into quantum states. This extension from classical to quantum domain has been made possible due to the development of hybrid quantum-classical algorithms that allow a parameterized quantum circuit to be optimized using gradient based algorithms that run on a classical computer. The similarities in training of these hybrid algorithms and classical neural networks has further led to the development of Quantum Neural Networks (QNNs). However, in the current training regime for QNNs, the gradients w.r.t objective function have to be computed on the quantum device. This computation is highly non-scalable and is affected by hardware and sampling noise present in the current generation of quantum hardware. In this paper, we propose a training algorithm that does not rely on gradient information. Specifically, we introduce a novel meta-optimizati",
    "path": "papers/23/04/2304.07442.json",
    "total_tokens": 1112,
    "translated_title": "无需梯度的量子神经网络优化算法",
    "translated_abstract": "量子机器学习是机器学习中的新兴子领域之一，其目标之一是通过将数据编码成量子态来执行模式识别任务。从经典到量子的扩展得以实现是由于混合量子-经典算法的发展，该算法允许使用在经典计算机上运行的基于梯度的算法来优化参数化量子电路。这些混合算法的训练与经典神经网络的相似性进一步促进了量子神经网络（QNN）的发展。然而，在当前的QNN训练过程中，必须在量子设备上计算相对目标函数的梯度。这种计算非常难以扩展，并受到当前量子硬件中存在的硬件和采样噪声的影响。本文提出了一种无需梯度信息的训练算法。具体地，我们介绍了一种新颖的元优化算法Meta-Quantum Optimization（MQO)，可用于训练QNNs。我们通过成功训练二元分类和生成建模任务的QNNs，展示了我们方法的可行性。我们的结果表明，MQO可以成功优化QNNs，并且是训练QNNs的一种有前途的方法。",
    "tldr": "本文提出了一种量子神经网络的无梯度训练算法，该算法名为Meta-Quantum Optimization (MQO)，相比当前依赖于量子设备计算梯度的算法，具有更高的扩展性和稳定性。作者在二元分类和生成建模任务中成功地应用了MQO算法来优化QNNs，为QNNs的训练提供了一种有效的解决方案。",
    "en_tdlr": "This paper proposes a gradient-free training algorithm, named Meta-Quantum Optimization (MQO), for quantum neural networks (QNNs). Compared with the current gradient-based methods relying on quantum devices, MQO exhibits higher scalability and stability. The authors demonstrate the feasibility of MQO by successfully training QNNs for binary classification and generative modeling tasks, providing an effective solution for training QNNs."
}