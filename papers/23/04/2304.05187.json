{
    "title": "Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])",
    "abstract": "The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im",
    "link": "http://arxiv.org/abs/2304.05187",
    "context": "Title: Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])\nAbstract: The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im",
    "path": "papers/23/04/2304.05187.json",
    "total_tokens": 819,
    "translated_title": "自动梯度下降：无超参数的深度学习",
    "translated_abstract": "本文提出了一种新的方法来派生特定于神经网络结构的优化算法，实现了无超参数的一阶优化器，称为“自动梯度下降”。该方法利用神经体系结构显式地定义网络结构参数来优化深度全连接网络和卷积网络，证明了在标准基准测试数据集上与手动调整优化器效果相当。该算法扩展了镜像下降方法以处理非凸性复合目标函数。",
    "tldr": "本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。",
    "en_tdlr": "This paper proposes a novel approach to derive optimization algorithms that utilize explicit neural architecture information to achieve automatic gradient descent, a first-order optimizer without manual hyperparameter tuning. The method demonstrates comparable performance to manually tuned optimizers on standard benchmark datasets for both fully connected and convolutional networks."
}