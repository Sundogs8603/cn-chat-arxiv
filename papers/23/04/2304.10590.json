{
    "title": "A Review of Symbolic, Subsymbolic and Hybrid Methods for Sequential Decision Making. (arXiv:2304.10590v1 [cs.AI])",
    "abstract": "The field of Sequential Decision Making (SDM) provides tools for solving Sequential Decision Processes (SDPs), where an agent must make a series of decisions in order to complete a task or achieve a goal. Historically, two competing SDM paradigms have view for supremacy. Automated Planning (AP) proposes to solve SDPs by performing a reasoning process over a model of the world, often represented symbolically. Conversely, Reinforcement Learning (RL) proposes to learn the solution of the SDP from data, without a world model, and represent the learned knowledge subsymbolically. In the spirit of reconciliation, we provide a review of symbolic, subsymbolic and hybrid methods for SDM. We cover both methods for solving SDPs (e.g., AP, RL and techniques that learn to plan) and for learning aspects of their structure (e.g., world models, state invariants and landmarks). To the best of our knowledge, no other review in the field provides the same scope. As an additional contribution, we discuss w",
    "link": "http://arxiv.org/abs/2304.10590",
    "context": "Title: A Review of Symbolic, Subsymbolic and Hybrid Methods for Sequential Decision Making. (arXiv:2304.10590v1 [cs.AI])\nAbstract: The field of Sequential Decision Making (SDM) provides tools for solving Sequential Decision Processes (SDPs), where an agent must make a series of decisions in order to complete a task or achieve a goal. Historically, two competing SDM paradigms have view for supremacy. Automated Planning (AP) proposes to solve SDPs by performing a reasoning process over a model of the world, often represented symbolically. Conversely, Reinforcement Learning (RL) proposes to learn the solution of the SDP from data, without a world model, and represent the learned knowledge subsymbolically. In the spirit of reconciliation, we provide a review of symbolic, subsymbolic and hybrid methods for SDM. We cover both methods for solving SDPs (e.g., AP, RL and techniques that learn to plan) and for learning aspects of their structure (e.g., world models, state invariants and landmarks). To the best of our knowledge, no other review in the field provides the same scope. As an additional contribution, we discuss w",
    "path": "papers/23/04/2304.10590.json",
    "total_tokens": 1087,
    "translated_title": "顺序决策制定的符号、亚符号和混合方法综述",
    "translated_abstract": "顺序决策制定（SDM）领域提供了解决顺序决策过程（SDP）的工具，其中智能体必须做出一系列决策以完成任务或实现目标。历史上，两种竞争的SDM范例主导了该领域。自动化规划（AP）提出通过对世界模型的推理过程解决SDP，通常使用符号表示。相反，强化学习（RL）则提出从数据中学习SDP的解决方案，不需要世界模型，并以亚符号形式表示学习到的知识。本综述在协调两种方法的基础上，对SDM的符号、亚符号和混合方法进行了综述。我们涵盖了解决SDP的方法（例如AP、RL和学习规划的技术）以及学习其结构的方面（例如世界模型、状态不变量和地标）。据我们所知，该领域中没有其他综述提供相同的范围。作为额外的贡献，我们还讨论了各种方法在可扩展性方面的挑战，例如如何将方法扩展到更大、更复杂的领域中。",
    "tldr": "本文综述了顺序决策制定的符号、亚符号和混合方法，旨在解决顺序决策过程（SDP）中的问题。无论是基于自动化规划（AP）还是强化学习（RL），都涵盖了解决SDP的方法和学习其结构的方面。对于可扩展性方面的挑战，也进行了讨论。",
    "en_tdlr": "This paper reviews symbolic, subsymbolic and hybrid methods for sequential decision making (SDM), which aim to solve problems in sequential decision processes (SDPs). It covers methods for solving SDPs based on automated planning (AP) and reinforcement learning (RL), as well as learning aspects of their structure. The paper also discusses the challenges of scalability in each method."
}