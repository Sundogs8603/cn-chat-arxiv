{
    "title": "Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])",
    "abstract": "Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially",
    "link": "http://arxiv.org/abs/2304.12653",
    "context": "Title: Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])\nAbstract: Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially",
    "path": "papers/23/04/2304.12653.json",
    "total_tokens": 1013,
    "translated_title": "基于图注意力的部分可观察均场多智能体强化学习",
    "translated_abstract": "传统的多智能体强化学习算法难以在大规模多智能体环境中应用。最近引入的均场理论提高了多智能体强化学习的可扩展性。本文考虑部分可观察的多智能体强化学习，其中每个智能体只能观察到固定范围内的其他智能体。这种部分可观察性影响了智能体评估周围智能体行动质量的能力。本文着重于开发一种从局部观测中获取更有效信息以选择更有效行动的方法。在这个领域的以前工作使用概率分布或加权均场来更新邻居智能体平均行动，但它没有充分考虑周围邻居的特征信息，导致了局部最优。本文提出了一种新的多智能体强化学习算法，基于图注意力的部分可观察均场多智能体强化学习，它使用图注意力来捕获周围邻居智能体的特征信息。我们的方法可以提高大规模多智能体环境中部分可观察MARL的性能。",
    "tldr": "本文提出了一种新的基于图注意力的部分可观察均场多智能体强化学习算法，使用图注意力来捕获周围邻居智能体的特征信息，可以提高大规模多智能体环境中部分可观察MARL的性能。",
    "en_tdlr": "This paper proposes a novel multi-agent reinforcement learning algorithm based on graph-attention for partially observable mean field multi-agent reinforcement learning, which captures feature information of surrounding agents and improves the performance in large-scale multi-agent environments."
}