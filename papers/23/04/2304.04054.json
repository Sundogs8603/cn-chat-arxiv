{
    "title": "tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])",
    "abstract": "The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade",
    "link": "http://arxiv.org/abs/2304.04054",
    "context": "Title: tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])\nAbstract: The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade",
    "path": "papers/23/04/2304.04054.json",
    "total_tokens": 1098,
    "translated_title": "tmn在SemEval-2023任务9中的应用：使用XLM-T、Google翻译和集成学习进行多语言推特亲密度检测",
    "translated_abstract": "本文介绍了一种基于transformer的系统，针对SemEval-2023任务9：多语言推特亲密度分析进行设计。任务的目的是预测一系列推特的亲密度，范围从1（完全不亲密）到5（非常亲密）。比赛的官方训练集包含六种语言的推特（英语、西班牙语、意大利语、葡萄牙语、法语和中文）。测试集包括六种给定的语言以及外部数据，其中包括训练集中未出现的四种语言（印地语、阿拉伯语、荷兰语和韩语）。我们提出了一种基于XLM-T的解决方案，即适用于Twitter领域的多语种RoBERTa模型的集成。为了提高对未见语言的性能表现，我们对每条推特进行了英文翻译的补充。我们探究了将翻译数据应用于微调中看到的语言与未看到的语言的transformer模型的有效性，并估计使用翻译数据的策略。我们的解决方案在50个团队中排名第4，并实现了0.5688的宏平均F1分数。",
    "tldr": "本文介绍了对于SemEval-2023的任务9，提出了一种基于transformer的系统，使用了集成学习，在多语言推特亲密度检测中排名第4，达到了0.5688的宏平均F1分数。为了提高对未见语言的性能表现，每个推特都进行了英文翻译的补充。",
    "en_tdlr": "This paper presents a transformer-based system for the SemEval-2023 Task 9, using an ensemble of XLM-T and Google Translate to detect tweet intimacy in multiple languages. Ranking 4th among 50 teams with a macro-average F1 score of 0.5688, the system supplemented each tweet with its English translation for improved performance on unseen languages."
}