{
    "title": "Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects. (arXiv:2304.03081v1 [cs.LG])",
    "abstract": "In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results",
    "link": "http://arxiv.org/abs/2304.03081",
    "context": "Title: Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects. (arXiv:2304.03081v1 [cs.LG])\nAbstract: In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results",
    "path": "papers/23/04/2304.03081.json",
    "total_tokens": 882,
    "translated_title": "通过学习不良轨迹的时间模式和防止负面副作用实现安全MDP规划",
    "translated_abstract": "在安全MDP规划中，基于当前状态和动作的代价函数通常用于指定安全方面，但现实世界中使用的状态表示通常缺乏足够的准确度来指定这样的安全约束条件，基于不完整模型工作常常会产生意外的负面副作用（NSEs），为了解决这些挑战，我们首先将安全信号与状态-动作轨迹相关联（而不仅仅是状态-动作即时）使我们的安全模型具有高度的通用性。我们还假设为不同的轨迹提供了分类安全标签，而不是更难由问题设计者指定的数值代价函数。然后，我们采用监督学习模型来学习这样的非马尔科夫安全模式。其次，我们开发了一种拉格朗日乘数方法，将安全模型和基础MDP模型合并成一个计算图，以促进代理学习安全行为。最后，我们的实证结果...",
    "tldr": "通过学习不良轨迹的时间模式和防止负面副作用实现安全MDP规划"
}