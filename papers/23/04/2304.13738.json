{
    "title": "Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])",
    "abstract": "In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc",
    "link": "http://arxiv.org/abs/2304.13738",
    "context": "Title: Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])\nAbstract: In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc",
    "path": "papers/23/04/2304.13738.json",
    "total_tokens": 974,
    "translated_title": "可扩展的分布式AI框架：利用云计算提高深度学习性能和效率",
    "translated_abstract": "近年来，将人工智能（AI）和云计算相结合已成为解决AI应用程序不断增长的计算需求的有效方法。本文介绍了一项关于利用云计算提高深度学习性能和效率的可扩展的分布式AI框架的全面研究。首先，我们概述了常用的AI框架和云服务，强调了它们各自的优缺点。接着，我们深入探讨了基于云的AI系统中数据存储和管理的关键方面，包括数据预处理、特征工程、隐私和安全。接下来，我们探讨了AI模型的并行和分布式训练技术，重点讨论了模型的分区、通信策略和基于云的训练架构。在随后的章节中，我们讨论了云中AI工作负载的优化策略，涵盖了负载平衡、资源分配、自动缩放和性能基准测试等方面。",
    "tldr": "本文介绍了可扩展的分布式AI框架，利用云计算提高深度学习性能和效率。通过概述常用的AI框架和云服务、探讨基于云的AI系统的关键方面和讨论云中AI工作负载的优化策略，提供了解决AI应用程序不断增长的计算需求的有效方法。",
    "en_tdlr": "This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. By providing an overview of popular AI frameworks and cloud services, delving into the critical aspects of data storage and management in cloud-based AI systems and discussing optimization strategies for AI workloads in the cloud, it offers an effective solution to the growing computational demands of AI applications."
}