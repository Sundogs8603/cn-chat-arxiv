{
    "title": "LipsFormer: Introducing Lipschitz Continuity to Vision Transformers. (arXiv:2304.09856v1 [cs.CV])",
    "abstract": "We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts: CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such ",
    "link": "http://arxiv.org/abs/2304.09856",
    "context": "Title: LipsFormer: Introducing Lipschitz Continuity to Vision Transformers. (arXiv:2304.09856v1 [cs.CV])\nAbstract: We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts: CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such ",
    "path": "papers/23/04/2304.09856.json",
    "total_tokens": 890,
    "translated_title": "LipsFormer：向Vision Transformer引入Lipschitz连续性",
    "translated_abstract": "我们提出了一个名为LipsFormer的Lipschitz连续Transformer，旨在从理论上和实践上追求Transformer-based模型的训练稳定性。相比于以前通过学习率warmup、层归一化、attention公式、权重初始化等方法解决训练不稳定性的实际技巧，我们证明了Lipschitz连续性是确保训练稳定性的更重要的属性。在LipsFormer中，我们用Lipschitz连续的中心归一化代替不稳定的Transformer组件模块：中心归一化代替层归一化，谱初始化代替Xavier初始化，缩放余弦相似度注意代替点积注意，并使用加权残差快捷方式。我们证明了这些引入的模块是Lipschitz连续的，并推导出LipsFormer的Lipschitz常数的上界。我们的实验表明，LipsFormer允许深度Transformer架构的稳定训练，无需仔细调整学习率。",
    "tldr": "本文介绍了一种Lipschitz连续的Transformer模型LipsFormer，其中引入了多项Lipschitz连续的组件来保证训练的稳定性，并在实验证明其可以让深度Transformer架构稳定进行训练，无需过多调整学习率等参数。",
    "en_tdlr": "This paper introduces a Lipschitz continuous Transformer model called LipsFormer, which incorporates various Lipschitz continuous components to ensure training stability for deep Transformer architectures, and achieves stable training without the need for careful adjustment of learning rates or other parameters."
}