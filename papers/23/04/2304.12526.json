{
    "title": "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])",
    "abstract": "Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\\mathbf{\\ge 2\\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve ",
    "link": "http://arxiv.org/abs/2304.12526",
    "context": "Title: Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])\nAbstract: Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\\mathbf{\\ge 2\\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve ",
    "path": "papers/23/04/2304.12526.json",
    "total_tokens": 927,
    "translated_title": "Patch Diffusion: 更快更高效的扩散模型训练方法",
    "translated_abstract": "扩散模型是强大的，但需要大量时间和数据来训练。我们提出了一种通用的基于块的训练框架 Patch Diffusion，显著降低了训练时间成本，同时提高了数据效率，从而有助于将扩散模型的训练推广到更广泛的用户。我们的创新核心是新的条件评分函数，它在块级别上进行操作，并将原始图像中的块位置作为附加坐标通道，同时通过在训练过程中随机化和多样化块大小来编码多尺度的跨区域依赖关系。在采样方面，我们的方法与原始扩散模型一样简单易用。通过 Patch Diffusion，我们能够实现 $\\mathbf{\\ge 2\\times}$ 更快的训练速度，同时保持可比较或更好的生成质量。同时，Patch Diffusion 提高了在相对较小的数据集上训练的扩散模型的性能，例如，使用仅 5,000 张图像进行从头开始训练。",
    "tldr": "Patch Diffusion 提出了一种通用的基于块的训练框架，可将扩散模型训练时间缩短至少一倍，并在更小的数据集上实现更好的性能表现。",
    "en_tdlr": "Patch Diffusion proposes a generic patch-wise training framework that significantly reduces diffusion model training time with better data efficiency. Their innovation lies in a new conditional score function at the patch level and diversified patch size. Patch Diffusion achieves at least 2x faster training with comparable or better generation quality and improves performance on smaller datasets."
}