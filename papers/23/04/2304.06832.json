{
    "title": "Task Adaptive Feature Transformation for One-Shot Learning. (arXiv:2304.06832v1 [cs.LG])",
    "abstract": "We introduce a simple non-linear embedding adaptation layer, which is fine-tuned on top of fixed pre-trained features for one-shot tasks, improving significantly transductive entropy-based inference for low-shot regimes. Our norm-induced transformation could be understood as a re-parametrization of the feature space to disentangle the representations of different classes in a task specific manner. It focuses on the relevant feature dimensions while hindering the effects of non-relevant dimensions that may cause overfitting in a one-shot setting. We also provide an interpretation of our proposed feature transformation in the basic case of few-shot inference with K-means clustering. Furthermore, we give an interesting bound-optimization link between K-means and entropy minimization. This emphasizes why our feature transformation is useful in the context of entropy minimization. We report comprehensive experiments, which show consistent improvements over a variety of one-shot benchmarks, ",
    "link": "http://arxiv.org/abs/2304.06832",
    "context": "Title: Task Adaptive Feature Transformation for One-Shot Learning. (arXiv:2304.06832v1 [cs.LG])\nAbstract: We introduce a simple non-linear embedding adaptation layer, which is fine-tuned on top of fixed pre-trained features for one-shot tasks, improving significantly transductive entropy-based inference for low-shot regimes. Our norm-induced transformation could be understood as a re-parametrization of the feature space to disentangle the representations of different classes in a task specific manner. It focuses on the relevant feature dimensions while hindering the effects of non-relevant dimensions that may cause overfitting in a one-shot setting. We also provide an interpretation of our proposed feature transformation in the basic case of few-shot inference with K-means clustering. Furthermore, we give an interesting bound-optimization link between K-means and entropy minimization. This emphasizes why our feature transformation is useful in the context of entropy minimization. We report comprehensive experiments, which show consistent improvements over a variety of one-shot benchmarks, ",
    "path": "papers/23/04/2304.06832.json",
    "total_tokens": 836,
    "translated_title": "一种适应任务的特征转换方法用于一次性学习",
    "translated_abstract": "我们引入了一种简单的非线性嵌入调整层，它在预训练的固定特征之上进行微调，以改善低样本情况下转导熵推理的效果。我们的范数引导转换可以理解为在特定任务中重新参数化特征空间，以解开不同类别的表示方法。它专注于相关的特征维度，同时阻止可能会在一次性学习中导致过拟合的非相关维度的影响。我们还在K均值聚类的基本情况下提供了对我们提出的特征转换的解释。此外，我们在K均值和熵最小化之间提供了一个有趣的界限优化链接。这强调了我们的特征转换在熵最小化的情况下为什么有用。我们报告了全面的实验，展示了在各种一次性测试中的持续改进。",
    "tldr": "该论文提出了一种针对一次性学习的适应任务的特征转换方法，可在低样本情况下改善推理效果，并在多个一次性测试中得到验证。",
    "en_tdlr": "This paper proposes a task adaptive feature transformation method for one-shot learning, which significantly improves the transductive entropy-based inference in low-shot regimes, and is validated through comprehensive experiments on various one-shot benchmarks."
}