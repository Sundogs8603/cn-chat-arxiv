{
    "title": "LaSNN: Layer-wise ANN-to-SNN Distillation for Effective and Efficient Training in Deep Spiking Neural Networks. (arXiv:2304.09101v1 [cs.NE])",
    "abstract": "Spiking Neural Networks (SNNs) are biologically realistic and practically promising in low-power computation because of their event-driven mechanism. Usually, the training of SNNs suffers accuracy loss on various tasks, yielding an inferior performance compared with ANNs. A conversion scheme is proposed to obtain competitive accuracy by mapping trained ANNs' parameters to SNNs with the same structures. However, an enormous number of time steps are required for these converted SNNs, thus losing the energy-efficient benefit. Utilizing both the accuracy advantages of ANNs and the computing efficiency of SNNs, a novel SNN training framework is proposed, namely layer-wise ANN-to-SNN knowledge distillation (LaSNN). In order to achieve competitive accuracy and reduced inference latency, LaSNN transfers the learning from a well-trained ANN to a small SNN by distilling the knowledge other than converting the parameters of ANN. The information gap between heterogeneous ANN and SNN is bridged by ",
    "link": "http://arxiv.org/abs/2304.09101",
    "context": "Title: LaSNN: Layer-wise ANN-to-SNN Distillation for Effective and Efficient Training in Deep Spiking Neural Networks. (arXiv:2304.09101v1 [cs.NE])\nAbstract: Spiking Neural Networks (SNNs) are biologically realistic and practically promising in low-power computation because of their event-driven mechanism. Usually, the training of SNNs suffers accuracy loss on various tasks, yielding an inferior performance compared with ANNs. A conversion scheme is proposed to obtain competitive accuracy by mapping trained ANNs' parameters to SNNs with the same structures. However, an enormous number of time steps are required for these converted SNNs, thus losing the energy-efficient benefit. Utilizing both the accuracy advantages of ANNs and the computing efficiency of SNNs, a novel SNN training framework is proposed, namely layer-wise ANN-to-SNN knowledge distillation (LaSNN). In order to achieve competitive accuracy and reduced inference latency, LaSNN transfers the learning from a well-trained ANN to a small SNN by distilling the knowledge other than converting the parameters of ANN. The information gap between heterogeneous ANN and SNN is bridged by ",
    "path": "papers/23/04/2304.09101.json",
    "total_tokens": 985,
    "tldr": "本文提出了一种名为LaSNN的新型SNN训练框架，通过分层ANN-to-SNN知识蒸馏将训练有素的ANN的精度优势与SNN的计算效率结合起来，以实现竞争性的精度和降低推理延迟。",
    "en_tdlr": "This paper proposes a novel SNN training framework named LaSNN, which combines the accuracy advantage of well-trained ANNs and the computing efficiency of SNNs by using layer-wise ANN-to-SNN knowledge distillation to achieve competitive accuracy and reduced inference latency."
}