{
    "title": "Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing. (arXiv:2304.10808v1 [cs.CL])",
    "abstract": "This paper proposes a method to optimize tokenization for the performance improvement of already trained downstream models. Our method generates tokenization results attaining lower loss values of a given downstream model on the training data for restricting vocabularies and trains a tokenizer reproducing the tokenization results. Therefore, our method can be applied to variety of tokenization methods, while existing work cannot due to the simultaneous learning of the tokenizer and the downstream model. This paper proposes an example of the BiLSTM-based tokenizer with vocabulary restriction, which can capture wider contextual information for the tokenization process than non-neural-based tokenization methods used in existing work. Experimental results on text classification in Japanese, Chinese, and English text classification tasks show that the proposed method improves performance compared to the existing methods for tokenization optimization.",
    "link": "http://arxiv.org/abs/2304.10808",
    "context": "Title: Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing. (arXiv:2304.10808v1 [cs.CL])\nAbstract: This paper proposes a method to optimize tokenization for the performance improvement of already trained downstream models. Our method generates tokenization results attaining lower loss values of a given downstream model on the training data for restricting vocabularies and trains a tokenizer reproducing the tokenization results. Therefore, our method can be applied to variety of tokenization methods, while existing work cannot due to the simultaneous learning of the tokenizer and the downstream model. This paper proposes an example of the BiLSTM-based tokenizer with vocabulary restriction, which can capture wider contextual information for the tokenization process than non-neural-based tokenization methods used in existing work. Experimental results on text classification in Japanese, Chinese, and English text classification tasks show that the proposed method improves performance compared to the existing methods for tokenization optimization.",
    "path": "papers/23/04/2304.10808.json",
    "total_tokens": 826,
    "translated_title": "限制词汇的神经网络分词器优化方法",
    "translated_abstract": "本文提出了一种针对预先训练的下游模型优化 tokenization 的方法。我们的方法通过限制词汇的方式生成 tokenization 结果，使得在训练数据上给定下游模型的损失值更低，并训练一个可以复现 tokenization 结果的分词器。因此，我们的方法可以应用于各种分词方法，而现有的工作由于分词器和下游模型的同时学习，因此不能应用于各种分词方法。本文提出了 BiLSTM-based 分词器的示例，它可以比现有工作中使用的非神经网络分词方法捕捉更广泛的上下文信息。在日语、汉语和英语文本分类任务上的实验结果表明，所提出的方法在 tokenization 优化方面比现有方法表现更好。",
    "tldr": "本文提出了一种针对预先训练的下游模型优化 tokenization 的方法，通过限制词汇的方式可以生成更低损失值的 tokenization 结果，并训练一个复现 tokenization 结果的分词器。实验证明该方法可以提高 tokenization 的性能。",
    "en_tdlr": "This paper proposes a method to optimize tokenization for already trained downstream models by generating lower loss tokenization results with vocabulary restriction, and trains a tokenizer to replicate these results. The proposed approach can be applied to various tokenization methods, outperforming existing methods in terms of tokenization optimization."
}