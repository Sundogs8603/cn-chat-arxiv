{
    "title": "Model Predictive Control with Self-supervised Representation Learning. (arXiv:2304.07219v1 [cs.LG])",
    "abstract": "Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC fram",
    "link": "http://arxiv.org/abs/2304.07219",
    "context": "Title: Model Predictive Control with Self-supervised Representation Learning. (arXiv:2304.07219v1 [cs.LG])\nAbstract: Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC fram",
    "path": "papers/23/04/2304.07219.json",
    "total_tokens": 918,
    "translated_title": "具有自监督表示学习的模型预测控制",
    "translated_abstract": "在过去的几年中，我们没有看到模型无关或模型基础学习方法有任何重大进展，使得其中一个相对于另一个过时。在大多数情况下，所使用的技术严重依赖于用例场景或其他属性，例如环境。两种方法都有自己的优点，例如样本效率或计算效率。然而，当将两种方法结合起来时，可以结合各自的优点，从而实现更好的性能。TD-MPC框架就是这种方法的一个例子。一方面，结合模型预测控制的世界模型用于获得良好的值函数初始估计。另一方面，Q函数用于提供良好的长期估计。与MuZero等算法类似，使用潜在状态表示，其中仅对任务相关信息进行编码以减少复杂性。本文提出了在TD-MPC框架内使用重构函数进行自监督表示学习。这可以创建更具信息性和鲁棒性的潜在状态表示，从而提高了一系列机器人控制任务的性能。",
    "tldr": "该论文提出了在TD-MPC框架内使用重构函数进行自监督表示学习的方法，在机器人控制任务中实现更好的性能表现。",
    "en_tdlr": "This paper proposes a method of self-supervised representation learning using a reconstruction function within the TD-MPC framework, resulting in improved performance on a range of robotic control tasks."
}