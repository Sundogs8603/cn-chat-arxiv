{
    "title": "What Do GNNs Actually Learn? Towards Understanding their Representations. (arXiv:2304.10851v1 [cs.LG])",
    "abstract": "In recent years, graph neural networks (GNNs) have achieved great success in the field of graph representation learning. Although prior work has shed light into the expressiveness of those models (\\ie whether they can distinguish pairs of non-isomorphic graphs), it is still not clear what structural information is encoded into the node representations that are learned by those models. In this paper, we investigate which properties of graphs are captured purely by these models, when no node attributes are available. Specifically, we study four popular GNN models, and we show that two of them embed all nodes into the same feature vector, while the other two models generate representations that are related to the number of walks over the input graph. Strikingly, structurally dissimilar nodes can have similar representations at some layer $k>1$, if they have the same number of walks of length $k$. We empirically verify our theoretical findings on real datasets.",
    "link": "http://arxiv.org/abs/2304.10851",
    "context": "Title: What Do GNNs Actually Learn? Towards Understanding their Representations. (arXiv:2304.10851v1 [cs.LG])\nAbstract: In recent years, graph neural networks (GNNs) have achieved great success in the field of graph representation learning. Although prior work has shed light into the expressiveness of those models (\\ie whether they can distinguish pairs of non-isomorphic graphs), it is still not clear what structural information is encoded into the node representations that are learned by those models. In this paper, we investigate which properties of graphs are captured purely by these models, when no node attributes are available. Specifically, we study four popular GNN models, and we show that two of them embed all nodes into the same feature vector, while the other two models generate representations that are related to the number of walks over the input graph. Strikingly, structurally dissimilar nodes can have similar representations at some layer $k>1$, if they have the same number of walks of length $k$. We empirically verify our theoretical findings on real datasets.",
    "path": "papers/23/04/2304.10851.json",
    "total_tokens": 844,
    "translated_title": "GNNs到底在学什么？——理解它们的表示方法",
    "translated_abstract": "最近几年，图神经网络（GNNs）在图嵌入学习领域取得了巨大成功。尽管以往的研究揭示了这些模型的表达能力（即它们是否能区分非同构图对），但仍不清楚这些模型所学习的节点表示中编码了哪些结构信息。本文研究了四种流行的GNN模型，并展示了其中两种将所有节点嵌入同一特征向量中，而另外两种模型生成的表示与输入图中的步长数量相关。令人惊讶的是，如果两个不同结构的节点在某一层$k>1$ 中的步长相同，则它们的表示可能相似。我们在真实数据集上进行了实证验证，从而验证了我们的理论发现。",
    "tldr": "本文研究了四种GNN模型，指出其中两种将所有节点嵌入同一特征向量中，而另外两种模型生成的表示与输入图中的步长数量相关。在一定条件下，不同结构的节点可能有相似的表示。",
    "en_tdlr": "This paper investigates four popular GNN models and shows that two of them embed all nodes into the same feature vector, while the other two models generate representations that are related to the number of walks over the input graph. Under certain conditions, structurally dissimilar nodes may have similar representations."
}