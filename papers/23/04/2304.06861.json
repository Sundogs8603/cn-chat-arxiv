{
    "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])",
    "abstract": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re",
    "link": "http://arxiv.org/abs/2304.06861",
    "context": "Title: Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])\nAbstract: Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re",
    "path": "papers/23/04/2304.06861.json",
    "total_tokens": 941,
    "translated_title": "对最新大规模预训练模型中的社会偏见进行评估",
    "translated_abstract": "大规模预训练语言模型广泛应用在社区中，通常使用来自于互联网等开放来源的未审核或未筛选的数据进行训练。由于这一点，我们在在线平台上看到的偏见反映了社会上的偏见，并被这些模型所捕捉和学习。这些模型被应用于影响数百万人的应用程序中，它们内在的偏见对于定向的社会群体是有害的。在本文中，我们研究新预训练模型发布后的偏见缩减趋势。选择了三个最新模型(ELECTRA、DeBERTa和DistilBERT)，并对两个偏见基准（StereoSet和CrowS-Pairs）进行评估。它们使用相关度量标准与BERT进行比较。我们探索是否随着技术的进步和新的、更快、更轻的模型发布，它们是否负责任地发展，使其内在的社会偏见与旧模型相比有所降低？",
    "tldr": "本文研究了最近发布的三个预训练模型的偏见问题，并评估了它们在两个偏见基准上的表现，探讨了是否随着技术进步，最新的、更快、更轻的模型在开发时负责任地降低了与旧模型相比的社会偏见。"
}