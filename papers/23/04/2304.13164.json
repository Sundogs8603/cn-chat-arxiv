{
    "title": "Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])",
    "abstract": "The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.",
    "link": "http://arxiv.org/abs/2304.13164",
    "context": "Title: Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])\nAbstract: The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.",
    "path": "papers/23/04/2304.13164.json",
    "total_tokens": 864,
    "translated_title": "迈向计算优化的迁移学习",
    "translated_abstract": "预训练模型的出现使得迁移学习领域发生了重大变革，这些模型在下游任务中展现了强大的适应性。但是，微调或使用这些模型的高计算和存储要求可能会阻碍它们的广泛使用。在本研究中，我们提出了一个解决方案，通过提出一种简单而有效的方法，将计算效率与渐近性能之间的权衡，我们将其定义为学习算法在计算趋近于无穷大时实现的性能。具体而言，我们认为对预训练模型进行零-shot结构剪枝，可以使它们在最小降低性能的情况下提高计算效率。我们在提供多种迁移场景的Nevis'22连续学习基准上评估了我们的方法。我们的结果表明，在低计算范围内，剪枝预先训练模型的卷积过滤器可以带来超过20%的性能提高。",
    "tldr": "本文提出一种计算优化的迁移学习方法，通过对预训练模型进行零-shot结构剪枝，使其在最小降低性能的情况下提高计算效率，实现了20%以上的性能提升。",
    "en_tdlr": "This paper proposes a compute-optimal transfer learning method, which achieves more than 20% performance improvement by applying zero-shot structured pruning to pretrained models to increase computational efficiency with minimal reduction in performance."
}