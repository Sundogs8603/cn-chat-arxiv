{
    "title": "LSFSL: Leveraging Shape Information in Few-shot Learning. (arXiv:2304.06672v1 [cs.CV])",
    "abstract": "Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.",
    "link": "http://arxiv.org/abs/2304.06672",
    "context": "Title: LSFSL: Leveraging Shape Information in Few-shot Learning. (arXiv:2304.06672v1 [cs.CV])\nAbstract: Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.",
    "path": "papers/23/04/2304.06672.json",
    "total_tokens": 856,
    "translated_title": "利用形状信息进行少样本学习：LSFSL",
    "translated_abstract": "少样本学习技术旨在使用较少的样本来学习数据中的潜在模式，类似于人类从有限的经验中学习。在这种有限数据的情况下，与深度神经网络相关的快捷学习和纹理偏差行为等挑战更为严峻。此外，在少样本设置中解决快捷学习的重要性尚未得到充分探索。为解决这些问题，我们提出了LSFSL，该模型利用数据中的隐含先验信息强制模型学习更具有泛化性的特征。通过全面分析，我们证明了在数据的全局语义中利用LSFSL训练的模型对颜色方案、统计相关性和对抗扰动的改变更不易受到攻击和干扰，凸显出在少样本方法中融入相关先验以增强鲁棒性和泛化性的潜力。",
    "tldr": "LSFSL是一种少样本学习方法，利用数据中的隐含先验信息强制模型学习更具有泛化性的特征。通过全面分析，证明该方法提高了模型的鲁棒性和泛化性。"
}