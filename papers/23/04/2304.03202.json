{
    "title": "SLM: End-to-end Feature Selection via Sparse Learnable Masks. (arXiv:2304.03202v1 [cs.LG])",
    "abstract": "Feature selection has been widely used to alleviate compute requirements during training, elucidate model interpretability, and improve model generalizability. We propose SLM -- Sparse Learnable Masks -- a canonical approach for end-to-end feature selection that scales well with respect to both the feature dimension and the number of samples. At the heart of SLM lies a simple but effective learnable sparse mask, which learns which features to select, and gives rise to a novel objective that provably maximizes the mutual information (MI) between the selected features and the labels, which can be derived from a quadratic relaxation of mutual information from first principles. In addition, we derive a scaling mechanism that allows SLM to precisely control the number of features selected, through a novel use of sparsemax. This allows for more effective learning as demonstrated in ablation studies. Empirically, SLM achieves state-of-the-art results against a variety of competitive baselines",
    "link": "http://arxiv.org/abs/2304.03202",
    "context": "Title: SLM: End-to-end Feature Selection via Sparse Learnable Masks. (arXiv:2304.03202v1 [cs.LG])\nAbstract: Feature selection has been widely used to alleviate compute requirements during training, elucidate model interpretability, and improve model generalizability. We propose SLM -- Sparse Learnable Masks -- a canonical approach for end-to-end feature selection that scales well with respect to both the feature dimension and the number of samples. At the heart of SLM lies a simple but effective learnable sparse mask, which learns which features to select, and gives rise to a novel objective that provably maximizes the mutual information (MI) between the selected features and the labels, which can be derived from a quadratic relaxation of mutual information from first principles. In addition, we derive a scaling mechanism that allows SLM to precisely control the number of features selected, through a novel use of sparsemax. This allows for more effective learning as demonstrated in ablation studies. Empirically, SLM achieves state-of-the-art results against a variety of competitive baselines",
    "path": "papers/23/04/2304.03202.json",
    "total_tokens": 904,
    "translated_title": "SLM：通过稀疏可学习掩码进行端到端特征选择",
    "translated_abstract": "特征选择已被广泛应用于减少训练过程中的计算要求，阐明模型的可解释性以及提高模型的泛化能力。本文提出了SLM - 稀疏可学习掩码，这是一种经典的端到端特征选择方法，可以很好地适应特征维度和样本数量的变化。SLM的核心是一个简单但有效的可学习稀疏掩码，它学习选择哪些特征，并产生一种新的目标函数，可以证明它最大化了所选特征和标签之间的互信息（MI），这可以从互信息的一次方根松弛中导出。此外，我们还推导出一种缩放机制，利用sparsemax精确地控制选择的特征数量。通过消融实验证明了其有效性。在实验中，SLM在各种竞争基线模型上取得了最先进的结果。",
    "tldr": "SLM是一种经典的端到端特征选择方法，通过可学习的稀疏掩码来最大化所选特征和标签之间的互信息，同时精确地控制选择的特征数量，实验结果表明其取得了最先进的结果。",
    "en_tdlr": "SLM is a classical method for end-to-end feature selection that maximizes the mutual information between the selected features and labels using a learnable sparse mask. It also enables precise control over the number of selected features through a novel use of sparsemax, and achieves state-of-the-art results in various competitive baseline models."
}