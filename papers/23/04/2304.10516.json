{
    "title": "Distributed Neural Representation for Reactive in situ Visualization. (arXiv:2304.10516v1 [cs.DC])",
    "abstract": "In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.",
    "link": "http://arxiv.org/abs/2304.10516",
    "context": "Title: Distributed Neural Representation for Reactive in situ Visualization. (arXiv:2304.10516v1 [cs.DC])\nAbstract: In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.",
    "path": "papers/23/04/2304.10516.json",
    "total_tokens": 791,
    "translated_title": "分布式神经表示技术用于反应式原位可视化",
    "translated_abstract": "利用反应式编程实现计算模型的原位可视化和控制十分高效，它利用时间抽象和数据缓存机制来创建动态工作流。然而，对于大规模模拟，实现时间缓存可能存在挑战。隐式神经网络已被证明在压缩大型数据方面是有效的。然而，它们在分布式数据上的应用还没有被充分探索。在本研究中，我们开发了一种分布式体积数据的隐式神经表示，并将其结合到DIVA反应式编程系统中。这种实现使我们能够构建一个原位时间缓存系统，其容量比以前的容量大100倍。我们将这种方法集成到Ascent基础架构中，并使用实际模拟来评估其性能。",
    "tldr": "本研究开发了一种分布式体积数据的隐式神经表示技术，结合到反应式编程系统中构建了一个原位时间缓存系统，并在Ascent基础架构中使用实际模拟评估了其100倍容量的性能。",
    "en_tdlr": "This paper develops a novel implicit neural representation for distributed volume data and integrates it into a reactive programming system to build an in situ temporal caching system with a 100 times larger capacity. Its performance is evaluated using real-world simulations in the Ascent infrastructure."
}