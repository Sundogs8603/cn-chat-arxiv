{
    "title": "AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])",
    "abstract": "Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a",
    "link": "http://arxiv.org/abs/2304.09595",
    "context": "Title: AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])\nAbstract: Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a",
    "path": "papers/23/04/2304.09595.json",
    "total_tokens": 1007,
    "translated_title": "AdapterGNN：高效的δ调节提高了图神经网络的泛化能力",
    "translated_abstract": "最近，在图神经网络（GNNs）中微调预训练模型已经取得了显著的性能提升。除了预训练技术外，由于自然语言领域的最新工作的启示，更近期的研究转向应用有效的微调方法，例如参数有效的调节（δ调节）。然而，考虑到GNNs和基于transformer的模型之间存在重大差异，将这些方法直接应用于GNNs证明效果较弱。在本文中，我们对GNNs的δ调节技术进行了全面比较，并提出了一种专门为GNNs设计的新型δ调节方法——AdapterGNN。AdapterGNN保留了大型预训练模型的知识，并利用高度表达的GNN适配器，在仅有少量参数的情况下有效地适应下游任务，同时提高了模型的下游任务的泛化能力。广泛的实验表明，AdapterGNN在几个基准数据集上取得了最先进的性能，同时具有高效的特点。",
    "tldr": "本文提出了一种专为图神经网络设计的δ调节方法——AdapterGNN，该方法保留了预训练模型的知识，利用高度表达的适配器能够在仅有少量参数的情况下有效地适应下游任务，并提高模型的泛化能力，实验结果表明其在多个基准数据集上取得了最优性能。",
    "en_tdlr": "This paper proposes a novel delta tuning method specifically designed for GNNs, called AdapterGNN, which preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs to effectively adapt to downstream tasks with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN achieves state-of-the-art performance on several benchmark datasets while being highly efficient."
}