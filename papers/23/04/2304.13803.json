{
    "title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models. (arXiv:2304.13803v1 [cs.CL])",
    "abstract": "Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can be finetuned to perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD). However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best le",
    "link": "http://arxiv.org/abs/2304.13803",
    "context": "Title: Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models. (arXiv:2304.13803v1 [cs.CL])\nAbstract: Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can be finetuned to perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD). However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best le",
    "path": "papers/23/04/2304.13803.json",
    "total_tokens": 1199,
    "translated_title": "使用预训练语言模型的零射多语言词义消歧",
    "translated_abstract": "预训练语言模型（PLMs）学习丰富的跨语言知识，在翻译和多语言词义消歧（WSD）等多种任务上表现良好。本文提出了一项新研究，研究了如何使用上下文词级翻译（C-WLT）捕捉PLMs的跨语言词义能力。我们发现，随着模型大小的增加，PLMs编码更多的跨语言词义知识，并更好地利用上下文来提高WLT性能。在C-WLT的基础上，我们引入了一种零射WSD方法，并在XL-WSD数据集的18种语言上进行了测试。",
    "tldr": "本研究提出了一个使用预训练语言模型的零射多语言词义消歧方法，该方法在无需额外训练或微调的情况下，在XL-WSD数据集的18种语言上实现了超越完全监督基线的召回率。",
    "en_tdlr": "This study presents a zero-shot multilingual word sense disambiguation approach using pre-trained language models, which outperforms fully supervised baselines on recall for 18 languages without additional training or finetuning in the XL-WSD dataset."
}