{
    "title": "N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models. (arXiv:2304.12918v1 [cs.LG])",
    "abstract": "Understanding the function of individual neurons within language models is essential for mechanistic interpretability research. We propose $\\textbf{Neuron to Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and automatically distills the neuron's behaviour on those examples to an interpretable graph. This presents a less labour intensive approach to interpreting neurons than current manual methods, that will better scale these methods to Large Language Models (LLMs). We use truncation and saliency methods to only present the important tokens, and augment the dataset examples with more diverse samples to better capture the extent of neuron behaviour. These graphs can be visualised to aid manual interpretation by researchers, but can also output token activations on text to compare to the neuron's ground truth activations for automatic validation. N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretab",
    "link": "http://arxiv.org/abs/2304.12918",
    "context": "Title: N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models. (arXiv:2304.12918v1 [cs.LG])\nAbstract: Understanding the function of individual neurons within language models is essential for mechanistic interpretability research. We propose $\\textbf{Neuron to Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and automatically distills the neuron's behaviour on those examples to an interpretable graph. This presents a less labour intensive approach to interpreting neurons than current manual methods, that will better scale these methods to Large Language Models (LLMs). We use truncation and saliency methods to only present the important tokens, and augment the dataset examples with more diverse samples to better capture the extent of neuron behaviour. These graphs can be visualised to aid manual interpretation by researchers, but can also output token activations on text to compare to the neuron's ground truth activations for automatic validation. N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretab",
    "path": "papers/23/04/2304.12918.json",
    "total_tokens": 951,
    "translated_title": "N2G：一种在大型语言模型中量化可解释神经元表示的可扩展方法",
    "translated_abstract": "理解语言模型中个别神经元的功能对于机械解释性研究至关重要。我们提出了一种名为“神经元到图（N2G）”的工具，该工具接收神经元及其数据集示例，并自动将神经元在这些示例上的行为提炼为一个可解释的图形，为解释神经元提供了比当前手动方法更轻松的方法，并更好地将这些方法扩展到大型语言模型（LLM）。我们使用截断和显著性方法仅呈现重要令牌，并利用更多样的样本增强数据集示例，以更好地捕捉神经元行为的程度。这些图形可以通过可视化来帮助研究人员进行手动解释，但也可以输出文本上的令牌激活情况，以与神经元的参考激活情况进行比较，进行自动验证。N2G代表了可扩展的解释性方法向前迈出的一步，因为它允许我们自动将LLM中的神经元转换为可解释的图形。",
    "tldr": "N2G是一种在大型语言模型中实现解释神经元的方法，它通过自动将神经元在数据集示例上的行为提炼为可解释的图形，并且可以输出文本上令牌的激活情况来进行自动验证。",
    "en_tdlr": "N2G is a scalable approach for interpreting neurons in large language models by automatically distilling the neuron's behavior on dataset examples to an interpretable graph, which can also output token activations on text for automatic validation."
}