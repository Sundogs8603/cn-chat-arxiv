{
    "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third",
    "link": "http://arxiv.org/abs/2304.04675",
    "context": "Title: Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third",
    "path": "papers/23/04/2304.04675.json",
    "total_tokens": 1209,
    "translated_title": "大语言模型实现多语机器翻译：实证结果和分析",
    "translated_abstract": "大语言模型(LLMs)在处理多语机器翻译(MMT)方面表现出了卓越的潜力。本文通过回答两个问题系统地研究了LLMs在MMT中的优势和挑战：1) LLMs在翻译大量语言方面表现如何？2) 哪些因素会影响LLMs在翻译中的表现？我们评估了包括XGLM、OPT、BLOOMZ和ChatGPT在内的几个受欢迎的LLMs在102种语言上的表现。我们的实证结果显示，即使是最好的模型ChatGPT在83.33%的翻译方向上也落后于监督基线NLLB。通过进一步的分析，我们发现当用于MMT时，LLMs表现出新的工作模式。首先，在给定上下文示例时，提示语义可能会被意外地忽略，即使提示不合理，LLMs仍然表现出强大的性能。其次，跨语言示例可以为低资源翻译提供比相同语言对中的示例更好的任务指导。第三，当翻译低资源语言时，LLMs往往表现得更好。总的来说，我们的研究为LLMs在MMT中的潜力和局限性提供了新的见解，为未来的研究提供了有用的启示。",
    "tldr": "本文系统地研究了大语言模型在多语机器翻译中的优势和挑战，证明其表现出卓越的潜力。本研究发现LLMs在给定上下文示例时可以意外地忽略提示语义，并且跨语言示例可以为低资源翻译提供更好的任务指导。但实证结果表明，即使是最好的模型ChatGPT仍然落后于监督基线NLLB。",
    "en_tdlr": "This paper systematically investigates the advantages and challenges of large language models (LLMs) for multilingual machine translation (MMT). The study shows that LLMs have remarkable potential in MMT, but even the best-performing model still lags behind supervised baselines in many translation directions. The authors also discover new working patterns of LLMs in MMT, including the surprising ability to ignore prompt semantics when given in-context exemplars and the effectiveness of cross-lingual exemplars for low-resource translation."
}