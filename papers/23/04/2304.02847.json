{
    "title": "Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])",
    "abstract": "Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.",
    "link": "http://arxiv.org/abs/2304.02847",
    "context": "Title: Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])\nAbstract: Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.",
    "path": "papers/23/04/2304.02847.json",
    "total_tokens": 992,
    "translated_title": "Robustmix：通过正则化深度网络的频率偏差来提高鲁棒性",
    "translated_abstract": "深度网络在一系列经过精心策划的基准数据集上取得了令人印象深刻的结果。令人惊讶的是，它们的性能对于对人类性能几乎没有影响的扰动仍然很敏感。在这项工作中，我们提出了一种名为Robustmix的Mixup新扩展，该扩展通过正则化网络以基于低频空间特征进行分类。我们表明，这种类型的正则化改善了在一系列基准测试中的鲁棒性，例如Imagenet-C和Stylized Imagenet。它几乎没有计算开销，并且不需要先验知识的大量图像变换。我们发现，这种方法进一步补充了模型架构和数据增强的最新进展，使用EfficientNet-B8模型和RandAugment达到了44.8的最新状态平均峰值误差（mCE），相比基线降低了16个mCE。",
    "tldr": "本研究提出一种叫做Robustmix的方法，通过正则化网络以低频空间特征进行分类来提高深度网络的鲁棒性，在Imagenet-C和Stylized Imagenet等基准测试上取得了最新的最优状态平均峰值误差（mCE），在避免计算开销和先验知识的大量图像变换的同时对模型架构和数据增强的最新进展提供了补充。",
    "en_tdlr": "The paper proposes a method called Robustmix, which improves the robustness of deep networks by regularizing the frequency bias of the network and classifying based on lower-frequency spatial features. The method achieves state-of-the-art mCE on benchmark tests such as Imagenet-C and Stylized Imagenet, while adding little computational overhead and not requiring prior knowledge of a large set of image transformations. The approach complements recent advances in model architecture and data augmentation."
}