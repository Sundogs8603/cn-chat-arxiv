{
    "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])",
    "abstract": "We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \\texttt{image-reward} package at \\url{https://github.com/THUDM/ImageReward}.",
    "link": "http://arxiv.org/abs/2304.05977",
    "context": "Title: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])\nAbstract: We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \\texttt{image-reward} package at \\url{https://github.com/THUDM/ImageReward}.",
    "path": "papers/23/04/2304.05977.json",
    "total_tokens": 823,
    "translated_title": "ImageReward：学习和评估文本到图像生成的人类喜好",
    "translated_abstract": "本文提出一种通用的文本到图像生成人类喜好奖励模型ImageReward，旨在解决生成模型中存在的各种问题，并使其与人类价值和偏好保持一致。该奖励模型的训练基于我们的系统注释流程，其中包括评分和排名组件，迄今已收集了137k的专家比较数据集。在人类评估中，ImageReward的表现优于现有的评分方法（例如比CLIP高38.6\\%），因此它是一种有前途的用于评估和改进文本到图像合成的自动度量标准。该奖励模型通过\\texttt {image-reward}程序包公开提供，网址为\\url{https://github.com/THUDM/ImageReward}。",
    "tldr": "ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。",
    "en_tdlr": "ImageReward is a general-purpose human preference reward model for text-to-image generation that addresses prevalent issues in generative models and aligns them with human values and preferences through systematic annotation pipeline. It outperforms existing scoring methods in human evaluation and has the potential to be a promising automatic metric for evaluating and improving text-to-image synthesis. The code for the reward model is publicly available."
}