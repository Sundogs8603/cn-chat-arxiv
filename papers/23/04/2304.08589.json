{
    "title": "Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load. (arXiv:2304.08589v1 [cs.DC])",
    "abstract": "In distributed machine learning, a central node outsources computationally expensive calculations to external worker nodes. The properties of optimization procedures like stochastic gradient descent (SGD) can be leveraged to mitigate the effect of unresponsive or slow workers called stragglers, that otherwise degrade the benefit of outsourcing the computation. This can be done by only waiting for a subset of the workers to finish their computation at each iteration of the algorithm. Previous works proposed to adapt the number of workers to wait for as the algorithm evolves to optimize the speed of convergence. In contrast, we model the communication and computation times using independent random variables. Considering this model, we construct a novel scheme that adapts both the number of workers and the computation load throughout the run-time of the algorithm. Consequently, we improve the convergence speed of distributed SGD while significantly reducing the computation load, at the ex",
    "link": "http://arxiv.org/abs/2304.08589",
    "context": "Title: Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load. (arXiv:2304.08589v1 [cs.DC])\nAbstract: In distributed machine learning, a central node outsources computationally expensive calculations to external worker nodes. The properties of optimization procedures like stochastic gradient descent (SGD) can be leveraged to mitigate the effect of unresponsive or slow workers called stragglers, that otherwise degrade the benefit of outsourcing the computation. This can be done by only waiting for a subset of the workers to finish their computation at each iteration of the algorithm. Previous works proposed to adapt the number of workers to wait for as the algorithm evolves to optimize the speed of convergence. In contrast, we model the communication and computation times using independent random variables. Considering this model, we construct a novel scheme that adapts both the number of workers and the computation load throughout the run-time of the algorithm. Consequently, we improve the convergence speed of distributed SGD while significantly reducing the computation load, at the ex",
    "path": "papers/23/04/2304.08589.json",
    "total_tokens": 841,
    "translated_title": "快速并容错的分布式SGD算法，降低计算负载。",
    "translated_abstract": "在分布式机器学习中，一个中心节点将计算密集型的运算外包给外部的工作节点。优化过程的属性，如随机梯度下降（SGD），可以利用以减轻不响应或速度慢的工人（称为迟钝者）的影响，因为这些情况会降低计算外包的收益。这可以通过仅等待每个算法迭代中的一部分工作节点完成其计算来实现。之前的工作提出了适应等待工人数量随算法演化以优化收敛速度的方法。相反，本文构建了一个新的方案，通过使用独立的随机变量对通信和计算时间进行建模，来适应算法的运行时间内的工作节点数量和计算负载。因此，我们提高了分布式SGD的收敛速度，同时显着降低了计算负载。",
    "tldr": "本文提出了一种基于模型的分布式SGD算法方案，通过适应算法的运行时间内的工作节点数量和计算负载，优化收敛速度同时降低计算负载。"
}