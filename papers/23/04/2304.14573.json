{
    "title": "SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis. (arXiv:2304.14573v1 [cs.CV])",
    "abstract": "Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging.  To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieve",
    "link": "http://arxiv.org/abs/2304.14573",
    "context": "Title: SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis. (arXiv:2304.14573v1 [cs.CV])\nAbstract: Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging.  To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieve",
    "path": "papers/23/04/2304.14573.json",
    "total_tokens": 914,
    "translated_title": "SceneGenie: 场景图引导扩散模型用于图像合成。",
    "translated_abstract": "近年来，基于文本生成图像的技术在生成对抗网络和最新的扩散模型的支持下取得了显着进展。然而，以文本提示为条件的扩散模型在产生印象深刻的高质量图像方面表现出色，但在准确表示特定对象的实例数量等复杂文本提示方面仍然具有挑战性。为了解决这个问题，我们提出了一种新的扩散模型采样过程指导方法，在推理时利用边界框和分割地图信息，无需额外的训练数据。通过采样过程中的新损失，我们的方法使用来自CLIP嵌入的语义特征引导模型，并强制执行几何约束，从而生成准确表示场景的高分辨率图像。为了获得边界框和分割地图信息，我们将文本提示结构化为一个场景图，并使用CLIP嵌入丰富节点信息。我们的模型在从复杂文本提示生成图像方面实现了前所未有的保真度。",
    "tldr": "本文提出了SceneGenie模型，利用边界框和分割地图信息引导扩散模型生成高分辨率图像，处理复杂的文本提示。该方法使用了CLIP嵌入的语义特征，从而实现了前所未有的保真度。",
    "en_tdlr": "The paper proposes a SceneGenie model that leverages bounding box and segmentation map information to guide the diffusion model in generating high-resolution images from complex text prompts. The model uses semantic features from CLIP embeddings and geometric constraints to achieve unprecedented fidelity."
}