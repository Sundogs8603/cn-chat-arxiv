{
    "title": "Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])",
    "abstract": "In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident performance gap between large and small models exists in practice. Hence, due to space and time hardware limitations, there is a need to attain comparable results when using the smaller model, which is usually a distilled version of the large language model. In this paper, we assess the model distillation of the sentence representation model Sentence-BERT by augmenting the pre-trained distilled model with a projection layer additionally learned on the Maximum Coding Rate Reduction (MCR2)objective, a novel approach developed for general-purpose manifold clustering. We demonstrate that the new language model with reduced complexity and sentence embedding size can achieve comparable results on semantic",
    "link": "http://arxiv.org/abs/2304.12674",
    "context": "Title: Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])\nAbstract: In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident performance gap between large and small models exists in practice. Hence, due to space and time hardware limitations, there is a need to attain comparable results when using the smaller model, which is usually a distilled version of the large language model. In this paper, we assess the model distillation of the sentence representation model Sentence-BERT by augmenting the pre-trained distilled model with a projection layer additionally learned on the Maximum Coding Rate Reduction (MCR2)objective, a novel approach developed for general-purpose manifold clustering. We demonstrate that the new language model with reduced complexity and sentence embedding size can achieve comparable results on semantic",
    "path": "papers/23/04/2304.12674.json",
    "total_tokens": 853,
    "translated_title": "最大编码速率降低下的句子表示压缩方法",
    "translated_abstract": "在大多数自然语言推理问题中，需要使用句子表示来进行语义检索任务。在近年来，预训练的大型语言模型已经相当有效地计算这些表示。这些模型产生高维句子嵌入。实际上存在大型和小型模型之间明显的性能差距。因此，由于空间和时间硬件限制，需要在使用较小模型(通常是大型语言模型的精简版本)时获得可比较的结果。在本文中，我们通过在最大编码速率降低(MCR2)目标的基础上学习额外的投影层，评估了句子表示模型Sentence-BERT的模型蒸馏，在这种方法中，MCR2是一种为了通用流形聚类而开发的新方法。我们证明，通过在复杂度和句子嵌入大小方面减小的新语言模型可以在语义相关任务中获得可比较的结果。",
    "tldr": "提出了一种在最大编码速率降低下的句子表示压缩方法，通过在句子表示模型Sentence-BERT中加入一个额外的学习投影层，并证明其可以在语义相关任务中获得与大型语言模型相当的结果。"
}