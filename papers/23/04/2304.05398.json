{
    "title": "Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space. (arXiv:2304.05398v1 [math.ST])",
    "abstract": "Variational inference (VI) seeks to approximate a target distribution $\\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\\pi$ is only log-smooth.",
    "link": "http://arxiv.org/abs/2304.05398",
    "context": "Title: Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space. (arXiv:2304.05398v1 [math.ST])\nAbstract: Variational inference (VI) seeks to approximate a target distribution $\\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\\pi$ is only log-smooth.",
    "path": "papers/23/04/2304.05398.json",
    "total_tokens": 849,
    "translated_title": "通过在Bures-Wasserstein空间中使用JKO进行正反高斯变分推断",
    "translated_abstract": "变分推断旨在通过可追溯的分布族中的元素逼近目标分布$\\pi$。统计学和机器学习中的主要关注点是高斯变分推断，它通过在高斯空间上最小化Kullback-Leibler散度到$\\pi$来逼近$\\pi$。本文开发了(随机)正反高斯变分推断(FB-GVI)算法来解决高斯变分推断问题，我们的方法利用了KL散度的复合结构，KL散度可被写作高斯在Bures-Wasserstein(BW)空间上的势函数与熵函数的和。对于我们提出的算法，当$\\pi$是log-smooth和log-concave时，我们获得了最先进的收敛性保证，同时在$\\pi$只是log-smooth时，我们首次获得了到一阶稳定解的收敛保证。",
    "tldr": "本研究基于Bures-Wasserstein空间，开发了(随机)正反高斯变分推断算法，同时分析了算法在不同分布下的收敛保证。"
}