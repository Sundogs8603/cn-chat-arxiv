{
    "title": "Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition. (arXiv:2304.04991v1 [cs.SD])",
    "abstract": "In recent years, a great deal of attention has been paid to the Transformer network for speech recognition tasks due to its excellent model performance. However, the Transformer network always involves heavy computation and large number of parameters, causing serious deployment problems in devices with limited computation sources or storage memory. In this paper, a new lightweight model called Sim-T has been proposed to expand the generality of the Transformer model. Under the help of the newly developed multiplexing technique, the Sim-T can efficiently compress the model with negligible sacrifice on its performance. To be more precise, the proposed technique includes two parts, that are, module weight multiplexing and attention score multiplexing. Moreover, a novel decoder structure has been proposed to facilitate the attention score multiplexing. Extensive experiments have been conducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when the proposed Sim-T is 48% para",
    "link": "http://arxiv.org/abs/2304.04991",
    "context": "Title: Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition. (arXiv:2304.04991v1 [cs.SD])\nAbstract: In recent years, a great deal of attention has been paid to the Transformer network for speech recognition tasks due to its excellent model performance. However, the Transformer network always involves heavy computation and large number of parameters, causing serious deployment problems in devices with limited computation sources or storage memory. In this paper, a new lightweight model called Sim-T has been proposed to expand the generality of the Transformer model. Under the help of the newly developed multiplexing technique, the Sim-T can efficiently compress the model with negligible sacrifice on its performance. To be more precise, the proposed technique includes two parts, that are, module weight multiplexing and attention score multiplexing. Moreover, a novel decoder structure has been proposed to facilitate the attention score multiplexing. Extensive experiments have been conducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when the proposed Sim-T is 48% para",
    "path": "papers/23/04/2304.04991.json",
    "total_tokens": 810,
    "translated_title": "使用复用技术简化Transformer网络的语音识别模型Sim-T",
    "translated_abstract": "近年来，由于其出色的模型性能，Transformer网络在语音识别任务中受到了广泛的关注。然而，Transformer网络始终涉及大量计算和参数，导致在计算资源或存储内存有限的设备上部署时存在严重的问题。因此，本文通过提出一种名为Sim-T的新型轻量级模型，通过新开发的复用技术可以有效地压缩模型，而对模型性能的牺牲几乎可以忽略不计。具体地，所提出的技术包括模块权重复用和注意力得分复用。此外，还提出了一种新颖的解码器结构来促进注意力得分复用。实验证明，Sim-T模型非常有效。",
    "tldr": "本文提出了一种名为Sim-T的轻量级模型，采用了复用技术有效压缩模型，保持了模型性能，可以很好地解决在计算资源或存储内存有限的设备上部署的问题。",
    "en_tdlr": "This paper proposes a lightweight model called Sim-T, which uses a multiplexing technique to effectively compress the model while maintaining its performance, and solves deployment problems for devices with limited computation resources or storage memory."
}