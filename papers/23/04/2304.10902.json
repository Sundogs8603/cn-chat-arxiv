{
    "title": "Near-Optimal Decentralized Momentum Method for Nonconvex-PL Minimax Problems. (arXiv:2304.10902v1 [math.OC])",
    "abstract": "Minimax optimization plays an important role in many machine learning tasks such as generative adversarial networks (GANs) and adversarial training. Although recently a wide variety of optimization methods have been proposed to solve the minimax problems, most of them ignore the distributed setting where the data is distributed on multiple workers. Meanwhile, the existing decentralized minimax optimization methods rely on the strictly assumptions such as (strongly) concavity and variational inequality conditions. In the paper, thus, we propose an efficient decentralized momentum-based gradient descent ascent (DM-GDA) method for the distributed nonconvex-PL minimax optimization, which is nonconvex in primal variable and is nonconcave in dual variable and satisfies the Polyak-Lojasiewicz (PL) condition. In particular, our DM-GDA method simultaneously uses the momentum-based techniques to update variables and estimate the stochastic gradients. Moreover, we provide a solid convergence anal",
    "link": "http://arxiv.org/abs/2304.10902",
    "context": "Title: Near-Optimal Decentralized Momentum Method for Nonconvex-PL Minimax Problems. (arXiv:2304.10902v1 [math.OC])\nAbstract: Minimax optimization plays an important role in many machine learning tasks such as generative adversarial networks (GANs) and adversarial training. Although recently a wide variety of optimization methods have been proposed to solve the minimax problems, most of them ignore the distributed setting where the data is distributed on multiple workers. Meanwhile, the existing decentralized minimax optimization methods rely on the strictly assumptions such as (strongly) concavity and variational inequality conditions. In the paper, thus, we propose an efficient decentralized momentum-based gradient descent ascent (DM-GDA) method for the distributed nonconvex-PL minimax optimization, which is nonconvex in primal variable and is nonconcave in dual variable and satisfies the Polyak-Lojasiewicz (PL) condition. In particular, our DM-GDA method simultaneously uses the momentum-based techniques to update variables and estimate the stochastic gradients. Moreover, we provide a solid convergence anal",
    "path": "papers/23/04/2304.10902.json",
    "total_tokens": 723,
    "translated_title": "分布式PL非凸最小化极小问题的近似最优去中心化动量法",
    "translated_abstract": "最小最大化优化在许多机器学习任务中扮演着重要角色，如生成式对抗网络（GANs）和对抗性训练。本文提出了一种高效的去中心化动量梯度下降上升法（DM-GDA）方法，用于分布式非凸PL极小化极小优化。",
    "tldr": "提出一种高效的去中心化动量法（DM-GDA），用于分布式非凸PL极小化极小优化，能够同时使用动量和随机梯度估计，解决了现有分布式极小最大化优化方法在实践中使用受限的问题。"
}