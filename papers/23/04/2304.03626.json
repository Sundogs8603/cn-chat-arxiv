{
    "title": "Asynchronous Federated Continual Learning. (arXiv:2304.03626v1 [cs.LG])",
    "abstract": "The standard class-incremental continual learning setting assumes a set of tasks seen one after the other in a fixed and predefined order. This is not very realistic in federated learning environments where each client works independently in an asynchronous manner getting data for the different tasks in time-frames and orders totally uncorrelated with the other ones. We introduce a novel federated learning setting (AFCL) where the continual learning of multiple tasks happens at each client with different orderings and in asynchronous time slots. We tackle this novel task using prototype-based learning, a representation loss, fractal pre-training, and a modified aggregation policy. Our approach, called FedSpace, effectively tackles this task as shown by the results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively. The code and federated splits are available at https://github.com/LTTM/FedSpace.",
    "link": "http://arxiv.org/abs/2304.03626",
    "context": "Title: Asynchronous Federated Continual Learning. (arXiv:2304.03626v1 [cs.LG])\nAbstract: The standard class-incremental continual learning setting assumes a set of tasks seen one after the other in a fixed and predefined order. This is not very realistic in federated learning environments where each client works independently in an asynchronous manner getting data for the different tasks in time-frames and orders totally uncorrelated with the other ones. We introduce a novel federated learning setting (AFCL) where the continual learning of multiple tasks happens at each client with different orderings and in asynchronous time slots. We tackle this novel task using prototype-based learning, a representation loss, fractal pre-training, and a modified aggregation policy. Our approach, called FedSpace, effectively tackles this task as shown by the results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively. The code and federated splits are available at https://github.com/LTTM/FedSpace.",
    "path": "papers/23/04/2304.03626.json",
    "total_tokens": 953,
    "translated_title": "异步联邦连续学习",
    "translated_abstract": "在标准的类别增量连续学习设置中，假定看到一组任务，这些任务按照固定的、预定义的顺序一个接一个地出现。在联邦学习环境中，这不太现实，因为每个客户端都是独立地以异步方式工作，根据完全不相关的时间段和顺序获取不同任务的数据。我们引入了一种新的联邦学习设置（AFCL），其中多个任务的连续学习在每个客户端上，使用不同的顺序和异步时间段。我们使用基于原型的学习、表示损失、分形预训练和修改的聚合策略来解决这个新任务。我们的方法称为FedSpace，在CIFAR-100数据集上的结果以3种不同的联邦划分为50、100和500个客户端作为实验，表明其有效性。",
    "tldr": "该论文介绍了一种新的联邦学习环境——异步联邦连续学习(AFCL)，它使用基于原型的学习、表示损失、分形预训练以及修改的聚合策略，名为FedSpace。通过在CIFAR-100数据集上的实验，该方法在三种联邦划分下，分别使用50、100和500个客户端，得到了令人满意的结果。",
    "en_tdlr": "This paper presents a novel federated learning setting - Asynchronous Federated Continual Learning (AFCL), in which multiple task continual learning happens at each client with different orderings and in asynchronous time slots. The approach, called FedSpace, uses prototype-based learning, representation loss, fractal pre-training, and a modified aggregation policy to address this novel task. The results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively, show the effectiveness of FedSpace."
}