{
    "title": "Learning in latent spaces improves the predictive accuracy of deep neural operators. (arXiv:2304.07599v1 [cs.LG])",
    "abstract": "Operator regression provides a powerful means of constructing discretization-invariant emulators for partial-differential equations (PDEs) describing physical systems. Neural operators specifically employ deep neural networks to approximate mappings between infinite-dimensional Banach spaces. As data-driven models, neural operators require the generation of labeled observations, which in cases of complex high-fidelity models result in high-dimensional datasets containing redundant and noisy features, which can hinder gradient-based optimization. Mapping these high-dimensional datasets to a low-dimensional latent space of salient features can make it easier to work with the data and also enhance learning. In this work, we investigate the latent deep operator network (L-DeepONet), an extension of standard DeepONet, which leverages latent representations of high-dimensional PDE input and output functions identified with suitable autoencoders. We illustrate that L-DeepONet outperforms the ",
    "link": "http://arxiv.org/abs/2304.07599",
    "context": "Title: Learning in latent spaces improves the predictive accuracy of deep neural operators. (arXiv:2304.07599v1 [cs.LG])\nAbstract: Operator regression provides a powerful means of constructing discretization-invariant emulators for partial-differential equations (PDEs) describing physical systems. Neural operators specifically employ deep neural networks to approximate mappings between infinite-dimensional Banach spaces. As data-driven models, neural operators require the generation of labeled observations, which in cases of complex high-fidelity models result in high-dimensional datasets containing redundant and noisy features, which can hinder gradient-based optimization. Mapping these high-dimensional datasets to a low-dimensional latent space of salient features can make it easier to work with the data and also enhance learning. In this work, we investigate the latent deep operator network (L-DeepONet), an extension of standard DeepONet, which leverages latent representations of high-dimensional PDE input and output functions identified with suitable autoencoders. We illustrate that L-DeepONet outperforms the ",
    "path": "papers/23/04/2304.07599.json",
    "total_tokens": 1005,
    "translated_title": "在潜在空间学习改善了深度神经算子的预测准确性",
    "translated_abstract": "算子回归提供了一种强有力的构建描述物理系统的偏微分方程(PDEs)不变离散化仿真器的方法。神经算子特别是利用深度神经网络来逼近无限维Banach空间之间的映射。作为数据驱动模型，神经算子需要生成标记观测数据，在复杂高保真度模型的情况下，这些数据集通常是高维的，包含冗余和噪声特征，这可能会影响基于梯度的优化。将这些高维数据映射到低维潜在特征空间中可以使数据处理更方便，也可以增强学习。在这项工作中，我们研究了潜在深度算子网络(L-DeepONet)，它是标准DeepONet的扩展，利用适当的自编码器识别高维PDE输入和输出函数的潜在表示。我们证明了L-DeepONet优于传统的DeepONet模型，通过在低维潜在空间中学习，提高了对偏微分方程的精确预测能力。",
    "tldr": "该论文研究了潜在深度算子网络(L-DeepONet)，使用自编码器将高维偏微分方程输入和输出函数转换为低维潜在空间中的特征，从而提高了深度神经算子对偏微分方程的预测准确性。",
    "en_tdlr": "This paper investigates the latent deep operator network (L-DeepONet) which uses autoencoders to transform the high-dimensional inputs and outputs of partial-differential equations (PDEs) into low-dimensional latent space features to improve the accuracy of deep neural operators in predicting PDEs."
}