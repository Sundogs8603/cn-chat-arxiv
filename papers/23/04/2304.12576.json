{
    "title": "Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])",
    "abstract": "During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order",
    "link": "http://arxiv.org/abs/2304.12576",
    "context": "Title: Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])\nAbstract: During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order",
    "path": "papers/23/04/2304.12576.json",
    "total_tokens": 846,
    "translated_title": "利用高级循环和张量抽象在CPU架构上通过深度学习和HPC内核",
    "translated_abstract": "在过去的十年中，深度学习（DL）算法、编程系统和硬件已经与高性能计算（HPC）相结合。然而，DL和HPC系统的编程方法却停滞不前，依赖于高度优化、特定于平台、僵化的供应商优化库。这项工作介绍了一个框架，用于开发现代CPU架构的高效、可移植的DL和HPC内核。我们将内核开发分解为两个步骤：1）使用张量处理原语（TPP）表达计算核心：一个紧凑、多功能的2D张量运算符，2）以高级、声明性的方式表达TPP周围的逻辑循环，而确切的实例化（顺序，内存布局）则通过将TPL视为黑盒，根据优化目标和约束由自动优化器完成。",
    "tldr": "该论文介绍了一种新的方法来开发适用于现代CPU体系结构的高效、可移植的深度学习和高性能计算内核，使用高级循环和张量抽象。",
    "en_tdlr": "This paper introduces a new approach to develop efficient and portable deep learning and high-performance computing kernels for modern CPU architectures using high-level loop and tensor abstractions."
}