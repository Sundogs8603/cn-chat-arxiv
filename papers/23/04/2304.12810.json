{
    "title": "Transcending the \"Male Code\": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])",
    "abstract": "Critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (VAs). Most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. Yet, we must also interrogate how masculinities are \"coded\" into language and the assumption of \"male\" as the linguistic default: implicit masculine biases. To this end, we examined two natural language processing (NLP) data sets. We found that when gendered language was present, so were gender biases and especially masculine biases. Moreover, these biases related in nuanced ways to the NLP context. We offer a new dictionary called AVA that covers ambiguous associations between gendered language and the langua",
    "link": "http://arxiv.org/abs/2304.12810",
    "context": "Title: Transcending the \"Male Code\": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])\nAbstract: Critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (VAs). Most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. Yet, we must also interrogate how masculinities are \"coded\" into language and the assumption of \"male\" as the linguistic default: implicit masculine biases. To this end, we examined two natural language processing (NLP) data sets. We found that when gendered language was present, so were gender biases and especially masculine biases. Moreover, these biases related in nuanced ways to the NLP context. We offer a new dictionary called AVA that covers ambiguous associations between gendered language and the langua",
    "path": "papers/23/04/2304.12810.json",
    "total_tokens": 946,
    "translated_title": "超越“男性准则”：NLP语境中的隐性男性偏见",
    "translated_abstract": "关于虚拟助手（VAs）的性别偏差问题，批判性学说已经提高了人们的注意。大部分研究集中在语言中的显性偏见，尤其是针对女性、女孩、女性认同人群和性别酷儿的歧视，以及通过词向量嵌入的隐性关联；而对于男性和毒性男性，性别和性别二元分类的混为一谈，很少有基于男性和男性气概的有限模型。然而，我们还必须质询如何将男性气概“编码”到语言中及其将“男性”作为语言默认值的假设：隐性男性偏见。为此，我们调查了两个自然语言处理（NLP）数据集。我们发现当存在性别化语言时，性别偏见尤其是男性偏见也存在。此外，这些偏见与NLP上下文的关系细微且相关。我们提供了一个名为AVA的新字典，涵盖了性别化语言与语言之间的歧义关系。",
    "tldr": "研究发现，当存在性别化语言时，NLP语境中也存在着性别偏见，尤其是男性偏见。调查者提供了一个涵盖了性别化语言与语言之间歧义关系的新字典“Ava”。",
    "en_tdlr": "The research found that gender biases, especially masculine biases, exist in NLP contexts when gendered language is present. The investigators provide a new dictionary called AVA that covers ambiguous associations between gendered language and language."
}