{
    "title": "Flow Away your Differences: Conditional Normalizing Flows as an Improvement to Reweighting. (arXiv:2304.14963v1 [hep-ph])",
    "abstract": "We present an alternative to reweighting techniques for modifying distributions to account for a desired change in an underlying conditional distribution, as is often needed to correct for mis-modelling in a simulated sample. We employ conditional normalizing flows to learn the full conditional probability distribution from which we sample new events for conditional values drawn from the target distribution to produce the desired, altered distribution. In contrast to common reweighting techniques, this procedure is independent of binning choice and does not rely on an estimate of the density ratio between two distributions.  In several toy examples we show that normalizing flows outperform reweighting approaches to match the distribution of the target.We demonstrate that the corrected distribution closes well with the ground truth, and a statistical uncertainty on the training dataset can be ascertained with bootstrapping. In our examples, this leads to a statistical precision up to th",
    "link": "http://arxiv.org/abs/2304.14963",
    "context": "Title: Flow Away your Differences: Conditional Normalizing Flows as an Improvement to Reweighting. (arXiv:2304.14963v1 [hep-ph])\nAbstract: We present an alternative to reweighting techniques for modifying distributions to account for a desired change in an underlying conditional distribution, as is often needed to correct for mis-modelling in a simulated sample. We employ conditional normalizing flows to learn the full conditional probability distribution from which we sample new events for conditional values drawn from the target distribution to produce the desired, altered distribution. In contrast to common reweighting techniques, this procedure is independent of binning choice and does not rely on an estimate of the density ratio between two distributions.  In several toy examples we show that normalizing flows outperform reweighting approaches to match the distribution of the target.We demonstrate that the corrected distribution closes well with the ground truth, and a statistical uncertainty on the training dataset can be ascertained with bootstrapping. In our examples, this leads to a statistical precision up to th",
    "path": "papers/23/04/2304.14963.json",
    "total_tokens": 695,
    "translated_title": "论文标题：条件归一化流作为重加权的改进方法",
    "translated_abstract": "本文提出了一种不同于重重新加权技术的选择，用于修改分布以考虑基础条件分布的期望变化。我们使用条件归一化流来学习从中采样新事件的完整条件概率分布，以其目标分布为条件值产生所需的修改分布。与常见的重新加权技术相比，此过程不依赖于分组选择，并且不依赖于两个分布之间密度比的估计。",
    "tldr": "本论文提出了一种新的方法，使用条件归一化流来修改分布，不依赖于分组选择和密度比的估计，从而更好地匹配目标分布。",
    "en_tdlr": "This paper presents a new method to modify distributions using conditional normalizing flows, which does not rely on binning choice and density ratio estimation, thus achieving a better match to the target distribution."
}