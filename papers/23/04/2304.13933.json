{
    "title": "Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])",
    "abstract": "Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende",
    "link": "http://arxiv.org/abs/2304.13933",
    "context": "Title: Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])\nAbstract: Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende",
    "path": "papers/23/04/2304.13933.json",
    "total_tokens": 904,
    "translated_title": "机器学习模型训练过程中对表现更好的少数族群进行过采样会稍微降低不良影响，但也会降低模型精度",
    "translated_abstract": "组织越来越倾向于采用机器学习来进行员工评估。然而，人们对机器学习评估公平性的担忧也日益增加。本研究系统地对少数族裔（黑人和西班牙裔）进行了欠采样和过采样，以改变训练数据中的不良影响比率，并调查训练数据中不良影响比率如何影响机器学习模型的不良影响和准确性。我们使用工作申请人的自我报告和面试记录（N = 2,501）训练了9,702个机器学习模型来预测筛选决策。我们发现，训练数据中的不良影响与机器学习模型的不良影响呈线性相关。然而，从训练数据中消除不良影响仅稍微降低了机器学习模型的不良影响，而且也降低了模型精度。",
    "tldr": "本研究发现，在机器学习模型训练中，对表现更好的少数族裔进行过采样会稍微减少不良影响，但也会降低模型精度。",
    "en_tdlr": "This study found that oversampling higher-performing minorities during machine learning model training reduces adverse impact slightly but also reduces model accuracy."
}