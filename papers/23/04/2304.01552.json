{
    "title": "Meta-Learning with a Geometry-Adaptive Preconditioner. (arXiv:2304.01552v1 [cs.CV])",
    "abstract": "Model-agnostic meta-learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level optimization structure where the outer-loop process learns a shared initialization and the inner-loop process optimizes task-specific weights. Although MAML relies on the standard gradient descent in the inner-loop, recent studies have shown that controlling the inner-loop's gradient descent with a meta-learned preconditioner can be beneficial. Existing preconditioners, however, cannot simultaneously adapt in a task-specific and path-dependent way. Additionally, they do not satisfy the Riemannian metric condition, which can enable the steepest descent learning with preconditioned gradient. In this study, we propose Geometry-Adaptive Preconditioned gradient descent (GAP) that can overcome the limitations in MAML; GAP can efficiently meta-learn a preconditioner that is dependent on task-specific parameters, and its preconditioner can be shown to be a Riemannian metric. Thanks to ",
    "link": "http://arxiv.org/abs/2304.01552",
    "context": "Title: Meta-Learning with a Geometry-Adaptive Preconditioner. (arXiv:2304.01552v1 [cs.CV])\nAbstract: Model-agnostic meta-learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level optimization structure where the outer-loop process learns a shared initialization and the inner-loop process optimizes task-specific weights. Although MAML relies on the standard gradient descent in the inner-loop, recent studies have shown that controlling the inner-loop's gradient descent with a meta-learned preconditioner can be beneficial. Existing preconditioners, however, cannot simultaneously adapt in a task-specific and path-dependent way. Additionally, they do not satisfy the Riemannian metric condition, which can enable the steepest descent learning with preconditioned gradient. In this study, we propose Geometry-Adaptive Preconditioned gradient descent (GAP) that can overcome the limitations in MAML; GAP can efficiently meta-learn a preconditioner that is dependent on task-specific parameters, and its preconditioner can be shown to be a Riemannian metric. Thanks to ",
    "path": "papers/23/04/2304.01552.json",
    "total_tokens": 855,
    "tldr": "本研究提出了一种名为 GAP 的新算法，利用元学习方法高效地学习依赖于任务参数的预条件器，从而取得更好的优化效果。",
    "en_tdlr": "This study proposes a new algorithm named GAP, which efficiently learns a task-dependent preconditioner using meta-learning, achieving better optimization performance."
}