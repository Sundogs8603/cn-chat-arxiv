{
    "title": "Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])",
    "abstract": "Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics",
    "link": "http://arxiv.org/abs/2304.02911",
    "context": "Title: Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])\nAbstract: Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics",
    "path": "papers/23/04/2304.02911.json",
    "total_tokens": 1025,
    "translated_title": "深度神经网络的重尾部正则化",
    "translated_abstract": "深度神经网络成功和显著的泛化能力背后的原因仍然是一个巨大的挑战。从随机矩阵理论得到的最新信息，特别是涉及深度神经网络中权重矩阵的谱分析的信息，为解决这个问题提供了有价值的线索。一个关键发现是，神经网络的泛化性能与其权重矩阵的谱的重尾程度相关。为了利用这一发现，我们介绍了一种新的正则化技术，称为重尾部正则化，通过正则化明确提倡权重矩阵中更重的重尾谱。首先，我们采用加权阿尔法和稳定秩作为惩罚项，两者都可微分，从而可以直接计算它们的梯度。为了避免过度正则化，我们介绍了两种惩罚函数的变体。然后，采用贝叶斯统计视角，我们提出了重尾部正则化的概率解释，使我们能够将其效果理解为权重矩阵的先验。在多个基准数据集上的实证评估表明，与标准正则化技术相比，我们的方法明显提高了泛化性能。",
    "tldr": "本文介绍了一种名为重尾部正则化的技术，在深度神经网络中通过明确提倡更重的重尾谱来提高泛化性能。与标准正则化技术相比，该方法在基准数据集上实现了显着的改进。"
}