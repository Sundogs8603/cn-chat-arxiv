{
    "title": "Prompt-Learning for Cross-Lingual Relation Extraction. (arXiv:2304.10354v1 [cs.CL])",
    "abstract": "Relation Extraction (RE) is a crucial task in Information Extraction, which entails predicting relationships between entities within a given sentence. However, extending pre-trained RE models to other languages is challenging, particularly in real-world scenarios where Cross-Lingual Relation Extraction (XRE) is required. Despite recent advancements in Prompt-Learning, which involves transferring knowledge from Multilingual Pre-trained Language Models (PLMs) to diverse downstream tasks, there is limited research on the effective use of multilingual PLMs with prompts to improve XRE. In this paper, we present a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. To evaluate its effectiveness, we design and implement several prompt templates, including hard, soft, and hybrid prompts, and empirically test their performance on competitive multilingual PLMs, specifically mBART. Our extensive experiments, conducted on the low-resource ACE05 benchmark across multiple language",
    "link": "http://arxiv.org/abs/2304.10354",
    "context": "Title: Prompt-Learning for Cross-Lingual Relation Extraction. (arXiv:2304.10354v1 [cs.CL])\nAbstract: Relation Extraction (RE) is a crucial task in Information Extraction, which entails predicting relationships between entities within a given sentence. However, extending pre-trained RE models to other languages is challenging, particularly in real-world scenarios where Cross-Lingual Relation Extraction (XRE) is required. Despite recent advancements in Prompt-Learning, which involves transferring knowledge from Multilingual Pre-trained Language Models (PLMs) to diverse downstream tasks, there is limited research on the effective use of multilingual PLMs with prompts to improve XRE. In this paper, we present a novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. To evaluate its effectiveness, we design and implement several prompt templates, including hard, soft, and hybrid prompts, and empirically test their performance on competitive multilingual PLMs, specifically mBART. Our extensive experiments, conducted on the low-resource ACE05 benchmark across multiple language",
    "path": "papers/23/04/2304.10354.json",
    "total_tokens": 770,
    "translated_title": "跨语言关系抽取中的Prompt学习",
    "translated_abstract": "关系抽取(RE)是信息提取中的关键任务，涉及预测给定句子中实体之间的关系。然而，将预先训练好的RE模型扩展到其他语言是具有挑战性的，特别是在需要进行跨语言关系抽取(XRE)的实际场景中。本文介绍了一种基于Prompt调优的新型XRE算法，称为Prompt-XRE。为了评估其有效性，我们设计并实现了几个Prompt模板，包括硬Prompt、软Prompt和混合Prompt，并在竞争性多语种PLM（特别是mBART）上进行了实证测试。我们在低资源ACE05基准测试上进行了大量实验，涵盖多种语言。",
    "tldr": "本文提出了一种新型的XRE算法Prompt-XRE，基于Prompt调优方法，它能够有效提高多语种预训练语言模型在跨语言环境下的表现。"
}