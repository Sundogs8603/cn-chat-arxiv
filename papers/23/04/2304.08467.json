{
    "title": "Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)",
    "abstract": "Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of \"gist\" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.",
    "link": "http://arxiv.org/abs/2304.08467",
    "context": "Title: Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)\nAbstract: Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of \"gist\" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.",
    "path": "papers/23/04/2304.08467.json",
    "total_tokens": 930,
    "translated_title": "学习使用要点标记压缩提示语",
    "translated_abstract": "提示是利用语言模型的多任务能力的主要方式，但是提示占据了输入上下文窗口中宝贵的空间，重复编码相同的提示在计算上是低效的。微调和蒸馏方法可以实现语言模型的专门化，但需要为每个任务重新训练模型。为了完全避免这种权衡，我们提出了gist，它训练一个语言模型将提示压缩成更小的“要点”标记集，可以用于计算效率的缓存和重用。通过简单地修改Transformer的注意力掩码，可以在没有额外成本的情况下对gist模型进行训练，从而实现对提示的高达26倍的压缩，从而减少高达40％的FLOPs、4.2％的墙时速度提升，并节省存储空间，同时最小化输出质量损失。",
    "tldr": "该论文提出了一种名为\"gisting\"的方法，通过训练语言模型将提示压缩为更小的\"要点\"标记集，以提高计算效率。通过这种方法，可以实现高达26倍的提示压缩，减少40％的FLOPs、4.2％的墙时速度提升，并节省存储空间，同时最小化输出质量损失。"
}