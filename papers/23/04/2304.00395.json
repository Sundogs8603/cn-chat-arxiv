{
    "title": "Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis. (arXiv:2304.00395v1 [cs.LG])",
    "abstract": "Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper boun",
    "link": "http://arxiv.org/abs/2304.00395",
    "context": "Title: Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis. (arXiv:2304.00395v1 [cs.LG])\nAbstract: Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper boun",
    "path": "papers/23/04/2304.00395.json",
    "total_tokens": 1074,
    "translated_title": "通过相似性结构解析对比学习机制：理论分析",
    "translated_abstract": "对比学习是一种有效的自监督表示学习方法。虽然近期的研究在理论上对对比学习有了一定的了解，但对于如何表征学习表示的聚类仍然有限。本文旨在从理论角度阐明这种聚类的特征。为此，我们考虑一种基于核的对比学习框架，称为核对比学习（KCL），核函数在将我们的理论结果应用于其他框架时起重要作用。我们利用统计依赖观点引入一个学习表示的相似性结构的公式。我们通过这个公式研究了基于核的对比损失的理论性质。我们首先证明这个公式表征了利用核对比学习框架学习的表示结构。我们证明了一个新的上界，对于负样本的边际分布有一个温和的条件，期望对比损失受到限制。此外，我们还确定了基于核的对比损失是一种新的信息论下界的特例，这促使我们开发一个新的目标，可以进一步约束学习表示在输入的亲密性和不同样本之间的可分性方面。最后，我们在几个基准测试上进行了实验，这些实验支持我们的理论发现，并表明了所提出的目标的有效性。",
    "tldr": "通过一个新的公式，本文理论分析了基于核的对比学习损失的特点，证明了它能描述学习表示的结构和表现，提供一个新的限制方法，并在多个基准测试中验证其有效性。",
    "en_tdlr": "This paper provides a theoretical analysis of the characteristics of kernel-based contrastive learning loss via a new formula, which can describe the structure and performance of learned representations, offer a novel constraint method and validate its effectiveness on multiple benchmarks."
}