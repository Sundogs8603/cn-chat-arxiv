{
    "title": "Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])",
    "abstract": "Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\\% of the baseline without compromising the Pairwise Comparis",
    "link": "http://arxiv.org/abs/2304.05430",
    "context": "Title: Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])\nAbstract: Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\\% of the baseline without compromising the Pairwise Comparis",
    "path": "papers/23/04/2304.05430.json",
    "total_tokens": 825,
    "translated_title": "适用于异构特征的迁移学习，实现高效的张量程序生成",
    "translated_abstract": "改进张量程序生成需要在目标硬件上为给定程序搜索各种可能的程序转换组合，以优化张量程序的执行。由于庞大的搜索空间和指数级别的变换组合，自动调整张量程序的生成变得更加困难，尤其是当需要面对异构的目标时。本研究旨在通过学习联合神经网络和硬件特征，并将它们转移到新的目标硬件上，从而解决这些问题。我们广泛研究现有的最先进数据集TenSet，在测试集分割策略上进行比较分析，并提出优化数据集的方法。我们采用注意力启发式方法，为调整张量程序提供支持，使它们能够融入神经网络和硬件特定特征。我们的方法能够将数据集的基线精简高达45％，而不会影响Pairwise Comparis。",
    "tldr": "本研究提出了适用于异构特征的迁移学习方法，在新的目标硬件上通过学习联合神经网络和硬件特征，解决了张量程序生成的自动调整问题。"
}