{
    "title": "Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])",
    "abstract": "The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.",
    "link": "http://arxiv.org/abs/2304.11267",
    "context": "Title: Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])\nAbstract: The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.",
    "path": "papers/23/04/2304.11267.json",
    "total_tokens": 910,
    "translated_title": "速度即一切：通过GPU-aware优化在设备上加速大型扩散模型",
    "translated_abstract": "基础模型的快速发展和应用已经彻底改变了人工智能领域。大型扩散模型因其生成逼真图像和支持各种任务的能力而受到了重视。这些模型在设备上的部署带来了许多好处，比如降低服务器成本、离线功能和改善用户隐私。然而，在设备上共同的大型扩散模型具有超过10亿的参数，由于设备的受限计算和内存资源存在挑战。我们提出了一系列大型扩散模型实现优化，以在配备GPU的移动设备上实现迄今为止最快的推论延迟（对于一个512x512的图像，在三星S23 Ultra上的\"稳定扩散1.4\"下，进行20次迭代的情况下，无需int8量化，推论的延迟小于12秒）。这些优化扩大了生成性人工智能的适用范围并改善了各种设备上的整体用户体验。",
    "tldr": "该研究提出一系列通过GPU-aware优化大型扩散模型的方法，实现在配备GPU的移动设备上极快的推论延迟，扩大了生成性人工智能的适用范围并改善了用户体验。",
    "en_tdlr": "This study presents a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date on GPU-equipped mobile devices, broadening the applicability of generative AI and improving the overall user experience across a wide range of devices."
}