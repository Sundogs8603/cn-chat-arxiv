{
    "title": "Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning. (arXiv:2304.04824v1 [cs.LG])",
    "abstract": "Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency.",
    "link": "http://arxiv.org/abs/2304.04824",
    "context": "Title: Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning. (arXiv:2304.04824v1 [cs.LG])\nAbstract: Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency.",
    "path": "papers/23/04/2304.04824.json",
    "total_tokens": 858,
    "translated_title": "基于梯度的不确定性归因于可解释的贝叶斯深度学习",
    "translated_abstract": "深度学习模型所作出的预测容易受到数据扰动、对抗攻击和超出分布范围的输入的影响。为了构建一个可信赖的AI系统，准确量化预测的不确定性至关重要。虽然当前的工作重点是提高不确定性量化的准确性和效率，但有必要确定不确定性源并采取措施减轻对预测的影响。因此，我们提出了开发可解释和可操作的贝叶斯深度学习方法来不仅进行准确的不确定性量化，而且解释不确定性、确定其来源并提出减轻不确定性影响的策略。具体而言，我们引入了一种基于梯度的不确定性归因方法来确定最具问题性的输入区域，从而导致预测的不确定性。与现有方法相比，所提出的UA-Backprop方法具有竞争性的准确性、松弛的假设和高效性。",
    "tldr": "本论文提出了一个基于梯度的不确定性归因方法来确定导致预测的不确定性的最具问题性的输入区域，从而实现可解释和可操作的贝叶斯深度学习。",
    "en_tdlr": "This paper proposes a gradient-based uncertainty attribution method to identify problematic input regions that contribute to prediction uncertainties, achieving explainable and actionable Bayesian deep learning."
}