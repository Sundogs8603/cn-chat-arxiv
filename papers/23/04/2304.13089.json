{
    "title": "Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])",
    "abstract": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences expl",
    "link": "http://arxiv.org/abs/2304.13089",
    "context": "Title: Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])\nAbstract: Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences expl",
    "path": "papers/23/04/2304.13089.json",
    "total_tokens": 973,
    "translated_title": "目标很重要：理解自监督目标对视觉Transformer表示形式的影响",
    "translated_abstract": "联合嵌入学习(SimCLR、MoCo、DINO等)和重建学习(BEiT、SimMIM、MAE等)是自监督学习视觉transformers的两种主要范例，但它们在转换性能上有很大差异。本文旨在通过分析这些目标对所学表示的结构和可转移性的影响来解释这些差异。我们的分析揭示了重建学习特征与联合嵌入学习特征相比较显著的不同，并且即使在不同架构下也能通过类似目标来训练。这些差异早在网络的早期就产生了，并且主要受到注意力和归一化层的驱动。我们发现，联合嵌入特征产生更好的线性探测转移分类，因为不同的目标驱动不同的信息分布和不变性在所学表示中。本文分析提供了自监督学习目标设计的见解，并为未来的研究提供了方向。",
    "tldr": "本文分析了联合嵌入学习和重建学习两种自监督学习视觉transformers的目标对所学表示的影响及其转换性能差异，发现联合嵌入学习特征更利于线性探测转移分类，进而提供了该领域未来研究的方向。"
}