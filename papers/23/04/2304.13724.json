{
    "title": "GPU accelerated matrix factorization of large scale data using block based approach. (arXiv:2304.13724v1 [cs.LG])",
    "abstract": "Matrix Factorization (MF) on large scale data takes substantial time on a Central Processing Unit (CPU). While Graphical Processing Unit (GPU)s could expedite the computation of MF, the available memory on a GPU is finite. Leveraging GPUs require alternative techniques that allow not only parallelism but also address memory limitations. Synchronization between computation units, isolation of data related to a computational unit, sharing of data between computational units and identification of independent tasks among computational units are some of the challenges while leveraging GPUs for MF. We propose a block based approach to matrix factorization using Stochastic Gradient Descent (SGD) that is aimed at accelerating MF on GPUs. The primary motivation for the approach is to make it viable to factorize extremely large data sets on limited hardware without having to compromise on results. The approach addresses factorization of large scale data by identifying independent blocks, each of",
    "link": "http://arxiv.org/abs/2304.13724",
    "context": "Title: GPU accelerated matrix factorization of large scale data using block based approach. (arXiv:2304.13724v1 [cs.LG])\nAbstract: Matrix Factorization (MF) on large scale data takes substantial time on a Central Processing Unit (CPU). While Graphical Processing Unit (GPU)s could expedite the computation of MF, the available memory on a GPU is finite. Leveraging GPUs require alternative techniques that allow not only parallelism but also address memory limitations. Synchronization between computation units, isolation of data related to a computational unit, sharing of data between computational units and identification of independent tasks among computational units are some of the challenges while leveraging GPUs for MF. We propose a block based approach to matrix factorization using Stochastic Gradient Descent (SGD) that is aimed at accelerating MF on GPUs. The primary motivation for the approach is to make it viable to factorize extremely large data sets on limited hardware without having to compromise on results. The approach addresses factorization of large scale data by identifying independent blocks, each of",
    "path": "papers/23/04/2304.13724.json",
    "total_tokens": 940,
    "translated_title": "使用基于块的方法的GPU加速矩阵分解以处理大规模数据",
    "translated_abstract": "在中央处理器上进行大规模数据的矩阵分解需要相当长的时间，而图形处理器可以加快矩阵分解的计算速度，但GPU上可用的内存是有限的。利用GPU需要替代技术，不仅可以并行处理，还可以解决内存限制。在利用GPU进行矩阵分解时，计算单元之间的同步、与计算单元相关的数据的隔离、计算单元之间数据的共享以及识别计算单元之间的独立任务等是一些挑战。因此，我们提出了一种使用随机梯度下降的基于块的矩阵分解方法，旨在加速GPU上的矩阵分解。该方法的主要动机是，在有限的硬件上处理极大规模的数据集，而不必在结果上妥协。该方法通过识别独立块来解决大规模数据的矩阵分解，并使用随机梯度下降来优化它们的分解。我们通过将其与各种数据集上的最先进的方法进行比较，展示了我们方法的有效性。",
    "tldr": "该论文提出了一种使用基于块的方法的GPU加速矩阵分解的方案，旨在加快对大规模数据的分解并避免内存的限制。",
    "en_tdlr": "This paper proposes a block based approach to GPU accelerated matrix factorization for processing large scale data, aimed at speeding up the process and avoiding memory limitations. The approach identifies independent blocks for factorization and optimizes them using stochastic gradient descent."
}