{
    "title": "Learning Stability Attention in Vision-based End-to-end Driving Policies. (arXiv:2304.02733v1 [cs.RO])",
    "abstract": "Modern end-to-end learning systems can learn to explicitly infer control from perception. However, it is difficult to guarantee stability and robustness for these systems since they are often exposed to unstructured, high-dimensional, and complex observation spaces (e.g., autonomous driving from a stream of pixel inputs). We propose to leverage control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduce stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. We also present an uncertainty propagation technique that is tightly integrated into att-CLFs. We demonstrate the effectiveness of att-CLFs via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle.",
    "link": "http://arxiv.org/abs/2304.02733",
    "context": "Title: Learning Stability Attention in Vision-based End-to-end Driving Policies. (arXiv:2304.02733v1 [cs.RO])\nAbstract: Modern end-to-end learning systems can learn to explicitly infer control from perception. However, it is difficult to guarantee stability and robustness for these systems since they are often exposed to unstructured, high-dimensional, and complex observation spaces (e.g., autonomous driving from a stream of pixel inputs). We propose to leverage control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduce stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. We also present an uncertainty propagation technique that is tightly integrated into att-CLFs. We demonstrate the effectiveness of att-CLFs via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle.",
    "path": "papers/23/04/2304.02733.json",
    "total_tokens": 911,
    "translated_title": "基于视觉端到端驾驶策略的学习稳定性注意力",
    "translated_abstract": "现代端到端学习系统可以从感知中学习控制。然而，由于这些系统往往接收到未经结构化、高维度和复杂的观测空间，如从像素输入流中进行自主驾驶，因此保证这些系统的稳定性和鲁棒性很困难。我们提出利用控制李亚普诺夫函数（CLFs）为端到端基于视觉的策略配备稳定属性，并引入CLFs中的稳定性注意力（att-CLFs）来处理环境变化并提高学习的灵活性。我们还提出了一种紧密集成到att-CLFs中的不确定性传播技术。我们在真实环境和真实全尺寸自主车辆上，通过与经典CLFs、模型预测控制和香草端到端学习的比较，展示了att-CLFs的有效性。",
    "tldr": "这篇论文提出了一种利用控制李亚普诺夫函数（CLFs）为端到端基于视觉的策略配备稳定属性，并引入CLFs中的稳定性注意力（att-CLFs）来处理环境变化并提高学习灵活性的方法。",
    "en_tdlr": "This paper proposes a method that leverages control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduces stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. The effectiveness of att-CLFs is demonstrated via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle."
}