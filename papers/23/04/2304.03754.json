{
    "title": "Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering. (arXiv:2304.03754v1 [cs.CL])",
    "abstract": "Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pre-trained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., ``what is someone doing...'') and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like ``why is someone doing ...''. Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., ``score a goal'' triggers ``soccer player kicking ball'') by prompting LM w",
    "link": "http://arxiv.org/abs/2304.03754",
    "context": "Title: Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering. (arXiv:2304.03754v1 [cs.CL])\nAbstract: Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pre-trained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., ``what is someone doing...'') and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like ``why is someone doing ...''. Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., ``score a goal'' triggers ``soccer player kicking ball'') by prompting LM w",
    "path": "papers/23/04/2304.03754.json",
    "total_tokens": 1051,
    "translated_title": "语言模型是零样本视频问答的因果知识提取器",
    "translated_abstract": "因果视频问答（CVidQA）不仅查询相关或时间关系，还查询视频中的因果关系。现有的问题生成方法在阅读理解数据集上预先训练问题生成（QG）系统，输入为文本描述。但是，QG模型只学习提出相关问题（例如，“某人在做什么...”），导致转换到CVidQA的关联知识差，CVidQA重点关注“某人为什么这样做...”这样的因果问题。在观察到这一点后，我们提议利用因果知识生成问题-答案对，并提出了一种新颖的框架，即从语言模型中提取因果知识（CaKE-LM），利用语言模型中的因果常识知识来处理CVidQA。为了从LM中提取知识，CaKE-LM生成包含两个事件的因果问题（例如，“得分”触发“足球运动员踢球”），通过提示LM来提取因果常识知识作为这两个事件之间的关系，形成问题-答案对。在TVQA +数据集上的实验结果表明，我们的方法在CVidQA任务上明显优于现有方法。",
    "tldr": "本文提出了一种新颖的框架CaKE-LM，它利用语言模型中的因果常识知识来处理因果视频问答（CVidQA）任务，能够从语言模型中提取因果知识帮助生成问题-答案对，相较于现有方法在CVidQA任务上有着显著的性能优势。",
    "en_tdlr": "This paper proposes a novel framework CaKE-LM, which leverages causal commonsense knowledge from language models to tackle causal video question answering (CVidQA) tasks, and can extract causal knowledge from language models to generate question-answer pairs. Compared to existing methods, it significantly outperforms them on CVidQA tasks."
}