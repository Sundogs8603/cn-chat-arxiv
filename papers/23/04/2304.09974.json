{
    "title": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])",
    "abstract": "Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long p",
    "link": "http://arxiv.org/abs/2304.09974",
    "context": "Title: SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])\nAbstract: Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long p",
    "path": "papers/23/04/2304.09974.json",
    "total_tokens": 1051,
    "translated_title": "论文标题：SurgicalGPT：端到端的语言-视觉GPT模型用于手术中的视觉问答",
    "translated_abstract": "基于GPT的大型语言模型（LLM）的进展正在彻底改变自然语言处理，并在各个领域的使用呈指数级增长。这些自回归LLM可以生成长而连贯的段落，但是在需要同时处理视觉及语言信息的视觉问答（VQA）任务中，通常需要双向注意力或融合技术来捕获多种模态的环境信息。由于GPT本身不支持视觉标记处理，为了充分利用GPT模型在机器人手术VQA中的优势，我们设计了一种端到端可训练的语言-视觉GPT（LV-GPT）模型，将GPT2模型扩展为包括视觉输入（图像）。所提出的LV-GPT包括功能提取器（视觉标记器）和视觉标记嵌入（标记类型和姿态）。鉴于GPT模型中单向关注的限制以及生成连贯长段落的能力，我们提出了一种新方法，通过微调GPT模型的注意力机制，加入双向关注，从而更好地理解手术VQA任务中的视觉环境。",
    "tldr": "本文提出了一种端到端的语言-视觉GPT模型，增强GPT2模型以包括视觉输入，然后微调注意力机制，以在手术VQA任务中更好地理解视觉环境。",
    "en_tdlr": "This paper proposes an end-to-end trainable Language-Vision GPT (LV-GPT) model to exploit advancements in GPT models for visual question answering (VQA) in robot-assisted surgery. The proposed LV-GPT expands the GPT2 model to include vision input and incorporates a feature extractor and vision token embedding. The approach fine-tunes the attention mechanisms of the GPT model to incorporate bidirectional attention for better visual context understanding in surgical VQA tasks."
}