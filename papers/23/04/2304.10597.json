{
    "title": "Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])",
    "abstract": "Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug",
    "link": "http://arxiv.org/abs/2304.10597",
    "context": "Title: Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])\nAbstract: Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug",
    "path": "papers/23/04/2304.10597.json",
    "total_tokens": 788,
    "translated_title": "Text2Seg: 通过文本引导视觉基础模型的遥感图像语义分割",
    "translated_abstract": "最近，基础模型（FMs），如 GPT-4 和 LLaMA，在零样本学习方案中表现出色，吸引了大量关注。类似地，在视觉学习领域，Grounding DINO 和 Segment Anything Model（SAM）等模型在开放式检测和实例分割任务中展现了显著的进步。本研究专注于遥感领域，其中图片与传统场景中的图片明显不同。我们开发了一个流程，利用多个 FMs，以文本提示为指导，促进遥感图像语义分割任务，我们将其称为 Text2Seg 。该管道在多个广泛使用的遥感数据集上进行基准测试，并提供初步结果以证明其有效性。",
    "tldr": "本文介绍了一种名为 Text2Seg 的遥感图像语义分割流程，利用多个基础模型和文本引导，取得了初步成果。",
    "en_tdlr": "This paper introduces a remote sensing image semantic segmentation process, Text2Seg, that leverages multiple foundation models and text guidance, and presents preliminary results on several widely-used datasets."
}