{
    "title": "Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])",
    "abstract": "We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\\mathcal{G}^\\dagger\\,:\\, \\mathcal{U}\\to \\mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\\phi(u_i), \\varphi(v_i)$ of input/output functions $v_i=\\mathcal{G}^\\dagger(u_i)$ ($i=1,\\ldots,N$), and the measurement operators $\\phi\\,:\\, \\mathcal{U}\\to \\mathbb{R}^n$ and $\\varphi\\,:\\, \\mathcal{V} \\to \\mathbb{R}^m$ are linear. Writing $\\psi\\,:\\, \\mathbb{R}^n \\to \\mathcal{U}$ and $\\chi\\,:\\, \\mathbb{R}^m \\to \\mathcal{V}$ for the optimal recovery maps associated with $\\phi$ and $\\varphi$, we approximate $\\mathcal{G}^\\dagger$ with $\\bar{\\mathcal{G}}=\\chi \\circ \\bar{f} \\ci",
    "link": "http://arxiv.org/abs/2304.13202",
    "context": "Title: Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])\nAbstract: We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\\mathcal{G}^\\dagger\\,:\\, \\mathcal{U}\\to \\mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\\phi(u_i), \\varphi(v_i)$ of input/output functions $v_i=\\mathcal{G}^\\dagger(u_i)$ ($i=1,\\ldots,N$), and the measurement operators $\\phi\\,:\\, \\mathcal{U}\\to \\mathbb{R}^n$ and $\\varphi\\,:\\, \\mathcal{V} \\to \\mathbb{R}^m$ are linear. Writing $\\psi\\,:\\, \\mathbb{R}^n \\to \\mathcal{U}$ and $\\chi\\,:\\, \\mathbb{R}^m \\to \\mathcal{V}$ for the optimal recovery maps associated with $\\phi$ and $\\varphi$, we approximate $\\mathcal{G}^\\dagger$ with $\\bar{\\mathcal{G}}=\\chi \\circ \\bar{f} \\ci",
    "path": "papers/23/04/2304.13202.json",
    "total_tokens": 1141,
    "translated_title": "核方法在算子学习中表现竞争力",
    "translated_abstract": "我们提出了一个基于核的算子学习框架，并提供了先验误差分析和与流行的神经网络方法（如Deep Operator Net（DeepONet）[Lu et al.]和Fourier神经算子（FNO）[Li et al.]）的全面数字比较。我们考虑目标算子$\\mathcal{G}^\\dagger:\\mathcal{U}\\to\\mathcal{V}$的输入/输出空间是再生核希尔伯特空间（RKHS）的情况，数据以输入/输出函数的部分观测$\\varphi(v_i),\\phi(u_i)$的形式出现，其中$v_i=\\mathcal{G}^\\dagger(u_i)$（$i=1,\\ldots,N$），测量算子$\\varphi:\\mathcal{V}\\to\\mathbb{R}^m$和$\\phi:\\mathcal{U}\\to\\mathbb{R}^n$是线性的。在写$\\psi:\\mathbb{R}^n\\to\\mathcal{U}$和$\\chi:\\mathbb{R}^m\\to\\mathcal{V}$作为与$\\phi$和$\\varphi$相关的最佳恢复映射时，我们使用$\\bar{f}$ 核映射 $L^2(\\mathcal{U},\\mathbb{R}^n)$ 定义一个$k$ 类型的最小二乘模型， 然后用 $\\bar{\\mathcal{G}}=\\chi\\circ\\bar{f}\\circ\\psi$ 来近似$\\mathcal{G}^\\dagger$。 我们的分析涉及多个例子，包括常见的偏微分方程的算子近似，结果表明在多种设置下核方法都是一种具有竞争力的算子学习方法。",
    "tldr": "本文提出了一个核方法算子学习框架，在对多组数据进行全面比较后，结果表明该方法在多种设置下都是一种具有竞争力的算子学习方法。",
    "en_tdlr": "This study proposes a kernel-based framework for operator learning, and comprehensively compares it with popular neural net approaches. The results show that the proposed method is competitive for operator learning in various settings."
}