{
    "title": "Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation. (arXiv:2304.01541v1 [stat.ML])",
    "abstract": "Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\\varepsilon, \\delta)$-differential privacy (DP) constraints. We show that in order to achieve the optimal error under $(\\varepsilon, \\delta)$-DP, it is sufficient for each client to send $\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$ bits for FL and $\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$ bits for FA to the server, where $n$ is the number of participating clients. Without compression, each client needs $O(d)$ bits and $\\log d$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime $ n \\min\\left(\\varepsilon, \\va",
    "link": "http://arxiv.org/abs/2304.01541",
    "context": "Title: Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation. (arXiv:2304.01541v1 [stat.ML])\nAbstract: Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\\varepsilon, \\delta)$-differential privacy (DP) constraints. We show that in order to achieve the optimal error under $(\\varepsilon, \\delta)$-DP, it is sufficient for each client to send $\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$ bits for FL and $\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$ bits for FA to the server, where $n$ is the number of participating clients. Without compression, each client needs $O(d)$ bits and $\\log d$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime $ n \\min\\left(\\varepsilon, \\va",
    "path": "papers/23/04/2304.01541.json",
    "total_tokens": 1185,
    "translated_title": "通过压缩实现隐私放大：在分布式均值估计中实现最优隐私-精度-通信权衡",
    "translated_abstract": "隐私和通信约束是联合学习（FL）和分析（FA）中的两个主要瓶颈。我们研究了在联合通信和$(\\varepsilon, \\delta)$-差分隐私（DP）约束下平均值和频率估计（FL和FA的标准模型）的最优准确性。我们展示了为了在$(\\varepsilon, \\delta)$-DP下达到最优误差，每个客户端只需要向服务器发送$\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$比特的FL问题和$\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$比特的FA问题。如果没有压缩，每个客户机需要$O(d)$比特和$\\log d$比特来解决平均值估计和频率估计问题（其中$d$对应于FL中可训练参数的数量或FA中域的大小），这意味着我们可以获得在$n\\min\\left(\\varepsilon,\\varepsilon^2\\right)$的区间中获得显著的节省。",
    "tldr": "本论文研究了在通信和差分隐私约束下，平均值和频率估计的最优准确性，证明每个客户端只需发送$\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$比特的FL问题和$\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$比特的FA问题即可实现最优误差，从而在联合学习和分析中实现了隐私、精确性和通信的最优权衡。",
    "en_tdlr": "This paper studies the optimal accuracy of mean and frequency estimation under joint communication and $(\\varepsilon, \\delta)$-differential privacy (DP) constraints, and proves that it is sufficient for each client to send $\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$ bits for FL and $\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$ bits for FA to achieve the optimal error, achieving the optimal trade-off between privacy, accuracy, and communication in federated learning and analytics."
}