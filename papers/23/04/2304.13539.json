{
    "title": "Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])",
    "abstract": "Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.",
    "link": "http://arxiv.org/abs/2304.13539",
    "context": "Title: Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])\nAbstract: Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.",
    "path": "papers/23/04/2304.13539.json",
    "total_tokens": 906,
    "translated_title": "张量分解用于神经网络模型简化：一项综述",
    "translated_abstract": "现代神经网络在计算机视觉和自然语言处理等领域取得了革命性的进展。它们被广泛用于解决复杂的计算机视觉任务和自然语言处理任务，如图像分类、图像生成和机器翻译。大多数最先进的神经网络都是过度参数化的，需要高昂的计算成本。一种直接的解决方案是使用不同的张量分解方法将网络的层替换为其低秩张量近似。本文综述了六种张量分解方法并阐述了它们在卷积神经网络、循环神经网络和Transformer的模型参数压缩中的能力。一些压缩模型的准确性甚至可以高于原始版本。评估表明，张量分解可以在模型大小、运行时间和能量消耗方面实现显著降低，并且非常适合在边缘设备上实现神经网络。",
    "tldr": "本文综述了六种张量分解方法，并讨论了将神经网络层替换为低秩张量逼近的方案。实验结果表明这种方法能够显著降低模型大小、运行时间和能量消耗，并且适合于在边缘设备上实现神经网络。",
    "en_tdlr": "This paper reviews six tensor decomposition methods and discusses the approach of replacing layers of neural networks with low-rank tensor approximations. The experimental results show that this technique can significantly reduce the model size, run-time, and energy consumption, and is suitable for implementing neural networks on edge devices."
}