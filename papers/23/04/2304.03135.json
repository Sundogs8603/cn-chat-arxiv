{
    "title": "VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision. (arXiv:2304.03135v1 [cs.CV])",
    "abstract": "Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection an",
    "link": "http://arxiv.org/abs/2304.03135",
    "context": "Title: VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision. (arXiv:2304.03135v1 [cs.CV])\nAbstract: Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection an",
    "path": "papers/23/04/2304.03135.json",
    "total_tokens": 1114,
    "translated_title": "VLPD: 基于视觉-语义自监督的上下文感知行人检测",
    "translated_abstract": "在城市场景中准确检测行人对于自动驾驶或视频监控等现实应用非常重要。然而，混淆的人类样式物体常常导致错误的检测，而小尺度或严重遮挡的行人由于其不寻常的外观往往被忽略。为了解决这些挑战，仅考虑对象区域是不够的，因此如何充分利用更明确和语义化的上下文成为一个关键问题。同时，先前的上下文感知行人检测器要么只学习具有视觉线索的潜在上下文，要么需要繁琐的注释来获取明确和语义上下文。因此，我们在本文中提出了一种新颖的视觉-语义自监督的上下文感知行人检测 (VLPD) 方法，以在不使用额外注释的情况下明确建模语义上下文。首先，我们提出了一种自监督的视觉-语义 (VLS) 分割方法，该方法学习了既有监督的行人检测，又有语义分割，而无需手动注释。然后，我们开发了一种上下文自适应模块 (CAM) 来整合所学习的 VLS 特征，这显著提高了行人检测性能。广泛的实验表明，我们的方法在 Caltech、KITTI 和 Citypersons 等基准数据集上优于现有的最先进方法。",
    "tldr": "本文提出了一种基于视觉-语义自监督的上下文感知行人检测方法，通过学习明确的语义上下文来解决混淆的人类样式物体和小尺度或严重遮挡的行人常常导致错误检测的问题。",
    "en_tdlr": "This paper proposes a novel approach for context-aware pedestrian detection via Vision-Language semantic self-supervision, which models explicitly semantic contexts without any extra annotations to address the challenges of detecting confusing human-like objects and missed small scale or heavily occluded pedestrians."
}