{
    "title": "PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels. (arXiv:2304.00047v1 [cs.LG])",
    "abstract": "Allowing organizations to share their data for training of machine learning (ML) models without unintended information leakage is an open problem in practice. A promising technique for this still-open problem is to train models on the encoded data. Our approach, called Privately Encoded Open Datasets with Public Labels (PEOPL), uses a certain class of randomly constructed transforms to encode sensitive data. Organizations publish their randomly encoded data and associated raw labels for ML training, where training is done without knowledge of the encoding realization. We investigate several important aspects of this problem: We introduce information-theoretic scores for privacy and utility, which quantify the average performance of an unfaithful user (e.g., adversary) and a faithful user (e.g., model developer) that have access to the published encoded data. We then theoretically characterize primitives in building families of encoding schemes that motivate the use of random deep neura",
    "link": "http://arxiv.org/abs/2304.00047",
    "context": "Title: PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels. (arXiv:2304.00047v1 [cs.LG])\nAbstract: Allowing organizations to share their data for training of machine learning (ML) models without unintended information leakage is an open problem in practice. A promising technique for this still-open problem is to train models on the encoded data. Our approach, called Privately Encoded Open Datasets with Public Labels (PEOPL), uses a certain class of randomly constructed transforms to encode sensitive data. Organizations publish their randomly encoded data and associated raw labels for ML training, where training is done without knowledge of the encoding realization. We investigate several important aspects of this problem: We introduce information-theoretic scores for privacy and utility, which quantify the average performance of an unfaithful user (e.g., adversary) and a faithful user (e.g., model developer) that have access to the published encoded data. We then theoretically characterize primitives in building families of encoding schemes that motivate the use of random deep neura",
    "path": "papers/23/04/2304.00047.json",
    "total_tokens": 1155,
    "translated_title": "PEOPL: 具有公共标签的私下编码开放数据集的特征",
    "translated_abstract": "解决让机构在没有意外信息泄露的情况下共享数据以训练机器学习（ML）模型的问题一直是实际应用中的一个难题。这项仍未解决的问题的一种有希望的技术是在编码数据上训练模型。我们的方法称为具有公共标签的私人编码开放数据集（PEOPL），使用一定类别的随机构造变换来编码敏感数据。机构们发布它们的随机编码数据和与之关联的原始标签以便进行ML训练，其中训练是在没有编码实现的情况下进行。我们研究了这个问题的几个重要方面：我们引入了隐私和效用的信息论分数，量化了使用发布的编码数据的不忠实的用户（例如，攻击者）和忠实的用户（例如，模型开发人员）的平均表现。然后，我们从理论上对构建编码方案系列的原语进行了特征化，并将它们适应于编码设置。通过对公开可用数据集的模拟评估不同的编码方案选择，我们观察到我们的方法既有效又可扩展到高维数据。最后，我们讨论了我们的方法的局限性和未来工作的潜在方向。",
    "tldr": "PEOPL是一种有希望的解决机构共享数据以训练机器学习（ML）模型的难题的方法，使用一定类别的随机构造变换来编码敏感数据。这些方法通过一些信息论分数来量化隐私和效用，并引入了原语构建编码方案系列，并将其适应于编码设置，通过对公开可用数据集的模拟评估不同的编码方案选择，我们观察到我们的方法既有效又可扩展到高维数据。",
    "en_tdlr": "PEOPL is a promising solution to the problem of sharing data among organizations for the training of machine learning models without unintended information leakage. It uses a certain class of randomly constructed transforms to encode sensitive data, introducing information-theoretic scores for privacy and utility, theoretically characterizing primitives in building families of encoding schemes and adapting them to the encoding setting. The approach is scalable to high-dimensional data, with evaluated simulations on publicly available datasets."
}