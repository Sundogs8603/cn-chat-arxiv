{
    "title": "MMRDN: Consistent Representation for Multi-View Manipulation Relationship Detection in Object-Stacked Scenes. (arXiv:2304.12592v1 [cs.CV])",
    "abstract": "Manipulation relationship detection (MRD) aims to guide the robot to grasp objects in the right order, which is important to ensure the safety and reliability of grasping in object stacked scenes. Previous works infer manipulation relationship by deep neural network trained with data collected from a predefined view, which has limitation in visual dislocation in unstructured environments. Multi-view data provide more comprehensive information in space, while a challenge of multi-view MRD is domain shift. In this paper, we propose a novel multi-view fusion framework, namely multi-view MRD network (MMRDN), which is trained by 2D and 3D multi-view data. We project the 2D data from different views into a common hidden space and fit the embeddings with a set of Von-Mises-Fisher distributions to learn the consistent representations. Besides, taking advantage of position information within the 3D data, we select a set of $K$ Maximum Vertical Neighbors (KMVN) points from the point cloud of eac",
    "link": "http://arxiv.org/abs/2304.12592",
    "context": "Title: MMRDN: Consistent Representation for Multi-View Manipulation Relationship Detection in Object-Stacked Scenes. (arXiv:2304.12592v1 [cs.CV])\nAbstract: Manipulation relationship detection (MRD) aims to guide the robot to grasp objects in the right order, which is important to ensure the safety and reliability of grasping in object stacked scenes. Previous works infer manipulation relationship by deep neural network trained with data collected from a predefined view, which has limitation in visual dislocation in unstructured environments. Multi-view data provide more comprehensive information in space, while a challenge of multi-view MRD is domain shift. In this paper, we propose a novel multi-view fusion framework, namely multi-view MRD network (MMRDN), which is trained by 2D and 3D multi-view data. We project the 2D data from different views into a common hidden space and fit the embeddings with a set of Von-Mises-Fisher distributions to learn the consistent representations. Besides, taking advantage of position information within the 3D data, we select a set of $K$ Maximum Vertical Neighbors (KMVN) points from the point cloud of eac",
    "path": "papers/23/04/2304.12592.json",
    "total_tokens": 936,
    "translated_title": "MMRDN: 多视角物体堆叠场景中多视角操作关系检测的一致表示",
    "translated_abstract": "操作关系检测(MRD)旨在指导机器人按正确的顺序抓取物体，这在物体堆叠场景中，确保抓取的安全可靠性非常重要。以往的研究通过用预定义的视角收集的数据，利用深度神经网络来推断操作关系，这在非结构化环境中存在视觉错位的局限性。多视角数据提供了更全面的空间信息，但多视角MRD的挑战是领域偏移。本文提出了一种新的多视角融合框架，即多视角MRD网络(MMRDN)，它是通过2D和3D多视角数据训练的。我们将不同视角的2D数据投影到共同的隐藏空间中，并用一组Von-Mises-Fisher分布拟合嵌入，以学习一致的表示。此外，利用3D数据中的位置信息，我们从每个点云中选择一组$K$个最大垂直邻居(KMVN)点。",
    "tldr": "本文提出了一种用于多视角物体堆叠场景中操作关系检测的多视角MRD网络框架，能够通过2D和3D多视角数据学习一致表示，进而指导机器人按正确的顺序抓取物体。",
    "en_tdlr": "This paper proposes a multi-view MRD network (MMRDN) framework for manipulation relationship detection in object-stacked scenes, which can learn consistent representations through 2D and 3D multi-view data, and guide the robot to grasp objects in the right order."
}