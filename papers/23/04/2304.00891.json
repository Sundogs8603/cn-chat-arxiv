{
    "title": "Online Algorithms for Hierarchical Inference in Deep Learning applications at the Edge",
    "abstract": "arXiv:2304.00891v2 Announce Type: replace  Abstract: We consider a resource-constrained Edge Device (ED), such as an IoT sensor or a microcontroller unit, embedded with a small-size ML model (S-ML) for a generic classification application and an Edge Server (ES) that hosts a large-size ML model (L-ML). Since the inference accuracy of S-ML is lower than that of the L-ML, offloading all the data samples to the ES results in high inference accuracy, but it defeats the purpose of embedding S-ML on the ED and deprives the benefits of reduced latency, bandwidth savings, and energy efficiency of doing local inference. In order to get the best out of both worlds, i.e., the benefits of doing inference on the ED and the benefits of doing inference on ES, we explore the idea of Hierarchical Inference (HI), wherein S-ML inference is only accepted when it is correct, otherwise the data sample is offloaded for L-ML inference. However, the ideal implementation of HI is infeasible as the correctness o",
    "link": "https://arxiv.org/abs/2304.00891",
    "context": "Title: Online Algorithms for Hierarchical Inference in Deep Learning applications at the Edge\nAbstract: arXiv:2304.00891v2 Announce Type: replace  Abstract: We consider a resource-constrained Edge Device (ED), such as an IoT sensor or a microcontroller unit, embedded with a small-size ML model (S-ML) for a generic classification application and an Edge Server (ES) that hosts a large-size ML model (L-ML). Since the inference accuracy of S-ML is lower than that of the L-ML, offloading all the data samples to the ES results in high inference accuracy, but it defeats the purpose of embedding S-ML on the ED and deprives the benefits of reduced latency, bandwidth savings, and energy efficiency of doing local inference. In order to get the best out of both worlds, i.e., the benefits of doing inference on the ED and the benefits of doing inference on ES, we explore the idea of Hierarchical Inference (HI), wherein S-ML inference is only accepted when it is correct, otherwise the data sample is offloaded for L-ML inference. However, the ideal implementation of HI is infeasible as the correctness o",
    "path": "papers/23/04/2304.00891.json",
    "total_tokens": 919,
    "translated_title": "边缘深度学习应用中的层次推断在线算法",
    "translated_abstract": "我们考虑一种资源受限的边缘设备（如物联网传感器或微控制单元），其中嵌入有一个小型机器学习模型（S-ML）用于通用分类应用，以及一个托管大型机器学习模型（L-ML）的边缘服务器（ES）。由于S-ML的推断准确性低于L-ML，在所有数据样本都被发送到ES进行推断会导致较高的推断准确性，但这违背了在ED上嵌入S-ML的目的，剥夺了进行本地推断的低延迟、带宽节省和能量效率的好处。为了充分发挥ED推断和ES推断的优势，我们探索了层次推断（HI）的思想，即只有在S-ML推断正确时才接受，否则将数据样本发送到L-ML进行推断。然而，理想的HI实现是不可行的，因为正确性的判断。。。",
    "tldr": "这项研究提出了一种在线算法，用于解决边缘深度学习应用中资源受限的边缘设备的层次推断问题，以在保证推断准确性的同时实现低延迟、带宽节省和能量效率的好处。",
    "en_tdlr": "This research proposes an online algorithm to address the hierarchical inference problem in resource-constrained edge devices for deep learning applications at the edge, achieving the benefits of low latency, bandwidth savings, and energy efficiency while ensuring inference accuracy."
}