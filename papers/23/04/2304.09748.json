{
    "title": "Reference-based Image Composition with Sketch via Structure-aware Diffusion Model. (arXiv:2304.09748v1 [cs.CV])",
    "abstract": "Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., sketch) and content (i.e., reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.",
    "link": "http://arxiv.org/abs/2304.09748",
    "context": "Title: Reference-based Image Composition with Sketch via Structure-aware Diffusion Model. (arXiv:2304.09748v1 [cs.CV])\nAbstract: Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., sketch) and content (i.e., reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.",
    "path": "papers/23/04/2304.09748.json",
    "total_tokens": 891,
    "translated_title": "通过结构感知扩散模型利用素描进行参考图像合成",
    "translated_abstract": "最近在大规模文本与图像生成模型方面的显著改进已经显示出在生成高保真图像方面的良好结果。为了进一步增强可编辑性并使细粒度生成成为可能，我们引入了一种多输入条件的图像合成模型，其中包含素描作为一种新的模式，以及参考图像。由于使用素描对边界进行控制，我们的方法使用户能够使用所需的结构（即素描）和内容（即参考图像）来编辑或完成图像的子部分。我们的框架通过微调预先训练的扩散模型来使用参考图像来完成缺失的区域，同时保持素描引导。虽然简单，但这将带来广泛的机会，以满足用户需要获取所需图像。通过大量实验证明，我们提出的方法为图像操作提供了独特的用例，使用户驱动任意场景的修改成为可能。",
    "tldr": "该论文提出了一种利用素描进行图像合成的方法，允许用户通过所需的结构和内容对图像的子部分进行编辑或完成。该方法采用预训练的扩散模型来完成缺失的区域并维持素描的引导，为图像操作提供了独特的用例。",
    "en_tdlr": "This paper proposes a method of image composition using sketches, which allows users to edit or complete sub-parts of images through desired structures and contents. The method uses a pre-trained diffusion model to complete missing regions while maintaining sketch guidance, providing unique use cases for image manipulation."
}