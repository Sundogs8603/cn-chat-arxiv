{
    "title": "Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])",
    "abstract": "Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze \"MI overfitting\", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel \"model augmentation\" ide",
    "link": "http://arxiv.org/abs/2304.01669",
    "context": "Title: Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])\nAbstract: Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze \"MI overfitting\", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel \"model augmentation\" ide",
    "path": "papers/23/04/2304.01669.json",
    "total_tokens": 900,
    "translated_title": "重新审视针对深度神经网络的模型逆推攻击",
    "translated_abstract": "模型逆推（MI）攻击旨在通过滥用对模型的访问来推断和重构私有培训数据。MI攻击引起了有关泄露敏感信息（例如用于训练人脸识别系统的私人面部图像）的担忧。最近，已经提出了几种算法来改善MI的攻击表现。在这项工作中，我们重新审视MI，研究了所有最先进（SOTA） MI算法所涉及的两个基本问题，并提出了解决这些问题的解决方案，这些解决方案可以显著提高所有SOTA MI的攻击表现。特别是，我们的贡献有两个方面：1）我们分析了SOTA MI算法的优化目标，认为该目标对于实现MI是次优的，并提出了一种改进的优化目标，显著提高了攻击性能。2）我们分析了“MI过度拟合”，展示了它会阻止重构图像从学习培训数据的语义，提出了一种新型的“模型增强”思路。",
    "tldr": "本文重新审视深度学习中的模型逆推攻击，提出了一种改进的优化目标和一个新型的“模型增强”思路，可以显著提高攻击性能。",
    "en_tdlr": "This paper rethinks model inversion attacks against deep neural networks, proposing an improved optimization objective and a novel \"model augmentation\" idea to significantly boost attack performance."
}