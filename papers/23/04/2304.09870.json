{
    "title": "Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])",
    "abstract": "The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP",
    "link": "http://arxiv.org/abs/2304.09870",
    "context": "Title: Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])\nAbstract: The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP",
    "path": "papers/23/04/2304.09870.json",
    "total_tokens": 1051,
    "translated_title": "异构智能体强化学习",
    "translated_abstract": "协作多智能体强化学习（MARL）在人工智能研究中越来越受欢迎，然而，许多研究仍然严重依赖于智能体之间的参数共享，这将它们限制在同质异构智能体设置下，从而导致训练不稳定和缺乏收敛保证。为了在一般的异构智能体设置下实现有效的协作，我们提出了解决上述问题的异构智能体强化学习（HARL）算法。我们的发现核心是多智能体优势分解引理和序列更新方案。基于这些，我们开发了经过验证的无参数共享约束的异构智能体信任区域学习（HATRL）算法，并通过易处理的逼近方式得出了HATRPO和HAPPO。此外，我们发现了一种名为异构智能体镜像学习（HAML）的新型框架，它加强了对HATRPO和HAPPO的理论保证。",
    "tldr": "提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。",
    "en_tdlr": "The paper proposes a Heterogeneous-Agent Reinforcement Learning (HARL) algorithm that resolves the parameter-sharing constraint issue in cooperative multi-agent reinforcement learning (MARL) and introduces multi-agent advantage decomposition lemma and sequential update scheme to establish a provably correct Heterogeneous-Agent Trust Region Learning (HATRL) algorithm along with its tractable approximations HATRPO and HAPPO. The paper also introduces a novel framework named Heterogeneous-Agent Mirror Learning (HAML) that strengthens theoretical guarantees for HATRPO and HAPPO."
}