{
    "title": "Bayesian optimization for sparse neural networks with trainable activation functions. (arXiv:2304.04455v2 [cs.LG] UPDATED)",
    "abstract": "In the literature on deep neural networks, there is considerable interest in developing activation functions that can enhance neural network performance. In recent years, there has been renewed scientific interest in proposing activation functions that can be trained throughout the learning process, as they appear to improve network performance, especially by reducing overfitting. In this paper, we propose a trainable activation function whose parameters need to be estimated. A fully Bayesian model is developed to automatically estimate from the learning data both the model weights and activation function parameters. An MCMC-based optimization scheme is developed to build the inference. The proposed method aims to solve the aforementioned problems and improve convergence time by using an efficient sampling scheme that guarantees convergence to the global maximum. The proposed scheme is tested on three datasets with three different CNNs. Promising results demonstrate the usefulness of o",
    "link": "http://arxiv.org/abs/2304.04455",
    "context": "Title: Bayesian optimization for sparse neural networks with trainable activation functions. (arXiv:2304.04455v2 [cs.LG] UPDATED)\nAbstract: In the literature on deep neural networks, there is considerable interest in developing activation functions that can enhance neural network performance. In recent years, there has been renewed scientific interest in proposing activation functions that can be trained throughout the learning process, as they appear to improve network performance, especially by reducing overfitting. In this paper, we propose a trainable activation function whose parameters need to be estimated. A fully Bayesian model is developed to automatically estimate from the learning data both the model weights and activation function parameters. An MCMC-based optimization scheme is developed to build the inference. The proposed method aims to solve the aforementioned problems and improve convergence time by using an efficient sampling scheme that guarantees convergence to the global maximum. The proposed scheme is tested on three datasets with three different CNNs. Promising results demonstrate the usefulness of o",
    "path": "papers/23/04/2304.04455.json",
    "total_tokens": 864,
    "translated_title": "可训练激活函数的稀疏神经网络贝叶斯优化",
    "translated_abstract": "在深度神经网络的文献中，人们对开发能增强神经网络性能的激活函数非常感兴趣。最近，科学界提出了可以在学习过程中进行训练的激活函数，因为它们似乎可以提高网络性能，特别是通过减少过拟合。本文提出了一种可训练的激活函数，需要估计其参数。开发了一个完全贝叶斯模型，自动从学习数据中估计出模型权重和激活函数参数。开发了一个基于MCMC的优化方案来构建推理。提出的方法旨在通过使用有效的采样方案来保证收敛到全局最大值，从而解决上述问题并改善收敛时间。在三个不同CNN数据集上测试了所提出的方案。有希望的结果证明了它的有效性。",
    "tldr": "本文提出了一种可训练的激活函数以提高神经网络性能，在此基础上开发了一个基于贝叶斯优化和MCMC采样的模型，能通过有效的采样和全局优化来解决过拟合并提高收敛速度。",
    "en_tdlr": "This paper proposes a trainable activation function to enhance neural network performance and develops a fully Bayesian model using MCMC-based optimization to estimate both model weights and activation function parameters, thus solving the overfitting problem and improving convergence speed with efficient sampling and global optimization."
}