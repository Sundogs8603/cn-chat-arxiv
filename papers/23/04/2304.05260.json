{
    "title": "Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])",
    "abstract": "In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia",
    "link": "http://arxiv.org/abs/2304.05260",
    "context": "Title: Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])\nAbstract: In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia",
    "path": "papers/23/04/2304.05260.json",
    "total_tokens": 882,
    "translated_title": "控制联邦学习中遗忘问题的重新加权Softmax交叉熵方法",
    "translated_abstract": "在联邦学习中，通过聚合在一组独立客户节点计算的模型更新来学习全局模型，在聚合之前在每个节点上执行多个梯度步骤，以减少通信成本。在这种情况下，数据异构性会导致不同的客户端具有不同的本地目标，这可能会导致客户端过度减少其自己的本地目标，使其与全局解分歧。本文提出了一种有效的方法，对每个客户端的交叉熵目标进行修改，通过重新加权softmax的logits以计算损失，从而解决了每个客户端模型对来自其他客户端的数据的灾难性遗忘问题。这种方法可以保护不在客户端标签集中的类别免受突然的表示变化，并且通过实验证明，可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。 我们的方法特别有益。",
    "tldr": "本文提出一种重新加权softmax的交叉熵方法来解决联邦学习中客户端的灾难性遗忘问题，并证明这种方法可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。",
    "en_tdlr": "This paper proposes a re-weighted softmax cross-entropy method to address the catastrophic forgetting problem of individual client models in federated learning, and demonstrates that this approach can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms."
}