{
    "title": "Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])",
    "abstract": "The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.",
    "link": "http://arxiv.org/abs/2304.12330",
    "context": "Title: Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])\nAbstract: The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.",
    "path": "papers/23/04/2304.12330.json",
    "total_tokens": 902,
    "translated_title": "基于并行bootstrap的连续流控制应用的on-policy深度强化学习",
    "translated_abstract": "深度强化学习与数值流控问题的耦合近期引起了相当大的关注，取得了突破性的成果并为该领域开辟了新的前景。然而，由于流体动力学求解器的计算成本通常很高，在学习过程中使用并行环境是实现有效控制的必要手段。尽管如此，大多数基于流控的深度强化学习文献仍依赖于on-policy算法，而这种算法的高并行转移收集可能会破坏理论假设并导致次优的控制模型。为了克服这个问题，我们提出了一个基于部分轨迹缓冲区的并行模式，通过一个返回bootstrapping步骤，允许灵活地使用并行环境，同时保持更新的on-policy性。该方法在文献中一个耗费大量计算的连续流控制问题上进行了说明。",
    "tldr": "本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。"
}