{
    "title": "Self-supervised Auxiliary Loss for Metric Learning in Music Similarity-based Retrieval and Auto-tagging. (arXiv:2304.07449v1 [cs.SD])",
    "abstract": "In the realm of music information retrieval, similarity-based retrieval and auto-tagging serve as essential components. Given the limitations and non-scalability of human supervision signals, it becomes crucial for models to learn from alternative sources to enhance their performance. Self-supervised learning, which exclusively relies on learning signals derived from music audio data, has demonstrated its efficacy in the context of auto-tagging. In this study, we propose a model that builds on the self-supervised learning approach to address the similarity-based retrieval challenge by introducing our method of metric learning with a self-supervised auxiliary loss. Furthermore, diverging from conventional self-supervised learning methodologies, we discovered the advantages of concurrently training the model with both self-supervision and supervision signals, without freezing pre-trained models. We also found that refraining from employing augmentation during the fine-tuning phase yields",
    "link": "http://arxiv.org/abs/2304.07449",
    "context": "Title: Self-supervised Auxiliary Loss for Metric Learning in Music Similarity-based Retrieval and Auto-tagging. (arXiv:2304.07449v1 [cs.SD])\nAbstract: In the realm of music information retrieval, similarity-based retrieval and auto-tagging serve as essential components. Given the limitations and non-scalability of human supervision signals, it becomes crucial for models to learn from alternative sources to enhance their performance. Self-supervised learning, which exclusively relies on learning signals derived from music audio data, has demonstrated its efficacy in the context of auto-tagging. In this study, we propose a model that builds on the self-supervised learning approach to address the similarity-based retrieval challenge by introducing our method of metric learning with a self-supervised auxiliary loss. Furthermore, diverging from conventional self-supervised learning methodologies, we discovered the advantages of concurrently training the model with both self-supervision and supervision signals, without freezing pre-trained models. We also found that refraining from employing augmentation during the fine-tuning phase yields",
    "path": "papers/23/04/2304.07449.json",
    "total_tokens": 971,
    "translated_title": "自监督辅助损失用于基于音乐相似度检索和自动标注的度量学习",
    "translated_abstract": "在音乐信息检索领域，基于相似度的检索和自动标记是关键组成部分。考虑到人类监督信号的限制性和不可扩展性，让模型从其他来源学习以提高其性能变得至关重要。自监督学习，仅依赖于从音乐音频数据中派生的学习信号，在自动标记的背景下已经证明了其有效性。在这项研究中，我们提出了一种模型，采用自监督学习方法来解决基于相似度的检索问题，并引入了我们的度量学习方法，使用自监督辅助损失。此外，与传统的自监督学习方法不同的是，我们发现了同时使用自监督和监督信号训练模型的优点，而不冻结预训练模型。我们还发现，避免在微调阶段使用数据增强可以提高性能。",
    "tldr": "本论文提出了一种自监督学习方法，在自动标注方面已经证明其有效性。我们引入了自监督辅助损失的度量学习方法来解决音乐相似度检索问题，并发现同时使用自监督和监督信号训练模型的优势，而不冻结预训练模型。此外，避免在微调阶段使用数据增强可以提高性能。"
}