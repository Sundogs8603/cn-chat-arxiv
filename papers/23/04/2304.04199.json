{
    "title": "Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks. (arXiv:2304.04199v1 [cs.SE])",
    "abstract": "The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions -amplifying existing biases or introducing new ones -- that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids -- such as severity and causal explanations -- crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework ",
    "link": "http://arxiv.org/abs/2304.04199",
    "context": "Title: Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks. (arXiv:2304.04199v1 [cs.SE])\nAbstract: The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions -amplifying existing biases or introducing new ones -- that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids -- such as severity and causal explanations -- crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework ",
    "path": "papers/23/04/2304.04199.json",
    "total_tokens": 882,
    "translated_title": "深度神经网络中公平性缺陷的信息论测试和调试",
    "translated_abstract": "深度前馈神经网络(DNN)在各种决策支持软件中得到了广泛应用。 DNN在它们的训练数据中能够寻找到最小、足够的统计模式。因此，DNN可能学习到编码决策的能力——放大现有的偏见或引入新的偏见——这些可能会使受保护的个体/群体处于不利地位，并可能违反法律保护。虽然现有的基于搜索的软件测试方法在发现公正性缺陷方面效果很好，但它们没有提供与严重性和因果解释等调试辅助工具，这对于帮助开发人员进行处理并决定下一步行动至关重要。我们能否度量DNN中公平性缺陷的严重性？这些缺陷是由不当的训练引起的，还是仅仅反映了训练数据中存在的偏见？为了回答这些问题，我们提出了DICE：一个信息论测试和调试框架。",
    "tldr": "本文介绍了一个信息论测试和调试框架DICE，用于衡量DNN中公平性缺陷的严重性及其根源，以帮助开发人员进行处理。",
    "en_tdlr": "This paper presents DICE, an information-theoretic testing and debugging framework to measure the severity and root causes of fairness defects in DNNs, aiming to assist developers in addressing these issues."
}