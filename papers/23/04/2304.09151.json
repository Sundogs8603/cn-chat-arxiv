{
    "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])",
    "abstract": "Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMa",
    "link": "http://arxiv.org/abs/2304.09151",
    "context": "Title: UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])\nAbstract: Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMa",
    "path": "papers/23/04/2304.09151.json",
    "total_tokens": 1001,
    "translated_title": "UniMax: 更公平和更有效的大规模多语言预训练语言采样方法",
    "translated_abstract": "预训练的多语言大型语言模型通常使用基于温度的启发式采样来平衡不同语言，但先前的研究没有系统地评估不同预训练语言分布在模型规模上的功效。本文提出了一种新的采样方法UniMax，通过明确地限制每种语言语料库上的重复次数，提供更均匀的核心语言覆盖率，同时减轻了对尾部语言的过度拟合。我们在一系列多语言基准测试中执行了广泛的消融测试，测试了一系列采样策略，同时变化模型规模。我们发现UniMax优于标准的基于温度的采样方法，并且这些好处随着规模的增加而持续存在。作为我们的贡献的一部分，我们发布了：（i）29万亿个字符跨107种语言的改进和更新的mC4多语言语料库，以及（ii）使用UniMax训练的预训练umT5模型检查点套件。",
    "tldr": "本文提出UniMax，一种对多语言模型进行更公平和更有效的预训练语言采样方法。该方法通过明确限制每种语言语料库上的重复次数来提供更均匀的核心语言覆盖率，并减轻了对尾部语言的过度拟合。UniMax优于标准的基于温度的采样方法，而且这些好处随着规模的增加而持续存在。"
}