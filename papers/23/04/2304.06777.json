{
    "title": "Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots. (arXiv:2304.06777v1 [cs.RO])",
    "abstract": "Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10 DGs using artificial neural networks. These results compare equally or favorably with dif",
    "link": "http://arxiv.org/abs/2304.06777",
    "context": "Title: Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots. (arXiv:2304.06777v1 [cs.RO])\nAbstract: Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10 DGs using artificial neural networks. These results compare equally or favorably with dif",
    "path": "papers/23/04/2304.06777.json",
    "total_tokens": 1001,
    "translated_title": "在线识别不完整的手势数据以接口协作机器人",
    "translated_abstract": "在实现人机交互和进一步推动协作机器人进入市场，使机器人能够更广泛地被人们接受时，手势的在线识别对于实现直观的人机交互至关重要。然而，在真实的非结构化环境中，使用失真和不完整的多传感器数据实现准确的手势识别却很困难。本文介绍了一个人机交互框架，使用可穿戴的传感器捕捉交织的静态手势（SG）和动态手势（DG），通过对原始传感器数据进行数据降维（使用三次插值和主成分分析重采样）来获取DG特征。实验测试使用了UC2017手势数据集从8个不同的受试者中采集的样本。分类模型对于包含24个SG的库使用随机森林显示出95.6％的准确率，并使用人工神经网络对10个DG显示出99.3％的准确率。这些结果与不同的先前研究相比表现一致或更有优势。",
    "tldr": "本文介绍了一个人机交互框架，通过可穿戴传感器捕捉交织的静态手势和动态手势，并使用数据降维技术以获得DG特征。实验结果表明，通过随机森林和人工神经网络，分别可以对24个SG和10个DG进行准确的分类，从而实现在非结构化环境中的手势识别。"
}