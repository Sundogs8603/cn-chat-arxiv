{
    "title": "Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks. (arXiv:2304.12829v1 [cs.LG])",
    "abstract": "Reducing the memory footprint of Machine Learning (ML) models, particularly Deep Neural Networks (DNNs), is essential to enable their deployment into resource-constrained tiny devices. However, a disadvantage of DNN models is their vulnerability to adversarial attacks, as they can be fooled by adding slight perturbations to the inputs. Therefore, the challenge is how to create accurate, robust, and tiny DNN models deployable on resource-constrained embedded devices. This paper reports the results of devising a tiny DNN model, robust to adversarial black and white box attacks, trained with an automatic quantizationaware training framework, i.e. QKeras, with deep quantization loss accounted in the learning loop, thereby making the designed DNNs more accurate for deployment on tiny devices. We investigated how QKeras and an adversarial robustness technique, Jacobian Regularization (JR), can provide a co-optimization strategy by exploiting the DNN topology and the per layer JR approach to ",
    "link": "http://arxiv.org/abs/2304.12829",
    "context": "Title: Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks. (arXiv:2304.12829v1 [cs.LG])\nAbstract: Reducing the memory footprint of Machine Learning (ML) models, particularly Deep Neural Networks (DNNs), is essential to enable their deployment into resource-constrained tiny devices. However, a disadvantage of DNN models is their vulnerability to adversarial attacks, as they can be fooled by adding slight perturbations to the inputs. Therefore, the challenge is how to create accurate, robust, and tiny DNN models deployable on resource-constrained embedded devices. This paper reports the results of devising a tiny DNN model, robust to adversarial black and white box attacks, trained with an automatic quantizationaware training framework, i.e. QKeras, with deep quantization loss accounted in the learning loop, thereby making the designed DNNs more accurate for deployment on tiny devices. We investigated how QKeras and an adversarial robustness technique, Jacobian Regularization (JR), can provide a co-optimization strategy by exploiting the DNN topology and the per layer JR approach to ",
    "path": "papers/23/04/2304.12829.json",
    "total_tokens": 980,
    "translated_title": "用深度量化神经网络提高对抗攻击的鲁棒性",
    "translated_abstract": "减小机器学习（ML）模型尤其是深度神经网络（DNNs）的存储空间，是将它们部署到资源受限的小型设备中的重要前提。然而，DNN模型的一个缺点是它们易受到对抗攻击的影响，因为在输入中添加轻微的扰动可以欺骗模型。因此，挑战在于如何创建准确、稳健并且可部署在资源受限嵌入式设备上的小型DNN模型。本文通过采用一种自动量化训练框架QKeras训练一个小型DNN模型，该模型对白盒和黑盒对抗攻击具有鲁棒性，并在学习过程中计算深度量化误差，从而提高了设计的DNN模型的精度，使它适用于小型设备的部署。我们研究了QKeras和一种对抗鲁棒性技术Jacobian Regularization（JR）如何通过利用DNN拓扑结构和每层JR方法提供一种共同优化策略，以提高深度量化神经网络对抗攻击的鲁棒性。",
    "tldr": "本文研究了在深度量化神经网络中采用自动量化训练框架和深度量化误差计算等方法来提高对抗攻击的鲁棒性。",
    "en_tdlr": "This paper investigates methods of improving the robustness of deeply quantized neural networks against adversarial attacks, by using an automatic quantization training framework and accounting for deep quantization loss during learning."
}