{
    "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v1 [cs.CL])",
    "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on",
    "link": "http://arxiv.org/abs/2304.06762",
    "context": "Title: Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v1 [cs.CL])\nAbstract: Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on",
    "path": "papers/23/04/2304.06762.json",
    "total_tokens": 1036,
    "translated_abstract": "通过检索（例如RETRO），大型仅解码的语言模型（LMs）可以在概率困惑度方面得到大幅改进，但它对文本生成质量和下游任务准确性的影响尚不清楚。因此，是否应该使用检索预训练大型自回归LMs仍然是一个未解决的问题。为了解答这个问题，我们进行了一项全面的研究，其中包括可扩展的检索增强LMs（即RETRO），并将其与标准GPT和在Fine-tuning或推理阶段增强的检索GPT进行了对比。我们首先提供了重现RETRO的方法，达到了9.5B个参数的规模，同时检索了330B个令牌的文本语料库。基于此，我们得出以下新发现： i）RETRO在文本生成方面表现优于GPT，重复性较小（即重复），具有适度的更高事实准确性，以及使用无毒的检索数据库略低的毒性。 ii） 在LM Evaluation Harness基准测试中，RETRO在知识密集型任务中表现优异，但在其他任务上与GPT相当。",
    "tldr": "本研究发现，在自然语言处理任务中，使用检索预训练的自回归语言模型，如RETRO，可以显著提高文本生成的质量和下游任务的准确性。",
    "en_tdlr": "This study found that using retrieval-pretrained autoregressive language models, such as RETRO, can significantly improve the quality of text generation and downstream task accuracy in natural language processing tasks."
}