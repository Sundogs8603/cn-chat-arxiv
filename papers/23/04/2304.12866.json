{
    "title": "Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy. (arXiv:2304.12866v1 [cs.NE])",
    "abstract": "Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the b",
    "link": "http://arxiv.org/abs/2304.12866",
    "context": "Title: Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy. (arXiv:2304.12866v1 [cs.NE])\nAbstract: Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the b",
    "path": "papers/23/04/2304.12866.json",
    "total_tokens": 818,
    "translated_title": "二进制随机性支持的神经形态深度学习实现比软件更高准确性",
    "translated_abstract": "深度学习需要精准的信号转发处理、反向传播误差和权重更新。这是由于梯度下降学习规则依赖于偏导数的链式乘积，学习算法本质上需要这一点。然而，使用具有噪声的人工突触的硬件系统中实现深度学习是具有挑战性的，因为这不符合生物学原理。基于膜电阻器的实施通常导致过多的神经电路成本和对理想化突触设备的严格需求。因此，我们展示了高精度不是必要的，没有这个要求可以实现更高效的深度学习。我们提出了一个二进制随机学习算法，通过引入(i)正、反向信号的随机二值化 (ii) 有符号二值化的神经网络的所有基本操作进行修改。",
    "tldr": "二进制随机学习算法提高了深度学习效率，消除了使用噪声人工突触硬件系统的挑战性。",
    "en_tdlr": "Binary stochastic learning algorithm improves efficiency of deep learning and removes challenges in using hardware systems with noisy artificial synapses."
}