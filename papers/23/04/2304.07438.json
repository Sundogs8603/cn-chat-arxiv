{
    "title": "Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])",
    "abstract": "Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\\Pr(\\text{text} | \\alpha)$ is intractable for even the simplest lexical constraints $\\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.",
    "link": "http://arxiv.org/abs/2304.07438",
    "context": "Title: Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])\nAbstract: Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\\Pr(\\text{text} | \\alpha)$ is intractable for even the simplest lexical constraints $\\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.",
    "path": "papers/23/04/2304.07438.json",
    "total_tokens": 848,
    "translated_title": "可操作的自回归语言生成控制方法",
    "translated_abstract": "尽管自回归大语言模型在文本生成方面取得了成功，但生成满足复杂限制的文本仍然是一个重大挑战：即使是最简单的词汇限制也使条件分布$\\Pr(\\text{text} | \\alpha)$的采样变得不可计算。为了克服这个挑战，我们提出使用可操作的概率模型将词汇限制强加于自回归文本生成中，我们将其称为 GeLaTo。为了证明这个框架的有效性，我们使用了精简的隐马尔可夫模型来控制从GPT2到自回归的生成。GeLaTo在约束文本生成的具有挑战性的基准测试CommonGen上取得了最先进的性能，大幅击败了各种强基线。我们的工作不仅为控制大型语言模型开辟了新的途径，还激励人们开发更具表现力的可操作概率模型。",
    "tldr": "本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。",
    "en_tdlr": "The paper proposes a controllable method using tractable probabilistic models to impose lexical constraints in autoregressive text generation, named GeLaTo, which achieves state-of-the-art performance on the challenging benchmark, CommonGen."
}