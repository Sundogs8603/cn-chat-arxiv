{
    "title": "Word Sense Induction with Knowledge Distillation from BERT. (arXiv:2304.10642v1 [cs.CL])",
    "abstract": "Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems. Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark d",
    "link": "http://arxiv.org/abs/2304.10642",
    "context": "Title: Word Sense Induction with Knowledge Distillation from BERT. (arXiv:2304.10642v1 [cs.CL])\nAbstract: Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems. Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark d",
    "path": "papers/23/04/2304.10642.json",
    "total_tokens": 742,
    "translated_title": "从BERT中蒸馏知识进行词义识别",
    "translated_abstract": "预训练的上下文语言模型被广泛用于语言理解任务，但对于资源受限的系统来说并不适用。在这种情况下，非上下文词向量是一种有效的替代方案。本文提出了一种两阶段方法，通过在上下文中利用词的多个语义感知来从一个预训练的语言模型（BERT）中蒸馏多个词义，并将这种信息传递到类skip-gram框架下的多词义嵌入中。",
    "tldr": "本文提出了一种从BERT中蒸馏知识进行多词义识别的方法，可有效地利用词的多个语义感知，在资源限制的情况下获得与最先进的多词义嵌入相当的结果。",
    "en_tdlr": "This paper proposes a method of word sense induction by distilling multiple senses from BERT, which effectively utilizes the sense information of a word in context and achieves comparable results to state-of-the-art multi-sense embeddings in resource-constrained settings."
}