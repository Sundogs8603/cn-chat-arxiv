{
    "title": "Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])",
    "abstract": "We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\\left(D O \\sqrt{A T K_T \\log\\left (\\frac{T}{\\delta} \\right) + \\frac{K_T \\log \\frac{K_T}{\\delta}}{\\min\\limits_\\ell \\: \\mathbf{KL}\\left( {\\mathbf{\\theta}^{(\\ell+1)}}\\mid\\mid{\\mathbf{\\theta}^{(\\ell)}}\\right)}}\\right)$,",
    "link": "http://arxiv.org/abs/2304.00232",
    "context": "Title: Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])\nAbstract: We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\\left(D O \\sqrt{A T K_T \\log\\left (\\frac{T}{\\delta} \\right) + \\frac{K_T \\log \\frac{K_T}{\\delta}}{\\min\\limits_\\ell \\: \\mathbf{KL}\\left( {\\mathbf{\\theta}^{(\\ell+1)}}\\mid\\mid{\\mathbf{\\theta}^{(\\ell)}}\\right)}}\\right)$,",
    "path": "papers/23/04/2304.00232.json",
    "total_tokens": 1094,
    "translated_title": "重启贝叶斯在线变点检测用于非平稳马尔科夫决策过程",
    "translated_abstract": "我们考虑在一个非平稳的强化学习（RL）环境中进行学习的问题，其中该设置可以被完全描述为分段平稳的离散时间马尔科夫决策过程（MDP）。我们引入了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD）变体，该算法适用于从更一般的多项式分布中生成的输入流，并在误警率和检测延迟方面提供接近最优的理论保证。基于此，我们提出了一种针对从多项式分布中采样的状态转移内核的MDPs的改进版本UCRL2算法，我们称之为R-BOCPD-UCRL2。我们进行了有限时间的性能分析，并表明R-BOCPD-UCRL2具有有利的遗憾界的$O\\left(D O \\sqrt{A T K_T \\log\\left (\\frac{T}{\\delta} \\right) + \\frac{K_T \\log \\frac{K_T}{\\delta}}{\\min\\limits_\\ell \\: \\mathbf{KL}\\left( {\\mathbf{\\theta}^{(\\ell+1)}}\\mid\\mid{\\mathbf{\\theta}^{(\\ell)}}\\right)}}\\right)$。",
    "tldr": "该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。",
    "en_tdlr": "This paper introduces an algorithm, called R-BOCPD-UCRL2, for the problem of learning in a non-stationary reinforcement learning environment, which uses a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) and provides near-optimal theoretical guarantees on MDPs with state transition kernel sampled from a multinomial distribution. The favorable regret bound of R-BOCPD-UCRL2 is also presented in the paper."
}