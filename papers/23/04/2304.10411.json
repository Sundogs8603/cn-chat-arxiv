{
    "title": "Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \\in \\mathbb{R}^{n \\times d}$ and ",
    "link": "http://arxiv.org/abs/2304.10411",
    "context": "Title: Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])\nAbstract: Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \\in \\mathbb{R}^{n \\times d}$ and ",
    "path": "papers/23/04/2304.10411.json",
    "total_tokens": 889,
    "translated_title": "基于注意机制的 softmax 回归",
    "translated_abstract": "大型语言模型（LLMs）已经给人类社会带来了巨大的变革。LLMs 的关键计算之一是 softmax 单元。这个操作在 LLMs 中非常重要，因为它允许模型在给定输入单词序列的情况下生成可能的下一个单词或短语的分布。这个分布然后用来选择最有可能的下一个单词或短语，基于模型分配的概率。softmax 单元在训练 LLMs 中起着至关重要的作用，因为它允许模型通过调整神经网络的权重和偏差从数据中学习。在凸优化领域，例如使用中心路径法解决线性规划，softmax 函数已经成为控制潜在函数的进展和稳定性的关键工具。在这项工作中，我们从 softmax 单元中获得灵感，定义了一个 softmax 回归问题。形式上讲，给定一个矩阵 $A \\in \\mathbb{R}^{n \\times d}$ 和...",
    "tldr": "本研究从 softmax 单元中获得灵感，提出了注意机制 inspired 的 softmax 回归问题，该问题可用于控制潜在函数的进展和稳定性。"
}