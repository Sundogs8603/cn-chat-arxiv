{
    "title": "Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation. (arXiv:2304.01782v1 [cs.LG])",
    "abstract": "This work presents a novel loss function for learning nonlinear Model Predictive Control policies via Imitation Learning. Standard approaches to Imitation Learning neglect information about the expert and generally adopt a loss function based on the distance between expert and learned controls. In this work, we present a loss based on the Q-function directly embedding the performance objectives and constraint satisfaction of the associated Optimal Control Problem (OCP). However, training a Neural Network with the Q-loss requires solving the associated OCP for each new sample. To alleviate the computational burden, we derive a second Q-loss based on the Gauss-Newton approximation of the OCP resulting in a faster training time. We validate our losses against Behavioral Cloning, the standard approach to Imitation Learning, on the control of a nonlinear system with constraints. The final results show that the Q-function-based losses significantly reduce the amount of constraint violations ",
    "link": "http://arxiv.org/abs/2304.01782",
    "context": "Title: Imitation Learning from Nonlinear MPC via the Exact Q-Loss and its Gauss-Newton Approximation. (arXiv:2304.01782v1 [cs.LG])\nAbstract: This work presents a novel loss function for learning nonlinear Model Predictive Control policies via Imitation Learning. Standard approaches to Imitation Learning neglect information about the expert and generally adopt a loss function based on the distance between expert and learned controls. In this work, we present a loss based on the Q-function directly embedding the performance objectives and constraint satisfaction of the associated Optimal Control Problem (OCP). However, training a Neural Network with the Q-loss requires solving the associated OCP for each new sample. To alleviate the computational burden, we derive a second Q-loss based on the Gauss-Newton approximation of the OCP resulting in a faster training time. We validate our losses against Behavioral Cloning, the standard approach to Imitation Learning, on the control of a nonlinear system with constraints. The final results show that the Q-function-based losses significantly reduce the amount of constraint violations ",
    "path": "papers/23/04/2304.01782.json",
    "total_tokens": 802,
    "translated_title": "通过精确 Q-loss 及其 Gauss-Newton 近似从非线性 MPC 中进行模仿学习",
    "translated_abstract": "本文提出了一种新颖的损失函数，通过模仿学习学习非线性模型预测控制策略。我们提出了一种基于 Q 函数的损失，直接嵌入了与最优控制问题相关的性能目标和约束满足性。为了缓解计算负担，我们得出了一个基于 Gauss-Newton 近似的第二个 Q-loss，从而实现更快的训练时间。通过在带约束的非线性系统的控制上对我们的损失进行验证，我们将其与行为克隆相比较，结果显示，基于 Q 函数的损失显著减少了约束违规的数量。",
    "tldr": "本文提出了一种新的 Q-loss 函数，通过精确嵌入最优控制问题的目标和约束，以及 Gauss-Newton 近似损失加速训练，从而在带约束的非线性系统控制中进行模仿学习。",
    "en_tdlr": "The paper proposes a novel Q-loss function for learning nonlinear MPC policies through imitation learning that significantly reduces constraint violations, and presents a Gauss-newton approximation-based Q-loss that accelerates training time."
}