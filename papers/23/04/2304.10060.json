{
    "title": "Optimality of Robust Online Learning. (arXiv:2304.10060v1 [stat.ML])",
    "abstract": "In this paper, we study an online learning algorithm with a robust loss function $\\mathcal{L}_{\\sigma}$ for regression over a reproducing kernel Hilbert space (RKHS). The loss function $\\mathcal{L}_{\\sigma}$ involving a scaling parameter $\\sigma>0$ can cover a wide range of commonly used robust losses. The proposed algorithm is then a robust alternative for online least squares regression aiming to estimate the conditional mean function. For properly chosen $\\sigma$ and step size, we show that the last iterate of this online algorithm can achieve optimal capacity independent convergence in the mean square distance. Moreover, if additional information on the underlying function space is known, we also establish optimal capacity dependent rates for strong convergence in RKHS. To the best of our knowledge, both of the two results are new to the existing literature of online learning.",
    "link": "http://arxiv.org/abs/2304.10060",
    "context": "Title: Optimality of Robust Online Learning. (arXiv:2304.10060v1 [stat.ML])\nAbstract: In this paper, we study an online learning algorithm with a robust loss function $\\mathcal{L}_{\\sigma}$ for regression over a reproducing kernel Hilbert space (RKHS). The loss function $\\mathcal{L}_{\\sigma}$ involving a scaling parameter $\\sigma>0$ can cover a wide range of commonly used robust losses. The proposed algorithm is then a robust alternative for online least squares regression aiming to estimate the conditional mean function. For properly chosen $\\sigma$ and step size, we show that the last iterate of this online algorithm can achieve optimal capacity independent convergence in the mean square distance. Moreover, if additional information on the underlying function space is known, we also establish optimal capacity dependent rates for strong convergence in RKHS. To the best of our knowledge, both of the two results are new to the existing literature of online learning.",
    "path": "papers/23/04/2304.10060.json",
    "total_tokens": 882,
    "translated_title": "鲁棒性在线学习算法的最优性分析",
    "translated_abstract": "本文研究在再生核希尔伯特空间上使用鲁棒损失函数 $\\mathcal{L}_{\\sigma}$ 进行回归的在线学习算法。这个涉及到缩放参数 $\\sigma>0$ 的损失函数可以覆盖一系列常用的鲁棒损失函数。提出的算法是针对在线最小二乘回归的鲁棒替代方案，旨在估计条件均值函数。在选择适当的 $\\sigma$ 和步长的情况下，我们证明了该在线算法的最终迭代可以在均方距离上实现无容量依赖的收敛最优性。此外，如果已知底层函数空间的其他信息，则我们还建立了强收敛的最优容量依赖速率。据我们所知，这两个结果都是在线学习现有文献中的新结果。",
    "tldr": "本文提出了一种基于鲁棒损失函数 $\\mathcal{L}_{\\sigma}$ 的在线学习算法，可用作在线最小二乘回归的鲁棒替代方案。并证明了在适当选择参数的情况下，该算法具有无容量依赖的最优性收敛性以及强收敛的最优容量依赖速率。",
    "en_tdlr": "This paper proposes an online learning algorithm with a robust loss function for regression over a reproducing kernel Hilbert space. The algorithm achieves optimal capacity independent convergence and optimal capacity dependent rates for strong convergence in the mean square distance. These results are new to the existing literature of online learning."
}