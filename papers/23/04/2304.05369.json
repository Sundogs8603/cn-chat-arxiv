{
    "title": "A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation. (arXiv:2304.05369v1 [cs.LG])",
    "abstract": "Self-Supervised Learning (SSL) models rely on a pretext task to learn representations. Because this pretext task differs from the downstream tasks used to evaluate the performance of these models, there is an inherent misalignment or pretraining bias. A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector (usually a 2 or 3 layer multi-layer perceptron) on top of a backbone network during training. In contrast to previous work that studied the impact of the projector architecture, we here focus on a simpler, yet overlooked lever to control the information in the backbone representation. We show that merely changing its dimensionality -- by changing only the size of the backbone's very last block -- is a remarkably effective technique to mitigate the pretraining bias. It significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models.",
    "link": "http://arxiv.org/abs/2304.05369",
    "context": "Title: A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation. (arXiv:2304.05369v1 [cs.LG])\nAbstract: Self-Supervised Learning (SSL) models rely on a pretext task to learn representations. Because this pretext task differs from the downstream tasks used to evaluate the performance of these models, there is an inherent misalignment or pretraining bias. A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector (usually a 2 or 3 layer multi-layer perceptron) on top of a backbone network during training. In contrast to previous work that studied the impact of the projector architecture, we here focus on a simpler, yet overlooked lever to control the information in the backbone representation. We show that merely changing its dimensionality -- by changing only the size of the backbone's very last block -- is a remarkably effective technique to mitigate the pretraining bias. It significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models.",
    "path": "papers/23/04/2304.05369.json",
    "total_tokens": 893,
    "translated_title": "一种意外简单的技术来控制预训练偏差以实现更好的迁移：扩展或缩小你的表示",
    "translated_abstract": "自监督学习（SSL）模型依赖于预文本任务来学习表示。由于这个任务与用于评估这些模型性能的下游任务不同，因此存在固有的不对齐或预训练偏差。在SSL中常用的一个技巧是在训练期间在骨干网络之上添加一个小的投影机（通常是一个2或3层的多层感知器），以使深度网络更具抗偏差性。与先前研究投影机架构的影响不同，我们在这里专注于一个更简单但被忽视的方法来控制骨干表示中的信息。我们展示了仅通过改变骨干网络的最后一个块的大小来改变其维数是一种极其有效的技术，可以减轻预训练偏差。它显着改善了自监督和监督预训练模型的下游传输性能。",
    "tldr": "本研究提出了一种简单的方法来控制预训练偏差，即通过扩展或缩小骨干网络的表示维度，从而显著提高了下游迁移性能。",
    "en_tdlr": "This study proposes a simple technique of controlling the pretraining bias by expanding or narrowing the representation dimensionality of the backbone network, which significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models."
}