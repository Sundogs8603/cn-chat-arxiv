{
    "title": "Rethinking Dense Retrieval's Few-Shot Ability. (arXiv:2304.05845v1 [cs.CL])",
    "abstract": "Few-shot dense retrieval (DR) aims to effectively generalize to novel search scenarios by learning a few samples. Despite its importance, there is little study on specialized datasets and standardized evaluation protocols. As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress. In this paper, we propose a customized FewDR dataset and a unified evaluation benchmark. Specifically, FewDR employs class-wise sampling to establish a standardized \"few-shot\" setting with finely-defined classes, reducing variability in multiple sampling rounds. Moreover, the dataset is disjointed into base and novel classes, allowing DR models to be continuously trained on ample data from base classes and a few samples in novel classes. This benchmark eliminates the risk of novel class leakage, providing a reliable estimation o",
    "link": "http://arxiv.org/abs/2304.05845",
    "context": "Title: Rethinking Dense Retrieval's Few-Shot Ability. (arXiv:2304.05845v1 [cs.CL])\nAbstract: Few-shot dense retrieval (DR) aims to effectively generalize to novel search scenarios by learning a few samples. Despite its importance, there is little study on specialized datasets and standardized evaluation protocols. As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress. In this paper, we propose a customized FewDR dataset and a unified evaluation benchmark. Specifically, FewDR employs class-wise sampling to establish a standardized \"few-shot\" setting with finely-defined classes, reducing variability in multiple sampling rounds. Moreover, the dataset is disjointed into base and novel classes, allowing DR models to be continuously trained on ample data from base classes and a few samples in novel classes. This benchmark eliminates the risk of novel class leakage, providing a reliable estimation o",
    "path": "papers/23/04/2304.05845.json",
    "total_tokens": 918,
    "translated_title": "重新思考密集检索的少样本能力",
    "translated_abstract": "少样本密集检索（DR）旨在通过学习少量样本有效地推广到新的搜索场景。尽管它非常重要，但对于专门的数据集和标准化的评估协议的研究很少。因此，当前的方法通常采用从监督数据集中随机采样来创建“少量数据”设置，并在评估过程中采用不一致的训练策略，这会在准确比较最近的进展方面带来挑战。在本文中，我们提出了一个定制的FewDR数据集和一个统一的评估基准。具体来说，FewDR采用按类别抽样的方法建立了一个标准化的“少样本”设置，定义了精细的类别，减少了多次抽样的变异性。此外，该数据集被分为基类和新颖类，允许DR模型在基类的丰富数据和新颖类的少量样本上进行连续训练。这个基准消除了新颖类泄漏的风险，提供了可靠的估计。",
    "tldr": "本文提出了一个定制的FewDR数据集和一个统一的评估基准，以解决现有方法在少量数据集上评估时的不一致性问题，使得DR模型能够在基类和新颖类上进行连续训练，从而提高少样本密集检索的推广能力。",
    "en_tdlr": "This paper proposes a customized FewDR dataset and a unified evaluation benchmark to address the inconsistency issue in current methods when evaluating on small data sets, which enables DR models to be continuously trained on base and novel classes for increased few-shot dense retrieval ability."
}