{
    "title": "On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])",
    "abstract": "Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021)) have received huge success in multimodal learning, where the model tries to minimize the distance between the representations of different views (e.g., image and its caption) of the same data point while keeping the representations of different data points away from each other. However, from a theoretical perspective, it is unclear how contrastive learning can learn the representations from different views efficiently, especially when the data is not isotropic. In this work, we analyze the training dynamics of a simple multimodal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. In particular, we show that the positive pairs will drive the model to align the representations at the cost of increasing the condition number, while the negative pairs will reduce the condition number, keeping the learned representations balance",
    "link": "http://arxiv.org/abs/2304.03717",
    "context": "Title: On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])\nAbstract: Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021)) have received huge success in multimodal learning, where the model tries to minimize the distance between the representations of different views (e.g., image and its caption) of the same data point while keeping the representations of different data points away from each other. However, from a theoretical perspective, it is unclear how contrastive learning can learn the representations from different views efficiently, especially when the data is not isotropic. In this work, we analyze the training dynamics of a simple multimodal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. In particular, we show that the positive pairs will drive the model to align the representations at the cost of increasing the condition number, while the negative pairs will reduce the condition number, keeping the learned representations balance",
    "path": "papers/23/04/2304.03717.json",
    "total_tokens": 822,
    "translated_title": "关于对比损失在多模态学习中的重要性",
    "translated_abstract": "最近，对比学习方法（例如 CLIP（Radford 等人，2021））在多模态学习中取得了巨大的成功，其中模型尝试最小化同一数据点的不同视图（例如图像和其标题）的表示之间的距离，同时使不同数据点的表示彼此分离。然而，从理论的角度来看，当数据不是各向同性时，对比学习如何有效地学习来自不同视图的表示仍不清楚。在这项工作中，我们分析了一个简单的多模态对比学习模型的训练动态，并表明对比对是模型能够有效平衡所学表示的重要因素。尤其是，我们表明正对是能够推动模型在增加条件数的代价下对齐表示，而负对则降低条件数，保持学习到的表示平衡。",
    "tldr": "对比损失在多模态学习中是非常重要的，使模型能够有效地平衡所学表示，正对推动模型对齐表示，而负对则保持学习到的表示平衡。",
    "en_tdlr": "Contrastive loss is important in multimodal learning, enabling the model to balance learned representations effectively, with positive pairs driving the model to align representations and negative pairs keeping the learned representations balanced."
}