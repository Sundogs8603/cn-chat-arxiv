{
    "title": "Regularizing Second-Order Influences for Continual Learning. (arXiv:2304.10177v1 [cs.LG])",
    "abstract": "Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowledge. Prevalent replay-based methods address this challenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required. However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection. Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence functions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To regularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient implementation for optimizing the proposed criterion. Experiment",
    "link": "http://arxiv.org/abs/2304.10177",
    "context": "Title: Regularizing Second-Order Influences for Continual Learning. (arXiv:2304.10177v1 [cs.LG])\nAbstract: Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowledge. Prevalent replay-based methods address this challenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required. However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection. Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence functions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To regularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient implementation for optimizing the proposed criterion. Experiment",
    "path": "papers/23/04/2304.10177.json",
    "total_tokens": 830,
    "translated_title": "正则化连续学习中的二阶影响",
    "translated_abstract": "连续学习旨在学习非静态数据流，而不会灾难性地忘记以前学到的知识。目前的回放方法通过在保存已经看过的数据的小缓冲区上演奏，解决了这个难题，需要一个精细的样本选择策略。然而，现有的选择方案通常只寻求最大化正在进行的选择的效用，忽略了在选择的连续回合之间的干扰。受此启发，我们在影响函数的框架下剖析了顺序选择步骤的相互作用。我们设法识别出一类新的二阶影响，它将逐渐放大重复缓冲区中的偶然偏差，并损害选择过程。为了规范二阶效应，提出了一种新的选择目标，其还与两种广泛采用的标准具有明显的联系。此外，我们还提出了一种有效的实现方法，以优化所提出的标准。",
    "tldr": "本文提出了一种新的选择目标来规范连续学习中的二阶影响，并提供了一种有效的实现方法来优化所提出的标准。",
    "en_tdlr": "This paper proposes a novel selection objective to regularize second-order effects in continual learning and provides an efficient implementation to optimize the criterion."
}