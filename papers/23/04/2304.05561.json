{
    "title": "On the Adversarial Inversion of Deep Biometric Representations. (arXiv:2304.05561v1 [cs.CV])",
    "abstract": "Biometric authentication service providers often claim that it is not possible to reverse-engineer a user's raw biometric sample, such as a fingerprint or a face image, from its mathematical (feature-space) representation. In this paper, we investigate this claim on the specific example of deep neural network (DNN) embeddings. Inversion of DNN embeddings has been investigated for explaining deep image representations or synthesizing normalized images. Existing studies leverage full access to all layers of the original model, as well as all possible information on the original dataset. For the biometric authentication use case, we need to investigate this under adversarial settings where an attacker has access to a feature-space representation but no direct access to the exact original dataset nor the original learned model. Instead, we assume varying degree of attacker's background knowledge about the distribution of the dataset as well as the original learned model (architecture and t",
    "link": "http://arxiv.org/abs/2304.05561",
    "context": "Title: On the Adversarial Inversion of Deep Biometric Representations. (arXiv:2304.05561v1 [cs.CV])\nAbstract: Biometric authentication service providers often claim that it is not possible to reverse-engineer a user's raw biometric sample, such as a fingerprint or a face image, from its mathematical (feature-space) representation. In this paper, we investigate this claim on the specific example of deep neural network (DNN) embeddings. Inversion of DNN embeddings has been investigated for explaining deep image representations or synthesizing normalized images. Existing studies leverage full access to all layers of the original model, as well as all possible information on the original dataset. For the biometric authentication use case, we need to investigate this under adversarial settings where an attacker has access to a feature-space representation but no direct access to the exact original dataset nor the original learned model. Instead, we assume varying degree of attacker's background knowledge about the distribution of the dataset as well as the original learned model (architecture and t",
    "path": "papers/23/04/2304.05561.json",
    "total_tokens": 821,
    "translated_title": "关于深度生物特征表示的对抗性反演",
    "translated_abstract": "生物认证服务提供商通常声称，无法从其数学（特征空间）表示提取出用户的原始生物特征样本，例如指纹或面部图像。本文在深度神经网络（DNN）嵌入的特定示例上研究了这一主张。现有研究利用了原始模型的所有层以及所有可能的数据集信息，研究反演DNN嵌入以解释深度图像表示或合成标准化图像。对于生物认证用例，我们需要在对抗性设置下进行研究，其中攻击者可以访问特征空间表示，但无法直接访问确切的原始数据集或原始学习模型。",
    "tldr": "本研究研究了对抗性背景下，无法直接访问原始数据集和模型的情况下的生物特征的反演，证明在一定程度的攻击背景知识下，黑盒模型生物特征表示仍然很容易被反演。",
    "en_tdlr": "This paper investigates the adversarial inversion of biometric representation in the context of black-box models, demonstrating that under certain levels of attacker knowledge, it is possible to invert deep neural network embeddings of biometric data even without direct access to the original data or model."
}