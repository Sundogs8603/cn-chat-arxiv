{
    "title": "Non-Generative Energy Based Models. (arXiv:2304.01297v1 [cs.LG])",
    "abstract": "Energy-based models (EBM) have become increasingly popular within computer vision. EBMs bring a probabilistic approach to training deep neural networks (DNN) and have been shown to enhance performance in areas such as calibration, out-of-distribution detection, and adversarial resistance. However, these advantages come at the cost of estimating input data probabilities, usually using a Langevin based method such as Stochastic Gradient Langevin Dynamics (SGLD), which bring additional computational costs, require parameterization, caching methods for efficiency, and can run into stability and scaling issues. EBMs use dynamical methods to draw samples from the probability density function (PDF) defined by the current state of the network and compare them to the training data using a maximum log likelihood approach to learn the correct PDF.  We propose a non-generative training approach, Non-Generative EBM (NG-EBM), that utilizes the {\\it{Approximate Mass}}, identified by Grathwohl et al.,",
    "link": "http://arxiv.org/abs/2304.01297",
    "context": "Title: Non-Generative Energy Based Models. (arXiv:2304.01297v1 [cs.LG])\nAbstract: Energy-based models (EBM) have become increasingly popular within computer vision. EBMs bring a probabilistic approach to training deep neural networks (DNN) and have been shown to enhance performance in areas such as calibration, out-of-distribution detection, and adversarial resistance. However, these advantages come at the cost of estimating input data probabilities, usually using a Langevin based method such as Stochastic Gradient Langevin Dynamics (SGLD), which bring additional computational costs, require parameterization, caching methods for efficiency, and can run into stability and scaling issues. EBMs use dynamical methods to draw samples from the probability density function (PDF) defined by the current state of the network and compare them to the training data using a maximum log likelihood approach to learn the correct PDF.  We propose a non-generative training approach, Non-Generative EBM (NG-EBM), that utilizes the {\\it{Approximate Mass}}, identified by Grathwohl et al.,",
    "path": "papers/23/04/2304.01297.json",
    "total_tokens": 1007,
    "translated_title": "非生成的能量模型",
    "translated_abstract": "能量基模型在计算机视觉中越来越受欢迎。EBM将概率方法引入到深度神经网络（DNN）的训练中，并已经表明在校准、越界检测和对抗性抵抗等领域中可以提高性能。然而，这些优点付出了估算输入数据概率的代价，通常使用Langevin方法，如随机梯度Langevin动力学（SGLD），这带来了额外的计算成本，需要参数化、缓存方法来提高效率，并且可能会遇到稳定性和规模化问题。EBM使用动态方法从由网络当前状态定义的概率密度函数（PDF）中抽取样本，并使用最大对数似然方法将其与训练数据进行比较，以学习正确的PDF。我们提出了一种非生成式训练方法，称为非生成式EBM（NG-EBM），它利用Grathwohl等人确定的“近似质量”作为衡量模型质量的指标，而不是估算输入数据概率。这种转变可以实现更快的训练、更少的内存需求和更好的可扩展性。我们在几个图像分类和生成任务上证明了我们方法的有效性。",
    "tldr": "NG-EBM是一种非生成式的能量基模型训练方法，它利用近似质量作为衡量模型质量的指标，可以实现更快的训练、更少的内存需求和更好的可扩展性。",
    "en_tdlr": "NG-EBM is a non-generative training approach for energy-based models that utilizes the \"Approximate Mass\" as a measure of the model's quality, leading to faster training, reduced memory requirements, and improved scalability."
}