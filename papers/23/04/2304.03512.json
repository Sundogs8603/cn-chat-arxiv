{
    "title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])",
    "abstract": "Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met",
    "link": "http://arxiv.org/abs/2304.03512",
    "context": "Title: Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])\nAbstract: Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met",
    "path": "papers/23/04/2304.03512.json",
    "total_tokens": 1005,
    "translated_title": "文献综述的分层目录生成：一个基准测试",
    "translated_abstract": "多文档科学摘要可以从大量的论文中提取和组织重要信息，最近引起了广泛关注。然而，现有的研究主要集中在产生缺乏清晰和逻辑层次结构的冗长概述上。为了缓解这个问题，我们提出了一个名为“Hierarchical Catalogue Generation for Literature Review (HiCatGLR)”的原子和具有挑战性的任务，其目标是根据各种参考文献为综述论文生成分层目录。我们精心构建了一个新的英文文献综述分层目录数据集(HiCaD)，其中包含13.8k篇文献综述目录和120k篇参考论文，并通过端到端和流水线方法进行了各种实验的基准测试。为了准确评估模型性能，我们设计了从语义和结构上与参考标准相似度的评估指标。此外，我们的广泛分析验证了我们数据集的高质量和我们评估指标的有效性。",
    "tldr": "研究提出了一个名为HiCatGLR任务，致力于为文献综述生成分层目录，它可以从多篇论文中提取和组织重要信息。为了解决现有研究中缺少清晰逻辑层次结构概述的问题，提供了一个具有挑战性的解决方案并创建了新的数据集。通过此项研究，可以更加准确地评估模型性能并验证数据集的高质量和评估指标的有效性。",
    "en_tdlr": "The study proposes the task of HiCatGLR, aiming to generate hierarchical catalogues for literature reviews by extracting and organizing important information from multiple papers. It provides a challenging solution to the problem of lacking clear and logical hierarchy in current research and creates a new dataset. Through this study, more accurate model performance evaluation can be achieved, and the high quality of the dataset and the effectiveness of the evaluation metrics can be verified."
}