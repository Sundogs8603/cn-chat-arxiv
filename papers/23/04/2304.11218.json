{
    "title": "Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK. (arXiv:2304.11218v1 [cs.CY])",
    "abstract": "Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an",
    "link": "http://arxiv.org/abs/2304.11218",
    "context": "Title: Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK. (arXiv:2304.11218v1 [cs.CY])\nAbstract: Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an",
    "path": "papers/23/04/2304.11218.json",
    "total_tokens": 991,
    "translated_title": "AI政策的可解释性：欧盟、美国和英国通信、报告、规则和标准的批判性审查",
    "translated_abstract": "近年来，公众对人工智能（AI）系统的可解释性引起了越来越多的关注，以提供人类监督的方法。这已经转化为大量的研究成果，例如来自可解释AI的成果，以增强系统的透明度和控制，进行系统调试和监控，并提高用户服务的系统流程和输出的可理解性。然而，由于缺乏共同的监管基线和解释的情境性，这些产出在实践层面上很难采用。政府政策现在正在尝试解决这一迫切需求，然而，发表的通信、规章和标准在多大程度上采用了有根据的观点，支持研究、产业和公民利益，这仍然不清楚。在本研究中，我们对欧盟、美国和英国的可解释性政策和标准文献进行了首次主题和差距分析。通过对政策文件进行严格的调查，我们首先对这个大量的政策和标准做出了一个贡献。",
    "tldr": "本文对欧盟、美国和英国关于AI可解释性的政策和标准文献进行了主题和差距分析，旨在解决由于缺乏共同的监管基线和解释的情境性而难以采用的可解释性产出的问题。"
}