{
    "title": "Deeply-Coupled Convolution-Transformer with Spatial-temporal Complementary Learning for Video-based Person Re-identification. (arXiv:2304.14122v1 [cs.CV])",
    "abstract": "Advanced deep Convolutional Neural Networks (CNNs) have shown great success in video-based person Re-Identification (Re-ID). However, they usually focus on the most obvious regions of persons with a limited global representation ability. Recently, it witnesses that Transformers explore the inter-patch relations with global observations for performance improvements. In this work, we take both sides and propose a novel spatial-temporal complementary learning framework named Deeply-Coupled Convolution-Transformer (DCCT) for high-performance video-based person Re-ID. Firstly, we couple CNNs and Transformers to extract two kinds of visual features and experimentally verify their complementarity. Further, in spatial, we propose a Complementary Content Attention (CCA) to take advantages of the coupled structure and guide independent features for spatial complementary learning. In temporal, a Hierarchical Temporal Aggregation (HTA) is proposed to progressively capture the inter-frame dependenc",
    "link": "http://arxiv.org/abs/2304.14122",
    "context": "Title: Deeply-Coupled Convolution-Transformer with Spatial-temporal Complementary Learning for Video-based Person Re-identification. (arXiv:2304.14122v1 [cs.CV])\nAbstract: Advanced deep Convolutional Neural Networks (CNNs) have shown great success in video-based person Re-Identification (Re-ID). However, they usually focus on the most obvious regions of persons with a limited global representation ability. Recently, it witnesses that Transformers explore the inter-patch relations with global observations for performance improvements. In this work, we take both sides and propose a novel spatial-temporal complementary learning framework named Deeply-Coupled Convolution-Transformer (DCCT) for high-performance video-based person Re-ID. Firstly, we couple CNNs and Transformers to extract two kinds of visual features and experimentally verify their complementarity. Further, in spatial, we propose a Complementary Content Attention (CCA) to take advantages of the coupled structure and guide independent features for spatial complementary learning. In temporal, a Hierarchical Temporal Aggregation (HTA) is proposed to progressively capture the inter-frame dependenc",
    "path": "papers/23/04/2304.14122.json",
    "total_tokens": 951,
    "translated_title": "带有空时互补学习的深度耦合卷积Transformer用于基于视频的人员再识别",
    "translated_abstract": "先进的深度卷积神经网络在基于视频的人员再识别中取得了巨大成功。然而，它们通常专注于人员的最明显的区域，具有有限的全局表示能力。最近，Transformer探索了全局观察下的补丁间关系以提高性能。在这项工作中，我们结合了卷积神经网络和Transformer，提出了一种名为Deeply-Coupled Convolution-Transformer (DCCT)的新型空时互补学习框架，用于高性能的基于视频的人员再识别。首先，我们将CNN和Transformer组合起来提取两种视觉特征，并通过实验证明了它们的互补性。进一步在空间上，我们提出了互补内容注意(CCA)来利用耦合结构，并指导独立特征进行空间互补学习。在时间上，我们提出了分层时间聚合(HTA)来逐步捕捉帧间的依赖性。",
    "tldr": "本论文结合卷积神经网络和Transformer，提出了一种名为Deeply-Coupled Convolution-Transformer的新型空时互补学习框架，用于高性能的基于视频的人员再识别，并通过互补内容注意和分层时间聚合，实验验证了其优越性能。",
    "en_tdlr": "This paper proposes a novel spatial-temporal complementary learning framework named Deeply-Coupled Convolution-Transformer (DCCT) for high-performance video-based person Re-ID by combining convolutional neural networks and transformers and experimentally verifying their complementarity using Complementary Content Attention and Hierarchical Temporal Aggregation."
}