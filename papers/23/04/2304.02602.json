{
    "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models. (arXiv:2304.02602v1 [cs.CV])",
    "abstract": "We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.",
    "link": "http://arxiv.org/abs/2304.02602",
    "context": "Title: Generative Novel View Synthesis with 3D-Aware Diffusion Models. (arXiv:2304.02602v1 [cs.CV])\nAbstract: We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.",
    "path": "papers/23/04/2304.02602.json",
    "total_tokens": 900,
    "translated_title": "基于3D感知扩散模型的生成新视角插图",
    "translated_abstract": "我们提出了一个基于扩散的模型，用于从仅有的一个输入图像生成3D感知的新视角插图。我们的模型从与输入一致的可能渲染分布中进行采样，即使存在歧义，也能够生成多样化且逼真的新视角。为了实现这一点，我们的方法利用现有的二维扩散主干，但关键地，将3D特征体积作为几何先验项加入其中。这个潜在的特征场捕捉了可能的场景表示分布，并提高了我们的方法生成视角一致的新渲染的能力。除了生成新视角，我们的方法还能自回归地合成3D一致的序列。我们展示了对于合成渲染和房间规模的场景的最先进的结果。我们也展示了对于具有挑战性的真实对象的令人信服的结果。",
    "tldr": "我们提出了一个基于扩散的模型，用于从仅有的一个输入图像生成3D感知的新视角插图，通过整合3D几何先验项，使生成结果更加逼真，能够生成视角一致的新渲染，且具有生成3D一致的序列的能力。",
    "en_tdlr": "We propose a diffusion-based model to generate 3D-aware novel views from a single input image, utilizing a 3D feature volume to improve view consistency, and capable of autoregressively synthesizing 3D-consistent sequences. State-of-the-art results are achieved on synthetic renderings and room-scale scenes, as well as real-world objects."
}