{
    "title": "GINA-3D: Learning to Generate Implicit Neural Assets in the Wild. (arXiv:2304.02163v1 [cs.CV])",
    "abstract": "Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling re",
    "link": "http://arxiv.org/abs/2304.02163",
    "context": "Title: GINA-3D: Learning to Generate Implicit Neural Assets in the Wild. (arXiv:2304.02163v1 [cs.CV])\nAbstract: Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling re",
    "path": "papers/23/04/2304.02163.json",
    "total_tokens": 948,
    "translated_title": "GINA-3D：在真实场景中学习生成隐式神经资产",
    "translated_abstract": "从传感器数据中建模3D世界以进行仿真是开发自动驾驶等机器人学习问题的测试和验证环境的可扩展方法。然而，手动创建或重新创建类似真实世界的环境是困难，昂贵且不可扩展的。最近的生成模型技术，通过仅使用丰富的2D图像来学习3D资产，已经显示出解决这些挑战的有希望的进展 -- 但仍然存在局限性，因为它们利用人类策划的图像数据集或手动创建的合成3D环境的渲染。在本文中，我们介绍GINA-3D，这是一个生成模型，它使用来自相机和LiDAR传感器的真实驾驶数据创建真实3D隐式神经资产，包括各种车辆和行人。与现有的图像数据集相比，真实驾驶环境由于遮挡，光照变化和长尾分布而面临新的挑战。GINA-3D通过解耦从单个视角生成3D达到了解决这些挑战的目的，进而使3D场景的自动化成为可能。",
    "tldr": "GINA-3D是一种学习从真实场景中生成3D隐式神经资产的生成模型，这对于自主驾驶等机器人学习问题的测试和验证环境是重要的创新。",
    "en_tdlr": "GINA-3D is a generative model that learns to generate 3D implicit neural assets from real-world scenes, which is a significant innovation for developing testing and validation environments for robotic learning problems like autonomous driving."
}