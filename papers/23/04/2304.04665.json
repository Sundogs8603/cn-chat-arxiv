{
    "title": "On algorithmically boosting fixed-point computations. (arXiv:2304.04665v2 [cs.GT] UPDATED)",
    "abstract": "The main topic of this paper are algorithms for computing Nash equilibria. We cast our particular methods as instances of a general algorithmic abstraction, namely, a method we call {\\em algorithmic boosting}, which is also relevant to other fixed-point computation problems. Algorithmic boosting is the principle of computing fixed points by taking (long-run) averages of iterated maps and it is a generalization of exponentiation. We first define our method in the setting of nonlinear maps. Secondly, we restrict attention to convergent linear maps (for computing dominant eigenvectors, for example, in the PageRank algorithm) and show that our algorithmic boosting method can set in motion {\\em exponential speedups in the convergence rate}. Thirdly, we show that algorithmic boosting can convert a (weak) non-convergent iterator to a (strong) convergent one. We also consider a {\\em variational approach} to algorithmic boosting providing tools to convert a non-convergent continuous flow to a c",
    "link": "http://arxiv.org/abs/2304.04665",
    "context": "Title: On algorithmically boosting fixed-point computations. (arXiv:2304.04665v2 [cs.GT] UPDATED)\nAbstract: The main topic of this paper are algorithms for computing Nash equilibria. We cast our particular methods as instances of a general algorithmic abstraction, namely, a method we call {\\em algorithmic boosting}, which is also relevant to other fixed-point computation problems. Algorithmic boosting is the principle of computing fixed points by taking (long-run) averages of iterated maps and it is a generalization of exponentiation. We first define our method in the setting of nonlinear maps. Secondly, we restrict attention to convergent linear maps (for computing dominant eigenvectors, for example, in the PageRank algorithm) and show that our algorithmic boosting method can set in motion {\\em exponential speedups in the convergence rate}. Thirdly, we show that algorithmic boosting can convert a (weak) non-convergent iterator to a (strong) convergent one. We also consider a {\\em variational approach} to algorithmic boosting providing tools to convert a non-convergent continuous flow to a c",
    "path": "papers/23/04/2304.04665.json",
    "total_tokens": 909,
    "translated_title": "关于算法增强固定点计算的论文",
    "translated_abstract": "本文的主要内容是关于计算纳什均衡的算法。我们将我们的特定方法转化为一个称之为\"算法增强\"的通用算法抽象，该抽象对于其他固定点计算问题也是相关的。算法增强是通过迭代映射的长期平均来计算固定点的原理，它是指数运算的一种推广。我们首先在非线性映射的框架中定义了我们的方法。其次，我们将注意力限制在收敛的线性映射上（例如，在PageRank算法中计算优势特征向量），并展示了我们的算法增强方法可以以指数速度加速收敛。第三，我们展示算法增强可以将一个（弱）不收敛的迭代算法转换为（强）收敛的算法。我们还考虑了算法增强的一种\"变分方法\"，该方法提供了将非收敛连续流转换为收敛的工具。",
    "tldr": "本论文提出了一种称为\"算法增强\"的通用算法抽象，针对纳什均衡问题进行了研究，并展示了该方法可以以指数速度加速线性映射的收敛，同时还可以将非收敛的迭代算法转换为收敛的算法。",
    "en_tdlr": "This paper presents a generic algorithmic abstraction called \"algorithmic boosting\" for computing Nash equilibria, and shows that it can exponentially speed up the convergence of linear maps while also converting non-convergent iterative algorithms to convergent ones."
}