{
    "title": "Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])",
    "abstract": "In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\\epsilon$-stationary points with $\\mathcal{O}(\\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien",
    "link": "http://arxiv.org/abs/2304.13972",
    "context": "Title: Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])\nAbstract: In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\\epsilon$-stationary points with $\\mathcal{O}(\\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien",
    "path": "papers/23/04/2304.13972.json",
    "total_tokens": 881,
    "translated_title": "松弛假设下Adam收敛性的证明",
    "translated_abstract": "本文针对一类广泛的优化目标，对自适应矩估计（Adam）算法的收敛性进行了严格证明。虽然Adam算法在训练深度神经网络中的流行度和效率很高，但其理论性质尚未完全理解，现有的收敛性证明需要过于强的假设，如全局梯度有界，以证明收敛到稳定点。本文证明了在更为现实的条件下，Adam能以$\\mathcal{O}(\\epsilon^{-4})$梯度复杂度收敛到$\\epsilon$-稳定点。我们分析的关键是根据一种广义光滑性假设给出的，沿着优化轨迹的梯度有界的新证明。根据该假设，局部光滑性(即存在时的Hessian norm)受梯度范数的次平方函数限制。此外，我们提出了一种方差约减版本的Adam与加速Gradient。",
    "tldr": "本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。",
    "en_tdlr": "This paper presents a new proof for the convergence of Adam algorithm under relaxed assumptions, showing that it can achieve $\\epsilon$-stationary points with $\\mathcal{O}(\\epsilon^{-4})$ gradient complexity under more realistic conditions, and proposes a variance-reduced version of Adam with an accelerated gradient."
}