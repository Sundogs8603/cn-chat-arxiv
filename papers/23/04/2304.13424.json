{
    "title": "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])",
    "abstract": "In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \\emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \\emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on",
    "link": "http://arxiv.org/abs/2304.13424",
    "context": "Title: Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])\nAbstract: In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \\emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \\emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on",
    "path": "papers/23/04/2304.13424.json",
    "total_tokens": 866,
    "translated_title": "与陌生人一起跑接力赛？强化学习在超出分布轨迹上的泛化能力",
    "translated_abstract": "本文定义、评估和改进各种状态下的强化学习（RL）代理对超出分布的“可控”状态的“接力泛化”性能。通过将测试代理从其他独立训练良好的“陌生”代理的轨迹的中间开始，我们实际评估了这种泛化类型。通过大量实验评估，我们展示了来自陌生代理的可控状态几乎普遍存在泛化失效。例如，在人形环境中，我们观察到一个经过良好训练的PPO代理，在正常测试期间只有3.9％的失败率，但在10个陌生代理的轨迹中，失败率升高到31.4％。",
    "tldr": "本文研究了强化学习代理对超出分布的“可控”状态的“接力泛化”性能。通过让测试代理从其他陌生代理的轨迹中间开始，发现这种泛化普遍存在泛化失效问题。",
    "en_tdlr": "This paper investigates the \"relay-generalization\" performance of reinforcement learning (RL) agents on out-of-distribution \"controllable\" states. It is found that such generalization failure is prevalent when testing agents start from the middle of trajectories of other independently trained \"stranger\" agents."
}