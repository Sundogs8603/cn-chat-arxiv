{
    "title": "Revisiting Single-gated Mixtures of Experts. (arXiv:2304.05497v1 [cs.CV])",
    "abstract": "Mixture of Experts (MoE) are rising in popularity as a means to train extremely large-scale models, yet allowing for a reasonable computational cost at inference time. Recent state-of-the-art approaches usually assume a large number of experts, and require training all experts jointly, which often lead to training instabilities such as the router collapsing In contrast, in this work, we propose to revisit the simple single-gate MoE, which allows for more practical training. Key to our work are (i) a base model branch acting both as an early-exit and an ensembling regularization scheme, (ii) a simple and efficient asynchronous training pipeline without router collapse issues, and finally (iii) a per-sample clustering-based initialization. We show experimentally that the proposed model obtains efficiency-to-accuracy trade-offs comparable with other more complex MoE, and outperforms non-mixture baselines. This showcases the merits of even a simple single-gate MoE, and motivates further ex",
    "link": "http://arxiv.org/abs/2304.05497",
    "context": "Title: Revisiting Single-gated Mixtures of Experts. (arXiv:2304.05497v1 [cs.CV])\nAbstract: Mixture of Experts (MoE) are rising in popularity as a means to train extremely large-scale models, yet allowing for a reasonable computational cost at inference time. Recent state-of-the-art approaches usually assume a large number of experts, and require training all experts jointly, which often lead to training instabilities such as the router collapsing In contrast, in this work, we propose to revisit the simple single-gate MoE, which allows for more practical training. Key to our work are (i) a base model branch acting both as an early-exit and an ensembling regularization scheme, (ii) a simple and efficient asynchronous training pipeline without router collapse issues, and finally (iii) a per-sample clustering-based initialization. We show experimentally that the proposed model obtains efficiency-to-accuracy trade-offs comparable with other more complex MoE, and outperforms non-mixture baselines. This showcases the merits of even a simple single-gate MoE, and motivates further ex",
    "path": "papers/23/04/2304.05497.json",
    "total_tokens": 920,
    "translated_title": "重新审视单门限专家混合模型",
    "translated_abstract": "专家混合模型（MoE）由于其能够在训练极其大规模模型的同时，允许合理的推理计算成本，因而越来越受欢迎。但是，最近的最先进方法通常假设有很多专家，并要求联合训练所有专家，这经常导致路由器坍塌等训练不稳定问题。相反，在这项工作中，我们建议重新审视简单的单门限MoE，这使得其训练更为实用。我们的工作的主要优势在于：（i）基础模型分为早期退出和集成正则化方案，（ii）一个简单且高效的异步训练管道，无路由器崩溃问题，以及（iii）每个样本基于聚类的初始化。实验证明，所提出的模型获得了与其他更复杂的MoE相当的效率-准确性权衡，并且优于非混合基线。这展示了即使是简单的单门限MoE的优点，并激励进一步的探索。",
    "tldr": "本文重新审视了单门限专家混合模型(MoE)，提出了一种更加实用的训练方式，且实验证明该模型的效率-准确性权衡具备竞争力并优于非混合基线。",
    "en_tdlr": "This paper revisits the single-gated mixture of experts (MoE) and proposes a more practical training method. Experimental results show the proposed method achieves competitive efficiency-to-accuracy trade-offs compared to more complex MoE models and outperforms non-mixture baselines."
}