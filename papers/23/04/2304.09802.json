{
    "title": "Generalization and Estimation Error Bounds for Model-based Neural Networks. (arXiv:2304.09802v1 [cs.LG])",
    "abstract": "Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced ",
    "link": "http://arxiv.org/abs/2304.09802",
    "context": "Title: Generalization and Estimation Error Bounds for Model-based Neural Networks. (arXiv:2304.09802v1 [cs.LG])\nAbstract: Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced ",
    "path": "papers/23/04/2304.09802.json",
    "total_tokens": 835,
    "translated_title": "基于模型的神经网络的泛化和估计误差界限",
    "translated_abstract": "基于模型的神经网络在各种任务（如稀疏编码和压缩感知问题）中提供了无与伦比的性能。由于与传感模型的强关联，这些网络是可解释的并继承了问题的先前结构。实践中，与ReLU神经网络相比，基于模型的神经网络表现出更高的泛化能力。然而，这种现象在理论上还没有得到解决。在本文中，我们利用复杂度量（包括全局和局部Rademacher复杂度）的方法，为基于模型的网络提供泛化和估计误差的上限。我们表明，基于模型的网络在稀疏恢复方面的泛化能力优于常规的ReLU网络，并导出允许构建具有高保证性的基于模型的网络的实际设计规则。我们通过一系列实验演示了我们的理论见解为深度学习实践者提供了一些启示。",
    "tldr": "基于模型的神经网络在稀疏恢复中表现出较高的泛化能力，复杂度量有助于提高其泛化和估计误差界限",
    "en_tdlr": "Model-based neural networks exhibit higher generalization capability in sparse recovery, and complexity measures can help improve their generalization and estimation error bounds."
}