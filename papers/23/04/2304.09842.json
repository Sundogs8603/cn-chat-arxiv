{
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in various natural language processing tasks with emergent abilities. However, they face inherent limitations, such as an inability to access up-to-date information, utilize external tools, or perform precise mathematical reasoning. In this paper, we introduce Chameleon, a plug-and-play compositional reasoning framework that augments LLMs to help address these challenges. Chameleon synthesizes programs to compose various tools, including LLM models, off-the-shelf vision models, web search engines, Python functions, and rule-based modules tailored to user interests. Built on top of an LLM as a natural language planner, Chameleon infers the appropriate sequence of tools to compose and execute in order to generate a final response. We showcase the adaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP. Notably, Chameleon with GPT-4 achieves an 86.54% accuracy on ScienceQA, significantly improving upon t",
    "link": "http://arxiv.org/abs/2304.09842",
    "context": "Title: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v1 [cs.CL])\nAbstract: Large language models (LLMs) have achieved remarkable progress in various natural language processing tasks with emergent abilities. However, they face inherent limitations, such as an inability to access up-to-date information, utilize external tools, or perform precise mathematical reasoning. In this paper, we introduce Chameleon, a plug-and-play compositional reasoning framework that augments LLMs to help address these challenges. Chameleon synthesizes programs to compose various tools, including LLM models, off-the-shelf vision models, web search engines, Python functions, and rule-based modules tailored to user interests. Built on top of an LLM as a natural language planner, Chameleon infers the appropriate sequence of tools to compose and execute in order to generate a final response. We showcase the adaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP. Notably, Chameleon with GPT-4 achieves an 86.54% accuracy on ScienceQA, significantly improving upon t",
    "path": "papers/23/04/2304.09842.json",
    "total_tokens": 958,
    "translated_abstract": "大语言模型（LLM）在各种自然语言处理任务中取得了显着进展，具有新兴的能力。然而，它们面临固有的限制，例如无法访问最新信息、利用外部工具或进行精确的数学推理。本文介绍了Chameleon，这是一个即插即用的组合推理框架，可增强LLM以应对这些挑战。Chamelon综合合成程序以组合各种工具，包括LLM模型、现成的视觉模型、网络搜索引擎、Python函数和针对用户兴趣的基于规则的模块。Chameleon以LLM为自然语言规划器构建，推断合适的工具序列以组合并执行，生成最终响应。我们展示了Chameleon在两个任务上的适应性和有效性：ScienceQA和TabMWP。值得注意的是，具有GPT-4的Chameleon在ScienceQA上实现了86.54％的准确性，明显优于 ...",
    "tldr": "Chameleon是一个增强大语言模型的组合推理框架，可以合成各种工具来应对LLMs无法处理的挑战，包括访问最新信息、利用外部工具或进行精确的数学推理。基于LLM，Chameleon推断出最合适的工具序列以生成最终响应。",
    "en_tdlr": "Chameleon is a compositional reasoning framework that augments Large Language Models (LLMs) to address their inherent limitations, including the inability to access up-to-date information, external tools, or perform precise mathematical reasoning. Built on top of an LLM, Chameleon synthesizes programs to compose various tools and infers the appropriate sequence of tools to execute in order to generate a final response."
}