{
    "title": "Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning. (arXiv:2304.10638v1 [cs.LG])",
    "abstract": "Federated Learning (FL) enables collaborative deep learning training across multiple participants without exposing sensitive personal data. However, the distributed nature of FL and the unvetted participants' data makes it vulnerable to backdoor attacks. In these attacks, adversaries inject malicious functionality into the centralized model during training, leading to intentional misclassifications for specific adversary-chosen inputs. While previous research has demonstrated successful injections of persistent backdoors in FL, the persistence also poses a challenge, as their existence in the centralized model can prompt the central aggregation server to take preventive measures to penalize the adversaries. Therefore, this paper proposes a methodology that enables adversaries to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents stra",
    "link": "http://arxiv.org/abs/2304.10638",
    "context": "Title: Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning. (arXiv:2304.10638v1 [cs.LG])\nAbstract: Federated Learning (FL) enables collaborative deep learning training across multiple participants without exposing sensitive personal data. However, the distributed nature of FL and the unvetted participants' data makes it vulnerable to backdoor attacks. In these attacks, adversaries inject malicious functionality into the centralized model during training, leading to intentional misclassifications for specific adversary-chosen inputs. While previous research has demonstrated successful injections of persistent backdoors in FL, the persistence also poses a challenge, as their existence in the centralized model can prompt the central aggregation server to take preventive measures to penalize the adversaries. Therefore, this paper proposes a methodology that enables adversaries to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents stra",
    "path": "papers/23/04/2304.10638.json",
    "total_tokens": 905,
    "translated_title": "摆脱隐患：远程擦除联邦学习中的后门",
    "translated_abstract": "联邦学习（FL）使多个参与者能够在不暴露敏感个人数据的情况下进行协作深度学习训练。然而，FL的分布式性质和未经审核的参与者数据使其容易受到后门攻击。在这些攻击中，对手在训练期间向集中式模型注入恶意功能，导致特定对手选择的输入故意错分。虽然以前的研究已经证明了在FL中成功注入持久后门的可能性，但其持久性也带来了挑战，因为它们在集中式模型中的存在可能会促使中央聚合服务器采取预防措施来惩罚对手。因此，本文提出了一种方法，可以使敌人在实现其目标或怀疑可能被检测到时有效地从集中模型中移除后门。所提出的方法扩展了机器“遗忘”的概念，并提出了策略。",
    "tldr": "本文提出了一种远程擦除联邦学习中后门的方法，使得攻击者可以在实现目标或怀疑被检测到时有效地从集中模型中移除后门。此方法扩展了机器“遗忘”的概念，并提供了具体策略。",
    "en_tdlr": "This paper proposes a method to remotely erase backdoors in federated learning, allowing attackers to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents specific strategies."
}