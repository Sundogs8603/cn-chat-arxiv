{
    "title": "Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization. (arXiv:2304.03374v1 [cs.LG])",
    "abstract": "Automated machine learning (AutoML) methods improve upon existing models by optimizing various aspects of their design. While present methods focus on hyperparameters and neural network topologies, other aspects of neural network design can be optimized as well. To further the state of the art in AutoML, this dissertation introduces techniques for discovering more powerful activation functions and establishing more robust weight initialization for neural networks. These contributions improve performance, but also provide new perspectives on neural network optimization. First, the dissertation demonstrates that discovering solutions specialized to specific architectures and tasks gives better performance than reusing general approaches. Second, it shows that jointly optimizing different components of neural networks is synergistic, and results in better performance than optimizing individual components alone. Third, it demonstrates that learned representations are easier to optimize tha",
    "link": "http://arxiv.org/abs/2304.03374",
    "context": "Title: Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization. (arXiv:2304.03374v1 [cs.LG])\nAbstract: Automated machine learning (AutoML) methods improve upon existing models by optimizing various aspects of their design. While present methods focus on hyperparameters and neural network topologies, other aspects of neural network design can be optimized as well. To further the state of the art in AutoML, this dissertation introduces techniques for discovering more powerful activation functions and establishing more robust weight initialization for neural networks. These contributions improve performance, but also provide new perspectives on neural network optimization. First, the dissertation demonstrates that discovering solutions specialized to specific architectures and tasks gives better performance than reusing general approaches. Second, it shows that jointly optimizing different components of neural networks is synergistic, and results in better performance than optimizing individual components alone. Third, it demonstrates that learned representations are easier to optimize tha",
    "path": "papers/23/04/2304.03374.json",
    "total_tokens": 799,
    "translated_title": "通过激活函数发现和自动权重初始化优化神经网络",
    "translated_abstract": "自动机器学习（AutoML）方法通过优化已有模型的各个方面来改进。当前方法侧重于超参数和神经网络拓扑，但神经网络设计的其他方面也可以进行优化。为了进一步提高AutoML的现状，本篇论文介绍了发现更强大的激活函数和建立更稳健的神经网络权重初始化技术。这些贡献不仅提高了性能，而且提供了神经网络优化的新视角。首先，本文表明发现针对特定架构和任务的解决方案比重复使用通用方法性能更好。其次，它表明联合优化神经网络的不同组件是协同的，比单独优化组件的性能更好。第三，它证明了学习表示更容易优化。",
    "tldr": "本文介绍了发现更强大的激活函数和建立更稳健的神经网络权重初始化技术的方法，这些方法比传统方法更优秀，同时提供了神经网络优化的新视角。",
    "en_tdlr": "This paper introduces methods for discovering more powerful activation functions and establishing more robust weight initialization for neural networks, which are superior to traditional methods and provide new perspectives on neural network optimization through demonstrating that specialized solutions and joint optimization are more effective."
}