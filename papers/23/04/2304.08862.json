{
    "title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])",
    "abstract": "This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.",
    "link": "http://arxiv.org/abs/2304.08862",
    "context": "Title: Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])\nAbstract: This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.",
    "path": "papers/23/04/2304.08862.json",
    "total_tokens": 881,
    "translated_title": "近似最近邻短语挖掘在上下文语音识别中的应用",
    "translated_abstract": "本文提出了一种使用近似最近邻短语挖掘的方法来训练端到端上下文感知Transformer转录器(CATT)模型的扩展方法。在训练过程中，给定一个参考查询，我们使用近似最近邻搜索挖掘了若干相似的短语作为负例，并将这些短语与随机和真实的上下文信息一起用作上下文列表中的负例。通过将近似最近邻短语（ANN-P）包含在上下文列表中，我们鼓励学习表示来区分相似但不完全相同的偏见短语，从而在偏见清单中存在几个相似的短语时提高偏见准确性。我们在大规模数据情况下进行实验，获得了相对字误率达7％的上下文部分的实验效果。我们还扩展并评估了CATT方法在串流应用中的应用。",
    "tldr": "本文提出了一种使用近似最近邻短语挖掘的方法来训练上下文感知Transformer转录器(CATT)模型，并在大规模数据情况下进行了实验，取得了显著的实验结果。"
}