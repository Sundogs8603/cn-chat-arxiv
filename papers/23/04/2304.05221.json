{
    "title": "Towards preserving word order importance through Forced Invalidation. (arXiv:2304.05221v1 [cs.CL])",
    "abstract": "Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks. However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed. To help preserve the importance of word order, we propose a simple approach called Forced Invalidation (FI): forcing the model to identify permuted sequences as invalid samples. We perform an extensive evaluation of our approach on various English NLU and QA based tasks over BERT-based and attention-based models over word embeddings. Our experiments demonstrate that Forced Invalidation significantly improves the sensitivity of the models to word order.",
    "link": "http://arxiv.org/abs/2304.05221",
    "context": "Title: Towards preserving word order importance through Forced Invalidation. (arXiv:2304.05221v1 [cs.CL])\nAbstract: Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks. However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed. To help preserve the importance of word order, we propose a simple approach called Forced Invalidation (FI): forcing the model to identify permuted sequences as invalid samples. We perform an extensive evaluation of our approach on various English NLU and QA based tasks over BERT-based and attention-based models over word embeddings. Our experiments demonstrate that Forced Invalidation significantly improves the sensitivity of the models to word order.",
    "path": "papers/23/04/2304.05221.json",
    "total_tokens": 755,
    "translated_title": "通过强制无效化实现保护单词顺序重要性的目标",
    "translated_abstract": "大型预训练语言模型例如BERT已经广泛应用于自然语言理解（NLU）任务中。然而，最近的研究发现预训练语言模型对单词顺序不敏感。即使对一个句子中单词进行随机排列，语法上的关键信息也被破坏，但NLU任务的性能仍然不变。为了保护单词顺序的重要性，我们提出了一种简单的方法：强制无效化（Forced Invalidation，FI）：强制模型将排列错误的序列识别为无效样本。我们在基于BERT和注意力的模型上进行了各种英语NLU和QA任务的广泛评估，证明了Force Invalidation可以显著提高模型对单词顺序的敏感性。",
    "tldr": "强制无效化技术可帮助预训练语言模型识别错误的单词序列，从而提高模型对单词顺序的敏感性。",
    "en_tdlr": "Forced Invalidation technique can help pre-trained language models to identify erroneous word sequences and improve the sensitivity of the models to word order."
}