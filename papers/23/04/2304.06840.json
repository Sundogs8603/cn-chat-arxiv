{
    "title": "Structured Pruning for Multi-Task Deep Neural Networks. (arXiv:2304.06840v1 [cs.LG])",
    "abstract": "Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to ac",
    "link": "http://arxiv.org/abs/2304.06840",
    "context": "Title: Structured Pruning for Multi-Task Deep Neural Networks. (arXiv:2304.06840v1 [cs.LG])\nAbstract: Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to ac",
    "path": "papers/23/04/2304.06840.json",
    "total_tokens": 945,
    "translated_title": "多任务深度神经网络的结构化剪枝",
    "translated_abstract": "虽然相对于单个单任务DNN模型，多任务深度神经网络模型具有计算和存储优势，但是它们可以通过模型压缩进一步优化。许多结构化剪枝方法已经被开发出来，可以轻松地实现单任务模型的加速，但是对于多任务网络的剪枝尚未得到广泛研究。在这项工作中，我们研究了结构化剪枝在多任务模型上的有效性。我们使用现有的单任务滤波器剪枝准则，并引入了一个基于MTL的滤波器剪枝准则来估计滤波器重要性分数。我们使用迭代剪枝策略使用两种剪枝方法来剪枝模型。我们展示了，在仔细的超参数调整下，当参数数量相似时，来自不同剪枝方法的架构在任务之间的性能上没有显着差异。我们还展示了迭代结构剪枝可能不是实现多任务网络最优结构的最佳方法。",
    "tldr": "本研究探索了在多任务模型上应用结构化剪枝的有效性，通过实验发现，在参数数量相似的情况下，来自不同剪枝方法的架构在任务性能上没有显着差异，迭代结构剪枝可能不是实现最优结构的最佳方法。",
    "en_tdlr": "This study investigates the effectiveness of structured pruning on multi-task deep neural network models, and shows through experiments that architectures obtained from different pruning methods do not have significant differences in task performance when the number of parameters is similar. It also suggests that iterative structure pruning may not be the best method to achieve optimal structures in multi-task networks."
}