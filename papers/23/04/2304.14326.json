{
    "title": "A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints. (arXiv:2304.14326v1 [cs.LG])",
    "abstract": "We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are",
    "link": "http://arxiv.org/abs/2304.14326",
    "context": "Title: A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints. (arXiv:2304.14326v1 [cs.LG])\nAbstract: We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are",
    "path": "papers/23/04/2304.14326.json",
    "total_tokens": 860,
    "translated_title": "一种针对带长期约束的约束MDPs的双赢算法",
    "translated_abstract": "本文研究了环形约束马尔科夫决策过程（CMDPs）的在线学习，其中学习者的目标是在收集尽可能多的奖励的同时，在学习过程中保证满足一些长期约束。奖励和约束可以随机或敌对地选择，并且转移函数对学习者是未知的。虽然在经典的无约束MDPs中的在线学习在过去几年中受到了大量关注，但CMDP的设置仍然大部分未被探索。这一点令人惊讶，因为在实际应用中，例如自动驾驶、自动投标和推荐系统中，通常存在额外的约束和规范，代理必须在学习过程中遵守这些规定。本文提出了一种面向长期约束的CMDPs的双赢算法。我们的算法能够处理奖励和约束随机或敌对的情况。",
    "tldr": "本文提出了一种针对约束MDPs的双赢算法，能够处理奖励和约束随机或敌对的情况。"
}