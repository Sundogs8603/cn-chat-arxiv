{
    "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies. (arXiv:2304.02532v1 [cs.LG])",
    "abstract": "We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture \"$\\textbf{BE}$havior generation with $\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\" (BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clusteri",
    "link": "http://arxiv.org/abs/2304.02532",
    "context": "Title: Goal-Conditioned Imitation Learning using Score-based Diffusion Policies. (arXiv:2304.02532v1 [cs.LG])\nAbstract: We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture \"$\\textbf{BE}$havior generation with $\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\" (BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clusteri",
    "path": "papers/23/04/2304.02532.json",
    "total_tokens": 1017,
    "tldr": "本文介绍了一种基于得分扩散模型的目标指导模仿学习策略，利用生成式的、基于得分的扩散模型作为其策略，可以从大规模未分类数据中学习通用的目标指定策略，同时还能够快速地生成目标指定的行为。"
}