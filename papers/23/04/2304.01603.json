{
    "title": "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA. (arXiv:2304.01603v1 [cs.CV])",
    "abstract": "In this paper, we propose a novel multi-modal framework for Scene Text Visual Question Answering (STVQA), which requires models to read scene text in images for question answering. Apart from text or visual objects, which could exist independently, scene text naturally links text and visual modalities together by conveying linguistic semantics while being a visual object in an image simultaneously. Different to conventional STVQA models which take the linguistic semantics and visual semantics in scene text as two separate features, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG), which explicitly unifies this two semantics with the spatial bounding box as a bridge connecting them. Specifically, at first, LTG locates the region in an image that may contain the answer words with an answer location module (ALM) consisting of a region proposal network and a language refinement network, both of which can transform to each other with one-to-one mapping via the scene text",
    "link": "http://arxiv.org/abs/2304.01603",
    "context": "Title: Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA. (arXiv:2304.01603v1 [cs.CV])\nAbstract: In this paper, we propose a novel multi-modal framework for Scene Text Visual Question Answering (STVQA), which requires models to read scene text in images for question answering. Apart from text or visual objects, which could exist independently, scene text naturally links text and visual modalities together by conveying linguistic semantics while being a visual object in an image simultaneously. Different to conventional STVQA models which take the linguistic semantics and visual semantics in scene text as two separate features, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG), which explicitly unifies this two semantics with the spatial bounding box as a bridge connecting them. Specifically, at first, LTG locates the region in an image that may contain the answer words with an answer location module (ALM) consisting of a region proposal network and a language refinement network, both of which can transform to each other with one-to-one mapping via the scene text",
    "path": "papers/23/04/2304.01603.json",
    "total_tokens": 956,
    "tldr": "本文提出了一种新的多模态框架，用于场景文本视觉问答（STVQA）。通过空间边界框将场景文本中的语义信息和视觉特征明确地统一起来，实现了定位并生成（LTG）的范例。",
    "en_tdlr": "This paper proposes a novel multi-modal framework for Scene Text Visual Question Answering (STVQA) by unifying the linguistic semantics and visual semantics in scene text with the spatial bounding box as a bridge connecting them. The paradigm of \"Locate Then Generate\" (LTG) is implemented by using an answer location module (ALM) consisting of a region proposal network and a language refinement network."
}