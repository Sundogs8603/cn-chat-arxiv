{
    "title": "A Multiagent CyberBattleSim for RL Cyber Operation Agents. (arXiv:2304.11052v1 [cs.CR])",
    "abstract": "Hardening cyber physical assets is both crucial and labor-intensive. Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more specifically has shown great promise to automate tasks that otherwise would require significant human insight/intelligence. The development of autonomous RL agents requires a suitable training environment that allows us to quickly evaluate various alternatives, in particular how to arrange training scenarios that pit attackers and defenders against each other. CyberBattleSim is a training environment that supports the training of red agents, i.e., attackers. We added the capability to train blue agents, i.e., defenders. The paper describes our changes and reports on the results we obtained when training blue agents, either in isolation or jointly with red agents. Our results show that training a blue agent does lead to stronger defenses against attacks. In particular, training a blue agent jointly with a red agent increases the blue agent's",
    "link": "http://arxiv.org/abs/2304.11052",
    "context": "Title: A Multiagent CyberBattleSim for RL Cyber Operation Agents. (arXiv:2304.11052v1 [cs.CR])\nAbstract: Hardening cyber physical assets is both crucial and labor-intensive. Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more specifically has shown great promise to automate tasks that otherwise would require significant human insight/intelligence. The development of autonomous RL agents requires a suitable training environment that allows us to quickly evaluate various alternatives, in particular how to arrange training scenarios that pit attackers and defenders against each other. CyberBattleSim is a training environment that supports the training of red agents, i.e., attackers. We added the capability to train blue agents, i.e., defenders. The paper describes our changes and reports on the results we obtained when training blue agents, either in isolation or jointly with red agents. Our results show that training a blue agent does lead to stronger defenses against attacks. In particular, training a blue agent jointly with a red agent increases the blue agent's",
    "path": "papers/23/04/2304.11052.json",
    "total_tokens": 946,
    "translated_title": "RL网络操作代理的多智能体网络战仿真",
    "translated_abstract": "硬化网络资产既至关重要，又需要耗费大量的人力。近年来，机器学习（ML）和强化学习（RL）等技术已经展现了在自动化任务方面的巨大潜力，可以自主完成人力无法胜任的重复性任务。然而，开发自主RL代理需要一个对抗训练环境，可以快速评估各种情况，并针对不同场景进行训练。CyberBattleSim便是这样一个针对红色代理（即攻击者）的训练环境，在其基础上添加了针对蓝色代理（即防御者）的训练控制，本文介绍了我们的这些改进，以及在使用这些改进后针对蓝色代理训练时所获得的结果。我们的结果表明针对蓝色代理的训练确实能够增强其对抗攻击的能力，特别是和红色代理一起训练时其效果更佳。",
    "tldr": "本文介绍了一种新的对抗训练环境——CyberBattleSim，设计用于RL网络操作代理的训练。本文着重报道了对防御型蓝色代理训练的改进，结果表明红色代理与蓝色代理联合训练可以有效提高蓝色代理的防御能力。",
    "en_tdlr": "This paper introduces a new training environment called CyberBattleSim, designed for training RL cyber operation agents. The paper focuses on the improvement of training for defensive blue agents, and the results showed that joint training with red agents can effectively enhance the defense capability of blue agents."
}