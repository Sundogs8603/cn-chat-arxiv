{
    "title": "Towards Evaluating Explanations of Vision Transformers for Medical Imaging. (arXiv:2304.06133v1 [cs.CV])",
    "abstract": "As deep learning models increasingly find applications in critical domains such as medical imaging, the need for transparent and trustworthy decision-making becomes paramount. Many explainability methods provide insights into how these models make predictions by attributing importance to input features. As Vision Transformer (ViT) becomes a promising alternative to convolutional neural networks for image classification, its interpretability remains an open research question. This paper investigates the performance of various interpretation methods on a ViT applied to classify chest X-ray images. We introduce the notion of evaluating faithfulness, sensitivity, and complexity of ViT explanations. The obtained results indicate that Layerwise relevance propagation for transformers outperforms Local interpretable model-agnostic explanations and Attention visualization, providing a more accurate and reliable representation of what a ViT has actually learned. Our findings provide insights int",
    "link": "http://arxiv.org/abs/2304.06133",
    "context": "Title: Towards Evaluating Explanations of Vision Transformers for Medical Imaging. (arXiv:2304.06133v1 [cs.CV])\nAbstract: As deep learning models increasingly find applications in critical domains such as medical imaging, the need for transparent and trustworthy decision-making becomes paramount. Many explainability methods provide insights into how these models make predictions by attributing importance to input features. As Vision Transformer (ViT) becomes a promising alternative to convolutional neural networks for image classification, its interpretability remains an open research question. This paper investigates the performance of various interpretation methods on a ViT applied to classify chest X-ray images. We introduce the notion of evaluating faithfulness, sensitivity, and complexity of ViT explanations. The obtained results indicate that Layerwise relevance propagation for transformers outperforms Local interpretable model-agnostic explanations and Attention visualization, providing a more accurate and reliable representation of what a ViT has actually learned. Our findings provide insights int",
    "path": "papers/23/04/2304.06133.json",
    "total_tokens": 876,
    "translated_title": "面向医学图像视觉转换器解释的评估",
    "translated_abstract": "随着深度学习模型在医学图像等关键领域的应用越来越普遍，透明和可信的决策变得至关重要。许多解释方法通过将重要性归因于输入特征来为这些模型的预测提供深入的洞察。随着视觉转换器（ViT）成为卷积神经网络的一种有希望的替代方案，其可解释性仍然是一个未解决的研究问题。本文研究了多种解释方法在应用于对胸部X射线图像进行分类的ViT上的性能。我们引入了评估ViT解释的忠实度、敏感度和复杂度的概念。所得结果表明，基于转换器的逐层相关性传播优于局部可解释的模型无关解释和关注可视化，提供了更准确和可靠的ViT所学习的表示。我们的发现提供了洞见。",
    "tldr": "本文研究了多种方法在应用于医学图像分类的Vision Transformer上的性能，并说明逐层相关性传播比局部可解释的模型无关解释和关注可视化提供更准确和可靠的ViT所学习的表示。"
}