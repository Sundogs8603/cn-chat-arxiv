{
    "title": "Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])",
    "abstract": "Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \\emph{Task Oriented MOO} to address this issue, in the contex",
    "link": "http://arxiv.org/abs/2304.13229",
    "context": "Title: Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])\nAbstract: Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \\emph{Task Oriented MOO} to address this issue, in the contex",
    "path": "papers/23/04/2304.13229.json",
    "total_tokens": 887,
    "translated_title": "基于任务导向多目标优化的对抗样本生成",
    "translated_abstract": "深度学习模型，即使是最先进的模型，也很容易受到对抗样本的攻击。对抗训练是提高模型稳健性的最有效方法之一。对于对抗训练的成功来说，关键因素是要能够生成满足某些目标/目标的合格且有差异的对抗样本（例如，找到最大化模型损失以同时攻击多个模型的对抗样本）。因此，多目标优化（MOO）是实现对抗性样本生成以同时实现多个目标/目标的自然工具。本文观察到，MOO的朴素应用往往会平等地最大化所有目标/目标，而不关心目标/目标是否已经实现。这导致了在已实现目标/目标的任务上做无用的努力，而在未实现目标/目标的任务上则投入了较少的关注。因此，本文提出了“基于任务导向的MOO”来解决这一问题，在此情况下...",
    "tldr": "本文提出了“基于任务导向的MOO”方法来实现对抗样本生成以同时实现多个目标，避免了朴素MOO最大化所有目标的弊端。",
    "en_tdlr": "The paper proposes a Task Oriented Multi-Objective Optimization method to generate adversarial examples that satisfy multiple objectives simultaneously, avoiding the issue of naively maximizing all objectives in multi-objective optimization."
}