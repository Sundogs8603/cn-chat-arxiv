{
    "title": "Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])",
    "abstract": "We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.",
    "link": "http://arxiv.org/abs/2304.13530",
    "context": "Title: Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])\nAbstract: We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.",
    "path": "papers/23/04/2304.13530.json",
    "total_tokens": 851,
    "translated_title": "从全手写页面中提取键值信息",
    "translated_abstract": "我们提出了一种基于Transformer的方法，用于从数字化的手写文档中提取信息。我们的方法结合了目前由独立模型执行的不同步骤：特征提取、手写识别和命名实体识别。我们将这种综合方法与传统的两阶段方法进行比较，传统方法在命名实体识别之前进行手写识别，并在不同层次上呈现结果：行、段落和页面。我们的实验表明，在应用于整个页面时，基于注意力的模型特别有趣，因为它们不需要任何预先分割步骤。最后，我们展示了它们能够从键值注释中进行学习：即重要单词和相应命名实体的列表。我们将我们的模型与三个公共数据库（IAM、ESPOSALLES和POPP）的最新方法进行比较，并在所有三个数据集上优于以前的表现。",
    "tldr": "这篇论文提出了一种从手写文档中提取信息的方法，该方法基于Transformer，在同一模型中结合了特征提取、手写识别和命名实体识别步骤。该方法不需要预先分割并在三个公共数据库上表现出色。",
    "en_tdlr": "This paper proposes a Transformer-based approach that integrates feature extraction, handwriting recognition and named entity recognition for information extraction from digitized handwritten documents. The attention-based models don't require any prior segmentation step and can learn from key-value annotations. The proposed method outperforms previous performances on three public databases."
}