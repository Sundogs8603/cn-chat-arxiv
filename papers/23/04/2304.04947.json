{
    "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])",
    "abstract": "We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.",
    "link": "http://arxiv.org/abs/2304.04947",
    "context": "Title: Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])\nAbstract: We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.",
    "path": "papers/23/04/2304.04947.json",
    "total_tokens": 880,
    "translated_title": "条件适配器：具有快速推理的参数高效的迁移学习方法",
    "translated_abstract": "我们提出了一种名为条件适配器（CoDA）的参数高效的迁移学习方法，同时提高了推理效率。CoDA不仅适用于标准适配器方法，还可以通过条件计算来实现平衡速度和准确性的新方式。通过在现有的密集预训练模型中增加稀疏激活、少量新参数以及轻量级的训练阶段，CoDA方法提供了一种出乎意料的传递知识的高效方法。我们的实验表明，与最先进的适配器方法相比，CoDA在各种语言、视觉和语音任务中都实现了2倍至8倍的推理加速，而且准确率有轻微或无损失，且参数效率相同。",
    "tldr": "提出一种名为条件适配器（CoDA）的参数高效的迁移学习方法，它可以通过在现有的密集预训练模型中增加稀疏激活、少量新参数以及轻量级的训练阶段来实现平衡速度和准确性的新方式，实验结果表明，这种方法可以在各种任务中实现2倍至8倍的推理加速，且准确率有轻微或无损失，且参数效率相同。",
    "en_tdlr": "The paper proposes a parameter-efficient transfer learning method called Conditional Adapter (CoDA), which introduces sparse activation, a small number of new parameters, and a light-weight training phase to balance speed and accuracy using conditional computation. CoDA achieves 2x to 8x inference speed-up with moderate to no accuracy loss compared to the state-of-the-art Adapter approach across language, vision, and speech tasks."
}