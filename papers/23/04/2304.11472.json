{
    "title": "A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition. (arXiv:2304.11472v1 [eess.AS])",
    "abstract": "Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeec",
    "link": "http://arxiv.org/abs/2304.11472",
    "context": "Title: A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition. (arXiv:2304.11472v1 [eess.AS])\nAbstract: Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeec",
    "path": "papers/23/04/2304.11472.json",
    "total_tokens": 933,
    "translated_title": "预训练语音和音频嵌入与情感识别的比较研究",
    "translated_abstract": "预训练模型（PTMs）在语音和音频领域中表现出巨大的应用潜力。从这些模型中提取出的嵌入可以作为输入，用于学习算法，可以应用于各种下游任务。其中一个关键任务是情感识别，它具有广泛的应用，包括对顾客呼叫的动态分析、心理健康评估和个性化语言学习等。PTM嵌入有助于推动情感识别的发展，但缺乏一个考虑多个方面的综合比较，例如嵌入模型架构、用于预训练的数据以及预训练过程等。PTM嵌入的彻底比较将有助于更快，更高效地开发模型，并使它们能够在实际场景中得到应用。本文利用这一研究空白，对来自八个语音和音频PTMs提取的嵌入进行了比较分析（包括wav2vec 2.0，data2vec，wavLM，UniSpeec）",
    "tldr": "本文对来自八个语音和音频PTMs提取的嵌入进行了比较分析，旨在提高情感识别模型的发展速度和效率，并使其能够在实际环境中得到应用。",
    "en_tdlr": "This paper performs a comparative analysis of embeddings extracted from eight speech and audio PTMs, aiming to improve the development speed and efficiency of SER models and enable their deployment in real-world scenarios."
}