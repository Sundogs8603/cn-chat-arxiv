{
    "title": "Exact Subspace Diffusion for Decentralized Multitask Learning. (arXiv:2304.07358v1 [cs.LG])",
    "abstract": "Classical paradigms for distributed learning, such as federated or decentralized gradient descent, employ consensus mechanisms to enforce homogeneity among agents. While these strategies have proven effective in i.i.d. scenarios, they can result in significant performance degradation when agents follow heterogeneous objectives or data. Distributed strategies for multitask learning, on the other hand, induce relationships between agents in a more nuanced manner, and encourage collaboration without enforcing consensus. We develop a generalization of the exact diffusion algorithm for subspace constrained multitask learning over networks, and derive an accurate expression for its mean-squared deviation when utilizing noisy gradient approximations. We verify numerically the accuracy of the predicted performance expressions, as well as the improved performance of the proposed approach over alternatives based on approximate projections.",
    "link": "http://arxiv.org/abs/2304.07358",
    "context": "Title: Exact Subspace Diffusion for Decentralized Multitask Learning. (arXiv:2304.07358v1 [cs.LG])\nAbstract: Classical paradigms for distributed learning, such as federated or decentralized gradient descent, employ consensus mechanisms to enforce homogeneity among agents. While these strategies have proven effective in i.i.d. scenarios, they can result in significant performance degradation when agents follow heterogeneous objectives or data. Distributed strategies for multitask learning, on the other hand, induce relationships between agents in a more nuanced manner, and encourage collaboration without enforcing consensus. We develop a generalization of the exact diffusion algorithm for subspace constrained multitask learning over networks, and derive an accurate expression for its mean-squared deviation when utilizing noisy gradient approximations. We verify numerically the accuracy of the predicted performance expressions, as well as the improved performance of the proposed approach over alternatives based on approximate projections.",
    "path": "papers/23/04/2304.07358.json",
    "total_tokens": 838,
    "translated_title": "分布化多任务学习中的精确子空间扩散",
    "translated_abstract": "传统的分布式学习方法，如联邦学习或分散式梯度下降，采用共识机制来强制实现代理之间的同质性。虽然这些策略在独立同分布场景下被证明是有效的，但在代理遵循异构目标或数据时，它们可能导致严重的性能降低。另一方面，多任务学习的分布式策略以更加微妙的方式在代理之间建立关系，并鼓励协作而不是强制共识。我们发展了用于通过网络进行子空间约束的多任务学习的精确扩散算法的推广，并导出了在利用噪声梯度逼近时其均方偏差的准确表达式。我们在数值上验证了预测性能表达式的准确性，以及所提出的方法相对于基于近似投影的其他方法的性能改进。",
    "tldr": "本论文提出了一种新的分布式多任务学习算法，通过精确扩散算法的推广，并在网络中进行子空间约束。相比于现有的基于近似投影的方法，其性能得到了明显提升。",
    "en_tdlr": "This paper proposes a new method for distributed multitask learning, which utilizes an exact diffusion algorithm to constrain subspaces over networks. The proposed method outperforms existing methods based on approximate projections."
}