{
    "title": "Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning. (arXiv:2304.05600v1 [cs.SD])",
    "abstract": "Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech re",
    "link": "http://arxiv.org/abs/2304.05600",
    "context": "Title: Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning. (arXiv:2304.05600v1 [cs.SD])\nAbstract: Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech re",
    "path": "papers/23/04/2304.05600.json",
    "total_tokens": 834,
    "translated_title": "相貌相似，声音不同：利用反事实跨模态对学习音视频表示",
    "translated_abstract": "音视频表示学习通常依赖于视听之间的对应关系。然而，在一个视觉场景中可能存在多个声音轨道与之对应。例如，在同一拥挤的街道上有不同的交谈声。这些反事实对于音视频表示学习的影响尚未研究。为了研究这个问题，我们使用电影的配音版本来增加跨模态对比学习的方法。我们的方法学习表示类似于相同视频的仅在语音内容上不同的替代音轨。我们的实验结果表明，增加配音的训练在一系列听觉和视听任务中提高了性能，而对语言任务的整体表现影响不大。我们还将这种方法与在预训练之前去除语音的强大基线进行了比较，发现增加配音的训练更有效，包括语音外语和视听任务，其中语音恢复任务性能提高了。",
    "tldr": "该论文通过增加配音来增强跨模态对比学习，结果表明这种方法可以提高音频和视听任务的性能。",
    "en_tdlr": "This paper enhances cross-modal contrastive learning by adding dubbed versions of movies to represent alternate audio tracks, and the results show improved performance on both auditory and audiovisual tasks."
}