{
    "title": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])",
    "abstract": "Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that ",
    "link": "http://arxiv.org/abs/2304.06813",
    "context": "Title: Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])\nAbstract: Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that ",
    "path": "papers/23/04/2304.06813.json",
    "total_tokens": 904,
    "translated_title": "模型特定视角下的统一外部分布检测",
    "translated_abstract": "外部分布检测旨在识别不属于训练分布并不可靠预测的测试样例。虽然已有大量相关工作，但其中大多数只关注来自语义转换（如未见过的类别）的OOD例子，而忽略了其他可能的原因（如协变量转换）。本文提出了一种新颖的统一框架，以更广泛的范围研究OOD检测。我们建议不是检测特定原因导致的OOD例子，而是检测已部署机器学习模型（例如图像分类器）无法正确预测的例子。也就是说，是否应该检测和拒绝测试例子是“模型特定”的。我们展示了该框架统一了由语义变化和协变量变化引起的OOD例子的检测，并密切关注将机器学习模型应用于不受控制的环境的问题。我们对该框架进行了广泛的分析和实验。",
    "tldr": "本文提出一种新颖的统一框架，用于将机器学习模型中的外部分布检测扩展到更广泛的范围，该框架旨在检测模型无法正确预测的测试示例，而不是特定的外部分布原因。"
}