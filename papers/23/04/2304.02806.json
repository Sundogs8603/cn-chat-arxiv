{
    "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \\textit{information a",
    "link": "http://arxiv.org/abs/2304.02806",
    "context": "Title: Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \\textit{information a",
    "path": "papers/23/04/2304.02806.json",
    "total_tokens": 857,
    "translated_title": "图混合专家：显式多样性建模下的大规模图学习",
    "translated_abstract": "图神经网络已被广泛应用于图数据的学习。然而，现实世界中的图通常具有多样的图结构，并且包含异构节点和边。为了增强GNN的泛化能力，进一步提高训练图结构的多样性已成为常见做法。但是，简单地增加GNN模型容量将会导致更高的推理成本和GNN难以训练的问题。本文将专家混合（MoE）的思想引入到GNN中，旨在增强其适应多样的训练图结构的能力，而不会增加计算开销。我们的新图混合专家（GMoE）模型使得图中的每个节点可以动态地选择其自己的最佳信息。",
    "tldr": "本文提出了一种新的图混合专家（GMoE）模型，旨在解决现实世界中的图具有多样的图结构和包含异构节点和边的问题。该模型可以增强GNN的泛化能力，适应多样的训练图结构的能力，并且不会增加计算开销。",
    "en_tdlr": "This paper introduces a new Graph Mixture of Expert (GMoE) model to enhance the generalization ability of GNNs and accommodate the diversity of training graph structures without incurring computational overheads."
}