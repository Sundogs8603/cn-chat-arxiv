{
    "title": "Lessons Learned from a Citizen Science Project for Natural Language Processing. (arXiv:2304.12836v1 [cs.CL])",
    "abstract": "Many Natural Language Processing (NLP) systems use annotated corpora for training and evaluation. However, labeled data is often costly to obtain and scaling annotation projects is difficult, which is why annotation tasks are often outsourced to paid crowdworkers. Citizen Science is an alternative to crowdsourcing that is relatively unexplored in the context of NLP. To investigate whether and how well Citizen Science can be applied in this setting, we conduct an exploratory study into engaging different groups of volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing crowdsourced dataset. Our results show that this can yield high-quality annotations and attract motivated volunteers, but also requires considering factors such as scalability, participation over time, and legal and ethical issues. We summarize lessons learned in the form of guidelines and provide our code and data to aid future work on Citizen Science.",
    "link": "http://arxiv.org/abs/2304.12836",
    "context": "Title: Lessons Learned from a Citizen Science Project for Natural Language Processing. (arXiv:2304.12836v1 [cs.CL])\nAbstract: Many Natural Language Processing (NLP) systems use annotated corpora for training and evaluation. However, labeled data is often costly to obtain and scaling annotation projects is difficult, which is why annotation tasks are often outsourced to paid crowdworkers. Citizen Science is an alternative to crowdsourcing that is relatively unexplored in the context of NLP. To investigate whether and how well Citizen Science can be applied in this setting, we conduct an exploratory study into engaging different groups of volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing crowdsourced dataset. Our results show that this can yield high-quality annotations and attract motivated volunteers, but also requires considering factors such as scalability, participation over time, and legal and ethical issues. We summarize lessons learned in the form of guidelines and provide our code and data to aid future work on Citizen Science.",
    "path": "papers/23/04/2304.12836.json",
    "total_tokens": 852,
    "translated_title": "一项面向自然语言处理的公民科学项目的经验教训",
    "translated_abstract": "许多自然语言处理系统使用带注释的语料库进行训练和评估。然而，标记数据通常很难获得，并且扩展注释项目也很困难，因此注释任务常常被外包给有偿的众包工人。公民科学是一个相对未被开发的众包替代方案。为了调查公民科学是否以及如何适用于此领域，我们进行了一项探索性研究，通过重新注释现有众包数据集的部分内容，与不同志愿者群体参与公民科学。结果显示，这可以产生高质量的注释并吸引积极的志愿者，但也需要考虑可扩展性、长期参与和法律和伦理问题等因素。我们总结了指南，并提供了我们的代码和数据，以帮助未来进行公民科学工作。",
    "tldr": "本论文探讨了公民科学在自然语言处理领域的应用，研究表明这可以产生高质量的注释并吸引积极的志愿者，但需要考虑可扩展性、长期参与和法律和伦理问题等因素。",
    "en_tdlr": "This paper discusses the application of Citizen Science in natural language processing and found that it can yield high-quality annotations and attract motivated volunteers but requires considering factors such as scalability, participation over time, and legal and ethical issues."
}