{
    "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])",
    "abstract": "Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri",
    "link": "http://arxiv.org/abs/2304.13649",
    "context": "Title: A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])\nAbstract: Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri",
    "path": "papers/23/04/2304.13649.json",
    "total_tokens": 1123,
    "translated_title": "一种用于知识密集型视觉问答的对称双编码稠密检索框架",
    "translated_abstract": "知识密集型视觉问答（KI-VQA）是指回答关于图像的问题，其答案不在图像中。本文提出了一种新的KI-VQA任务流程，包括一个检索器和一个阅读器。首先，我们介绍了DEDR，它是一种对称双编码密集检索框架，其中使用单模（文本）和多模编码器将文档和查询编码为共享嵌入空间。我们引入了一种迭代知识蒸馏方法，以弥合这两个编码器中的表示空间之间的差距。对两个成熟的KI-VQA数据集OK-VQA和FVQA进行广泛的评估表明，DEDR在OK-VQA和FVQA上的性能比最先进的基线方法分别提高了11.6％和30.9％。利用DEDR检索到的段落，我们还进一步介绍了MM-FiD，一种编码器-解码器多模式融合解码器模型，用于为KI-VQA任务生成文本答案。MM-FiD将问题、图像和每个检索到的段落编码为单独的向量，并通过从它们的连接解码生成答案。广泛的实验表明，MM-FiD在OK-VQA和FVQA数据集上均优于最先进的方法。",
    "tldr": "本文提出了一种新的对称双编码稠密检索框架DEDR，以弥合文本和图像之间的空间差距，并进一步引入了MM-FiD，一种多模式融合解码器模型，用于知识密集型视觉问答任务，其效果优于现有最先进方法。"
}