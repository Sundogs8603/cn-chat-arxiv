{
    "title": "Distance Weighted Supervised Learning for Offline Interaction Data. (arXiv:2304.13774v1 [cs.LG])",
    "abstract": "Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised learning are robust, but require optimal demonstrations, which are hard to collect. Offline goal-conditioned reinforcement learning (RL) algorithms promise to learn from sub-optimal data, but face optimization challenges especially with high-dimensional data. To bridge the gap between IL and RL, we introduce Distance Weighted Supervised Learning or DWSL, a supervised method for learning goal-conditioned policies from offline data. DWSL models the entire distribution of time-steps between states in offline data with only supervised learning, and uses this distribution to approximate shortest path distances. To extract a policy, we weight actions by their reduction in distance estimates. Theoretically, DWSL converges to an optimal policy constrained to the data distribution, an attractive property for offline lear",
    "link": "http://arxiv.org/abs/2304.13774",
    "context": "Title: Distance Weighted Supervised Learning for Offline Interaction Data. (arXiv:2304.13774v1 [cs.LG])\nAbstract: Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised learning are robust, but require optimal demonstrations, which are hard to collect. Offline goal-conditioned reinforcement learning (RL) algorithms promise to learn from sub-optimal data, but face optimization challenges especially with high-dimensional data. To bridge the gap between IL and RL, we introduce Distance Weighted Supervised Learning or DWSL, a supervised method for learning goal-conditioned policies from offline data. DWSL models the entire distribution of time-steps between states in offline data with only supervised learning, and uses this distribution to approximate shortest path distances. To extract a policy, we weight actions by their reduction in distance estimates. Theoretically, DWSL converges to an optimal policy constrained to the data distribution, an attractive property for offline lear",
    "path": "papers/23/04/2304.13774.json",
    "total_tokens": 989,
    "translated_title": "离线交互数据的距离加权监督学习",
    "translated_abstract": "序列决策算法通常很难利用不同来源的非结构化离线交互数据。基于监督学习的模仿学习方法是稳健的，但需要收集最佳演示，而这很难。离线目标条件强化学习算法承诺从次优数据中学习，但面对高维数据时面临优化挑战。为弥合模仿学习和强化学习之间的差距，我们引入了距离加权监督学习或DWSL，一种从离线数据中学习目标条件策略的监督方法。DWSL只用监督学习模拟离线数据状态之间的所有时间步的分布，并利用这个分布来近似最短路径距离。为了提取策略，我们通过它们在距离估计中的减少程度来加权行动。理论上，DWSL收敛于受数据分布约束的最优策略，这对于离线学习的代理人是一个有吸引力的属性。实践上，DWSL显著优于以前的监督方法，并在两个标准基准上与离线RL方法相比取得了竞争性的结果。",
    "tldr": "这篇论文提出一种距离加权监督学习方法，可以利用离线交互数据中的最短路径距离来提取策略，较之以往的监督方法和离线强化学习方法表现更好。",
    "en_tdlr": "This paper proposes a distance weighted supervised learning method that utilizes the shortest path distance in offline interaction data to extract policies, outperforming previous supervised methods and achieving competitive results with offline reinforcement learning methods on two standard benchmarks."
}