{
    "title": "Attention Mixtures for Time-Aware Sequential Recommendation. (arXiv:2304.08158v2 [cs.IR] UPDATED)",
    "abstract": "Transformers emerged as powerful methods for sequential recommendation. However, existing architectures often overlook the complex dependencies between user preferences and the temporal context. In this short paper, we introduce MOJITO, an improved Transformer sequential recommender system that addresses this limitation. MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling. Such an approach permits to accurately predict which items should be recommended next to users depending on past actions and the temporal context. We demonstrate the relevance of our approach, by empirically outperforming existing Transformers for sequential recommendation on several real-world datasets.",
    "link": "http://arxiv.org/abs/2304.08158",
    "context": "Title: Attention Mixtures for Time-Aware Sequential Recommendation. (arXiv:2304.08158v2 [cs.IR] UPDATED)\nAbstract: Transformers emerged as powerful methods for sequential recommendation. However, existing architectures often overlook the complex dependencies between user preferences and the temporal context. In this short paper, we introduce MOJITO, an improved Transformer sequential recommender system that addresses this limitation. MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling. Such an approach permits to accurately predict which items should be recommended next to users depending on past actions and the temporal context. We demonstrate the relevance of our approach, by empirically outperforming existing Transformers for sequential recommendation on several real-world datasets.",
    "path": "papers/23/04/2304.08158.json",
    "total_tokens": 752,
    "translated_title": "时间感知顺序推荐中的注意力混合",
    "translated_abstract": "Transformer模型在顺序推荐中表现出强大的能力。然而，现有的架构经常忽视用户偏好和时间背景之间的复杂依赖关系。在本篇短文中，我们介绍了MOJITO，一种改进的Transformer顺序推荐系统，它解决了这个局限性。MOJITO利用基于注意力的时间背景和物品嵌入表示的高斯混合进行顺序建模。这种方法可以准确地预测下一个应该向用户推荐哪些物品，这取决于过去的行为和时间背景。我们通过在多个真实世界数据集上进行实证实验，证明了我们方法的相关性，优于现有的Transformer顺序推荐模型。",
    "tldr": "MOJITO是一种改进的Transformer顺序推荐系统，利用注意力混合建模用户偏好和时间背景的复杂依赖关系，从而准确预测下一个推荐物品。在多个真实数据集中，MOJITO表现优于现有的Transformer模型。",
    "en_tdlr": "MOJITO is an improved Transformer sequential recommender system that accurately predicts the next recommended item by leveraging attention mixtures to model the complex dependencies between user preferences and temporal context. It outperforms existing Transformers on multiple real-world datasets."
}