{
    "title": "Disagreement amongst counterfactual explanations: How transparency can be deceptive. (arXiv:2304.12667v1 [cs.AI])",
    "abstract": "Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement probl",
    "link": "http://arxiv.org/abs/2304.12667",
    "context": "Title: Disagreement amongst counterfactual explanations: How transparency can be deceptive. (arXiv:2304.12667v1 [cs.AI])\nAbstract: Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement probl",
    "path": "papers/23/04/2304.12667.json",
    "total_tokens": 1049,
    "translated_title": "论反事实解释的分歧：透明可能具有误导性。",
    "translated_abstract": "反事实解释作为可解释人工智能(XAI)技术之一，越来越多地被用于为数据驱动决策的利益相关者提供解释。尽管目前存在许多生成反事实解释的算法，但并非每个算法都能为同一实例创建一致的解释。在某些情况下，多个可能的解释是有益的，但在一些场景下，反事实解释之间的差异会导致利益相关者之间的潜在分歧问题。当恶意主体利用这种差异来掩盖敏感特征，公正掩饰不公平的机器学习模型时，就会出现道德问题。随着全球立法者开始将解释数据驱动高风险决策的权利纳入其政策中，应该了解和解决这些伦理问题。我们对反事实解释之间的分歧问题进行的文献综述表明，解释可能是具有误导性的：多样化的反事实解释可能会掩盖或淡化原始模型中的敏感和不公平特征。本文提出了更好地理解和管理反事实解释之间差异的论点，以防止欺骗，并确保XAI的道德使用。",
    "tldr": "反事实解释的多样性会对利益相关者之间产生潜在分歧问题，并可能掩盖或淡化原始模型中的敏感和不公平特征，因此需要更好地理解和管理这种差异，以防止欺骗，并确保XAI的道德使用。",
    "en_tdlr": "The diversity among counterfactual explanations can potentially lead to disagreement among stakeholders and may hide or downplay sensitive and unfair features in the original model, making it deceptive. To prevent deception and ensure the ethical use of XAI, there is a need for a better understanding and management of the differences among counterfactual explanations."
}