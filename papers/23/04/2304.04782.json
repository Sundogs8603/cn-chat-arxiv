{
    "title": "Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])",
    "abstract": "Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m",
    "link": "http://arxiv.org/abs/2304.04782",
    "context": "Title: Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])\nAbstract: Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m",
    "path": "papers/23/04/2304.04782.json",
    "total_tokens": 769,
    "translated_title": "通过潜在意图从被动数据中进行强化学习",
    "translated_abstract": "被动观察数据丰富而富有信息，然而当前强化学习方法很少能够利用该数据。本文提出了一种通过建模意图从被动数据中进行学习的方法，该方法通过衡量当智能体为实现特定任务而采取行动时未来结果的可能性如何变化来学习意图。我们提出了一个时差学习目标来学习意图，得到了一个类似于传统强化学习的算法，但是完全是从被动数据中学习得到的。通过优化该目标，我们的智能体可以同时从原始的观察数据中学习出状态、策略和环境下的可能结果。从理论和实验上看，该方法学习出的特征可用于下游任务的价值预测。",
    "tldr": "本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。",
    "en_tdlr": "This paper proposes a reinforcement learning method based on intention modeling, which can learn features from passive data for downstream value prediction tasks."
}