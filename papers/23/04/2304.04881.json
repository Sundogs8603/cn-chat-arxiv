{
    "title": "DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])",
    "abstract": "Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.",
    "link": "http://arxiv.org/abs/2304.04881",
    "context": "Title: DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])\nAbstract: Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.",
    "path": "papers/23/04/2304.04881.json",
    "total_tokens": 926,
    "translated_title": "使用基于负采样的方法评估多项选择题中的干扰选项",
    "translated_abstract": "多项选择题是一种评估阅读理解能力的高效常见方法。每道多项选择题需要一组干扰选项，这些选项虽然不正确，但要足够合理以考查学生的知识掌握情况。已经提出了干扰选项生成模型，并且它们的性能通常使用机器翻译度量标准来评估。然而，机器翻译度量标准经常误判生成的干扰选项的合适性。我们提出了DISTO：用于评估生成干扰选项的第一个学习度量标准。我们通过展示DISTO得分与人类对干扰选项质量的评分高度相关来验证DISTO。与此同时，DISTO排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不应用于干扰选项评估。",
    "tldr": "DISTO提出了一种新的学习度量标准来评估多项选择题中生成的干扰选项。DISTO与人类对干扰选项的评分高度相关，且排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不适用于干扰选项评估。",
    "en_tdlr": "DISTO proposes a novel learned evaluation metric to evaluate the generated distractors in multiple choice questions. DISTO is highly correlated with human ratings of distractor quality and ranks the performance of state-of-the-art distractor generation models differently from machine translation-based metrics, suggesting that MT metrics should not be used for distractor evaluation."
}