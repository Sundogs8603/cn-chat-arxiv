{
    "title": "Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models. (arXiv:2304.04344v1 [cs.CV])",
    "abstract": "Recent advances in diffusion models enable many powerful instruments for image editing. One of these instruments is text-driven image manipulations: editing semantic attributes of an image according to the provided text description. % Popular text-conditional diffusion models offer various high-quality image manipulation methods for a broad range of text prompts. Existing diffusion-based methods already achieve high-quality image manipulations for a broad range of text prompts. However, in practice, these methods require high computation costs even with a high-end GPU. This greatly limits potential real-world applications of diffusion-based image editing, especially when running on user devices.  In this paper, we address efficiency of the recent text-driven editing methods based on unconditional diffusion models and develop a novel algorithm that learns image manipulations 4.5-10 times faster and applies them 8 times faster. We carefully evaluate the visual quality and expressiveness ",
    "link": "http://arxiv.org/abs/2304.04344",
    "context": "Title: Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models. (arXiv:2304.04344v1 [cs.CV])\nAbstract: Recent advances in diffusion models enable many powerful instruments for image editing. One of these instruments is text-driven image manipulations: editing semantic attributes of an image according to the provided text description. % Popular text-conditional diffusion models offer various high-quality image manipulation methods for a broad range of text prompts. Existing diffusion-based methods already achieve high-quality image manipulations for a broad range of text prompts. However, in practice, these methods require high computation costs even with a high-end GPU. This greatly limits potential real-world applications of diffusion-based image editing, especially when running on user devices.  In this paper, we address efficiency of the recent text-driven editing methods based on unconditional diffusion models and develop a novel algorithm that learns image manipulations 4.5-10 times faster and applies them 8 times faster. We carefully evaluate the visual quality and expressiveness ",
    "path": "papers/23/04/2304.04344.json",
    "total_tokens": 880,
    "translated_title": "基于无条件扩散模型的实时文本驱动图像处理",
    "translated_abstract": "最近扩散模型的进展为图像编辑提供了许多强大的工具。其中之一是文本驱动的图像操作：根据提供的文本描述编辑图像的语义属性。现有的扩散模型已经为广泛的文本提示提供了高质量的图像操作方法。然而，在实践中，即使使用高端GPU，这些方法也需要高计算成本。这极大地限制了扩散模型图像编辑在潜在的实际应用中的应用，特别是在用户设备上运行时。在本文中，我们解决了基于无条件扩散模型的最近的文本驱动编辑方法的效率问题，并开发了一种新算法，该算法可以比旧算法学习图像操作4.5-10倍的更快，并且应用更快速的8倍。我们仔细评估了视觉质量和表现力。",
    "tldr": "本研究提出了一种基于无条件扩散模型的图像编辑算法，可根据提供的文本描述快速学习和应用高质量的图像操作，比旧算法学习图像操作更快4.5-10倍，应用更快速的8倍。",
    "en_tdlr": "This paper proposes an image editing algorithm based on unconditional diffusion models, which can quickly learn and apply high-quality image operations according to the provided text description. It achieves 4.5-10 times faster learning and 8 times faster application of image manipulations compared to the old algorithm, and carefully evaluates the visual quality and expressiveness."
}