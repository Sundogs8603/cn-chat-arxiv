{
    "title": "Generative Relevance Feedback with Large Language Models. (arXiv:2304.13157v1 [cs.IR])",
    "abstract": "Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve the best R@1k effectiveness on all datasets compared to state-of-the-art sparse, dense, and expansion models.",
    "link": "http://arxiv.org/abs/2304.13157",
    "context": "Title: Generative Relevance Feedback with Large Language Models. (arXiv:2304.13157v1 [cs.IR])\nAbstract: Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve the best R@1k effectiveness on all datasets compared to state-of-the-art sparse, dense, and expansion models.",
    "path": "papers/23/04/2304.13157.json",
    "total_tokens": 901,
    "translated_title": "使用大型语言模型的生成式相关反馈",
    "translated_abstract": "当前的查询扩展模型使用伪相关反馈来提高第一遍检索的有效性, 但是当初始结果不相关时则会失败。我们提出了生成式相关反馈（GRF），该模型从大型语言模型生成的长形文本中构建概率反馈模型。我们通过改变零样本生成子任务-查询，实体，事实，新闻文章，文档和文章-来研究生成文本的有效方法。我们在涵盖各种查询和文档集合的文档检索基准测试中评估了GRF，并且结果表明，GRF方法比先前的PRF方法显著提高了效果。特别地，相比RM3扩展，我们提高了5-19%的MAP和17-24%的NDCG@10，并在所有数据集上实现了最佳R @ 1k效果，相比于现有的稀疏、密集和扩展模型。",
    "tldr": "本文提出了一种基于大型语言模型的生成式相关反馈方法（GRF），不同于以往的伪相关反馈方法，它从长形文本中构建概率反馈模型，综合实验结果显示，在各种文献检索基准测试中，GRF方法具有显着的优势。",
    "en_tdlr": "This paper proposes a novel Generative Relevance Feedback (GRF) approach based on large language models, which constructs a probabilistic feedback model from long-form text. Unlike previous pseudo-relevance feedback methods, it improves the initial results by generating text instead of building a language model from retrieved results. Experimental results show that GRF significantly outperforms previous PRF methods in various document retrieval benchmarks."
}