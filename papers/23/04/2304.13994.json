{
    "title": "SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])",
    "abstract": "We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.",
    "link": "http://arxiv.org/abs/2304.13994",
    "context": "Title: SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])\nAbstract: We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.",
    "path": "papers/23/04/2304.13994.json",
    "total_tokens": 933,
    "translated_title": "SweCTRL-Mini：一种基于Transformer的数据透明的大型语言模型，用于控制性文本生成的瑞典语言版",
    "translated_abstract": "我们介绍了SweCTRL-Mini，它是一个大规模的瑞典语言模型，可用于单个消费级GPU上的推理和fine-tuning。该模型基于由Keskar、McCann、Varshney、Xiong和Socher（2019）开发的CTRL体系结构，这意味着SweCTRL-Mini模型的用户可以通过在生成提示中插入特殊标记来控制生成文本的流派。SweCTRL-Mini在瑞典部分mC4语料库和一组瑞典小说的子集上进行了训练，我们在本文中提供了(1)所使用的训练数据和文本预处理步骤的详细说明，以使可以检查特定短语/来源是否是训练数据的一部分;(2)使用自动评估方法进行辨别性任务的模型评估，使用人工裁判进行生成性任务的评估;我们还将模型的生成能力与GPT-3的生成能力进行了比较。SweCTRL-Mini 是完全开放的，可供下载。",
    "tldr": "SweCTRL-Mini是一种基于Transformer的大型瑞典语言模型，用户可以控制它生成的文本流派，完全开放下载。生成能力比较GPT-3。",
    "en_tdlr": "SweCTRL-Mini is a large Swedish language model based on Transformer, and users can control the genre of the generated text. It is fully open for download and its generative capability is compared with that of GPT-3."
}