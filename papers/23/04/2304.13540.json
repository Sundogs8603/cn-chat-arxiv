{
    "title": "Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search. (arXiv:2304.13540v1 [cs.DC])",
    "abstract": "Modern machine learning (ML) models are capable of impressive performances. However, their prowess is not due only to the improvements in their architecture and training algorithms but also to a drastic increase in computational power used to train them.  Such a drastic increase led to a growing interest in distributed ML, which in turn made worker failures and adversarial attacks an increasingly pressing concern. While distributed byzantine resilient algorithms have been proposed in a differentiable setting, none exist in a gradient-free setting.  The goal of this work is to address this shortcoming. For that, we introduce a more general definition of byzantine-resilience in ML - the \\textit{model-consensus}, that extends the definition of the classical distributed consensus. We then leverage this definition to show that a general class of gradient-free ML algorithms - ($1,\\lambda$)-Evolutionary Search - can be combined with classical distributed consensus algorithms to generate gradi",
    "link": "http://arxiv.org/abs/2304.13540",
    "context": "Title: Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search. (arXiv:2304.13540v1 [cs.DC])\nAbstract: Modern machine learning (ML) models are capable of impressive performances. However, their prowess is not due only to the improvements in their architecture and training algorithms but also to a drastic increase in computational power used to train them.  Such a drastic increase led to a growing interest in distributed ML, which in turn made worker failures and adversarial attacks an increasingly pressing concern. While distributed byzantine resilient algorithms have been proposed in a differentiable setting, none exist in a gradient-free setting.  The goal of this work is to address this shortcoming. For that, we introduce a more general definition of byzantine-resilience in ML - the \\textit{model-consensus}, that extends the definition of the classical distributed consensus. We then leverage this definition to show that a general class of gradient-free ML algorithms - ($1,\\lambda$)-Evolutionary Search - can be combined with classical distributed consensus algorithms to generate gradi",
    "path": "papers/23/04/2304.13540.json",
    "total_tokens": 950,
    "translated_title": "基于演化搜索的拜占庭容错学习：超越梯度下降",
    "translated_abstract": "现代机器学习（ML）模型表现出色，其能力的提升不仅因于其架构和训练算法的改进，还在于用于训练它们的计算能力的巨大提升。这种巨大提升导致对分布式ML的兴趣日益增长，而这反过来又使工人故障和对抗性攻击变得越来越紧迫。虽然在可微设置中提出了分布式拜占庭容错算法，但在无梯度设置中不存在。本文的目标就是解决这个缺陷。为此，我们引入了一个更一般的拜占庭容错ML定义——模型共识，该定义扩展了经典分布式共识的定义。然后，我们利用这个定义来展示一般类的无梯度ML算法——（1，λ）-演化搜索可以与经典分布式共识算法相结合，生成无梯度拜占庭容错学习算法。",
    "tldr": "本文介绍了一种新的拜占庭容错ML定义——模型共识，并展示了如何利用一类无梯度ML算法和经典分布式共识算法生成无梯度拜占庭容错学习算法。",
    "en_tdlr": "This paper proposes a new definition of byzantine-resilience in machine learning, called \"model-consensus\", and shows how a specific class of gradient-free ML algorithms, called ($1,\\lambda$)-Evolutionary Search, can be combined with classical distributed consensus algorithms to generate gradient-free byzantine-resilient learning algorithms."
}