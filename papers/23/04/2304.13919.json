{
    "title": "Detection of Adversarial Physical Attacks in Time-Series Image Data. (arXiv:2304.13919v1 [cs.CV])",
    "abstract": "Deep neural networks (DNN) have become a common sensing modality in autonomous systems as they allow for semantically perceiving the ambient environment given input images. Nevertheless, DNN models have proven to be vulnerable to adversarial digital and physical attacks. To mitigate this issue, several detection frameworks have been proposed to detect whether a single input image has been manipulated by adversarial digital noise or not. In our prior work, we proposed a real-time detector, called VisionGuard (VG), for adversarial physical attacks against single input images to DNN models. Building upon that work, we propose VisionGuard* (VG), which couples VG with majority-vote methods, to detect adversarial physical attacks in time-series image data, e.g., videos. This is motivated by autonomous systems applications where images are collected over time using onboard sensors for decision-making purposes. We emphasize that majority-vote mechanisms are quite common in autonomous system ap",
    "link": "http://arxiv.org/abs/2304.13919",
    "context": "Title: Detection of Adversarial Physical Attacks in Time-Series Image Data. (arXiv:2304.13919v1 [cs.CV])\nAbstract: Deep neural networks (DNN) have become a common sensing modality in autonomous systems as they allow for semantically perceiving the ambient environment given input images. Nevertheless, DNN models have proven to be vulnerable to adversarial digital and physical attacks. To mitigate this issue, several detection frameworks have been proposed to detect whether a single input image has been manipulated by adversarial digital noise or not. In our prior work, we proposed a real-time detector, called VisionGuard (VG), for adversarial physical attacks against single input images to DNN models. Building upon that work, we propose VisionGuard* (VG), which couples VG with majority-vote methods, to detect adversarial physical attacks in time-series image data, e.g., videos. This is motivated by autonomous systems applications where images are collected over time using onboard sensors for decision-making purposes. We emphasize that majority-vote mechanisms are quite common in autonomous system ap",
    "path": "papers/23/04/2304.13919.json",
    "total_tokens": 892,
    "translated_title": "识别时间序列图像数据中的对抗性物理攻击",
    "translated_abstract": "深度神经网络已成为自主系统中常见的感知模态，因为它们能够在给定输入图像的情况下对环境进行语义感知。然而，DNN模型已被证明易受到对抗性数字和物理攻击的影响。为了解决这个问题，已经提出了几个检测框架来检测单个输入图像是否受到对抗性数字噪声的操纵。在我们之前的工作中，我们提出了一种实时检测器，称为VisionGuard（VG），用于检测针对单个输入图像的DNN模型的对抗性物理攻击。在此基础上，我们提出了VisionGuard *（VG），它将VG与多数投票方法相结合，以检测时间序列图像数据中的对抗性物理攻击，例如视频。这受到自主系统应用的启发，其中使用机载传感器随时间收集图像以进行决策。我们强调，多数投票机制在自主系统应用中非常普遍。",
    "tldr": "本论文提出了一种针对时间序列图像数据的对抗性物理攻击检测方法，使用VisionGuard和多数投票结合的方法，以应对自主系统应用中普遍存在的对抗性攻击问题。",
    "en_tdlr": "This paper proposes a method for detecting adversarial physical attacks in time-series image data using the combination of VisionGuard and majority-vote methods, as a response to the common existence of adversarial attacks in autonomous systems applications."
}