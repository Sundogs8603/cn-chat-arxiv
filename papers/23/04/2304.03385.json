{
    "title": "Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training. (arXiv:2304.03385v1 [cs.LG])",
    "abstract": "Recent developments in applications of artificial neural networks with over $n=10^{14}$ parameters make it extremely important to study the large $n$ behaviour of such networks. Most works studying wide neural networks have focused on the infinite width $n \\to +\\infty$ limit of such networks and have shown that, at initialization, they correspond to Gaussian processes. In this work we will study their behavior for large, but finite $n$. Our main contributions are the following:  (1) The computation of the corrections to Gaussianity in terms of an asymptotic series in $n^{-\\frac{1}{2}}$. The coefficients in this expansion are determined by the statistics of parameter initialization and by the activation function.  (2) Controlling the evolution of the outputs of finite width $n$ networks, during training, by computing deviations from the limiting infinite width case (in which the network evolves through a linear flow). This improves previous estimates and yields sharper decay rates for t",
    "link": "http://arxiv.org/abs/2304.03385",
    "context": "Title: Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training. (arXiv:2304.03385v1 [cs.LG])\nAbstract: Recent developments in applications of artificial neural networks with over $n=10^{14}$ parameters make it extremely important to study the large $n$ behaviour of such networks. Most works studying wide neural networks have focused on the infinite width $n \\to +\\infty$ limit of such networks and have shown that, at initialization, they correspond to Gaussian processes. In this work we will study their behavior for large, but finite $n$. Our main contributions are the following:  (1) The computation of the corrections to Gaussianity in terms of an asymptotic series in $n^{-\\frac{1}{2}}$. The coefficients in this expansion are determined by the statistics of parameter initialization and by the activation function.  (2) Controlling the evolution of the outputs of finite width $n$ networks, during training, by computing deviations from the limiting infinite width case (in which the network evolves through a linear flow). This improves previous estimates and yields sharper decay rates for t",
    "path": "papers/23/04/2304.03385.json",
    "total_tokens": 951,
    "translated_title": "宽神经网络：从初始化的非高斯随机场到训练中的NTK几何（arXiv:2304.03385v1 [cs.LG]）",
    "translated_abstract": "近期神经网络应用中参数达到$n=10^{14}$，因此研究此类网络的大规模行为变得极为重要。此前的大部分研究都聚焦于宽神经网络的宽度无限大，即$n \\to +\\infty$时的极限情况，表明它们在初始化时符合高斯过程。本研究将研究大但有限规模的神经网络的行为。我们的主要贡献为：（1）计算以$n^{-\\frac{1}{2}}$为渐近级数的高斯性修正，该展开式的系数由参数初始化和激活函数的统计学确定。(2) 通过计算有限宽度$n$网络与极限情况下（在该情况下网络通过线性流演化）的偏差来控制网络在训练时的输出，具有更好的效果。这提高了以前的估计，得到了更好的衰减率。",
    "tldr": "本研究研究大规模但有限的神经网络行为。主要贡献为：（1）计算高斯性的修正，系数由参数初始化和激活函数的统计学确定。（2）通过计算网络与极限情况下的偏差来控制网络在训练时的输出，具有更好的效果。",
    "en_tdlr": "This study investigates the behavior of large but finite neural networks, with main contributions including computing the corrections to Gaussianity and controlling the output of the networks during training by computing deviations from the limiting infinite width case."
}