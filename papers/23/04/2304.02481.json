{
    "title": "Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])",
    "abstract": "Training and inference on edge devices often requires an efficient setup due to computational limitations. While pre-computing data representations and caching them on a server can mitigate extensive edge device computation, this leads to two challenges. First, the amount of storage required on the server that scales linearly with the number of instances. Second, the bandwidth required to send extensively large amounts of data to an edge device. To reduce the memory footprint of pre-computed data representations, we propose a simple, yet effective approach that uses randomly initialized hyperplane projections. To further reduce their size by up to 98.96%, we quantize the resulting floating-point representations into binary vectors. Despite the greatly reduced size, we show that the embeddings remain effective for training models across various English and German sentence classification tasks that retain 94%--99% of their floating-point.",
    "link": "http://arxiv.org/abs/2304.02481",
    "context": "Title: Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])\nAbstract: Training and inference on edge devices often requires an efficient setup due to computational limitations. While pre-computing data representations and caching them on a server can mitigate extensive edge device computation, this leads to two challenges. First, the amount of storage required on the server that scales linearly with the number of instances. Second, the bandwidth required to send extensively large amounts of data to an edge device. To reduce the memory footprint of pre-computed data representations, we propose a simple, yet effective approach that uses randomly initialized hyperplane projections. To further reduce their size by up to 98.96%, we quantize the resulting floating-point representations into binary vectors. Despite the greatly reduced size, we show that the embeddings remain effective for training models across various English and German sentence classification tasks that retain 94%--99% of their floating-point.",
    "path": "papers/23/04/2304.02481.json",
    "total_tokens": 866,
    "translated_title": "重新发现基于哈希的随机投影，用于有效量化上下文化句子嵌入",
    "translated_abstract": "由于计算能力的限制，对边缘设备进行训练和推断通常需要高效的设置。尽管预先计算数据表示并在服务器上缓存可以减少边缘设备的计算量，但这会带来两个挑战。首先，存储在服务器上所需的存储量随实例数量呈线性增长。其次，需要发送大量数据的带宽到边缘设备。为了减少预先计算的数据表示的存储空间开销，我们提出了一种简单但有效的方法，即使用随机初始化的超平面投影。为了将它们的大小进一步缩小至98.96％，我们将得到的浮点表示量化为二进制向量。尽管大小大大缩小，但我们表明这些嵌入对多种保留了94％-99％浮点值的英语和德语句子分类任务训练模型仍然有效。",
    "tldr": "本文提出了一种有效的方法，使用哈希随机投影和量化技术有效量化上下文化句子嵌入，以降低存储空间的开销，并可以用于在多种英语和德语句子分类任务上训练模型。",
    "en_tdlr": "This paper proposes an efficient method for quantizing contextualized sentence embeddings using hashed random projections and quantization techniques, reducing their memory footprint while retaining effectiveness for training models across various English and German sentence classification tasks."
}