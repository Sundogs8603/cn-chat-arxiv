{
    "title": "One-Step Distributional Reinforcement Learning. (arXiv:2304.14421v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.",
    "link": "http://arxiv.org/abs/2304.14421",
    "context": "Title: One-Step Distributional Reinforcement Learning. (arXiv:2304.14421v1 [cs.LG])\nAbstract: Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.",
    "path": "papers/23/04/2304.14421.json",
    "total_tokens": 855,
    "translated_title": "一步分布式强化学习",
    "translated_abstract": "强化学习（Reinforcement Learning，RL）允许一个能代理与环境进行连续交互的系统最大化预期收益。在分布式RL（DistrRL）范式下，代理不仅局限于期望值，而是捕捉跨越所有时间步骤的回报概率分布。DistrRL算法的集合提高了经验性能，但DistrRL的理论仍未完全理解，尤其在控制案例中。本文提出了简单的一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性。与DistrRL相反，我们证明了我们的方法针对策略评估和控制都具有统一的理论。我们提出了两种OS-DistrRL算法，并提供了几乎确定的收敛分析。所提出的方法在多种环境中比分类DistrRL好。",
    "tldr": "本文提出一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性，提供了统一的理论，具有更好的表现。",
    "en_tdlr": "This paper proposes a one-step distributional reinforcement learning (OS-DistrRL) framework only covering the randomness induced by the one-step dynamics of the environment, provides a unified theory for both policy evaluation and control, and achieves better performance compared to categorical DistrRL on various environments."
}