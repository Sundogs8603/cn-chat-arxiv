{
    "title": "Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])",
    "abstract": "The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra",
    "link": "http://arxiv.org/abs/2304.01335",
    "context": "Title: Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])\nAbstract: The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra",
    "path": "papers/23/04/2304.01335.json",
    "total_tokens": 860,
    "translated_title": "用热噪声描绘神经网络景观的地形",
    "translated_abstract": "神经网络训练是一个复杂的高维、非凸且嘈杂的优化问题，其理论理解在应用角度和基础研究方面均具有重要意义。本文采用标准的统计力学方法，即通过Langevin动态相空间探测方法研究过参数全连接网络在随机数据上执行分类任务的梯度下降过程。通过分析涨落统计数据，类比于体系在恒定温度下的热动力学，我们推断出了一个清晰的地形描述——低损失区域是一个低维流形，其维度可以轻易地从波动性中获得。此外，该维度受到靠近分类决策边界的数据点数量的控制。重要的是，我们发现一个四阶相互作用是关键的，而标准的 Langevin 方法不能准确描述这种相互作用。",
    "tldr": "本文采用统计力学方法研究全连接神经网络的优化问题，发现在低损失区域存在一个低维流形，并由靠近分类决策边界的数据点数量决定维度。",
    "en_tdlr": "This paper uses statistical mechanics methods to study the optimization problem of fully connected neural networks, and finds a low-dimensional manifold in the low-loss region, whose dimension is determined by the number of data points near the classification decision boundary."
}