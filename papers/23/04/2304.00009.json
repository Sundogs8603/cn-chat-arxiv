{
    "title": "The challenge of redundancy on multi-agent value factorisation. (arXiv:2304.00009v1 [cs.AI])",
    "abstract": "In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redu",
    "link": "http://arxiv.org/abs/2304.00009",
    "context": "Title: The challenge of redundancy on multi-agent value factorisation. (arXiv:2304.00009v1 [cs.AI])\nAbstract: In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redu",
    "path": "papers/23/04/2304.00009.json",
    "total_tokens": 934,
    "translated_title": "多智能体价值因子化中冗余性的挑战",
    "translated_abstract": "在合作多智能体强化学习（MARL）领域中，标准范式是使用集中式训练和分散式执行，在此范式中，中央批评家基于中央状态调节合作智能体的策略。研究表明，在存在大量冗余智能体的情况下，这些方法变得不太有效。在更一般的情况下，环境中的智能体数量可能比解决任务所需的数量要多。这些冗余智能体通过增加状态空间的维度和增加用于解决环境的联合策略的大小来降低性能。我们建议利用层次相关传播（LRP）来分离联合值函数的学习和局部奖励信号的生成，并创建一种新 的MARL算法：关联分解网络（RDN）。我们发现，尽管两个基线VDN和Qmix的性能随着冗余智能体数量的增加而降低，但RDN在这种情况下仍然表现出色。",
    "tldr": "该论文提出了一种新的合作多智能体强化学习算法：关联分解网络，使用层次相关传播将联合值函数的学习和局部奖励信号的生成分开，针对存在大量冗余智能体的环境中，该算法比现有方法表现更出色。",
    "en_tdlr": "This paper proposes a new algorithm for cooperative multi-agent reinforcement learning, called relevance decomposition network (RDN), which separates the learning of the joint value function and generation of local reward signals using layerwise relevance propagation (LRP). RDN outperforms existing methods in environments with large numbers of redundant agents."
}