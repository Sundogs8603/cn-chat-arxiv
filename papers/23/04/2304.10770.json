{
    "title": "DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])",
    "abstract": "Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval",
    "link": "http://arxiv.org/abs/2304.10770",
    "context": "Title: DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])\nAbstract: Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval",
    "path": "papers/23/04/2304.10770.json",
    "total_tokens": 975,
    "translated_title": "DEIR: 基于区分性模型的情节内在奖励，高效且鲁棒的探索方法",
    "translated_abstract": "探索是强化学习中的一个基本方面，其有效性关键地影响着强化学习算法的性能，尤其是面对稀疏的外部奖励时更为重要。最近的研究表明，从观测中估计新颖性的内在奖励可以有效鼓励探索。然而，由于环境的随机性以及代理的行为可能会影响观察结果，因此一个观测的新颖性与探索之间存在差距。为了准确估计探索行为，我们提出了DEIR，一种新颖的方法，其中我们从条件互信息项中理论上导出内在奖励，该奖励主要与代理的探索行为所贡献的新颖性成比例，并借助区分性的前向模型实现奖励。我们在MiniGrid中进行了广泛的实验，包括标准和硬核探索游戏，在结果上DEIR比基线学习更快并且具有更高的成功率和鲁棒性，适应环境动态变化。",
    "tldr": "提出了一种探索强化学习算法DEIR，借助区分性模型实现理论上导出的内在奖励，能够高效且鲁棒地进行探索，适用于面对外部奖励稀疏的情况。",
    "en_tdlr": "DEIR is a novel exploration method in reinforcement learning that proposes intrinsic rewards estimated from novelty in observations and implemented through a discriminative forward model to accurately estimate exploratory behaviors. The method shows faster learning, higher success rate, and robustness against changes in environment dynamics in both standard and hardened exploration games in MiniGrid."
}