{
    "title": "Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)",
    "abstract": "The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional ",
    "link": "http://arxiv.org/abs/2304.11705",
    "context": "Title: Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)\nAbstract: The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional ",
    "path": "papers/23/04/2304.11705.json",
    "total_tokens": 1036,
    "translated_title": "多领域LiDAR语义分割中的方法研究和探讨",
    "translated_abstract": "在开发智能机器人的过程中，能够安全地在多样化的环境中运行是至关重要的。我们在同一领域的LiDAR语义分割方面取得了巨大的进展。然而，这些方法是否能够在不同领域之间泛化是一个问题。为了回答这个问题，我们设计了第一个用于研究领域泛化（DG-LSS）的实验设置。我们的结果表明，在跨领域的设置中评估方法之间存在显著差距：例如，在源数据集（SemanticKITTI）上训练的模型在目标数据上获得了26.53的mIoU，而在目标域（nuScenes）上训练的模型则获得了48.49的mIoU。为了解决这个差距，我们提出了第一个专门为DG-LSS设计的方法，在目标领域上获得了34.88的mIoU，超越了所有其他基线模型。我们的方法通过将稀疏卷积编码器-解码器3D分割网络与额外的密集2D卷积相结合，",
    "tldr": "本研究通过设计第一个实验设置，探讨了LiDAR语义分割在不同领域间的泛化能力。研究结果表明，现有方法在跨领域设置中存在显著差距。为了解决这个问题，我们提出了一种新的方法，通过结合稀疏-密集卷积网络，实现了在目标领域上的显著优化。"
}