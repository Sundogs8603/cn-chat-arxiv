{
    "title": "Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. (arXiv:2304.13826v1 [cs.AI])",
    "abstract": "Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional pretraining-finetuning pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose ProgramPort, a modular approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities.",
    "link": "http://arxiv.org/abs/2304.13826",
    "context": "Title: Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. (arXiv:2304.13826v1 [cs.AI])\nAbstract: Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional pretraining-finetuning pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose ProgramPort, a modular approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities.",
    "path": "papers/23/04/2304.13826.json",
    "total_tokens": 889,
    "translated_title": "可编程接地，组合通用的机器人操作",
    "translated_abstract": "在现实世界中操作的机器人需要丰富的操作技能以及在何时应用这些技能方面具有语义推理的能力。为此，最近的工作将来自大规模预训练视觉-语言（VL）模型的语义表示集成到操作模型中，赋予其更通用的推理能力。然而，我们发现用于整合此类表示的传统预训练微调流程将领域特定的行动信息和领域通用的视觉信息纠缠在一起，导致训练数据效率低下且泛化到未见过的对象和任务很差。为此，我们提出了 ProgramPort，这是一种更好地利用预训练 VL 模型的模块化方法，通过利用语言指令的句法和语义结构。我们的框架使用语义解析器恢复一个可执行程序，由跨不同模态的基于视觉和行动的功能模块组成。",
    "tldr": "本文提出了一种模块化的方法ProgramPort，它利用语言指令的句法和语义结构，以更好地利用预训练视觉-语言（VL）模型。该框架使用语义解析器恢复一个可执行程序，由跨不同模态的基于视觉和行动的功能模块组成。",
    "en_tdlr": "This paper proposes a modular approach called ProgramPort that leverages the syntactic and semantic structures of language instructions to better utilize pretrained vision-language (VL) models. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities."
}