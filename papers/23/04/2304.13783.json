{
    "title": "Fine Tuning with Abnormal Examples. (arXiv:2304.13783v1 [cs.CL])",
    "abstract": "Given the prevalence of crowd sourced labor in creating Natural Language processing datasets, these aforementioned sets have become increasingly large. For instance, the SQUAD dataset currently sits at over 80,000 records. However, because the English language is rather repetitive in structure, the distribution of word frequencies in the SQUAD dataset's contexts are relatively unchanged. By measuring each sentences distance from the co-variate distance of frequencies of all sentences in the dataset, we identify 10,500 examples that create a more uniform distribution for training. While fine-tuning ELECTRA [4] on this subset of examples reaches better performance to a model trained on all 87,000 examples. Herein we introduce a methodology for systematically pruning datasets for fine tuning reaching better out of sample performance.",
    "link": "http://arxiv.org/abs/2304.13783",
    "context": "Title: Fine Tuning with Abnormal Examples. (arXiv:2304.13783v1 [cs.CL])\nAbstract: Given the prevalence of crowd sourced labor in creating Natural Language processing datasets, these aforementioned sets have become increasingly large. For instance, the SQUAD dataset currently sits at over 80,000 records. However, because the English language is rather repetitive in structure, the distribution of word frequencies in the SQUAD dataset's contexts are relatively unchanged. By measuring each sentences distance from the co-variate distance of frequencies of all sentences in the dataset, we identify 10,500 examples that create a more uniform distribution for training. While fine-tuning ELECTRA [4] on this subset of examples reaches better performance to a model trained on all 87,000 examples. Herein we introduce a methodology for systematically pruning datasets for fine tuning reaching better out of sample performance.",
    "path": "papers/23/04/2304.13783.json",
    "total_tokens": 763,
    "translated_title": "带有异常案例的微调",
    "translated_abstract": "鉴于众包劳动在自然语言处理数据集中的普遍应用，上述数据集变得越来越大。例如，SQUAD数据集当前已经超过80,000条记录。然而，由于英语的结构相对重复，因此SQUAD数据集上下文中单词频率的分布相对不变。通过测量每个句子与数据集中所有句子频率的共变距离，我们识别出了10,500个例子，为训练创造了更加均匀的分布。在这个例子的子集上微调ELECTRA [4]达到了比在所有87,000个例子上训练的模型更好的性能。因此，我们介绍了一种方法来系统地修剪用于微调的数据集，以提高外部样本的性能。",
    "tldr": "通过识别出数据集中的异常例子，我们提出了一种系统修剪数据集的方法，这可以使得以这些数据集子集作为训练集微调模型的性能更优。"
}