{
    "title": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks. (arXiv:2304.03782v1 [cs.LG])",
    "abstract": "Exploring the expected quantizing scheme with suitable mixed-precision policy is the key point to compress deep neural networks (DNNs) in high efficiency and accuracy. This exploration implies heavy workloads for domain experts, and an automatic compression method is needed. However, the huge search space of the automatic method introduces plenty of computing budgets that make the automatic process challenging to be applied in real scenarios. In this paper, we propose an end-to-end framework named AutoQNN, for automatically quantizing different layers utilizing different schemes and bitwidths without any human labor. AutoQNN can seek desirable quantizing schemes and mixed-precision policies for mainstream DNN models efficiently by involving three techniques: quantizing scheme search (QSS), quantizing precision learning (QPL), and quantized architecture generation (QAG). QSS introduces five quantizing schemes and defines three new schemes as a candidate set for scheme search, and then u",
    "link": "http://arxiv.org/abs/2304.03782",
    "context": "Title: AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks. (arXiv:2304.03782v1 [cs.LG])\nAbstract: Exploring the expected quantizing scheme with suitable mixed-precision policy is the key point to compress deep neural networks (DNNs) in high efficiency and accuracy. This exploration implies heavy workloads for domain experts, and an automatic compression method is needed. However, the huge search space of the automatic method introduces plenty of computing budgets that make the automatic process challenging to be applied in real scenarios. In this paper, we propose an end-to-end framework named AutoQNN, for automatically quantizing different layers utilizing different schemes and bitwidths without any human labor. AutoQNN can seek desirable quantizing schemes and mixed-precision policies for mainstream DNN models efficiently by involving three techniques: quantizing scheme search (QSS), quantizing precision learning (QPL), and quantized architecture generation (QAG). QSS introduces five quantizing schemes and defines three new schemes as a candidate set for scheme search, and then u",
    "path": "papers/23/04/2304.03782.json",
    "total_tokens": 736,
    "translated_title": "AutoQNN: 一种自动量化神经网络的端到端框架",
    "translated_abstract": "探索适合深度神经网络（DNN）压缩的量化方案与混合精度策略是提高效率和准确性的关键。然而，自动压缩方法的巨大搜索空间导致了大量的计算预算，这使得自动过程难以在实际场景中应用。在本文中，我们提出了一种名为AutoQNN的端到端框架，可以自动量化不同层次的DNN模型，而无需任何人工干预。",
    "tldr": "AutoQNN是一种可自动量化DNN模型的端到端框架，利用三种技术实现了对适合不同层次的混合精度策略的搜索，并可有效提高效率和准确性。",
    "en_tdlr": "AutoQNN is an end-to-end framework that can automatically quantize DNN models utilizing three techniques for the search of mixed-precision policies suitable for different layers, improving efficiency and accuracy."
}