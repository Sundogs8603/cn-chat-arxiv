{
    "title": "Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])",
    "abstract": "Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr",
    "link": "http://arxiv.org/abs/2304.14065",
    "context": "Title: Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])\nAbstract: Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr",
    "path": "papers/23/04/2304.14065.json",
    "total_tokens": 936,
    "translated_title": "面向遥感时序数据的轻量级预训练Transformer",
    "translated_abstract": "远程传感数据的机器学习算法在社会相关应用方面具有广泛的应用，但用于训练这些算法的标签可能很难或不可能获得。这个挑战已经推动了自监督学习领域的研究，旨在通过遥感数据解锁在标记数据集较小的地理位置或应用领域中使用机器学习。我们展示了为遥感数据设计模型和自监督训练技术可以得到更小、更优秀的模型。我们介绍了Remote Sensing Transformer（Presto），它是一种基于Transformer的模型，使用新颖的自监督目标对遥感时间序列数据进行预训练。我们的实验表明，与在自然图像上训练的可比模型相比，Presto在几个遥感基准测试中实现了最先进的结果，同时需要数量级更少的参数。",
    "tldr": "设计针对远程传感器数据的自监督学习模型和训练技术，可以得到表现更好且更小的模型。预训练的遥感时间序列Transformer（Presto）在几个遥感基准测试中实现了最先进的结果。",
    "en_tdlr": "Designing self-supervised learning models and training techniques specifically for remote sensing data can result in smaller and more performant models. The Pretrained Remote Sensing Transformer (Presto), a transformer-based model pretrained on remote sensing timeseries data using a novel self-supervised objective, achieves state-of-the-art results on several remote sensing benchmarks while requiring orders of magnitude fewer parameters than comparable models trained on natural images."
}