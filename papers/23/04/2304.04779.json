{
    "title": "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner. (arXiv:2304.04779v1 [cs.LG])",
    "abstract": "Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to reg",
    "link": "http://arxiv.org/abs/2304.04779",
    "context": "Title: GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner. (arXiv:2304.04779v1 [cs.LG])\nAbstract: Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to reg",
    "path": "papers/23/04/2304.04779.json",
    "total_tokens": 867,
    "translated_title": "GraphMAE2：一种解码增强的自监督图学习方法",
    "translated_abstract": "图自监督学习（SSL）包括对比和生成方法，为解决现实世界中标签稀缺的图数据的根本难题提供了很大的潜力。在这两种图SSL技术中，掩码图自动编码器（例如GraphMAE）是最近取得有希望的结果的一种生成方法。其思想是使用自动编码器架构对从输入中随机屏蔽的节点特征（或结构）进行重构。然而，掩码特征重构的性能自然取决于输入特征的可辨别性，并且通常会对特征中的扰动产生影响。本文提出了一种掩码自监督学习框架GraphMAE2，旨在克服这个问题。想法是在图SSL中对特征重构施加正则化。具体而言，我们设计了多视角随机重新掩码解码和潜在表示预测的策略以实现正则化。",
    "tldr": "GraphMAE2是一种掩码自监督学习框架，采用多视角随机重新屏蔽解码和潜在表示预测策略对图SSL中的特征重构进行正则化。"
}