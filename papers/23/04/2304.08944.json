{
    "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning. (arXiv:2304.08944v1 [cs.LG])",
    "abstract": "An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorith",
    "link": "http://arxiv.org/abs/2304.08944",
    "context": "Title: Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning. (arXiv:2304.08944v1 [cs.LG])\nAbstract: An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorith",
    "path": "papers/23/04/2304.08944.json",
    "total_tokens": 990,
    "translated_title": "通过主动奖励学习实现可证明反馈高效的强化学习",
    "translated_abstract": "在强化学习中，合适的奖励函数对于明确任务非常重要。但是，即使对于简单的任务而言，设计正确的奖励函数也是非常具有挑战性的。人机交互强化学习（HiL RL）允许人类通过提供各种类型的反馈来向RL代理传达复杂的目标。然而，尽管取得了巨大的经验成功，但HiL RL通常需要太多人类教师的反馈，并且理论理解不足。本文从理论角度解决了这个问题，旨在提供可证明的反馈高效的算法框架，以采用人机交互来指定给定任务的奖励。我们提供了一种基于主动学习的RL算法，该算法首先在不指定奖励函数的情况下探索环境，然后仅询问人类教师有关某些状态动作对的任务奖励的少量查询。之后，该算法学习具有可证明遗憾界限的奖励函数，并实现接近最优的性能。与先前的HiL RL方法相比，我们的算法大大减少了所需的人类反馈，并在效率上提供了理论保证。",
    "tldr": "本文提出了一种主动奖励学习的强化学习算法，采用人机交互来指定任务的奖励，大大减少了所需的人类反馈，并在效率上提供了理论保证。",
    "en_tdlr": "The paper proposes an active-learning-based RL algorithm for specifying task rewards in reinforcement learning, which significantly reduces the required human feedback and provides theoretical guarantees on its efficiency."
}