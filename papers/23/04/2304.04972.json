{
    "title": "Federated Learning with Classifier Shift for Class Imbalance. (arXiv:2304.04972v1 [cs.LG])",
    "abstract": "Federated learning aims to learn a global model collaboratively while the training data belongs to different clients and is not allowed to be exchanged. However, the statistical heterogeneity challenge on non-IID data, such as class imbalance in classification, will cause client drift and significantly reduce the performance of the global model. This paper proposes a simple and effective approach named FedShift which adds the shift on the classifier output during the local training phase to alleviate the negative impact of class imbalance. We theoretically prove that the classifier shift in FedShift can make the local optimum consistent with the global optimum and ensure the convergence of the algorithm. Moreover, our experiments indicate that FedShift significantly outperforms the other state-of-the-art federated learning approaches on various datasets regarding accuracy and communication efficiency.",
    "link": "http://arxiv.org/abs/2304.04972",
    "context": "Title: Federated Learning with Classifier Shift for Class Imbalance. (arXiv:2304.04972v1 [cs.LG])\nAbstract: Federated learning aims to learn a global model collaboratively while the training data belongs to different clients and is not allowed to be exchanged. However, the statistical heterogeneity challenge on non-IID data, such as class imbalance in classification, will cause client drift and significantly reduce the performance of the global model. This paper proposes a simple and effective approach named FedShift which adds the shift on the classifier output during the local training phase to alleviate the negative impact of class imbalance. We theoretically prove that the classifier shift in FedShift can make the local optimum consistent with the global optimum and ensure the convergence of the algorithm. Moreover, our experiments indicate that FedShift significantly outperforms the other state-of-the-art federated learning approaches on various datasets regarding accuracy and communication efficiency.",
    "path": "papers/23/04/2304.04972.json",
    "total_tokens": 815,
    "translated_title": "带有分类器偏移的联邦学习解决类别不平衡问题",
    "translated_abstract": "联邦学习旨在协作学习全局模型，而训练数据属于不同的客户端，并且不允许交换。然而，在非IID数据上的统计异质性挑战，如分类中的类别不平衡，会导致客户端漂移并显著降低全局模型的性能。本文提出了一种称为FedShift的简单有效方法，通过在本地训练阶段在分类器输出上添加偏移以减轻类别不平衡的消极影响。我们从理论上证明了FedShift中的分类器偏移可以使本地最优解与全局最优解一致，并确保算法的收敛。此外，我们的实验表明，在各种数据集上，FedShift在准确性和通信效率方面显着优于其他最先进的联邦学习方法。",
    "tldr": "本文提出了一种名为FedShift的简单有效方法，通过在本地训练阶段在分类器输出上添加偏移以解决类别不平衡问题，并在各种数据集上表现优异。",
    "en_tdlr": "This paper proposes a simple and effective approach named FedShift to address the problem of class imbalance in federated learning by adding shift on the classifier output during the local training phase, and outperforms other state-of-the-art methods on various datasets in terms of accuracy and communication efficiency."
}