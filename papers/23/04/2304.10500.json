{
    "title": "Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])",
    "abstract": "Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent",
    "link": "http://arxiv.org/abs/2304.10500",
    "context": "Title: Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])\nAbstract: Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent",
    "path": "papers/23/04/2304.10500.json",
    "total_tokens": 1010,
    "translated_title": "使用 Transformer 模型进行 Simply Typed Lambda Calculus 的类型推理：深度学习在代码中的案例研究",
    "translated_abstract": "尽管深度学习和形式语言的交叉领域的研究越来越多，但系统性地探索使用 Transformer 模型推理有类型 lambda 演算方面的研究相对较少。这是一个有趣的研究领域，有两个原因。首先，有类型的 lambda 演算是编程语言的基础。将各种有类型 lambda 演算与有效神经网络结构相关联的一组启发式方法会提供一种将语言特征（例如多态性，子类型，继承等）映射到架构选择的系统方法。其次，Transformer 模型广泛用于应用于代码的深度学习架构中，但其设计和超参数空间在编程语言应用中尚未得到充分探索。因此，我们建议通过也许是编程语言最简单和最基本的特性，即术语和类型之间的关系，来探索这一点的基准测试。",
    "tldr": "本文探索使用 Transformer 模型进行形式语言的推理。通过使用编程语言最基本的特性——术语和类型之间的关系，提出了一种基准测试，并提出了一种基于 Transformer 的类型推理模型。实验结果表明，TTI 模型在推理准确性和速度方面都优于现有的最先进方法。",
    "en_tdlr": "This paper systematically explores using Transformer models for reasoning about typed lambda calculi, proposing a benchmark to explore the use of transformer models in programming language applications. The proposed Transformer-based Type Inference (TTI) model outperforms existing state-of-the-art methods in both inference accuracy and speed, demonstrating the feasibility and effectiveness of using transformers for reasoning about formal languages."
}