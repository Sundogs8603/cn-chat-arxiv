{
    "title": "Bayesian Optimization of Catalysts With In-context Learning. (arXiv:2304.05341v1 [physics.chem-ph])",
    "abstract": "Large language models (LLMs) are able to do accurate classification with zero or only a few examples (in-context learning). We show a prompting system that enables regression with uncertainty for in-context learning with frozen LLM (GPT-3, GPT-3.5, and GPT-4) models, allowing predictions without features or architecture tuning. By incorporating uncertainty, our approach enables Bayesian optimization for catalyst or molecule optimization using natural language, eliminating the need for training or simulation. Here, we performed the optimization using the synthesis procedure of catalysts to predict properties. Working with natural language mitigates difficulty synthesizability since the literal synthesis procedure is the model's input. We showed that in-context learning could improve past a model context window (maximum number of tokens the model can process at once) as data is gathered via example selection, allowing the model to scale better. Although our method does not outperform all",
    "link": "http://arxiv.org/abs/2304.05341",
    "context": "Title: Bayesian Optimization of Catalysts With In-context Learning. (arXiv:2304.05341v1 [physics.chem-ph])\nAbstract: Large language models (LLMs) are able to do accurate classification with zero or only a few examples (in-context learning). We show a prompting system that enables regression with uncertainty for in-context learning with frozen LLM (GPT-3, GPT-3.5, and GPT-4) models, allowing predictions without features or architecture tuning. By incorporating uncertainty, our approach enables Bayesian optimization for catalyst or molecule optimization using natural language, eliminating the need for training or simulation. Here, we performed the optimization using the synthesis procedure of catalysts to predict properties. Working with natural language mitigates difficulty synthesizability since the literal synthesis procedure is the model's input. We showed that in-context learning could improve past a model context window (maximum number of tokens the model can process at once) as data is gathered via example selection, allowing the model to scale better. Although our method does not outperform all",
    "path": "papers/23/04/2304.05341.json",
    "total_tokens": 983,
    "translated_abstract": "大语言模型（LLMs）能够在零个或仅有少量示例的情况下进行准确分类（上下文学习）。我们展示了一个提示系统，使用冻结的LLM（GPT-3、GPT-3.5和GPT-4）模型实现带不确定性的上下文学习回归，允许在不需要特征或架构调整的情况下进行预测。通过结合不确定性，我们的方法实现了基于自然语言的催化剂或分子优化的贝叶斯优化，消除了训练或模拟的需要。在这里，我们使用催化剂的合成程序来优化预测属性。与自然语言一起工作，减轻了合成困难，因为文本合成程序是模型的输入。我们展示了随着数据通过示例选择而聚集，上下文学习能够超越模型上下文窗口（模型一次能够处理的最大标记数），使模型能够更好地扩展。虽然我们的方法不能胜任所有",
    "tldr": "该论文展示了一种基于自然语言的催化剂或分子优化的贝叶斯优化方法，使用大语言模型进行具有不确定性的上下文学习回归，通过结合不确定性和示例选择，允许模型在零个或少量示例的情况下进行准确分类，能够消除训练或模拟的需要。",
    "en_tdlr": "This paper presents a Bayesian optimization method for catalyst or molecule optimization using natural language, with uncertainty-aware in-context learning enabled by large language models (LLMs) such as GPT-3, GPT-3.5, and GPT-4. The approach eliminates the need for training or simulation and allows accurate classification with zero or few examples through the combination of uncertainty and example selection, allowing the model to scale well beyond its context window."
}