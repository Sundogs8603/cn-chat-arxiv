{
    "title": "Points of non-linearity of functions generated by random neural networks. (arXiv:2304.09837v1 [cs.LG])",
    "abstract": "We consider functions from the real numbers to the real numbers, output by a neural network with 1 hidden activation layer, arbitrary width, and ReLU activation function. We assume that the parameters of the neural network are chosen uniformly at random with respect to various probability distributions, and compute the expected distribution of the points of non-linearity. We use these results to explain why the network may be biased towards outputting functions with simpler geometry, and why certain functions with low information-theoretic complexity are nonetheless hard for a neural network to approximate.",
    "link": "http://arxiv.org/abs/2304.09837",
    "context": "Title: Points of non-linearity of functions generated by random neural networks. (arXiv:2304.09837v1 [cs.LG])\nAbstract: We consider functions from the real numbers to the real numbers, output by a neural network with 1 hidden activation layer, arbitrary width, and ReLU activation function. We assume that the parameters of the neural network are chosen uniformly at random with respect to various probability distributions, and compute the expected distribution of the points of non-linearity. We use these results to explain why the network may be biased towards outputting functions with simpler geometry, and why certain functions with low information-theoretic complexity are nonetheless hard for a neural network to approximate.",
    "path": "papers/23/04/2304.09837.json",
    "total_tokens": 621,
    "translated_title": "随机神经网络生成的函数的非线性点",
    "translated_abstract": "我们考虑由具有1个隐藏激活层，任意宽度和ReLU激活函数的神经网络产生的从实数到实数的函数。我们假设神经网络的参数基于不同概率分布的均匀随机选择，并计算非线性点的期望分布。我们利用这些结果阐明为什么网络可能偏向于输出具有更简单几何形状的函数，以及为什么某些信息熵复杂度较低的函数仍然难以用神经网络近似。",
    "tldr": "论文从神经网络模型的参数分布出发，探究网络输出函数几何形状的简单性与信息熵复杂度的关系。",
    "en_tdlr": "This paper explores the relationship between the simplicity of geometric shapes and the entropy complexity of functions based on the distribution of neural network parameters."
}