{
    "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. (arXiv:2304.03589v1 [cs.LG])",
    "abstract": "The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules,",
    "link": "http://arxiv.org/abs/2304.03589",
    "context": "Title: On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. (arXiv:2304.03589v1 [cs.LG])\nAbstract: The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules,",
    "path": "papers/23/04/2304.03589.json",
    "total_tokens": 947,
    "translated_title": "大规模深度学习模型高效训练方法：文献综述",
    "translated_abstract": "深度学习领域在计算机视觉、自然语言处理和语音等方面取得了显著进展。采用大规模模型并在海量数据上进行训练，对于实际应用具有巨大的潜力，可以增强工业生产力，促进社会发展。然而，随着对计算能力要求的增加，尽管有许多研究探讨了高效训练方法，对于训练深度学习模型的加速技术的全面总结仍然备受期待。在本综述中，我们详细回顾了训练加速的研究进展，并将基本组件分为五个主要视角：（1）数据中心化：包括数据集正则化、数据采样和数据中心化课程学习技术，这些方法可以显著减少数据样本的计算复杂度；（2）模型中心化：包括基本模块的加速、模型压缩和模型蒸馏技术；(剩下同上原文)",
    "tldr": "研究深度学习模型高效训练方法已有不少成果，但尚缺乏全面总结。本文综述分为数据中心化、模型中心化、超参数优化、深度学习硬件和训练策略等五个方面，比较详尽地回顾了加速深度学习模型训练的基本组件。",
    "en_tdlr": "This literature review presents efficient training techniques for large-scale deep learning models, covering five main perspectives: data-centric, model-centric, hyperparameter optimization, hardware, and training strategy. It comprehensively summarizes the basic components for accelerating the training of deep learning models."
}