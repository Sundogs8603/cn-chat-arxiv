{
    "title": "Shuffle & Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])",
    "abstract": "We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in",
    "link": "http://arxiv.org/abs/2304.09374",
    "context": "Title: Shuffle & Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])\nAbstract: We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in",
    "path": "papers/23/04/2304.09374.json",
    "total_tokens": 921,
    "translated_title": "洗牌和切割：长文本的对比学习",
    "translated_abstract": "我们提出了一种基于对比学习的自监督学习方法，用于长文本文档。我们的方法的关键在于“洗牌和切割”（SaD），这是一种简单的文本增广算法，可为基于BERT的文档嵌入所需的对比更新设置一个前置任务。SaD将文档拆分为两个子文档，其中包含随机洗牌的单词。这些子文档被视为正样本，将所有其他文档视为负样本。在SaD之后，我们重复对比更新和聚类阶段，直至收敛。我们的方法可以帮助减轻人力资源的工作量，从而节省昂贵的AI资源。我们通过对20 Newsgroups、Reuters-21578、BBC和BBCSport数据集进行无监督文本分类的经验评估。特别是，我们的方法在20 Newsgroups上将当前的最新技术SS-SB-MT提高了20.94％。",
    "tldr": "本文介绍了一种基于对比学习的自监督学习方法，通过“洗牌和切割”算法对长文本进行预处理，提取BERT嵌入，在无监督的情况下对文本进行分类，比当前最先进的技术提高20.94%。"
}