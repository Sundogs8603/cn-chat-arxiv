{
    "title": "TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training. (arXiv:2304.06947v1 [cs.LG])",
    "abstract": "In cross-device Federated Learning (FL) environments, scaling synchronous FL methods is challenging as stragglers hinder the training process. Moreover, the availability of each client to join the training is highly variable over time due to system heterogeneities and intermittent connectivity. Recent asynchronous FL methods (e.g., FedBuff) have been proposed to overcome these issues by allowing slower users to continue their work on local training based on stale models and to contribute to aggregation when ready. However, we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate. The primary reason is that fast-speed devices contribute to many more rounds of aggregation while others join more intermittently or not at all, and with stale model updates. To overcome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronous FL framework with adaptive partial training. During the training, TimelyFL adjusts the ",
    "link": "http://arxiv.org/abs/2304.06947",
    "context": "Title: TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training. (arXiv:2304.06947v1 [cs.LG])\nAbstract: In cross-device Federated Learning (FL) environments, scaling synchronous FL methods is challenging as stragglers hinder the training process. Moreover, the availability of each client to join the training is highly variable over time due to system heterogeneities and intermittent connectivity. Recent asynchronous FL methods (e.g., FedBuff) have been proposed to overcome these issues by allowing slower users to continue their work on local training based on stale models and to contribute to aggregation when ready. However, we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate. The primary reason is that fast-speed devices contribute to many more rounds of aggregation while others join more intermittently or not at all, and with stale model updates. To overcome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronous FL framework with adaptive partial training. During the training, TimelyFL adjusts the ",
    "path": "papers/23/04/2304.06947.json",
    "total_tokens": 1116,
    "translated_title": "TimelyFL：面向异构异步联邦学习的自适应部分训练",
    "translated_abstract": "在跨设备联邦学习环境中，同步联邦学习方法的规模化存在挑战，因为滞后者会阻碍训练过程。此外，由于系统异构性和间歇连接，每个客户端加入训练的可用性随时间高度变化。最近提出了异步联邦学习方法（例如FedBuff）以克服这些问题，允许较慢的用户基于旧模型继续进行本地训练，并在准备好时进行贡献汇总。然而，我们经验证明，这种方法可能会导致训练精度大幅下降，收敛速度变慢。主要原因是快速设备对许多汇总轮次做出贡献，而其他设备不定期加入，模型更新也过时了。为克服这一障碍，我们提出了TimelyFL，一种面向异构异步联邦学习的自适应部分训练框架。在训练过程中，TimelyFL根据每个设备的系统异构性和通信模式来调整汇总计划，并自适应地选择每轮要更新模型的设备子集。因此，在实际异构网络条件下，我们的方法可以实现更快的收敛和更高的精度。",
    "tldr": "TimelyFL提出了一种面向异构异步联邦学习的自适应部分训练框架，在训练过程中根据设备的异构性和通信模式调整汇总计划，并自适应地选择要更新模型的设备子集，从而克服了现有方法在面对异构网络时可能导致的训练精度下降和收敛速度变慢的问题。",
    "en_tdlr": "TimelyFL proposes an adaptive partial training framework for heterogeneity-aware asynchronous Federated Learning in which the aggregation schedule is adjusted based on each device's system heterogeneity and communication pattern, and a subset of devices to update their models is adaptively chosen in each round, achieving faster convergence and higher accuracy than existing approaches under realistic heterogeneous network conditions."
}