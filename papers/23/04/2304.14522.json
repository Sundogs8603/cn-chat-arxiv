{
    "title": "Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])",
    "abstract": "Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range ",
    "link": "http://arxiv.org/abs/2304.14522",
    "context": "Title: Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])\nAbstract: Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range ",
    "path": "papers/23/04/2304.14522.json",
    "total_tokens": 795,
    "translated_title": "信息检索的多元表示学习",
    "translated_abstract": "稠密检索模型使用双编码器网络架构来学习查询和文档的表示形式，这些表示形式通常采用向量表示，它们的相似性通常使用点积函数计算。本文提出一种新的稠密检索表示学习框架。我们的框架不是学习每个查询和文档的向量，而是学习多元分布，并使用负多元KL散度计算分布之间的相似性。为了简化和提高效率，我们假设这些分布是多维正态分布，然后训练大型语言模型来生成这些分布的均值和方差向量。我们为所提出的框架提供了理论基础，并展示了它可以无缝地集成到现有的近似最近邻算法中以实现高效检索。我们进行了广泛的实验，覆盖了各种不同的基准数据集和评估指标。",
    "tldr": "本论文提出一种多元分布模型的信息检索表示学习框架，可无缝集成到现有近似最近邻算法中以实现高效检索。",
    "en_tdlr": "This paper proposes a multivariate distribution-based representation learning framework for information retrieval, which can be seamlessly integrated into existing approximate nearest neighbor algorithms to achieve efficient retrieval."
}