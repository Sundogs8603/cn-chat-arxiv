{
    "title": "Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])",
    "abstract": "Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillat",
    "link": "http://arxiv.org/abs/2304.04326",
    "context": "Title: Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])\nAbstract: Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillat",
    "path": "papers/23/04/2304.04326.json",
    "total_tokens": 1037,
    "translated_title": "基于分布中知识蒸馏的非IID数据同质化方法来进行分散式学习",
    "translated_abstract": "分散式学习允许在多个节点上以分散式方式训练深度神经网络（DNN），并使用各种数据源进行训练。然而，分散式学习的一个主要挑战是节点间数据分布的异质性。本文提出了基于分布中知识蒸馏（IDKD）的方法来解决此挑战。IDKD的目标是同质化节点之间的数据分布。虽然这种数据同质化可以通过在节点之间交换数据来实现，但这样会牺牲隐私。IDKD使用公共数据集来从每个节点中提取知识，并通过生成的标签将其传递给其邻居，以实现相同的目标，同时保持隐私约束。我们评估了IDKD在非i.i.d.数据集上的性能，并显示它在同质数据分布方面显着优于现有方法，同时保持隐私和通信效率。",
    "tldr": "本文提出了基于分布中知识蒸馏（IDKD）的方法来解决节点间同质化数据分布的问题，该方法使用公共数据集来从每个节点中提取知识，并通过生成的标签将其传递给其邻居，以实现相同的目标，同时保持隐私约束。在非i.i.d.数据集上，IDKD的性能显着优于现有方法，同时保持隐私和通信效率。",
    "en_tdlr": "This paper proposes an In-Distribution Knowledge Distillation (IDKD) method to address the challenge of data heterogeneity in decentralized learning, using a common public dataset to extract knowledge from each node and communicate it to its neighbors through generated labels. IDKD significantly outperforms existing approaches for homogeneous data distribution on non-i.i.d. datasets, while maintaining privacy and communication efficiency."
}