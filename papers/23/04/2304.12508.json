{
    "title": "Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])",
    "abstract": "We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\\% test cases and defeats baselines.",
    "link": "http://arxiv.org/abs/2304.12508",
    "context": "Title: Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])\nAbstract: We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\\% test cases and defeats baselines.",
    "path": "papers/23/04/2304.12508.json",
    "total_tokens": 857,
    "translated_title": "通过无模型强化学习尽快实现正式规范",
    "translated_abstract": "本文提出了一个无模型强化学习解决方案，即 ASAP-Phi框架，以鼓励代理尽快满足正式规范。该框架利用一个分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励。然后，使用基于演员-评论家的算法（例如软演员-评论家(SAC)或深度确定性策略梯度（DDPG））训练代理。此外，我们证明ASAP-Phi生成的策略优先考虑尽快实现规范。对最先进的基准测试中进行了大量实验，包括消融研究。结果显示，我们的框架成功地为多达97％的测试用例找到了足够快的轨迹，并击败了基线。",
    "tldr": "本文提出了 ASAP-Phi框架，利用分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励，使用基于演员-评论家的算法训练代理来尽快实现规范。",
    "en_tdlr": "This article proposes the ASAP-Phi framework, which uses a piece-wise reward function to assign quantitative semantic reward to traces not satisfying the specification and a high constant reward to the remaining, and trains agents with actor-critic-based algorithms to fulfill the formal specification as soon as possible."
}