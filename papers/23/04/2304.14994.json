{
    "title": "A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])",
    "abstract": "Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m",
    "link": "http://arxiv.org/abs/2304.14994",
    "context": "Title: A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])\nAbstract: Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m",
    "path": "papers/23/04/2304.14994.json",
    "total_tokens": 868,
    "translated_title": "用神经网络求解初值偏微分方程的稳定可扩展方法",
    "translated_abstract": "与传统的网格和基于网格的方法不同，神经网络有可能打破维数灾难，在使用经典求解器困难或不可能的问题中提供近似解。全局最小化神经网络参数中的 PDE 残差对于边界值问题效果良好，但是灾难性忘却损害了这种方法对于初值问题的适用性。在替代的局部时间方法中，可以将优化问题转化为网络参数上的常微分方程（ODE），并将解向前传播。然而，我们证明了目前基于这种方法的方法存在两个关键问题。首先，遵循 ODE 会导致问题条件增长无法控制，最终导致不可接受的大数值误差。其次，随着 ODE 方法随着 m 的数量呈立方级别扩展。",
    "tldr": "本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。",
    "en_tdlr": "This paper proposes a stable and scalable method for solving initial value partial differential equations (PDEs) using neural networks, solving the catastrophic forgetting encountered in global minimization of the PDE residual over the network parameters and cubic scaling issues in ODE methods."
}