{
    "title": "Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])",
    "abstract": "We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.",
    "link": "http://arxiv.org/abs/2304.13479",
    "context": "Title: Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])\nAbstract: We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.",
    "path": "papers/23/04/2304.13479.json",
    "total_tokens": 844,
    "translated_title": "先验信息学习中的基本权衡",
    "translated_abstract": "本文旨在探讨学习者在所学问题上的先验信息的准确性和其学习性能之间的基本权衡。我们引入了优先风险的概念，它不同于传统的极小极大和贝叶斯风险，可以让我们研究现实不一定符合学习者先验的情况下这些基本权衡。我们提出了一种基于缩减的方法来扩展经典的极小极大下界技术，以便为统计估计问题的优先风险提供下界。我们还介绍了一种新颖的法诺不等式的推广（可能具有独立的兴趣），用于在涉及无限损失的更一般的设置下，下界优先风险。我们展示了我们的框架揭示了在估计、回归和强化学习问题中，先验信息与学习性能之间权衡的能力。",
    "tldr": "本文研究了先验信息准确性和学习性能之间的基本权衡，引入了优先风险概念，并为统计估计问题提供了下界，展现了框架在不同问题中的应用。",
    "en_tdlr": "This paper explores the fundamental tradeoffs between the accuracy of prior information and learning performance, introduces prioritized risk, provides lower bounds for statistical estimation problems, and illustrates the framework's application in various problems."
}