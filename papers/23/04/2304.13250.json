{
    "title": "Exploring the Curious Case of Code Prompts. (arXiv:2304.13250v1 [cs.CL])",
    "abstract": "Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some but not all tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.",
    "link": "http://arxiv.org/abs/2304.13250",
    "context": "Title: Exploring the Curious Case of Code Prompts. (arXiv:2304.13250v1 [cs.CL])\nAbstract: Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some but not all tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.",
    "path": "papers/23/04/2304.13250.json",
    "total_tokens": 833,
    "translated_title": "探究代码提示的神奇案例",
    "translated_abstract": "最近的研究表明，在自然语言的代码表示上提示语言模型可以提高结构化推理任务的性能。然而，这些任务仅构成所有自然语言任务的一小部分。在我们的工作中，我们将回答是否代码提示是一般与语言模型互动的首选方式。我们在三种流行的 GPT 模型 (davinci, code-davinci-002 和 text-davinci-002) 上比较代码提示和文本提示在更广泛的任务选择上 (如 QA, 情感分析, 摘要等) 的性能，并发现除了少数例外，代码提示并不总是比文本提示更好。此外，我们还表明，代码提示风格在某些任务上对性能有很大影响，但并非所有任务都会受到影响，对文本说明进行微调会导致代码提示相对性能更好。",
    "tldr": "研究比较了代码提示和文本提示在多种自然语言任务上的性能表现，发现除了少数任务外，代码提示并不总是比文本提示更好，代码提示风格对性能有一定影响，而微调可以提高代码提示的性能。",
    "en_tdlr": "This paper compares the performance of code prompts and text prompts on various natural language tasks and finds that code prompts are not always better than text prompts, except for a few tasks. The style of code prompt has an impact on performance for some tasks, and fine-tuning can improve the performance of code prompts."
}