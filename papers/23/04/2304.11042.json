{
    "title": "Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])",
    "abstract": "Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur",
    "link": "http://arxiv.org/abs/2304.11042",
    "context": "Title: Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])\nAbstract: Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur",
    "path": "papers/23/04/2304.11042.json",
    "total_tokens": 817,
    "translated_title": "无需反向传播的深度物理神经网络训练",
    "translated_abstract": "近年来，深度学习在诸如视觉和自然语言处理等各个领域取得了杰出的成功。这一成功很大程度上归功于深度学习模型的大规模，预计会不断增加。这种深度学习模型的增长伴随着与其可扩展性和训练、推理阶段中的能耗等问题相关的问题。虽然已经提出了一些基于非传统物理系统的工作来解决推理阶段的能效问题，但深度学习模型的有效训练仍未得到解决。迄今为止，数字深度学习模型的训练主要依赖于反向传播，但这种方法不适用于物理实现，因为它需要完全了解所谓前向传递的计算。在这里，我们通过提出一种简单的深度神经网络结构来解决这个问题。",
    "tldr": "该论文提出了一种新方法来训练深度学习模型，不需要使用反向传播算法。该方法可以有效地应用于基于物理系统的深度学习。",
    "en_tdlr": "This paper proposes a new method to train deep learning models without using backpropagation algorithm, which can effectively apply to physical-based deep learning."
}