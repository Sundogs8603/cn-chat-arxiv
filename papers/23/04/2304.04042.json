{
    "title": "Deep Anti-Regularized Ensembles provide reliable out-of-distribution uncertainty quantification. (arXiv:2304.04042v1 [cs.LG])",
    "abstract": "We consider the problem of uncertainty quantification in high dimensional regression and classification for which deep ensemble have proven to be promising methods. Recent observations have shown that deep ensemble often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-",
    "link": "http://arxiv.org/abs/2304.04042",
    "context": "Title: Deep Anti-Regularized Ensembles provide reliable out-of-distribution uncertainty quantification. (arXiv:2304.04042v1 [cs.LG])\nAbstract: We consider the problem of uncertainty quantification in high dimensional regression and classification for which deep ensemble have proven to be promising methods. Recent observations have shown that deep ensemble often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-",
    "path": "papers/23/04/2304.04042.json",
    "total_tokens": 977,
    "translated_title": "深度反正则化集成提供可靠的域外不确定性量化",
    "translated_abstract": "本文考虑了高维回归和分类中不确定性量化的问题，深度集成已被证明是一种有前途的方法。最近的研究表明，在训练范围之外，深度集成往往返回过度自信的估计值，这是一种主要的限制，因为在现实场景中经常遇到偏移分布。本文的主要挑战是解决增加集成输出的多样性和进行准确的内部预测之间的权衡问题。我们展示了一种解决方法，即使用具有大权重的网络的集成很可能实现这两个目标。我们提出了一种简单实用的方法来生成这样的集成，该方法基于惩罚小权重的原始反正则化术语和控制权重增加的过程，以保持内部分布损失在可接受的阈值范围内。所开发的方法不需要任何外部数据或复杂的训练技术。在各种基准数据集上进行的广泛实验表明，与最先进的深度集成相比，我们的方法在内部和外部不确定性估计方面具有优越性。",
    "tldr": "本文提出一种基于反正则化的深度集成方法，适用于高维回归和分类中不确定性量化问题。实验证明该方法在内部和外部不确定性估计方面具有优越性。",
    "en_tdlr": "This paper proposes a deep ensemble method based on anti-regularization for uncertainty quantification in high dimensional regression and classification, which outperforms the state-of-the-art deep ensembles in both in-distribution and out-of-distribution uncertainty estimation according to extensive experiments on various benchmark datasets."
}