{
    "title": "Dimensionality Reduction as Probabilistic Inference. (arXiv:2304.07658v1 [stat.ML])",
    "abstract": "Dimensionality reduction (DR) algorithms compress high-dimensional data into a lower dimensional representation while preserving important features of the data. DR is a critical step in many analysis pipelines as it enables visualisation, noise reduction and efficient downstream processing of the data. In this work, we introduce the ProbDR variational framework, which interprets a wide range of classical DR algorithms as probabilistic inference algorithms in this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps, kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent variable is used to construct a covariance, precision, or a graph Laplacian matrix, which can be used as part of a generative model for the data. Inference is done by optimizing an evidence lower bound. We demonstrate the internal consistency of our framework and show that it enables the use of probabilistic programming languages (PPLs) for DR. Additionally, we illustrate that the f",
    "link": "http://arxiv.org/abs/2304.07658",
    "context": "Title: Dimensionality Reduction as Probabilistic Inference. (arXiv:2304.07658v1 [stat.ML])\nAbstract: Dimensionality reduction (DR) algorithms compress high-dimensional data into a lower dimensional representation while preserving important features of the data. DR is a critical step in many analysis pipelines as it enables visualisation, noise reduction and efficient downstream processing of the data. In this work, we introduce the ProbDR variational framework, which interprets a wide range of classical DR algorithms as probabilistic inference algorithms in this framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps, kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latent variable is used to construct a covariance, precision, or a graph Laplacian matrix, which can be used as part of a generative model for the data. Inference is done by optimizing an evidence lower bound. We demonstrate the internal consistency of our framework and show that it enables the use of probabilistic programming languages (PPLs) for DR. Additionally, we illustrate that the f",
    "path": "papers/23/04/2304.07658.json",
    "total_tokens": 987,
    "translated_title": "作为概率推断的降维方法",
    "translated_abstract": "降维算法将高维数据压缩到低维表示中，同时保留数据的重要特征。降维是许多分析流程中的关键步骤，因为它实现了数据的可视化、噪声降低和高效的下游处理。在本文中，我们引入了ProbDR变分框架，将广泛的经典DR算法解释为该框架中的概率推断算法。ProbDR包括PCA、CMDS、LLE、LE、MVU、扩散映射、kPCA、Isomap、(t-)SNE和UMAP。在我们的框架中，一个低维潜变量用于构建协方差、精度或图拉普拉斯矩阵，可以作为数据的生成模型的一部分。推断是通过优化一个证据下界来完成的。我们展示了我们框架的内部一致性，并表明它支持使用概率编程语言（PPL）进行DR。此外，我们证明了该框架可以完成常规DR算法的操作，并赋予了它通过概率变分推断的强大表达力。",
    "tldr": "该论文提出了ProbDR变分框架，将经典降维算法解释为概率推断算法，通过优化一个证据下界来完成推断操作。该框架不仅可以完成常规降维算法，还支持使用概率编程语言进行降维操作，具有强大的表达能力。",
    "en_tdlr": "This paper introduces the ProbDR variational framework which interprets classic dimensionality reduction algorithms as probabilistic inference algorithms. The framework encompasses a variety of algorithms and utilizes a low-dimensional latent variable to construct a covariance, precision, or graph Laplacian matrix. Inference is done by optimizing an evidence lower bound, and the framework supports the use of probabilistic programming languages for dimensionality reduction."
}