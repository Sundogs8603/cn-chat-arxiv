{
    "title": "Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism. (arXiv:2304.11414v1 [cs.DC])",
    "abstract": "The Mixture of Experts (MoE) model becomes an important choice of large language models nowadays because of its scalability with sublinear computational complexity for training and inference. However, existing MoE models suffer from two critical drawbacks, 1) tremendous inner-node and inter-node communication overhead introduced by all-to-all dispatching and gathering, and 2) limited scalability for the backbone because of the bound data parallel and expert parallel to scale in the expert dimension. In this paper, we systematically analyze these drawbacks in terms of training efficiency in the parallel framework view and propose a novel MoE architecture called Pipeline MoE (PPMoE) to tackle them. PPMoE builds expert parallel incorporating with tensor parallel and replaces communication-intensive all-to-all dispatching and gathering with a simple tensor index slicing and inner-node all-reduce. Besides, it is convenient for PPMoE to integrate pipeline parallel to further scale the backbo",
    "link": "http://arxiv.org/abs/2304.11414",
    "context": "Title: Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism. (arXiv:2304.11414v1 [cs.DC])\nAbstract: The Mixture of Experts (MoE) model becomes an important choice of large language models nowadays because of its scalability with sublinear computational complexity for training and inference. However, existing MoE models suffer from two critical drawbacks, 1) tremendous inner-node and inter-node communication overhead introduced by all-to-all dispatching and gathering, and 2) limited scalability for the backbone because of the bound data parallel and expert parallel to scale in the expert dimension. In this paper, we systematically analyze these drawbacks in terms of training efficiency in the parallel framework view and propose a novel MoE architecture called Pipeline MoE (PPMoE) to tackle them. PPMoE builds expert parallel incorporating with tensor parallel and replaces communication-intensive all-to-all dispatching and gathering with a simple tensor index slicing and inner-node all-reduce. Besides, it is convenient for PPMoE to integrate pipeline parallel to further scale the backbo",
    "path": "papers/23/04/2304.11414.json",
    "total_tokens": 1004,
    "translated_title": "Pipeline MoE:一种具有管道并行性的灵活MoE 实现",
    "translated_abstract": "随着深度学习模型的规模化，混合专家模型（MoE）因其训练和推断的亚线性计算复杂度而成为大型语言模型的重要选择。然而，现有的MoE模型存在两个关键缺点：1）全部调度和收集引入了巨大的内部节点和节点间通信开销，2）数据并行和专家并行维度受限，无法在专家维度上进行扩展。本文从并行框架的角度系统分析了这些缺点，并提出了一种称为Pipeline MoE（PPMoE）的新型MoE架构来解决它们。PPMoE以张量并行为基础构建专家并行，并用简单的张量索引切片和内部节点全局汇聚代替了通信密集的全部调度与收集。此外，PPMoE还可以方便地集成管道并行以进一步扩展骨干。在标准基准测试上的实验结果显示，PPMoE可以实现高达17％的训练效率提高，并在专家数量和批量大小方面显示出比现有最新模型更好的可扩展性。",
    "tldr": "Pipeline MoE是一种利用管道并行性架构解决了MoE模型通信开销和限制性扩展问题的灵活实现， 在标准基准测试中比现有最新模型具有更好的可扩展性和高达17%的训练效率提高。",
    "en_tdlr": "Pipeline MoE is a flexible implementation of MoE model with pipeline parallelism which solves the communication overhead and limited scalability issues, achieving up to 17% improvement in training efficiency and better scalability than state-of-the-art models in terms of number of experts and batch size according to the standard benchmark."
}