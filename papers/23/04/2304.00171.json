{
    "title": "Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])",
    "abstract": "Conformer models maintain a large number of internal states, the vast majority of which are associated with self-attention layers. With limited memory bandwidth, reading these from memory at each inference step can slow down inference. In this paper, we design an optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs. We explore various ideas to improve the execution speed, including replacing lower conformer blocks with convolution-only blocks, strategically downsizing the architecture, and utilizing an RNNAttention-Performer. Our optimized conformer can be readily incorporated into a cascaded-encoder setting, allowing a second-pass decoder to operate on its output and improve the accuracy whenever more resources are available. Altogether, we find that these optimizations can reduce latency by a factor of 6.8x, and come at a reasonable trade-off in quality. With the cascaded second-pass, we show that the recognition accuracy is completel",
    "link": "http://arxiv.org/abs/2304.00171",
    "context": "Title: Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])\nAbstract: Conformer models maintain a large number of internal states, the vast majority of which are associated with self-attention layers. With limited memory bandwidth, reading these from memory at each inference step can slow down inference. In this paper, we design an optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs. We explore various ideas to improve the execution speed, including replacing lower conformer blocks with convolution-only blocks, strategically downsizing the architecture, and utilizing an RNNAttention-Performer. Our optimized conformer can be readily incorporated into a cascaded-encoder setting, allowing a second-pass decoder to operate on its output and improve the accuracy whenever more resources are available. Altogether, we find that these optimizations can reduce latency by a factor of 6.8x, and come at a reasonable trade-off in quality. With the cascaded second-pass, we show that the recognition accuracy is completel",
    "path": "papers/23/04/2304.00171.json",
    "total_tokens": 929,
    "translated_title": "实用Conformer：优化设备和云ASR的Conformer大小，速度和FLOPS",
    "translated_abstract": "Conformer模型维护大量的内部状态，其中绝大部分与自注意力层相关。在有限的内存带宽下，每次推理从内存中读取这些状态可能会减慢推理速度。在本文中，我们设计了一个优化的Conformer，它足够小，以满足设备的限制，并可以在TPU上快速推理。我们探索了各种想法来提高执行速度，包括用仅包含卷积的块替换较低的Conformer块，策略性地缩小体系结构，并利用RNNAttention-Performer。我们的优化Conformer可以轻松地并入级联编码器设置，允许第二遍解码器对其输出进行操作，并在更多资源可用时提高准确性。总的来说，我们发现这些优化可以将延迟降低6.8倍，且在质量上取得合理的折衷。通过级联第二次通过，我们展示了识别精度的完全实现。",
    "tldr": "本文介绍了一种优化的Conformer模型，它在大小、速度和FLOPS方面经过优化，可以在有限的设备上具有快速的推理速度。优化的Conformer可以轻松地并入级联编码器设置，并在更多资源可用时提高准确性。",
    "en_tdlr": "This paper introduces an optimized Conformer model that is optimized for size, speed, and FLOPS, and has fast inference on limited devices. The optimized Conformer can be easily incorporated into a cascaded-encoder setting and improves accuracy when more resources are available."
}