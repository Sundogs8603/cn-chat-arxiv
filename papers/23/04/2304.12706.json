{
    "title": "What does BERT learn about prosody?. (arXiv:2304.12706v1 [cs.CL])",
    "abstract": "Language models have become nearly ubiquitous in natural language processing applications achieving state-of-the-art results in many tasks including prosody. As the model design does not define predetermined linguistic targets during training but rather aims at learning generalized representations of the language, analyzing and interpreting the representations that models implicitly capture is important in bridging the gap between interpretability and model performance. Several studies have explored the linguistic information that models capture providing some insights on their representational capacity. However, the current studies have not explored whether prosody is part of the structural information of the language that models learn. In this work, we perform a series of experiments on BERT probing the representations captured at different layers. Our results show that information about prosodic prominence spans across many layers but is mostly focused in middle layers suggesting th",
    "link": "http://arxiv.org/abs/2304.12706",
    "context": "Title: What does BERT learn about prosody?. (arXiv:2304.12706v1 [cs.CL])\nAbstract: Language models have become nearly ubiquitous in natural language processing applications achieving state-of-the-art results in many tasks including prosody. As the model design does not define predetermined linguistic targets during training but rather aims at learning generalized representations of the language, analyzing and interpreting the representations that models implicitly capture is important in bridging the gap between interpretability and model performance. Several studies have explored the linguistic information that models capture providing some insights on their representational capacity. However, the current studies have not explored whether prosody is part of the structural information of the language that models learn. In this work, we perform a series of experiments on BERT probing the representations captured at different layers. Our results show that information about prosodic prominence spans across many layers but is mostly focused in middle layers suggesting th",
    "path": "papers/23/04/2304.12706.json",
    "total_tokens": 872,
    "translated_title": "BERT学习到了什么关于韵律的知识?",
    "translated_abstract": "语言模型已经在自然语言处理领域变得非常普遍，取得了许多任务的最先进结果，包括韵律。在训练期间，模型设计并不定义预先确定的语言目标，而是旨在学习语言的广义表示，因此分析和解释模型隐式捕获的表示对于弥合可解释性和模型性能之间的差距非常重要。几项研究探讨了模型捕获的语言信息，提供了一些有关它们表示能力的见解。然而，当前的研究尚未探索韵律是否是模型学习的语言结构信息的组成部分。在这项工作中，我们在BERT上进行了一系列实验，探究了不同层次捕捉到的表示。我们的结果表明，有关韵律突出的信息跨越许多层，但主要集中在中间层，这表明韵律确实是BERT学习到的结构信息的一部分。",
    "tldr": "本论文在BERT上进行了一系列实验，探究了不同层次捕捉到的表示。结果表明，韵律是BERT学习到的结构信息的一部分，主要集中在中间层。",
    "en_tdlr": "This paper performs a series of experiments on BERT to explore the representations captured at different layers and shows that prosody is indeed part of the structural information that BERT learns, mostly concentrated in the middle layers."
}