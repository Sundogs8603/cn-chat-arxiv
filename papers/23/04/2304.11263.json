{
    "title": "Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])",
    "abstract": "Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.",
    "link": "http://arxiv.org/abs/2304.11263",
    "context": "Title: Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])\nAbstract: Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.",
    "path": "papers/23/04/2304.11263.json",
    "total_tokens": 888,
    "translated_title": "自然分布漂移下低样本稳健性的基准测试",
    "translated_abstract": "近年来，结合更好的微调方法的预训练策略已经取得了针对自然分布漂移的鲁棒性的显著进展。然而，这样的微调假设可以访问大量标记数据，而当训练数据量不高时观察到的情况尚不清楚。我们通过对不同少样本数据集、架构、预训练初始化和最先进的稳健性干预的自然分布漂移的稳健性进行了首次深入研究，填补了这一空白。最重要的是，我们发现没有单一的选择模型比其他模型更稳健，即使在完整样本下，现有的干预措施也可能无法提高某些数据集的稳健性。我们希望我们的工作能够激励社区关注这个实际重要性的问题。",
    "tldr": "本文通过对不同少样本数据集、架构、预训练初始化和稳健性干预的自然分布漂移的稳健性进行了首次深入研究，发现没有单一的选择模型比其他模型更稳健，现有的干预措施也可能无法提高某些数据集的稳健性。",
    "en_tdlr": "This paper conducts the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes and finds that there is no single choice model that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime."
}