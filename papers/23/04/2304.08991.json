{
    "title": "D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])",
    "abstract": "This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\\% of existing approaches while substantially improving the quality of sentence embeddings. W",
    "link": "http://arxiv.org/abs/2304.08991",
    "context": "Title: D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])\nAbstract: This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\\% of existing approaches while substantially improving the quality of sentence embeddings. W",
    "path": "papers/23/04/2304.08991.json",
    "total_tokens": 989,
    "translated_title": "D2CSE: 基于差异感知的深度连续提示用于对比句子嵌入",
    "translated_abstract": "本文介绍了一种名为D2CSE的基于差异感知的深度连续提示模型，用于学习句子嵌入。与现有方法相比，D2CSE采用了简单的神经架构来计算句子向量，使得其对于在类似句子中区分微妙差异有很好的表现。与需要多个预训练语言模型（PLMs）处理原始和损坏（微妙修改）句子对的现有神经网络不同，D2CSE仅通过执行多个任务（即，对比学习和条件替换标记检测）自主引导地优化了连续提示，从而避免了繁琐的多个PLMs的微调。D2CSE将单个PLM重载到连续提示上，大大节省了存储空间。 D2CSE的训练参数数量约为现有方法的1％，同时显著提高了句子嵌入的质量。",
    "tldr": "D2CSE是一种用于学习句子嵌入的新模型，采用基于差异感知的深度连续提示来计算具有区分微妙差异能力的句子向量。与现有方法相比，D2CSE只使用一个预训练语言模型，避免了繁琐的微调，并大大减少了训练参数数量，同时显著提高了句子嵌入的质量。",
    "en_tdlr": "D2CSE is a new model for learning sentence embeddings that employs difference-aware deep continuous prompts to compute sentence vectors with exceptional ability to distinguish subtle differences in similar sentences. Compared to existing methods, D2CSE avoids cumbersome fine-tuning of multiple pre-trained language models by using only one and significantly reduces the number of training parameters while improving the quality of sentence embeddings."
}