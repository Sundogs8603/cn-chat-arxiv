{
    "title": "DropDim: A Regularization Method for Transformer Networks. (arXiv:2304.10321v1 [cs.CL])",
    "abstract": "We introduceDropDim, a structured dropout method designed for regularizing the self-attention mechanism, which is a key component of the transformer. In contrast to the general dropout method, which randomly drops neurons, DropDim drops part of the embedding dimensions. In this way, the semantic information can be completely discarded. Thus, the excessive coadapting between different embedding dimensions can be broken, and the self-attention is forced to encode meaningful featureswith a certain number of embedding dimensions erased. Experiments on a wide range of tasks executed on the MUST-C English-Germany dataset show that DropDim can effectively improve model performance, reduce over-fitting, and show complementary effects with other regularization methods. When combined with label smoothing, the WER can be reduced from 19.1% to 15.1% on the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the MT task. On the ST task, the model can reach a BLEU score of 22.99, an ",
    "link": "http://arxiv.org/abs/2304.10321",
    "context": "Title: DropDim: A Regularization Method for Transformer Networks. (arXiv:2304.10321v1 [cs.CL])\nAbstract: We introduceDropDim, a structured dropout method designed for regularizing the self-attention mechanism, which is a key component of the transformer. In contrast to the general dropout method, which randomly drops neurons, DropDim drops part of the embedding dimensions. In this way, the semantic information can be completely discarded. Thus, the excessive coadapting between different embedding dimensions can be broken, and the self-attention is forced to encode meaningful featureswith a certain number of embedding dimensions erased. Experiments on a wide range of tasks executed on the MUST-C English-Germany dataset show that DropDim can effectively improve model performance, reduce over-fitting, and show complementary effects with other regularization methods. When combined with label smoothing, the WER can be reduced from 19.1% to 15.1% on the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the MT task. On the ST task, the model can reach a BLEU score of 22.99, an ",
    "path": "papers/23/04/2304.10321.json",
    "total_tokens": 918,
    "tldr": "DropDim是一种结构化的dropout方法，可以规范transformer中的自注意机制，通过擦除一定数量的嵌入维度来编码有意义的特征。实验表明，DropDim能有效地提高模型性能，在多个任务上显示出互补效应和降低过拟合的作用。",
    "en_tdlr": "DropDim is a structured dropout method designed to regularize the self-attention mechanism in transformers by erasing a certain number of embedding dimensions to encode meaningful features. Experiments show that DropDim can effectively improve model performance and reduce over-fitting, showing complementary effects with other regularization methods in various tasks on the MUST-C English-Germany dataset."
}