{
    "title": "Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])",
    "abstract": "Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. We are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of \"code analysis\" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto",
    "link": "http://arxiv.org/abs/2304.06815",
    "context": "Title: Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])\nAbstract: Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. We are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of \"code analysis\" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto",
    "path": "papers/23/04/2304.06815.json",
    "total_tokens": 783,
    "translated_title": "用相关静态分析产品改善少样本提示",
    "translated_abstract": "大型语言模型是一类新型计算引擎，通过提示工程实现\"编程\"。我们仍在学习如何最好地\"编程\"这些大型语言模型以帮助开发人员。我们的研究从这样一种直觉出发，即开发人员在处理编码任务时会有一系列意识和无意识的语义事实。对于一个函数而言，这些语义事实可能包括参数和局部变量名称、返回表达式、简单的前置和后置条件以及基本的控制和数据流，等等。我们的目标是调查这个问题，使用代码摘要任务并评估是否使用显式添加信息能够帮助大型语言模型提取这些语义事实。",
    "tldr": "本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。",
    "en_tdlr": "This paper investigates how to improve the performance of large language models in few-shot prompts with relevant static analysis products, by exploring the possibility of extracting semantic facts in code through explicit information addition."
}