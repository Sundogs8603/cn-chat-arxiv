{
    "title": "Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach. (arXiv:2304.14115v1 [cs.AI])",
    "abstract": "Many decision-making problems feature multiple objectives. In such problems, it is not always possible to know the preferences of a decision-maker for different objectives. However, it is often possible to observe the behavior of decision-makers. In multi-objective decision-making, preference inference is the process of inferring the preferences of a decision-maker for different objectives. This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering. The performance of the proposed DWPI approach is compared to two existing preference inference methods from the literature, and empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both",
    "link": "http://arxiv.org/abs/2304.14115",
    "context": "Title: Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach. (arXiv:2304.14115v1 [cs.AI])\nAbstract: Many decision-making problems feature multiple objectives. In such problems, it is not always possible to know the preferences of a decision-maker for different objectives. However, it is often possible to observe the behavior of decision-makers. In multi-objective decision-making, preference inference is the process of inferring the preferences of a decision-maker for different objectives. This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering. The performance of the proposed DWPI approach is compared to two existing preference inference methods from the literature, and empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both",
    "path": "papers/23/04/2304.14115.json",
    "total_tokens": 914,
    "translated_title": "多目标强化学习中基于动态权重的演示偏好推断方法",
    "translated_abstract": "许多决策问题都涉及多个目标，而在这些问题中，不一定总是能知道决策者对不同目标的偏好。然而，观察决策者的行为往往是可行的。在多目标决策中，偏好推理是推断决策者对不同目标的偏好的过程。本研究提出了一种基于动态权重的偏好推理算法(DWPI)，通过观察环境中的行为轨迹，可以推断执行多目标决策问题的代理的偏好。提出的方法在三个多目标马尔可夫决策过程(Deep Sea Treasure、Traffic和Item Gathering)上进行了评估。实证结果表明，与文献中的两种现有偏好推理方法相比，所提出的DWPI方法在基线算法方面均有显著改进。",
    "tldr": "本研究提出了一种基于动态权重的偏好推理算法(DWPI)，通过观察环境中的行为轨迹，可以推断执行多目标决策问题的代理的偏好。实验结果表明，DWPI在多个多目标马尔科夫决策过程中的表现优于两种现有偏好推理方法。",
    "en_tdlr": "This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes and empirical results demonstrate significant improvements compared to the baseline algorithms."
}