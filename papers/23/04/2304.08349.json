{
    "title": "Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)",
    "abstract": "Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in ",
    "link": "http://arxiv.org/abs/2304.08349",
    "context": "Title: Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)\nAbstract: Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in ",
    "path": "papers/23/04/2304.08349.json",
    "total_tokens": 957,
    "translated_title": "深度可解释关系强化学习：一种神经符号化方法",
    "translated_abstract": "尽管深度强化学习（DRL）取得了许多成功，学习到的策略仍然是不可解释的。此外，由于DRL没有利用符号关系表示，它在处理环境结构变化（如对象数量增加）方面存在困难。另一方面，关系强化学习从符号计划继承了关系表示，以学习可重用的策略。然而，迄今为止，关系强化学习无法扩展并充分利用深度神经网络的能力。我们提出了一种深度可解释关系强化学习（DERRL）框架，它结合了神经和符号世界的优势。通过采用神经符号化方法，DERRL将符号计划中的关系表示和约束与深度学习相结合，提取出可解释的策略。这些策略以逻辑规则的形式解释了每个决策（或动作）的产生过程。通过多个实验，我们展示了DERRL在处理环境结构变化方面的优越性能。",
    "tldr": "本论文提出了一种深度可解释关系强化学习框架，通过将神经和符号世界相结合，利用神经符号化的方法提取出可解释的策略，解决了深度强化学习中不可解释性和处理环境结构变化的困难。",
    "en_tdlr": "This paper proposes a framework called Deep Explainable Relational Reinforcement Learning (DERRL) that combines neural and symbolic approaches to extract interpretable policies in deep reinforcement learning. It addresses the challenge of policy interpretability and coping with structural changes in the environment."
}