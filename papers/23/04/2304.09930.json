{
    "title": "Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives. (arXiv:2304.09930v1 [cs.AI])",
    "abstract": "A classic solution technique for Markov decision processes (MDP) and stochastic games (SG) is value iteration (VI). Due to its good practical performance, this approximative approach is typically preferred over exact techniques, even though no practical bounds on the imprecision of the result could be given until recently. As a consequence, even the most used model checkers could return arbitrarily wrong results. Over the past decade, different works derived stopping criteria, indicating when the precision reaches the desired level, for various settings, in particular MDP with reachability, total reward, and mean payoff, and SG with reachability.  In this paper, we provide the first stopping criteria for VI on SG with total reward and mean payoff, yielding the first anytime algorithms in these settings. To this end, we provide the solution in two flavours: First through a reduction to the MDP case and second directly on SG. The former is simpler and automatically utilizes any advances ",
    "link": "http://arxiv.org/abs/2304.09930",
    "context": "Title: Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives. (arXiv:2304.09930v1 [cs.AI])\nAbstract: A classic solution technique for Markov decision processes (MDP) and stochastic games (SG) is value iteration (VI). Due to its good practical performance, this approximative approach is typically preferred over exact techniques, even though no practical bounds on the imprecision of the result could be given until recently. As a consequence, even the most used model checkers could return arbitrarily wrong results. Over the past decade, different works derived stopping criteria, indicating when the precision reaches the desired level, for various settings, in particular MDP with reachability, total reward, and mean payoff, and SG with reachability.  In this paper, we provide the first stopping criteria for VI on SG with total reward and mean payoff, yielding the first anytime algorithms in these settings. To this end, we provide the solution in two flavours: First through a reduction to the MDP case and second directly on SG. The former is simpler and automatically utilizes any advances ",
    "path": "papers/23/04/2304.09930.json",
    "total_tokens": 1172,
    "translated_title": "具有定量目标的随机博弈价值迭代的停止准则",
    "translated_abstract": "Markov决策过程（MDP）和随机博弈（SG）的经典解决技术是价值迭代（VI）。尽管在最近之前无法给出结果不准确性的实际界限，但由于其良好的实际性能，这种近似方法通常优于确切技术。因此，即使是最常用的模型检查器也可能返回任意错误的结果。在过去的十年中，不同的作品为各种设置派生了停止准则，特别是具有可达性、总奖励和平均收益的MDP，以及具有可达性的SG。在本文中，我们为具有总奖励和平均收益的SG提供了第一个VI停止准则，从而在这些设置中提供了第一个随时算法。为此，我们以两种方式提供解决方案：第一种是通过将其约化为MDP案例，第二种是直接在SG上。前者更简单，可以自动利用MDP停止准则的所有进展。我们展示了它在有限次VI迭代后具有概率为1的近似SG值，可以完全接近所需的SG值。后者其实更强大，因为它直接考虑了游戏结构，不需要将其归约为MDP。我们在不同的领域中展示了这两种算法，并将它们应用于不同领域，例如POMDP中的可达性目标和来自自适应脉冲耦合振荡器的平均收益游戏。",
    "tldr": "本文提供了具有总奖励和平均收益的随机博弈的第一个价值迭代停止准则，为这些设置中提供了第一个随时算法。",
    "en_tdlr": "This paper presents the first stopping criteria for value iteration on stochastic games with total reward and mean payoff, providing the first anytime algorithms in these settings. It offers a solution in two flavors, one through a reduction to the Markov decision process case and the other directly on stochastic games, both of which have been experimentally demonstrated to yield an approximate value perfectly close to the desired one with probability one after a finite number of iterations."
}