{
    "title": "Hierarchical Weight Averaging for Deep Neural Networks. (arXiv:2304.11519v1 [cs.LG])",
    "abstract": "Despite the simplicity, stochastic gradient descent (SGD)-like algorithms are successful in training deep neural networks (DNNs). Among various attempts to improve SGD, weight averaging (WA), which averages the weights of multiple models, has recently received much attention in the literature. Broadly, WA falls into two categories: 1) online WA, which averages the weights of multiple models trained in parallel, is designed for reducing the gradient communication overhead of parallel mini-batch SGD, and 2) offline WA, which averages the weights of one model at different checkpoints, is typically used to improve the generalization ability of DNNs. Though online and offline WA are similar in form, they are seldom associated with each other. Besides, these methods typically perform either offline parameter averaging or online parameter averaging, but not both. In this work, we firstly attempt to incorporate online and offline WA into a general training framework termed Hierarchical Weight ",
    "link": "http://arxiv.org/abs/2304.11519",
    "context": "Title: Hierarchical Weight Averaging for Deep Neural Networks. (arXiv:2304.11519v1 [cs.LG])\nAbstract: Despite the simplicity, stochastic gradient descent (SGD)-like algorithms are successful in training deep neural networks (DNNs). Among various attempts to improve SGD, weight averaging (WA), which averages the weights of multiple models, has recently received much attention in the literature. Broadly, WA falls into two categories: 1) online WA, which averages the weights of multiple models trained in parallel, is designed for reducing the gradient communication overhead of parallel mini-batch SGD, and 2) offline WA, which averages the weights of one model at different checkpoints, is typically used to improve the generalization ability of DNNs. Though online and offline WA are similar in form, they are seldom associated with each other. Besides, these methods typically perform either offline parameter averaging or online parameter averaging, but not both. In this work, we firstly attempt to incorporate online and offline WA into a general training framework termed Hierarchical Weight ",
    "path": "papers/23/04/2304.11519.json",
    "total_tokens": 911,
    "translated_title": "深度神经网络的分层加权平均方法",
    "translated_abstract": "尽管随机梯度下降（SGD）算法非常简单，但对于深度神经网络（DNNs）的训练非常成功。在各种试图改进SGD的尝试中，加权平均（WA）近来在文献中受到了广泛的关注，它平均了多个模型的权重。WA分为两类：在线WA和离线WA。我们在本工作中将这两种方法结合起来，提出了一种名为层次加权平均（HWA）的通用训练框架。我们展示了HWA能够更好地利用在线和离线WA的优势，并且可以用于改进SGD。我们也从理论上和实际上研究了HWA的收敛性质，并在基准数据集上进行了大量实验。我们的实验表明，HWA可以持续改善SGD的性能，并且相对于最先进的方法，可以取得竞争性的结果。",
    "tldr": "HWA是一种将在线WA和离线WA相结合的通用训练框架，在提高SGD性能方面表现出色。",
    "en_tdlr": "HWA is a universal training framework that combines online and offline weight averaging, which consistently improves the performance of SGD with competitive results compared to state-of-the-art methods."
}