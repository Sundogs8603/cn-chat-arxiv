{
    "title": "W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])",
    "abstract": "Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and ",
    "link": "http://arxiv.org/abs/2304.08754",
    "context": "Title: W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])\nAbstract: Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and ",
    "path": "papers/23/04/2304.08754.json",
    "total_tokens": 950,
    "translated_title": "W-MAE：具有遮蔽自编码器的预训练天气模型，用于多变量天气预测",
    "translated_abstract": "天气预测是具有直接社会和经济影响的长期计算挑战。该任务涉及大量的连续数据收集，并在长时间内表现出丰富的时空依赖性，因此非常适合深度学习模型。本文将预训练技术应用于天气预测，并提出了一种用于多变量天气预测的具有遮蔽自编码器预训练的天气模型W-MAE。W-MAE以自监督的方式进行预训练，以重建气象变量之间的空间相关性。在时间尺度上，我们微调预训练的W-MAE以预测气象变量的未来状态，从而对天气数据中存在的时间依赖关系进行建模。我们使用每六小时选择一次样本，仅使用两年的ERA5数据，对W-MAE进行预训练。在相同的训练数据条件下，我们将W-MAE与FourCastNet进行比较。",
    "tldr": "本文介绍了一种名为 W-MAE 的预训练天气模型，它使用遮蔽自编码器重建气象变量之间的空间相关性，并通过微调预测气象变量的未来状态，从而对天气数据中存在的时空依赖关系进行建模。",
    "en_tdlr": "This paper introduces a pre-trained weather model called W-MAE, which uses masked autoencoder to reconstruct spatial correlations within meteorological variables, and fine-tunes to predict the future states of meteorological variables, thereby modeling the spatiotemporal dependencies present in weather data."
}