{
    "title": "Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])",
    "abstract": "We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \\textit{Journal of Machine Learning Research}, \\textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.",
    "link": "http://arxiv.org/abs/2304.05655",
    "context": "Title: Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])\nAbstract: We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \\textit{Journal of Machine Learning Research}, \\textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.",
    "path": "papers/23/04/2304.05655.json",
    "total_tokens": 858,
    "translated_title": "正则化和多视角支持向量机学习的本地化",
    "translated_abstract": "本文证明了 H.Q. Minh、L. Bazzani 和 V. Murino 在《机器学习研究》（Journal of Machine Learning Research）中介绍的一种涉及算子值正定核及其再生核希尔伯特空间的正则化和多视角支持向量机学习问题的本地化版本的一些表示定理。结果涉及到考虑凸或非凸损失函数以及有限或无限维输入空间的一般情况。我们展示了该一般框架允许一些特殊情况下的无限维输入空间和非凸损失函数，特别是当损失函数为 Gâteaux 可微函数时。对导致部分非线性问题的指数最小二乘损失函数进行了详细计算。",
    "tldr": "本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。",
    "en_tdlr": "This paper presents some representer theorems for a localised version of the regularised and multiview support vector machine learning problem, which involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The paper studies special cases related to loss functions and input space dimensions, particularly when the loss functions are Gâteaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that lead to partially nonlinear problems."
}