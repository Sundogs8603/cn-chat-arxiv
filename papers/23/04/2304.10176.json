{
    "title": "Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])",
    "abstract": "Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.",
    "link": "http://arxiv.org/abs/2304.10176",
    "context": "Title: Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])\nAbstract: Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.",
    "path": "papers/23/04/2304.10176.json",
    "total_tokens": 794,
    "translated_title": "通过权重锚定实现鲁棒深度强化学习调度",
    "translated_abstract": "当学习方法从仿真到现实中跨越鸿沟时，数据驱动学习方法的鲁棒性仍存在问题。本文提出了一种名为权重锚定的方法，该方法在持续学习中已知，并用于培养和固定神经网络中期望的行为。可以使用权重锚定方法找到一个与另一个学习问题的解接近的学习问题的解。这样，在优化环境下进行学习时，不会忽略或忘记期望的行为。我们通过学习混合的QoS高效离散资源调度与不频繁的优先消息的示例来演示此方法。结果表明，该方法提供了与增加仿真环境的现有技术相当的性能，同时显著提高了鲁棒性和可操纵性。",
    "tldr": "本研究提出了一种使用权重锚定来实现深度强化学习调度的方法，可避免在优化环境下忽略或忘记期望行为，提高了鲁棒性和可操纵性。",
    "en_tdlr": "This paper proposes a method for deep reinforcement learning scheduling through weight anchoring, which avoids neglecting or forgetting desired behavior in optimized environment and significantly improves robustness and steerability."
}