{
    "title": "Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])",
    "abstract": "We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p < 10^{-5}$ significance level. The same set-up improves \\textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine ",
    "link": "http://arxiv.org/abs/2304.10004",
    "context": "Title: Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])\nAbstract: We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p < 10^{-5}$ significance level. The same set-up improves \\textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine ",
    "path": "papers/23/04/2304.10004.json",
    "total_tokens": 956,
    "translated_title": "速通与机器学习中的幂律趋势",
    "translated_abstract": "我们发现，在速通世界纪录的改进中存在幂律模式。利用这一观察结果，我们回答了之前研究中的一个未解决问题：如何在预测某个时间跨度（如一个月）内的速通世界纪录时，提高基线预测不改进的精度？通过使用随机效应模型，在预测样本外的世界纪录改进的相对均方误差上，我们在$p<10^{-5}$的显著性水平上提高了基线预测的精度。尽管使用的数据点远少于先前表现最佳的指数移动平均预测模型，但相同的设置在$p=0.15$的显著性水平上提高了预测的准确率。我们将这种方法应用于机器学习基准并取得了超过基线的预测效果。最后，通过解释所得到的模型，我们认为1）ML基准远未饱和，2）机器学习中的突然大幅改进",
    "tldr": "该论文发现了速通世界纪录改进的幂律模式，并利用这一发现提高了预测速通世界纪录精度的方法，并在机器学习基准上得到了类似的结果。结果表明，ML基准远未饱和，而机器学习中的改进具有突然性。",
    "en_tdlr": "This paper discovers a power law pattern in speedrunning world record improvement, and proposes a method to improve the accuracy of predicting these improvements using a random effects model. The same approach is applied to machine learning benchmarks and achieves better forecasts than the baseline. The results suggest that ML benchmarks are far from saturation and sudden large improvements can occur in machine learning."
}