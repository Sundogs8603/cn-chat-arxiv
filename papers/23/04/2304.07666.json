{
    "title": "ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])",
    "abstract": "AI generated content (AIGC) presents considerable challenge to educators around the world. Instructors need to be able to detect such text generated by large language models, either with the naked eye or with the help of some tools. There is also growing need to understand the lexical, syntactic and stylistic features of AIGC. To address these challenges in English language teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing tasks. Machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts. We then hire English instructors to distinguish machine essays from human ones. Results show that when first exposed to machine-generated essays, the instructors only have an accuracy of 61% in detecting them. But the number rises to 67% after one round of",
    "link": "http://arxiv.org/abs/2304.07666",
    "context": "Title: ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])\nAbstract: AI generated content (AIGC) presents considerable challenge to educators around the world. Instructors need to be able to detect such text generated by large language models, either with the naked eye or with the help of some tools. There is also growing need to understand the lexical, syntactic and stylistic features of AIGC. To address these challenges in English language teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing tasks. Machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts. We then hire English instructors to distinguish machine essays from human ones. Results show that when first exposed to machine-generated essays, the instructors only have an accuracy of 61% in detecting them. But the number rises to 67% after one round of",
    "path": "papers/23/04/2304.07666.json",
    "total_tokens": 1024,
    "translated_title": "ArguGPT：评估、理解和识别由GPT模型生成的论证文章",
    "translated_abstract": "人工智能生成的内容（AIGC）对全球教育工作者提出了巨大的挑战。教师们需要能够用肉眼或工具检测出由大型语言模型生成的文本。需要更多地了解AIGC的词汇、句法和风格特征。为了解决这些英语教学方面的挑战，我们首先提出了ArguGPT，这是一个由7个GPT模型生成的4038篇有平衡的论证文章语料库，这些论证文章是在以下三个来源的论文提示下生成的：（1）课堂或家庭作业练习，（2）托福和（3）GRE写作任务。机器生成的文本与大致相等数量的人工编写的文章配对，这些文章的三个得分级别匹配论文提示。然后，我们雇用英语教师来区分机器论文和人工论文。结果表明，当教师们首次接触机器生成的论文时，他们仅能以61%的准确度检测出它们。但经过一轮训练后，这个数字提高到了67%。",
    "tldr": "该研究提出了ArguGPT，它是由7个GPT模型生成的论证文章语料库，旨在解决AI生成内容带来的挑战，研究结果表明教师首次接触机器生成的论文时只有61%的准确度，但经过一轮训练后提高到了67%。",
    "en_tdlr": "This study introduces ArguGPT, a corpus of 4,038 argumentative essays generated by 7 GPT models, aimed at addressing the challenges posed by AI generated content. Results show that English instructors only have an accuracy of 61% in detecting machine-generated essays when first exposed to them, but the number rises to 67% after one round of training."
}