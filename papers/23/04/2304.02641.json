{
    "title": "Self-Distillation for Gaussian Process Regression and Classification. (arXiv:2304.02641v1 [stat.ML])",
    "abstract": "We propose two approaches to extend the notion of knowledge distillation to Gaussian Process Regression (GPR) and Gaussian Process Classification (GPC); data-centric and distribution-centric. The data-centric approach resembles most current distillation techniques for machine learning, and refits a model on deterministic predictions from the teacher, while the distribution-centric approach, re-uses the full probabilistic posterior for the next iteration. By analyzing the properties of these approaches, we show that the data-centric approach for GPR closely relates to known results for self-distillation of kernel ridge regression and that the distribution-centric approach for GPR corresponds to ordinary GPR with a very particular choice of hyperparameters. Furthermore, we demonstrate that the distribution-centric approach for GPC approximately corresponds to data duplication and a particular scaling of the covariance and that the data-centric approach for GPC requires redefining the mod",
    "link": "http://arxiv.org/abs/2304.02641",
    "context": "Title: Self-Distillation for Gaussian Process Regression and Classification. (arXiv:2304.02641v1 [stat.ML])\nAbstract: We propose two approaches to extend the notion of knowledge distillation to Gaussian Process Regression (GPR) and Gaussian Process Classification (GPC); data-centric and distribution-centric. The data-centric approach resembles most current distillation techniques for machine learning, and refits a model on deterministic predictions from the teacher, while the distribution-centric approach, re-uses the full probabilistic posterior for the next iteration. By analyzing the properties of these approaches, we show that the data-centric approach for GPR closely relates to known results for self-distillation of kernel ridge regression and that the distribution-centric approach for GPR corresponds to ordinary GPR with a very particular choice of hyperparameters. Furthermore, we demonstrate that the distribution-centric approach for GPC approximately corresponds to data duplication and a particular scaling of the covariance and that the data-centric approach for GPC requires redefining the mod",
    "path": "papers/23/04/2304.02641.json",
    "total_tokens": 869,
    "translated_title": "高斯过程回归和分类的自我蒸馏",
    "translated_abstract": "我们提出了两种方法来将知识蒸馏的概念扩展到高斯过程回归（GPR）和高斯过程分类（GPC）中；数据中心方法和分布中心方法。数据中心方法最像目前机器学习的大多数蒸馏技术，它在教师的确定性预测上重新适合一个模型，而分布中心方法则重新使用完整概率后验进行下一次迭代。通过分析这些方法的特性，我们表明GPR的数据中心方法与已知的内核岭回归自我蒸馏结果密切相关，而GPR的分布中心方法与具有特定超参数选择的普通GPR相对应。此外，我们演示了GPC的分布中心方法近似对应于数据复制和协方差的特定缩放，而GPC的数据中心方法需要重新定义模型。",
    "tldr": "本文针对高斯过程回归和分类提出了数据中心方法和分布中心方法，分析后发现其与内核岭回归自我蒸馏和普通GPR对应。其中GPC的分布中心方法近似对应于数据复制和协方差的特定缩放。"
}