{
    "title": "Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])",
    "abstract": "AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform monolingual sentiment classification (sub-task A) for 12 African languages, multilingual sentiment classification (sub-task B), and zero-shot sentiment classification (task C). For sub-task A, we conducted experiments using classical machine learning classifiers, Afro-centric language models, and language-specific models. For task B, we fine-tuned multilingual pre-trained language models that support many of the languages in the task. For task C, we used we make use of a parameter-efficient Adapter approach that leverages monolingual texts in the target language for effective zero-shot transfer. Our findings suggest that using pre-trained Afro-centric language models improves performance for low-resource African languages. We also ran experiments using adapters for zero-shot tasks, and the results suggest that we can obtain promising results by using adapters with a limited amount of resources.",
    "link": "http://arxiv.org/abs/2304.06459",
    "context": "Title: Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])\nAbstract: AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform monolingual sentiment classification (sub-task A) for 12 African languages, multilingual sentiment classification (sub-task B), and zero-shot sentiment classification (task C). For sub-task A, we conducted experiments using classical machine learning classifiers, Afro-centric language models, and language-specific models. For task B, we fine-tuned multilingual pre-trained language models that support many of the languages in the task. For task C, we used we make use of a parameter-efficient Adapter approach that leverages monolingual texts in the target language for effective zero-shot transfer. Our findings suggest that using pre-trained Afro-centric language models improves performance for low-resource African languages. We also ran experiments using adapters for zero-shot tasks, and the results suggest that we can obtain promising results by using adapters with a limited amount of resources.",
    "path": "papers/23/04/2304.06459.json",
    "total_tokens": 904,
    "translated_title": "Masakhane-Afrisenti在SemEval-2023任务12中的应用：使用非洲中心语言模型和适配器进行低资源非洲语言的情感分析",
    "translated_abstract": "AfriSenti-SemEval共享任务12旨在为12种非洲语言执行单语情感分类（子任务A）、多语言情感分类（子任务B）和零样本情感分类（任务C）。对于子任务A，我们使用传统的机器学习分类器、非洲中心语言模型和特定语言模型进行了实验。对于任务B，我们微调了支持任务中多种语言的多语言预训练语言模型。对于任务C，我们使用参数高效的适配器方法，利用目标语言的单语文本实现有效的零样本迁移。我们的发现表明，使用预训练的非洲中心语言模型可以改善低资源非洲语言的性能。我们还使用适配器进行零样本任务的实验，结果表明，使用有限的资源可以获得有前途的结果。",
    "tldr": "该论文介绍了在SemEval-2023任务12中使用Afro-centric语言模型和适配器进行非洲低资源语言的情感分析。使用预训练的Afro-centric语言模型可以提高性能。使用适配器方法可以实现对于有限资源语言的零样本迁移。",
    "en_tdlr": "This paper introduces the use of Afro-centric language models and adapters for sentiment analysis of low-resource African languages in SemEval-2023 Task 12. Pre-trained Afro-centric language models improve performance, and adapters enable zero-shot transfer for languages with limited resources."
}