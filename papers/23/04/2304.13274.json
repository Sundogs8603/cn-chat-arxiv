{
    "title": "Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference. (arXiv:2304.13274v1 [cs.LG])",
    "abstract": "Large number of ReLU and MAC operations of Deep neural networks make them ill-suited for latency and compute-efficient private inference. In this paper, we present a model optimization method that allows a model to learn to be shallow. In particular, we leverage the ReLU sensitivity of a convolutional block to remove a ReLU layer and merge its succeeding and preceding convolution layers to a shallow block. Unlike existing ReLU reduction methods, our joint reduction method can yield models with improved reduction of both ReLUs and linear operations by up to 1.73x and 1.47x, respectively, evaluated with ResNet18 on CIFAR-100 without any significant accuracy-drop.",
    "link": "http://arxiv.org/abs/2304.13274",
    "context": "Title: Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference. (arXiv:2304.13274v1 [cs.LG])\nAbstract: Large number of ReLU and MAC operations of Deep neural networks make them ill-suited for latency and compute-efficient private inference. In this paper, we present a model optimization method that allows a model to learn to be shallow. In particular, we leverage the ReLU sensitivity of a convolutional block to remove a ReLU layer and merge its succeeding and preceding convolution layers to a shallow block. Unlike existing ReLU reduction methods, our joint reduction method can yield models with improved reduction of both ReLUs and linear operations by up to 1.73x and 1.47x, respectively, evaluated with ResNet18 on CIFAR-100 without any significant accuracy-drop.",
    "path": "papers/23/04/2304.13274.json",
    "total_tokens": 838,
    "translated_title": "让模型变得更浅：联合学习降低深度和非线性以实现延迟高效的私人推断",
    "translated_abstract": "深度神经网络的大量ReLU和MAC操作使它们不适合进行延迟和计算高效的私人推断。本文提出了一种模型优化方法，允许模型学习变得浅。具体地，我们利用卷积块的ReLU灵敏度来移除ReLU层，并将其前后的卷积层合并为一个浅层。与现有的ReLU降低方法不同，我们的联合降低方法可以获得更好的ReLU和线性操作降低，分别提高了1.73倍和1.47倍，使用CIFAR-100上的ResNet18进行评估时，几乎没有显著的准确度下降。",
    "tldr": "本文提出了一种联合优化方法，可以学习深度神经网络变得更浅，具体地，利用卷积块的ReLU灵敏度来合并卷积层和去除ReLU层，可以在减少ReLU和线性操作的情况下提高模型计算效率，同时减小模型模型延迟和大小，准确率不显著下降。",
    "en_tdlr": "The paper proposes a joint optimization method that can make deep neural networks become shallow by removing the ReLU layer and merging the convolution layers based on the ReLU sensitivity of a convolutional block, thereby improving the computational efficiency without significant accuracy drop on CIFAR-100. The method can also reduce model latency and size."
}