{
    "title": "When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?. (arXiv:2304.04724v1 [stat.CO])",
    "abstract": "We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with the leapfrog integrator to sample from a distribution on $\\mathbb{R}^d$ whose log-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies isoperimetry. We bound the gradient complexity to reach $\\epsilon$ error in total variation distance from a warm start by $\\tilde O(d^{1/4}\\text{polylog}(1/\\epsilon))$ and demonstrate the benefit of choosing the number of leapfrog steps to be larger than 1. To surpass previous analysis on Metropolis-adjusted Langevin algorithm (MALA) that has $\\tilde{O}(d^{1/2}\\text{polylog}(1/\\epsilon))$ dimension dependency in Wu et al. (2022), we reveal a key feature in our proof that the joint distribution of the location and velocity variables of the discretization of the continuous HMC dynamics stays approximately invariant. This key feature, when shown via induction over the number of leapfrog steps, enables us to obtain estimates on moments of various quantities tha",
    "link": "http://arxiv.org/abs/2304.04724",
    "context": "Title: When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?. (arXiv:2304.04724v1 [stat.CO])\nAbstract: We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with the leapfrog integrator to sample from a distribution on $\\mathbb{R}^d$ whose log-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies isoperimetry. We bound the gradient complexity to reach $\\epsilon$ error in total variation distance from a warm start by $\\tilde O(d^{1/4}\\text{polylog}(1/\\epsilon))$ and demonstrate the benefit of choosing the number of leapfrog steps to be larger than 1. To surpass previous analysis on Metropolis-adjusted Langevin algorithm (MALA) that has $\\tilde{O}(d^{1/2}\\text{polylog}(1/\\epsilon))$ dimension dependency in Wu et al. (2022), we reveal a key feature in our proof that the joint distribution of the location and velocity variables of the discretization of the continuous HMC dynamics stays approximately invariant. This key feature, when shown via induction over the number of leapfrog steps, enables us to obtain estimates on moments of various quantities tha",
    "path": "papers/23/04/2304.04724.json",
    "total_tokens": 1089,
    "translated_title": "Metropolized Hamiltonian Monte Carlo何时能证明优于Metropolis-adjusted Langevin算法？",
    "translated_abstract": "本文分析了Metropolized Hamiltonian Monte Carlo (HMC)的混合时间，使用leapfrog积分器从$\\mathbb{R}^d$分布中采样，该分布的对数密度平滑，具有Frobenius范数上的李普希茨黑塞，并满足等周性。我们将梯度复杂度限制为从一个暖启动达到$\\epsilon$误差的总变异距离所需的$\\tilde O(d^{1/4}\\text{polylog}(1/\\epsilon))$，并展示了选择比1更大的leapfrog步数的好处。为了超越Wu等人（2022）对Metropolis-adjusted Langevin algorithm (MALA)的分析，其在维度依赖性上是$\\tilde{O}(d^{1/2}\\text{polylog}(1/\\epsilon))$，我们揭示了证明中的一个关键特征：连续HMC动态的位置和速度变量的离散化的联合分布近似不变。当通过leapfrog步数的归纳来展示这个关键特征时，我们能够获得各种量的矩的估计，这些量在限制Metropolized HMC的混合时间时是至关重要的，而在MALA中已知的类似结果是错误的。我们的结果表明，在采样高维平滑目标分布时，使用具有大量leapfrog步骤的Metropolized HMC可能比使用MALA更有效。",
    "tldr": "本文表明，从高维平滑目标分布采样时，Metropolized Hamiltonian Monte Carlo (HMC)比Metropolis-adjusted Langevin算法（MALA）更有效。",
    "en_tdlr": "This paper shows that when sampling from high-dimensional smooth target distributions, Metropolized Hamiltonian Monte Carlo (HMC) is more efficient than Metropolis-adjusted Langevin algorithm (MALA)."
}