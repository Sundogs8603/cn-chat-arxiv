{
    "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. (arXiv:2304.03153v1 [cs.IR])",
    "abstract": "Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks, demonstrating their capabilities for inference without training examples. Despite their success, no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences. To address this gap, we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically, the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that captur",
    "link": "http://arxiv.org/abs/2304.03153",
    "context": "Title: Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. (arXiv:2304.03153v1 [cs.IR])\nAbstract: Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks, demonstrating their capabilities for inference without training examples. Despite their success, no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences. To address this gap, we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically, the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that captur",
    "path": "papers/23/04/2304.03153.json",
    "total_tokens": 727,
    "translated_title": "利用大型预训练语言模型进行零样本下一个项目推荐",
    "translated_abstract": "大型语言模型（LLM）在各种自然语言处理（NLP）任务中取得了令人印象深刻的零样本表现，展示了它们在没有训练示例的情况下进行推理的能力。尽管取得了成功，但尚未有研究探索LLMs在零样本情况下执行下一个项目推荐的潜力。作者们确定了必须解决的两个主要问题，以使LLMs有效地充当推荐者。",
    "tldr": "本研究通过提出零样本下一个项目推荐策略，解决了使用大型预训练语言模型进行下一个项目推荐中遇到的挑战。",
    "en_tdlr": "This study proposes a strategy called Zero-Shot Next-Item Recommendation prompting to address the challenges in using large pre-trained language models for next-item recommendation in the zero-shot setting."
}