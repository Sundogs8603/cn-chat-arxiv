{
    "title": "Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])",
    "abstract": "Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language (NL) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both Programming Language (PL) and Natural Language (NL), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre",
    "link": "http://arxiv.org/abs/2304.07840",
    "context": "Title: Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])\nAbstract: Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language (NL) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both Programming Language (PL) and Natural Language (NL), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre",
    "path": "papers/23/04/2304.07840.json",
    "total_tokens": 992,
    "translated_title": "基于代码审查的自动程序修复: 预训练Transformer模型表现如何？",
    "translated_abstract": "序列到序列模型已被用于将错误的程序转换为正确的程序，当它们被训练使用足够大的数据集时。一些最近的研究也证明了代码审查(关于代码中建议性更改的自然语言指令)可以进一步改善程序修复。使用自然语言和计算机程序语料库训练的大型语言模型具有包含两种语言的固有知识的能力。在本研究中，我们探究了是否可以利用代码和自然语言的固有知识来提高自动程序修复的效果。我们将PLBART和CodeT5这两个最先进的语言模型，应用于两个基于自然语言的程序修复数据集，并发现使用包含代码审查和随后代码更改的数据集微调的预训练语言模型，明显优于之前的模型。我们观察到预训练模型具有更好的泛化能力，可以从之前未见过的数据中学习并得到更好的结果，相比之前的方法，我们的方法具有更高的修复能力，相对于之前的SOTA技术，CODET5取得了3%左右的提升",
    "tldr": "本研究探究使用大型预训练语言模型，结合自然语言和编程语言，来改善自动程序修复的效果，发现预训练模型通过使用代码审查和代码更改的数据集微调，能显著优于传统方法，并具有更好的泛化能力。"
}