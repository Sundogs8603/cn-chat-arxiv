{
    "title": "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs. (arXiv:2304.10668v1 [cs.LG])",
    "abstract": "How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments i",
    "link": "http://arxiv.org/abs/2304.10668",
    "context": "Title: Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs. (arXiv:2304.10668v1 [cs.LG])\nAbstract: How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments i",
    "path": "papers/23/04/2304.10668.json",
    "total_tokens": 928,
    "translated_title": "训练你自己的GNN教师：面向文本图的图感知蒸馏",
    "translated_abstract": "如何在文本图上实现有效的节点表示学习？将语言模型（LM）与图神经网络（GNN）相结合，以对图形的文本信息进行编码的GNN在许多节点分类任务中取得了最先进的性能。然而，由于可扩展性问题，将GNN与LM相结合进行实际部署并没有得到广泛探索。本文通过开发一种名为Graph-Aware Distillation（GRAD）的框架来解决这一难题，以在没有图形的情况下实现快速推理的LM将图形结构编码。与传统的知识蒸馏不同，GRAD通过共享LM在图的节点上同时优化GNN教师和无图学生。这鼓励无图学生利用由GNN教师编码的图形信息，同时使GNN教师更好地利用未标记节点的文本信息。因此，教师和学生模型相互学习以提高其整体性能。",
    "tldr": "本研究提出一种名为Graph-Aware Distillation（GRAD）的框架，以在没有图形的情况下实现快速推理的LM将图形结构编码，此框架可以同时优化GNN教师和无图学生，互相学习以提高性能。",
    "en_tdlr": "This paper proposes a framework called Graph-Aware Distillation (GRAD) to encode graph structures into a language model (LM) for fast inference without graphs. GRAD jointly optimizes a GNN teacher and a graph-free student via a shared LM, allowing them to learn from each other for improved performance."
}