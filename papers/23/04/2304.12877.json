{
    "title": "Proximal Curriculum for Reinforcement Learning Agents. (arXiv:2304.12877v1 [cs.LG])",
    "abstract": "We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.",
    "link": "http://arxiv.org/abs/2304.12877",
    "context": "Title: Proximal Curriculum for Reinforcement Learning Agents. (arXiv:2304.12877v1 [cs.LG])\nAbstract: We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.",
    "path": "papers/23/04/2304.12877.json",
    "total_tokens": 909,
    "translated_title": "强化学习智能体的近端课程设计",
    "translated_abstract": "我们考虑针对上下文多任务设置中的强化学习智能体的课程设计问题。自动课程设计的现有技术通常需要特定领域中的超参数调整或具有有限的理论基础。为了解决这些限制，我们设计了课程策略 ProCuRL，它受到了教育概念“近端发展区”的启发。ProCuRL 触发了这样一个直觉，即当选择既不太难也不太容易的任务时，学习进展是最大化的。我们通过分析两个简单的学习设置来数学推导 ProCuRL。我们还提出了一种实用的 ProCuRL 变体，可以通过最小的超参数调整直接集成到深度强化学习框架中。在各种域上的实验结果表明，在加速深度强化学习智能体的训练过程方面，我们的课程策略比现有技术水平的基线更加有效。",
    "tldr": "本文基于近端发展区概念，提出了 ProCuRL 课程策略，用于设计深度强化学习智能体的课程，以加速训练过程，并在各种域上实验表明其优越性。",
    "en_tdlr": "This paper proposes a curriculum strategy, ProCuRL, based on the concept of Zone of Proximal Development, which can design courses for reinforcement learning agents in contextual multi-task settings, with the aim of accelerating the training process. Mathematical derivation and practical variant of ProCuRL are also presented in the paper, and experimental results show the superiority of the proposed strategy over state-of-the-art baselines in various domains."
}