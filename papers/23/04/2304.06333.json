{
    "title": "Priors for symbolic regression. (arXiv:2304.06333v1 [cs.LG])",
    "abstract": "When choosing between competing symbolic models for a data set, a human will naturally prefer the \"simpler\" expression or the one which more closely resembles equations previously seen in a similar context. This suggests a non-uniform prior on functions, which is, however, rarely considered within a symbolic regression (SR) framework. In this paper we develop methods to incorporate detailed prior information on both functions and their parameters into SR. Our prior on the structure of a function is based on a $n$-gram language model, which is sensitive to the arrangement of operators relative to one another in addition to the frequency of occurrence of each operator. We also develop a formalism based on the Fractional Bayes Factor to treat numerical parameter priors in such a way that models may be fairly compared though the Bayesian evidence, and explicitly compare Bayesian, Minimum Description Length and heuristic methods for model selection. We demonstrate the performance of our pri",
    "link": "http://arxiv.org/abs/2304.06333",
    "context": "Title: Priors for symbolic regression. (arXiv:2304.06333v1 [cs.LG])\nAbstract: When choosing between competing symbolic models for a data set, a human will naturally prefer the \"simpler\" expression or the one which more closely resembles equations previously seen in a similar context. This suggests a non-uniform prior on functions, which is, however, rarely considered within a symbolic regression (SR) framework. In this paper we develop methods to incorporate detailed prior information on both functions and their parameters into SR. Our prior on the structure of a function is based on a $n$-gram language model, which is sensitive to the arrangement of operators relative to one another in addition to the frequency of occurrence of each operator. We also develop a formalism based on the Fractional Bayes Factor to treat numerical parameter priors in such a way that models may be fairly compared though the Bayesian evidence, and explicitly compare Bayesian, Minimum Description Length and heuristic methods for model selection. We demonstrate the performance of our pri",
    "path": "papers/23/04/2304.06333.json",
    "total_tokens": 878,
    "translated_title": "符号回归的先验知识",
    "translated_abstract": "在为数据集选择符号模型时，人们自然倾向于选择“简单”的表达式或更接近之前在类似情况下看到的方程式。这表明函数应该具有非均匀先验知识，然而，在符号回归（SR）框架内很少考虑。在本文中，我们开发了一种方法，将有关函数和参数的详细先验信息纳入SR中。我们对函数结构的先验是基于n-gram语言模型的，该模型对各个运算符的排列方式以及每个运算符的出现频率都非常敏感。我们还开发了一种基于分式贝叶斯因子的形式体系，以处理数值参数的先验知识，使得可以通过贝叶斯证据公平比较模型，同时明确比较了贝叶斯、最小描述长度和启发式方法用于模型选择。我们通过对基准数据集进行实验以及在材料科学中的应用演示了我们的先验的性能。",
    "tldr": "本文提出了一种在符号回归（SR）框架内将有关函数和参数的详细先验信息纳入的方法，并且演示了该先验在基准数据集和材料科学应用中的性能。",
    "en_tdlr": "This paper proposes a method to incorporate detailed prior information on both functions and their parameters into symbolic regression (SR) framework, and the performance of this prior is demonstrated through experiments on benchmark data sets and an application in materials science."
}