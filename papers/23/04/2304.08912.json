{
    "title": "Generalized Weak Supervision for Neural Information Retrieval. (arXiv:2304.08912v1 [cs.IR])",
    "abstract": "Neural ranking models (NRMs) have demonstrated effective performance in several information retrieval (IR) tasks. However, training NRMs often requires large-scale training data, which is difficult and expensive to obtain. To address this issue, one can train NRMs via weak supervision, where a large dataset is automatically generated using an existing ranking model (called the weak labeler) for training NRMs. Weakly supervised NRMs can generalize from the observed data and significantly outperform the weak labeler. This paper generalizes this idea through an iterative re-labeling process, demonstrating that weakly supervised models can iteratively play the role of weak labeler and significantly improve ranking performance without using manually labeled data. The proposed Generalized Weak Supervision (GWS) solution is generic and orthogonal to the ranking model architecture. This paper offers four implementations of GWS: self-labeling, cross-labeling, joint cross- and self-labeling, and",
    "link": "http://arxiv.org/abs/2304.08912",
    "context": "Title: Generalized Weak Supervision for Neural Information Retrieval. (arXiv:2304.08912v1 [cs.IR])\nAbstract: Neural ranking models (NRMs) have demonstrated effective performance in several information retrieval (IR) tasks. However, training NRMs often requires large-scale training data, which is difficult and expensive to obtain. To address this issue, one can train NRMs via weak supervision, where a large dataset is automatically generated using an existing ranking model (called the weak labeler) for training NRMs. Weakly supervised NRMs can generalize from the observed data and significantly outperform the weak labeler. This paper generalizes this idea through an iterative re-labeling process, demonstrating that weakly supervised models can iteratively play the role of weak labeler and significantly improve ranking performance without using manually labeled data. The proposed Generalized Weak Supervision (GWS) solution is generic and orthogonal to the ranking model architecture. This paper offers four implementations of GWS: self-labeling, cross-labeling, joint cross- and self-labeling, and",
    "path": "papers/23/04/2304.08912.json",
    "total_tokens": 1000,
    "translated_title": "神经信息检索的广义弱监督学习方法",
    "translated_abstract": "神经排序模型(NRM)在多个信息检索(IR)任务中表现出有效的性能。然而，训练NRM通常需要大规模的训练数据，这很难且昂贵。为了解决这个问题，可以通过弱监督学习的方式来训练NRM，其中使用现有的排序模型(称为弱标注器)自动产生了一个大规模的训练数据集来训练NRM。弱监督的NRM可以从观察到的数据中推广，并显著优于弱标注器。本文通过迭代的重新标注过程对这个想法进行了推广，证明了弱监督模型可以迭代地扮演弱标注器的角色，并且不需要使用已手动标注的数据就可以显著提高排序性能。所提出的广义弱监督学习(GWS)解决方案是通用的，并且与排序模型体系结构相互独立。本文提供了四个GWS的实现：自我标注、跨标注、联合跨标注和自我标注、自举法。在几个基准数据集上的实验表明，GWS优于现有的最先进的方法，并与全监督方法相当。",
    "tldr": "本文提出了一种名为广义弱监督学习(GWS)的解决方案，它能够迭代地使用现有的排序模型(弱标注器)产生大量的训练数据，无需手动标注就能显著提高排序性能。",
    "en_tdlr": "This paper proposes a Generalized Weak Supervision (GWS) solution for neural information retrieval, which iteratively uses an existing ranking model (weak labeler) to generate a large dataset for training, without the need for manual labeling, thus significantly improving ranking performance."
}