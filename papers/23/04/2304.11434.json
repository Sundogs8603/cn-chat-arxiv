{
    "title": "L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])",
    "abstract": "The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M",
    "link": "http://arxiv.org/abs/2304.11434",
    "context": "Title: L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])\nAbstract: The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M",
    "path": "papers/23/04/2304.11434.json",
    "total_tokens": 988,
    "translated_title": "L3Cube-IndicSBERT: 使用多语言BERT学习跨语言句子表示的简单方法",
    "translated_abstract": "多语言句子BERT (SBERT) 模型将不同语言映射到共同的表示空间，对于跨语言相似性和挖掘任务非常有用。我们提出了一种简单而有效的方法，使用合成语料库将普通的多语言BERT模型转换成多语言句子BERT模型。我们简单地聚合低资源目标语言的翻译 NLI 或 STS 数据集，并对普通的多语言BERT模型进行类似SBERT的微调。我们表明，多语言BERT模型具有内在的跨语言学习能力，这种简单的微调方法没有显式的跨语言训练，却产生了非常出色的跨语言表示效果。我们展示了我们的方法在10种主要的印欧语言中的有效性，并展示了我们的方法适用于非印欧语言德语和法语。利用这种方法，我们进一步提出了L3Cube-IndicSBERT，这是第一个专门针对印度语言印地语和马来语的多语言句子表示模型。",
    "tldr": "该论文提出了一种简单但有效的方法，使用合成语料库将BERT模型转换成SBERT模型。该方法在10种主要的印欧语言中具有很好的效果，并展示了其在非印欧语言上的应用性。",
    "en_tdlr": "The paper proposes a simple but effective method to convert a vanilla multilingual BERT model into a multilingual sentence BERT model using synthetic corpus. This approach yields exceptional cross-lingual representation performance without explicit cross-lingual training and shows promising results in 10 major Indic languages and also applies to non-Indic languages such as German and French. The authors further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, Malayalam and Tamil."
}