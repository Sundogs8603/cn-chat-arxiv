{
    "title": "A Cubic-regularized Policy Newton Algorithm for Reinforcement Learning. (arXiv:2304.10951v1 [cs.LG])",
    "abstract": "We consider the problem of control in the setting of reinforcement learning (RL), where model information is not available. Policy gradient algorithms are a popular solution approach for this problem and are usually shown to converge to a stationary point of the value function. In this paper, we propose two policy Newton algorithms that incorporate cubic regularization. Both algorithms employ the likelihood ratio method to form estimates of the gradient and Hessian of the value function using sample trajectories. The first algorithm requires an exact solution of the cubic regularized problem in each iteration, while the second algorithm employs an efficient gradient descent-based approximation to the cubic regularized problem. We establish convergence of our proposed algorithms to a second-order stationary point (SOSP) of the value function, which results in the avoidance of traps in the form of saddle points. In particular, the sample complexity of our algorithms to find an $\\epsilon$",
    "link": "http://arxiv.org/abs/2304.10951",
    "context": "Title: A Cubic-regularized Policy Newton Algorithm for Reinforcement Learning. (arXiv:2304.10951v1 [cs.LG])\nAbstract: We consider the problem of control in the setting of reinforcement learning (RL), where model information is not available. Policy gradient algorithms are a popular solution approach for this problem and are usually shown to converge to a stationary point of the value function. In this paper, we propose two policy Newton algorithms that incorporate cubic regularization. Both algorithms employ the likelihood ratio method to form estimates of the gradient and Hessian of the value function using sample trajectories. The first algorithm requires an exact solution of the cubic regularized problem in each iteration, while the second algorithm employs an efficient gradient descent-based approximation to the cubic regularized problem. We establish convergence of our proposed algorithms to a second-order stationary point (SOSP) of the value function, which results in the avoidance of traps in the form of saddle points. In particular, the sample complexity of our algorithms to find an $\\epsilon$",
    "path": "papers/23/04/2304.10951.json",
    "total_tokens": 839,
    "translated_title": "强化学习中的三次正则化策略牛顿算法",
    "translated_abstract": "本文研究了在没有模型信息的强化学习（RL）环境下的控制问题。针对这个问题，我们提出了两种策略牛顿算法，其中包含了三次正则化。这两种算法采用似然比方法使用样本轨迹形成价值函数梯度和黑塞矩阵的估计。第一种算法在每次迭代中需要三次正则化问题的精确解，而第二种算法则使用了一种高效的梯度下降近似方法。我们证明了所提出的算法收敛到价值函数的二阶稳定点（SOSP），从而避免了类型为鞍点的陷阱。特别地，我们的算法的样本复杂度为$\\epsilon$",
    "tldr": "本文提出了两种三次正则化策略牛顿算法，其使用似然比方法形成价值函数梯度和黑塞矩阵的估计。我们证明了算法收敛到价值函数的二阶稳定点，从而避免了类型为鞍点的陷阱。",
    "en_tdlr": "This paper proposes two policy Newton algorithms with cubic regularization for reinforcement learning. They use the likelihood ratio method to estimate the gradient and Hessian of the value function with sample trajectories. The algorithms converge to a second-order stationary point and avoid traps in the form of saddle points."
}