{
    "title": "Chain of Thought Prompt Tuning in Vision Language Models. (arXiv:2304.07919v2 [cs.CV] UPDATED)",
    "abstract": "Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and h",
    "link": "http://arxiv.org/abs/2304.07919",
    "context": "Title: Chain of Thought Prompt Tuning in Vision Language Models. (arXiv:2304.07919v2 [cs.CV] UPDATED)\nAbstract: Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and h",
    "path": "papers/23/04/2304.07919.json",
    "total_tokens": 945,
    "translated_title": "链式思维提示调整视觉语言模型",
    "translated_abstract": "语言-图像预训练通过使用自然语言提示对视觉模型进行提示，在零样本和少样本下游任务中展示了有希望的结果。但是，大多数最近的研究只使用单个提示进行调整，忽略了人类在处理来自陌生领域的图像时在复杂任务设置中进行的内在逐步认知推理过程，例如，链式思维是一种简单而有效的近似人类推理过程的方法，已被证明在自然语言处理任务中非常有用。基于这种认知直觉，我们认为进行有效的推理也是视觉任务中的一个重要问题，链式思维可能是解决这个问题的一种方法。在这项工作中，我们提出了一种用于视觉语言建模的全新链式思维提示调整方法。广泛的实验表明，我们的方法不仅在图像分类任务中更好地进行泛化，并且具有比单个提示调整更高的可迁移性，还通过提供逐步推理过程来提高了视觉模型的可解释性。",
    "tldr": "本文提出了一种链式思维提示调整的新方法，以有效地解决视觉任务中的推理问题，在图像分类任务中表现出更好的泛化性，更高的可迁移性和更高的准确性，并通过提供逐步推理过程来提高了视觉模型的可解释性。",
    "en_tdlr": "This paper proposes a novel approach called chain of thought prompt tuning for vision-language modeling, which effectively addresses reasoning problems in visual tasks and shows better generalization, transferability, and interpretability in image classification tasks by providing a step-by-step reasoning process."
}