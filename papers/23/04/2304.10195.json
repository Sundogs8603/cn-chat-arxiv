{
    "title": "CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval. (arXiv:2304.10195v1 [cs.CL])",
    "abstract": "Passage retrieval aims to retrieve relevant passages from large collections of the open-domain corpus. Contextual Masked Auto-Encoding has been proven effective in representation bottleneck pre-training of a monolithic dual-encoder for passage retrieval. Siamese or fully separated dual-encoders are often adopted as basic retrieval architecture in the pre-training and fine-tuning stages for encoding queries and passages into their latent embedding spaces. However, simply sharing or separating the parameters of the dual-encoder results in an imbalanced discrimination of the embedding spaces. In this work, we propose to pre-train Contextual Masked Auto-Encoder with Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate textual-specific experts for individually encoding the distinct properties of queries and passages. Meanwhile, a shared self-attention layer is still kept for unified attention modeling. Results on large-scale passage retrieval benchmarks show steady improvemen",
    "link": "http://arxiv.org/abs/2304.10195",
    "context": "Title: CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval. (arXiv:2304.10195v1 [cs.CL])\nAbstract: Passage retrieval aims to retrieve relevant passages from large collections of the open-domain corpus. Contextual Masked Auto-Encoding has been proven effective in representation bottleneck pre-training of a monolithic dual-encoder for passage retrieval. Siamese or fully separated dual-encoders are often adopted as basic retrieval architecture in the pre-training and fine-tuning stages for encoding queries and passages into their latent embedding spaces. However, simply sharing or separating the parameters of the dual-encoder results in an imbalanced discrimination of the embedding spaces. In this work, we propose to pre-train Contextual Masked Auto-Encoder with Mixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate textual-specific experts for individually encoding the distinct properties of queries and passages. Meanwhile, a shared self-attention layer is still kept for unified attention modeling. Results on large-scale passage retrieval benchmarks show steady improvemen",
    "path": "papers/23/04/2304.10195.json",
    "total_tokens": 886,
    "translated_title": "CoT-MoTE：探索基于文本专家混合的上下文掩码自编码器预训练在段落检索中的应用",
    "translated_abstract": "段落检索旨在从大规模开放式语料库中检索相关段落。上下文掩码自编码器在单体双编码器的表示瓶颈预训练中证明有效，并常常被采用为基本的检索架构，在预训练和微调阶段中将查询和段落编码为它们的潜在嵌入空间。然而，简单地共享或分离双编码器的参数会导致嵌入空间的不平衡判别。本文中，我们提出了一种预先训练具有文本专家混合的上下文掩码自编码器（CoT-MoTE）。具体来说，我们为查询和段落的不同属性分别编码文本特定的专家。同时，仍保留一个共享的自我注意层，用于统一的注意建模。对大规模段落检索基准测试的结果显示稳定的改进。",
    "tldr": "本文提出了一种在段落检索中采用基于文本专家混合的上下文掩码自编码器预训练的新方法，可以有效改进嵌入空间的判别效果。",
    "en_tdlr": "This paper proposes a new approach that uses a Contextual Masked Auto-Encoder pre-trained with a mixture of textual experts for passage retrieval, which can effectively improve the discriminative effect of the embedding space."
}