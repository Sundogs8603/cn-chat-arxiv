{
    "title": "Effective Neural Network $L_0$ Regularization With BinMask. (arXiv:2304.11237v1 [cs.LG])",
    "abstract": "$L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.",
    "link": "http://arxiv.org/abs/2304.11237",
    "context": "Title: Effective Neural Network $L_0$ Regularization With BinMask. (arXiv:2304.11237v1 [cs.LG])\nAbstract: $L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.",
    "path": "papers/23/04/2304.11237.json",
    "total_tokens": 1021,
    "translated_title": "采用BinMask的高效神经网络$L_0$正则化",
    "translated_abstract": "$L_0$正则化是神经网络的一个基本问题。除了为更好的泛化性而正则化模型外，$L_0$正则化还适用于选择输入特征和训练稀疏神经网络。相关领域中有大量的研究，其中一些方法相当复杂。本文显示，一种简单的形式化方法，即使用确定性二进制掩码将权重乘以并使用标识直通估计器进行反向传播的BinMask，是一种有效的$L_0$正则化器。我们在三个任务上评估了BinMask: 特征选择，网络稀疏化和模型正则化。尽管它很简单，但相比为每个任务设计的方法，BinMask在所有基准测试中都能够实现有竞争力的表现而无需特定的调整。我们的结果表明，解耦权重与掩码优化（这在以前的工作中被广泛采用）是实现有效$L_0$正则化的关键组成部分。",
    "tldr": "本文提出了一种简单而高效的神经网络$L_0$正则化方法——BinMask，该方法采用确定性二进制掩码乘以权重并使用标识直通估计器进行反向传播。实验证明，与其他方法相比，Binmask不需要针对不同任务进行专门调整即可在特征选择、网络稀疏化和模型正则化等任务中实现有竞争力的表现。解耦权重与掩码优化是实现有效$L_0$正则化的关键组成部分。",
    "en_tdlr": "This paper proposes a simple and efficient neural network $L_0$ regularization method, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation. Experimental results show that BinMask achieves competitive performance on feature selection, network sparsification, and model regularization tasks without task-specific tuning compared to methods designed for each task. Decoupling weights from mask optimization is a key component for effective $L_0$ regularization."
}