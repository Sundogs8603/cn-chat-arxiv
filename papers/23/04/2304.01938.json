{
    "title": "Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])",
    "abstract": "We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 an",
    "link": "http://arxiv.org/abs/2304.01938",
    "context": "Title: Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics. (arXiv:2304.01938v1 [physics.med-ph])\nAbstract: We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 an",
    "path": "papers/23/04/2304.01938.json",
    "total_tokens": 878,
    "translated_title": "在高度专业化的放射肿瘤物理学领域中评估大型语言模型",
    "translated_abstract": "本文提出对大型语言模型（LLMs）在回答放射肿瘤物理问题方面进行评估，这是第一项针对该领域的研究。我们开发了一个由100个放射肿瘤物理学问题组成的考试，并使用四种LLMs和医学物理学的专家和非专家进行评估。ChatGPT（GPT-4）平均而言表现最好，超过了所有其他LLMs以及医疗物理学家的表现。同时，本文也提出了一些LLMs模型改进和集成策略的潜力。",
    "tldr": "本研究评估了四种大型语言模型在回答放射肿瘤物理问题方面的表现，结果发现ChatGPT（GPT-4）的表现最好，同时提出了LLMs模型改进和集成策略的潜力。",
    "en_tdlr": "This study evaluated the performance of four large language models in answering radiation oncology physics questions, and found that ChatGPT (GPT-4) performed the best. The potential for future model improvements and ensemble strategies was also discussed."
}