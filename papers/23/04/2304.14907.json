{
    "title": "A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])",
    "abstract": "A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.",
    "link": "http://arxiv.org/abs/2304.14907",
    "context": "Title: A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])\nAbstract: A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.",
    "path": "papers/23/04/2304.14907.json",
    "total_tokens": 869,
    "translated_title": "用基于随机梯度下降的内点算法求解光滑有界约束优化问题",
    "translated_abstract": "本文提出并分析了一种基于随机梯度估计的内点算法，用于求解存在约束的连续可微非凸目标函数最小化问题，并通过实验结果进行了演示。该算法在解决光滑（非凸）优化问题时与其他内点方法不同之处在于搜索方向是通过随机梯度估计计算得到的。它在使用可行域的内部邻域（由正且消失的邻域参数序列定义）的过程中也很独特，通过将迭代强制保留在该邻域内。实验结果表明，通过精心平衡屏障、步长和邻域序列，该算法能够满足确定性和随机性设置下的收敛保证。在两种设置下，数值实验的结果表明，该算法可以优于投影-（随机）梯度方法。",
    "tldr": "本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。",
    "en_tdlr": "This paper proposes an interior-point algorithm for solving smooth bound-constrained optimization problems using stochastic gradient estimates for search direction and inner neighborhoods. It guarantees convergence in both deterministic and stochastic settings and outperforms traditional methods in experiments."
}