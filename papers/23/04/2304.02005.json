{
    "title": "Risk-Aware Distributed Multi-Agent Reinforcement Learning. (arXiv:2304.02005v1 [cs.AI])",
    "abstract": "Autonomous cyber and cyber-physical systems need to perform decision-making, learning, and control in unknown environments. Such decision-making can be sensitive to multiple factors, including modeling errors, changes in costs, and impacts of events in the tails of probability distributions. Although multi-agent reinforcement learning (MARL) provides a framework for learning behaviors through repeated interactions with the environment by minimizing an average cost, it will not be adequate to overcome the above challenges. In this paper, we develop a distributed MARL approach to solve decision-making problems in unknown environments by learning risk-aware actions. We use the conditional value-at-risk (CVaR) to characterize the cost function that is being minimized, and define a Bellman operator to characterize the value function associated to a given state-action pair. We prove that this operator satisfies a contraction property, and that it converges to the optimal value function. We t",
    "link": "http://arxiv.org/abs/2304.02005",
    "context": "Title: Risk-Aware Distributed Multi-Agent Reinforcement Learning. (arXiv:2304.02005v1 [cs.AI])\nAbstract: Autonomous cyber and cyber-physical systems need to perform decision-making, learning, and control in unknown environments. Such decision-making can be sensitive to multiple factors, including modeling errors, changes in costs, and impacts of events in the tails of probability distributions. Although multi-agent reinforcement learning (MARL) provides a framework for learning behaviors through repeated interactions with the environment by minimizing an average cost, it will not be adequate to overcome the above challenges. In this paper, we develop a distributed MARL approach to solve decision-making problems in unknown environments by learning risk-aware actions. We use the conditional value-at-risk (CVaR) to characterize the cost function that is being minimized, and define a Bellman operator to characterize the value function associated to a given state-action pair. We prove that this operator satisfies a contraction property, and that it converges to the optimal value function. We t",
    "path": "papers/23/04/2304.02005.json",
    "total_tokens": 834,
    "translated_title": "面向风险的分布式多智能体强化学习",
    "translated_abstract": "自主的网络和物理系统需要在未知环境中进行决策、学习和控制。这种决策可能会受到多种因素的影响，包括建模误差、成本变化以及概率分布尾部事件的影响。虽然多智能体强化学习为通过与环境反复交互以最小化平均成本来学习行为提供了一个框架，但它无法克服上述挑战。本文提出了一种分布式多智能体强化学习方法，在学习风险意识动作的同时解决了未知环境下的决策问题。我们使用条件风险价值（CVaR）来表征被最小化的成本函数，并定义了贝尔曼算子来表征与给定状态-动作对相关联的价值函数。我们证明了这个算子满足收缩特性，并且收敛于最优的价值函数。",
    "tldr": "本文提出了一种分布式多智能体强化学习方法，通过学习风险意识动作解决了未知环境下的决策问题。",
    "en_tdlr": "The paper proposes a distributed multi-agent reinforcement learning approach that solves decision-making problems in unknown environments by learning risk-aware actions."
}