{
    "title": "Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])",
    "abstract": "While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d",
    "link": "http://arxiv.org/abs/2304.09362",
    "context": "Title: Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])\nAbstract: While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d",
    "path": "papers/23/04/2304.09362.json",
    "total_tokens": 884,
    "translated_title": "未知动态下的长期公平性",
    "translated_abstract": "尽管机器学习可能会强化社会不平等，但它也可以用于动态地寻求公平的结果。在本文中，我们在在线强化学习的上下文中规范了长期公正性。该公式可以容纳动态控制目标，例如推动人群状态中固有的平等，这些目标不能被纳入到公平性的静态公式中。我们证明这种方法允许算法通过牺牲短期激励，将分类器-人群系统推向更理想的平衡，以适应未知的动态系统。针对这种情况，我们开发了一种算法，该算法适应了最近的在线学习工作。我们证明，这种算法在累积损失和公平性违规的累积性（作为人群之间的统计规律）上实现了同时的概率界限。我们将我们的算法与基准的细微分类器的重复训练以及一个解题器进行比较，在两个协调目标之间实现了更好的权衡。",
    "tldr": "本文提出一种新方法，通过在未知动态下追求长期公平性，实现算法的动态适应和权衡，可为分类器-人群系统推向更理想的平衡。",
    "en_tdlr": "This paper proposes a new method that achieves dynamic adaptation and trade-offs for algorithms by pursuing long-term fairness under unknown dynamics, driving the classifier-population system towards more desirable equilibria."
}