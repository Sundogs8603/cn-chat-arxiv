{
    "title": "Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])",
    "abstract": "We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an \"embedding -> backbone -> prediction head\" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial ",
    "link": "http://arxiv.org/abs/2304.10505",
    "context": "Title: Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])\nAbstract: We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an \"embedding -> backbone -> prediction head\" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial ",
    "path": "papers/23/04/2304.10505.json",
    "total_tokens": 904,
    "translated_title": "视频预训练Transformer: 多模态预训练专家混合体",
    "translated_abstract": "本文提出了视频预训练Transformer (VPT)。VPT使用来自之前工作的四个最先进的编码器模型将视频转换成紧凑的嵌入序列。我们的骨干网络基于参考Flan-T5-11B架构，学习视频的通用表示形式，这是编码器模型的非线性总和。它使用自回归因果语言建模损失通过预测YouTube视频中讲话的单词进行学习。最后，我们通过为每个任务训练全连接预测头，在标准的下游基准测试中进行评估。据我们所知，这是第一个在“嵌入->骨干网络->预测头”设计模式中使用多个冻结的最先进模型作为编码器的研究，而其他研究都是训练自己的联合编码器模型。此外，我们通过添加显式的场景图信息，包含了比当前SOTA Merlot Reserve更多的模态。出于这两个原因，我们相信它可以将世界上最好的开源模型结合起来，实现SOTA性能。",
    "tldr": "本文提出了一种视频预训练Transformer，它使用多个最先进的编码器模型将视频转换成通用表示，可以结合多种模式获得最佳性能。",
    "en_tdlr": "This paper presents a video pre-trained Transformer using multiple state-of-the-art encoder models to convert videos into a universal representation. With the ability to combine various modalities, it achieves SOTA performance."
}