{
    "title": "Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v1 [cs.MA])",
    "abstract": "In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents in",
    "link": "http://arxiv.org/abs/2304.10351",
    "context": "Title: Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v1 [cs.MA])\nAbstract: In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents in",
    "path": "papers/23/04/2304.10351.json",
    "total_tokens": 962,
    "translated_title": "多智能体强化学习中的时空序贯决策制导斯塔克伯格均衡的产生",
    "translated_abstract": "在多智能体强化学习(MARL)中，自利的智能体试图建立均衡并根据游戏结构实现协调。然而，现有的MARL方法大多受制于所有智能体在马可夫博弈(MG)框架中的同时行动，很少有作品考虑通过异步行动协调形成均衡策略。鉴于斯塔克伯格均衡(SE)相对纳什均衡的优势，我们构建了一个从MG导出的时空序贯决策结构，并提出了一个基于所有智能体共享的条件超网络的N级策略模型。这种方法允许不对称训练和对称执行，每个智能体响应于上级智能体决策的最优反应。智能体可以学习不同的SE策略，同时仍然保持参数共享，这导致了学习和存储成本的降低以及扩展性的提高，因为智能体数量增加时，存储和计算成本不会显著增加。",
    "tldr": "该论文提出了一个基于多智能体强化学习的时空序贯决策结构，实现了斯塔克伯格均衡策略的异步行动协调，并在参数共享的同时实现了不同智能体之间的不对称训练。",
    "en_tdlr": "This paper proposes a spatio-temporal sequential decision-making structure based on multi-agent reinforcement learning to induce Stackelberg Equilibrium strategies via asynchronous action coordination. The approach allows for asymmetric training with symmetric execution, and different agents can learn heterogeneous SE policies while still maintaining parameter sharing."
}