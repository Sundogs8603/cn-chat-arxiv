{
    "title": "Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])",
    "abstract": "Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.",
    "link": "http://arxiv.org/abs/2304.00046",
    "context": "Title: Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])\nAbstract: Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.",
    "path": "papers/23/04/2304.00046.json",
    "total_tokens": 843,
    "translated_title": "使用离线预训练加速探索和表示学习",
    "translated_abstract": "串行决策制定代理在长期任务中面临挑战，因为需要多步推理才能解决。大多数强化学习（RL）算法通过改进信用分配，引入记忆能力，改变代理的内在动机（即探索）或其世界观（即知识表示）来应对这一挑战。这些组成部分中的许多都可以从离线数据中学习。本文提出了一个假设，即通过从单个离线数据集中分别学习两个不同的模型，可以改善探索和表示学习。我们展示了使用噪声对比估计学习状态表示以及从单个人类演示的模型辅助奖励可以显著提高在具有挑战性的 NetHack 基准测试上的样本效率。我们还消融了实验设置的各个组成部分并突显了重要的见解。",
    "tldr": "本论文提出了一个假设，即基于离线数据可以通过分别学习状态表示和辅助奖励模型来改善探索和表示学习，实验证明这种方法显著提高了在具有挑战性的 NetHack 基准测试上的样本效率。",
    "en_tdlr": "This paper proposes that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. The efficient sample results on the challenging NetHack benchmark show the validity of this hypothesis."
}