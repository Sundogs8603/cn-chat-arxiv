{
    "title": "A Common Misassumption in Online Experiments with Machine Learning Models. (arXiv:2304.10900v1 [cs.LG])",
    "abstract": "Online experiments such as Randomised Controlled Trials (RCTs) or A/B-tests are the bread and butter of modern platforms on the web. They are conducted continuously to allow platforms to estimate the causal effect of replacing system variant \"A\" with variant \"B\", on some metric of interest. These variants can differ in many aspects. In this paper, we focus on the common use-case where they correspond to machine learning models. The online experiment then serves as the final arbiter to decide which model is superior, and should thus be shipped.  The statistical literature on causal effect estimation from RCTs has a substantial history, which contributes deservedly to the level of trust researchers and practitioners have in this \"gold standard\" of evaluation practices. Nevertheless, in the particular case of machine learning experiments, we remark that certain critical issues remain. Specifically, the assumptions that are required to ascertain that A/B-tests yield unbiased estimates of t",
    "link": "http://arxiv.org/abs/2304.10900",
    "context": "Title: A Common Misassumption in Online Experiments with Machine Learning Models. (arXiv:2304.10900v1 [cs.LG])\nAbstract: Online experiments such as Randomised Controlled Trials (RCTs) or A/B-tests are the bread and butter of modern platforms on the web. They are conducted continuously to allow platforms to estimate the causal effect of replacing system variant \"A\" with variant \"B\", on some metric of interest. These variants can differ in many aspects. In this paper, we focus on the common use-case where they correspond to machine learning models. The online experiment then serves as the final arbiter to decide which model is superior, and should thus be shipped.  The statistical literature on causal effect estimation from RCTs has a substantial history, which contributes deservedly to the level of trust researchers and practitioners have in this \"gold standard\" of evaluation practices. Nevertheless, in the particular case of machine learning experiments, we remark that certain critical issues remain. Specifically, the assumptions that are required to ascertain that A/B-tests yield unbiased estimates of t",
    "path": "papers/23/04/2304.10900.json",
    "total_tokens": 835,
    "translated_abstract": "在现代网络平台中，随机对照试验或A/B测试等在线实验是很常见的，这些实验不断进行以便让平台能够评估将系统变体“A”替换为变体“B”的因果效应。在本文中，我们关注于将这些变体应用于机器学习模型的常见情况。在线实验随后成为决定哪个模型更优秀、应该被发布的最终仲裁员。虽然因随机对照试验的因果效应估计在统计学文献中已有相当长的历史，这在某种程度上为人们对这种评估实践的信任做出了贡献，但是，在机器学习实验的特殊情况下，我们注意到仍然存在某些关键问题。具体来说，我们需要确认A/B测试所需的假设是否能够产生无偏的估计值。",
    "tldr": "本文讨论了在机器学习模型在线实验中一个常见的错误假设，即需要确认A/B测试所需的假设是否能够产生无偏的估计值。",
    "en_tdlr": "This paper discusses a common mistaken assumption in online experiments with machine learning models and raises the question of whether the necessary assumptions for unbiased estimates are met in A/B testing."
}