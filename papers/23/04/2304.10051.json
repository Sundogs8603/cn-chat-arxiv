{
    "title": "HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services. (arXiv:2304.10051v1 [cs.LG])",
    "abstract": "Hyper-parameters optimization (HPO) is vital for machine learning models. Besides model accuracy, other tuning intentions such as model training time and energy consumption are also worthy of attention from data analytic service providers. Hence, it is essential to take both model hyperparameters and system parameters into consideration to execute cross-layer multi-objective hyperparameter auto-tuning. Towards this challenging target, we propose HyperTuner in this paper. To address the formulated high-dimensional black-box multi-objective optimization problem, HyperTuner first conducts multi-objective parameter importance ranking with its MOPIR algorithm and then leverages the proposed ADUMBO algorithm to find the Pareto-optimal configuration set. During each iteration, ADUMBO selects the most promising configuration from the generated Pareto candidate set via maximizing a new well-designed metric, which can adaptively leverage the uncertainty as well as the predicted mean across all t",
    "link": "http://arxiv.org/abs/2304.10051",
    "context": "Title: HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services. (arXiv:2304.10051v1 [cs.LG])\nAbstract: Hyper-parameters optimization (HPO) is vital for machine learning models. Besides model accuracy, other tuning intentions such as model training time and energy consumption are also worthy of attention from data analytic service providers. Hence, it is essential to take both model hyperparameters and system parameters into consideration to execute cross-layer multi-objective hyperparameter auto-tuning. Towards this challenging target, we propose HyperTuner in this paper. To address the formulated high-dimensional black-box multi-objective optimization problem, HyperTuner first conducts multi-objective parameter importance ranking with its MOPIR algorithm and then leverages the proposed ADUMBO algorithm to find the Pareto-optimal configuration set. During each iteration, ADUMBO selects the most promising configuration from the generated Pareto candidate set via maximizing a new well-designed metric, which can adaptively leverage the uncertainty as well as the predicted mean across all t",
    "path": "papers/23/04/2304.10051.json",
    "total_tokens": 905,
    "translated_title": "HyperTuner：数据分析服务的跨层多目标超参数自动调整框架",
    "translated_abstract": "超参数优化对于机器学习模型至关重要，除了模型准确性之外，模型训练时间和能耗等调节意图也值得吸引数据分析服务提供商的关注。因此，考虑到模型超参数和系统参数，执行跨层多目标超参数自动调整是至关重要的。为了解决这个具有挑战性的目标，本文提出了HyperTuner。为了解决该高维黑盒多目标优化问题，HyperTuner首先使用MOPIR算法进行多目标参数重要性排序，然后利用提出的ADUMBO算法找到Pareto最优配置集合。在每个迭代过程中，ADUMBO通过最大化一个新的设计良好的度量标准来自动生成Pareto候选集合，并选择最有前途的配置。",
    "tldr": "本论文提出了HyperTuner，它是一个跨层多目标超参数自动调整框架，可同时考虑模型超参数和系统参数。HyperTuner使用MOPIR算法进行多目标参数重要性排序，然后利用ADUMBO算法找到Pareto最优配置集合，帮助数据分析服务提供商解决高维黑盒多目标优化问题。"
}