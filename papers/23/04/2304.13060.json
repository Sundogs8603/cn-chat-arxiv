{
    "title": "Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])",
    "abstract": "Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z",
    "link": "http://arxiv.org/abs/2304.13060",
    "context": "Title: Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])\nAbstract: Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z",
    "path": "papers/23/04/2304.13060.json",
    "total_tokens": 1010,
    "translated_title": "只用结构先预训练：利用迁移学习理解语言归纳偏置",
    "translated_abstract": "无论是人类还是变压器语言模型都能在没有明确的结构监督下学习语言。什么样的归纳式学习偏置使得这种学习成为可能？在这项研究中，我们通过在人造结构数据上预先训练并在英语上微调来采用不同的归纳式学习偏置对语言模型进行偏置。我们的实验设置使我们能够积极控制语言模型的归纳偏置。通过我们的实验，我们研究了三种归纳偏置的比较成功:1)递归的层级处理的归纳偏置2)不受限的标记-标记依赖，这些依赖关系不能由上下文无关文法建模3)Zipfian幂律词汇分布的归纳偏置。我们表明，复杂的标记-标记交互形成了最好的归纳偏置，并且这在非上下文无关情况下最为强烈。我们还表明，Z(targetEntity)分布在英语上也是合适的预先训练分布。",
    "tldr": "通过在人造结构数据上进行预先训练和在英语上微调，我们研究了自然语言处理中三种归纳偏置类型：递归的层级处理、无限制的标记-标记依赖以及基于Zipfian幂律词汇分布的归纳偏置，我们得出复杂标记-标记交互形成了最好的归纳偏置的结论。",
    "en_tdlr": "By pretraining on artificial structured data and fine-tuning on English, we investigated three types of inductive biases in natural language processing: recursive hierarchical processing, unrestricted token-token dependencies, and Zipfian power-law vocabulary distribution, and found that complex token-token interactions form the best inductive biases."
}