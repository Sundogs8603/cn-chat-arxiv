{
    "title": "HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])",
    "abstract": "Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network",
    "link": "http://arxiv.org/abs/2304.06910",
    "context": "Title: HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])\nAbstract: Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network",
    "path": "papers/23/04/2304.06910.json",
    "total_tokens": 982,
    "translated_title": "HCAM--多模态情感识别的分层交叉注意模型",
    "translated_abstract": "对话情感识别由于情感表达的多模态性而具有挑战性。我们提出了一种采用递归和共同注意神经网络模型的分层交叉注意模型（HCAM）方法用于多模态情感识别。模型的输入包括两种模态，即通过可学习wav2vec方法处理的音频数据和使用双向编码器来自变压器（BERT）模型表示的文本数据。音频和文本表示使用一组双向递归神经网络层进行处理，使用自注意将给定对话中的每个话语转换为固定维度的嵌入。为了整合上下文知识和两种模态的信息，使用共同注意层将音频和文本嵌入进行组合，试图衡量与情感识别任务相关的话语级嵌入。在CMU-MOSI数据集上训练和评估神经网络模型，这是一个大规模的多模态会话数据集。实验结果表明，所提出的HCAM方法优于现有的情感识别最先进方法。",
    "tldr": "该论文提出了一种分层交叉注意模型（HCAM）用于多模态情感识别，使用递归和共同注意神经网络模型进行音频和文本表示，将这两种模态信息以共同注意方式结合，取得了比现有方法更好的情感识别效果。",
    "en_tdlr": "This paper proposes a hierarchical cross-attention model (HCAM) for multi-modal emotion recognition, which uses recurrent and co-attention neural network models to process audio and text representations, and combines the two modalities using a co-attention layer. The approach outperforms existing state-of-the-art methods for emotion recognition on the CMU-MOSI dataset."
}