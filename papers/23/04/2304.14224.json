{
    "title": "Self-discipline on multiple channels. (arXiv:2304.14224v1 [cs.LG])",
    "abstract": "Self-distillation relies on its own information to improve the generalization ability of the model and has a bright future. Existing self-distillation methods either require additional models, model modification, or batch size expansion for training, which increases the difficulty of use, memory consumption, and computational cost. This paper developed Self-discipline on multiple channels(SMC), which combines consistency regularization with self-distillation using the concept of multiple channels. Conceptually, SMC consists of two steps: 1) each channel data is simultaneously passed through the model to obtain its corresponding soft label, and 2) the soft label saved in the previous step is read together with the soft label obtained from the current channel data through the model to calculate the loss function. SMC uses consistent regularization and self-distillation to improve the generalization ability of the model and the robustness of the model to noisy labels. We named the SMC con",
    "link": "http://arxiv.org/abs/2304.14224",
    "context": "Title: Self-discipline on multiple channels. (arXiv:2304.14224v1 [cs.LG])\nAbstract: Self-distillation relies on its own information to improve the generalization ability of the model and has a bright future. Existing self-distillation methods either require additional models, model modification, or batch size expansion for training, which increases the difficulty of use, memory consumption, and computational cost. This paper developed Self-discipline on multiple channels(SMC), which combines consistency regularization with self-distillation using the concept of multiple channels. Conceptually, SMC consists of two steps: 1) each channel data is simultaneously passed through the model to obtain its corresponding soft label, and 2) the soft label saved in the previous step is read together with the soft label obtained from the current channel data through the model to calculate the loss function. SMC uses consistent regularization and self-distillation to improve the generalization ability of the model and the robustness of the model to noisy labels. We named the SMC con",
    "path": "papers/23/04/2304.14224.json",
    "total_tokens": 833,
    "translated_title": "多通道自律",
    "translated_abstract": "自我蒸馏依靠自身信息提高模型的泛化能力，并具有广阔的发展前景。现有的自我蒸馏方法要么需要额外的模型，要么需要修改模型，要么需要扩大批量大小进行训练，增加了使用难度、内存消耗和计算成本。本文提出了一个名为“多通道自律”（SMC）的方法，将一致性正则化与自我蒸馏相结合，使用多通道的概念。SMC从概念上分为两个步骤：1）每个通道数据同时通过模型，获得其相应的软标签；2）存储在上一步的软标签与从当前通道数据通过模型获得的软标签一起读取，计算损失函数。SMC使用一致性正则化和自我蒸馏来提高模型的泛化能力和对噪声标签的鲁棒性。",
    "tldr": "本文提出了一种名为“多通道自律”的方法，使用一致性正则化和自我蒸馏来提高模型的泛化能力和对噪声标签的鲁棒性。",
    "en_tdlr": "This paper proposes a method called Self-discipline on multiple channels (SMC), which combines consistency regularization with self-distillation using the concept of multiple channels to improve the generalization ability and robustness of the model to noisy labels."
}