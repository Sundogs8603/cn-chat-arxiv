{
    "title": "An Offline Metric for the Debiasedness of Click Models. (arXiv:2304.09560v1 [cs.IR])",
    "abstract": "A well-known problem when learning from user clicks are inherent biases prevalent in the data, such as position or trust bias. Click models are a common method for extracting information from user clicks, such as document relevance in web search, or to estimate click biases for downstream applications such as counterfactual learning-to-rank, ad placement, or fair ranking. Recent work shows that the current evaluation practices in the community fail to guarantee that a well-performing click model generalizes well to downstream tasks in which the ranking distribution differs from the training distribution, i.e., under covariate shift. In this work, we propose an evaluation metric based on conditional independence testing to detect a lack of robustness to covariate shift in click models. We introduce the concept of debiasedness and a metric for measuring it. We prove that debiasedness is a necessary condition for recovering unbiased and consistent relevance scores and for the invariance o",
    "link": "http://arxiv.org/abs/2304.09560",
    "context": "Title: An Offline Metric for the Debiasedness of Click Models. (arXiv:2304.09560v1 [cs.IR])\nAbstract: A well-known problem when learning from user clicks are inherent biases prevalent in the data, such as position or trust bias. Click models are a common method for extracting information from user clicks, such as document relevance in web search, or to estimate click biases for downstream applications such as counterfactual learning-to-rank, ad placement, or fair ranking. Recent work shows that the current evaluation practices in the community fail to guarantee that a well-performing click model generalizes well to downstream tasks in which the ranking distribution differs from the training distribution, i.e., under covariate shift. In this work, we propose an evaluation metric based on conditional independence testing to detect a lack of robustness to covariate shift in click models. We introduce the concept of debiasedness and a metric for measuring it. We prove that debiasedness is a necessary condition for recovering unbiased and consistent relevance scores and for the invariance o",
    "path": "papers/23/04/2304.09560.json",
    "total_tokens": 928,
    "translated_title": "离线度量点击模型的去偏差性",
    "translated_abstract": "在学习用户点击时，固有偏见是数据中普遍存在的一个问题，例如位置偏见或信任偏见。点击模型是从用户点击中提取信息的常用方法，例如在Web搜索中提取文档相关性，或者估计点击偏差以用于下游应用，例如反事实的学习排序、广告位置和公平排序。最近的研究表明，社区中的当前评估实践不能保证性能良好的点击模型对于下游任务的泛化能力，其中排名分布与训练分布不同，即在协变偏移下。在这项工作中，我们提出了一个基于条件独立性测试的评估度量，以检测点击模型对协变偏移的缺乏鲁棒性。我们引入了去偏差性的概念和一种测量方法。我们证明，去偏差性是恢复无偏的一致相关性评分以及使点击模型对排名分布变化的不变性的必要条件。",
    "tldr": "该论文介绍了一种离线评估点击模型去协变偏移的鲁棒性的方法，并提出了去偏差性这一概念和测量方法，这是恢复无偏一致相关性评分和点击模型对排名分布变化不变性的必要条件。",
    "en_tdlr": "This paper introduces an offline evaluation method for the robustness of click models against covariate shift, and proposes the concepts of debiasedness and the metric for measuring it, which are necessary conditions for recovering unbiased and consistent relevance scores and for the invariance of click models to variations in the ranking distribution."
}