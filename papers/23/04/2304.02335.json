{
    "title": "Correcting Flaws in Common Disentanglement Metrics. (arXiv:2304.02335v1 [cs.LG])",
    "abstract": "Recent years have seen growing interest in learning disentangled representations, in which distinct features, such as size or shape, are represented by distinct neurons. Quantifying the extent to which a given representation is disentangled is not straightforward; multiple metrics have been proposed. In this paper, we identify two failings of existing metrics, which mean they can assign a high score to a model which is still entangled, and we propose two new metrics, which redress these problems. We then consider the task of compositional generalization. Unlike prior works, we treat this as a classification problem, which allows us to use it to measure the disentanglement ability of the encoder, without depending on the decoder. We show that performance on this task is (a) generally quite poor, (b) correlated with most disentanglement metrics, and (c) most strongly correlated with our newly proposed metrics.",
    "link": "http://arxiv.org/abs/2304.02335",
    "context": "Title: Correcting Flaws in Common Disentanglement Metrics. (arXiv:2304.02335v1 [cs.LG])\nAbstract: Recent years have seen growing interest in learning disentangled representations, in which distinct features, such as size or shape, are represented by distinct neurons. Quantifying the extent to which a given representation is disentangled is not straightforward; multiple metrics have been proposed. In this paper, we identify two failings of existing metrics, which mean they can assign a high score to a model which is still entangled, and we propose two new metrics, which redress these problems. We then consider the task of compositional generalization. Unlike prior works, we treat this as a classification problem, which allows us to use it to measure the disentanglement ability of the encoder, without depending on the decoder. We show that performance on this task is (a) generally quite poor, (b) correlated with most disentanglement metrics, and (c) most strongly correlated with our newly proposed metrics.",
    "path": "papers/23/04/2304.02335.json",
    "total_tokens": 850,
    "translated_title": "修正普遍解缠度量中的错误",
    "translated_abstract": "近年来，越来越多的人对学习解缠表示产生了浓厚的兴趣，其中不同的特征，如大小或形状，由不同的神经元表示。量化特定表示解缠的程度并非易事，已经提出了多种指标。在本文中，我们确定了现有指标的两个缺陷，这意味着它们可以将一个仍然纠缠的模型评分高，并且我们提出了两个新指标，以纠正这些问题。然后我们考虑了组合泛化任务。与以往的工作不同，我们将其视为一个分类问题，这使得我们可以用它来衡量编码器的解缠能力，而不依赖于解码器。我们展示了这个任务的表现通常相当差，与大多数解缠度量相关，最强烈地与我们新提出的度量相关。",
    "tldr": "本文提出了两个新的解缠指标，用于修正现有指标的两个缺陷，并通过将组合泛化任务作为分类问题来衡量编码器的解缠能力。新指标与组合泛化任务的相关性最强。",
    "en_tdlr": "This paper proposes two new disentanglement metrics that correct two flaws in existing metrics and uses the classification task of compositional generalization to measure the disentanglement ability of the encoder. The new metrics are most strongly correlated with this task's performance."
}