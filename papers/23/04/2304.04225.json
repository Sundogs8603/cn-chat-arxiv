{
    "title": "Transformer Utilization in Medical Image Segmentation Networks. (arXiv:2304.04225v1 [cs.CV])",
    "abstract": "Owing to success in the data-rich domain of natural images, Transformers have recently become popular in medical image segmentation. However, the pairing of Transformers with convolutional blocks in varying architectural permutations leaves their relative effectiveness to open interpretation. We introduce Transformer Ablations that replace the Transformer blocks with plain linear operators to quantify this effectiveness. With experiments on 8 models on 2 medical image segmentation tasks, we explore -- 1) the replaceable nature of Transformer-learnt representations, 2) Transformer capacity alone cannot prevent representational replaceability and works in tandem with effective design, 3) The mere existence of explicit feature hierarchies in transformer blocks is more beneficial than accompanying self-attention modules, 4) Major spatial downsampling before Transformer modules should be used with caution.",
    "link": "http://arxiv.org/abs/2304.04225",
    "context": "Title: Transformer Utilization in Medical Image Segmentation Networks. (arXiv:2304.04225v1 [cs.CV])\nAbstract: Owing to success in the data-rich domain of natural images, Transformers have recently become popular in medical image segmentation. However, the pairing of Transformers with convolutional blocks in varying architectural permutations leaves their relative effectiveness to open interpretation. We introduce Transformer Ablations that replace the Transformer blocks with plain linear operators to quantify this effectiveness. With experiments on 8 models on 2 medical image segmentation tasks, we explore -- 1) the replaceable nature of Transformer-learnt representations, 2) Transformer capacity alone cannot prevent representational replaceability and works in tandem with effective design, 3) The mere existence of explicit feature hierarchies in transformer blocks is more beneficial than accompanying self-attention modules, 4) Major spatial downsampling before Transformer modules should be used with caution.",
    "path": "papers/23/04/2304.04225.json",
    "total_tokens": 803,
    "translated_title": "在医学图像分割网络中使用Transformer",
    "translated_abstract": "由于在数据丰富的自然图像领域取得成功，Transformer最近在医学图像分割中变得流行起来。然而，Transformer与卷积块的匹配以及不同架构排列方式的选择，使得它们的相对有效性需要进一步探究。我们引入了Transformer削弱实验，用普通线性算子替代Transformer块，以量化该有效性。通过对两个医学图像分割任务中的8个模型进行实验，我们探索了：1）Transformer学习表示的可替代性；2）Transformer的容量单独并不能防止表示的可替换性，并需要与有效的设计搭配使用；3）Transformer块中显式特征层次结构的存在本身比伴随注意力机制更有益；4）应谨慎使用Transformer模块之前的主要空间下采样操作。",
    "tldr": "Transformer在医学图像分割中最有效的设计是伴随着显式的特征层次结构，而单独使用不能防止表示的可替换性；应慎用主要空间下采样操作。",
    "en_tdlr": "The most effective design for using Transformer in medical image segmentation is to have explicit feature hierarchy accompanying it, and its capacity alone cannot prevent representational replaceability. Major spatial downsampling before Transformer modules should be used with caution."
}