{
    "title": "RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer. (arXiv:2304.05659v1 [cs.CV])",
    "abstract": "This paper studies how to keep a vision backbone effective while removing token mixers in its basic building blocks. Token mixers, as self-attention for vision transformers (ViTs), are intended to perform information communication between different spatial tokens but suffer from considerable computational cost and latency. However, directly removing them will lead to an incomplete model structure prior, and thus brings a significant accuracy drop. To this end, we first develop an RepIdentityFormer base on the re-parameterizing idea, to study the token mixer free model architecture. And we then explore the improved learning paradigm to break the limitation of simple token mixer free backbone, and summarize the empirical practice into 5 guidelines. Equipped with the proposed optimization strategy, we are able to build an extremely simple vision backbone with encouraging performance, while enjoying the high efficiency during inference. Extensive experiments and ablative analysis also demo",
    "link": "http://arxiv.org/abs/2304.05659",
    "context": "Title: RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer. (arXiv:2304.05659v1 [cs.CV])\nAbstract: This paper studies how to keep a vision backbone effective while removing token mixers in its basic building blocks. Token mixers, as self-attention for vision transformers (ViTs), are intended to perform information communication between different spatial tokens but suffer from considerable computational cost and latency. However, directly removing them will lead to an incomplete model structure prior, and thus brings a significant accuracy drop. To this end, we first develop an RepIdentityFormer base on the re-parameterizing idea, to study the token mixer free model architecture. And we then explore the improved learning paradigm to break the limitation of simple token mixer free backbone, and summarize the empirical practice into 5 guidelines. Equipped with the proposed optimization strategy, we are able to build an extremely simple vision backbone with encouraging performance, while enjoying the high efficiency during inference. Extensive experiments and ablative analysis also demo",
    "path": "papers/23/04/2304.05659.json",
    "total_tokens": 881,
    "translated_title": "RIFormer：移除Token Mixer后保持视觉Backbone的有效性",
    "translated_abstract": "本文研究了如何在去除基本构建块中的Token Mixer的同时保持视觉Backbone的有效性。Token Mixer作为视觉Transformer（ViTs）的自注意力，旨在执行不同空间Token之间的信息通信，但会导致相当大的计算成本和延迟。然而，直接删除它们将导致不完整的模型结构，因此会带来显著的准确度下降。为此，我们首先开发出Reparameterization Idea的RepIdentityFormer来研究无Token Mixer的模型架构。然后，我们探索了改进的学习范式来打破简单无Token Mixer的Backbone的局限性，并将经验实践总结为5条准则。配备所提出的优化策略，我们能够构建一个非常简单的视觉Backbone并获得令人鼓舞的性能，同时在推断期间享受高效性。广泛的实验和剖析分析也展示了其有效性。",
    "tldr": "本文提出一种在去除视觉Transformer中的Token Mixer的同时，保持视觉Backbone有效性的方法，配备所提出的优化策略能够构建一个非常简单的视觉Backbone并获得令人鼓舞的性能，同时在推断期间享受高效性。",
    "en_tdlr": "This paper proposes a method to maintain the effectiveness of a vision backbone while removing token mixers in vision transformers, and presents an optimization strategy for building an extremely simple vision backbone with encouraging performance while enjoying high efficiency during inference."
}