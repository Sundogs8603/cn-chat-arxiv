{
    "title": "Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization. (arXiv:2304.11524v1 [cs.AI])",
    "abstract": "Text summarization is essential for information aggregation and demands large amounts of training data. However, concerns about data privacy and security limit data collection and model training. To eliminate this concern, we propose a federated learning text summarization scheme, which allows users to share the global model in a cooperative learning manner without sharing raw data. Personalized federated learning (PFL) balances personalization and generalization in the process of optimizing the global model, to guide the training of local models. However, multiple local data have different distributions of semantics and context, which may cause the local model to learn deviated semantic and context information. In this paper, we propose FedSUMM, a dynamic gradient adapter to provide more appropriate local parameters for local model. Simultaneously, FedSUMM uses differential privacy to prevent parameter leakage during distributed training. Experimental evidence verifies FedSUMM can ach",
    "link": "http://arxiv.org/abs/2304.11524",
    "context": "Title: Personalized Federated Learning via Gradient Modulation for Heterogeneous Text Summarization. (arXiv:2304.11524v1 [cs.AI])\nAbstract: Text summarization is essential for information aggregation and demands large amounts of training data. However, concerns about data privacy and security limit data collection and model training. To eliminate this concern, we propose a federated learning text summarization scheme, which allows users to share the global model in a cooperative learning manner without sharing raw data. Personalized federated learning (PFL) balances personalization and generalization in the process of optimizing the global model, to guide the training of local models. However, multiple local data have different distributions of semantics and context, which may cause the local model to learn deviated semantic and context information. In this paper, we propose FedSUMM, a dynamic gradient adapter to provide more appropriate local parameters for local model. Simultaneously, FedSUMM uses differential privacy to prevent parameter leakage during distributed training. Experimental evidence verifies FedSUMM can ach",
    "path": "papers/23/04/2304.11524.json",
    "total_tokens": 941,
    "translated_title": "基于梯度调制的个性化联合学习在异构文本摘要中的应用",
    "translated_abstract": "文本摘要对于信息聚合至关重要，并且需要大量的训练数据。但是，对数据隐私和安全的担忧限制了数据的收集和模型的训练。为了解决这个问题，我们提出了一种联合学习文本摘要方案，它允许用户在不共享原始数据的情况下以合作学习的方式分享全局模型。个性化联合学习（PFL）在全局模型优化过程中平衡个性化和泛化，以指导本地模型的训练。然而，多个本地数据具有不同的语义和上下文分布，这可能导致本地模型学习到偏离的语义和上下文信息。在本文中，我们提出了FedSUMM，一种动态梯度适配器，为本地模型提供更适当的本地参数。同时，FedSUMM使用差分隐私来防止分布式训练期间的参数泄露。实验证据验证了FedSUMM可以在模型准确性和隐私保护方面实现竞争性的性能。",
    "tldr": "本研究提出了一种基于梯度调制的个性化联合学习方案，名为FedSUMM，用于异构文本摘要。它能够在隐私保护下实现模型的准确性和良好的性能。",
    "en_tdlr": "This paper proposes a personalized federated learning scheme named FedSUMM for heterogeneous text summarization, which uses gradient modulation to balance personalization and generalization while preventing parameter leakage with differential privacy. FedSUMM achieves competitive performance in both accuracy and privacy protection."
}