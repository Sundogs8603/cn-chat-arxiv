{
    "title": "Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems. (arXiv:2304.10892v1 [cs.LG])",
    "abstract": "The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).",
    "link": "http://arxiv.org/abs/2304.10892",
    "context": "Title: Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems. (arXiv:2304.10892v1 [cs.LG])\nAbstract: The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).",
    "path": "papers/23/04/2304.10892.json",
    "total_tokens": 921,
    "translated_title": "协调推理服务系统的高准确性、成本效益和低延迟",
    "translated_abstract": "机器学习（ML）推理服务的使用正在急剧增加。ML推理服务与用户直接交互，需要快速准确的响应。此外，这些服务面临不断变化的请求工作负载，需要调整其计算资源。计算资源不合理会导致延迟服务级别目标 (SLOs) 违规或浪费计算资源。考虑准确性、延迟和资源成本等方面的所有因素来适应动态工作负载具有挑战性。为了应对这些挑战，我们提出了 InfAdapter，它会主动选择一组带有资源分配的 ML 模型变体，以满足延迟 SLO，并最大化由准确性和成本组成的目标函数。相较于流行的行业自动缩放器 (Kubernetes Vertical Pod Autoscaler)，InfAdapter 分别降低了 SLO 违规和成本达 65% 和 33%。",
    "tldr": "InfAdapter提出了一个解决高准确性、低延迟和成本效益之间权衡问题的方法，通过主动选择一组带有资源分配的 ML 模型变体来满足延迟 SLO，并最大化由准确性和成本组成的目标函数，相较于其他方法降低了 SLO 违规和成本。",
    "en_tdlr": "InfAdapter proposes a solution to reconcile the trade-off between high accuracy, low latency, and cost-efficiency in ML inference serving systems by proactively selecting a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. It reduces SLO violations and costs compared to a popular industry autoscaler called Kubernetes Vertical Pod Autoscaler."
}