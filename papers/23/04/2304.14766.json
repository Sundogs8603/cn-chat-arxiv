{
    "title": "Hyperparameter Optimization through Neural Network Partitioning. (arXiv:2304.14766v1 [cs.LG])",
    "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance -- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly c",
    "link": "http://arxiv.org/abs/2304.14766",
    "context": "Title: Hyperparameter Optimization through Neural Network Partitioning. (arXiv:2304.14766v1 [cs.LG])\nAbstract: Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance -- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly c",
    "path": "papers/23/04/2304.14766.json",
    "total_tokens": 895,
    "translated_title": "通过神经网络分割进行超参数优化",
    "translated_abstract": "调整恰当的超参对于获得神经网络中良好的泛化行为至关重要。它们可以强制适当的归纳偏差，正则化模型并提高性能，特别是在有限的数据情况下。在本文中，我们提出了一种简单而有效的方法来优化超参数，该方法受边缘似然的启发，这是一种不需要验证数据的优化目标。我们的方法将训练数据和神经网络模型分为 K 个数据分片和参数分区。每个分区仅与特定的数据片段关联并进行优化。将这些分区组合成子网络使我们能够将子网络的“训练之外的样本”损失定义为超参数优化的目标，即在子网络看不到的数据片段上计算损失。我们证明，我们可以将这个目标应用到单次训练运行中，同时显着降低超参数优化的代价。",
    "tldr": "本文提出了一种将训练数据和神经网络模型分区的方法，将每个分区与特定的数据片段关联并进行优化，通过优化这些分区的子网络的“训练之外的样本”损失，实现了在单次训练运行中降低超参数优化代价的效果。",
    "en_tdlr": "This paper proposes a method that partitions the training data and neural network model into K data shards and parameter partitions, respectively, and optimizes them separately by optimizing the out-of-training-sample loss of each subnetwork on specific data shards, achieving the effect of reducing the cost of hyperparameter optimization in a single training run."
}