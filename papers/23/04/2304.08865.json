{
    "title": "Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])",
    "abstract": "Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r",
    "link": "http://arxiv.org/abs/2304.08865",
    "context": "Title: Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])\nAbstract: Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r",
    "path": "papers/23/04/2304.08865.json",
    "total_tokens": 937,
    "translated_title": "基于罗马化的多语言模型大规模适应",
    "translated_abstract": "大型多语言预训练语言模型（mPLMs）已成为跨语言NLP中的事实标准，但是，它们在许多语言的大规模部署方面受到诸多限制，包括预训练数据稀缺、词汇量增加和参数预算的限制。为了增强mPLMs处理低资源和未知语言的能力，我们探索大规模利用转写的潜力。具体而言，我们探索UROMAN转写工具的潜力，该工具为所有书写系统提供了从UTF-8到拉丁字符的映射，从而实现了几乎任何语言的廉价罗马化。首先，我们重点研究了UROMAN相对于其他语言特定和手动策划的转写工具在适应多语言PLMs方面的差异。然后，我们研究并比较了一系列数据和参数高效的策略，以适应罗马化和非罗马化的14种不同以上语言数据。",
    "tldr": "该论文探索利用大规模转写来提升大型多语言预训练语言模型的处理低资源和未知语言的能力，利用UROMAN转写工具的潜力，并研究了一系列高效的策略，以适应各种语言数据。",
    "en_tdlr": "This paper explores the potential of utilizing massive transliteration to enhance the ability of large multilingual pretrained language models (mPLMs) to deal with low-resource and unseen languages by leveraging the UROMAN transliteration tool, and studies a range of efficient strategies for adapting to various language data."
}