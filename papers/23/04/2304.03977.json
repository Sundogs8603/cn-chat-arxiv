{
    "title": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch. (arXiv:2304.03977v1 [cs.CV])",
    "abstract": "Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather \"inefficient\" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermor",
    "link": "http://arxiv.org/abs/2304.03977",
    "context": "Title: EMP-SSL: Towards Self-Supervised Learning in One Training Epoch. (arXiv:2304.03977v1 [cs.CV])\nAbstract: Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather \"inefficient\" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermor",
    "path": "papers/23/04/2304.03977.json",
    "total_tokens": 1105,
    "translated_title": "EMP-SSL：自监督学习中一次训练时代的探索",
    "translated_abstract": "最近，自监督学习（SSL）在学习图像表示方面取得了巨大成功。虽然取得了实验证据，但大多数自监督学习方法都是相当“低效”的学习方法，通常需要数百个训练时代才能完全收敛。本文表明，实现有效的自监督学习的关键是增加每个图像实例的裁剪数量。我们利用现有领先的SSL方法之一，引入了一种名为Extreme-Multi-Patch（EMP）自监督学习方法，它不依赖于许多用于SSL的启发式技术，例如分支之间的重量共享、特征归一化、输出量化和停止梯度等，并将训练时代缩短了两个数量级。我们展示了该方法能够在仅一个时代内收敛到CIFAR-10上的85.1％，CIFAR-100上的58.5％，Tiny ImageNet上的38.1％和ImageNet-100上的58.5％。此外，我们证明了我们的方法与最先进的SSL方法和监督预训练方法相比具有竞争性能。我们的结果表明，EMP-SSL是一种简单、高效且有效的自监督学习方法。",
    "tldr": "本文介绍了一种名为EMP-SSL的自监督学习方法，它通过增加每个图像实例的裁剪数量来提高学习效率，缩短了训练时代数量，并在CIFAR-10、CIFAR-100、Tiny ImageNet和ImageNet-100数据集上仅使用一次训练时代而获得了竞争性能。",
    "en_tdlr": "This paper proposes a self-supervised learning method called EMP-SSL, which improves learning efficiency by increasing the number of crops from each image instance, reduces training epochs by two orders of magnitude and achieves competitive performance on CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet-100 data sets using only one training epoch."
}