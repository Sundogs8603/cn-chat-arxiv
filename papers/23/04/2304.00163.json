{
    "title": "Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])",
    "abstract": "Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa",
    "link": "http://arxiv.org/abs/2304.00163",
    "context": "Title: Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])\nAbstract: Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa",
    "path": "papers/23/04/2304.00163.json",
    "total_tokens": 1023,
    "translated_title": "仿射马尔科夫博弈中的软Bellman平衡：前向解与逆向学习",
    "translated_abstract": "马尔科夫博弈在随机动态环境中模拟多个玩家之间的交互。每个玩家在马尔科夫博弈中最大化其期望的总折现奖励，该奖励取决于其他玩家的策略。我们提出了一类马尔科夫博弈，称为仿射马尔科夫博弈，在其中，仿射奖励函数耦合了玩家的行动。我们引入了一种新的解决方案概念，即软Bellman平衡，在其中，每个玩家都是有限理性的，并选择软Bellman策略，而不是像著名的Nash平衡概念中那样选择纯理性策略。我们提供了软Bellman平衡存在和唯一性的条件，并提出了一个非线性最小二乘算法来计算前向问题中的这种平衡。然后，我们通过投影梯度算法解决了推断玩家奖励参数的逆向博弈问题。在掠食者-猎物OpenAI Gym环境中的实验表明，使用软Bellman策略可以更有效地控制掠食者和猎物之间的交互。",
    "tldr": "本文提出了一种新的解决方案概念——软Bellman平衡，解决了仿射马尔科夫博弈中的多个玩家交互问题，并提出了一种非线性最小二乘算法来计算此平衡，同时通过投影梯度算法解决推断玩家奖励参数的问题。",
    "en_tdlr": "This paper proposed a novel solution concept called the soft-Bellman equilibrium in affine Markov games, providing a nonlinear least squares algorithm to compute the equilibrium and a projected gradient algorithm to solve the inverse game problem. The soft-Bellman equilibrium allows for bounded rationality and better control predator-prey interactions."
}