{
    "title": "Level Assembly as a Markov Decision Process. (arXiv:2304.13922v1 [cs.AI])",
    "abstract": "Many games feature a progression of levels that doesn't adapt to the player. This can be problematic because some players may get stuck if the progression is too difficult, while others may find it boring if the progression is too slow to get to more challenging levels. This can be addressed by building levels based on the player's performance and preferences. In this work, we formulate the problem of generating levels for a player as a Markov Decision Process (MDP) and use adaptive dynamic programming (ADP) to solve the MDP before assembling a level. We tested with two case studies and found that using an ADP outperforms two baselines. Furthermore, we experimented with player proxies and switched them in the middle of play, and we show that a simple modification prior to running ADP results in quick adaptation. By using ADP, which searches the entire MDP, we produce a dynamic progression of levels that adapts to the player.",
    "link": "http://arxiv.org/abs/2304.13922",
    "context": "Title: Level Assembly as a Markov Decision Process. (arXiv:2304.13922v1 [cs.AI])\nAbstract: Many games feature a progression of levels that doesn't adapt to the player. This can be problematic because some players may get stuck if the progression is too difficult, while others may find it boring if the progression is too slow to get to more challenging levels. This can be addressed by building levels based on the player's performance and preferences. In this work, we formulate the problem of generating levels for a player as a Markov Decision Process (MDP) and use adaptive dynamic programming (ADP) to solve the MDP before assembling a level. We tested with two case studies and found that using an ADP outperforms two baselines. Furthermore, we experimented with player proxies and switched them in the middle of play, and we show that a simple modification prior to running ADP results in quick adaptation. By using ADP, which searches the entire MDP, we produce a dynamic progression of levels that adapts to the player.",
    "path": "papers/23/04/2304.13922.json",
    "total_tokens": 842,
    "translated_title": "基于马尔科夫决策过程的关卡组装",
    "translated_abstract": "许多游戏都采用不适应玩家的关卡进度。这可能会导致玩家在进度过于困难时卡住，而在进度过慢时会感到无聊，导致不愿意等待更具挑战性的关卡。本文中，我们将为玩家生成关卡的问题转化为一个马尔科夫决策过程（MDP），并利用自适应动态规划（ADP）来解决MDP，然后再组装出关卡。我们进行了两个案例研究，发现使用ADP优于两个基线。此外，我们还进行了玩家代理的实验，并在游戏过程中切换了它们，结果表明在运行ADP之前进行简单修改可以快速适应。通过使用ADP，我们可以搜索整个MDP，并产生适应玩家的动态关卡进度。",
    "tldr": "本文中提出了基于马尔科夫决策过程的自适应动态规划方法来生成适应玩家的动态关卡进度，并在两个案例研究中展示了其优于两个基线的效果。",
    "en_tdlr": "This paper proposes an adaptive dynamic programming method based on Markov decision process to generate dynamic level progression adapted to players, and demonstrates its superiority over two baselines in two case studies."
}