{
    "title": "TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])",
    "abstract": "Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis",
    "link": "http://arxiv.org/abs/2304.08821",
    "context": "Title: TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])\nAbstract: Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis",
    "path": "papers/23/04/2304.08821.json",
    "total_tokens": 878,
    "translated_title": "TTIDA: 通过文本到文本模型和文本到图像模型进行可控生成数据增强",
    "translated_abstract": "数据增强已被证明是一种增补低资源数据集有用信息的有效方法。传统的增强技术，如噪声注入和图像变换，已被广泛使用。此外，生成式数据增强（GDA）已被证明能够产生更多样化和灵活的数据。虽然生成对抗网络（GAN）经常用于GDA，但与文本到图像扩散模型相比，它们缺乏多样性和可控性。在本文中，我们提出了TTIDA（文本到文本到图像数据增强），利用大规模预训练的文本到文本（T2T）和文本到图像（T2I）生成模型进行数据增强。通过将T2I模型的条件设置为T2T模型生成的详细描述，我们能够以灵活和可控的方式生成逼真的标记图像。在领域内分类、跨领域分类和图像字幕任务的实验中，展示了一致的结果。",
    "tldr": "本论文提出了一种名为TTIDA的生成式数据增强方法，利用文本到文本和文本到图像模型生成可控的逼真标记图像。",
    "en_tdlr": "This paper proposes a controllable generative data augmentation method called TTIDA, which leverages pre-trained Text-to-Text and Text-to-Image models to generate flexible and realistic labeled images."
}