{
    "title": "Multi-annotator Deep Learning: A Probabilistic Framework for Classification. (arXiv:2304.02539v1 [cs.LG])",
    "abstract": "Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of t",
    "link": "http://arxiv.org/abs/2304.02539",
    "context": "Title: Multi-annotator Deep Learning: A Probabilistic Framework for Classification. (arXiv:2304.02539v1 [cs.LG])\nAbstract: Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of t",
    "path": "papers/23/04/2304.02539.json",
    "total_tokens": 1058,
    "translated_title": "多注释深度学习：分类问题的概率框架",
    "translated_abstract": "使用深度神经网络解决复杂分类任务通常需要大量注释数据，然而由易错注释者（如众包工人）提供的对应类别标签有噪音。在这样的多注释监督学习中，训练标准的深度神经网络会导致次优表现。我们通过提出名为多注释深度学习（MaDL）的概率训练框架来解决这个问题。在端到端的学习方法中，联合训练一个地面真相模型和一个注释者表现模型。地面真相模型学习预测实例的真实类别，而注释者表现模型推断注释者表现的概率估计。模块化的网络结构使我们能够对注释者的表现做出不同的假设，例如，可以考虑类别或实例依赖性。此外，我们学习注释者嵌入以估计潜在空间中注释者的密度作为迁移学习的代理。我们的方法不仅使我们能够估计真实的类别标签，而且能够估计所分配标签的可信度。在合成数据和真实数据上的实验表明，在注释者性能假设不同的情况下，我们的方法都具有有效性。",
    "tldr": "该论文提出了一个名为多注释深度学习（MaDL）的概率训练框架，可以在由易错注释者提供的有噪音类别标签上进行端到端的联合训练，并有效地解决多注释监督学习中的次优表现问题。",
    "en_tdlr": "This paper proposes a probabilistic training framework called Multi-annotator Deep Learning (MaDL), which can jointly train a ground truth model and an annotator performance model in an end-to-end learning approach and effectively solve the subpar performance issue in multi-annotator supervised learning with noisy class labels provided by error-prone annotators."
}