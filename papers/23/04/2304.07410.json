{
    "title": "Text-Conditional Contextualized Avatars For Zero-Shot Personalization. (arXiv:2304.07410v1 [cs.CV])",
    "abstract": "Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user's identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to le",
    "link": "http://arxiv.org/abs/2304.07410",
    "context": "Title: Text-Conditional Contextualized Avatars For Zero-Shot Personalization. (arXiv:2304.07410v1 [cs.CV])\nAbstract: Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user's identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to le",
    "path": "papers/23/04/2304.07410.json",
    "total_tokens": 974,
    "translated_title": "为零样本个性化提供条件化文本人物形象",
    "translated_abstract": "近期的大型文本到图像生成模型在合成图像的质量、真实性和多样性方面取得了显著进展，并且使用户通过语言来控制生成的内容。然而，这些生成模型的个性化方面仍然具有挑战性并且未经深入探索。在本研究中，我们提出了一种能够以令人愉悦的方式捕捉用户身份的人物形象，从而实现图像生成的个性化管道。我们的管道是零样本、人物形象纹理和风格不可知，并且不需要对人物形象进行训练 - 它可以扩展到数百万用户，这些用户可以用他们的人物形象生成场景。为了以忠实于给定的文本提示的姿势呈现人物形象，我们提出了一种新颖的文本到3D姿势扩散模型，该模型在一个策划的大规模野外人类姿势数据集上进行训练，显著提高了SOTA文本到动作模型的性能。我们首次展示了利用大规模图像数据集来提高人物形象生成的个性化效果。",
    "tldr": "本研究提出了一个零样本管道，使得可以捕捉用户身份的人物形象以令人愉悦的方式进行图像生成的个性化处理，并在大规模图像数据集上提高了性能，扩展适用于数百万用户。",
    "en_tdlr": "This work proposes a zero-shot pipeline that enables delightful personalization of image generation with avatars capturing a user's identity and significantly improves the performance of SOTA text-to-motion models with a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses."
}