{
    "title": "Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering. (arXiv:2304.13911v1 [cs.AI])",
    "abstract": "We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that",
    "link": "http://arxiv.org/abs/2304.13911",
    "context": "Title: Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering. (arXiv:2304.13911v1 [cs.AI])\nAbstract: We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that",
    "path": "papers/23/04/2304.13911.json",
    "total_tokens": 897,
    "translated_title": "提高LLM答案准确度的联邦提示和链式推理",
    "translated_abstract": "本文研究如何使用基于云的大型语言模型（LLMs）增强分布式用户提出的常见问题的回答精度。我们的研究侧重于典型情况，即用户询问涉及相同的数学推理步骤和问题解决过程的相似查询。由于LLMs独立问题的零-shot提示的准确性不尽如人意，我们提出了使用自洽性（SC）和链式思考（CoT）技术来改进分布式同义问题的方法。具体而言，我们首先从众包数据库中检索同义问题，并创建一个联邦问题池。我们称这些具有相同或不同参数的联邦同义问题为SP问题或DP问题，分别。我们将我们的方法称为Fed-SP-SC和Fed-DP-CoT，它们可以为所有用户查询生成更准确的答案，而不需要复杂的模型调整。通过大量实验证明",
    "tldr": "本论文提出一种名为Fed-SP-SC和Fed-DP-CoT的技术，通过联邦提示和链式推理改进分布式同义问题的准确性，从而提高基于云的大型语言模型（LLMs）回答常见问题的精度，并进行了充分实验验证。",
    "en_tdlr": "This paper proposes a technique named Fed-SP-SC and Fed-DP-CoT to improve the accuracy of distributed synonymous questions through fed-prompting and chain-of-thought reasoning, thereby enhancing the precision of cloud-based Large Language Models (LLMs) in answering frequently asked questions, and is extensively validated through experiments."
}