{
    "title": "SparDL: Distributed Deep Learning Training with Efficient Sparse Communication",
    "abstract": "arXiv:2304.00737v2 Announce Type: replace  Abstract: Top-k sparsification has recently been widely used to reduce the communication volume in distributed deep learning. However, due to the Sparse Gradient Accumulation (SGA) dilemma, the performance of top-k sparsification still has limitations. Recently, a few methods have been put forward to handle the SGA dilemma. Regrettably, even the state-of-the-art method suffers from several drawbacks, e.g., it relies on an inefficient communication algorithm and requires extra transmission steps. Motivated by the limitations of existing methods, we propose a novel efficient sparse communication framework, called SparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is based on an efficient Reduce-Scatter model, to handle the SGA dilemma without additional communication operations. Besides, to further reduce the latency cost and improve the efficiency of SparDL, we propose the Spar-All-Gather algorithm. Moreover, we propose ",
    "link": "https://arxiv.org/abs/2304.00737",
    "context": "Title: SparDL: Distributed Deep Learning Training with Efficient Sparse Communication\nAbstract: arXiv:2304.00737v2 Announce Type: replace  Abstract: Top-k sparsification has recently been widely used to reduce the communication volume in distributed deep learning. However, due to the Sparse Gradient Accumulation (SGA) dilemma, the performance of top-k sparsification still has limitations. Recently, a few methods have been put forward to handle the SGA dilemma. Regrettably, even the state-of-the-art method suffers from several drawbacks, e.g., it relies on an inefficient communication algorithm and requires extra transmission steps. Motivated by the limitations of existing methods, we propose a novel efficient sparse communication framework, called SparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is based on an efficient Reduce-Scatter model, to handle the SGA dilemma without additional communication operations. Besides, to further reduce the latency cost and improve the efficiency of SparDL, we propose the Spar-All-Gather algorithm. Moreover, we propose ",
    "path": "papers/23/04/2304.00737.json",
    "total_tokens": 878,
    "translated_title": "SparDL：高效稀疏通信的分布式深度学习训练",
    "translated_abstract": "近年来，Top-k稀疏化被广泛应用于减少分布式深度学习中的通信量，然而由于稀疏梯度累积（SGA）困境，Top-k稀疏化的性能仍然存在局限性。为了应对SGA困境，一些方法被提出，然而即使最先进的方法也存在一些缺陷，例如依赖低效的通信算法，需要额外的传输步骤。受现有方法局限性的启发，我们提出了一种新颖高效的稀疏通信框架，称为SparDL。具体来说，SparDL使用了Spar-Reduce-Scatter算法，基于高效的Reduce-Scatter模型处理SGA困境而不需要额外的通信操作。此外，为了进一步降低延迟成本并提高SparDL的效率，我们提出了Spar-All-Gather算法。",
    "tldr": "SparDL提出了一种高效稀疏通信框架，使用Spar-Reduce-Scatter和Spar-All-Gather算法来解决稀疏梯度累积困境，避免依赖低效通信算法和额外传输步骤。",
    "en_tdlr": "SparDL proposes an efficient sparse communication framework using Spar-Reduce-Scatter and Spar-All-Gather algorithms to address the Sparse Gradient Accumulation dilemma, avoiding reliance on inefficient communication algorithms and extra transmission steps."
}