{
    "title": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])",
    "abstract": "Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the ",
    "link": "http://arxiv.org/abs/2304.03916",
    "context": "Title: Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])\nAbstract: Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the ",
    "path": "papers/23/04/2304.03916.json",
    "total_tokens": 892,
    "translated_title": "在微调时缓解多模态模型中的错误相关性",
    "translated_abstract": "损害模型泛化能力或导致模型基于错误原因的错误相关性是实际部署面临的主要鲁棒性问题之一。然而，在预训练大型模型期间缓解这些相关性可能成本高昂且不切实际，特别是对于没有高性能计算资源的人来说。本文提出了一种新方法，以解决特定领域的微调期间的错误相关性。针对多模态模型（例如CLIP），所提出的方法利用这些模型中的不同模态来检测并明确区分受影响类别的错误属性，通过表达语言的多模态对比损失函数来实现。我们在CLIP上进行的实验证明和深入的可视化显示，这种介入能够有效地提高模型精度，而不存在错误属性，并将模型指向目标领域的有意义特征。",
    "tldr": "本文提出了一种使用多模态对比损失函数的方法，通过在微调期间检测和明确区分受影响类别的错误属性，缓解多模态模型的错误相关性，同时提高模型精度和指向目标领域的有意义特征。",
    "en_tdlr": "This paper proposes a novel approach using a multi-modal contrastive loss function to mitigate spurious correlations during fine-tuning in multi-modal models. The method leverages different modalities in the model to explicitly set apart spurious attributes from the affected class, and has been shown to effectively improve model accuracy and direct attention to meaningful features in the target domain."
}