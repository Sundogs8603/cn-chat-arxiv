{
    "title": "REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])",
    "abstract": "Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su",
    "link": "http://arxiv.org/abs/2304.01904",
    "context": "Title: REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])\nAbstract: Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su",
    "path": "papers/23/04/2304.01904.json",
    "total_tokens": 909,
    "translated_title": "REFINER: 基于中间表示的推理反馈。",
    "translated_abstract": "最近语言模型在推理任务上表现出了remarkable的性能，通过显式生成中间推理步骤，例如链式思考提示等。然而，这些中间推理步骤可能并不是根据初始上下文得出的适当推导，从而导致不正确的最终预测。在这里，我们介绍了REFINER，这是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互。具体而言，批评家提供了结构化反馈，推理语言模型使用它来迭代改进其中间参数。REFINER的三个不同推理任务的实证评估显示出了与基线具有可比规模的语言模型相比的显着改进。此外，当使用GPT3.5作为推理器时，经过训练的批评家显着改善了推理而无需微调推理器。最后，我们的批评模型是在没有昂贵的人类参与数据的情况下进行的，但可以通过对新颖上下文提供低成本反馈进行继续改进。",
    "tldr": "REFINER 是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互，其在三个不同推理任务上取得了显着改进。",
    "en_tdlr": "REFINER is a framework for fine-tuning language models to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning, which shows significant improvements on three diverse reasoning tasks."
}