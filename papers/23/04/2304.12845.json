{
    "title": "(Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])",
    "abstract": "In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the",
    "link": "http://arxiv.org/abs/2304.12845",
    "context": "Title: (Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])\nAbstract: In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the",
    "path": "papers/23/04/2304.12845.json",
    "total_tokens": 937,
    "translated_title": "（本地）差分隐私对公平性没有带来不平等影响",
    "translated_abstract": "近年来，本地差分隐私（LDP）作为强大隐私保护方法，在实际应用中得到了广泛的应用。通过 LDP，用户可以在将数据传输出去前在设备上对其进行扰动。然而，随着在各个行业中收集多个敏感信息的情况越来越普遍，仅收集单个敏感属性可能已经不足以保障用户的隐私。数据中的相关属性仍然可能导致对敏感属性的推断。本文通过实验证明了在 LDP 下收集多个敏感属性对公平性的影响。我们提出了一个新的隐私预算分配方案，考虑到敏感属性的不同域大小。在我们的实验中，这通常比现有最新解决方案的隐私-效用-公平性权衡方式更好。我们的结果表明，LDP 在学习问题中带来了略微改善的公平性，而不会明显影响性能。",
    "tldr": "本文研究了在 LDP 下收集多个敏感属性对公平性的影响并提出了考虑域大小的新的隐私预算分配方案，实验表明该方案在隐私、效用和公平性方面均优于最新的解决方案，LDP 带来了略微改善的公平性而不会明显影响性能。",
    "en_tdlr": "This paper studies the impact of collecting multiple sensitive attributes under Local Differential Privacy (LDP) on fairness and proposes a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes, which generally leads to a better privacy-utility-fairness trade-off in experiments, showing that LDP improves fairness slightly without significantly affecting performance."
}