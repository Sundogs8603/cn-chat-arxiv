{
    "title": "A Survey on Recent Teacher-student Learning Studies. (arXiv:2304.04615v1 [cs.LG])",
    "abstract": "Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.",
    "link": "http://arxiv.org/abs/2304.04615",
    "context": "Title: A Survey on Recent Teacher-student Learning Studies. (arXiv:2304.04615v1 [cs.LG])\nAbstract: Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.",
    "path": "papers/23/04/2304.04615.json",
    "total_tokens": 933,
    "translated_title": "最近关于师生学习的研究综述",
    "translated_abstract": "知识蒸馏是一种将复杂深度神经网络（DNN）的知识传递给更小更快的DNN的方法，同时保持其准确性。最近的知识蒸馏变体包括教学助理蒸馏，课程蒸馏，掩码蒸馏和解耦蒸馏，旨在通过引入附加组件或更改学习过程来改善知识蒸馏的性能。教学助理蒸馏涉及中间模型教学助理，而课程蒸馏则遵循类似于人类教育的课程。掩码蒸馏专注于传递老师学到的注意力机制，而解耦蒸馏将蒸馏损失与任务损失分离。总体而言，这些知识蒸馏的变体已经显示出在改善知识蒸馏性能方面有希望的结果。",
    "tldr": "本文综述了最近关于师生学习的研究进展，重点讨论了知识蒸馏的变体，如教学助理蒸馏、课程蒸馏、掩码蒸馏和解耦蒸馏等，这些变体通过引入新的组件或改变学习过程来提高知识蒸馏的性能，已经取得了有希望的结果。",
    "en_tdlr": "This survey paper discusses recent advances in teacher-student learning, focusing on variants of knowledge distillation, such as teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which have shown promising results in improving the performance of knowledge distillation by introducing additional components or by changing the learning process."
}