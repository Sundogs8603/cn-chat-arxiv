{
    "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])",
    "abstract": "How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This ",
    "link": "http://arxiv.org/abs/2304.15010",
    "context": "Title: LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])\nAbstract: How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This ",
    "path": "papers/23/04/2304.15010.json",
    "total_tokens": 774,
    "translated_title": "LLaMA-Adapter V2: 参数高效的视觉指令模型",
    "translated_abstract": "近期的研究方向是如何将大型语言模型（LLMs）高效地转化为指令跟随者，而为多模态推理训练LLM的研究仍然较少。虽然最近的LLaMA-Adapter证明了用LLM处理视觉输入的潜力，但它仍然不能很好地推广到开放式视觉指令，并且落后于GPT-4。本文提出了LLaMA-Adapter V2，这是一个参数高效的视觉指令模型。",
    "tldr": "本文提出了LLaMA-Adapter V2，这是一个参数高效的视觉指令模型，通过解锁更多可学习的参数，早期融合策略和联合训练策略，能够更好地处理视觉输入和精确地执行开放式视觉指令。",
    "en_tdlr": "This paper presents LLaMA-Adapter V2, a parameter-efficient visual instruction model which can better handle visual inputs and accurately perform open-ended visual instructions through unlocking more learnable parameters, early fusion strategy and joint training paradigm."
}