{
    "title": "NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs. (arXiv:2304.05866v1 [cs.CV])",
    "abstract": "StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by $\\sim 19\\%$ on FID, establishing a new state-of-the-art.",
    "link": "http://arxiv.org/abs/2304.05866",
    "context": "Title: NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs. (arXiv:2304.05866v1 [cs.CV])\nAbstract: StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by $\\sim 19\\%$ on FID, establishing a new state-of-the-art.",
    "path": "papers/23/04/2304.05866.json",
    "total_tokens": 970,
    "translated_title": "NoisyTwins: 通过StyleGANs实现一致类别和多样化图像生成",
    "translated_abstract": "StyleGANs是可控图片生成的前沿技术，其产生了一个语义解耦的潜空间，适用于图像编辑和操作。但是，当通过类别条件在大规模长尾数据集上进行训练时，StyleGANs的性能严重下降。我们发现导致性能下降的一个原因是在W潜空间中每个类别的潜空间坍塌。使用NoisyTwins，我们首先引入了一种有效且低成本的类别嵌入增强策略，然后基于W空间的自我监督来去相关化潜空间。这种去相关化缓解了坍塌，确保我们的方法在图像生成中保留了类内差异和一致性。我们展示了我们的方法在ImageNet-LT和iNaturalist 2019大规模真实世界长尾数据集上的有效性，在FID上优于其他方法约19％，建立了新的最高纪录。",
    "tldr": "本文提出了一种名为NoisyTwins的方法，可以通过增强类别嵌入并使用自我监督在W空间中去相关化潜空间来改善大规模长尾数据集上StyleGANs的性能表现，从而在图像生成中保留了类内多样性和一致性。",
    "en_tdlr": "This paper proposes a method called NoisyTwins, which improves the performance of StyleGANs on large-scale long-tailed datasets by enhancing class embeddings and decorrelating the latent space using self-supervision in the W space, thereby preserving intra-class diversity with class-consistency in image generation. It establishes a new state-of-the-art on ImageNet-LT and iNaturalist 2019 datasets by outperforming other methods by about 19% on FID."
}