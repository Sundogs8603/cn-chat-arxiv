{
    "title": "Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)",
    "abstract": "Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU",
    "link": "http://arxiv.org/abs/2304.04660",
    "context": "Title: Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)\nAbstract: Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU",
    "path": "papers/23/04/2304.04660.json",
    "total_tokens": 925,
    "translated_title": "数据增强中基于不确定性的轨迹截断在离线强化学习中的应用",
    "translated_abstract": "模型驱动的离线强化学习算法通常能够从固定大小的数据集中学到好的策略，甚至一些质量较差的数据集。然而，生成的合成样本是否可靠无法得到保证（例如，一些合成样本可能位于静态数据集支持区域之外）。为了解决这个问题，我们提出了具有不确定性的轨迹截断（TATU）方法，如果累积不确定性超过阈值，则自适应截断合成轨迹。我们理论上证明了TATU的性能边界以证明其优势。为了在实证上显示出TATU的优势，我们首先将其与两个经典的模型驱动离线强化学习算法MOPO和COMBO相结合。此外，我们还将TATU与多个现成的无模型离线强化学习算法（例如BCQ）进行整合。在D4RL基准测试上的实验结果表明，TATU的表现优越。",
    "tldr": "本研究提出了一种基于不确定性的轨迹截断方法（TATU），用于解决模型驱动的离线强化学习中生成样本不可靠的问题。实验证明TATU相较于其他方法表现优越。",
    "en_tdlr": "This study proposes a trajectory truncation method based on uncertainty (TATU) to address the issue of unreliable generated samples in model-driven offline reinforcement learning. Experimental results demonstrate the superiority of TATU compared to other methods."
}