{
    "title": "When do you need Chain-of-Thought Prompting for ChatGPT?. (arXiv:2304.03262v1 [cs.AI])",
    "abstract": "Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step reasoning from Large Language Models~(LLMs). For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7\\% to 78.7\\%. However, it is not clear whether CoT is still effective on more recent instruction finetuned (IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer effective for certain tasks such as arithmetic reasoning while still keeping effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT usually achieves the best performance and can generate CoT even without being instructed to do so. Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instruct",
    "link": "http://arxiv.org/abs/2304.03262",
    "context": "Title: When do you need Chain-of-Thought Prompting for ChatGPT?. (arXiv:2304.03262v1 [cs.AI])\nAbstract: Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step reasoning from Large Language Models~(LLMs). For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7\\% to 78.7\\%. However, it is not clear whether CoT is still effective on more recent instruction finetuned (IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer effective for certain tasks such as arithmetic reasoning while still keeping effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT usually achieves the best performance and can generate CoT even without being instructed to do so. Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instruct",
    "path": "papers/23/04/2304.03262.json",
    "total_tokens": 1095,
    "translated_title": "ChatGPT何时需要连续思考提示？",
    "translated_abstract": "连续思考提示可以有效地引出大型语言模型的复杂多步推理，例如，在每个输入查询中添加连续思考提示“让我们逐步思考”，可以将GPT-3在MultiArith数据集上的准确性从17.7％提高到78.7％。然而，不清楚连续思考提示是否对更近期的指令微调型大型语言模型（如ChatGPT）仍然有效。令人惊讶的是，在ChatGPT上，连续思考提示对某些任务（如算术推理）不再有效，但对其他推理任务仍然有效。此外，在前者的任务上，ChatGPT通常表现最佳，甚至可以在没有被指示的情况下生成连续思考提示。因此，ChatGPT可能已经通过连续思考提示在这些任务上进行了训练，并且即使没有连续思考提示，也会在应用于相同的查询时隐含地遵循此类提示。我们的分析反映了大型语言模型在受训练推理时存在过拟合/偏差的潜在风险，并强调了评估和改进连续思考提示鲁棒性在更多任务和模型上的必要性。",
    "tldr": "该论文讨论了连续思考提示（CoT）对ChatGPT的有效性，发现在算术推理等任务中，这种提示不再有效，但在其他推理任务中仍有效。分析表明，在大型语言模型受训练推理时存在过拟合/偏差的风险，需要在更多任务和模型上评估和改进连续思考提示的鲁棒性。",
    "en_tdlr": "This paper discusses the effectiveness of Chain-of-Thought Prompting (CoT) on ChatGPT and finds that it is no longer effective in certain tasks such as arithmetic reasoning, but still effective in other reasoning tasks. The analysis reflects a potential risk of overfitting/bias toward instructed reasoning for Large Language Models and highlights the need for evaluating and improving CoT robustness on a more diverse set of tasks and models."
}