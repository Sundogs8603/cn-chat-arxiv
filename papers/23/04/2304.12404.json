{
    "title": "Semantic Tokenizer for Enhanced Natural Language Processing. (arXiv:2304.12404v1 [cs.CL])",
    "abstract": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X ",
    "link": "http://arxiv.org/abs/2304.12404",
    "context": "Title: Semantic Tokenizer for Enhanced Natural Language Processing. (arXiv:2304.12404v1 [cs.CL])\nAbstract: Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X ",
    "path": "papers/23/04/2304.12404.json",
    "total_tokens": 913,
    "translated_title": "基于语义的标记器用于增强自然语言处理",
    "translated_abstract": "传统上，自然语言处理的性能改进一直集中于改进模型和增加模型参数数量。NLP词汇构建一直专注于通过子词规则最大化表示的单词数量。我们提出了一种利用语义引导词汇构建的新型标记器。标记器包括使用词干提取增强子词形成的训练器。进一步优化和适应被实现以最小化不能编码的单词数量。编码器被更新以与训练器集成。该标记器已被实现为SentencePiece标记器的替代品。新标记器的词形数量超过了之前的两倍。增强词汇显着提高了NLP模型的收敛速度，并改善了单词和句子嵌入的质量。我们的实验结果表明，在使用BERT-base的两个Glue任务中，我们的性能最佳，比其他模型提高了50倍以上。",
    "tldr": "本文提出一种基于语义的标记器，使用训练器来增强子词形成，优化和适应以最小化不能编码的单词数量。该标记器的复杂性超过了之前的两倍，但显着提高了NLP模型的收敛速度，并改善了单词和句子嵌入的质量。",
    "en_tdlr": "This paper proposes a semantic tokenizer that uses a trainer to enhance subword formation, with optimization and adaptation to minimize the number of un-encodable words. The tokenizer more than doubles the number of wordforms represented in the vocabulary and significantly improves NLP model convergence and embedding quality. Top performance on two Glue tasks using BERT-base was achieved, improving on models by over 50X."
}