{
    "title": "Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])",
    "abstract": "We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\\%$ relative and improves generic rare word error rate by $3\\%$ relative compared to the baselines.",
    "link": "http://arxiv.org/abs/2304.01905",
    "context": "Title: Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])\nAbstract: We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\\%$ relative and improves generic rare word error rate by $3\\%$ relative compared to the baselines.",
    "path": "papers/23/04/2304.01905.json",
    "total_tokens": 903,
    "translated_title": "双关注神经变换器用于语音识别时的高效唤醒词识别",
    "translated_abstract": "本文提出了一种称为双关注神经网络的架构，旨在提高唤醒词识别的准确率并改善语音识别任务的推理时间。该架构通过利用唤醒词检测来选择哪个注意力网络执行输入音频帧的运行时计算路径。使用这种方法，作者有效提高了唤醒词识别的准确性，并定义了浮点运算（FLOPs）的运行时计算成本。在使用作者的内部数据集时，作者证明了所提出的双关注网络可以将唤醒词音频帧的计算成本降低$90\\%$，而参数数量仅增加$1\\%$。与基线相比，该架构提高了唤醒词F1得分$16\\%$，并将一般的罕见词错误率提高了$3\\%$。",
    "tldr": "本文介绍了一种新的“双关注神经变换器”，可以通过优化唤醒词检测来选择计算路径，从而提高唤醒词的准确性和推理时间效率，并且计算成本可以降低90％而仅增加1％的参数。这种架构可以在语音识别领域中大有裨益。",
    "en_tdlr": "This paper presents a novel \"Dual-Attention Neural Transducer\" architecture that dynamically switches compute paths by optimizing wake word detection, improving wake word accuracy and inference time while reducing compute cost by 90% with only a 1% increase in parameters. It has potential benefits in the field of speech recognition."
}