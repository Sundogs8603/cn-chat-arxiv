{
    "title": "A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])",
    "abstract": "The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u",
    "link": "http://arxiv.org/abs/2304.05073",
    "context": "Title: A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])\nAbstract: The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u",
    "path": "papers/23/04/2304.05073.json",
    "total_tokens": 932,
    "translated_title": "折扣强化学习中的采样和估计故事",
    "translated_abstract": "折扣强化学习中最相关的问题包括在马尔可夫奖励过程的稳态分布下对函数均值进行估计，例如策略评估中的预期回报或策略优化中的策略梯度。在实践中，这些估计通过有限地进行周期采样来产生，这种采样方式忽略了马尔可夫过程的混合特性。目前还不清楚这种实际和理论设置之间的不匹配如何影响估计，文献中也缺乏对周期采样的缺陷以及如何最优地进行周期采样的正式研究。在本文中，我们提出了折扣均值估计问题的最小值上界，明确地将估计误差与马尔可夫过程的混合特性和折扣因子联系起来。然后，我们对一组显著估计器及其对应的采样程序进行了统计分析，包括通常使用的有限时间估计器。",
    "tldr": "本文通过最小值上界提出了折扣均值估计问题的估计误差与马尔可夫过程混合特性和折扣因子之间的明确联系，并对一组显著估计器及其对应的采样程序进行了统计分析。",
    "en_tdlr": "This paper establishes a minimax lower bound on the estimation error of discounted mean in reinforcement learning, explicitly connecting the error with the mixing properties of the underlying Markov process and the discount factor. It provides a statistical analysis on a set of notable estimators and corresponding sampling procedures, including commonly used finite-horizon estimators."
}