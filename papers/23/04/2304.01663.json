{
    "title": "On the Stability-Plasticity Dilemma of Class-Incremental Learning. (arXiv:2304.01663v1 [cs.CV])",
    "abstract": "A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initi",
    "link": "http://arxiv.org/abs/2304.01663",
    "context": "Title: On the Stability-Plasticity Dilemma of Class-Incremental Learning. (arXiv:2304.01663v1 [cs.CV])\nAbstract: A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initi",
    "path": "papers/23/04/2304.01663.json",
    "total_tokens": 870,
    "translated_title": "对类增量学习的稳定性-可塑性困境的探讨",
    "translated_abstract": "类增量学习的主要目标是在稳定性和可塑性之间取得平衡，即模型应该既稳定到足以保留从先前看到的类中学到的知识，又应该具备足够的可塑性，可以学习新的类别。本文旨在探讨最近的类增量学习算法如何有效地解决稳定性-可塑性权衡问题。我们建立了度量特征表示的稳定性和可塑性的分析工具，并利用这些工具来研究各种算法在大规模的类增量基准测试中训练的模型。令人惊讶的是，我们发现大部分类增量学习算法都非常倾向于稳定性而不是可塑性，以至于训练在初始类上的模型的特征提取器几乎没有改变。",
    "tldr": "本文探讨类增量学习算法如何有效解决稳定性-可塑性权衡问题，发现大多数算法更倾向于保持稳定性而不是可塑性，并对训练在初始类上的模型的特征提取器几乎没有改变。",
    "en_tdlr": "This paper explores how recent class-incremental learning algorithms effectively address the stability-plasticity trade-off, finding that the majority of algorithms heavily favor stability over plasticity and feature extractor of models trained on initial classes almost remains unchanged."
}