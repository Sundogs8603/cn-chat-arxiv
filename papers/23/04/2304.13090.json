{
    "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers. (arXiv:2304.13090v1 [cs.LG])",
    "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also ca",
    "link": "http://arxiv.org/abs/2304.13090",
    "context": "Title: Model Extraction Attacks Against Reinforcement Learning Based Controllers. (arXiv:2304.13090v1 [cs.LG])\nAbstract: We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also ca",
    "path": "papers/23/04/2304.13090.json",
    "total_tokens": 927,
    "translated_title": "面向强化学习控制器的模型提取攻击",
    "translated_abstract": "我们提出了在网络-物理系统中的模型提取攻击问题，攻击者试图估计或获取系统的反馈控制器。提取或估计控制器可以给攻击者带来无与伦比的优势，因为它允许他们预测系统的未来控制动作并相应地计划攻击。因此，了解攻击者执行此类攻击的能力非常重要。在本文中，我们着重研究了使用强化学习算法训练深度神经网络控制器并用于控制随机系统的情况。我们扮演攻击者的角色，旨在估计这种未知的深度神经网络控制器，并提出了一个两阶段算法。在第一阶段，即离线阶段，攻击者使用关于RL奖励函数和系统动态的侧信道信息来识别一组候选的未知DNN估计。在第二阶段，也称为在线阶段，攻击者使用轨迹优化算法选择最佳估计。",
    "tldr": "本文研究了在强化学习控制器中的模型提取攻击，提出了一个两阶段算法，第一阶段使用侧信道信息进行估计候选项的识别，第二阶段使用轨迹优化算法选择最佳估计。",
    "en_tdlr": "This paper investigates model-extraction attacks in reinforcement learning controllers, proposing a two-phase algorithm with the first phase identifying candidate estimates using side-channel information and the second phase using trajectory optimization to select the best estimate."
}