{
    "title": "Escaping the sentence-level paradigm in machine translation. (arXiv:2304.12959v1 [cs.CL])",
    "abstract": "It is well-known that document context is vital for resolving a range of translation ambiguities, and in fact the document setting is the most natural setting for nearly all translation. It is therefore unfortunate that machine translation -- both research and production -- largely remains stuck in a decades-old sentence-level translation paradigm. It is also an increasingly glaring problem in light of competitive pressure from large language models, which are natively document-based. Much work in document-context machine translation exists, but for various reasons has been unable to catch hold. This paper suggests a path out of this rut by addressing three impediments at once: what architectures should we use? where do we get document-level information for training them? and how do we know whether they are any good? In contrast to work on specialized architectures, we show that the standard Transformer architecture is sufficient, provided it has enough capacity. Next, we address the t",
    "link": "http://arxiv.org/abs/2304.12959",
    "context": "Title: Escaping the sentence-level paradigm in machine translation. (arXiv:2304.12959v1 [cs.CL])\nAbstract: It is well-known that document context is vital for resolving a range of translation ambiguities, and in fact the document setting is the most natural setting for nearly all translation. It is therefore unfortunate that machine translation -- both research and production -- largely remains stuck in a decades-old sentence-level translation paradigm. It is also an increasingly glaring problem in light of competitive pressure from large language models, which are natively document-based. Much work in document-context machine translation exists, but for various reasons has been unable to catch hold. This paper suggests a path out of this rut by addressing three impediments at once: what architectures should we use? where do we get document-level information for training them? and how do we know whether they are any good? In contrast to work on specialized architectures, we show that the standard Transformer architecture is sufficient, provided it has enough capacity. Next, we address the t",
    "path": "papers/23/04/2304.12959.json",
    "total_tokens": 1039,
    "translated_title": "逃离机器翻译中句子级范式的限制",
    "translated_abstract": "众所周知，文档语境对于解决一系列翻译模糊性至关重要，事实上，文档设置几乎是所有翻译的自然设置。然而，机器翻译（包括研究和生产）在几十年前的句子级翻译范式中仍然停滞不前，这是一个越来越明显的问题，由于来自大型语言模型的竞争压力，这些模型天生就是基于文档的。本文提出了一种摆脱这种困境的方法，同时解决了三个障碍：我们应该使用什么架构？我们从哪里获取训练它们的文档级信息？以及我们如何知道它们是否足够好？",
    "tldr": "本文提出了一种摆脱机器翻译中句子级范式限制的方法，通过处理三个障碍来实现：使用足够大的标准Transformer架构、引入一种简单而有效的技术来将文档级信息转化为适合训练的形式、基于自动文档分类的评估协议来有效地识别文档级翻译质量。在两个非常不同的文档级翻译任务上，我们的实验表明，在此数据上训练的Transformer模型明显优于强大的基线模型。",
    "en_tdlr": "This paper proposes a method of escaping the sentence-level paradigm in machine translation by addressing three impediments: using a sufficient size standard Transformer architecture, introducing a simple and effective technique for converting document-level information into a form amenable to training, and introducing an evaluation protocol for document-level machine translation. The experiments show that the model outperforms strong baselines on two document-level translation tasks."
}