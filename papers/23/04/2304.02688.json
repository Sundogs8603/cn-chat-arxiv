{
    "title": "Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability. (arXiv:2304.02688v1 [cs.LG])",
    "abstract": "Transferability is the property of adversarial examples to be misclassified by other models than the surrogate model for which they were crafted. Previous research has shown that transferability is substantially increased when the training of the surrogate model has been early stopped. A common hypothesis to explain this is that the later training epochs are when models learn the non-robust features that adversarial attacks exploit. Hence, an early stopped model is more robust (hence, a better surrogate) than fully trained models. We demonstrate that the reasons why early stopping improves transferability lie in the side effects it has on the learning dynamics of the model. We first show that early stopping benefits transferability even on models learning from data with non-robust features. We then establish links between transferability and the exploration of the loss landscape in the parameter space, on which early stopping has an inherent effect. More precisely, we observe that tran",
    "link": "http://arxiv.org/abs/2304.02688",
    "context": "Title: Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability. (arXiv:2304.02688v1 [cs.LG])\nAbstract: Transferability is the property of adversarial examples to be misclassified by other models than the surrogate model for which they were crafted. Previous research has shown that transferability is substantially increased when the training of the surrogate model has been early stopped. A common hypothesis to explain this is that the later training epochs are when models learn the non-robust features that adversarial attacks exploit. Hence, an early stopped model is more robust (hence, a better surrogate) than fully trained models. We demonstrate that the reasons why early stopping improves transferability lie in the side effects it has on the learning dynamics of the model. We first show that early stopping benefits transferability even on models learning from data with non-robust features. We then establish links between transferability and the exploration of the loss landscape in the parameter space, on which early stopping has an inherent effect. More precisely, we observe that tran",
    "path": "papers/23/04/2304.02688.json",
    "total_tokens": 881,
    "translated_abstract": "可转移性是指对抗性样本被非原模型误分类的特性。先前的研究表明当代理模型的训练被早停止时，可转移性会大幅提高。通常的假设是模型在训练后期学习到了对抗攻击利用的非鲁棒性特征，因此，早停止模型比完全训练的模型更加鲁棒（因此是更好的代理）。我们证明了早停止如何改善可转移性的原因在于其对模型学习动态的副作用。我们首先展示早停止即使对于学习具有非鲁棒性特征数据的模型也有益于可转移性。然后我们建立了转移性和参数空间中损失函数表面探索之间的联系，早停止对其有着内在作用。更准确地说，我们观察到在训练过程中，损失函数表面的平坦区域会影响可转移性。",
    "tldr": "本论文建立了早停止如何提高可转移性的原因，即在学习动态的副作用中，早停止对损失函数表面的平坦区域有着内在作用。"
}