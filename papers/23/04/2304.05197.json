{
    "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])",
    "abstract": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.",
    "link": "http://arxiv.org/abs/2304.05197",
    "context": "Title: Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])\nAbstract: With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.",
    "path": "papers/23/04/2304.05197.json",
    "total_tokens": 909,
    "translated_title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
    "translated_abstract": "随着大型语言模型（LLMs）的迅速发展，许多下游NLP任务可以通过良好的提示得到很好的解决。尽管模型开发人员和研究人员努力确保避免从LLMs生成有害内容，但仍然难以引导AI生成的内容（AIGC）为人类带来好处。由于强大的LLMs正在吞噬来自各个领域的现有文本数据（例如，GPT-3训练了45TB的文本），因此人们自然会怀疑训练数据中是否包含私人信息以及这些LLMs及其下游应用程序可以带来什么隐私威胁。在本文中，我们研究了来自OpenAI的模型API和通过ChatGPT增强的New Bing所带来的隐私威胁，并显示应用程序集成的LLMs可能导致比以往更严重的隐私威胁。为此，我们进行了大量实验证明我们的说法，并讨论LLMs的隐私影响。",
    "tldr": "本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。"
}