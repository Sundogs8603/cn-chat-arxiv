{
    "title": "Martingale Posterior Neural Processes. (arXiv:2304.09431v1 [cs.LG])",
    "abstract": "A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more \"data-driven\" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated fu",
    "link": "http://arxiv.org/abs/2304.09431",
    "context": "Title: Martingale Posterior Neural Processes. (arXiv:2304.09431v1 [cs.LG])\nAbstract: A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more \"data-driven\" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated fu",
    "path": "papers/23/04/2304.09431.json",
    "total_tokens": 820,
    "translated_title": "鞅后验神经过程",
    "translated_abstract": "神经过程(NP)可用于估计使用神经网络隐式定义的随机过程，而不是预先规定已知先验的过程，例如高斯过程。理想的NP将从数据中学习一切而没有任何归纳偏差，但在实践中，我们常常为了方便估计而限制了随机过程的类别。本文提出了一种基于鞅后验的方法，为未来数据指定了一种预测分布，从而减小了生成函数时的不确定性，并提出一个新的MPNP框架，表现出比现有NP方法更高的精度和样本效率。",
    "tldr": "本文提出了一种基于鞅后验的神经过程方法，用于估计使用神经网络隐式定义的随机过程，并在 benchmark 数据集上表现出更高的精度和样本效率。",
    "en_tdlr": "In this paper, we propose a martingale posterior neural process (MPNP) method for estimating stochastic processes implicitly defined with neural networks. By specifying a predictive distribution for future data, the uncertainty in generated functions is minimized, and our proposed MPNP outperforms existing methods in terms of accuracy and sample efficiency on benchmark datasets."
}