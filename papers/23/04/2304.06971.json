{
    "title": "Preserving Locality in Vision Transformers for Class Incremental Learning. (arXiv:2304.06971v1 [cs.LG])",
    "abstract": "Learning new classes without forgetting is crucial for real-world applications for a classification model. Vision Transformers (ViT) recently achieve remarkable performance in Class Incremental Learning (CIL). Previous works mainly focus on block design and model expansion for ViTs. However, in this paper, we find that when the ViT is incrementally trained, the attention layers gradually lose concentration on local features. We call this interesting phenomenon as \\emph{Locality Degradation} in ViTs for CIL. Since the low-level local information is crucial to the transferability of the representation, it is beneficial to preserve the locality in attention layers. In this paper, we encourage the model to preserve more local information as the training procedure goes on and devise a Locality-Preserved Attention (LPA) layer to emphasize the importance of local features. Specifically, we incorporate the local information directly into the vanilla attention and control the initial gradients ",
    "link": "http://arxiv.org/abs/2304.06971",
    "context": "Title: Preserving Locality in Vision Transformers for Class Incremental Learning. (arXiv:2304.06971v1 [cs.LG])\nAbstract: Learning new classes without forgetting is crucial for real-world applications for a classification model. Vision Transformers (ViT) recently achieve remarkable performance in Class Incremental Learning (CIL). Previous works mainly focus on block design and model expansion for ViTs. However, in this paper, we find that when the ViT is incrementally trained, the attention layers gradually lose concentration on local features. We call this interesting phenomenon as \\emph{Locality Degradation} in ViTs for CIL. Since the low-level local information is crucial to the transferability of the representation, it is beneficial to preserve the locality in attention layers. In this paper, we encourage the model to preserve more local information as the training procedure goes on and devise a Locality-Preserved Attention (LPA) layer to emphasize the importance of local features. Specifically, we incorporate the local information directly into the vanilla attention and control the initial gradients ",
    "path": "papers/23/04/2304.06971.json",
    "total_tokens": 894,
    "translated_title": "保留视觉Transformer中的局部特征以应用于类增量学习",
    "translated_abstract": "对于分类模型而言，能够在不忘记以前学到的知识的情况下学习新的类别对于其在实际应用中具有重要意义。在类增量学习方面，Vision Transformer（ViT）近来表现出了显著的性能。以往的研究主要集中于ViT的块设计和模型扩展。然而，本文发现在ViT进行增量训练时，注意力层逐渐失去对局部特征的关注能力。我们将这种现象称为ViT在类增量学习中的\\emph{局部信息降解}。由于低级别的局部信息对于特征传递的效果至关重要，保留局部特征对于模型的性能提升具有积极的作用。因此，本文鼓励模型在训练过程中保留更多的局部信息，设计了一种Locality-Preserved Attention（LPA）层来强调局部特征的重要性，并将局部信息直接整合到原始注意力中以控制初始梯度。",
    "tldr": "本文提出了一种Locality-Preserved Attention（LPA）层，以保留ViT中的局部特征以增强其在类增量学习中的性能。",
    "en_tdlr": "The paper proposes a Locality-Preserved Attention (LPA) layer to preserve local information in Vision Transformers (ViTs) for better performance in class incremental learning (CIL). The layer incorporates local information directly into attention and controls initial gradients."
}