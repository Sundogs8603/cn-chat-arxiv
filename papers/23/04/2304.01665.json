{
    "title": "Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])",
    "abstract": "Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call \"Neural Comprehension\", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \\url{ht",
    "link": "http://arxiv.org/abs/2304.01665",
    "context": "Title: Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])\nAbstract: Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call \"Neural Comprehension\", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \\url{ht",
    "path": "papers/23/04/2304.01665.json",
    "total_tokens": 894,
    "translated_title": "编译神经网络的语言模型：神经理解",
    "translated_abstract": "语言模型在自然语言处理任务中取得了令人瞩目的成果，但其进行符号操作和算术操作的能力仍然有限，这归因于它们隐式地从数据中学习规则。我们探讨如何将特别设计得到的加权的编译神经网络（CoNNs）并入语言模型的架构中，使得通过梯度训练的语言模型获得完全的规则理解能力。编译神经网络的并入为改善语言模型在复合任务中的性能提供了一个有前途的方向，特别是在需要深入理解抽象规则的领域。我们的方法称为“神经理解”，有助于语言模型在符号操作、规则推理、算术推理等方面实现绝对准确度。我们的代码公开可用：\\url{https://github.com/...}",
    "tldr": "本文探讨了如何将编译神经网络CoNNs并入语言模型的架构中，以使语言模型在复合任务中提高性能，特别是在需要深入理解抽象规则的领域。方法称为“神经理解”，提高了语言模型在符号操作、规则推理、算术推理等方面的准确度。",
    "en_tdlr": "This paper explores how to incorporate compiled neural networks (CoNNs) into the architecture of language models to improve their performance on compound tasks, particularly in areas that require a deeper comprehension of abstract rules. The method, called \"Neural Comprehension\", enhances the accuracy of language models in symbolic operations, rule reasoning, and arithmetic reasoning."
}