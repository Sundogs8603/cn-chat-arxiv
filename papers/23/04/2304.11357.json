{
    "title": "Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])",
    "abstract": "We introduce GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime.",
    "link": "http://arxiv.org/abs/2304.11357",
    "context": "Title: Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])\nAbstract: We introduce GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime.",
    "path": "papers/23/04/2304.11357.json",
    "total_tokens": 908,
    "translated_title": "通过联合生成式和判别式训练学习符号表示",
    "translated_abstract": "我们介绍了GEDI，它是一种贝叶斯框架，将现有的自监督学习目标与基于似然的生成模型相结合。该框架利用生成式和判别式方法的优势，比独立解决方案产生了更好的符号表示。此外，GEDI可以轻松集成并与现有的神经符号框架联合训练，无需额外的监督或昂贵的预训练步骤。我们通过对包括SVHN、CIFAR10和CIFAR100在内的实际数据进行实验，证明了GEDI在聚类性能方面大大优于现有的自监督学习策略。符号组件进一步允许它利用逻辑约束形式的知识，提高小数据范围内的性能。",
    "tldr": "GEDI是一种将自监督学习和基于似然生成模型结合的贝叶斯框架。它与现有的神经符号框架联合训练，无需额外监督或预训练步骤，能够产生更好的符号表示。通过实验，证明GEDI可以在聚类性能上显著超越现有的自监督学习策略，在小数据范围内的性能也得到提高。",
    "en_tdlr": "GEDI is a Bayesian framework that combines self-supervised learning and likelihood-based generative models. It can be jointly trained with existing neuro-symbolic frameworks without additional supervision or pre-training, and produces better symbolic representations. Experiments show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance and improves performance in the small data regime by leveraging logical constraints."
}