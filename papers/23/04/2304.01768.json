{
    "title": "Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])",
    "abstract": "In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.",
    "link": "http://arxiv.org/abs/2304.01768",
    "context": "Title: Convergence of alternating minimisation algorithms for dictionary learning. (arXiv:2304.01768v1 [math.OC])\nAbstract: In this paper we derive sufficient conditions for the convergence of two popular alternating minimisation algorithms for dictionary learning - the Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought of as approximative K-SVD. We show that given a well-behaved initialisation that is either within distance at most $1/\\log(K)$ to the generating dictionary or has a special structure ensuring that each element of the initialisation only points to one generating element, both algorithms will converge with geometric convergence rate to the generating dictionary. This is done even for data models with non-uniform distributions on the supports of the sparse coefficients. These allow the appearance frequency of the dictionary elements to vary heavily and thus model real data more closely.",
    "path": "papers/23/04/2304.01768.json",
    "total_tokens": 787,
    "translated_title": "字典学习中交替极小化算法的收敛性",
    "translated_abstract": "本文导出了针对字典学习两种流行的交替极小化算法 - 最优方向法（MOD）和在线字典学习（ODL）的收敛性足够的条件。我们表明，只要初始值良好，即距离生成的字典不超过$1/\\log(K)$或具有一定的结构，确保初始值中的每个元素只指向一个生成元，两种算法将以几何收敛速率收敛于生成的字典。这在具有非均匀分布的数据模型上也能实现，该模型中稀疏系数的支撑集的出现频率可以变化很大，从而更接近真实数据。",
    "tldr": "本文探讨了字典学习中两种交替极小化算法的收敛性，在良好的初始化下，这两种算法能够以几何收敛速率收敛于生成的字典，且可适用于非均匀分布的数据模型。",
    "en_tdlr": "This paper discusses the convergence of two alternating minimization algorithms for dictionary learning - Method of Optimal Directions (MOD) and Online Dictionary Learning (ODL), and shows that with well-behaved initialization, these algorithms can converge to the generating dictionary at a geometric convergence rate, even for non-uniform data models."
}