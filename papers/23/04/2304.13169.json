{
    "title": "SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])",
    "abstract": "We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi",
    "link": "http://arxiv.org/abs/2304.13169",
    "context": "Title: SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])\nAbstract: We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi",
    "path": "papers/23/04/2304.13169.json",
    "total_tokens": 880,
    "translated_title": "SAFE: 使用 Shard Graphs 进行机器遗忘",
    "translated_abstract": "我们提出了 Synergy Aware Forgetting Ensemble（SAFE）方法，该方法可以在最小化从训练模型中消除训练样本影响的预期成本的同时，适应各种数据的大型模型。这个过程也被称为选择性遗忘或遗忘，通常是通过将数据集分成碎片，对每个碎片进行完全独立的模型训练，然后将所得模型合成来进行的。增加碎片的数量可以降低遗忘的预期成本，但同时也增加了推理成本，并降低了模型的最终准确性，因为独立的模型训练过程中失去了样本之间的协同信息。SAFE 引入了 shard graph 的概念，它允许在训练过程中从其他碎片中引入有限的信息，以牺牲一定的预期遗忘成本增加明显的准确性，同时仍然实现完全消除残留影响。",
    "tldr": "本论文提出了一种使用 shard graph 进行机器遗忘的方法，以实现在最小化遗忘成本的情况下适应多样数据的大型模型，并取得了较高的准确性。",
    "en_tdlr": "This paper presents a method for machine unlearning using shard graphs, which enables adaptation of large models on diverse data while minimizing expected cost to remove training sample influence. By incorporating limited information from other shards during training, it achieves significant increase in accuracy while still attaining complete removal of residual influence at a modest increase in expected forgetting cost."
}