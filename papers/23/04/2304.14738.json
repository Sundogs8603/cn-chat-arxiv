{
    "title": "Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])",
    "abstract": "Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy, whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate th",
    "link": "http://arxiv.org/abs/2304.14738",
    "context": "Title: Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])\nAbstract: Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy, whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate th",
    "path": "papers/23/04/2304.14738.json",
    "total_tokens": 904,
    "translated_title": "面向优化不可分解指标的代价敏感自训练",
    "translated_abstract": "基于自训练的半监督学习算法使得仅使用少量标记数据就能学习到高精度的深度神经网络。然而，大多数自训练的工作都集中在提高精度的目标上，而实际的机器学习系统可能具有不可分解的复杂目标（例如，最大化不同类别召回率的最小值等）。在这项工作中，我们引入了代价敏感自训练（CSST）框架，该框架推广了用于优化不可分解指标的基于自训练的方法。我们证明了我们的框架可以更好地利用未标记的数据优化所需的不可分解指标，假设数据分布与自我训练的分析所做的一样。使用所提出的CSST框架，我们使用深度神经网络获得了针对不同不可分解指标的实际自训练方法（用于视觉和NLP任务）。我们的结果证明了CSST的有效性和可行性。",
    "tldr": "本研究提出了代价敏感自训练（CSST）框架，可以更好地利用未标记的数据优化不可分解指标，为处理具有复杂目标的实际机器学习系统提供了实用的解决方案。",
    "en_tdlr": "This paper introduces the Cost-Sensitive Self-Training (CSST) framework, which can better utilize unlabeled data to optimize non-decomposable metrics and provides a practical solution for machine learning systems with complex objectives."
}