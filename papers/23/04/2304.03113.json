{
    "title": "Efficient SAGE Estimation via Causal Structure Learning. (arXiv:2304.03113v1 [stat.ML])",
    "abstract": "The Shapley Additive Global Importance (SAGE) value is a theoretically appealing interpretability method that fairly attributes global importance to a model's features. However, its exact calculation requires the computation of the feature's surplus performance contributions over an exponential number of feature sets. This is computationally expensive, particularly because estimating the surplus contributions requires sampling from conditional distributions. Thus, SAGE approximation algorithms only take a fraction of the feature sets into account. We propose $d$-SAGE, a method that accelerates SAGE approximation. $d$-SAGE is motivated by the observation that conditional independencies (CIs) between a feature and the model target imply zero surplus contributions, such that their computation can be skipped. To identify CIs, we leverage causal structure learning (CSL) to infer a graph that encodes (conditional) independencies in the data as $d$-separations. This is computationally more ef",
    "link": "http://arxiv.org/abs/2304.03113",
    "context": "Title: Efficient SAGE Estimation via Causal Structure Learning. (arXiv:2304.03113v1 [stat.ML])\nAbstract: The Shapley Additive Global Importance (SAGE) value is a theoretically appealing interpretability method that fairly attributes global importance to a model's features. However, its exact calculation requires the computation of the feature's surplus performance contributions over an exponential number of feature sets. This is computationally expensive, particularly because estimating the surplus contributions requires sampling from conditional distributions. Thus, SAGE approximation algorithms only take a fraction of the feature sets into account. We propose $d$-SAGE, a method that accelerates SAGE approximation. $d$-SAGE is motivated by the observation that conditional independencies (CIs) between a feature and the model target imply zero surplus contributions, such that their computation can be skipped. To identify CIs, we leverage causal structure learning (CSL) to infer a graph that encodes (conditional) independencies in the data as $d$-separations. This is computationally more ef",
    "path": "papers/23/04/2304.03113.json",
    "total_tokens": 1121,
    "translated_title": "通过因果结构学习实现高效的SAGE估计",
    "translated_abstract": "Shapley Additive Global Importance (SAGE)是一种理论上有吸引力的可解释性方法，它公平地将全局重要性归因于模型的特征。然而，它的精确计算需要计算特征集的指数数量的剩余性能贡献，这在计算上非常昂贵，尤其是因为估计剩余性能贡献需要从条件分布中采样。因此，SAGE逼近算法只考虑了一小部分特征集。我们提出了一种名为$d$-SAGE的方法，它可以加速SAGE逼近。$d$-SAGE是由于观察到特征和模型目标之间的条件独立性 (CI) 意味着零剩余贡献，因此可以跳过它们的计算。为了识别CI，我们利用因果结构学习(CSL)来推断一个图，该图将数据中的(条件)独立性编码为$d$分离。这在计算上更有效，因为我们只需要计算非$d$分离特征集的SAGE值。我们提供了理论保证，说明随着$d$的增加，$d$-SAGE的逼近误差会收敛于零。在实验上，我们证明了$d$-SAGE需要比现有的SAGE逼近算法更少的特征集，同时保持高精度。",
    "tldr": "提出了一种通过因果结构学习的方法名为d-SAGE，用于加速SAGE逼近算法，显著降低计算开销和提高计算效率，并在理论上展示了$d$-SAGE的逼近误差会收敛于零，实验上体现了高精度。",
    "en_tdlr": "The paper proposes a new method called d-SAGE that utilizes causal structure learning to accelerate SAGE approximation algorithms. By identifying conditional independencies between features and the model target, d-SAGE skips the computation of zero surplus contributions, resulting in significant reductions in computation time and improved efficiency. Experimental results show that d-SAGE maintains high accuracy while requiring fewer feature sets than existing SAGE approximation algorithms."
}