{
    "title": "Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be. (arXiv:2304.13960v1 [cs.LG])",
    "abstract": "The success of the Adam optimizer on a wide array of architectures has made it the default in settings where stochastic gradient descent (SGD) performs poorly. However, our theoretical understanding of this discrepancy is lagging, preventing the development of significant improvements on either algorithm. Recent work advances the hypothesis that Adam and other heuristics like gradient clipping outperform SGD on language tasks because the distribution of the error induced by sampling has heavy tails. This suggests that Adam outperform SGD because it uses a more robust gradient estimate. We evaluate this hypothesis by varying the batch size, up to the entire dataset, to control for stochasticity. We present evidence that stochasticity and heavy-tailed noise are not major factors in the performance gap between SGD and Adam. Rather, Adam performs better as the batch size increases, while SGD is less effective at taking advantage of the reduction in noise. This raises the question as to why",
    "link": "http://arxiv.org/abs/2304.13960",
    "context": "Title: Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be. (arXiv:2304.13960v1 [cs.LG])\nAbstract: The success of the Adam optimizer on a wide array of architectures has made it the default in settings where stochastic gradient descent (SGD) performs poorly. However, our theoretical understanding of this discrepancy is lagging, preventing the development of significant improvements on either algorithm. Recent work advances the hypothesis that Adam and other heuristics like gradient clipping outperform SGD on language tasks because the distribution of the error induced by sampling has heavy tails. This suggests that Adam outperform SGD because it uses a more robust gradient estimate. We evaluate this hypothesis by varying the batch size, up to the entire dataset, to control for stochasticity. We present evidence that stochasticity and heavy-tailed noise are not major factors in the performance gap between SGD and Adam. Rather, Adam performs better as the batch size increases, while SGD is less effective at taking advantage of the reduction in noise. This raises the question as to why",
    "path": "papers/23/04/2304.13960.json",
    "total_tokens": 990,
    "translated_title": "噪声并不是SGD和Adam在Transformers上差距的主要因素，但符号下降可能是。",
    "translated_abstract": "Adam优化器在各种结构上的成功使其成为随机梯度下降（SGD）表现较差的情况下的默认优化器。然而，我们对这种差异的理论理解滞后，阻碍了对任一算法的重大改进。最近的研究提出假设，Adam和其他剪贴梯度等伎俩在语言任务上优于SGD，是因为采样引起的误差分布具有重尾。这表明，Adam之所以优于SGD是因为它使用了更强大的梯度估计。我们通过改变批大小来评估这一假设，直到整个数据集，以控制随机性。我们提供了证据表明，随机性和重尾噪声不是SGD和Adam性能差距的主要因素。相反，随着批大小的增加，Adam的表现更好，而SGD则不太能利用减少噪声带来的优势。这引出了一个问题：为什么Adam在批处理上执行得那么好？",
    "tldr": "Adam优化器在各种架构上的成功使其成为随机梯度下降表现差的默认选择。然而，本研究发现随机性和重尾噪声并不是SGD和Adam表现差距的主要因素。相反，Adam的性能随着批量的增加而提高，而SGD则无法充分利用噪声的降低。"
}