{
    "title": "Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages. (arXiv:2304.10158v1 [cs.CL])",
    "abstract": "One of the challenges with finetuning pretrained language models (PLMs) is that their tokenizer is optimized for the language(s) it was pretrained on, but brittle when it comes to previously unseen variations in the data. This can for instance be observed when finetuning PLMs on one language and evaluating them on data in a closely related language variety with no standardized orthography. Despite the high linguistic similarity, tokenization no longer corresponds to meaningful representations of the target data, leading to low performance in, e.g., part-of-speech tagging.  In this work, we finetune PLMs on seven languages from three different families and analyze their zero-shot performance on closely related, non-standardized varieties. We consider different measures for the divergence in the tokenization of the source and target data, and the way they can be adjusted by manipulating the tokenization during the finetuning step. Overall, we find that the similarity between the percenta",
    "link": "http://arxiv.org/abs/2304.10158",
    "context": "Title: Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages. (arXiv:2304.10158v1 [cs.CL])\nAbstract: One of the challenges with finetuning pretrained language models (PLMs) is that their tokenizer is optimized for the language(s) it was pretrained on, but brittle when it comes to previously unseen variations in the data. This can for instance be observed when finetuning PLMs on one language and evaluating them on data in a closely related language variety with no standardized orthography. Despite the high linguistic similarity, tokenization no longer corresponds to meaningful representations of the target data, leading to low performance in, e.g., part-of-speech tagging.  In this work, we finetune PLMs on seven languages from three different families and analyze their zero-shot performance on closely related, non-standardized varieties. We consider different measures for the divergence in the tokenization of the source and target data, and the way they can be adjusted by manipulating the tokenization during the finetuning step. Overall, we find that the similarity between the percenta",
    "path": "papers/23/04/2304.10158.json",
    "total_tokens": 913,
    "translated_title": "前置语言模型的标记方法能否促进跨语言转移？一个非标准化语言词性标注研究",
    "translated_abstract": "微调预训练语言模型（PLM）的一个挑战是，它们的标记器虽然针对预训练语言进行了优化，但在处理以前没有见过的数据变化时容易出问题。当在一个语言上进行微调并在没有标准正字法的密切相关语言变体的数据上进行评估时，就会出现这种情况。尽管两种语言具有高度相似性，但标记化不再对目标数据的有意义表征相对应，导致部分语音标注等性能下降。在这项工作中，我们微调了来自三个不同家族的七种语言的 PLM，并分析了它们在密切相关的非标准化语言变体上的零样本表现。我们考虑了不同的标记化源和目标数据差异度量方式以及在微调过程中调整它们的方式。总的来说，我们发现通过调整标记化方法可以在跨语言转移方面提高 PLM 的性能。",
    "tldr": "本研究探讨了微调预训练语言模型在非标准化语言上的词性标注的问题，发现通过调整标记化方法可以提高模型在跨语言转移方面的性能。",
    "en_tdlr": "This study examines the problem of part-of-speech tagging for non-standardized languages when fine-tuning pretrained language models, and found that manipulating tokenization can improve the cross-lingual transfer of models."
}