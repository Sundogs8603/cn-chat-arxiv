{
    "title": "Combining Vision and Tactile Sensation for Video Prediction. (arXiv:2304.11193v1 [cs.RO])",
    "abstract": "In this paper, we explore the impact of adding tactile sensation to video prediction models for physical robot interactions. Predicting the impact of robotic actions on the environment is a fundamental challenge in robotics. Current methods leverage visual and robot action data to generate video predictions over a given time period, which can then be used to adjust robot actions. However, humans rely on both visual and tactile feedback to develop and maintain a mental model of their physical surroundings. In this paper, we investigate the impact of integrating tactile feedback into video prediction models for physical robot interactions. We propose three multi-modal integration approaches and compare the performance of these tactile-enhanced video prediction models. Additionally, we introduce two new datasets of robot pushing that use a magnetic-based tactile sensor for unsupervised learning. The first dataset contains visually identical objects with different physical properties, whil",
    "link": "http://arxiv.org/abs/2304.11193",
    "context": "Title: Combining Vision and Tactile Sensation for Video Prediction. (arXiv:2304.11193v1 [cs.RO])\nAbstract: In this paper, we explore the impact of adding tactile sensation to video prediction models for physical robot interactions. Predicting the impact of robotic actions on the environment is a fundamental challenge in robotics. Current methods leverage visual and robot action data to generate video predictions over a given time period, which can then be used to adjust robot actions. However, humans rely on both visual and tactile feedback to develop and maintain a mental model of their physical surroundings. In this paper, we investigate the impact of integrating tactile feedback into video prediction models for physical robot interactions. We propose three multi-modal integration approaches and compare the performance of these tactile-enhanced video prediction models. Additionally, we introduce two new datasets of robot pushing that use a magnetic-based tactile sensor for unsupervised learning. The first dataset contains visually identical objects with different physical properties, whil",
    "path": "papers/23/04/2304.11193.json",
    "total_tokens": 887,
    "translated_title": "视觉和触觉感觉相结合的视频预测",
    "translated_abstract": "本文探讨将触觉感觉添加到物理机器人交互的视频预测模型中的影响。预测机器人行为对环境的影响是机器人技术中的关键挑战。目前的方法利用视觉和机器人动作数据来生成一定时间段内的视频预测，然后可以用于调整机器人动作。然而，人类依赖于视觉和触觉反馈来发展和维护他们对物理环境的心理模型。本文研究了将触觉反馈集成到物理机器人交互的视频预测模型中的影响。我们提出了三种多模态集成方法，并比较了这些触觉增强的视频预测模型的表现。此外，我们还介绍了两个新的机器人推动数据集，使用基于磁性的触觉传感器进行无监督学习。第一个数据集包含具有不同物理特性的视觉上相同的对象，第二个数据集用于测试模型的泛化性能。",
    "tldr": "本文研究将触觉反馈集成到物理机器人交互的视频预测模型中的影响，并介绍了两个新的机器人推动数据集，使用基于磁性的触觉传感器进行无监督学习。",
    "en_tdlr": "This paper studies the impact of integrating tactile feedback into video prediction models for physical robot interactions, and introduces two new datasets of robot pushing that use a magnetic-based tactile sensor for unsupervised learning."
}