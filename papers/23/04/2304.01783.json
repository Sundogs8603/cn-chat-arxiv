{
    "title": "Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])",
    "abstract": "Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \\textbf{p}ersonalized \\textbf{Fed}erated learning with \\textbf{L}ocal \\textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit",
    "link": "http://arxiv.org/abs/2304.01783",
    "context": "Title: Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])\nAbstract: Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \\textbf{p}ersonalized \\textbf{Fed}erated learning with \\textbf{L}ocal \\textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit",
    "path": "papers/23/04/2304.01783.json",
    "total_tokens": 1010,
    "translated_title": "个性化联邦学习与本地注意力",
    "translated_abstract": "联邦学习旨在学习一个单一的全局模型，使得中央服务器可以帮助在本地客户端进行模型训练，而不必访问其本地数据。联邦学习的关键挑战是不同客户端中本地数据的异质性，例如异质标签分布和特征偏移，这可能导致学习模型的显着性能降低。为解决这个问题，我们提出了一种简单而有效的算法，即具有本地注意力的个性化联邦学习（pFedLA），通过将注意机制并入客户端的个性化模型，同时保持注意块特定于客户端。具体而言，pFedLA提出了两个模块，即个性化单注意模块和个性化混合注意模块。此外，我们还介绍了一个新的FL数据集SplitMNIST-C，通过引入训练和测试数据之间的偏移。在SplitMNIST-C和EMNIST上的实验结果表明，pFedLA在准确性和收敛速度方面优于最先进的FL算法，并且在缓解特征漂移问题方面特别有效。",
    "tldr": "本文提出了一个名为pFedLA的算法，通过将注意力机制并入个性化模型来解决联邦学习中客户端数据异质性的问题，并在实验中表现出了优异的表现，尤其是在缓解特征漂移问题方面。",
    "en_tdlr": "This paper proposes an algorithm called pFedLA to address the heterogeneity of local data in federated learning by incorporating attention mechanism into personalized models and demonstrates superior performance in mitigating feature shift issue in experiments on a new SplitMNIST-C dataset."
}