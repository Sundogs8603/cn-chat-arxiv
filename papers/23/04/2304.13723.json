{
    "title": "A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])",
    "abstract": "Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje",
    "link": "http://arxiv.org/abs/2304.13723",
    "context": "Title: A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])\nAbstract: Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje",
    "path": "papers/23/04/2304.13723.json",
    "total_tokens": 926,
    "translated_title": "一种以控制为中心的视频预测基准",
    "translated_abstract": "视频是学习世界动态模型的体现代理人的有希望的知识来源。大型深度网络在自我监督的情况下越来越有效地建模复杂的视频数据，评估基于人类感知相似性或像素比较的指标。然而，目前的指标是否准确预测下游任务的表现仍不清楚。我们实验证明，对于规划机器人操作来说，现有指标在预测任务执行成功方面可能不可靠。为了解决这个问题，我们提出了一种行动条件下的视频预测基准，即通过采样规划对给定模型进行评估的控制基准。我们的基准，用于视觉规划的视频预测 ($VP^2$)，包括11个任务类别和310个任务实例定义的模拟环境、完整的规划实现和包含脚本交互轨迹的训练数据集。",
    "tldr": "本文提出了一个以控制为中心的视频预测基准，评估给定模型在通过采样规划对模拟机器人操作的表现。该基准包含有11个任务类别和310个任务实例定义的模拟环境，以及完整的规划实现和训练数据集，以解决现有指标在预测任务执行成功方面不可靠的问题。",
    "en_tdlr": "This paper proposes a control-centric benchmark for action-conditioned video prediction, called Video Prediction for Visual Planning ($VP^2$), to evaluate models for simulated robotic manipulation through sampling-based planning. The benchmark includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction trajectories to address the issue of unreliable metrics in predicting task execution success."
}