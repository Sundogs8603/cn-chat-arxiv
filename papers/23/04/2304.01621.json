{
    "title": "SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])",
    "abstract": "Cross-lingual science journalism generates popular science stories of scientific articles different from the source language for a non-expert audience. Hence, a cross-lingual popular summary must contain the salient content of the input document, and the content should be coherent, comprehensible, and in a local language for the targeted audience. We improve these aspects of cross-lingual summary generation by joint training of two high-level NLP tasks, simplification and cross-lingual summarization. The former task reduces linguistic complexity, and the latter focuses on cross-lingual abstractive summarization. We propose a novel multi-task architecture - SimCSum consisting of one shared encoder and two parallel decoders jointly learning simplification and cross-lingual summarization. We empirically investigate the performance of SimCSum by comparing it with several strong baselines over several evaluation metrics and by human evaluation. Overall, SimCSum demonstrates statistically si",
    "link": "http://arxiv.org/abs/2304.01621",
    "context": "Title: SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism. (arXiv:2304.01621v1 [cs.CL])\nAbstract: Cross-lingual science journalism generates popular science stories of scientific articles different from the source language for a non-expert audience. Hence, a cross-lingual popular summary must contain the salient content of the input document, and the content should be coherent, comprehensible, and in a local language for the targeted audience. We improve these aspects of cross-lingual summary generation by joint training of two high-level NLP tasks, simplification and cross-lingual summarization. The former task reduces linguistic complexity, and the latter focuses on cross-lingual abstractive summarization. We propose a novel multi-task architecture - SimCSum consisting of one shared encoder and two parallel decoders jointly learning simplification and cross-lingual summarization. We empirically investigate the performance of SimCSum by comparing it with several strong baselines over several evaluation metrics and by human evaluation. Overall, SimCSum demonstrates statistically si",
    "path": "papers/23/04/2304.01621.json",
    "total_tokens": 971,
    "translated_title": "SimCSum：联合学习简化和跨语言摘要以用于跨语言科学新闻报道",
    "translated_abstract": "跨语言科学新闻报道为非专业读者生成了不同于来源语言的科学文章的通俗版本。因此，跨语言宣传摘要必须包含原始文档的要点内容，并且内容应该连贯、易懂，并且以受众的本地语言呈现。本文通过联合训练简化和跨语言摘要两个高级NLP任务来改进跨语言摘要生成的这些方面。前者减少语言复杂度，后者专注于跨语言摘要。我们提出了一个新颖的多任务体系结构——SimCSum，其中包含一个共享编码器和两个并行解码器，共同学习简化和跨语言摘要。我们通过比较几种强基线模型的几个评估指标和人工评估来实证研究了SimCSum的性能。总的来说，SimCSum在同时训练两个高级NLP任务的同时，呈现出与基线相似或更优越的性能。",
    "tldr": "本文提出了一个名为SimCSum的模型，联合训练简化和跨语言摘要两个高级NLP任务。该模型表现出与基线模型相近或更优异的性能，并且同时提高了语言复杂性和跨语言抽象摘要。",
    "en_tdlr": "This paper proposes a model called SimCSum, which jointly trains two high-level NLP tasks, simplification and cross-lingual summarization. The model demonstrates similar or superior performance compared to the baseline models, while improving both linguistic complexity and cross-lingual abstractive summarization at the same time."
}