{
    "title": "Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units. (arXiv:2304.09258v1 [cs.AR])",
    "abstract": "Tensor processing units (TPUs), specialized hardware accelerators for machine learning tasks, have shown significant performance improvements when executing convolutional layers in convolutional neural networks (CNNs). However, they struggle to maintain the same efficiency in fully connected (FC) layers, leading to suboptimal hardware utilization. In-memory analog computing (IMAC) architectures, on the other hand, have demonstrated notable speedup in executing FC layers. This paper introduces a novel, heterogeneous, mixed-signal, and mixed-precision architecture that integrates an IMAC unit with an edge TPU to enhance mobile CNN performance. To leverage the strengths of TPUs for convolutional layers and IMAC circuits for dense layers, we propose a unified learning algorithm that incorporates mixed-precision training techniques to mitigate potential accuracy drops when deploying models on the TPU-IMAC architecture. The simulations demonstrate that the TPU-IMAC configuration achieves up ",
    "link": "http://arxiv.org/abs/2304.09258",
    "context": "Title: Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units. (arXiv:2304.09258v1 [cs.AR])\nAbstract: Tensor processing units (TPUs), specialized hardware accelerators for machine learning tasks, have shown significant performance improvements when executing convolutional layers in convolutional neural networks (CNNs). However, they struggle to maintain the same efficiency in fully connected (FC) layers, leading to suboptimal hardware utilization. In-memory analog computing (IMAC) architectures, on the other hand, have demonstrated notable speedup in executing FC layers. This paper introduces a novel, heterogeneous, mixed-signal, and mixed-precision architecture that integrates an IMAC unit with an edge TPU to enhance mobile CNN performance. To leverage the strengths of TPUs for convolutional layers and IMAC circuits for dense layers, we propose a unified learning algorithm that incorporates mixed-precision training techniques to mitigate potential accuracy drops when deploying models on the TPU-IMAC architecture. The simulations demonstrate that the TPU-IMAC configuration achieves up ",
    "path": "papers/23/04/2304.09258.json",
    "total_tokens": 1011,
    "translated_title": "基于张量处理单元的内存模拟模拟计算异构集成",
    "translated_abstract": "张量处理单元（TPUs）是专门加速机器学习任务的硬件加速器，在执行卷积神经网络（CNNs）中的卷积层时显示出显着的性能提升。然而，在全连接（FC）层中，它们很难保持相同的效率，导致硬件利用率不佳。另一方面，内存模拟计算（IMAC）架构在执行FC层时表现出了显著的加速效果。本文介绍了一种新颖、异构、混合信号和混合精度架构，将IMAC单元与边缘TPU集成，以提高移动CNN性能。为了充分利用TPUs在卷积层中的优点和IMAC电路在密集层中的优势，我们提出了一种统一的学习算法，结合混合精度训练技术来减轻在TPU-IMAC架构上部署模型时可能出现的精度降低问题。模拟结果表明，TPU-IMAC配置实现了高达...",
    "tldr": "本文介绍了一种将内存模拟计算（IMAC）单元与张量处理单元（TPUs）集成的新型、异构、混合信号和混合精度架构，以提高移动CNN性能。结合混合精度训练技术，充分利用TPUs在卷积层中的优点和IMAC电路在密集层中的优势，实现了高达...的性能提升。",
    "en_tdlr": "This paper introduces a novel, heterogeneous, mixed-signal, and mixed-precision architecture that integrates an in-memory analog computing (IMAC) unit with a tensor processing unit (TPU) to enhance mobile convolutional neural network (CNN) performance. By leveraging the strengths of TPUs for convolutional layers and IMAC circuits for dense layers, and incorporating mixed-precision training techniques, the proposed unified learning algorithm achieves significant performance improvements up to..."
}