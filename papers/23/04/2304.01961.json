{
    "title": "AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])",
    "abstract": "This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research",
    "link": "http://arxiv.org/abs/2304.01961",
    "context": "Title: AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])\nAbstract: This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research",
    "path": "papers/23/04/2304.01961.json",
    "total_tokens": 966,
    "translated_title": "AToMiC：支持多媒体内容创建的图像/文本检索测试集",
    "translated_abstract": "本文介绍了AToMiC（多媒体内容创作工具）数据集，旨在推动图像/文本跨模态检索领域的研究。虽然视觉语言预训练模型已经在提高检索效果方面取得了显著进展，但现有研究仍依赖于仅具有简单图像-文本关系和检索任务用户模型不足的图像标题数据集。为了弥补这些过度简化的设置和多媒体内容创建的真实应用之间的差距，我们介绍了一种新的构建检索测试集的方法。我们利用维基百科中嵌入的大规模图像-文档关联，建立了包括分层结构、文本样式和类型在内的多样化领域的文本和图片。我们基于一个现实的用户模型制定了两个任务，并通过基线模型的检索实验验证了我们的数据集。AToMiC为可扩展、多样化、可复现的多媒体检索研究提供了一个测试平台。",
    "tldr": "本文介绍了一个支持多媒体内容创建的图像/文本检索测试集——AToMiC，它使用了维基百科中的大规模图像-文档关联，并建立了多样的领域文本和图片。AToMiC 为可扩展、多样化、可复现的多媒体检索研究提供了一个测试平台。",
    "en_tdlr": "The paper presents the AToMiC dataset, which is designed to advance research in image/text cross-modal retrieval for multimedia content creation. AToMiC leverages hierarchical structures, diverse domains of texts and types of images, as well as large-scale image-document associations embedded in Wikipedia. It offers a testbed for scalable, diverse, and reproducible multimedia retrieval research."
}