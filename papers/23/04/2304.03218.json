{
    "title": "Data AUDIT: Identifying Attribute Utility- and Detectability-Induced Bias in Task Models. (arXiv:2304.03218v1 [cs.LG])",
    "abstract": "To safely deploy deep learning-based computer vision models for computer-aided detection and diagnosis, we must ensure that they are robust and reliable. Towards that goal, algorithmic auditing has received substantial attention. To guide their audit procedures, existing methods rely on heuristic approaches or high-level objectives (e.g., non-discrimination in regards to protected attributes, such as sex, gender, or race). However, algorithms may show bias with respect to various attributes beyond the more obvious ones, and integrity issues related to these more subtle attributes can have serious consequences. To enable the generation of actionable, data-driven hypotheses which identify specific dataset attributes likely to induce model bias, we contribute a first technique for the rigorous, quantitative screening of medical image datasets. Drawing from literature in the causal inference and information theory domains, our procedure decomposes the risks associated with dataset attribut",
    "link": "http://arxiv.org/abs/2304.03218",
    "context": "Title: Data AUDIT: Identifying Attribute Utility- and Detectability-Induced Bias in Task Models. (arXiv:2304.03218v1 [cs.LG])\nAbstract: To safely deploy deep learning-based computer vision models for computer-aided detection and diagnosis, we must ensure that they are robust and reliable. Towards that goal, algorithmic auditing has received substantial attention. To guide their audit procedures, existing methods rely on heuristic approaches or high-level objectives (e.g., non-discrimination in regards to protected attributes, such as sex, gender, or race). However, algorithms may show bias with respect to various attributes beyond the more obvious ones, and integrity issues related to these more subtle attributes can have serious consequences. To enable the generation of actionable, data-driven hypotheses which identify specific dataset attributes likely to induce model bias, we contribute a first technique for the rigorous, quantitative screening of medical image datasets. Drawing from literature in the causal inference and information theory domains, our procedure decomposes the risks associated with dataset attribut",
    "path": "papers/23/04/2304.03218.json",
    "total_tokens": 844,
    "tldr": "该论文提出了一种用于严格、定量地筛查医学图像数据集的技术，以识别可能引起模型偏差的特定数据集属性。"
}