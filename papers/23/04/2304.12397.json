{
    "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])",
    "abstract": "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https",
    "link": "http://arxiv.org/abs/2304.12397",
    "context": "Title: On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])\nAbstract: Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https",
    "path": "papers/23/04/2304.12397.json",
    "total_tokens": 916,
    "translated_title": "论使用黑盒API进行毒性评估的挑战",
    "translated_abstract": "对毒性的感知随时间推移而不断演变，而且在不同的地理和文化背景中往往存在差异。同样，用于检测毒性的商业黑盒API（例如Perspective API）也不是静态的，而经常重新训练以解决任何未被关注的弱点和偏见。我们评估了这些变化对比较旨在遏制毒性的模型和方法的相对优劣的研究发现的可重复性的影响。我们的发现表明，依赖继承的自动毒性评分来比较模型和技术的研究可能导致不准确的结果。重新对HELM的所有模型进行最新版本API的毒性评分，导致了广泛使用的基础模型的不同排名。我们建议在将研究之间进行马蜂拼接型比较时要谨慎，并为评估毒性随时间变化的更加有结构化的方法提出建议。代码和数据可在https://github.com/X/XXX上找到。",
    "tldr": "本文讨论使用黑盒API进行毒性评估的挑战，发现依赖继承的自动毒性评分可能导致不准确的结果，建议采用更加结构化的方法评估毒性随时间变化的模型和方法。",
    "en_tdlr": "This article discusses the challenges of using black-box APIs for toxicity evaluation and suggests that relying on inherited automatic toxicity scores may lead to inaccurate results. A more structured approach to evaluating the relative merits of models and methods that aim to curb toxicity over time is recommended."
}