{
    "title": "Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions. (arXiv:2304.04008v1 [cs.LG])",
    "abstract": "There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a ``joint growth\" and under the assumption of a ``sequential growth\" of the width over the NN's layers. Here, assuming a ``sequential growth\" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super",
    "link": "http://arxiv.org/abs/2304.04008",
    "context": "Title: Infinitely wide limits for deep Stable neural networks: sub-linear, linear and super-linear activation functions. (arXiv:2304.04008v1 [cs.LG])\nAbstract: There is a growing literature on the study of large-width properties of deep Gaussian neural networks (NNs), i.e. deep NNs with Gaussian-distributed parameters or weights, and Gaussian stochastic processes. Motivated by some empirical and theoretical studies showing the potential of replacing Gaussian distributions with Stable distributions, namely distributions with heavy tails, in this paper we investigate large-width properties of deep Stable NNs, i.e. deep NNs with Stable-distributed parameters. For sub-linear activation functions, a recent work has characterized the infinitely wide limit of a suitable rescaled deep Stable NN in terms of a Stable stochastic process, both under the assumption of a ``joint growth\" and under the assumption of a ``sequential growth\" of the width over the NN's layers. Here, assuming a ``sequential growth\" of the width, we extend such a characterization to a general class of activation functions, which includes sub-linear, asymptotically linear and super",
    "path": "papers/23/04/2304.04008.json",
    "total_tokens": 915,
    "translated_abstract": "越来越多的文献研究了具有高斯分布参数或权重以及高斯随机过程的深度高斯神经网络（NN）的大宽度特性。在一些经验和理论研究的激励下，本文探讨了具有稳定分布参数的深度稳定神经网络（Stable NN）的大宽度特性，即深度NN。对于次线性激活函数，最近的工作通过“联合增长”和在NN的层上进行“顺序增长”的假设，表征了适当缩放的深度稳定NN的无限宽度极限。在这里，我们假设宽度的“顺序增长”，将这种表征扩展到一个包括次线性、渐近线性和超线性的激活函数的通用类中。",
    "tldr": "本文探讨具有稳定分布参数的深度稳定神经网络（Stable NN）的大宽度特性，对于次线性激活函数已有研究，本文将这种方法扩展到一个包括次线性、渐近线性和超线性的激活函数的通用类中。",
    "en_tdlr": "This paper investigates the large-width properties of deep stable neural networks with stable-distributed parameters, extending the characterization of the infinitely wide limit of a suitable rescaled deep stable NN for sub-linear activation functions and including a general class of activation functions such as sub-linear, asymptotically linear, and super-linear by assuming a sequential growth of width."
}