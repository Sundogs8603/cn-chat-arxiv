{
    "title": "A Comparison of Semi-Supervised Learning Techniques for Streaming ASR at Scale. (arXiv:2304.11053v1 [cs.CL])",
    "abstract": "Unpaired text and audio injection have emerged as dominant methods for improving ASR performance in the absence of a large labeled corpus. However, little guidance exists on deploying these methods to improve production ASR systems that are trained on very large supervised corpora and with realistic requirements like a constrained model size and CPU budget, streaming capability, and a rich lattice for rescoring and for downstream NLU tasks. In this work, we compare three state-of-the-art semi-supervised methods encompassing both unpaired text and audio as well as several of their combinations in a controlled setting using joint training. We find that in our setting these methods offer many improvements beyond raw WER, including substantial gains in tail-word WER, decoder computation during inference, and lattice density.",
    "link": "http://arxiv.org/abs/2304.11053",
    "context": "Title: A Comparison of Semi-Supervised Learning Techniques for Streaming ASR at Scale. (arXiv:2304.11053v1 [cs.CL])\nAbstract: Unpaired text and audio injection have emerged as dominant methods for improving ASR performance in the absence of a large labeled corpus. However, little guidance exists on deploying these methods to improve production ASR systems that are trained on very large supervised corpora and with realistic requirements like a constrained model size and CPU budget, streaming capability, and a rich lattice for rescoring and for downstream NLU tasks. In this work, we compare three state-of-the-art semi-supervised methods encompassing both unpaired text and audio as well as several of their combinations in a controlled setting using joint training. We find that in our setting these methods offer many improvements beyond raw WER, including substantial gains in tail-word WER, decoder computation during inference, and lattice density.",
    "path": "papers/23/04/2304.11053.json",
    "total_tokens": 840,
    "translated_title": "一种大规模语音识别的半监督学习技术比较",
    "translated_abstract": "在缺乏大规模标注数据集的情况下，无配对文本和音频注入已成为提高自动语音识别性能的主要方法。然而，对于训练有非常大的受监督语料库并且有一些现实的要求（如模型大小和CPU预算、流媒体能力以及用于重新评分和下游NLU任务的丰富栅格）的产品ASR系统，缺乏有效的部署指南。在这项工作中，我们在联合训练的控制环境中比较了三种最先进的半监督方法，包括无配对文本和音频以及几种它们的组合。我们发现，在我们的设置中，这些方法除了原始WER之外还提供了许多改进，包括尾部单词WER的大幅度增益，在推理过程中的解码器计算和栅格密度等方面。",
    "tldr": "比较三种半监督ASR方法的优劣，发现这些方法不仅在原始WER方面有所提高，而且在尾部单词WER、推理过程中的解码器计算和栅格密度等方面都有显著的提高。",
    "en_tdlr": "This paper compares three state-of-the-art semi-supervised methods for ASR and finds that they offer many improvements beyond raw WER, including substantial gains in tail-word WER, decoder computation during inference, and lattice density."
}