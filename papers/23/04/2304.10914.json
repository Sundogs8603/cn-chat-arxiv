{
    "title": "Self-Supervised Adversarial Imitation Learning. (arXiv:2304.10914v1 [cs.LG])",
    "abstract": "Behavioural cloning is an imitation learning technique that teaches an agent how to behave via expert demonstrations. Recent approaches use self-supervision of fully-observable unlabelled snapshots of the states to decode state pairs into actions. However, the iterative learning scheme employed by these techniques is prone to get trapped into bad local minima. Previous work uses goal-aware strategies to solve this issue. However, this requires manual intervention to verify whether an agent has reached its goal. We address this limitation by incorporating a discriminator into the original framework, offering two key advantages and directly solving a learning problem previous work had. First, it disposes of the manual intervention requirement. Second, it helps in learning by guiding function approximation based on the state transition of the expert's trajectories. Third, the discriminator solves a learning issue commonly present in the policy model, which is to sometimes perform a `no ac",
    "link": "http://arxiv.org/abs/2304.10914",
    "context": "Title: Self-Supervised Adversarial Imitation Learning. (arXiv:2304.10914v1 [cs.LG])\nAbstract: Behavioural cloning is an imitation learning technique that teaches an agent how to behave via expert demonstrations. Recent approaches use self-supervision of fully-observable unlabelled snapshots of the states to decode state pairs into actions. However, the iterative learning scheme employed by these techniques is prone to get trapped into bad local minima. Previous work uses goal-aware strategies to solve this issue. However, this requires manual intervention to verify whether an agent has reached its goal. We address this limitation by incorporating a discriminator into the original framework, offering two key advantages and directly solving a learning problem previous work had. First, it disposes of the manual intervention requirement. Second, it helps in learning by guiding function approximation based on the state transition of the expert's trajectories. Third, the discriminator solves a learning issue commonly present in the policy model, which is to sometimes perform a `no ac",
    "path": "papers/23/04/2304.10914.json",
    "total_tokens": 862,
    "translated_title": "自监督对抗仿真学习",
    "translated_abstract": "行为克隆是一种通过专家演示来教授智能体如何行为的仿真学习技术。最近的方法使用自我监督的完全可观察未标记状态的快照来将状态对解码为动作。然而，这些技术采用的迭代学习方案容易陷入坏的局部最小值。先前的工作使用目标感知策略来解决这个问题。然而，这需要人工介入来验证智能体是否达到了目标。我们通过将鉴别器纳入原始框架来解决这个限制，提供了两个关键优势，并直接解决了以前的一个学习问题。首先，它不需要人工干预。其次，它通过指导基于专家轨迹的状态转换的函数逼近来帮助学习。第三，鉴别器解决了策略模型常见的学习问题，即有时会执行“无动作”。",
    "tldr": "本文介绍了一种解决自我监督模型陷入坏局部最小值的方法，即通过将鉴别器纳入模型，不需要人工干预，帮助学习，并解决了常见的学习问题。",
    "en_tdlr": "This paper presents a method for solving the problem of self-supervised models getting trapped in bad local minima by incorporating a discriminator into the model to eliminate the need for manual intervention, assist in learning, and solve common learning problems."
}