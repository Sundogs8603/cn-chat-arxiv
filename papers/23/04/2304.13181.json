{
    "title": "Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])",
    "abstract": "Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo",
    "link": "http://arxiv.org/abs/2304.13181",
    "context": "Title: Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])\nAbstract: Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo",
    "path": "papers/23/04/2304.13181.json",
    "total_tokens": 911,
    "translated_title": "针对图文模型的样本特异性去偏方法",
    "translated_abstract": "对于图文数据的自监督表征学习在医学应用中具有重要意义，例如图像分类、视觉定位和跨模态检索。一种常见的方法涉及对语义上相似（正）和不相似（负）的数据点进行对比。从训练数据集中均匀地抽取负样本会引入错误的负面样本，即将同属一类的样本视为不相似。在医疗保健数据中，潜在的类别分布是不均匀的，意味着错误的负面样本出现的比例高度不同。为了提高学得的表示质量，我们提出了一种纠正错误负面样本的新方法。我们的方法可以被看作是使用估计的样本特异性类别概率的去偏对比学习的变体。我们提供了目标函数的理论分析，并在图像和配对的图文数据集上展示了所提出的方法。我们的实验证明了我们的方法取得的更好的效果。",
    "tldr": "发现从训练数据集中均匀地抽取负样本会引入错误的负面样本。我们提出了一种纠正错误负面样本的新方法，即针对图文模型的样本特异性去偏方法。"
}