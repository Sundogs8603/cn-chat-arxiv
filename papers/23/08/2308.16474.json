{
    "title": "Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])",
    "abstract": "Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtai",
    "link": "http://arxiv.org/abs/2308.16474",
    "context": "Title: Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])\nAbstract: Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtai",
    "path": "papers/23/08/2308.16474.json",
    "total_tokens": 777,
    "translated_title": "提升多模式大型语言模型的子任务性能",
    "translated_abstract": "多模式大型语言模型（MLLM）是指从大型语言模型（LLM）扩展而来的模型，具备处理和推理多模式数据的能力。当前的MLLM通常通过使用LLM将任务分解为多个子任务，然后使用各个预训练模型完成特定的子任务，并最终利用LLM整合每个子任务的结果来获得任务的结果。在现实世界中，处理大型项目时，常常将项目分解为较小的子项目，并由不同的团队提供相应的解决方案或结果。项目所有者随后决定使用哪个解决方案或结果，以确保每个子任务和整个项目能够达到最佳结果。受此启发，本研究考虑选择多个预训练模型来完成相同的子任务。通过将多个预训练模型的结果进行组合，获得最佳的子任务结果。",
    "tldr": "该论文提出了一种方法，通过选择多个预训练模型来完成相同的子任务，通过组合多个模型的结果获得最佳的子任务结果。"
}