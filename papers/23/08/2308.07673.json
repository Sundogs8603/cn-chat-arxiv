{
    "title": "A Review of Adversarial Attacks in Computer Vision. (arXiv:2308.07673v1 [cs.CV])",
    "abstract": "Deep neural networks have been widely used in various downstream tasks, especially those safety-critical scenario such as autonomous driving, but deep networks are often threatened by adversarial samples. Such adversarial attacks can be invisible to human eyes, but can lead to DNN misclassification, and often exhibits transferability between deep learning and machine learning models and real-world achievability. Adversarial attacks can be divided into white-box attacks, for which the attacker knows the parameters and gradient of the model, and black-box attacks, for the latter, the attacker can only obtain the input and output of the model. In terms of the attacker's purpose, it can be divided into targeted attacks and non-targeted attacks, which means that the attacker wants the model to misclassify the original sample into the specified class, which is more practical, while the non-targeted attack just needs to make the model misclassify the sample. The black box setting is a scenari",
    "link": "http://arxiv.org/abs/2308.07673",
    "context": "Title: A Review of Adversarial Attacks in Computer Vision. (arXiv:2308.07673v1 [cs.CV])\nAbstract: Deep neural networks have been widely used in various downstream tasks, especially those safety-critical scenario such as autonomous driving, but deep networks are often threatened by adversarial samples. Such adversarial attacks can be invisible to human eyes, but can lead to DNN misclassification, and often exhibits transferability between deep learning and machine learning models and real-world achievability. Adversarial attacks can be divided into white-box attacks, for which the attacker knows the parameters and gradient of the model, and black-box attacks, for the latter, the attacker can only obtain the input and output of the model. In terms of the attacker's purpose, it can be divided into targeted attacks and non-targeted attacks, which means that the attacker wants the model to misclassify the original sample into the specified class, which is more practical, while the non-targeted attack just needs to make the model misclassify the sample. The black box setting is a scenari",
    "path": "papers/23/08/2308.07673.json",
    "total_tokens": 885,
    "translated_title": "计算机视觉中对抗性攻击的综述",
    "translated_abstract": "深度神经网络在各种下游任务中被广泛应用，特别是在像自动驾驶这样的安全关键场景中，但深度网络经常受到对抗样本的威胁。这种对抗性攻击对人眼来说是看不见的，但却会导致深度神经网络误分类，并且在深度学习和机器学习模型以及现实环境中具有传递性。对抗性攻击可以分为白盒攻击，攻击者知道模型的参数和梯度；以及黑盒攻击，攻击者只能获取模型的输入和输出。根据攻击者的目的，可以分为有目标攻击和非目标攻击，前者是指攻击者希望模型将原始样本错误分类为指定的类，这更实际；而非目标攻击只需让模型将样本错误分类即可。黑盒设置是一种情况。",
    "tldr": "这项综述研究深度神经网络面临的对抗性攻击，探讨了攻击类型、攻击者目的以及黑白盒攻击的区别，并强调了对抗样本的传递性和现实应用的可行性。",
    "en_tdlr": "This review examines adversarial attacks in deep neural networks, discussing attack types, attacker motivations, and the distinctions between white-box and black-box attacks. It also highlights the transferability of adversarial samples and their real-world implications."
}