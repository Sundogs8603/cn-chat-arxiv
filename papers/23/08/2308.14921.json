{
    "title": "Gender bias and stereotypes in Large Language Models. (arXiv:2308.14921v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities ",
    "link": "http://arxiv.org/abs/2308.14921",
    "context": "Title: Gender bias and stereotypes in Large Language Models. (arXiv:2308.14921v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities ",
    "path": "papers/23/08/2308.14921.json",
    "total_tokens": 933,
    "translated_title": "大型语言模型中的性别偏见和刻板印象",
    "translated_abstract": "在过去几个月中，大型语言模型（LLMs）取得了显著的进展，在许多领域打破了最先进的测试基准。本文研究LLMs在性别刻板印象方面的行为，这是先前模型中已知的一个问题。我们使用一个简单的范例来测试性别偏见的存在，这一范例建立在但与WinoBias不同，后者是一个常用的性别偏见数据集，很可能包含在目前LLMs的训练数据中。我们测试了四个最近发布的LLMs，并证明它们在男性和女性职业方面表现出有偏见的假设。本文的贡献如下：（a）LLMs在选择与人的性别刻板印象一致的职业时的概率是3-6倍；（b）这些选择与人们的感知更加一致，而不是与官方职业统计数据的真实情况一致；（c）事实上，LLMs放大了偏见，超过了人们的感知或真实情况；（d）LLMs忽视了关键的歧义",
    "tldr": "研究发现大型语言模型存在性别偏见和刻板印象，它们更倾向于选择与个人性别刻板印象一致的职业，并且放大了偏见，超过了现实情况。",
    "en_tdlr": "The study reveals that Large Language Models (LLMs) exhibit gender bias and stereotypes. They are more likely to choose occupations that align with gender stereotypes, amplifying the bias beyond real-world perceptions."
}