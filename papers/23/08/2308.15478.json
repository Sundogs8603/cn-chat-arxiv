{
    "title": "An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])",
    "abstract": "In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat",
    "link": "http://arxiv.org/abs/2308.15478",
    "context": "Title: An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])\nAbstract: In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat",
    "path": "papers/23/08/2308.15478.json",
    "total_tokens": 855,
    "translated_title": "神经网络的自适应切向特征视角",
    "translated_abstract": "为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在切向特征空间中的线性模型，其中特征在训练过程中可以进行转换。我们考虑特征的线性变换，从而通过双线性插值约束在参数和变换上进行联合优化。我们证明了这个优化问题与具有结构化正则化的等价线性约束优化问题具有近似低秩解的关系。通过将其应用于神经网络结构，我们对特征以及核函数的变化获得了更深入的理解，为当目标函数在切向特征上表征不好时的核对齐现象提供了额外的细微差别。除了在简单回归问题上验证我们的理论观察结果外，我们还通过实验证明了切向特征分类的自适应特征实现方法。",
    "tldr": "本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。"
}