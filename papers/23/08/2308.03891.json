{
    "title": "A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])",
    "abstract": "Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.",
    "link": "http://arxiv.org/abs/2308.03891",
    "context": "Title: A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])\nAbstract: Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.",
    "path": "papers/23/08/2308.03891.json",
    "total_tokens": 865,
    "translated_title": "跨领域评估因果知识提取方法",
    "translated_abstract": "因果知识提取是从文本中提取相关因果关系的任务。尽管这个任务对于语言理解和知识发现很重要，但是最近在这个领域的工作主要集中在将文本片段二分类为因果或非因果。因此，本研究对三个序列标注模型进行了彻底分析，比较了使用基于跨度的方法提取因果关系的效果。实验结果表明，与复杂的模型架构相比，预训练语言模型（如BERT）的词嵌入在这个任务上提供了显著的性能提升。我们观察到，在涵盖不同领域和不同类型的因果短语的4个数据集中，基于跨度的模型表现优于简单的序列标注模型。",
    "tldr": "本研究进行了跨领域的评估，比较了因果知识提取的三个序列标注模型和基于跨度的提取方法。实验结果表明，预训练语言模型的词嵌入在这个任务中表现出了较好的性能，同时基于跨度的模型在不同类型的数据集上表现更好。",
    "en_tdlr": "This study conducts a cross-domain evaluation of approaches for causal knowledge extraction and compares three sequence tagging models with a span-based approach. The experiments show that embeddings from pre-trained language models, such as BERT, perform significantly better on this task compared to previous models, and span-based models outperform simple sequence tagging models on different datasets with various types of cause-effect phrases."
}