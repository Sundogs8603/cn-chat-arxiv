{
    "title": "ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v1 [cs.CV])",
    "abstract": "Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the avera",
    "link": "http://arxiv.org/abs/2308.03793",
    "context": "Title: ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v1 [cs.CV])\nAbstract: Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the avera",
    "path": "papers/23/08/2308.03793.json",
    "total_tokens": 941,
    "translated_title": "ReCLIP: 使用无源领域自适应优化对比性语言图像预训练",
    "translated_abstract": "大规模预训练的视觉语言模型（例如CLIP）在零样本分类方面表现出色，例如在没有看到任何示例的情况下，在ImageNet上实现了76.3％的top-1准确率，这为许多没有标注数据的任务带来了潜在的好处。然而，将CLIP应用于下游目标领域时，视觉和文本领域差异以及交叉模态对齐不准确可能会对模型性能产生很大影响。为了解决这些挑战，我们提出了ReCLIP，这是一种针对视觉语言模型的无源领域自适应方法，不需要任何源数据或目标标注数据。ReCLIP首先学习一个投影空间来减轻不对齐的视觉-文本嵌入，并学习伪标签，然后使用伪标签部署交叉模态自训练，以迭代地更新视觉和文本编码器，优化标签，并减少领域差距和嵌入不准确性。通过大量实验证明，ReCLIP能够减少平均...",
    "tldr": "ReCLIP是一种用于改善视觉语言模型的无源领域自适应方法，通过学习投影空间和使用伪标签进行自训练，减少领域差异和嵌入不准确性。",
    "en_tdlr": "ReCLIP is a source-free domain adaptation method for vision-language models, which reduces domain gaps and misalignments by learning projection space and using pseudo labels for self-training."
}