{
    "title": "Multi-attacks: Many images $+$ the same adversarial attack $\\to$ many target labels. (arXiv:2308.03792v1 [cs.CV])",
    "abstract": "We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \\textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spel",
    "link": "http://arxiv.org/abs/2308.03792",
    "context": "Title: Multi-attacks: Many images $+$ the same adversarial attack $\\to$ many target labels. (arXiv:2308.03792v1 [cs.CV])\nAbstract: We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \\textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spel",
    "path": "papers/23/08/2308.03792.json",
    "total_tokens": 965,
    "translated_title": "多重攻击: 多张图像+相同的对抗攻击 -> 多个目标标签",
    "translated_abstract": "我们展示了一个简单的对抗扰动P可以将n个图像X1，X2，...，Xn从原始的未受扰动的类别c1，c2，...，cn改变为期望的（不一定相同的）类别c^*_1，c^*_2，...，c^*_n。我们称之为\"多重攻击\"。在不同条件下，如图像分辨率等，我们估计在像素空间中某个图像周围存在高类别置信度的区域数量大约为10^（O（100）），这对于全面的防御策略构成了重要挑战。我们展示了几个立即产生的影响：根据强度改变结果类别的对抗攻击，以及与尺度无关的对抗样本。为了展示像素空间中类别决策边界的冗余和丰富性，我们寻找了其在二维平面上追踪图像和拼写的部分。",
    "tldr": "本文展示了通过设计一个单一的对抗扰动P，在多个图像上实现将其原始类别改变为不同目标类别的多重攻击。研究发现在像素空间中存在大量高类别置信度的区域，对全面的防御策略构成重要挑战。",
    "en_tdlr": "This paper demonstrates the effectiveness of multi-attacks by designing a single adversarial perturbation that can change the class of multiple images to different target classes. The study also reveals the challenge of defending against these multi-attacks due to the existence of numerous regions with high class confidence in the pixel space."
}