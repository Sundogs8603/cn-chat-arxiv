{
    "title": "GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])",
    "abstract": "Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h",
    "link": "http://arxiv.org/abs/2308.13646",
    "context": "Title: GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])\nAbstract: Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h",
    "path": "papers/23/08/2308.13646.json",
    "total_tokens": 871,
    "translated_title": "GRASP: 一种高效的在线渐进式学习的重演策略",
    "translated_abstract": "深度神经网络中的渐进学习涉及从不断增长的数据流中逐步累积知识。渐进学习的一个主要挑战是非平稳的数据流会导致之前学到的能力遭受灾难性遗忘。重演是一种常用且有效的缓解这个问题的方法，即将过去的观测结果存储在缓冲区中，并在学习过程中将它们与新的观测结果混合。这带来了一个问题：应该选择哪些存储样本进行重演？选择最适合学习的样本而不是随机选择样本，可能会导致学习速度显著加快。对于类增量学习，先前的研究表明简单的类均衡随机选择策略优于更复杂的方法。在这里，我们通过探索一种新的样本选择策略GRASP重新思考这个问题。GRASP首先选择最具代表性的样本，然后逐渐选择较不具代表性的样本。",
    "tldr": "GRASP是一种新的样本选择策略，根据样本的代表性选择最适合学习的样本，从而提高了在线渐进式学习的效率。"
}