{
    "title": "Tackling Diverse Minorities in Imbalanced Classification. (arXiv:2308.14838v1 [cs.LG])",
    "abstract": "Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate th",
    "link": "http://arxiv.org/abs/2308.14838",
    "context": "Title: Tackling Diverse Minorities in Imbalanced Classification. (arXiv:2308.14838v1 [cs.LG])\nAbstract: Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate th",
    "path": "papers/23/08/2308.14838.json",
    "total_tokens": 864,
    "translated_title": "应对不平衡分类中多样的少数群体问题",
    "translated_abstract": "在各种实际应用中普遍存在不平衡数据集，在训练分类器时会带来重大挑战。当处理大型数据集时，不平衡问题可能进一步恶化，使得有效训练分类器变得异常困难。为了解决这个问题，已经开发出了过采样技术，通过在少数群体和其邻居之间线性插值数据实例。然而，在许多实际场景中，例如异常检测，少数群体实例通常在特征空间中分散分布而不是聚集在一起。受领域无关数据混合的启发，我们提出通过混合少数群体和多数群体的数据样本来迭代生成合成样本。开发这样一个框架是非常复杂的，挑战包括源样本选择、混合策略选择以及底层模型和混合策略之间的协调。为了应对这些挑战，我们提出了一种解决方法。",
    "tldr": "该研究提出了一种应对不平衡分类中多样的少数群体问题的方法，通过混合少数和多数群体的数据样本来生成合成样本，来解决分散分布的少数群体问题。",
    "en_tdlr": "This research proposes a solution to tackle the problem of diverse minorities in imbalanced classification by generating synthetic samples through mixing data samples from both minority and majority classes, in order to address the issue of dispersed minority instances."
}