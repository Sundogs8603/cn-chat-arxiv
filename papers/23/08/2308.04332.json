{
    "title": "RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback. (arXiv:2308.04332v1 [cs.LG])",
    "abstract": "To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RL",
    "link": "http://arxiv.org/abs/2308.04332",
    "context": "Title: RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback. (arXiv:2308.04332v1 [cs.LG])\nAbstract: To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RL",
    "path": "papers/23/08/2308.04332.json",
    "total_tokens": 836,
    "translated_title": "RLHF-Blender: 可配置的与人类反馈交互学习的界面",
    "translated_abstract": "在实际应用中使用强化学习从人类反馈中学习（RLHF）是至关重要的，关键是通过多种人类反馈源学习奖励模型，并考虑涉及不同类型反馈的人类因素。然而，由于研究人员缺乏标准化的工具，对从不同类型反馈中学习的系统性研究受到阻碍。为此，我们提出了RLHF-Blender，一个可配置的、互动的用于从人类反馈中学习的界面。RLHF-Blender提供了一个模块化的实验框架和实现，使研究人员能够系统地研究人类反馈对奖励学习的特性和质量。该系统促进了各种反馈类型的探索，包括演示、排序、比较和自然语言指令，并考虑了人类因素对其有效性的影响。我们讨论了RLHF-Blender所实现的一系列具体研究机会。",
    "tldr": "RLHF-Blender是一个可配置的、互动的界面，用于从多种人类反馈中学习，研究人员可以系统地探索不同类型的反馈以及人类因素对奖励学习的影响。"
}