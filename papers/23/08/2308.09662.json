{
    "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)",
    "abstract": "Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, ",
    "link": "http://arxiv.org/abs/2308.09662",
    "context": "Title: Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)\nAbstract: Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, ",
    "path": "papers/23/08/2308.09662.json",
    "total_tokens": 1016,
    "translated_title": "使用连续发言的方式对大型语言模型进行红队评估以实现安全对齐",
    "translated_abstract": "大型语言模型（LLMs）通过优化下一个单词预测目标，以其巨大的多任务能力震撼世界。随着它们的属性和编码知识的出现，LLMs产生有害输出的风险增加，使它们不适合可扩展地部署给公众。在这项工作中，我们提出了一个新的安全评估基准RED-EVAL，进行红队评估。我们展示了即便是广泛部署的模型也容易受到基于连续发言的(CoU)提示的影响，使基于GPT-4和ChatGPT的闭源LLM系统违反伦理地对超过65%和73%的有害查询做出回应。我们还展示了RED-EVAL在8个开源LLM中的一致性，通过红队评估尝试生成86%以上的有害回应。接下来，我们提出了RED-INSTRUCT--一种用于LLM安全对齐的方法。它包括两个阶段：1）HARMFULQA数据收集：利用CoU提示,",
    "tldr": "这项工作以红队评估的方式对大型语言模型进行安全评估，发现即便是广泛部署的模型也容易受到连续发言提示的影响，导致违反伦理地对有害查询做出回应。通过红队评估尝试，发现多数开源LLM也会生成有害回应。研究者提出了一种LLM安全对齐的方法。",
    "en_tdlr": "This work conducts a red team evaluation on large language models, revealing their susceptibility to prompt chaining and unethical responses to harmful queries. The study also demonstrates that most open-source LLMs generate harmful outputs. The authors propose an approach for safety alignment of LLMs."
}