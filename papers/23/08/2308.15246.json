{
    "title": "A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (arXiv:2308.15246v1 [cs.CL])",
    "abstract": "Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations by the NMT model belong to a different class than the original translations in the target language. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which leads to a different class determined by a classifier. To evaluate the robustness of NMT models to this attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments in various settings, including a comp",
    "link": "http://arxiv.org/abs/2308.15246",
    "context": "Title: A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (arXiv:2308.15246v1 [cs.CL])\nAbstract: Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations by the NMT model belong to a different class than the original translations in the target language. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which leads to a different class determined by a classifier. To evaluate the robustness of NMT models to this attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments in various settings, including a comp",
    "path": "papers/23/08/2308.15246.json",
    "total_tokens": 901,
    "translated_title": "一种基于分类引导的对抗攻击神经机器翻译方法",
    "translated_abstract": "神经机器翻译（NMT）模型已经被证明容易受到对抗攻击的影响，攻击者可以通过精心设计的输入扰动来误导目标模型。本文介绍了一种名为ACT的新型对抗攻击框架，针对NMT系统进行攻击，攻击过程中引导了一个分类器。在我们的攻击中，攻击者旨在生成保持语义的对抗样本，使得NMT模型的翻译结果与目标语言中的原始翻译属于不同的类别。与之前的攻击不同，我们的新方法更能改变整体意义，从而通过分类器将其归为不同的类别。为了评估NMT模型对该攻击的抵抗能力，我们提出了对现有基于单词替换的黑盒攻击进行改进的方法，通过在攻击过程中引入目标NMT模型的输出翻译和一个分类器的输出logit。通过在各种设置下进行大量实验证明了我们的方法的有效性。",
    "tldr": "本文介绍了一种基于分类引导的对抗攻击神经机器翻译的方法，通过改变整体意义生成保持语义的对抗样本，从而使得翻译结果属于不同的类别。",
    "en_tdlr": "This paper presents a classification-guided approach for adversarial attacks against neural machine translation. By altering the overall meaning and generating meaning-preserving adversarial examples, the translation results can belong to different categories."
}