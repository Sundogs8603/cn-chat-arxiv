{
    "title": "Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])",
    "abstract": "Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting",
    "link": "http://arxiv.org/abs/2308.01823",
    "context": "Title: Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])\nAbstract: Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting",
    "path": "papers/23/08/2308.01823.json",
    "total_tokens": 923,
    "translated_title": "提高深度神经网络鲁棒性和公平性的困难对抗样本挖掘",
    "translated_abstract": "对抗训练（AT）被广泛认为是提高深度神经网络（DNNs）对抗性样本（AE）鲁棒性的最先进技术。然而，最近的研究发现，经过对抗训练的模型容易出现不公平问题，限制了它们的适用性。在本文中，我们通过经验观察发现，这个限制可能是由于严重的对抗置信过拟合，即某些具有过度自信的对抗性样本。为了缓解这个问题，我们提出了HAM，这是一个简单但有效的框架，通过自适应的困难对抗样本挖掘。HAM集中于以适应性的方式挖掘困难对抗性样本，同时丢弃容易的样本。具体地，HAM根据计算损失值时需要穿过决策边界的步长来识别困难的AE。此外，还引入了早期丢弃机制来在AE生成的初期丢弃容易的样本，从而使得网络更加鲁棒和公平。",
    "tldr": "该研究针对对抗训练模型容易出现不公平问题的限制，提出了通过自适应的困难对抗样本挖掘来改善深度神经网络的鲁棒性和公平性。",
    "en_tdlr": "This paper proposes a straightforward yet effective framework, called HAM, for improving the robustness and fairness of deep neural networks by adaptively mining hard adversarial examples."
}