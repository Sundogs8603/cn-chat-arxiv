{
    "title": "Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])",
    "abstract": "In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif",
    "link": "http://arxiv.org/abs/2308.14969",
    "context": "Title: Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])\nAbstract: In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif",
    "path": "papers/23/08/2308.14969.json",
    "total_tokens": 931,
    "translated_title": "受限制下的重新编程: 重新审视彩票式转移的高效可靠性",
    "translated_abstract": "在预训练预算巨大的基础模型时代，下游任务已经转向了高效快速适应的叙述。对于计算机视觉领域的基于分类的任务，最高效的方法是线性探测（LP）和视觉提示/重新编程（VP）; 前者旨在通过在预训练模型提取的特征上学习线性头部分类器，而后者将输入数据映射到最初在其上进行预训练的源数据领域。尽管广泛的研究已经证明了LP和VP在下游性能方面的差异，但我们通过稀疏度轴来探索这两种方法的能力: (a) 数据稀疏性: 少样本自适应的影响，以及 (b) 模型稀疏性: 彩票式转移的影响。我们证明了彩票式转移不是通用的重新编程器，即对于某些目标数据集，重新编程彩票式转移会产生显著效果。",
    "tldr": "重新审视彩票式转移的高效可靠性，研究了线性探测（LP）和视觉提示/重新编程（VP）方法在数据稀疏性和模型稀疏性方面的能力，并发现彩票式转移并非通用的重新编程器。",
    "en_tdlr": "This paper revisits the efficient and reliable transferability of lottery tickets and explores the capabilities of linear probing (LP) and visual prompting/reprogramming (VP) methods under data and model sparsity, finding that lottery tickets are not universal reprogrammers."
}