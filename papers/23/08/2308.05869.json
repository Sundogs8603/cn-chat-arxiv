{
    "title": "Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])",
    "abstract": "Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz",
    "link": "http://arxiv.org/abs/2308.05869",
    "context": "Title: Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])\nAbstract: Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz",
    "path": "papers/23/08/2308.05869.json",
    "total_tokens": 914,
    "translated_title": "面向异构 SoC 的共享内存竞争感知的并发 DNN 执行",
    "translated_abstract": "当前移动和自主系统的两个显著特点是：1）通常同时连续运行多个工作负载，主要是深度神经网络 (DNN) 推理；2）它们在嵌入了针对特定操作的异构加速器的共享内存系统芯片上运行。目前的技术缺乏高效的性能和资源管理技术，无法最大化总系统吞吐量或最小化端到端工作负载延迟。在本研究中，我们提出了一种名为 HaX-CoNN 的新方案，将并发执行的 DNN 推理工作负载的层进行特征化和映射到 SoC 中的多样加速器。我们的方案独特地考虑了每层的执行特性、共享内存 (SM) 的竞争以及加速器之间的转换，以找到最优的调度方案。我们在 NVIDIA Orin、NVIDIA Xavier 和 Qualcomm Snapdragon 865 SoC 上评估了 HaX-CoNN。我们的实验结果表明，HaX-CoNN 可以最小化",
    "tldr": "这项工作提出了一种名为 HaX-CoNN 的方案，可以实现在异构 SoC 上并发执行 DNN 推理工作负载，并考虑了共享内存竞争和加速器之间的转换以找到最优的调度方案。",
    "en_tdlr": "This work proposes a scheme called HaX-CoNN, which enables concurrent execution of DNN inference workloads on heterogeneous SoCs, taking into account shared memory contention and transitions between accelerators to find optimal schedules."
}