{
    "title": "ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment. (arXiv:2308.14448v2 [cs.CV] UPDATED)",
    "abstract": "The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions.We propose an innovative automatic annotation method, supported by Large Language Models (LLMs), to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes tex",
    "link": "http://arxiv.org/abs/2308.14448",
    "context": "Title: ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment. (arXiv:2308.14448v2 [cs.CV] UPDATED)\nAbstract: The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions.We propose an innovative automatic annotation method, supported by Large Language Models (LLMs), to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes tex",
    "path": "papers/23/08/2308.14448.json",
    "total_tokens": 856,
    "translated_title": "ExpCLIP: 通过语义对齐将文本和面部表情融合",
    "translated_abstract": "风格化语音驱动的面部动画的目标是创建包含特定情绪表达的动画。现有方法通常依赖预先设定的情绪标签或面部表情模板，这可能限制准确传达用户意图所必需的灵活性。在这项研究中，我们引入了一种通过利用自然语言作为情绪提示来控制任意风格的技术。这种技术在灵活性和用户友好性方面具有优势。为了实现这个目标，我们首先构建了一个文本-表情对齐数据集（TEAD），其中每个面部表情都与几个类似提示的描述配对。我们提出了一种创新的自动注释方法，支持大型语言模型（LLM），以加快数据集构建速度，从而消除手动注释的大量费用。在此之后，我们利用TEAD来训练一个基于CLIP的模型，称为ExpCLIP，它对文本和面部表达进行编码。",
    "tldr": "本研究介绍了ExpCLIP这一技术，通过语义对齐将文本和面部表情融合，使得风格化语音驱动的面部动画具有灵活性和用户友好性。"
}