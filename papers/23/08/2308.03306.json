{
    "title": "Implicit Graph Neural Diffusion Networks: Convergence, Generalization, and Over-Smoothing",
    "abstract": "arXiv:2308.03306v2 Announce Type: replace  Abstract: Implicit Graph Neural Networks (GNNs) have achieved significant success in addressing graph learning problems recently. However, poorly designed implicit GNN layers may have limited adaptability to learn graph metrics, experience over-smoothing issues, or exhibit suboptimal convergence and generalization properties, potentially hindering their practical performance. To tackle these issues, we introduce a geometric framework for designing implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the metrics of vertex and edge spaces, as well as the graph diffusion strength from data. We show how implicit GNN layers can be viewed as the fixed-point equation of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing during training (OST) and inference (OSI). We further propose a new implicit GNN model to avoid OST and OSI. We establi",
    "link": "https://arxiv.org/abs/2308.03306",
    "context": "Title: Implicit Graph Neural Diffusion Networks: Convergence, Generalization, and Over-Smoothing\nAbstract: arXiv:2308.03306v2 Announce Type: replace  Abstract: Implicit Graph Neural Networks (GNNs) have achieved significant success in addressing graph learning problems recently. However, poorly designed implicit GNN layers may have limited adaptability to learn graph metrics, experience over-smoothing issues, or exhibit suboptimal convergence and generalization properties, potentially hindering their practical performance. To tackle these issues, we introduce a geometric framework for designing implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the metrics of vertex and edge spaces, as well as the graph diffusion strength from data. We show how implicit GNN layers can be viewed as the fixed-point equation of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing during training (OST) and inference (OSI). We further propose a new implicit GNN model to avoid OST and OSI. We establi",
    "path": "papers/23/08/2308.03306.json",
    "total_tokens": 917,
    "translated_title": "隐式图神经扩散网络：收敛性、泛化性和过度平滑问题",
    "translated_abstract": "最近，隐式图神经网络在解决图学习问题方面取得了显著的成功。然而，设计不良的隐式图神经网络层可能对学习图度量具有有限的适应性，经验过度平滑问题，或者表现出次优收敛和泛化性能，可能阻碍它们的实际性能。为了解决这些问题，我们引入了一个基于参数化图拉普拉斯算子的几何框架，用于设计隐式图扩散层。我们的框架允许从数据中学习顶点和边缘空间的度量，以及图扩散强度。我们展示了隐式图神经网络层可以被看作是一个迭代解析能量最小化问题的定点方程，并给出了在训练过程中可能遭受过度平滑问题的条件。我们进一步提出了一个新的隐式图神经网络模型来避免过度平滑问题。",
    "tldr": "这篇论文介绍了隐式图神经扩散网络的设计框架，并解决了其收敛性、泛化性和过度平滑问题。这个框架允许学习度量和图扩散强度，同时提出了一个新的模型来避免过度平滑问题。",
    "en_tdlr": "This paper introduces a geometric framework for designing implicit graph diffusion layers, addressing the convergence, generalization, and over-smoothing issues. It allows learning metrics and the graph diffusion strength, and proposes a new model to avoid over-smoothing."
}