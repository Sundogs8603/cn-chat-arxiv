{
    "title": "Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])",
    "abstract": "State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were",
    "link": "http://arxiv.org/abs/2308.08033",
    "context": "Title: Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])\nAbstract: State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were",
    "path": "papers/23/08/2308.08033.json",
    "total_tokens": 833,
    "translated_title": "使用代码模型和领域适应性的自动化测试用例生成",
    "translated_abstract": "最先进的自动化测试生成技术，例如基于搜索的测试，通常对开发人员创建的测试用例一无所知。因此，它们通常生成的测试用例不易阅读，并且可能无法检测所有复杂缺陷，而开发人员编写的测试用例则可以。在这项研究中，我们利用基于Transformer的代码模型生成可以补充基于搜索测试生成的单元测试。具体而言，我们使用CodeT5，即最先进的大型代码模型，并对测试生成下游任务进行微调。我们使用Methods2test数据集对CodeT5进行微调，并使用Defects4j进行项目级领域适应性和评估。本研究的主要贡献是提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。结果显示，我们的方法可以生成新的测试用例，覆盖了已经被测试过的代码行。",
    "tldr": "本研究提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。"
}