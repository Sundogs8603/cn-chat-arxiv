{
    "title": "Towards Communication-Efficient Model Updating for On-Device Session-Based Recommendation. (arXiv:2308.12777v1 [cs.IR])",
    "abstract": "On-device recommender systems recently have garnered increasing attention due to their advantages of providing prompt response and securing privacy. To stay current with evolving user interests, cloud-based recommender systems are periodically updated with new interaction data. However, on-device models struggle to retrain themselves because of limited onboard computing resources. As a solution, we consider the scenario where the model retraining occurs on the server side and then the updated parameters are transferred to edge devices via network communication. While this eliminates the need for local retraining, it incurs a regular transfer of parameters that significantly taxes network bandwidth. To mitigate this issue, we develop an efficient approach based on compositional codes to compress the model update. This approach ensures the on-device model is updated flexibly with minimal additional parameters whilst utilizing previous knowledge. The extensive experiments conducted on mul",
    "link": "http://arxiv.org/abs/2308.12777",
    "context": "Title: Towards Communication-Efficient Model Updating for On-Device Session-Based Recommendation. (arXiv:2308.12777v1 [cs.IR])\nAbstract: On-device recommender systems recently have garnered increasing attention due to their advantages of providing prompt response and securing privacy. To stay current with evolving user interests, cloud-based recommender systems are periodically updated with new interaction data. However, on-device models struggle to retrain themselves because of limited onboard computing resources. As a solution, we consider the scenario where the model retraining occurs on the server side and then the updated parameters are transferred to edge devices via network communication. While this eliminates the need for local retraining, it incurs a regular transfer of parameters that significantly taxes network bandwidth. To mitigate this issue, we develop an efficient approach based on compositional codes to compress the model update. This approach ensures the on-device model is updated flexibly with minimal additional parameters whilst utilizing previous knowledge. The extensive experiments conducted on mul",
    "path": "papers/23/08/2308.12777.json",
    "total_tokens": 860,
    "translated_title": "面向通信高效的在线设备会话推荐模型更新",
    "translated_abstract": "最近，基于设备的推荐系统由于其提供即时响应和保护隐私的优势而受到越来越多的关注。为了与用户的兴趣变化保持同步，云端推荐系统会定期使用新的交互数据进行更新。然而，由于有限的设备计算资源，设备上的模型难以进行重新训练。作为解决方案，我们考虑了模型重新训练发生在服务器端的情景，然后通过网络通信将更新的参数传输到边缘设备。虽然这消除了本地重新训练的需求，但也导致了常规参数传输，给网络带宽带来了显著的压力。为了缓解这个问题，我们基于组合编码开发了一种高效的方法来压缩模型更新。通过这种方法，可以在尽量使用先前知识的同时，灵活地更新设备上的模型，减少额外的参数。我们进行了大量的实验来验证这种方法的有效性。",
    "tldr": "这项研究提出了一种基于组合编码的高效方法，用于在云端更新推荐系统模型并通过网络通信传输到设备上，以解决设备资源有限和网络带宽压力的问题。",
    "en_tdlr": "This research proposes an efficient approach based on compositional codes to update recommender system models on the cloud and transfer them to devices via network communication, addressing the issues of limited device resources and network bandwidth pressure."
}