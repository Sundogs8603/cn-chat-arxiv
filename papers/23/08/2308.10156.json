{
    "title": "SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-Image Generation",
    "abstract": "arXiv:2308.10156v2 Announce Type: replace-cross  Abstract: Despite significant progress in Text-to-Image (T2I) generative models, even lengthy and complex text descriptions still struggle to convey detailed controls. In contrast, Layout-to-Image (L2I) generation, aiming to generate realistic and complex scene images from user-specified layouts, has risen to prominence. However, existing methods transform layout information into tokens or RGB images for conditional control in the generative process, leading to insufficient spatial and semantic controllability of individual instances. To address these limitations, we propose a novel Spatial-Semantic Map Guided (SSMG) diffusion model that adopts the feature map, derived from the layout, as guidance. Owing to rich spatial and semantic information encapsulated in well-designed feature maps, SSMG achieves superior generation quality with sufficient spatial and semantic controllability compared to previous works. Additionally, we propose the ",
    "link": "https://arxiv.org/abs/2308.10156",
    "context": "Title: SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-Image Generation\nAbstract: arXiv:2308.10156v2 Announce Type: replace-cross  Abstract: Despite significant progress in Text-to-Image (T2I) generative models, even lengthy and complex text descriptions still struggle to convey detailed controls. In contrast, Layout-to-Image (L2I) generation, aiming to generate realistic and complex scene images from user-specified layouts, has risen to prominence. However, existing methods transform layout information into tokens or RGB images for conditional control in the generative process, leading to insufficient spatial and semantic controllability of individual instances. To address these limitations, we propose a novel Spatial-Semantic Map Guided (SSMG) diffusion model that adopts the feature map, derived from the layout, as guidance. Owing to rich spatial and semantic information encapsulated in well-designed feature maps, SSMG achieves superior generation quality with sufficient spatial and semantic controllability compared to previous works. Additionally, we propose the ",
    "path": "papers/23/08/2308.10156.json",
    "total_tokens": 836,
    "translated_title": "SSMG：空间语义地图引导扩散模型用于自由形式布局到图像生成",
    "translated_abstract": "尽管文本到图像(T2I)生成模型取得了显著进展，但即使是冗长复杂的文本描述仍然难以传达详细的控制。相比之下，布局到图像(L2I)生成旨在从用户指定的布局生成逼真且复杂的场景图像，现已引起人们的关注。然而，现有方法将布局信息转换为标记或RGB图像，用于生成过程中的条件控制，导致个体实例的空间和语义可控性不足。为解决这些限制，我们提出了一种新颖的空间语义地图引导(SSMG)扩散模型，采用从布局中获取的特征图作为引导。由于富含设计良好的特征图内的丰富空间和语义信息，SSMG相比之前的工作实现了更优秀的生成质量，具有足够的空间和语义可控性。此外，我们还提出了",
    "tldr": "SSMG模型采用特征图作为引导，相比其他方法具有更好的生成质量和空间语义可控性",
    "en_tdlr": "The SSMG model utilizes feature map as guidance, achieving superior generation quality and spatial-semantic controllability compared to other methods."
}