{
    "title": "Symphony: Optimized Model Serving using Centralized Orchestration. (arXiv:2308.07470v1 [cs.DC])",
    "abstract": "The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.",
    "link": "http://arxiv.org/abs/2308.07470",
    "context": "Title: Symphony: Optimized Model Serving using Centralized Orchestration. (arXiv:2308.07470v1 [cs.DC])\nAbstract: The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.",
    "path": "papers/23/08/2308.07470.json",
    "total_tokens": 898,
    "translated_title": "Symphony: 使用集中式协调来优化模型服务",
    "translated_abstract": "在GPU集群上进行深度神经网络(DNN)模型推理的协调存在两个重要挑战：在满足批处理属性的模型推理同时实现高加速器效率，并满足延迟服务级别目标(SLO)，以及适应工作负载的变化，无论是短期波动还是长期资源分配。为了解决这些挑战，我们提出了Symphony，这是一个可以扩展到每秒数百万个请求并协调数万个GPU的集中式调度系统。我们的系统使用非工作保持调度算法，能够实现高批处理效率，同时也能实现强大的自动缩放。此外，我们还开发了一个基于模型的计算和内存需求分配子集群的算法。通过广泛的实验，我们证明Symphony的性能优于之前的系统，最多可以提高4.7倍的吞吐量。",
    "tldr": "Symphony是一个集中式调度系统，可以优化深度神经网络模型服务，在满足高加速器效率和延迟SLO的同时适应工作负载变化。通过非工作保持调度算法和模型分配算法，Symphony能够实现高批处理效率和强大的自动缩放功能，比之前的系统提供高达4.7倍的吞吐量。"
}