{
    "title": "Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning. (arXiv:2308.14705v1 [stat.ML])",
    "abstract": "Ensembling a neural network is a widely recognized approach to enhance model performance, estimate uncertainty, and improve robustness in deep supervised learning. However, deep ensembles often come with high computational costs and memory demands. In addition, the efficiency of a deep ensemble is related to diversity among the ensemble members which is challenging for large, over-parameterized deep neural networks. Moreover, ensemble learning has not yet seen such widespread adoption, and it remains a challenging endeavor for self-supervised or unsupervised representation learning. Motivated by these challenges, we present a novel self-supervised training regime that leverages an ensemble of independent sub-networks, complemented by a new loss function designed to encourage diversity. Our method efficiently builds a sub-model ensemble with high diversity, leading to well-calibrated estimates of model uncertainty, all achieved with minimal computational overhead compared to traditional",
    "link": "http://arxiv.org/abs/2308.14705",
    "context": "Title: Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning. (arXiv:2308.14705v1 [stat.ML])\nAbstract: Ensembling a neural network is a widely recognized approach to enhance model performance, estimate uncertainty, and improve robustness in deep supervised learning. However, deep ensembles often come with high computational costs and memory demands. In addition, the efficiency of a deep ensemble is related to diversity among the ensemble members which is challenging for large, over-parameterized deep neural networks. Moreover, ensemble learning has not yet seen such widespread adoption, and it remains a challenging endeavor for self-supervised or unsupervised representation learning. Motivated by these challenges, we present a novel self-supervised training regime that leverages an ensemble of independent sub-networks, complemented by a new loss function designed to encourage diversity. Our method efficiently builds a sub-model ensemble with high diversity, leading to well-calibrated estimates of model uncertainty, all achieved with minimal computational overhead compared to traditional",
    "path": "papers/23/08/2308.14705.json",
    "total_tokens": 876,
    "translated_title": "自助的鲁棒性表示学习的独立子网络多样化集成",
    "translated_abstract": "集成神经网络是提高模型性能、估计不确定性和改善深度有监督学习鲁棒性的广泛承认方法。然而，深层集成通常具有高计算成本和内存需求。此外，深度集成的效率与集成成员之间的多样性有关，这对于大型的过参数化深度神经网络来说是具有挑战性的。而且，集成学习尚未得到如此广泛的采用，并且对于自助的或无监督的表示学习来说仍然是一项具有挑战性的工作。在这些挑战的推动下，我们提出了一个新颖的自助培训体制，利用独立子网络的集成，辅以一个旨在鼓励多样性的新损失函数。我们的方法以高多样性实现了高效的子模型集成，从而获得了良好校准的模型不确定性估计，并与传统方法相比，计算开销最小。",
    "tldr": "本文提出了一个新的自助培训体制，利用独立子网络的集成和新的损失函数来提高自助的鲁棒性表示学习的效率和多样性。",
    "en_tdlr": "This paper presents a novel self-supervised training regime that leverages an ensemble of independent sub-networks and a new loss function to improve the efficiency and diversity of self-supervised representation learning."
}