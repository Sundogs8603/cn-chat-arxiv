{
    "title": "Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems. (arXiv:2308.13246v1 [cs.LG])",
    "abstract": "Model-free RL-based recommender systems have recently received increasing research attention due to their capability to handle partial feedback and long-term rewards. However, most existing research has ignored a critical feature in recommender systems: one user's feedback on the same item at different times is random. The stochastic rewards property essentially differs from that in classic RL scenarios with deterministic rewards, which makes RL-based recommender systems much more challenging. In this paper, we first demonstrate in a simulator environment where using direct stochastic feedback results in a significant drop in performance. Then to handle the stochastic feedback more efficiently, we design two stochastic reward stabilization frameworks that replace the direct stochastic feedback with that learned by a supervised model. Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models. We demonstrate the superiority of the proposed framework",
    "link": "http://arxiv.org/abs/2308.13246",
    "context": "Title: Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems. (arXiv:2308.13246v1 [cs.LG])\nAbstract: Model-free RL-based recommender systems have recently received increasing research attention due to their capability to handle partial feedback and long-term rewards. However, most existing research has ignored a critical feature in recommender systems: one user's feedback on the same item at different times is random. The stochastic rewards property essentially differs from that in classic RL scenarios with deterministic rewards, which makes RL-based recommender systems much more challenging. In this paper, we first demonstrate in a simulator environment where using direct stochastic feedback results in a significant drop in performance. Then to handle the stochastic feedback more efficiently, we design two stochastic reward stabilization frameworks that replace the direct stochastic feedback with that learned by a supervised model. Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models. We demonstrate the superiority of the proposed framework",
    "path": "papers/23/08/2308.13246.json",
    "total_tokens": 927,
    "translated_title": "无模型强化学习在推荐系统中的应用: 随机奖励稳定化",
    "translated_abstract": "最近，基于无模型的强化学习（RL）的推荐系统因其处理部分反馈和长期奖励的能力而受到越来越多的研究关注。然而，大多数现有研究忽略了推荐系统中的一个关键特征：同一用户在不同时间对同一项的反馈是随机的。随机奖励的特性与具有确定性奖励的经典RL场景本质上不同，这使得基于RL的推荐系统更具挑战性。本文首先在一个模拟环境中展示了直接使用随机反馈会导致性能显著下降。为了更有效地处理随机反馈，我们设计了两种随机奖励稳定化框架，用于用监督模型学习到的奖励替代直接的随机反馈。这两个框架都是模型无关的，即它们可以有效地利用各种监督模型。我们证明了所提出的框架的优越性。",
    "tldr": "本文研究了在推荐系统中应用无模型强化学习的问题。针对推荐系统中随机奖励的特性，我们设计了两种随机奖励稳定化框架，用于更有效地处理随机反馈。我们的实验证明了这些框架的优越性。",
    "en_tdlr": "This paper investigates the use of model-free reinforcement learning in recommender systems. Considering the stochastic rewards in recommender systems, the authors design two stochastic reward stabilization frameworks to handle random feedback more efficiently. Experimental results demonstrate the superiority of these frameworks."
}