{
    "title": "ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. (arXiv:2308.02223v1 [cs.CL])",
    "abstract": "Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\\textit{e.g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (\\textit{e.g.,} a vocabulary) and a long action sequence (\\textit{e.g.,} a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-base",
    "link": "http://arxiv.org/abs/2308.02223",
    "context": "Title: ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. (arXiv:2308.02223v1 [cs.CL])\nAbstract: Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\\textit{e.g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (\\textit{e.g.,} a vocabulary) and a long action sequence (\\textit{e.g.,} a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-base",
    "path": "papers/23/08/2308.02223.json",
    "total_tokens": 851,
    "translated_title": "ESRL: 高效采样的基于强化学习的序列生成方法",
    "translated_abstract": "应用强化学习（RL）于序列生成模型能够直接优化长期回报（如BLEU和人类反馈），但通常需要对动作序列空间进行大规模采样。这在序列生成问题中是一个计算挑战，比如机器翻译，我们经常处理一个大的动作空间（如词汇表）和长的动作序列（如翻译）。在本工作中，我们引入了两阶段采样和动态采样方法，以提高训练序列生成模型的采样效率。我们在传统的序列生成任务上进行了实验，包括机器翻译和抽象摘要。此外，我们通过训练一个大型语言模型使用奖励模型，对我们的方法进行了RLHF评估。实验结果表明，这种高效采样的基于强化学习的方法",
    "tldr": "提出了两阶段采样和动态采样方法来提高训练序列生成模型的采样效率，实验证明这种方法在传统的序列生成任务中表现良好，同时在人类反馈强化学习方面也有很好的效果。",
    "en_tdlr": "Two-stage sampling and dynamic sampling approaches are proposed to improve the sampling efficiency during training sequence generation models. Experimental results show that this method performs well in traditional sequence generation tasks and also has good performance in reinforcement learning from human feedback."
}