{
    "title": "LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model. (arXiv:2308.11945v1 [cs.CV])",
    "abstract": "Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency bet",
    "link": "http://arxiv.org/abs/2308.11945",
    "context": "Title: LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model. (arXiv:2308.11945v1 [cs.CV])\nAbstract: Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency bet",
    "path": "papers/23/08/2308.11945.json",
    "total_tokens": 925,
    "translated_title": "长时舞蹈生成与条件扩散模型(LongDanceDiff)",
    "translated_abstract": "伴随音乐跳舞一直是表达情感的重要艺术形式。由于高度复杂的时间空间结构，与音乐同步的长时3D逼真舞蹈生成具有挑战性。现有的方法在生成长时间舞蹈时存在积累误差和训练-推理偏差引起的冻结问题。为解决这个问题，我们设计了一种条件扩散模型LongDanceDiff，用于序列到序列的长期舞蹈生成，解决了时间连贯性和空间约束的挑战。LongDanceDiff包含了一个基于Transformer的扩散模型，输入是音乐、过去的动作和噪声未来的动作的串联。这种部分加噪的策略利用了全注意机制，并学习了音乐和过去动作之间的依赖关系。为增加生成舞蹈动作的多样性和减轻冻结问题，我们引入了互信息最小化目标，用于规范化依赖关系。",
    "tldr": "本论文提出了一种条件扩散模型，LongDanceDiff，用于长时舞蹈生成。该模型解决了时间连贯性和空间约束的挑战，并通过部分加噪策略和互信息最小化目标来增加动作多样性和减轻冻结问题。",
    "en_tdlr": "This paper proposes a conditional diffusion model, LongDanceDiff, for long-term dance generation. The model addresses the challenges of temporal coherency and spatial constraint, and enhances motion diversity and mitigates freezing problem through partial noising strategy and mutual information minimization objective."
}