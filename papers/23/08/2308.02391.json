{
    "title": "Learning Optimal Admission Control in Partially Observable Queueing Networks. (arXiv:2308.02391v1 [cs.LG])",
    "abstract": "We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.  While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.  The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death proces",
    "link": "http://arxiv.org/abs/2308.02391",
    "context": "Title: Learning Optimal Admission Control in Partially Observable Queueing Networks. (arXiv:2308.02391v1 [cs.LG])\nAbstract: We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.  While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.  The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death proces",
    "path": "papers/23/08/2308.02391.json",
    "total_tokens": 918,
    "translated_title": "在部分可观察队列网络中学习最优入场控制",
    "translated_abstract": "我们提出了一种高效的强化学习算法，用于在部分可观察的队列网络中学习最优入场控制策略。具体而言，只能观察到网络的到达和离开时间，并且优化指的是无限时域内的平均持有/拒绝成本。虽然在部分可观察的马尔可夫决策过程（POMDP）中进行强化学习通常是非常昂贵的，但我们展示了我们的算法的遗憾仅仅依赖于网络中作业数量的最大值S的亚线性函数。特别地，与现有的遗憾分析相比，我们的遗憾上界不依赖于底层马尔可夫决策过程（MDP）的直径，而在大多数队列系统中，这个直径至少是指数级的。",
    "tldr": "本文提出了一种在部分可观察队列网络中学习最优入场控制策略的高效强化学习算法，通过利用闭合产品形式队列网络的Norton等效定理和结构化的出生死亡过程 MDP 的高效强化学习算法，实现了对最大作业数量的亚线性遗憾。",
    "en_tdlr": "This paper presents an efficient reinforcement learning algorithm for learning the optimal admission control policy in partially observable queueing networks. The algorithm leverages Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes, achieving sub-linear regret with respect to the maximal number of jobs in the network."
}