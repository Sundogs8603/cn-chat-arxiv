{
    "title": "Single-shot Bayesian approximation for neural networks. (arXiv:2308.12785v1 [cs.LG])",
    "abstract": "Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our ",
    "link": "http://arxiv.org/abs/2308.12785",
    "context": "Title: Single-shot Bayesian approximation for neural networks. (arXiv:2308.12785v1 [cs.LG])\nAbstract: Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our ",
    "path": "papers/23/08/2308.12785.json",
    "total_tokens": 922,
    "translated_title": "单次贝叶斯近似用于神经网络",
    "translated_abstract": "深度神经网络以其高预测性能而闻名。然而，当遇到完全新的情况并且没有指示其不确定性时，神经网络很容易产生不可靠的预测。贝叶斯变体的神经网络（BNNs），如蒙特卡洛（MC）dropout BNNs，在提供不确定度测量的同时提高了预测性能。BNNs唯一的缺点是它们在测试时计算时间较长，因为它们依赖于一种采样方法。在这里，我们提出了一种单次MC dropout近似，它保留了BNNs的优点，同时与神经网络一样快。我们的方法基于矩传播（MP），可以在常用的神经网络层（卷积、最大池化、全连接、softmax和dropout层）中解析地近似MC dropout信号的期望值和方差。MP方法可以在不重新训练的情况下将神经网络转换为BNN，只要NN已经使用标准的dropout进行了训练。我们评估了我们的方法。",
    "tldr": "这篇论文提出了一种单次MC dropout近似方法，以将神经网络转换为贝叶斯变体的神经网络，该方法具有与普通神经网络相同的计算速度，同时保留了贝叶斯变体神经网络提供的不确定度测量。",
    "en_tdlr": "This paper presents a single-shot MC dropout approximation method to convert neural networks into Bayesian variants, which has the same computational speed as regular neural networks while maintaining uncertainty measures provided by Bayesian variants."
}