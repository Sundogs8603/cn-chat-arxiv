{
    "title": "Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification. (arXiv:2308.15063v1 [cs.CV])",
    "abstract": "Visible-Infrared person re-identification (VI-ReID) is an important and challenging task in intelligent video surveillance. Existing methods mainly focus on learning a shared feature space to reduce the modality discrepancy between visible and infrared modalities, which still leave two problems underexplored: information redundancy and modality complementarity. To this end, properly eliminating the identity-irrelevant information as well as making up for the modality-specific information are critical and remains a challenging endeavor. To tackle the above problems, we present a novel mutual information and modality consensus network, namely CMInfoNet, to extract modality-invariant identity features with the most representative information and reduce the redundancies. The key insight of our method is to find an optimal representation to capture more identity-relevant information and compress the irrelevant parts by optimizing a mutual information bottleneck trade-off. Besides, we propos",
    "link": "http://arxiv.org/abs/2308.15063",
    "context": "Title: Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification. (arXiv:2308.15063v1 [cs.CV])\nAbstract: Visible-Infrared person re-identification (VI-ReID) is an important and challenging task in intelligent video surveillance. Existing methods mainly focus on learning a shared feature space to reduce the modality discrepancy between visible and infrared modalities, which still leave two problems underexplored: information redundancy and modality complementarity. To this end, properly eliminating the identity-irrelevant information as well as making up for the modality-specific information are critical and remains a challenging endeavor. To tackle the above problems, we present a novel mutual information and modality consensus network, namely CMInfoNet, to extract modality-invariant identity features with the most representative information and reduce the redundancies. The key insight of our method is to find an optimal representation to capture more identity-relevant information and compress the irrelevant parts by optimizing a mutual information bottleneck trade-off. Besides, we propos",
    "path": "papers/23/08/2308.15063.json",
    "total_tokens": 891,
    "translated_title": "学习异构人员再识别的跨模态信息瓶颈表示",
    "translated_abstract": "可见光红外人员再识别（VI-ReID）是智能视频监控中一项重要而具有挑战性的任务。现有方法主要集中于学习一个共享特征空间，以减少可见光和红外模态之间的差异，然而仍有两个问题尚未得到充分探索：信息冗余和模态补充。为此，合理地消除与身份无关的信息，同时补充模态特定的信息至关重要，且仍然是一个具有挑战性的任务。为解决上述问题，我们提出了一种新颖的相互信息和模态一致性网络，即CMInfoNet，以提取具有最具代表性信息的模态不变身份特征并减少冗余。我们方法的关键洞见是找到一种最佳表示来捕捉更多与身份相关的信息，并通过优化相互信息瓶颈折衷来压缩无关部分。此外，我们还提出了一种方法来解决模态补充问题。",
    "tldr": "本论文介绍了一种名为CMInfoNet的新型网络算法，通过优化相互信息瓶颈折衷，提取具有最具代表性信息的模态不变身份特征，并减少信息冗余和模态补充问题。",
    "en_tdlr": "This paper proposes a novel network algorithm called CMInfoNet, which extracts modality-invariant identity features with the most representative information and reduces information redundancy and modality complementarity by optimizing a mutual information bottleneck trade-off."
}