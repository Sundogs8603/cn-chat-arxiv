{
    "title": "GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism. (arXiv:2308.10087v2 [cs.DC] UPDATED)",
    "abstract": "Communication is a key bottleneck for distributed graph neural network (GNN) training. This paper proposes GNNPipe, a new approach that scales the distributed full-graph deep GNN training. Being the first to use layer-level model parallelism for GNN training, GNNPipe partitions GNN layers among GPUs, each device performs the computation for a disjoint subset of consecutive GNN layers on the whole graph. Compared to graph parallelism with each GPU handling a graph partition, GNNPipe reduces the communication volume by a factor of the number of GNN layers. GNNPipe overcomes the unique challenges for pipelined layer-level model parallelism on the whole graph by partitioning it into dependent chunks, allowing the use of historical vertex embeddings, and applying specific training techniques to ensure convergence. We also propose a hybrid approach by combining GNNPipe with graph parallelism to handle large graphs, achieve better computer resource utilization and ensure model convergence. We",
    "link": "http://arxiv.org/abs/2308.10087",
    "context": "Title: GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism. (arXiv:2308.10087v2 [cs.DC] UPDATED)\nAbstract: Communication is a key bottleneck for distributed graph neural network (GNN) training. This paper proposes GNNPipe, a new approach that scales the distributed full-graph deep GNN training. Being the first to use layer-level model parallelism for GNN training, GNNPipe partitions GNN layers among GPUs, each device performs the computation for a disjoint subset of consecutive GNN layers on the whole graph. Compared to graph parallelism with each GPU handling a graph partition, GNNPipe reduces the communication volume by a factor of the number of GNN layers. GNNPipe overcomes the unique challenges for pipelined layer-level model parallelism on the whole graph by partitioning it into dependent chunks, allowing the use of historical vertex embeddings, and applying specific training techniques to ensure convergence. We also propose a hybrid approach by combining GNNPipe with graph parallelism to handle large graphs, achieve better computer resource utilization and ensure model convergence. We",
    "path": "papers/23/08/2308.10087.json",
    "total_tokens": 919,
    "translated_title": "GNNPipe：使用流水线模型并行实现深度GNN训练的扩展",
    "translated_abstract": "通信是分布式图神经网络（GNN）训练的关键瓶颈。本文提出了GNNPipe，一种新的方法，用于扩展分布式的全图深度GNN训练。作为首次使用层级模型并行性进行GNN训练的方法，GNNPipe将GNN层划分在不同的GPU上，每个设备在整个图上为一组连续的GNN层执行计算。与每个GPU处理一个图划分的图并行性相比，GNNPipe将通信量减少了GNN层数的倍数。GNNPipe克服了整个图上流水线层级模型并行的独特挑战，通过将图划分为相互依赖的块，允许使用历史顶点嵌入，并应用特定的训练技术以确保收敛性。我们还提出了一种混合策略，将GNNPipe与图并行结合以处理大型图，实现更好的计算资源利用和模型收敛性的保证。",
    "tldr": "GNNPipe是一种扩展分布式全图深度GNN训练的方法，使用层级模型并行性将GNN层划分在不同的GPU上，通过减少通信量和处理特定挑战，实现了更好的计算资源利用和模型收敛性的保证。",
    "en_tdlr": "GNNPipe is a method to scale distributed full-graph deep GNN training by using layer-level model parallelism on different GPUs, reducing communication volume, overcoming specific challenges, and achieving better resource utilization and model convergence."
}