{
    "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])",
    "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinemen",
    "link": "http://arxiv.org/abs/2308.12033",
    "context": "Title: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])\nAbstract: As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinemen",
    "path": "papers/23/08/2308.12033.json",
    "total_tokens": 863,
    "translated_title": "PREFER: 通过反馈-反思-优化的提示集成学习",
    "translated_abstract": "作为发挥大型语言模型 (LLMs) 强大能力的有效工具，提示最近在各种复杂任务中展示出了前所未有的能力。为了进一步提高性能，提示集成引起了人们的广泛兴趣，以解决 LLMs 的幻觉和不稳定性问题。然而，现有方法通常采用两阶段的范式，需要大量手动准备的提示集合，并且无法针对不同的弱学习器进行有针对性的优化。在本文中，我们提出了一种简单、通用、自动化的方法，命名为 PREFER (通过反馈-反思-优化的提示集成学习)，来解决上述限制。具体来说，考虑到弱学习器应该关注提升过程中的困难样本，PREFER 构建了一个反馈机制，用于反思现有弱学习器的不足之处。基于此，LLM 需要自动合成新的提示来进行迭代优化。",
    "tldr": "本文提出了一种名为PREFER的方法，通过反馈机制和自动合成新的提示来解决大型语言模型中的幻觉和不稳定性问题，从而提高性能。",
    "en_tdlr": "This paper proposes a method named PREFER that addresses the hallucination and instability issues in large language models by using a feedback mechanism and automatically synthesizing new prompts, thereby improving performance"
}