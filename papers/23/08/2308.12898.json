{
    "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])",
    "abstract": "The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop",
    "link": "http://arxiv.org/abs/2308.12898",
    "context": "Title: Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])\nAbstract: The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop",
    "path": "papers/23/08/2308.12898.json",
    "total_tokens": 874,
    "translated_title": "语言知识能改进视觉-语言预训练中的多模态对齐吗？",
    "translated_abstract": "多媒体领域对通过多模态预训练神经网络模型感知和表达物理世界展现出了强烈的兴趣，其中视觉-语言相关的研究是当前最吸引人的话题之一。然而，目前对以下两个问题的探索非常有限：1）在视觉-语言预训练中是否可以提取关键的语言知识（如语义和句法），2）这种语言知识如何影响或增强多模态对齐。因此，本文旨在阐明全面的语言知识，包括语义表达和句法结构，对多模态对齐的影响。具体而言，我们设计并发布了SNARE，第一个大规模的多模态对齐探测基准，来检测关键的语言组成部分，如词汇、语义和句法知识，包含了四个任务：语义结构、否定逻辑、属性归属和关系组合。基于我们的实验结果，我们证明了.....",
    "tldr": "本论文研究了语言知识在多模态对齐中的作用，设计并发布了一个多模态对齐探测基准来检测关键的语言组成部分。"
}