{
    "title": "Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.",
    "link": "http://arxiv.org/abs/2308.09544",
    "context": "Title: Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)\nAbstract: In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.",
    "path": "papers/23/08/2308.09544.json",
    "total_tokens": 876,
    "translated_title": "适应您的教师: 改进知识蒸馏用于无样本连续学习",
    "translated_abstract": "在这项工作中，我们研究了无样本类别增量学习（CIL），并使用知识蒸馏（KD）作为正则化策略，旨在防止遗忘。KD方法在CIL中取得了成功，但是在没有访问先前任务的训练数据示例的情况下，它们往往很难对模型进行正则化。我们的分析发现，这个问题源于处理分布外数据时教师网络中的显著表示转换。这导致KD损失成分中出现较大的错误，从而导致CIL模型性能下降。受最近的测试时适应方法的启发，我们引入了教师适应（TA）方法，该方法在增量训练过程中同时更新教师和主模型。我们的方法与基于KD的CIL方法无缝集成，能够在多个无样本CIL基准测试中持续提升它们的性能。",
    "tldr": "本研究提出了一种适应教师的方法（TA）用于无样本连续学习，解决了知识蒸馏方法在这种情况下的性能下降问题，并在多个基准测试中持续提升模型性能。",
    "en_tdlr": "This study proposes a Teacher Adaptation (TA) method for exemplar-free continual learning, addressing the performance degradation issue of knowledge distillation methods in this scenario, and consistently improving model performance across multiple benchmarks."
}