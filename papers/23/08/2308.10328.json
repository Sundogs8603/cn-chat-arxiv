{
    "title": "A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)",
    "abstract": "Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re",
    "link": "http://arxiv.org/abs/2308.10328",
    "context": "Title: A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)\nAbstract: Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re",
    "path": "papers/23/08/2308.10328.json",
    "total_tokens": 846,
    "translated_title": "在线持续学习的综合实证评估",
    "translated_abstract": "在线持续学习旨在通过在数据流上直接学习，处理时间变化的分布，并仅存储一小部分数据，以更接近实时学习体验。在这个实证评估中，我们评估了文献中解决在线持续学习问题的各种方法。具体而言，我们在图像分类的类增量设置下，从数据流中逐步学习新的类别。我们在Split-CIFAR100和Split-TinyImagenet基准上比较了这些方法，并测量它们的平均准确性、遗忘率、稳定性和表示质量，以评估算法在整个训练过程中的多个方面。我们发现大多数方法都存在稳定性和欠拟合问题。然而，在相同的计算预算下，所学习的表示与独立同分布的训练相当。没有明确的优胜者从重新评估后的结果中出现。",
    "tldr": "这项综合实证评估了解决在线持续学习问题的各种方法，发现大多数方法存在稳定性和欠拟合问题，但所学习的表示与独立同分布的训练相当。",
    "en_tdlr": "This comprehensive empirical evaluation investigates various methods for tackling online continual learning and finds that most methods suffer from stability and underfitting issues, but the learned representations are comparable to i.i.d. training."
}