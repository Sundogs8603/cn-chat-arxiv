{
    "title": "InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning. (arXiv:2308.15609v1 [cs.LG])",
    "abstract": "One-Shot Neural Architecture Search (NAS) algorithms often rely on training a hardware agnostic super-network for a domain specific task. Optimal sub-networks are then extracted from the trained super-network for different hardware platforms. However, training super-networks from scratch can be extremely time consuming and compute intensive especially for large models that rely on a two-stage training process of pre-training and fine-tuning. State of the art pre-trained models are available for a wide range of tasks, but their large sizes significantly limits their applicability on various hardware platforms. We propose InstaTune, a method that leverages off-the-shelf pre-trained weights for large models and generates a super-network during the fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process happens during fine-tuning, it minimizes the overall time and compute resources required for NAS. Secondly, the sub-networks extracted are optimized for the target ta",
    "link": "http://arxiv.org/abs/2308.15609",
    "context": "Title: InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning. (arXiv:2308.15609v1 [cs.LG])\nAbstract: One-Shot Neural Architecture Search (NAS) algorithms often rely on training a hardware agnostic super-network for a domain specific task. Optimal sub-networks are then extracted from the trained super-network for different hardware platforms. However, training super-networks from scratch can be extremely time consuming and compute intensive especially for large models that rely on a two-stage training process of pre-training and fine-tuning. State of the art pre-trained models are available for a wide range of tasks, but their large sizes significantly limits their applicability on various hardware platforms. We propose InstaTune, a method that leverages off-the-shelf pre-trained weights for large models and generates a super-network during the fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process happens during fine-tuning, it minimizes the overall time and compute resources required for NAS. Secondly, the sub-networks extracted are optimized for the target ta",
    "path": "papers/23/08/2308.15609.json",
    "total_tokens": 801,
    "translated_title": "InstaTune: 在微调期间即时神经架构搜索",
    "translated_abstract": "一次性神经架构搜索（NAS）算法通常依赖于为特定领域任务训练硬件无关的超级网络。然后从训练好的超级网络中提取出适用于不同硬件平台的最佳子网络。然而，从头开始训练超级网络可能非常耗时且计算密集，特别是对于依赖于预训练和微调的两阶段训练过程的大型模型。现有的最先进的预训练模型适用于各种任务，但它们的尺寸较大，极大限制了它们在不同硬件平台上的适用性。我们提出了InstaTune，一种利用即时微调阶段生成超级网络的方法。InstaTune具有多个优点。首先，由于该过程发生在微调期间，它可以最小化进行NAS所需的总时间和计算资源。其次，提取出的子网络针对目标任务进行了优化。",
    "tldr": "InstaTune是一种在微调阶段即时生成超级网络的方法，可以最小化神经架构搜索所需的时间和计算资源。"
}