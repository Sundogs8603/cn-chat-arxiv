{
    "title": "Coarse-to-Fine Multi-Scene Pose Regression with Transformers. (arXiv:2308.11783v1 [cs.CV])",
    "abstract": "Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \\cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly b",
    "link": "http://arxiv.org/abs/2308.11783",
    "context": "Title: Coarse-to-Fine Multi-Scene Pose Regression with Transformers. (arXiv:2308.11783v1 [cs.CV])\nAbstract: Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \\cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly b",
    "path": "papers/23/08/2308.11783.json",
    "total_tokens": 839,
    "translated_title": "用Transformers进行粗到精多场景姿态回归",
    "translated_abstract": "绝对相机姿态回归器根据仅靠图像进行估计，给出相机的位置和方向。通常，使用卷积骨干网络和多层感知机（MLP）头部，通过图像和姿态标签进行训练，一次仅嵌入一个参考场景。最近，通过用一组全连接层替换MLP头部，将此方案扩展到学习多个场景。在本文中，我们提出了用Transformers学习多场景绝对相机姿态回归的方法，其中编码器用于通过自注意力聚合激活图，并且解码器将潜在特征和场景编码转化为姿态预测。这使得我们的模型能够专注于对定位有信息量的通用特征，同时并行嵌入多个场景。我们通过引入混合分类-回归架构改进了我们先前的MS-Transformer方法\\cite{shavit2021learning}，提高了定位准确性。我们的方法在常见的数据集上进行了评估。",
    "tldr": "本文提出了一种使用Transformers进行多场景精确相机姿态回归的方法，通过自注意力聚合激活图和解码潜在特征来实现对通用特征的定位，并在多个场景中并行嵌入。"
}