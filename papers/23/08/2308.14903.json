{
    "title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory. (arXiv:2308.14903v1 [cs.CL])",
    "abstract": "Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations.  We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora.",
    "link": "http://arxiv.org/abs/2308.14903",
    "context": "Title: MEMORY-VQ: Compression for Tractable Internet-Scale Memory. (arXiv:2308.14903v1 [cs.CL])\nAbstract: Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations.  We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora.",
    "path": "papers/23/08/2308.14903.json",
    "total_tokens": 817,
    "translated_title": "MEMORY-VQ：用于可操作的互联网规模内存的压缩",
    "translated_abstract": "检索增强是使语言模型更加了解世界的一种强大但昂贵的方法。像LUMEN这样的基于内存的方法通过预计算检索到的段落的令牌表示来大大加快推理速度。然而，内存也导致了更大的存储需求，用于存储预计算的表示。我们提出了MEMORY-VQ，一种新的方法来减少内存增强模型的存储需求，同时不牺牲性能。我们的方法使用向量量化变分自动编码器（VQ-VAE）来压缩令牌表示。我们将MEMORY-VQ应用于LUMEN模型，得到了LUMEN-VQ，这是一个在KILT基准测试中具有可比性能的内存模型，压缩率为16倍。LUMEN-VQ使得即使对于非常大的检索语料库，实际的检索增强也变得可行。",
    "tldr": "MEMORY-VQ是一种使用向量量化压缩方法来减少内存增强模型存储需求的新方法，不牺牲性能。应用于LUMEN模型后，LUMEN-VQ在KILT基准测试上获得了16倍的压缩率，使得对于极大的检索语料库而言实现实际的检索增强成为可能。"
}