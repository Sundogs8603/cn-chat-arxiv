{
    "title": "A free from local minima algorithm for training regressive MLP neural networks. (arXiv:2308.11532v1 [cs.LG])",
    "abstract": "In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the proper",
    "link": "http://arxiv.org/abs/2308.11532",
    "context": "Title: A free from local minima algorithm for training regressive MLP neural networks. (arXiv:2308.11532v1 [cs.LG])\nAbstract: In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the proper",
    "path": "papers/23/08/2308.11532.json",
    "total_tokens": 656,
    "translated_title": "一种克服局部最小值的算法用于训练回归MLP神经网络",
    "translated_abstract": "本文介绍了一种创新的方法用于训练回归MLP网络，该方法不易受到局部最小值的影响。误差反向传播算法在机器学习技术的发展中起到了重要作用，但是该算法存在局部最小值的问题。本文提出的算法避免了局部最小值的问题。",
    "tldr": "本文介绍了一种克服局部最小值问题的算法，用于训练回归MLP神经网络。",
    "en_tdlr": "This paper presents an algorithm that overcomes the problem of local minima in training regressive MLP neural networks."
}