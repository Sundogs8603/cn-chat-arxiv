{
    "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])",
    "abstract": "Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures",
    "link": "http://arxiv.org/abs/2308.12833",
    "context": "Title: Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])\nAbstract: Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures",
    "path": "papers/23/08/2308.12833.json",
    "total_tokens": 883,
    "translated_title": "使用LLMs进行非法目的：威胁、预防措施和漏洞",
    "translated_abstract": "近年来，大型语言模型（LLMs）在工业和学术界的快速发展和分发引起了人们的关注，许多最近的研究都关注了LLMs的安全和安全相关威胁和漏洞，包括在潜在的犯罪活动中。具体来说，已经表明LLMs可以被滥用用于欺诈、冒充和生成恶意软件；而其他作者则考虑了AI对齐的更一般的问题。开发人员和从业人员都需要意识到这些模型的安全问题的存在。在本文中，我们概述了现有的主要是科学努力，以确定和减轻由LLMs引起的威胁和漏洞。我们提供了一个描述由LLMs的生成能力引起的威胁、预防措施以及由于预防措施不完善而产生的漏洞的分类。",
    "tldr": "本研究回顾了有关大型语言模型（LLMs）的威胁和漏洞的现有科学努力，并提供了一种描述这些威胁、预防措施和由于预防措施不完善而产生的漏洞之间关系的分类。",
    "en_tdlr": "This study reviews existing scientific efforts on the threats and vulnerabilities of large language models (LLMs) and provides a taxonomy describing the relationship between these threats, prevention measures, and vulnerabilities resulting from imperfect prevention measures."
}