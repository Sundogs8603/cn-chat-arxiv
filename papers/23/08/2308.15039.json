{
    "title": "R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics. (arXiv:2308.15039v1 [cs.RO])",
    "abstract": "Autonomous robotic systems, like autonomous vehicles and robotic search and rescue, require efficient on-device training for continuous adaptation of Deep Reinforcement Learning (DRL) models in dynamic environments. This research is fundamentally motivated by the need to understand and address the challenges of on-device real-time DRL, which involves balancing timing and algorithm performance under memory constraints, as exposed through our extensive empirical studies. This intricate balance requires co-optimizing two pivotal parameters of DRL training -- batch size and replay buffer size. Configuring these parameters significantly affects timing and algorithm performance, while both (unfortunately) require substantial memory allocation to achieve near-optimal performance.  This paper presents R^3, a holistic solution for managing timing, memory, and algorithm performance in on-device real-time DRL training. R^3 employs (i) a deadline-driven feedback loop with dynamic batch sizing for ",
    "link": "http://arxiv.org/abs/2308.15039",
    "context": "Title: R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics. (arXiv:2308.15039v1 [cs.RO])\nAbstract: Autonomous robotic systems, like autonomous vehicles and robotic search and rescue, require efficient on-device training for continuous adaptation of Deep Reinforcement Learning (DRL) models in dynamic environments. This research is fundamentally motivated by the need to understand and address the challenges of on-device real-time DRL, which involves balancing timing and algorithm performance under memory constraints, as exposed through our extensive empirical studies. This intricate balance requires co-optimizing two pivotal parameters of DRL training -- batch size and replay buffer size. Configuring these parameters significantly affects timing and algorithm performance, while both (unfortunately) require substantial memory allocation to achieve near-optimal performance.  This paper presents R^3, a holistic solution for managing timing, memory, and algorithm performance in on-device real-time DRL training. R^3 employs (i) a deadline-driven feedback loop with dynamic batch sizing for ",
    "path": "papers/23/08/2308.15039.json",
    "total_tokens": 901,
    "translated_title": "R^3: 基于设备的实时深度强化学习在自主机器人中的应用",
    "translated_abstract": "自主机器人系统，如自动驾驶车辆和机器人搜救系统，需要在动态环境中连续适应深度强化学习(DRL)模型的高效设备上训练。本研究的基本动机是理解和应对基于设备的实时DRL的挑战，这涉及在内存约束下平衡时间和算法性能的能力，通过我们广泛的实证研究揭示。这种微妙的平衡需要共同优化DRL训练的两个关键参数--批量大小和回放缓冲区大小。配置这些参数对时间和算法性能有重要影响，然而两者都需要相当大的内存分配才能达到接近最优的性能。本文提出了R^3，一种在设备上管理时间、内存和算法性能的整体解决方案。R^3采用（i）一个以截止日期为驱动的反馈循环，带有动态批量大小，",
    "tldr": "R^3是一种在自主机器人中应用的基于设备的实时深度强化学习训练方法，它通过动态批量大小和回放缓冲区大小的优化，实现了在时间和算法性能之间的平衡，并有效地管理了内存和算法性能。",
    "en_tdlr": "R^3 is an on-device real-time deep reinforcement learning training method applied in autonomous robotics, which achieves a balance between timing and algorithm performance through optimizing dynamic batch size and replay buffer size, effectively managing memory and algorithm performance."
}