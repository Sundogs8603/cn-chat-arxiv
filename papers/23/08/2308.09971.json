{
    "title": "Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])",
    "abstract": "Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL",
    "link": "http://arxiv.org/abs/2308.09971",
    "context": "Title: Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])\nAbstract: Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL",
    "path": "papers/23/08/2308.09971.json",
    "total_tokens": 887,
    "translated_title": "可拆卸式迁移学习用于选择性源任务遗忘",
    "translated_abstract": "迁移学习被广泛用于训练深度神经网络(DNN)来构建强大的表示。即使在预训练模型适应目标任务后，特征提取器的表示性能仍然保留在一定程度上。由于预训练模型的性能可以被视为所有者的私有财产，自然而然地寻求预训练权重的广义性能的独占权是合理的。为了解决这个问题，我们提出了一种称为可拆卸式迁移学习(DTL)的新范式，它只处理源任务而不降低目标任务的性能。为了实现知识的遗忘，我们提出了一种新的损失函数，称为梯度碰撞损失(GC损失)。GC损失通过引导不同mini-batches的梯度向量朝不同方向前进，选择性地遗忘源知识。模型是否成功遗忘源任务通过“背驮式学习准确性”(PL准确性)来衡量。",
    "tldr": "本论文提出了一种称为可拆卸式迁移学习的新范式，通过引入梯度碰撞损失，该方法可以选择性地遗忘源任务而不降低目标任务的性能。",
    "en_tdlr": "This paper proposes a new paradigm called disposable transfer learning (DTL), which selectively forgets the source task without degrading the performance of the target task by introducing Gradient Collision loss."
}