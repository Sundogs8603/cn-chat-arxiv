{
    "title": "Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup. (arXiv:2308.00522v1 [cs.LG])",
    "abstract": "Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \\textit{FedLADA} with a linear spee",
    "link": "http://arxiv.org/abs/2308.00522",
    "context": "Title: Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup. (arXiv:2308.00522v1 [cs.LG])\nAbstract: Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \\textit{FedLADA} with a linear spee",
    "path": "papers/23/08/2308.00522.json",
    "total_tokens": 938,
    "translated_title": "高效的联邦学习：基于局部自适应修正优化器的线性加速",
    "translated_abstract": "自适应优化方法在分布式学习中取得了显著的成功，但将自适应优化器扩展到联邦学习中存在严重的效率问题，包括：（i）全局自适应优化器中由于梯度估计不准确导致的粗糙收敛；（ii）由于局部自适应优化器的局部过拟合导致的客户端漂移加剧。在本研究中，我们提出了一种新的基于动量的算法，利用全局梯度下降和局部自适应修正优化器来解决这些困难。具体而言，我们引入了一种局部修正技术到自适应优化器中，命名为联邦局部自适应修正优化器（FedLADA），它通过动量项估计上一轮通信中的全局平均偏差，并通过校正局部偏差来进一步提高经验训练速度并减轻异构过拟合问题。从理论上讲，我们建立了FedLADA的收敛速度，具有线性加速度",
    "tldr": "本论文提出了一种名为FedLADA的联邦局部自适应修正优化器，通过引入局部修正技术和动量项，解决了联邦学习中粗糙收敛和客户端漂移加剧的问题，具有线性加速度。",
    "en_tdlr": "This paper proposes a novel federated local adaptive amended optimizer (FedLADA) that tackles the issues of rugged convergence and client drifts in federated learning by incorporating local amended technique and a momentum-like term, achieving linear speedup."
}