{
    "title": "Decorrelating neurons using persistence. (arXiv:2308.04870v1 [cs.LG])",
    "abstract": "We propose a novel way to improve the generalisation capacity of deep learning models by reducing high correlations between neurons. For this, we present two regularisation terms computed from the weights of a minimum spanning tree of the clique whose vertices are the neurons of a given network (or a sample of those), where weights on edges are correlation dissimilarities. We provide an extensive set of experiments to validate the effectiveness of our terms, showing that they outperform popular ones. Also, we demonstrate that naive minimisation of all correlations between neurons obtains lower accuracies than our regularisation terms, suggesting that redundancies play a significant role in artificial neural networks, as evidenced by some studies in neuroscience for real networks. We include a proof of differentiability of our regularisers, thus developing the first effective topological persistence-based regularisation terms that consider the whole set of neurons and that can be applie",
    "link": "http://arxiv.org/abs/2308.04870",
    "context": "Title: Decorrelating neurons using persistence. (arXiv:2308.04870v1 [cs.LG])\nAbstract: We propose a novel way to improve the generalisation capacity of deep learning models by reducing high correlations between neurons. For this, we present two regularisation terms computed from the weights of a minimum spanning tree of the clique whose vertices are the neurons of a given network (or a sample of those), where weights on edges are correlation dissimilarities. We provide an extensive set of experiments to validate the effectiveness of our terms, showing that they outperform popular ones. Also, we demonstrate that naive minimisation of all correlations between neurons obtains lower accuracies than our regularisation terms, suggesting that redundancies play a significant role in artificial neural networks, as evidenced by some studies in neuroscience for real networks. We include a proof of differentiability of our regularisers, thus developing the first effective topological persistence-based regularisation terms that consider the whole set of neurons and that can be applie",
    "path": "papers/23/08/2308.04870.json",
    "total_tokens": 934,
    "translated_title": "使用持久性方法去相关神经元",
    "translated_abstract": "我们提出了一种改善深度学习模型泛化能力的新方法，通过减少神经元之间的高相关性。为此，我们从一个给定网络的神经元（或其中一部分样本）构成的团中，计算最小生成树的权重来计算两个正则化项，而边上的权重是相关性的差异。我们进行了大量的实验证明了我们的正则化项的有效性，并表明它们优于常见的正则化项。此外，我们还证明了仅仅最小化神经元之间的所有相关性得到的准确率比我们的正则化项要低，这表明冗余在人工神经网络中起到了重要作用，这一点在神经科学的一些研究中也有所证明。我们还证明了我们正则化项的可微性，从而开发了第一个考虑整个神经元集的基于拓扑持久性的有效的正则化方法，并且可以应用于实际网络。",
    "tldr": "本论文提出了一种使用持久性方法去除神经元之间高相关性的新方法，通过计算最小生成树的权重来构建正则化项，并通过大量实验证明了这些正则化项的有效性。结果表明，与常见的正则化项相比，这些正则化项能更好地提高深度学习模型的泛化能力，还发现冗余在人工神经网络中发挥着重要的作用。",
    "en_tdlr": "This paper proposes a novel method to reduce high correlations between neurons in deep learning models using persistence. The authors compute regularization terms based on the weights of a minimum spanning tree and demonstrate their effectiveness through extensive experiments. The results show that these regularization terms outperform popular ones and highlight the importance of redundancy in artificial neural networks."
}