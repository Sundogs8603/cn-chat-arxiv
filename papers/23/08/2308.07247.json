{
    "title": "Can we Agree? On the Rash\\=omon Effect and the Reliability of Post-Hoc Explainable AI. (arXiv:2308.07247v1 [cs.LG])",
    "abstract": "The Rash\\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous ",
    "link": "http://arxiv.org/abs/2308.07247",
    "context": "Title: Can we Agree? On the Rash\\=omon Effect and the Reliability of Post-Hoc Explainable AI. (arXiv:2308.07247v1 [cs.LG])\nAbstract: The Rash\\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous ",
    "path": "papers/23/08/2308.07247.json",
    "total_tokens": 979,
    "translated_title": "能否达成一致？罗织效应和事后可解释人工智能的可靠性",
    "translated_abstract": "罗织效应给从机器学习模型中获得可靠知识带来了挑战。本研究使用SHAP在一个罗织集中考察了样本大小对模型解释的影响。在5个公共数据集上的实验证明，随着样本大小的增加，解释逐渐趋于一致。少于128个样本的解释变异性高，限制了可靠知识的提取。然而，随着更多数据的增加，模型之间的一致性也得到了改善，允许形成共识。集成模型常常具有更高的一致性。这些结果为信任解释所需的充足数据提供了指导。样本数量较少时的变异性表明，没有经过验证的结论可能是不可靠的。还需要进一步研究更多的模型类型、数据领域和解释方法。在神经网络和特定模型解释方法中测试收敛性将是有影响力的。这里探索的方法为从模糊的情况中获取知识提供了有原则的技术。",
    "tldr": "本研究通过在罗织集中使用SHAP将样本大小与模型的解释进行了比较，发现随着样本大小的增加，解释逐渐趋于一致。然而，少于128个样本的解释变异性高，限制了可靠知识的提取。我们的研究结果为信任解释所需的充足数据提供了指导。",
    "en_tdlr": "This study compares the explanations from machine learning models in a Rash\\=omon set based on sample size, finding that explanations gradually converge as the sample size increases. However, explanations from fewer than 128 samples exhibit high variability, limiting reliable knowledge extraction. The results provide guidance on sufficient data to trust explanations."
}