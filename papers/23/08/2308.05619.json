{
    "title": "Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])",
    "abstract": "As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\\%$ confidence interval: ",
    "link": "http://arxiv.org/abs/2308.05619",
    "context": "Title: Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])\nAbstract: As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\\%$ confidence interval: ",
    "path": "papers/23/08/2308.05619.json",
    "total_tokens": 990,
    "translated_title": "使用基于排名的兼容性更新临床风险分层模型：评估和优化临床医生-模型团队性能的方法",
    "translated_abstract": "随着数据的变化或新数据的出现，更新临床机器学习模型可能是必要的，以保持或提高其性能。然而，更新模型可能会引入兼容性问题，当更新后的模型的行为与用户的期望不一致时，会导致用户-模型团队表现不佳。现有的兼容性度量依赖于模型的决策阈值，限制了它们在基于估计风险的排名生成模型的应用能力。为了解决这个限制，我们提出了一种新颖的基于排名的兼容性度量，$C^R$，以及一个旨在优化判别性能的新损失函数，同时鼓励良好的兼容性。在利用MIMIC数据的病死率风险分层的案例研究中，我们的方法相对于现有的模型选择技术，产生了更兼容的模型，同时保持了判别性能，$C^R$提高了0.019（$95\\%$置信区间：...",
    "tldr": "提出了一种基于排名的兼容性度量和一种新的损失函数来更新临床机器学习模型，以解决更新模型引入的兼容性问题。在使用MIMIC数据的病死率风险分层案例研究中，该方法相对于现有技术能产生更兼容的模型并保持判别性能。",
    "en_tdlr": "A novel rank-based compatibility measure and a new loss function are proposed to update clinical machine learning models and address compatibility issues introduced by model updates. Applied to a case study of mortality risk stratification using MIMIC data, this approach can generate more compatible models while maintaining discriminative performance compared to existing techniques."
}