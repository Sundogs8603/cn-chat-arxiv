{
    "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. (arXiv:2308.09975v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial kno",
    "link": "http://arxiv.org/abs/2308.09975",
    "context": "Title: FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. (arXiv:2308.09975v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial kno",
    "path": "papers/23/08/2308.09975.json",
    "total_tokens": 994,
    "translated_title": "FinEval：一个用于大型语言模型的中文金融领域知识评估基准",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言处理任务中展示出了出色的性能，但是它们在更具挑战性和专业领域的任务中的效果尚未得到深入研究。本文提出了FinEval，这是一个专门为LLMs中的金融领域知识设计的评估基准。FinEval是一个包含了金融、经济、会计和证书等34个学术科目的高质量多项选择题的集合，总计包含了4,661道题目。为了确保对模型性能进行全面评估，FinEval使用了多种提示类型，包括零样本和少样本提示，以及仅答案提示和思路链式提示。通过在FinEval上评估最先进的中文和英文LLMs，结果显示只有GPT-4在不同的提示设置下实现了接近70%的准确率，表明LLMs在金融领域知识中具有显著的增长潜力。我们的工作为金融领域的知识评估提供了更全面的基准。",
    "tldr": "本论文提出了一个专门用于评估大型语言模型在金融领域知识上的基准FinEval。通过在FinEval上评估中英文LLMs，结果显示只有GPT-4在不同提示设置下实现了接近70%的准确率，展示了LLMs在金融领域知识中的显著增长潜力。",
    "en_tdlr": "This paper presents FinEval, an evaluation benchmark specifically designed for assessing large language models in the financial domain knowledge. Evaluating Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge."
}