{
    "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. (arXiv:2308.11336v1 [cs.IR])",
    "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on",
    "link": "http://arxiv.org/abs/2308.11336",
    "context": "Title: On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. (arXiv:2308.11336v1 [cs.IR])\nAbstract: Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on",
    "path": "papers/23/08/2308.11336.json",
    "total_tokens": 901,
    "translated_title": "关于线下强化学习在推荐系统中的机遇和挑战",
    "translated_abstract": "强化学习作为一种建模动态用户兴趣的强大工具，近年来在推荐系统中得到了越来越多的研究关注。然而，它存在一个重要缺点：因为其交互性，其数据效率较低。强化学习推荐系统的训练需要昂贵的在线交互来积累足够的轨迹，这对于代理能够学习用户偏好至关重要。这种低效性使得基于强化学习的推荐系统成为一项艰巨的任务，需要探索潜在的解决方案。近期离线强化学习的进展提供了一种新的视角。离线强化学习使得代理能够从离线数据集中获取见解，并在在线环境中部署学习到的策略。鉴于推荐系统拥有广泛的线下数据集，离线强化学习框架与之紧密相符。尽管离线强化学习是一个新兴领域，但在离线强化学习方面的研究成果逐渐增多。",
    "tldr": "研究聚焦于解决强化学习推荐系统的数据效率问题，离线强化学习为从线下数据集中学习并在在线环境应用策略提供了新的解决方案。",
    "en_tdlr": "This research focuses on addressing the data efficiency problem of reinforcement learning-based recommender systems. Offline reinforcement learning offers a new solution by leveraging offline datasets to learn and apply policies in online settings."
}