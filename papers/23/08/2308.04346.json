{
    "title": "Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])",
    "abstract": "We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers",
    "link": "http://arxiv.org/abs/2308.04346",
    "context": "Title: Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])\nAbstract: We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers",
    "path": "papers/23/08/2308.04346.json",
    "total_tokens": 859,
    "translated_title": "揭示国家偏见：对AI生成文章中国籍知觉的研究",
    "translated_abstract": "我们使用人类评估方法，研究自然语言处理(NLP)模型中潜在的国籍偏见。有偏见的NLP模型可能会传播刻板印象，导致算法性别歧视，对AI系统的公平性和正义性构成重大挑战。我们的研究采用了两步混合方法，既包括定量分析，又包括定性分析，以识别和理解文本生成模型中的国籍偏见的影响。通过我们以人为中心的定量分析，我们评估了AI生成的文章中国籍偏见的程度。然后，我们对参与者进行了开放式访谈，进行了定性编码和主题分析，以了解这些偏见对人类读者的影响。我们的研究发现，有偏见的NLP模型倾向于复制和放大现有的社会偏见，在社会技术环境中使用会造成伤害。",
    "tldr": "通过人类评估方法，我们研究了自然语言处理模型中的国籍偏见。研究发现，有偏见的模型会复制和放大现有的社会偏见，从而可能造成伤害。",
    "en_tdlr": "We investigate the potential for nationality biases in natural language processing models and find that biased models tend to replicate and amplify existing societal biases, posing harm if used in sociotechnical settings."
}