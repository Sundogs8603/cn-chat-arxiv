{
    "title": "FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)",
    "abstract": "The Fon language, spoken by an average 2 million of people, is a truly low-resourced African language, with a limited online presence, and existing datasets (just to name but a few). Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks: this could be prevalent in very data-scarce scenarios. In this paper, we present the first explorative approach to multitask learning, for model capabilities enhancement in Natural Language Processing for the Fon language. Specifically, we explore the tasks of Named Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage two language model heads as encoders to build shared representations for the inputs, and we use linear layers blocks for classification relative to each task. Our results on the NER and POS tasks for Fon, show competitive (or better) performances compared to several multilingual pretrained language models finet",
    "link": "http://arxiv.org/abs/2308.14280",
    "context": "Title: FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)\nAbstract: The Fon language, spoken by an average 2 million of people, is a truly low-resourced African language, with a limited online presence, and existing datasets (just to name but a few). Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks: this could be prevalent in very data-scarce scenarios. In this paper, we present the first explorative approach to multitask learning, for model capabilities enhancement in Natural Language Processing for the Fon language. Specifically, we explore the tasks of Named Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage two language model heads as encoders to build shared representations for the inputs, and we use linear layers blocks for classification relative to each task. Our results on the NER and POS tasks for Fon, show competitive (or better) performances compared to several multilingual pretrained language models finet",
    "path": "papers/23/08/2308.14280.json",
    "total_tokens": 850,
    "translated_title": "FonMTL:面向Fon语的多任务学习",
    "translated_abstract": "Fon语是一种真正的低资源非洲语言，大约有200万人口，其在线存在有限，并且现有数据集也很有限。多任务学习是一种学习范式，旨在通过在不同但相关的任务间共享知识来提高模型的泛化能力，这在数据稀缺的场景中尤为重要。本文提出了第一种探索性的多任务学习方法，用于增强Fon语自然语言处理中的模型能力。具体来说，我们探索了Fon语的命名实体识别（NER）和词性标注（POS）任务。我们利用两个语言模型作为编码器来构建输入的共享表示，并使用线性层块进行每个任务的分类。我们在Fon语的NER和POS任务上的结果与几个多语言预训练语言模型微调的性能相比，表现出竞争力（或更好）。",
    "tldr": "本文面向Fon语的多任务学习，旨在通过在命名实体识别和词性标注任务上共享知识，增强模型在Fon语自然语言处理中的性能。"
}