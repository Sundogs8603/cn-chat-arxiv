{
    "title": "Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])",
    "abstract": "Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar",
    "link": "http://arxiv.org/abs/2308.15461",
    "context": "Title: Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])\nAbstract: Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar",
    "path": "papers/23/08/2308.15461.json",
    "total_tokens": 938,
    "translated_title": "混合神经网络领域的规范因素",
    "translated_abstract": "因子特征容量提供了一种简单的方法来构建更紧凑、高效和可解释的神经网络领域，但也引入了不一定有益于真实世界数据的偏差。在这项工作中，我们（1）对这些体系结构对齐轴信号的不良偏差进行了表征 - 它们可以导致辐射场重建的差异高达2 PSNR - 并（2）探索了通过学习一组规范化变换来提高表示的方法，从而消除这些偏差。在一个二维模型问题中，我们证明同时学习这些变换以及场景外观可以以大大提高的效率成功。我们使用图像、有符号距离和辐射场重建任务验证了所得到的体系结构，我们观察到质量、鲁棒性、紧凑性和运行时间方面的改进。结果表明，TILTED可以实现与基线相当的能力，而基线是2倍大。",
    "tldr": "该研究对混合神经网络领域的规范因素进行了研究，发现因子特征容量虽然简单高效，但存在不良偏差。通过学习一组规范化变换，该方法成功消除偏差，并在图像、距离和辐射场重建任务中实现了质量、鲁棒性和运行时间的改进。",
    "en_tdlr": "This paper investigates canonical factors for hybrid neural fields, finding that factored feature volumes, although simple and efficient, introduce undesirable biases. By learning a set of canonicalizing transformations, these biases are successfully eliminated, resulting in improved quality, robustness, and runtime in image, distance, and radiance field reconstruction tasks."
}