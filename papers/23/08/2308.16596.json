{
    "title": "The Quest of Finding the Antidote to Sparse Double Descent. (arXiv:2308.16596v1 [cs.AI])",
    "abstract": "In energy-efficient schemes, finding the optimal size of deep learning models is very important and has a broad impact. Meanwhile, recent studies have reported an unexpected phenomenon, the sparse double descent: as the model's sparsity increases, the performance first worsens, then improves, and finally deteriorates. Such a non-monotonic behavior raises serious questions about the optimal model's size to maintain high performance: the model needs to be sufficiently over-parametrized, but having too many parameters wastes training resources.  In this paper, we aim to find the best trade-off efficiently. More precisely, we tackle the occurrence of the sparse double descent and present some solutions to avoid it. Firstly, we show that a simple $\\ell_2$ regularization method can help to mitigate this phenomenon but sacrifices the performance/sparsity compromise. To overcome this problem, we then introduce a learning scheme in which distilling knowledge regularizes the student model. Suppo",
    "link": "http://arxiv.org/abs/2308.16596",
    "context": "Title: The Quest of Finding the Antidote to Sparse Double Descent. (arXiv:2308.16596v1 [cs.AI])\nAbstract: In energy-efficient schemes, finding the optimal size of deep learning models is very important and has a broad impact. Meanwhile, recent studies have reported an unexpected phenomenon, the sparse double descent: as the model's sparsity increases, the performance first worsens, then improves, and finally deteriorates. Such a non-monotonic behavior raises serious questions about the optimal model's size to maintain high performance: the model needs to be sufficiently over-parametrized, but having too many parameters wastes training resources.  In this paper, we aim to find the best trade-off efficiently. More precisely, we tackle the occurrence of the sparse double descent and present some solutions to avoid it. Firstly, we show that a simple $\\ell_2$ regularization method can help to mitigate this phenomenon but sacrifices the performance/sparsity compromise. To overcome this problem, we then introduce a learning scheme in which distilling knowledge regularizes the student model. Suppo",
    "path": "papers/23/08/2308.16596.json",
    "total_tokens": 936,
    "translated_title": "寻找稀疏双下降的解毒剂的探索",
    "translated_abstract": "在能源高效的方案中，找到深度学习模型的最优大小非常重要并具有广泛影响。与此同时，最近的研究报告了一种意外现象，稀疏双下降：随着模型的稀疏性增加，性能首先变差，然后改善，最后恶化。这种非单调行为对于保持高性能的最优模型大小提出了严重的问题：模型需要具有足够的超参数，但太多的参数会浪费训练资源。在本文中，我们旨在高效地找到最佳的权衡点。更具体地说，我们解决了稀疏双下降的出现，并提出一些解决方案来避免它。首先，我们展示了一个简单的l2正则化方法可以帮助缓解这种现象，但会牺牲性能/稀疏性的折衷。为了克服这个问题，我们引入了一种学习方案，其中蒸馏知识对学生模型进行正则化。",
    "tldr": "本文致力于寻找稀疏双下降的解毒剂，通过研究和提出解决方案，分别采用l2正则化和知识蒸馏来避免稀疏双下降现象，以找到性能和稀疏性的最佳平衡点。",
    "en_tdlr": "This paper aims to find an antidote to sparse double descent by investigating and proposing solutions. It suggests using l2 regularization and knowledge distillation to avoid the phenomenon and find the optimal balance between performance and sparsity."
}