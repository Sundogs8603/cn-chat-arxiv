{
    "title": "Spherical Vision Transformer for 360-degree Video Saliency Prediction. (arXiv:2308.13004v1 [cs.CV])",
    "abstract": "The growing interest in omnidirectional videos (ODVs) that capture the full field-of-view (FOV) has gained 360-degree saliency prediction importance in computer vision. However, predicting where humans look in 360-degree scenes presents unique challenges, including spherical distortion, high resolution, and limited labelled data. We propose a novel vision-transformer-based model for omnidirectional videos named SalViT360 that leverages tangent image representations. We introduce a spherical geometry-aware spatiotemporal self-attention mechanism that is capable of effective omnidirectional video understanding. Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360-degree dense-prediction models to reduce artefacts in the predictions that occur after inverse projection. Our approach is the first to employ tangent images for omnidirectional saliency prediction, and our experimental results on three ODV saliency datasets demonstrate its effect",
    "link": "http://arxiv.org/abs/2308.13004",
    "context": "Title: Spherical Vision Transformer for 360-degree Video Saliency Prediction. (arXiv:2308.13004v1 [cs.CV])\nAbstract: The growing interest in omnidirectional videos (ODVs) that capture the full field-of-view (FOV) has gained 360-degree saliency prediction importance in computer vision. However, predicting where humans look in 360-degree scenes presents unique challenges, including spherical distortion, high resolution, and limited labelled data. We propose a novel vision-transformer-based model for omnidirectional videos named SalViT360 that leverages tangent image representations. We introduce a spherical geometry-aware spatiotemporal self-attention mechanism that is capable of effective omnidirectional video understanding. Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360-degree dense-prediction models to reduce artefacts in the predictions that occur after inverse projection. Our approach is the first to employ tangent images for omnidirectional saliency prediction, and our experimental results on three ODV saliency datasets demonstrate its effect",
    "path": "papers/23/08/2308.13004.json",
    "total_tokens": 932,
    "translated_title": "用于360度视频显著性预测的球形视觉Transformer",
    "translated_abstract": "对全景视频（ODVs）的兴趣日益增长，该视频捕捉了整个视野（FOV），因此在计算机视觉中，360度显著性预测变得重要起来。然而，预测人类在360度场景中看向何处面临独特的挑战，包括球形失真、高分辨率和有限的标记数据。我们提出了一种名为SalViT360的基于视觉Transformer的全景视频模型，该模型利用切线图像表示。我们引入了一种球形几何感知的时空自注意机制，能够有效理解全景视频。此外，我们还提出了一种基于一致性的无监督正则化项，用于投影-based的360度密集预测模型，以减少投影反演后预测中出现的伪像。我们的方法是第一个采用切线图像进行全景显著性预测的方法，我们在三个全景视频显著性数据集上的实验结果证明了其效果。",
    "tldr": "本论文提出了一种用于360度视频显著性预测的球形视觉Transformer模型，通过引入切线图像表示和球形几何感知的机制，以及一种无监督正则化项来减少伪像，实现了有效的全景视频理解和显著性预测。"
}