{
    "title": "SOTASTREAM: A Streaming Approach to Machine Translation Training. (arXiv:2308.07489v1 [cs.CL])",
    "abstract": "Many machine translation toolkits make use of a data preparation step wherein raw data is transformed into a tensor format that can be used directly by the trainer. This preparation step is increasingly at odds with modern research and development practices because this process produces a static, unchangeable version of the training data, making common training-time needs difficult (e.g., subword sampling), time-consuming (preprocessing with large data can take days), expensive (e.g., disk space), and cumbersome (managing experiment combinatorics). We propose an alternative approach that separates the generation of data from the consumption of that data. In this approach, there is no separate pre-processing step; data generation produces an infinite stream of permutations of the raw training data, which the trainer tensorizes and batches as it is consumed. Additionally, this data stream can be manipulated by a set of user-definable operators that provide on-the-fly modifications, such ",
    "link": "http://arxiv.org/abs/2308.07489",
    "context": "Title: SOTASTREAM: A Streaming Approach to Machine Translation Training. (arXiv:2308.07489v1 [cs.CL])\nAbstract: Many machine translation toolkits make use of a data preparation step wherein raw data is transformed into a tensor format that can be used directly by the trainer. This preparation step is increasingly at odds with modern research and development practices because this process produces a static, unchangeable version of the training data, making common training-time needs difficult (e.g., subword sampling), time-consuming (preprocessing with large data can take days), expensive (e.g., disk space), and cumbersome (managing experiment combinatorics). We propose an alternative approach that separates the generation of data from the consumption of that data. In this approach, there is no separate pre-processing step; data generation produces an infinite stream of permutations of the raw training data, which the trainer tensorizes and batches as it is consumed. Additionally, this data stream can be manipulated by a set of user-definable operators that provide on-the-fly modifications, such ",
    "path": "papers/23/08/2308.07489.json",
    "total_tokens": 885,
    "translated_title": "SOTASTREAM：一种用于机器翻译训练的流式方法",
    "translated_abstract": "许多机器翻译工具包使用数据准备步骤将原始数据转换为可以直接被训练器使用的张量格式。然而，这个准备步骤与现代的研究和开发实践越来越不相符，因为这个过程产生了一个静态的、不可更改的训练数据版本，使得常见的训练时需求变得困难（例如，子词采样）、耗时（处理大量数据可能需要几天时间）、昂贵（例如，磁盘空间）和繁琐（管理实验组合）。我们提出了一种替代方法，将数据的生成与数据的使用分离。在这种方法中，没有单独的预处理步骤；数据的生成产生了原始训练数据的无限排列流，训练器在消费数据时将其转换为张量并进行批处理。此外，这个数据流可以通过一组可定义的操作符进行实时修改，例如子词采样等。",
    "tldr": "SOTASTREAM是一种机器翻译训练的流式方法，提供了无限排列的训练数据流，避免了传统的静态数据预处理步骤，以及耗时、昂贵和繁琐的问题。",
    "en_tdlr": "SOTASTREAM is a streaming approach to machine translation training that provides an infinite stream of permutations of training data, eliminating the need for traditional static data preprocessing steps and addressing issues of time-consuming, expensive, and cumbersome processes."
}