{
    "title": "SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets. (arXiv:2308.11880v1 [cs.CV])",
    "abstract": "Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. O",
    "link": "http://arxiv.org/abs/2308.11880",
    "context": "Title: SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets. (arXiv:2308.11880v1 [cs.CV])\nAbstract: Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. O",
    "path": "papers/23/08/2308.11880.json",
    "total_tokens": 865,
    "translated_title": "SUMMIT: 无源自适应单模型到多模态目标的方法",
    "translated_abstract": "在许多应用中，使用多模态数据进行场景理解是必要的，例如自主导航。为了在各种情况下实现这一目标，现有的模型必须能够适应不断变化的数据分布，而无需繁琐的数据标注。当前的方法假设在自适应过程中存在源数据，并且源数据是成对的多模态数据。然而，这些假设对许多应用来说可能存在问题。源数据可能由于隐私、安全或经济方面的考虑而不可用。在训练过程中假设存在成对的多模态数据还会导致显著的数据收集成本，并且无法充分利用广泛可用的免费分发的单模态模型。在本文中，我们通过解决以下问题来放宽这两个假设：如何将独立训练在单模态数据上的一组模型自适应到由无标签的多模态数据组成的目标域，而无需访问原始源数据集。",
    "tldr": "SUMMIT方法放宽了传统模型自适应方法的两个假设，通过解决无源数据、无配对数据的情况将独立训练的单模态模型适应到多模态目标领域。",
    "en_tdlr": "SUMMIT method relaxes the assumptions made by traditional model adaptation methods, enabling the adaptation of independently trained uni-modal models to multi-modal target domains without source data or paired data."
}