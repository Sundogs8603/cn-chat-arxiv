{
    "title": "Eva: A General Vectorized Approximation Framework for Second-order Optimization. (arXiv:2308.02123v1 [cs.LG])",
    "abstract": "Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datase",
    "link": "http://arxiv.org/abs/2308.02123",
    "context": "Title: Eva: A General Vectorized Approximation Framework for Second-order Optimization. (arXiv:2308.02123v1 [cs.LG])\nAbstract: Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datase",
    "path": "papers/23/08/2308.02123.json",
    "total_tokens": 911,
    "translated_title": "Eva：用于二阶优化的通用矢量化近似框架",
    "translated_abstract": "二阶优化算法对于训练深度学习模型具有优秀的收敛性能，但往往会导致显著的计算和内存开销。这可能导致比随机梯度下降（SGD）等一阶算法更低的训练效率。在这项工作中，我们提出了一种名为Eva的内存和时间高效的二阶算法，采用了两种新颖的技术：1）我们使用小批量训练数据上的Kronecker分解构建二阶信息，以减少内存消耗；2）我们利用Sherman-Morrison公式导出一种高效的更新公式，而无需显式计算矩阵的逆。我们进一步将Eva扩展为一个通用的矢量化近似框架，以提高两种现有二阶算法（FOOF和Shampoo）的计算和内存效率，而不影响它们的收敛性能。在不同模型和数据集上进行了大量的实验结果验证了我们方法的有效性。",
    "tldr": "Eva是一个内存和时间高效的二阶优化算法，使用Kronecker分解和Sherman-Morrison公式来减少内存消耗和计算矩阵逆，同时将其扩展为通用的矢量化近似框架以提高计算和内存效率。",
    "en_tdlr": "Eva is a memory- and time-efficient second-order optimization algorithm that reduces memory consumption and avoids explicit computation of matrix inverse using Kronecker factorization and the Sherman-Morrison formula. It is also extended to a general vectorized approximation framework to improve compute and memory efficiency."
}