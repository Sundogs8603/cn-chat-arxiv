{
    "title": "Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])",
    "abstract": "Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model's residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning.",
    "link": "http://arxiv.org/abs/2308.01544",
    "context": "Title: Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])\nAbstract: Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model's residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning.",
    "path": "papers/23/08/2308.01544.json",
    "total_tokens": 852,
    "translated_title": "预训练的仅文本变压器中的多模态神经元",
    "translated_abstract": "语言模型展现了在不同模态下将学习到的表示推广到其他模态下游任务的显著能力。我们研究了一个冻结的文本变压器增加视觉能力的情况，使用了一个自监督视觉编码器和一个在图像到文本任务上学习得到的单一线性映射。映射层的输出不是可以直接解码成描述图像内容的语言，相反，我们发现模态之间的转换发生在变压器的更深处。我们引入了一种识别将视觉表示转换为相应文本的“多模态神经元”的过程，并解码它们注入模型残差流中的概念。通过一系列实验，我们展示了多模态神经元在不同输入中对特定视觉概念进行操作，并对图像字幕生成具有系统性的因果效应。",
    "tldr": "研究了在预训练的文本变压器中如何通过引入视觉信息来实现多模态能力，在变压器的更深处进行模态之间的转换，并介绍了一种识别多模态神经元的方法，展示了它们对特定视觉概念的操作以及对图像字幕生成的因果效应。",
    "en_tdlr": "Explored the integration of visual information into pretrained text transformers, identifying and studying multimodal neurons that perform translation between modalities and have a causal effect on image captioning."
}