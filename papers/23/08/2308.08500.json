{
    "title": "InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])",
    "abstract": "Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization ",
    "link": "http://arxiv.org/abs/2308.08500",
    "context": "Title: InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])\nAbstract: Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization ",
    "path": "papers/23/08/2308.08500.json",
    "total_tokens": 957,
    "translated_title": "InTune:基于强化学习的数据流优化用于深度推荐模型",
    "translated_abstract": "基于深度学习的推荐模型(DLRM)已经成为许多现代推荐系统的重要组成部分。一些公司正在建设大型计算集群专门用于DLRM训练，进而推动了对成本和时间的节约优化的新兴兴趣。在这个场景中所面临的系统挑战是独特的；尽管典型的深度学习训练任务由模型执行主导，但DLRM训练性能中最重要的因素往往是线上数据摄入。在本文中，我们探讨了该数据摄入问题的独特特征，并深入研究了DLRM训练流程中的性能瓶颈和挑战。我们对Netflix计算集群中真实的DLRM数据处理流程进行了研究，观察了线上摄入的性能影响，并识别出现有流程优化器的不足之处。我们发现当前的工具要么产生次优性能，要么经常崩溃，要么需要不现实的集群重组。",
    "tldr": "本文提出了InTune，一个基于强化学习的数据流优化方法，应用于深度推荐模型。通过研究在Netflix计算集群中的DLRM数据处理流程，我们发现目前的流程优化器存在性能不佳、频繁崩溃或需要不切实际的集群重组等问题。",
    "en_tdlr": "This paper presents InTune, a reinforcement learning-based approach for data pipeline optimization in deep recommendation models. By studying the DLRM data processing pipelines in Netflix's compute cluster, we identify performance shortcomings, frequent crashes, and impractical cluster re-organizations in current pipeline optimizers."
}