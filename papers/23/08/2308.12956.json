{
    "title": "DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])",
    "abstract": "Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-o",
    "link": "http://arxiv.org/abs/2308.12956",
    "context": "Title: DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])\nAbstract: Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-o",
    "path": "papers/23/08/2308.12956.json",
    "total_tokens": 950,
    "translated_title": "DLIP: 提取语言-图像预训练的方法",
    "translated_abstract": "视觉-语言预训练 (VLP) 在极重的参数辅助下取得了显著进展，但这也给其在实际应用中的部署带来了挑战。知识提取作为模型压缩中的重要过程被广泛认可。然而，现有的知识提取技术对于VLP的深入调查和分析还不够，并且VLP导向的提取实践指南尚未被探索。在本文中，我们提出了DLIP，一个简单而高效的提取语言-图像预训练框架，通过它我们研究如何提取一个轻量级的VLP模型。具体地，我们从多个维度剖析了模型提取，如不同模块的架构特性和不同模态的信息传递。我们进行了全面的实验，并对如何提取轻量级但性能优越的VLP模型提供了深刻的见解。实验结果表明，DLIP在准确性和效率的权衡上达到了最先进水平。",
    "tldr": "本文提出了DLIP，提取语言-图像预训练的方法，通过对不同模块的架构特性和不同模态的信息传递进行深入研究，探索了如何提取轻量级但性能优越的视觉-语言预训练模型。实验结果显示DLIP能达到最先进的准确性和效率平衡。",
    "en_tdlr": "This paper proposes DLIP, a method for distilling language-image pre-training, which investigates how to extract a lightweight yet performant vision-language pre-training model by studying the architecture characteristics of different modules and the information transfer of different modalities. Experimental results show that DLIP achieves state-of-the-art accuracy/efficiency trade-off."
}