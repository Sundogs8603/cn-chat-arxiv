{
    "title": "Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])",
    "abstract": "Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle",
    "link": "http://arxiv.org/abs/2308.00566",
    "context": "Title: Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])\nAbstract: Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle",
    "path": "papers/23/08/2308.00566.json",
    "total_tokens": 879,
    "translated_title": "在随机位置预测掩盖的标记改善了掩盖图像建模",
    "translated_abstract": "自监督学习是深度学习中一种有前景的范式，通过构建需要学习有用表示的预训练任务，可以从无标签数据中进行学习。在自然语言处理中，主要的预训练任务是掩盖语言建模（MLM），而在计算机视觉中存在相应的掩盖图像建模（MIM）。然而，MIM具有挑战性，因为它需要在准确位置上预测语义内容。例如，给定一张不完整的狗的图片，我们可以猜测有一个尾巴，但我们无法确定它的确切位置。在这项工作中，我们提出了FlexPredict，这是一个考虑位置不确定性的随机模型，通过将模型条件化到随机掩盖的标记位置上，引导模型学习更加鲁棒对位置不确定性的特征。我们的方法改进了多个任务的下游性能，例如与MIM基准相比，FlexPredict在一系列任务上表现更好。",
    "tldr": "本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。",
    "en_tdlr": "This paper proposes a stochastic model called FlexPredict that improves masked image modeling by incorporating location uncertainty into the model and predicting the positions of masked tokens. Comparing to Masked Image Modeling (MIM) baselines, FlexPredict performs better in a range of tasks."
}