{
    "title": "Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])",
    "abstract": "We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\\big(\\sqrt{d}\\,n^{-1/8}(\\log n)^{1/4}\\big)$ and $O\\big(\\sqrt{d}\\,n^{-1/8}\\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\\iid$) case by \\cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\\ell_2$ norm of the error of SGD dynamics u",
    "link": "http://arxiv.org/abs/2308.01481",
    "context": "Title: Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])\nAbstract: We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\\big(\\sqrt{d}\\,n^{-1/8}(\\log n)^{1/4}\\big)$ and $O\\big(\\sqrt{d}\\,n^{-1/8}\\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\\iid$) case by \\cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\\ell_2$ norm of the error of SGD dynamics u",
    "path": "papers/23/08/2308.01481.json",
    "total_tokens": 1145,
    "translated_title": "在马尔可夫采样下，用于随机梯度下降的在线协方差估计",
    "translated_abstract": "我们研究了用于马尔可夫采样下随机梯度下降的在线重叠批次均值协方差估计器。我们证明了协方差估计器的收敛速率分别为$O\\big(\\sqrt{d}\\,n^{-1/8}(\\log n)^{1/4}\\big)$和$O\\big(\\sqrt{d}\\,n^{-1/8}\\big)$，其中$d$代表维度，$n$表示观测数量或SGD迭代次数。值得注意的是，这些速率与先前由\\cite{zhu2021online}在独立同分布($\\iid$)情况下建立的最佳收敛速率相匹配，除了对数因子。我们的分析克服了由于马尔可夫采样而产生的重要挑战，引入了额外的误差项和批次均值协方差估计器的复杂依赖关系。此外，我们还建立了SGD动态误差$\\ell_2$范数的前四阶矩的收敛速率。",
    "tldr": "本文研究了在马尔可夫采样下的随机梯度下降中的在线重叠批次均值协方差估计器，并证明了其收敛速率为$O\\big(\\sqrt{d}\\,n^{-1/8}(\\log n)^{1/4}\\big)$和$O\\big(\\sqrt{d}\\,n^{-1/8}\\big)$，分别对应于状态相关和状态无关的马尔可夫采样。这些速率与独立同分布情况下的最佳收敛速率相匹配，并且克服了由于马尔可夫采样而引起的挑战。",
    "en_tdlr": "This paper investigates the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling and proves its convergence rates to be $O\\big(\\sqrt{d}\\,n^{-1/8}(\\log n)^{1/4}\\big)$ and $O\\big(\\sqrt{d}\\,n^{-1/8}\\big)$ for state-dependent and state-independent Markovian sampling, respectively. The rates match the best-known convergence rate for the independent and identically distributed ($\\iid$) case and overcome challenges introduced by Markovian sampling."
}