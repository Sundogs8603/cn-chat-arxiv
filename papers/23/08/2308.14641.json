{
    "title": "Challenges of GPT-3-based Conversational Agents for Healthcare. (arXiv:2308.14641v2 [cs.CL] UPDATED)",
    "abstract": "The potential to provide patients with faster information access while allowing medical specialists to concentrate on critical tasks makes medical domain dialog agents appealing. However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences. This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA). We perform several evaluations contextualized in terms of standard medical principles. We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.",
    "link": "http://arxiv.org/abs/2308.14641",
    "context": "Title: Challenges of GPT-3-based Conversational Agents for Healthcare. (arXiv:2308.14641v2 [cs.CL] UPDATED)\nAbstract: The potential to provide patients with faster information access while allowing medical specialists to concentrate on critical tasks makes medical domain dialog agents appealing. However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences. This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA). We perform several evaluations contextualized in terms of standard medical principles. We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.",
    "path": "papers/23/08/2308.14641.json",
    "total_tokens": 865,
    "translated_title": "基于GPT-3的医疗对话代理的挑战",
    "translated_abstract": "医疗领域的对话代理具有提供患者更快信息访问的潜力，同时让医学专家专注于关键任务。然而，将大型语言模型（LLMs）整合到这些代理中存在一定的限制，可能导致严重后果。本文研究了在医学问答（MedQA）系统中使用基于GPT-3模型的挑战和风险。我们对几个评估进行了上下文化的分析，基于标准的医学原则。我们提供了手动设计患者查询的过程，以对LLMs在MedQA系统中的高风险限制进行压力测试。我们的分析揭示了LLMs无法对这些查询做出充分的回应，会生成错误的医学信息、不安全的建议以及可能被认为是冒犯性的内容。",
    "tldr": "本文研究了使用基于GPT-3的模型进行医学问答系统（MedQA）的挑战和风险。通过上下文化的分析和手动设计患者查询的压力测试，我们发现这些模型无法充分回应高风险限制，可能导致错误的医学信息、不安全的建议和冒犯性内容。",
    "en_tdlr": "This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA) systems. Through contextual analysis and stress-testing patient queries, it is found that these models fail to adequately respond to high-risk limitations, leading to erroneous medical information, unsafe recommendations, and offensive content."
}