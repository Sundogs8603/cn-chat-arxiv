{
    "title": "KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution. (arXiv:2308.08361v1 [cs.CV])",
    "abstract": "Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of \"$kernels$\" and \"$assembling$ $kernels$\" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter depen",
    "link": "http://arxiv.org/abs/2308.08361",
    "context": "Title: KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution. (arXiv:2308.08361v1 [cs.CV])\nAbstract: Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of \"$kernels$\" and \"$assembling$ $kernels$\" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter depen",
    "path": "papers/23/08/2308.08361.json",
    "total_tokens": 867,
    "translated_title": "KernelWarehouse：朝着参数有效的动态卷积迈进",
    "translated_abstract": "动态卷积学习一种带有样本相关注意力权重的$n$个静态卷积核的线性组合，与普通卷积相比表现出优异性能。然而，现有设计在参数效率上存在问题：它们将卷积参数数量增加了$n$倍。这与优化困难导致了动态卷积领域的研究进展缓慢，无法使用大的$n$值（例如$n>100$而不是典型设置$n<10$）来推动性能边界。本文提出了KernelWarehouse，一种更通用的动态卷积形式，它在参数效率和表示能力之间取得了有利的平衡。其关键思想是从减少卷积核维度和显著增加卷积核数量的角度重新定义了动态卷积中的\"$kernels$\"和\"$assembling$ $kernels$\"的基本概念。原则上，KernelWarehouse增强了卷积参数的依赖关系。",
    "tldr": "本文提出了KernelWarehouse，它是一种更通用的动态卷积形式，通过重新定义动态卷积中的卷积核的概念，实现了在参数效率和表示能力之间的有利平衡。",
    "en_tdlr": "This paper proposes KernelWarehouse, a more general form of dynamic convolution that strikes a favorable trade-off between parameter efficiency and representation power by redefining the concept of convolution kernels."
}