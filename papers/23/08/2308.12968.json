{
    "title": "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])",
    "abstract": "Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fi",
    "link": "http://arxiv.org/abs/2308.12968",
    "context": "Title: Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])\nAbstract: Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fi",
    "path": "papers/23/08/2308.12968.json",
    "total_tokens": 948,
    "translated_title": "Scenimefy: 通过半监督的图像到图像翻译学习制作动漫场景",
    "translated_abstract": "从复杂的现实世界图像自动高质量地渲染动漫场景具有重要的实际价值。这一任务的挑战在于场景的复杂性、动漫风格的独特特点以及缺乏用于填补领域差距的高质量数据集。尽管之前的努力有所进展，但是仍然不能满意地保持一致的语义保存、明显的风格化和精细的细节。在这项研究中，我们提出了Scenimefy，一个新颖的半监督图像到图像翻译框架，解决了这些挑战。我们的方法通过具有结构一致性的伪配对数据进行学习，简化了纯无监督设置。伪数据是通过语义约束的StyleGAN唯一地导出的，充分利用了像CLIP这样的丰富模型先验知识。我们进一步应用分割引导的数据选择来获得高质量的伪监督。引入了基于补丁的对比风格损失来改善风格化和细节。",
    "tldr": "Scenimefy是一个通过半监督的图像到图像翻译框架，可以自动从现实世界图像中渲染高质量的动漫场景，并且能够保持一致的语义、明显的风格化和精细的细节。",
    "en_tdlr": "Scenimefy is a semi-supervised image-to-image translation framework that can automatically render high-quality anime scenes from real-world images with consistent semantic preservation, evident stylization, and fine details."
}