{
    "title": "Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])",
    "abstract": "Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorit",
    "link": "http://arxiv.org/abs/2308.04061",
    "context": "Title: Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])\nAbstract: Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorit",
    "path": "papers/23/08/2308.04061.json",
    "total_tokens": 941,
    "translated_title": "通过自适应加权正则化和知识蒸馏提高低标签环境下的对抗鲁棒性",
    "translated_abstract": "对抗鲁棒性是近年来受到广泛关注的研究领域，它与构建可信人工智能密切相关。然而，现有的对抗鲁棒性研究主要集中在有大量标记数据的监督学习环境下。本文研究在标记数据稀缺的半监督对抗训练环境下进行。我们提出了两个鲁棒风险的上界，并通过这两个上界提出了一个适用于无标签数据的正则化项。然后，我们开发了一个半监督对抗训练算法，通过将提出的正则化项与使用半监督学习算法训练的半监督教师模型进行知识蒸馏相结合。我们的实验结果表明，我们提出的算法在性能上具有显著优势，超过了现有算法。尤其是与监督学习算法相比，我们的算法表现出更高的性能。",
    "tldr": "本文提出了一种适用于标记数据稀缺环境的半监督对抗训练算法，通过引入适用于无标签数据的正则化项和知识蒸馏，实现了对抗鲁棒性的增强，并在实验证明了其显著优势。"
}