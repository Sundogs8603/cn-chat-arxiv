{
    "title": "Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs. (arXiv:2308.16056v1 [cs.LG])",
    "abstract": "Multitask learning (MTL) leverages task-relatedness to enhance performance. With the emergence of multimodal data, tasks can now be referenced by multiple indices. In this paper, we employ high-order tensors, with each mode corresponding to a task index, to naturally represent tasks referenced by multiple indices and preserve their structural relations. Based on this representation, we propose a general framework of low-rank MTL methods with tensorized support vector machines (SVMs) and least square support vector machines (LSSVMs), where the CP factorization is deployed over the coefficient tensor. Our approach allows to model the task relation through a linear combination of shared factors weighted by task-specific factors and is generalized to both classification and regression problems. Through the alternating optimization scheme and the Lagrangian function, each subproblem is transformed into a convex problem, formulated as a quadratic programming or linear system in the dual form",
    "link": "http://arxiv.org/abs/2308.16056",
    "context": "Title: Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs. (arXiv:2308.16056v1 [cs.LG])\nAbstract: Multitask learning (MTL) leverages task-relatedness to enhance performance. With the emergence of multimodal data, tasks can now be referenced by multiple indices. In this paper, we employ high-order tensors, with each mode corresponding to a task index, to naturally represent tasks referenced by multiple indices and preserve their structural relations. Based on this representation, we propose a general framework of low-rank MTL methods with tensorized support vector machines (SVMs) and least square support vector machines (LSSVMs), where the CP factorization is deployed over the coefficient tensor. Our approach allows to model the task relation through a linear combination of shared factors weighted by task-specific factors and is generalized to both classification and regression problems. Through the alternating optimization scheme and the Lagrangian function, each subproblem is transformed into a convex problem, formulated as a quadratic programming or linear system in the dual form",
    "path": "papers/23/08/2308.16056.json",
    "total_tokens": 872,
    "translated_title": "基于张量化支持向量机和最小二乘支持向量机的低秩多任务学习",
    "translated_abstract": "多任务学习（MTL）利用任务相关性来提高性能。随着多模态数据的出现，任务现在可以由多个索引引用。在本文中，我们使用高阶张量，其中每个模式对应一个任务索引，以自然地表示由多个索引引用的任务并保留它们的结构关系。基于这种表示，我们提出了一种具有张量化支持向量机（SVM）和最小二乘支持向量机（LSSVM）的低秩多任务学习方法的通用框架，其中CP分解部署在系数张量上。我们的方法通过任务特定因子加权的共享因子的线性组合来建模任务关系，并推广到分类和回归问题。通过交替优化方案和Lagrangian函数，每个子问题都被转化为一个凸问题，形式化为对偶形式的二次规划或线性系统。",
    "tldr": "本文提出了一种基于张量化支持向量机和最小二乘支持向量机的低秩多任务学习方法，通过高阶张量表示任务之间的关系，并利用交替优化和Lagrangian函数解决相关的凸问题。",
    "en_tdlr": "This paper proposes a low-rank multitask learning method based on tensorized support vector machines and least square support vector machines. The method utilizes high-order tensors to represent the task-relatedness and solves the convex problems using alternating optimization and Lagrangian function."
}