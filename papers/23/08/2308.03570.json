{
    "title": "Partial identification of kernel based two sample tests with mismeasured data. (arXiv:2308.03570v1 [stat.ML])",
    "abstract": "Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\\epsilon$-contamination, where a possibly non-random $\\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically val",
    "link": "http://arxiv.org/abs/2308.03570",
    "context": "Title: Partial identification of kernel based two sample tests with mismeasured data. (arXiv:2308.03570v1 [stat.ML])\nAbstract: Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\\epsilon$-contamination, where a possibly non-random $\\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically val",
    "path": "papers/23/08/2308.03570.json",
    "total_tokens": 918,
    "translated_title": "基于核的两样本测试中的部分标识和测量数据",
    "translated_abstract": "非参数的两样本测试方法，例如最大均值差异(MMD)，在机器学习应用中经常用于检测两个分布之间的差异。然而，大部分现有文献假设两个感兴趣分布的无误差样本是可用的。我们放松这一假设，研究在ε-污染下估计MMD，其中可能非随机的ε比例一个分布错误地与另一个分布混杂在一起。我们证明，在ε-污染下，MMD的典型估计不可靠。相反，我们研究了MMD的部分标识，并确定了包含真实、未知MMD的尖锐上下界。我们提出了一种估计这些界限的方法，并证明随着样本大小增加，它给出了收敛速度比替代方法更快的对MMD的最尖锐界限的估计。通过三个数据集的实证验证，我们验证了这一方法。",
    "tldr": "这项研究提出了一种基于核的两样本测试方法，考虑了样本数据中的误差问题，并通过部分标识方法给出了MMD的尖锐上下界估计。该方法在样本大小增加时收敛速度更快，并在实证验证中得到了验证。",
    "en_tdlr": "This study introduces a kernel-based two-sample testing method that considers measurement errors in the data, and proposes a partial identification approach to estimate sharp upper and lower bounds of the Maximum Mean Discrepancy (MMD). The proposed method shows faster convergence as the sample size increases and is empirically validated using three datasets."
}