{
    "title": "Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis. (arXiv:2308.03128v1 [cs.LG])",
    "abstract": "This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed \"winning tickets\", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their \"universality\". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.",
    "link": "http://arxiv.org/abs/2308.03128",
    "context": "Title: Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis. (arXiv:2308.03128v1 [cs.LG])\nAbstract: This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed \"winning tickets\", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their \"universality\". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.",
    "path": "papers/23/08/2308.03128.json",
    "total_tokens": 894,
    "translated_title": "迭代幅度修剪作为重整化群的研究：基于中彩票假设的探索",
    "translated_abstract": "本论文深入研究了深度神经网络（DNN）中令人激动的中彩票假设（LTH）的复杂世界。LTH认为，在庞大的DNN中，较小的可训练子网络（称为\"中彩票\"）可以达到与完整模型相媲美的性能。LTH的关键过程是迭代幅度修剪（IMP），它逐步消除最小权重，模拟DNN中的逐步学习。一旦我们确定了这些中彩票，我们进一步研究它们的\"普遍性\"。换句话说，我们检查一种在一个特定问题上表现良好的中彩票是否也能在其他类似问题上表现良好。我们还建立了IMP与物理学中的重整化群（RG）理论之间的桥梁，推动对IMP的更严谨理解。",
    "tldr": "本论文研究了深度神经网络中彩票假设的概念，通过迭代幅度修剪逐步优化模型，并探索了中彩票的普遍性。同时，本论文还提出了IMP与RG理论的联系，促进对IMP的更深入理解。"
}