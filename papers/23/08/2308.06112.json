{
    "title": "Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping. (arXiv:2308.06112v1 [cs.SD])",
    "abstract": "Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed",
    "link": "http://arxiv.org/abs/2308.06112",
    "context": "Title: Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping. (arXiv:2308.06112v1 [cs.SD])\nAbstract: Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed",
    "path": "papers/23/08/2308.06112.json",
    "total_tokens": 969,
    "translated_title": "Lip2Vec:通过潜空间到潜空间的视听表达映射实现高效稳健的视觉语音识别",
    "translated_abstract": "视觉语音识别(VSR)与常见的感知任务不同，它需要对视频序列进行更深入的推理，即使对于人类专家来说也是如此。尽管VSR近年来取得了一些进展，但目前的方法依赖于有标签的数据来完全训练或微调模型以预测目标语音，这限制了它们在训练集之外的广泛泛化能力，并导致在面对具有挑战性的分布情景时性能退化。与以往涉及辅助损失或复杂训练过程和架构的方法不同，我们提出了一个简单的方法，名为Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，该网络将嘴唇序列的编码潜在表示映射到它们对应的音频对的潜空间，这些潜空间对于有效的文本解码具有足够的不变性。然后，使用现成的音频语音识别(ASR)模型将生成的音频表示解码为文本。该方法具备高效和稳健的特点。",
    "tldr": "Lip2Vec是一种通过学习先验模型将嘴唇序列的编码表示映射到音频对的潜空间，并使用音频语音识别模型将生成的音频表示解码为文本的高效稳健的视觉语音识别方法。"
}