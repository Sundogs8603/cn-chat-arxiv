{
    "title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])",
    "abstract": "Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly.",
    "link": "http://arxiv.org/abs/2308.04886",
    "context": "Title: Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])\nAbstract: Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly.",
    "path": "papers/23/08/2308.04886.json",
    "total_tokens": 910,
    "translated_title": "使用马氏距离进行无监督的方言识别",
    "translated_abstract": "方言分类被用于提高机器翻译和语音识别等多种应用系统的整体性能。在实际应用中，部署的方言分类模型可能会遇到与训练数据分布不同的异常输入，也称为离群样本（OOD 样本）。这些 OOD 样本可能导致意外的输出，因为模型训练过程中没有见过这些样本所属的方言。在方言分类领域，离群样本检测是一个鲜为人知的研究领域。为此，我们提出了一种简单而有效的无监督马氏距离特征方法来检测离群样本。我们利用了基于 wav2vec 2.0 变压器模型的方言分类器模型的所有中间层的潜在嵌入来进行多任务学习。我们的方法在离群样本检测方面明显优于其他最先进的方法。",
    "tldr": "提出了一种使用马氏距离进行无监督的方言识别方法，通过利用多任务学习从wav2vec 2.0变压器模型的中间层潜在嵌入中提取特征，实现了有效的离群样本检测，并在该问题上明显优于其他方法。"
}