{
    "title": "Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])",
    "abstract": "Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel \"guided burn-in\" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on ",
    "link": "http://arxiv.org/abs/2308.02668",
    "context": "Title: Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])\nAbstract: Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel \"guided burn-in\" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on ",
    "path": "papers/23/08/2308.02668.json",
    "total_tokens": 882,
    "translated_title": "半监督实例分割的引导蒸馏",
    "translated_abstract": "虽然实例分割方法有了显著的改进，但主导范式是依赖于完全带注释的训练图像，这需要费时费力。为了减轻这种依赖并提高结果，半监督方法利用未标记数据作为额外的训练信号，以限制对标记样本的过拟合。在这个背景下，我们提出了一些新颖的设计选择来显著改进师生蒸馏模型。特别是，我们(i)通过引入新的“引导预烧”阶段改进了蒸馏方法，(ii)评估了不同的实例分割架构、主干网络和预训练策略。与之前只使用监督数据来对学生模型进行预烧的工作相反，我们还利用导师模型的指导在预烧阶段中利用未标记数据。我们改进的蒸馏方法在之前最先进的结果上取得了显著的改进。",
    "tldr": "这项研究提出了一种半监督实例分割的引导蒸馏方法，通过引入新的“引导预烧”阶段和利用未标记数据的导师模型指导，取得了显著的改进。",
    "en_tdlr": "This research proposes a guided distillation method for semi-supervised instance segmentation, which significantly improves the results by introducing a novel \"guided burn-in\" stage and leveraging the guidance of the teacher model using unlabeled data."
}