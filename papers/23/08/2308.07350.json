{
    "title": "Efficient Neural PDE-Solvers using Quantization Aware Training. (arXiv:2308.07350v1 [cs.LG])",
    "abstract": "In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs p",
    "link": "http://arxiv.org/abs/2308.07350",
    "context": "Title: Efficient Neural PDE-Solvers using Quantization Aware Training. (arXiv:2308.07350v1 [cs.LG])\nAbstract: In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs p",
    "path": "papers/23/08/2308.07350.json",
    "total_tokens": 876,
    "translated_title": "使用量化感知训练的高效神经PDE求解器",
    "translated_abstract": "过去几年中，将神经网络应用作为解决偏微分方程的经典数值方法的替代方案已经成为这个有着百年历史的数学领域潜在范式转变。然而，在实际可行性方面，计算成本仍然是一个重大瓶颈。传统的方法通过限制PDE定义的空间分辨率来减轻这个挑战。对于神经PDE求解器，我们可以做得更好：在这里，我们研究了最先进的量化方法在降低计算成本方面的潜力。我们展示了对网络权重和激活进行量化可以成功降低推断的计算成本，同时保持性能。我们在四个标准PDE数据集和三个网络架构上的结果表明，量化感知训练适用于各种设置和三个数量级的FLOPs。最后，我们以实证的方式证明了计算成本与模型性能之间的帕累托最优关系。",
    "tldr": "使用量化感知训练的高效神经PDE求解器研究了在减少计算成本方面的潜力，并证明了量化网络权重和激活可以成功降低计算成本而不损害性能。"
}