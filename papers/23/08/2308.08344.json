{
    "title": "Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])",
    "abstract": "Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \\emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \\texttt{OOD-GMixup} to jointly manipulate the training distribution with \\emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r",
    "link": "http://arxiv.org/abs/2308.08344",
    "context": "Title: Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])\nAbstract: Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \\emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \\texttt{OOD-GMixup} to jointly manipulate the training distribution with \\emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r",
    "path": "papers/23/08/2308.08344.json",
    "total_tokens": 890,
    "translated_title": "使用可控数据增强实现图的带外分布泛化",
    "translated_abstract": "图神经网络（GNN）在图属性分类方面表现出非凡的性能。然而，由于训练和测试数据的选择偏差（例如，在小图上进行训练，在大图上进行测试，或在稠密图上进行训练，在稀疏图上进行测试），分布偏差很普遍。更重要的是，我们经常观察到尽管有单边偏向的数据分区，但却存在着同时具有规模和密度的混合结构分布偏移。混合分布偏移中的伪相关性降低了先前GNN方法的性能，并且在不同数据集之间显示出较大的不稳定性。为了缓解这个问题，我们提出了“OOD-GMixup”在度量空间中以联合操作训练分布的可控数据增强。具体来说，我们首先提取图合理性来消除由于不相关信息而引起的虚假相关性。其次，我们对图合理性进行扰动生成虚拟样本",
    "tldr": "本论文提出了一种名为“OOD-GMixup”的方法，利用可控数据增强来解决图的带外分布泛化问题。该方法通过提取图合理性和生成虚拟样本的方式来消除虚假相关性和稳定性问题。",
    "en_tdlr": "This paper presents a method called \"OOD-GMixup\" to address graph out-of-distribution generalization by utilizing controllable data augmentation. The method eliminates spurious correlations and instability issues by extracting graph rationales and generating virtual samples."
}