{
    "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])",
    "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% whi",
    "link": "http://arxiv.org/abs/2308.12261",
    "context": "Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])\nAbstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% whi",
    "path": "papers/23/08/2308.12261.json",
    "total_tokens": 842,
    "translated_title": "Prompt2Model：从自然语言指令生成可部署模型",
    "translated_abstract": "大型语言模型（LLM）使得系统构建者能够通过提示来创建能胜任的NLP系统，他们只需要用自然语言描述任务并提供一些示例。然而，在其他方面，LLM是传统专用NLP模型的一种退步；它们需要大量计算资源进行部署，并可能被API封锁。本文提出了Prompt2Model，这是一种通用的方法，它接收类似LLMs提供的任务自然语言描述的输入，并使用它来训练一个适合部署的专用模型。这通过多步骤的现有数据集和预训练模型的检索、使用LLMs生成数据集以及这些检索和生成的数据集上的监督微调来完成。在三个任务上，我们证明了在给定相同的少样本提示作为输入情况下，Prompt2Model训练出的模型相比强大的LLM - gpt-3.5-turbo平均性能提升20%。",
    "tldr": "Prompt2Model是一种方法，可以通过自然语言描述任务来训练适合部署的模型，相比大型语言模型，它们具有更高的性能。"
}