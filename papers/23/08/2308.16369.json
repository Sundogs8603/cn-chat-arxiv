{
    "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. (arXiv:2308.16369v1 [cs.LG])",
    "abstract": "Large Language Model (LLM) inference consists of two distinct phases prefill phase which processes the input prompt and decode phase which generates output tokens autoregressively. While the prefill phase effectively saturates GPU compute at small batch sizes, the decode phase results in low compute utilization as it generates one token at a time per request. The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.  We present SARATHI to address these challenges. SARATHI employs chunked-prefills, which splits a prefill request into equal sized chunks, and decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests 'piggyback' and cost up to an order of magnitude less compared to a decode-only batch. Chunked-prefills allows constr",
    "link": "http://arxiv.org/abs/2308.16369",
    "context": "Title: SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. (arXiv:2308.16369v1 [cs.LG])\nAbstract: Large Language Model (LLM) inference consists of two distinct phases prefill phase which processes the input prompt and decode phase which generates output tokens autoregressively. While the prefill phase effectively saturates GPU compute at small batch sizes, the decode phase results in low compute utilization as it generates one token at a time per request. The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.  We present SARATHI to address these challenges. SARATHI employs chunked-prefills, which splits a prefill request into equal sized chunks, and decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests 'piggyback' and cost up to an order of magnitude less compared to a decode-only batch. Chunked-prefills allows constr",
    "path": "papers/23/08/2308.16369.json",
    "total_tokens": 927,
    "translated_title": "SARATHI：通过与分块预填充解码相结合的方式实现高效的LLM推断",
    "translated_abstract": "大型语言模型（LLM）推断包括两个不同的阶段：预填充阶段处理输入提示，解码阶段按自回归方式生成输出令牌。虽然预填充阶段在小批量大小下有效利用了GPU计算能力，但解码阶段每次请求只生成一个令牌，导致计算利用率较低。使用管道并行时，预填充和解码时间的变化还会导致微批量之间的不平衡，进一步增加了效率上的损失。我们提出了SARATHI来应对这些挑战。SARATHI使用分块预填充，将预填充请求分成相等大小的块，并使用解码最大批处理将一个预填充块构造成一个批次，并用解码填充其余的插槽。在推断过程中，预填充块可充分利用GPU计算能力，而解码请求则“搭便车”，成本比仅解码的批次低一个数量级。分块预填充使得约束适应了GPU和CPU之间的差异，并提高了推断的效率。",
    "tldr": "SARATHI通过使用分块预填充和解码最大批处理的方式，高效地处理LLM推断中的预填充和解码阶段的不平衡问题，提高推断效率。",
    "en_tdlr": "SARATHI efficiently addresses the imbalance between the prefill and decode phases in LLM inference by using chunked-prefills and decode-maximal batching, resulting in improved inference efficiency."
}