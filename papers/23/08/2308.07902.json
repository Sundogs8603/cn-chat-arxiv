{
    "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. (arXiv:2308.07902v1 [cs.CL])",
    "abstract": "From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar ta",
    "link": "http://arxiv.org/abs/2308.07902",
    "context": "Title: Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. (arXiv:2308.07902v1 [cs.CL])\nAbstract: From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar ta",
    "path": "papers/23/08/2308.07902.json",
    "total_tokens": 956,
    "translated_title": "通过核心竞争力的视角：大型语言模型评估调查",
    "translated_abstract": "从预训练语言模型（PLM）到大型语言模型（LLM），自然语言处理（NLP）领域已经见证了快速的性能提升和广泛的实际应用。然而，由于LLM的卓越性能，对其进行全面评估极其困难，主要有两个原因：首先，传统的NLP任务在LLM表现出色之后变得不够适用；其次，现有的评估任务难以跟上实际场景中广泛应用的速度。为了解决这些问题，已有研究提出了各种基准来更好地评估LLM。为了澄清学术界和工业界中与LLM评估相关的众多论文，我们调查了多篇关于LLM评估的论文。我们总结了LLM的4个核心竞争力，包括推理能力、知识能力、可靠性和安全性。对于每个核心竞争力，我们介绍了其定义、对应的基准和评估指标。基于这个核心竞争力体系，类似的任务可进行比较。",
    "tldr": "本论文通过核心竞争力的视角，调查了大型语言模型(LLM)评估的多篇论文，总结出LLM的4个核心竞争力：推理能力、知识能力、可靠性和安全性，并介绍了对应的基准和评估指标。",
    "en_tdlr": "This study investigates multiple papers on the evaluation of large language models (LLMs) through the lens of core competencies, identifying four key competencies: reasoning, knowledge, reliability, and safety. Each competency is defined and corresponding benchmarks and metrics are introduced."
}