{
    "title": "Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])",
    "abstract": "Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.",
    "link": "http://arxiv.org/abs/2308.05724",
    "context": "Title: Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])\nAbstract: Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.",
    "path": "papers/23/08/2308.05724.json",
    "total_tokens": 857,
    "translated_title": "通过动态激活函数优化前馈和卷积神经网络的性能",
    "translated_abstract": "近年来，深度学习训练算法在语音、文本、图像和视频等许多领域取得了巨大成功。人们提出了越来越深的网络层次结构，如具有约152层的ResNet结构。浅层卷积神经网络（CNN）仍然是一个活跃的研究领域，其中一些现象仍然没有得到解释。网络中使用的激活函数非常重要，因为它们为网络提供了非线性。ReLU是最常用的激活函数。我们在隐藏层使用了复杂的分段线性（PWL）激活函数。我们证明了这些PWL激活函数在我们的卷积神经网络和多层感知器中比ReLU激活函数的性能更好。我们还提供了在PyTorch中比较浅层和深度CNN的结果，以进一步证实我们的观点。",
    "tldr": "该论文研究了使用动态激活函数优化前馈和卷积神经网络的性能。研究结果表明，在卷积神经网络和多层感知器中，复杂的分段线性激活函数比ReLU激活函数表现更好。"
}