{
    "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification. (arXiv:2308.15010v1 [cs.CL])",
    "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowled",
    "link": "http://arxiv.org/abs/2308.15010",
    "context": "Title: TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification. (arXiv:2308.15010v1 [cs.CL])\nAbstract: Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowled",
    "path": "papers/23/08/2308.15010.json",
    "total_tokens": 902,
    "translated_title": "TransPrompt v2: 一种用于跨任务文本分类的可转移提示框架",
    "translated_abstract": "文本分类是自然语言处理中最重要的任务之一。最近使用预训练语言模型（PLMs）取得了在这个任务上显著的成功。然而，PLMs获得满意的结果很大程度上取决于大量的任务特定标注数据，而在许多应用场景中，由于数据访问和隐私限制，这可能是不可行的。最近提出的基于提示的微调范式通过任务特定模板提高了PLMs在少样本文本分类中的性能。然而，目前还不清楚提示知识如何在任务之间进行传递，以实现相互增强的目的。我们提出了TransPrompt v2，一种新颖的可转移提示框架，适用于相似或不同的少样本学习文本分类任务。对于相似任务的学习，我们采用多任务元知识获取（MMA）过程来训练一个捕捉跨任务可转移知识的元学习器。",
    "tldr": "TransPrompt v2是一种可转移的提示框架，适用于跨任务文本分类。它通过多任务元知识获取来训练一个捕捉跨任务可转移知识的元学习器，以提升预训练语言模型在少样本学习中的性能。"
}