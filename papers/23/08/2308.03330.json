{
    "title": "Expediting Neural Network Verification via Network Reduction. (arXiv:2308.03330v2 [cs.SE] UPDATED)",
    "abstract": "A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also sh",
    "link": "http://arxiv.org/abs/2308.03330",
    "context": "Title: Expediting Neural Network Verification via Network Reduction. (arXiv:2308.03330v2 [cs.SE] UPDATED)\nAbstract: A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also sh",
    "path": "papers/23/08/2308.03330.json",
    "total_tokens": 847,
    "translated_title": "通过网络简化加速神经网络验证",
    "translated_abstract": "已经提出了各种验证方法来验证深度神经网络的安全性，以确保网络在关键应用中正确运行。然而，许多众所周知的验证工具在复杂的网络架构和大型网络大小上仍然存在困难。在这项工作中，我们提出了一种网络简化技术作为验证之前的预处理方法。所提出的方法通过消除稳定的ReLU神经元，并将其转化为由ReLU和仿射层组成的顺序神经网络，从而可以通过大多数验证工具处理。我们在最先进的完整和不完整验证工具上实现了简化技术，包括alpha-beta-crown，VeriNet和PRIMA。我们在大量基准测试中的实验表明，所提出的技术可以显著减小神经网络并加速现有的验证工具。此外，实验结果还表明...",
    "tldr": "本研究提出了一种网络简化技术，在验证之前对神经网络进行预处理，通过消除稳定的ReLU神经元，并将其转化为由ReLU和仿射层组成的顺序神经网络，从而显著减小了神经网络的规模并加速了现有的验证工具。",
    "en_tdlr": "This study proposes a network reduction technique as a preprocessing method to simplify neural networks for verification. By eliminating stable ReLU neurons and transforming them into a sequential neural network consisting of ReLU and affine layers, the proposed technique significantly reduces the network size and speeds up existing verification tools."
}