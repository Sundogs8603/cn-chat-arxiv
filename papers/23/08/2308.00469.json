{
    "title": "Mirror Natural Evolution Strategies. (arXiv:2308.00469v1 [cs.LG])",
    "abstract": "The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\\mu, \\Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \\texttt{MiNES} (mirror descent natu",
    "link": "http://arxiv.org/abs/2308.00469",
    "context": "Title: Mirror Natural Evolution Strategies. (arXiv:2308.00469v1 [cs.LG])\nAbstract: The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\\mu, \\Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \\texttt{MiNES} (mirror descent natu",
    "path": "papers/23/08/2308.00469.json",
    "total_tokens": 851,
    "translated_title": "镜像自然进化策略",
    "translated_abstract": "零阶优化在机器学习应用中被广泛使用。然而，对于近似梯度使用零阶函数值差异的零阶优化算法的理论研究集中在随机方向上。近似梯度和Hessian信息的零阶查询算法的理论研究较少。本文主要研究利用零阶查询近似一阶和二阶信息的零阶优化理论。首先，我们提出了一个新的重参数化目标函数，其参数为$(\\mu, \\Sigma)$。这个重参数化目标函数在原目标函数的最小化器和Hessian的逆之处分别达到其最优值，但有小的扰动。因此，我们提出了一种新的算法来最小化我们提出的重参数化目标函数，我们称之为MiNES（镜像下降自然进化策略）。",
    "tldr": "这篇论文研究了基于零阶查询近似一阶和二阶信息的零阶优化算法，提出了一种新的重参数化目标函数并给出了对应的最小化算法，名为MiNES（镜像下降自然进化策略）。",
    "en_tdlr": "This paper investigates zeroth-order optimization algorithms that approximate first-order and second-order information using zeroth-order queries, proposes a novel reparameterized objective function, and introduces a corresponding minimization algorithm called MiNES (mirror descent natural evolution strategies)."
}