{
    "title": "Graph Edit Distance Learning via Different Attention. (arXiv:2308.13871v1 [cs.AI])",
    "abstract": "Recently, more and more research has focused on using Graph Neural Networks (GNN) to solve the Graph Similarity Computation problem (GSC), i.e., computing the Graph Edit Distance (GED) between two graphs. These methods treat GSC as an end-to-end learnable task, and the core of their architecture is the feature fusion modules to interact with the features of two graphs. Existing methods consider that graph-level embedding is difficult to capture the differences in local small structures between two graphs, and thus perform fine-grained feature fusion on node-level embedding can improve the accuracy, but leads to greater time and memory consumption in the training and inference phases. However, this paper proposes a novel graph-level fusion module Different Attention (DiffAtt), and demonstrates that graph-level fusion embeddings can substantially outperform these complex node-level fusion embeddings. We posit that the relative difference structure of the two graphs plays an important rol",
    "link": "http://arxiv.org/abs/2308.13871",
    "context": "Title: Graph Edit Distance Learning via Different Attention. (arXiv:2308.13871v1 [cs.AI])\nAbstract: Recently, more and more research has focused on using Graph Neural Networks (GNN) to solve the Graph Similarity Computation problem (GSC), i.e., computing the Graph Edit Distance (GED) between two graphs. These methods treat GSC as an end-to-end learnable task, and the core of their architecture is the feature fusion modules to interact with the features of two graphs. Existing methods consider that graph-level embedding is difficult to capture the differences in local small structures between two graphs, and thus perform fine-grained feature fusion on node-level embedding can improve the accuracy, but leads to greater time and memory consumption in the training and inference phases. However, this paper proposes a novel graph-level fusion module Different Attention (DiffAtt), and demonstrates that graph-level fusion embeddings can substantially outperform these complex node-level fusion embeddings. We posit that the relative difference structure of the two graphs plays an important rol",
    "path": "papers/23/08/2308.13871.json",
    "total_tokens": 848,
    "translated_title": "通过不同的关注方式学习图编辑距离",
    "translated_abstract": "最近，越来越多的研究集中在使用图神经网络（GNN）来解决图相似性计算问题（GSC），即计算两个图之间的图编辑距离（GED）。这些方法把GSC视为一个可学习的端到端任务，其核心是特征融合模块，用于与两个图的特征交互。现有方法认为，图级嵌入很难捕捉两个图之间局部小结构的差异，因此对节点级嵌入进行细粒度特征融合可以提高准确性，但在训练和推理阶段会导致更大的时间和内存消耗。然而，本文提出了一种新颖的图级特征融合模块Different Attention（DiffAtt），并证明了图级融合嵌入可以显著优于这些复杂的节点级融合嵌入。",
    "tldr": "该论文提出了一种通过不同的关注方式学习图编辑距离的方法，该方法通过图级特征融合模块显著提高了计算的准确性，相对于节点级融合，具有更好的性能和效率。"
}