{
    "title": "Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])",
    "abstract": "Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para",
    "link": "http://arxiv.org/abs/2308.03767",
    "context": "Title: Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])\nAbstract: Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para",
    "path": "papers/23/08/2308.03767.json",
    "total_tokens": 886,
    "translated_title": "使用基于Transformer的框架结合深度信息增强图像字幕生成",
    "translated_abstract": "图像字幕生成是连接计算机视觉和自然语言处理的具有挑战性的场景理解任务。虽然图像字幕生成模型在生成优秀描述方面取得了成功，但该领域主要集中于为2D图像生成单个句子。本文研究了通过将深度信息与RGB图像集成，能否增强字幕生成任务并生成更好的描述。为此，我们提出了一个基于Transformer的编码器-解码器框架，用于生成3D场景的多句字幕描述。RGB图像及其对应的深度图作为我们框架的输入，将它们结合起来以更好地理解输入场景。深度图可以是真实值或估计值，这使得我们的框架在任何RGB字幕数据集上都可以广泛应用。我们探索了不同的融合方法来融合RGB和深度图像。实验在NYU-v2数据集和Stanford图像集上进行。",
    "tldr": "本研究提出了一个基于Transformer的框架，将深度信息与RGB图像结合，以增强图像字幕生成任务。通过使用多句字幕描述3D场景，我们的框架能够更好地理解输入场景，并在不同的融合方法下进行实验验证。",
    "en_tdlr": "This paper presents a Transformer-based framework that integrates depth information with RGB images to enhance image captioning. By generating multi-sentence descriptions of 3D scenes, our framework improves the understanding of input scenes and conducts experiments with different fusion approaches."
}