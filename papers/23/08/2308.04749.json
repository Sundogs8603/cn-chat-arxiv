{
    "title": "Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks. (arXiv:2308.04749v1 [cs.AI])",
    "abstract": "Children possess the ability to learn multiple cognitive tasks sequentially, which is a major challenge toward the long-term goal of artificial general intelligence. Existing continual learning frameworks are usually applicable to Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired, energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning mechanisms during child growth and development, we propose Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive continual learning. When learning a sequence of tasks, the DSD-SNN dynamically assigns and grows new neurons to new tasks and prunes redundant neurons, thereby increasing memory capacity and reducing computational overhead. In addition, the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks, empowering a single network capable of supporting multiple incremental tasks (without the separate sub-network mask for each ta",
    "link": "http://arxiv.org/abs/2308.04749",
    "context": "Title: Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks. (arXiv:2308.04749v1 [cs.AI])\nAbstract: Children possess the ability to learn multiple cognitive tasks sequentially, which is a major challenge toward the long-term goal of artificial general intelligence. Existing continual learning frameworks are usually applicable to Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired, energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning mechanisms during child growth and development, we propose Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive continual learning. When learning a sequence of tasks, the DSD-SNN dynamically assigns and grows new neurons to new tasks and prunes redundant neurons, thereby increasing memory capacity and reducing computational overhead. In addition, the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks, empowering a single network capable of supporting multiple incremental tasks (without the separate sub-network mask for each ta",
    "path": "papers/23/08/2308.04749.json",
    "total_tokens": 923,
    "translated_title": "提升带有动态结构发展的脉冲神经网络的高效连续学习",
    "translated_abstract": "孩童具备顺序学习多个认知任务的能力，这对于实现人工通用智能的长远目标是一个重要挑战。现有的连续学习框架通常适用于深度神经网络（DNNs），但对更加脑启发、能效更高的脉冲神经网络（SNNs）缺乏探索。借鉴孩童成长和发展过程中的连续学习机制，我们提出了动态结构发展的脉冲神经网络（DSD-SNN），用于高效和自适应的连续学习。在学习一系列任务时，DSD-SNN会动态地分配并增长新的神经元来处理新任务，并修剪多余的神经元，从而增加记忆容量并减少计算开销。此外，重叠的共享结构有助于快速将所有已获得的知识应用到新任务上，使单个网络能够支持多个增量任务（而无需为每个任务单独创建子网络的掩码）。",
    "tldr": "使用动态结构发展的脉冲神经网络来实现高效连续学习，通过动态增长和修剪神经元来提高记忆容量和减少计算开销，同时利用重叠的共享结构快速应用已获得的知识到新任务上。"
}