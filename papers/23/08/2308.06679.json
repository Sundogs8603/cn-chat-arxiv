{
    "title": "Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations. (arXiv:2308.06679v1 [cs.LG])",
    "abstract": "The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GR",
    "link": "http://arxiv.org/abs/2308.06679",
    "context": "Title: Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations. (arXiv:2308.06679v1 [cs.LG])\nAbstract: The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GR",
    "path": "papers/23/08/2308.06679.json",
    "total_tokens": 935,
    "translated_title": "可分离高斯神经网络：结构、分析和函数逼近",
    "translated_abstract": "高斯径向基函数神经网络（GRBFNN）一直是插值和分类的常见选择。然而，在输入向量的维度较高时，计算量很大。为解决这个问题，我们提出了一种新的前馈网络-可分离高斯神经网络（SGNN），利用高斯函数的可分离性，将输入数据分成多列，并依次将它们馈送到由单变量高斯函数形成的并行层中。这种结构将GRBFNN的神经元数量从O(N^d)降低到O(dN)，从而指数级提高了SGNN的计算速度，并使其能够线性地随着输入维度的增加而扩展。此外，SGNN可以在梯度下降训练中保留GRBFNN的Hessian矩阵的主要子空间，从而实现与GRBFNN类似水平的准确性。实验证明，SGNN可以以与GRBFNN相似的准确性实现100倍的加速。",
    "tldr": "可分离高斯神经网络（SGNN）通过利用高斯函数的可分离性，将输入数据分成多列并依次馈送到并行层中，从而实现了指数级的计算速度提升和线性的扩展性。同时，SGNN能够在训练过程中保持类似于GRBFNN的准确性水平。",
    "en_tdlr": "Separable Gaussian Neural Network (SGNN) achieves exponential improvement in computational speed and linear scalability by splitting input data into multiple columns and sequentially feeding them into parallel layers, while preserving a similar level of accuracy to GRBFNN in training."
}