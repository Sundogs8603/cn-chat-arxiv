{
    "title": "Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])",
    "abstract": "Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \\ie the adversarial robustness of models declines to near zero during training.  To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.  Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.  To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the l",
    "link": "http://arxiv.org/abs/2308.12857",
    "context": "Title: Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])\nAbstract: Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \\ie the adversarial robustness of models declines to near zero during training.  To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.  Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.  To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the l",
    "path": "papers/23/08/2308.12857.json",
    "total_tokens": 893,
    "translated_title": "快速对抗训练与平滑收敛",
    "translated_abstract": "快速对抗训练（FAT）有助于提高神经网络的对抗鲁棒性。然而，之前的FAT工作在处理大的扰动预算时遇到了一个重要问题，即在训练过程中，模型的对抗鲁棒性下降到接近零的程度，被称为灾难性过拟合。为了解决这个问题，我们分析了之前FAT工作的训练过程，并观察到灾难性过拟合伴随着收敛损失离群值的出现。因此，我们认为一个适度平滑的收敛过程将是一个稳定的FAT过程，可以解决灾难性过拟合。为了获得平滑的收敛过程，我们提出了一种新的振荡约束（称为ConvergeSmooth），来限制相邻周期之间的损失差异。ConvergeSmooth的收敛步长被引入来平衡收敛和平滑。同样，我们设计了不引入额外超参数的权重集中，除了l.",
    "tldr": "本论文提出了一种快速对抗训练方法（FAT），通过引入平滑收敛过程和振荡约束来解决在处理大的扰动预算时出现的灾难性过拟合问题。",
    "en_tdlr": "This paper proposes a Fast Adversarial Training (FAT) method that addresses the issue of catastrophic overfitting encountered in dealing with large perturbation budgets by introducing a smooth convergence process and an oscillatory constraint."
}