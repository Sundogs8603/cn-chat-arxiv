{
    "title": "ChatGPT for Arabic Grammatical Error Correction. (arXiv:2308.04492v1 [cs.AI])",
    "abstract": "Recently, large language models (LLMs) fine-tuned to follow human instruction have exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC) tasks, particularly in non-English languages, remains significantly unexplored. In this paper, we delve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task made complex due to Arabic's rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to $65.49$ F\\textsubscript{1} score under expert prompting (approximately $5$ points higher than our established baseline). This highlights the potential of LLMs in low-resource settings, offering a viable approach for generating useful synthetic data for model training. Despite these positive results, we find that instruction fine-tuned models, regardless of their size, significantly underperform compar",
    "link": "http://arxiv.org/abs/2308.04492",
    "context": "Title: ChatGPT for Arabic Grammatical Error Correction. (arXiv:2308.04492v1 [cs.AI])\nAbstract: Recently, large language models (LLMs) fine-tuned to follow human instruction have exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC) tasks, particularly in non-English languages, remains significantly unexplored. In this paper, we delve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task made complex due to Arabic's rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to $65.49$ F\\textsubscript{1} score under expert prompting (approximately $5$ points higher than our established baseline). This highlights the potential of LLMs in low-resource settings, offering a viable approach for generating useful synthetic data for model training. Despite these positive results, we find that instruction fine-tuned models, regardless of their size, significantly underperform compar",
    "path": "papers/23/08/2308.04492.json",
    "total_tokens": 1062,
    "translated_title": "用于阿拉伯语语法错误修正的ChatGPT",
    "translated_abstract": "最近，被调整以遵循人类指令的大型语言模型（LLM）在各种英文NLP任务中展示出了显著的能力。然而，在语法错误修正（GEC）任务中，特别是在非英文语言中，它们的表现仍然相当未被探索。在本文中，我们研究了指令调校的LLM在阿拉伯语GEC中的能力，这是由于阿拉伯语的丰富形态而变得复杂的任务。我们的研究结果表明，结合不同的提示方法，并结合（上下文）少样本学习，展示出了相当大的效果，GPT-4在专家提示下达到了65.49的F1得分（比我们已有的基准提高了大约5个点）。这突显了LLM在资源匮乏环境中的潜力，为模型训练生成有用的合成数据提供了可行的方法。尽管取得了积极的结果，我们发现，不论规模如何，指令调校模型的性能显著低于比较基准。",
    "tldr": "本研究针对阿拉伯语语法错误修正任务，探究了指令调校的大型语言模型（LLMs）的能力。通过结合多种提示方法和少样本学习，我们的研究发现，在阿拉伯语语法错误修正中，GPT-4能够达到65.49的F1得分，比我们已有的基准提高了大约5个点。这表明LLMs在资源匮乏环境中具有潜力，并为模型训练提供了生成合成数据的可行方法。"
}