{
    "title": "Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])",
    "abstract": "We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-bas",
    "link": "http://arxiv.org/abs/2308.12372",
    "context": "Title: Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])\nAbstract: We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-bas",
    "path": "papers/23/08/2308.12372.json",
    "total_tokens": 913,
    "translated_title": "Vision Transformer适配器用于可泛化的多任务学习",
    "translated_abstract": "我们引入了第一个多任务视觉变换器适配器，学习可以应用于新任务和领域的通用任务关联性。将其集成到现成的视觉变换器骨干中，我们的适配器可以以参数有效的方式同时解决多个密集视觉任务，而不像现有的多任务变换器那样具有参数的高昂成本。与并行方法相比，我们不需要在添加新任务或领域时进行重新训练或微调。我们在适配器框架中引入了一个任务自适应的注意机制，将基于梯度的任务相似性与基于注意力的相似性结合起来。学习的任务关联性在以下情况下具有泛化能力：零样本任务迁移，无监督域自适应以及不需要对新领域进行微调的泛化。我们证明了我们的方法不仅胜过现有的基于卷积神经网络的多任务方法，还胜过视觉变换器法。",
    "tldr": "该论文介绍了一种使用适配器的视觉变换器，用于可泛化的多任务学习，可以在无需重新训练或微调的情况下解决多个密集视觉任务，并且在零样本任务迁移、无监督域自适应和泛化到新领域时具有更好的性能。",
    "en_tdlr": "This paper introduces vision transformer adapters for generalizable multitask learning, which can solve multiple dense vision tasks without the need for retraining or fine-tuning. It outperforms existing methods in zero-shot task transfer, unsupervised domain adaptation, and generalization to novel domains."
}