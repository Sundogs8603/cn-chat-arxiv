{
    "title": "AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis. (arXiv:2308.08577v1 [cs.SD])",
    "abstract": "Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We sho",
    "link": "http://arxiv.org/abs/2308.08577",
    "context": "Title: AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis. (arXiv:2308.08577v1 [cs.SD])\nAbstract: Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We sho",
    "path": "papers/23/08/2308.08577.json",
    "total_tokens": 949,
    "translated_title": "AffectEcho：针对语音合成的无关说话人和无关语言的情感和情感传递模型",
    "translated_abstract": "情感是一种包括价值、唤醒度和强度的情绪特征，对于实现真实对话至关重要。现有的文本到语音(TTS)和语音到语音系统依赖于强度嵌入向量和全局风格标记来捕捉情感，这些模型将情感表示为风格的组成部分或以离散的类别表示。我们提出了一种情感转换模型AffectEcho，它使用了一个向量量化码书来在量化空间中对情感进行建模，该空间具有五个级别的情感强度，以捕捉同一情感中的复杂细微差别。这些量化的情感嵌入是从口语语音样本中隐含地派生出来的，消除了独热向量或显式强度嵌入的需要。实验结果表明，我们的方法在控制生成的语音情感的同时保持了每个说话人独特的身份、风格和情感节奏的有效性。",
    "tldr": "AffectEcho是一种无关说话人和无关语言的情感和情感传递模型，通过使用向量量化码书来建模情感，并在五个级别的情感强度上捕捉细微差别，从而成功地控制生成的语音的情感，同时保持每个说话人独特的身份、风格和情感节奏。"
}