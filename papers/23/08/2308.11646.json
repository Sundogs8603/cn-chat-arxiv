{
    "title": "Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations a",
    "link": "http://arxiv.org/abs/2308.11646",
    "context": "Title: Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations a",
    "path": "papers/23/08/2308.11646.json",
    "total_tokens": 864,
    "translated_title": "联邦学习中非独立和同分布数据的联合局部关系增强和全局纳什均衡",
    "translated_abstract": "联邦学习是一种需要服务器和一系列具有分散数据的客户端之间合作的分布式机器学习范 paradigm。为了使联邦学习在实际应用中有效，现有的研究致力于改进非独立相同分布(non-IID)的分散数据的建模。在非IID环境中，在数据建模中存在来自不平衡数据的客户端内一致性和异构客户端分布之间的客户端间不一致性，这不仅阻碍了少数数据的充分表示，还带来了不一致的模型偏差。然而，以往的工作忽视了同时处理上述两种耦合不一致性。在这项工作中，我们提出了FedRANE，它由两个主要模块组成，即局部关系增强(LRA)和全局纳什均衡(GNE)，以同时解决客户端内和客户端间的不一致性问题。",
    "tldr": "本论文中提出的FedRANE方法结合了局部关系增强和全局纳什均衡，可以解决分布式机器学习中的非独立非同分布数据问题。"
}