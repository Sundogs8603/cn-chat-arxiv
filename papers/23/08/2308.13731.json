{
    "title": "Learning variational autoencoders via MCMC speed measures. (arXiv:2308.13731v1 [stat.ML])",
    "abstract": "Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximizing an Evidence Lower Bound (ELBO). There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo (MCMC) methods for the construction of variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical repres",
    "link": "http://arxiv.org/abs/2308.13731",
    "context": "Title: Learning variational autoencoders via MCMC speed measures. (arXiv:2308.13731v1 [stat.ML])\nAbstract: Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximizing an Evidence Lower Bound (ELBO). There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo (MCMC) methods for the construction of variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical repres",
    "path": "papers/23/08/2308.13731.json",
    "total_tokens": 912,
    "translated_title": "通过MCMC速度度量学习变分自动编码器",
    "translated_abstract": "变分自动编码器（VAEs）是一种流行的基于似然的生成模型，可以通过最大化下界（ELBO）来有效训练。为了获得更紧的变分边界和更高的生成性能，改进变分分布的表达能力取得了很大进展。虽然先前的工作利用马尔可夫链蒙特卡洛（MCMC）方法构建了变分密度，但针对深度潜变量模型调整提案分布的基于梯度的方法受到的关注较少。本研究提出一种基于熵的短期调整Metropolis-adjusted Langevin（MALA）或Hamiltonian Monte Carlo（HMC）链的方法，并优化更紧的变分边界以获得对数似然。实验证明，该方法产生了更高的保留对数似然以及改进的生成指标。我们的隐式变分密度能够适应潜在层次表示的复杂后验几何形状。",
    "tldr": "本研究提出了一种基于熵的短期调整MCMC链的方法，用于在优化更紧的变分边界的同时，适应深度潜变量模型的提案分布。实验证明，这种方法能够使模型得到更高的保留对数似然和改进的生成性能。"
}