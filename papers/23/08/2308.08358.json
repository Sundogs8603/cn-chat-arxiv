{
    "title": "Convergence of Two-Layer Regression with Nonlinear Units. (arXiv:2308.08358v1 [cs.LG])",
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.",
    "link": "http://arxiv.org/abs/2308.08358",
    "context": "Title: Convergence of Two-Layer Regression with Nonlinear Units. (arXiv:2308.08358v1 [cs.LG])\nAbstract: Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.",
    "path": "papers/23/08/2308.08358.json",
    "total_tokens": 847,
    "translated_title": "两层非线性单元回归的收敛性研究",
    "translated_abstract": "大型语言模型（LLMs），如ChatGPT和GPT4，在许多人类生活任务中表现出色。注意力计算在训练LLMs中起着重要作用。Softmax单元和ReLU单元是注意力计算的关键结构。受到它们的启发，我们提出了一个softmax ReLU回归问题。总的来说，我们的目标是找到涉及ReLU单元的回归问题的最优解。在这项工作中，我们计算了损失函数的Hessian的闭合形式表示。在一定的假设下，我们证明了Hessian的Lipschitz连续性和PSD性质。然后，我们引入了基于近似牛顿法的贪婪算法，该算法在距离最优解的意义下收敛。最后，我们放宽了Lipschitz条件，并证明了在损失值的意义下的收敛性。",
    "tldr": "本研究研究了两层非线性单元回归的收敛性，提出了一个softmax ReLU回归问题，并证明了Hessian的性质，引入了基于近似牛顿法的贪婪算法，最后证明了收敛性。"
}