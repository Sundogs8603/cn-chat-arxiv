{
    "title": "SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification. (arXiv:2308.08669v1 [cs.CV])",
    "abstract": "Skin cancer is a treatable disease if discovered early. We provide a production-specific solution to the skin cancer classification problem that matches human performance in melanoma identification by training a vision transformer on melanoma medical images annotated by experts. Since inference cost, both time and memory wise is important in practice, we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost. Memory-wise, our model is 49.60% smaller than the teacher. Time-wise, our solution is 69.25% faster on GPU and 97.96% faster on CPU. By adding classification heads at each level of the transformer and employing a cascading distillation process, we improve the balanced multi-class accuracy of the base model by 2.1%, while creating a range of models of various sizes but comparable performance. We provide the code at https://github.com/Longman-Stan/SkinDistilVit.",
    "link": "http://arxiv.org/abs/2308.08669",
    "context": "Title: SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification. (arXiv:2308.08669v1 [cs.CV])\nAbstract: Skin cancer is a treatable disease if discovered early. We provide a production-specific solution to the skin cancer classification problem that matches human performance in melanoma identification by training a vision transformer on melanoma medical images annotated by experts. Since inference cost, both time and memory wise is important in practice, we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost. Memory-wise, our model is 49.60% smaller than the teacher. Time-wise, our solution is 69.25% faster on GPU and 97.96% faster on CPU. By adding classification heads at each level of the transformer and employing a cascading distillation process, we improve the balanced multi-class accuracy of the base model by 2.1%, while creating a range of models of various sizes but comparable performance. We provide the code at https://github.com/Longman-Stan/SkinDistilVit.",
    "path": "papers/23/08/2308.08669.json",
    "total_tokens": 927,
    "translated_title": "SkinDistilViT: 用于皮肤病变分类的轻量级视觉Transformer",
    "translated_abstract": "如果及早发现，皮肤癌是一种可治疗的疾病。我们提供了一个针对皮肤癌分类问题的生产专用解决方案，通过训练一种视觉Transformer来匹配专家注释的黑色素瘤医学图像中的人类表现。由于推理成本，在时间和内存方面都非常重要，我们采用知识蒸馏来获得一个在精度方面保留了98.33%的老师的平衡多类准确性的模型，成本只是老师模型的一小部分。就内存而言，我们的模型比老师模型小了49.60%。从时间上看，我们的解决方案在GPU上快了69.25%，在CPU上快了97.96%。通过在Transformer的每个层级上添加分类头并采用级联蒸馏过程，我们将基础模型的平衡多类准确性提高了2.1%，同时创建了一系列的模型，其尺寸各不相同但性能相当。我们在https://github.com/Longman-Stan/SkinDistilVit上提供了代码。",
    "tldr": "本论文提出了一个用于皮肤病变分类的轻量级视觉Transformer模型，采用知识蒸馏方法，既提高了模型的性能，又大幅减少了内存和推理时间的开销。",
    "en_tdlr": "This paper proposes a lightweight vision Transformer model for skin lesion classification, utilizing knowledge distillation to improve performance while significantly reducing memory and inference time costs."
}