{
    "title": "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-",
    "link": "http://arxiv.org/abs/2308.12674",
    "context": "Title: Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])\nAbstract: Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-",
    "path": "papers/23/08/2308.12674.json",
    "total_tokens": 918,
    "translated_title": "通过增强指令来提高大型语言模型的翻译真实性",
    "translated_abstract": "大型语言模型(LLMs)具有很强的通用能力，目前一个引人注目的挑战是通过低成本的指令调整来激发它们的专门能力，如机器翻译。标准的指令遵循数据是按顺序组织的，包括指令、输入和响应的连接。由于LLMs的注意机制在局部关注上存在局限性，LLMs倾向于在每个位置更多地关注附近的单词或句子。这导致在解码过程中遗忘指令的风险很高。为了缓解上述问题，我们提出了SWIE（分段加权指令嵌入）和一个指令遵循数据集OVERMISS。SWIE通过在后续的输入和响应表示上添加全局指令表示来改善模型对指令的理解。OVERMISS通过将过度翻译和遗漏翻译结果与正确翻译进行比较来提高模型的真实性。我们将我们的方法应用到两个主要的翻译任务中。",
    "tldr": "通过增强指令来提高大型语言模型的翻译真实性，我们提出了SWIE（分段加权指令嵌入）和一个指令遵循数据集OVERMISS，用于改善模型对指令的理解和提高模型的真实性。",
    "en_tdlr": "Improving the translation faithfulness of large language models, we propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS to enhance the model's understanding of instructions and improve its faithfulness."
}