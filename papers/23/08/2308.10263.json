{
    "title": "Scaling up Discovery of Latent Concepts in Deep NLP Models",
    "abstract": "Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.",
    "link": "https://arxiv.org/abs/2308.10263",
    "context": "Title: Scaling up Discovery of Latent Concepts in Deep NLP Models\nAbstract: Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.",
    "path": "papers/23/08/2308.10263.json",
    "total_tokens": 879,
    "translated_title": "深度自然语言处理模型中潜在概念的扩展发现",
    "translated_abstract": "尽管深度自然语言处理模型引起了一场革命，但它们仍然是黑匣子，需要研究来理解它们的决策过程。Dalvi等人（2022年）最近通过对预训练模型（PLMs）中的潜在空间进行聚类分析，开展了表示分析，但由于运行聚合层次聚类的高成本，该方法在小规模上受到限制。本文研究聚类算法，以便将PLM表示中编码的概念发现扩展到更大的数据集和模型上。我们提出了评估发现的潜在概念质量的指标，并使用这些指标来比较所研究的聚类算法。我们发现基于K-Means的概念发现在保持所获得概念质量的同时显著提高了效率。此外，我们通过将潜在概念发现扩展到LLMs和短语概念中，证明了这种新发现的效率的实用性。",
    "tldr": "本文研究了聚类算法，以将深度自然语言处理模型中编码的概念扩展到更大的数据集和模型上。我们发现基于K-Means的概念发现显著提高了效率并保持了质量。",
    "en_tdlr": "This paper studies clustering algorithms to scale up the discovery of encoded concepts in deep NLP models to larger datasets and models. K-Means-based concept discovery significantly enhances efficiency while maintaining quality."
}