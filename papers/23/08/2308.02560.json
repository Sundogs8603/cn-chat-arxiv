{
    "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion. (arXiv:2308.02560v1 [cs.SD])",
    "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation cod",
    "link": "http://arxiv.org/abs/2308.02560",
    "context": "Title: From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion. (arXiv:2308.02560v1 [cs.SD])\nAbstract: Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation cod",
    "path": "papers/23/08/2308.02560.json",
    "total_tokens": 851,
    "translated_title": "从离散标记到高保真音频：使用多频带扩散模型",
    "translated_abstract": "深度生成模型可以根据各种表示（如mel频谱、MFCC）生成高保真音频。最近，这样的模型已经被用于根据高度压缩的表示合成音频波形。尽管这些方法产生了令人印象深刻的结果，但是当条件不完美时，易于产生可听到的伪影。另一种建模方法是使用扩散模型。然而，这些模型主要用作语音模型（或基于mel频谱的条件模型）或生成相对较低采样率的信号。在这项工作中，我们提出了一个高保真多频带扩散模型，可以根据低比特率的离散表示生成任何类型的音频形式（如语音、音乐、环境声音）。在相同的比特率下，所提出的方法在感知质量上优于最先进的生成技术。",
    "tldr": "提出了一种高保真多频带扩散模型，可以根据低比特率的离散表示生成任何类型的音频形式，具有优于最先进生成技术的感知质量。",
    "en_tdlr": "A high-fidelity multi-band diffusion model is proposed to generate any type of audio modality from low-bitrate discrete representations, outperforming state-of-the-art generative techniques in terms of perceptual quality."
}