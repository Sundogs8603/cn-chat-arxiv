{
    "title": "An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training. (arXiv:2308.15602v1 [cs.DC])",
    "abstract": "Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs can exceed the capabilities of single machines or GPUs, making distributed GNN training a promising direction for large-scale GNN training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been extensively studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored.  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We ",
    "link": "http://arxiv.org/abs/2308.15602",
    "context": "Title: An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training. (arXiv:2308.15602v1 [cs.DC])\nAbstract: Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs can exceed the capabilities of single machines or GPUs, making distributed GNN training a promising direction for large-scale GNN training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been extensively studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored.  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We ",
    "path": "papers/23/08/2308.15602.json",
    "total_tokens": 858,
    "translated_title": "分布式图神经网络训练的分区策略的实验比较",
    "translated_abstract": "最近，图神经网络（GNNs）作为一种能够在图结构化数据上学习的深度学习领域，受到了广泛关注。然而，对于大规模图上的GNN训练，计算和内存要求可能超过单台机器或GPU的能力，因此分布式GNN训练成为大规模GNN训练的有前途的方向。分布式GNN训练的先决条件是将输入图分割成较小的部分，这些部分分布在计算集群的多台机器间。虽然图分区在图分析和图数据库方面已经得到了广泛研究，但其对GNN训练性能的影响尚未得到深入探索。在本文中，我们研究了分区对分布式GNN训练的效果。我们的研究旨在了解不同因素（如GNN参数、小批量大小、图类型、特征大小和扩展因子）对分区效果的影响。",
    "tldr": "本文研究了分布式图神经网络训练中分区策略的有效性，并探究了不同因素对分区效果的影响。",
    "en_tdlr": "This paper examines the effectiveness of partitioning strategies for distributed graph neural network training and investigates the impact of different factors on the partitioning outcome."
}