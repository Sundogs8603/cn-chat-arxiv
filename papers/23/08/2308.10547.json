{
    "title": "Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold",
    "abstract": "arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex",
    "link": "https://arxiv.org/abs/2308.10547",
    "context": "Title: Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold\nAbstract: arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex",
    "path": "papers/23/08/2308.10547.json",
    "total_tokens": 838,
    "translated_title": "基于Stiefel流形的分布式黎曼共轭梯度方法",
    "translated_abstract": "共轭梯度法是一种至关重要的一阶优化方法，通常比最速下降法收敛更快，计算成本也远低于二阶方法。然而，尽管在欧几里德空间和黎曼流形上已研究了各种类型的共轭梯度方法，但在分布式场景下的研究却很少。本文提出了一种旨在在Stiefel流形上最小化全局函数的分布式黎曼共轭梯度下降（DRCGD）方法。优化问题在一组代理网络中分布，每个代理与一个局部函数相关联，并且代理之间的通信在一个无向连通图上进行。由于Stiefel流形是一个非凸集，全局函数被表示为可能非凸（但平滑）局部函数的有限和。该方法不受最优非凸常数的限制。",
    "tldr": "该论文提出了一种在Stiefel流形上进行分布式优化的黎曼共轭梯度下降方法，克服了全局函数非凸的限制。",
    "en_tdlr": "This paper introduces a decentralized Riemannian conjugate gradient descent method on the Stiefel manifold for distributed optimization, overcoming the non-convexity of the global function."
}