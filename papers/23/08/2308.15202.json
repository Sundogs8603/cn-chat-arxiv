{
    "title": "Benchmarking the Generation of Fact Checking Explanations. (arXiv:2308.15202v1 [cs.CL])",
    "abstract": "Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of Fake News produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e. news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in ",
    "link": "http://arxiv.org/abs/2308.15202",
    "context": "Title: Benchmarking the Generation of Fact Checking Explanations. (arXiv:2308.15202v1 [cs.CL])\nAbstract: Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of Fake News produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e. news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in ",
    "path": "papers/23/08/2308.15202.json",
    "total_tokens": 897,
    "translated_title": "事实核查解释的基准化。 (arXiv:2308.15202v1 [cs.CL])",
    "translated_abstract": "打击错误信息是一项具有挑战性但至关重要的任务。尽管越来越多的专家参与手动事实核查，但这项活动耗时且无法跟得上每天产生的大量虚假新闻。因此，自动化此过程是必要的以帮助遏制错误信息。迄今为止，研究者主要关注声明真实度分类。而在本文中，我们着眼于解释生成（解释为什么声明被分类为真或假的文本解释）并与新颖数据集和先进基准进行了基准化。我们特别关注非结构化知识（即新闻文章）上的摘要方法，并尝试了几种提取和抽象策略。我们使用了两个具有不同风格和结构的数据集，以评估研究结果的普适性。结果表明，在解释生成摘要方面，由声明信息受益，并且某些抽象方法表现出色。",
    "tldr": "本论文基于新颖数据集和先进基准，旨在解决给出声明真实性解释的问题。研究结果表明，在使用非结构化知识的摘要方法中，命题信息是有益的，并且某些抽象策略表现出色。",
    "en_tdlr": "This paper addresses the generation of justifications for claim veracity classification, presenting novel datasets and advanced baselines. The study shows that using claim information is beneficial in summarization approaches over unstructured knowledge, and certain abstract strategies perform well."
}