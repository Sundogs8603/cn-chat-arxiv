{
    "title": "Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \\textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \\textit{simple} yet \\textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \\textbf{imperceptible, temporally distr",
    "link": "http://arxiv.org/abs/2308.11070",
    "context": "Title: Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \\textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \\textit{simple} yet \\textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \\textbf{imperceptible, temporally distr",
    "path": "papers/23/08/2308.11070.json",
    "total_tokens": 913,
    "translated_title": "基于时间分布的视频行为识别背门攻击",
    "translated_abstract": "深度神经网络在包括视频行为识别在内的各种应用中取得了巨大成功，但仍然容易受到背门攻击（特洛伊）。当测试实例（来自非目标类）嵌入特定触发器时，被背门破坏的模型会误分类为攻击者选择的目标类，同时在无攻击实例上保持高准确率。尽管对于图像数据的背门攻击已经进行了广泛研究，但视频系统在背门攻击下的易受攻击性仍然很少被探索。当前的研究是对图像数据的方法的直接延伸，例如，触发器是\\textbf{独立}嵌入帧中的，容易被现有防御机制检测到。在本文中，我们介绍了一种\\textit{简单}但\\textit{有效}的视频数据背门攻击。我们提出的攻击在一个转换的领域中添加扰动，以嵌入\\textbf{难以察觉的，时间上分布的}触发器。",
    "tldr": "本文介绍了一种针对视频数据的简单而有效的背门攻击方法，通过在转换领域中添加难以察觉的、时间上分布的触发器来实现误分类。",
    "en_tdlr": "This paper introduces a simple yet effective backdoor attack method against video data, achieving misclassification by adding imperceptible, temporally distributed triggers in a transformed domain."
}