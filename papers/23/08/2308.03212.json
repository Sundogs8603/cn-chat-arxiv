{
    "title": "Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)",
    "abstract": "Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.",
    "link": "http://arxiv.org/abs/2308.03212",
    "context": "Title: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)\nAbstract: Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.",
    "path": "papers/23/08/2308.03212.json",
    "total_tokens": 827,
    "translated_title": "平均困难注意力变换器是常深度均匀阈值电路",
    "translated_abstract": "转换器已成为各种自然语言处理任务中广泛使用的神经网络模型。先前的研究探索了它们与常深度阈值电路的关系，做出了两个假设：平均困难注意力和相对于输入长度的对数精度的内部计算。Merrill等人证明了平均困难注意力变换器可以识别属于复杂性类别TC0的语言，该类别表示可以由常深度多项式大小的阈值电路识别的语言集合。同样地，Merrill和Sabharwal证明了对数精度的变换器可以识别统一的TC0类语言。这表明这两种转换器模型都可以通过常深度阈值电路来模拟，而后者由于生成统一的电路族而更健壮。我们的论文表明，第一个结果也可以延伸到产生统一电路。",
    "tldr": "本文研究表明平均困难注意力变换器和对数精度变换器都可以模拟常深度阈值电路，其中后者由于生成统一的电路族而更健壮。",
    "en_tdlr": "This paper shows that both average-hard attention transformers and log-precision transformers can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family."
}