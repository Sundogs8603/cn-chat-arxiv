{
    "title": "Knowledge Prompt-tuning for Sequential Recommendation. (arXiv:2308.08459v1 [cs.IR])",
    "abstract": "Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (\\textbf{KP4SR}). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tre",
    "link": "http://arxiv.org/abs/2308.08459",
    "context": "Title: Knowledge Prompt-tuning for Sequential Recommendation. (arXiv:2308.08459v1 [cs.IR])\nAbstract: Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (\\textbf{KP4SR}). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tre",
    "path": "papers/23/08/2308.08459.json",
    "total_tokens": 887,
    "translated_title": "知识提示调优的顺序推荐",
    "translated_abstract": "预训练语言模型(PLMs)在顺序推荐(SR)中展示出强大的性能，用于提取通用知识。然而，现有方法仍然缺乏领域知识，并且很难捕捉用户的细粒度偏好。同时，许多传统的SR方法通过整合辅助信息来改善这个问题，但却遭受信息损失的困扰。总而言之，我们认为一个好的推荐系统应该同时利用通用知识和领域知识。因此，我们引入了一个外部知识库，并提出了知识提示调优的顺序推荐(KP4SR)。具体来说，我们构建了一组关系模板，并将结构化知识图谱(KG)转化为知识提示，以解决语义差距的问题。然而，知识提示破坏了原始数据结构并引入了大量的噪音。我们进一步构建了一个知识树，并提出了一个知识树的方法来减少噪音并提高推荐性能。",
    "tldr": "该论文提出了知识提示调优的顺序推荐(KP4SR)方法，通过引入外部知识库和构建知识提示，解决了顺序推荐中的语义差距和信息损失问题，从而提高了推荐性能。",
    "en_tdlr": "This paper proposes a Knowledge Prompt-tuning for Sequential Recommendation (KP4SR) method, which addresses the issues of semantic gap and information loss in sequential recommendation by introducing an external knowledge base and constructing knowledge prompts, leading to improved recommendation performance."
}