{
    "title": "Cyclophobic Reinforcement Learning. (arXiv:2308.15911v1 [cs.LG])",
    "abstract": "In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforc",
    "link": "http://arxiv.org/abs/2308.15911",
    "context": "Title: Cyclophobic Reinforcement Learning. (arXiv:2308.15911v1 [cs.LG])\nAbstract: In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforc",
    "path": "papers/23/08/2308.15911.json",
    "total_tokens": 913,
    "translated_title": "周期性排斥学习",
    "translated_abstract": "在奖励稀疏的环境中，寻找良好的归纳偏差以促进探索对于智能体的成功至关重要。然而，存在着两个竞争的目标：新颖性搜索和系统性探索。现有的方法，如好奇心驱动的探索，可以找到新颖性，但有时不会对整个状态空间进行系统性的探索，类似于深度优先搜索与广度优先搜索。在本文中，我们提出了一种新的内在奖励，即周期性排斥型，它不会奖励新颖性，而是通过避免循环来惩罚冗余。通过将周期性排斥的内在奖励与基于智能体裁剪观测的分层表示序列相结合，我们在MiniGrid和MiniHack环境中取得了极好的结果。这两个环境都特别难以解决，因为它们需要与不同对象进行复杂的交互才能解决。与先前方法的详细比较和彻底的消融研究表明，我们新提出的周期性排斥增强学习方法在这些环境中表现出色。",
    "tldr": "本文提出一种新的周期性排斥增强学习方法，通过避免循环来惩罚冗余而不奖励新颖性，在奖励稀疏的环境中能取得优异的结果。",
    "en_tdlr": "This paper proposes a novel cyclophobic reinforcement learning approach that punishes redundancy by avoiding cycles instead of rewarding novelty, achieving excellent results in environments with sparse rewards."
}