{
    "title": "Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])",
    "abstract": "In the rapidly evolving digital era, the analysis of document layouts plays a pivotal role in automated information extraction and interpretation. In our work, we have trained MViTv2 transformer model architecture with cascaded mask R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from a document. After training on 20365 document images for 36 epochs in a 3 phase cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work extends beyond training, delving into the exploration of potential enhancement avenues. We investigate the impact of rotation and flip augmentation, the effectiveness of slicing input images pre-inference, the implications of varying the resolution of the transformer backbone, and the potential of employing a dual-pass inference to uncover missed text-boxes. Through these explorations, we observe a spectrum of outcomes, where some modifications result in tangible performance improvements, while others offer unique insights ",
    "link": "http://arxiv.org/abs/2308.16571",
    "context": "Title: Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])\nAbstract: In the rapidly evolving digital era, the analysis of document layouts plays a pivotal role in automated information extraction and interpretation. In our work, we have trained MViTv2 transformer model architecture with cascaded mask R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from a document. After training on 20365 document images for 36 epochs in a 3 phase cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work extends beyond training, delving into the exploration of potential enhancement avenues. We investigate the impact of rotation and flip augmentation, the effectiveness of slicing input images pre-inference, the implications of varying the resolution of the transformer backbone, and the potential of employing a dual-pass inference to uncover missed text-boxes. Through these explorations, we observe a spectrum of outcomes, where some modifications result in tangible performance improvements, while others offer unique insights ",
    "path": "papers/23/08/2308.16571.json",
    "total_tokens": 919,
    "translated_title": "在BaDLAD数据集上的文档布局分析：一种基于MViTv2的综合方法",
    "translated_abstract": "在迅速发展的数字时代，文档布局分析在自动化信息提取和解释中起到关键作用。在我们的工作中，我们使用级联掩码R-CNN训练了MViTv2 transformer模型架构，从文档中提取文本框、段落、图片和表格。在一个3阶段循环中，我们在20365个文档图像上进行了36个周期的训练，实现了0.2125的训练损失和0.19的掩码损失。我们的工作不仅限于训练，还深入探讨了潜在的增强途径。我们调查了旋转和翻转增强的影响，切片输入图像预推论的有效性，变化的变换器骨干分辨率的影响，以及采用双通推论来发现漏掉的文本框的潜力。通过这些探索，我们观察到了一系列结果，一些修改导致了有形的性能改进，而其他修改则提供了独特的见解。",
    "tldr": "我们使用MViTv2模型在BaDLAD数据集上进行文档布局分析，实现了文本框、段落、图片和表格的自动提取。我们还探索了增强方法，并发现了一些有效的性能改进。",
    "en_tdlr": "We conducted document layout analysis on the BaDLAD dataset using the MViTv2 transformer model, achieving automated extraction of text boxes, paragraphs, images, and tables. We also explored enhancement avenues and discovered effective performance improvements."
}