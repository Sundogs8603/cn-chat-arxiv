{
    "title": "ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging. (arXiv:2308.02870v1 [cs.CL])",
    "abstract": "The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and",
    "link": "http://arxiv.org/abs/2308.02870",
    "context": "Title: ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging. (arXiv:2308.02870v1 [cs.CL])\nAbstract: The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and",
    "path": "papers/23/08/2308.02870.json",
    "total_tokens": 1006,
    "translated_title": "ApproBiVT: 使用近似的偏差-方差权衡来引导ASR模型更好地泛化的早停和检查点平均化",
    "translated_abstract": "传统的自动语音识别（ASR）模型训练通常是在训练集上训练多个检查点，同时依赖于验证集通过早停来防止过拟合，并使用多个最后几个检查点或最低验证损失的平均值得到最终模型。本文从偏差-方差的角度重新思考和更新了早停和检查点平均化的方法。理论上，偏差和方差分别表示模型的拟合和变异性，它们的权衡决定了整体泛化误差。但是，精确评估它们是不切实际的。作为一种替代，我们将训练损失和验证损失视为偏差和方差的代理，并使用它们的权衡，即近似的偏差-方差权衡（ApproBiVT），来引导早停和检查点平均化。在使用先进的ASR模型进行评估时，我们的方法可以在AISHELL-1和...",
    "tldr": "本文通过重新思考和更新早停和检查点平均化的方法，从偏差-方差的角度引导ASR模型更好地泛化，利用近似的偏差-方差权衡来指导早停和检查点平均化，在实验中证明可以显著降低CER。",
    "en_tdlr": "This paper guides Automatic Speech Recognition (ASR) models to generalize better by rethinking and updating the methods of early stopping and checkpoint averaging from the perspective of bias-variance tradeoff. By using an Approximated Bias-Variance Tradeoff (ApproBiVT) based on training loss and validation loss, significant reduction in CER is achieved according to experiments."
}