{
    "title": "Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])",
    "abstract": "We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect",
    "link": "http://arxiv.org/abs/2308.02599",
    "context": "Title: Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])\nAbstract: We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect",
    "path": "papers/23/08/2308.02599.json",
    "total_tokens": 916,
    "translated_title": "分支潜在神经算子",
    "translated_abstract": "我们介绍了分支潜在神经算子（BLNOs）来学习编码复杂物理过程的输入-输出映射。BLNO由一个简单紧凑的前馈部分连接神经网络定义，该网络在结构上将不同固有角色的输入进行解离，例如将微分方程的时间变量与模型参数分离，并将它们转化为感兴趣的通用领域。BLNO利用可解释的潜在输出增强了学习到的动态，并通过在单个处理器上使用小的训练数据集和短的训练时间展示了出色的泛化性能。实际上，它们的泛化误差在测试阶段采用的离散化方式相同的情况下保持可比性。此外，部分连接在全连接结构的基础上显著减少了可调参数的数量。我们展示了BLNO在涉及生物物理细节的具有挑战性的测试案例中的能力。",
    "tldr": "Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.",
    "en_tdlr": "Branched Latent Neural Operators (BLNOs) are introduced as a method to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are effective in a challenging biophysically detailed test case."
}