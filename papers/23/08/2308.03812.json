{
    "title": "Noncompact uniform universal approximation. (arXiv:2308.03812v1 [cs.LG])",
    "abstract": "The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\\varphi\\neq0$ with asymptotically linear behaviour at $\\pm\\infty$. When $\\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\\geq2$, $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\\varphi$ differs from its right limit (for instance, when $\\varphi$ is sigmoidal) the algebra $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ ($l\\geq2$) ",
    "link": "http://arxiv.org/abs/2308.03812",
    "context": "Title: Noncompact uniform universal approximation. (arXiv:2308.03812v1 [cs.LG])\nAbstract: The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\\varphi\\neq0$ with asymptotically linear behaviour at $\\pm\\infty$. When $\\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\\geq2$, $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\\varphi$ differs from its right limit (for instance, when $\\varphi$ is sigmoidal) the algebra $\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}$ ($l\\geq2$) ",
    "path": "papers/23/08/2308.03812.json",
    "total_tokens": 986,
    "translated_title": "非紧致统一逼近",
    "translated_abstract": "将通用逼近定理推广到在（非紧致）输入空间 \\(\\mathbb R^n\\) 上的一致收敛。所有在无穷远处为零的连续函数都可以用具有一个隐藏层的神经网络进行一致逼近，对于所有具有渐近线性行为的连续激活函数 \\(\\varphi\\neq0\\)。当 \\(\\varphi\\) 还被限制在有界时，我们准确确定了哪些函数可以通过神经网络进行一致逼近，得到了以下意想不到的结果。让 \\(\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}\\) 表示可以通过具有 \\(l\\) 个隐藏层和 \\(n\\) 个输入的神经网络进行一致逼近的函数的向量空间。对于所有的 \\(n\\) 和所有的 \\(l\\geq2\\)，\\(\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}\\) 在逐点乘积下是一个代数。如果 \\(\\varphi\\) 的左极限不等于其右极限（例如，当 \\(\\varphi\\) 是sigmoid函数时），代数 \\(\\overline{\\mathcal{N}_\\varphi^l(\\mathbb R^n)}\\)（\\(l\\geq2\\)）会产生一些意外的结果。",
    "tldr": "这篇论文将通用逼近定理推广到非紧致输入空间，并确定了在有界激活函数条件下可以通过神经网络一致逼近的函数类别，并提出了代数结构的意外结果。"
}