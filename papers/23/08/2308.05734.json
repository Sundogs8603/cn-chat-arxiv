{
    "title": "AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])",
    "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE",
    "link": "http://arxiv.org/abs/2308.05734",
    "context": "Title: AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])\nAbstract: Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE",
    "path": "papers/23/08/2308.05734.json",
    "total_tokens": 929,
    "translated_title": "AudioLDM 2: 利用自监督预训练学习进行整体音频生成的方法",
    "translated_abstract": "虽然音频生成在不同类型的音频中共享一些共性，比如语音、音乐和音效，但为每种类型设计模型需要仔细考虑特定的目标和偏差，这些偏差可能与其他类型的目标有显著的差异。为了更好地实现音频生成的统一视角，本文提出了一种利用相同的学习方法进行语音、音乐和音效生成的框架。我们的框架引入了一种称为“语言音频（LOA）”的音频通用表示。任何音频都可以基于自监督预训练学习模型AudioMAE转换为LOA。在生成过程中，我们使用GPT-2模型将任何形式的音频转换为LOA，并利用以LOA为条件的潜在扩散模型进行自监督音频生成学习。所提出的框架自然地带来了诸如上下文学习能力和可重用的自监督预训练AudioMAE的优势。",
    "tldr": "本文提出了一种利用自监督预训练学习方法进行语音、音乐和音效生成的框架，通过引入通用音频表示LOA，将任何音频转换为LOA，并利用以LOA为条件的潜在扩散模型进行自监督音频生成学习。",
    "en_tdlr": "This paper proposes a framework for speech, music, and sound effect generation using self-supervised pretraining. By introducing a general audio representation called LOA, any audio can be translated into LOA and used for self-supervised audio generation learning with a latent diffusion model conditioned on LOA."
}