{
    "title": "From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])",
    "abstract": "Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r",
    "link": "http://arxiv.org/abs/2308.15457",
    "context": "Title: From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])\nAbstract: Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r",
    "path": "papers/23/08/2308.15457.json",
    "total_tokens": 1092,
    "translated_title": "从SMOTE到Mixup用于深度不平衡分类",
    "translated_abstract": "鉴于不平衡的数据，使用深度学习训练好的分类器因为少数类的泛化能力差而困难重重。传统上，用于数据增强的知名少数类合成过采样技术（SMOTE），作为一种面向不平衡学习的数据挖掘方法，被用来改善这种泛化。然而，SMOTE在深度学习中是否也有益处仍不清楚。在这项工作中，我们研究了为什么原始的SMOTE对深度学习来说是不足的，并使用软标签增强了SMOTE。将得到的软SMOTE与Mixup，一种现代数据增强技术，连接在一起，形成了一个统一的框架，将传统和现代的数据增强技术纳入同一个范畴。在这个框架中进行系统研究表明，Mixup通过隐式地实现多数类和少数类之间的不平衡间隙来改善泛化能力。然后，我们提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。大量的实验表明，我们的方法在多个数据集上相对于最先进的算法都取得了最好的性能。",
    "tldr": "本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。",
    "en_tdlr": "This study proposes a method from SMOTE to Mixup for deep imbalanced classification. By enhancing SMOTE and combining it with Mixup, a unified data augmentation framework is built. The study shows that Mixup improves generalization by achieving uneven margins between majority and minority classes. A novel margin-aware Mixup technique is also proposed to explicitly achieve this uneven margin. Experimental results demonstrate the superior performance of the proposed method on multiple datasets."
}