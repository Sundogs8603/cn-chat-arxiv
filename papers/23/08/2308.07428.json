{
    "title": "UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. (arXiv:2308.07428v1 [cs.CV])",
    "abstract": "Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both ",
    "link": "http://arxiv.org/abs/2308.07428",
    "context": "Title: UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. (arXiv:2308.07428v1 [cs.CV])\nAbstract: Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both ",
    "path": "papers/23/08/2308.07428.json",
    "total_tokens": 848,
    "translated_title": "UniBrain: 统一图像重建和标题生成于一体的人脑活动扩散模型",
    "translated_abstract": "通过对视觉刺激引起的脑活动进行图像重建和标题生成，研究人员可以进一步理解人脑与视觉感知系统之间的联系。尽管最近在该领域中使用了深度生成模型，但是在保持低层细节和高语义保真度的情况下重建逼真的标题和图像仍然是一个具有挑战性的问题。在这项工作中，我们提出了UniBrain：通过人脑活动的统一图像重建和标题生成在一个扩散模型中。这是第一次通过名为Versatile Diffusion的潜在扩散模型将图像重建和标题生成从视觉诱导功能磁共振成像（fMRI）统一起来。具体而言，我们将fMRI体素转换为文本和图像潜变量，用于低层信息，并通过基于fMRI的图像和文本条件从CLIP导引反向扩散过程，生成逼真的标题和图像。UniBrain在性能上超越了当前的方法",
    "tldr": "UniBrain 是一个统一的图像重建和标题生成模型，通过使用名为Versatile Diffusion的潜变量扩散模型，结合fMRI的图像和文本条件，实现了从人脑活动中生成逼真的图像和标题。"
}