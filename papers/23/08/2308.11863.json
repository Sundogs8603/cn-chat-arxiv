{
    "title": "KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])",
    "abstract": "Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art",
    "link": "http://arxiv.org/abs/2308.11863",
    "context": "Title: KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])\nAbstract: Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art",
    "path": "papers/23/08/2308.11863.json",
    "total_tokens": 984,
    "translated_title": "KinSPEAK: 通过半监督学习方法提高基尼亚兰达语语音识别的准确性",
    "translated_abstract": "尽管最近具备了大规模记录的基尼亚兰达语语音数据，但是实现基尼亚兰达语的强大语音识别仍然具有挑战性。本研究表明，使用自监督预训练，遵循简单的课程进度进行微调，以及使用半监督学习来利用大规模无标签语音数据，显著提高了基尼亚兰达语的语音识别性能。我们的方法仅关注使用公共领域数据。我们从公共网站收集了一个新的制作室级别的语音数据集，然后使用该数据集训练一个干净的基准模型。然后，使用该干净的基准模型对来自更多多样和嘈杂的公共数据集的样本进行排序，定义一个简单的课程训练进度。最后，我们将半监督学习应用于连续四代对大规模无标签数据进行标记和学习。我们的最终模型在新数据集上实现了3.2％的字错误率（WER），在Mozilla Common Voice基准测试中实现了15.9％的WER，达到了最先进的水平。",
    "tldr": "通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。",
    "en_tdlr": "By utilizing self-supervised pre-training, curriculum schedule fine-tuning, and semi-supervised learning with unlabelled speech data, this paper proposes a method to improve speech recognition for Kinyarwanda, achieving state-of-the-art results."
}