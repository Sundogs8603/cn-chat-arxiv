{
    "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
    "abstract": "arXiv:2308.03449v2 Announce Type: replace  Abstract: Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose K-prune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. K-prune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, K-prune shows significant accuracy improv",
    "link": "https://arxiv.org/abs/2308.03449",
    "context": "Title: Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models\nAbstract: arXiv:2308.03449v2 Announce Type: replace  Abstract: Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose K-prune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. K-prune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, K-prune shows significant accuracy improv",
    "path": "papers/23/08/2308.03449.json",
    "total_tokens": 929,
    "translated_title": "针对预训练编码器型语言模型的精确无需重训练剪枝方法",
    "translated_abstract": "给定一个预训练的编码器型语言模型，我们如何在不重新训练的情况下对其进行精确压缩？对于预训练语言模型的压缩来说，无需重训练的结构化剪枝算法非常关键，因为它们能够显著降低剪枝成本并能够剪枝大型语言模型。然而，现有的无需重训练算法存在严重的精度下降问题，因为它们未能处理剪枝错误，尤其在高压缩率情况下。本文提出了K-prune（知识保留剪枝），这是一种针对预训练编码器型语言模型的精确无需重训练的结构化剪枝算法。K-prune专注于保留预训练模型的有用知识，通过精心设计的迭代剪枝过程（包括知识测量、知识保留蒙版搜索和知识保留权重调整），以最小化剪枝错误。因此，K-prune显示出显著的精确度提升。",
    "tldr": "K-prune 是一种针对预训练编码器型语言模型的精确无需重训练的结构化剪枝算法，通过迭代剪枝过程保留有用知识以最小化剪枝错误，显著提升精确度。"
}