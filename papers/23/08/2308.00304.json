{
    "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])",
    "abstract": "We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b",
    "link": "http://arxiv.org/abs/2308.00304",
    "context": "Title: Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])\nAbstract: We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b",
    "path": "papers/23/08/2308.00304.json",
    "total_tokens": 954,
    "translated_title": "在大型语言模型中解锁组合性的上下文提示: 技能指导策略",
    "translated_abstract": "本文考虑了如何通过一种新颖的提示策略，在大型语言模型（LLMs）中激发组合性泛化能力的问题。组合性泛化使得LLMs能够解决比它们所见过的问题更困难的问题（即易于难的泛化），这是人类智能的关键推理能力。然而，即使是当前最先进的LLMs在这种形式的推理上仍然存在困难。为了弥合这一差距，我们提出了在上下文中的技能指导（SKiC）提示，它指导LLMs如何组合基本技能来解决更复杂的问题。我们发现，在相同的提示上展示技能和组合性示例是至关重要的。仅仅通过两个示例，我们的SKiC提示在技能和它们的组合能力之间形成了强大的协同效应。值得注意的是，它赋予了LLMs解决需要创新技能组合的未见问题的能力，实现了几乎完美的泛化。",
    "tldr": "本文介绍了一种名为技能指导 (SKiC) 的提示策略，通过在上下文中演示基本技能和组合性示例，使大型语言模型具备解决更复杂问题的能力，并在泛化能力上取得几乎完美的表现。",
    "en_tdlr": "This paper introduces a novel prompting strategy called Skills-in-Context (SKiC), which enables large language models (LLMs) to solve more complex problems by demonstrating basic skills and compositional examples within the same context. It achieves near-perfect generalization on unseen problems that require innovative skill compositions."
}