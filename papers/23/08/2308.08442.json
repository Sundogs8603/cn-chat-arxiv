{
    "title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction. (arXiv:2308.08442v1 [cs.CL])",
    "abstract": "Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.",
    "link": "http://arxiv.org/abs/2308.08442",
    "context": "Title: Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction. (arXiv:2308.08442v1 [cs.CL])\nAbstract: Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.",
    "path": "papers/23/08/2308.08442.json",
    "total_tokens": 879,
    "translated_title": "减轻句子级音素转换中的曝光偏差",
    "translated_abstract": "文本转文本传输转换器 (T5) 最近被考虑用于音素图( G2P )转换。作为后续研究，一种基于 T5 的无分词字节级模型 ByT5，在表示每个输入字符时使用其相应的 UTF-8 编码，最近在单词级 G2P 转换方面取得了有希望的结果。虽然人们普遍认为句子级或段落级 G2P 在实际应用中能提高可用性，因为它更适合处理异音字和单词之间的连接音，但我们发现在这些情况下使用 ByT5 并不简单。由于 ByT5 在字符级别上操作，它需要较长的解码步骤，这会因自回归生成模型中常见的曝光偏差而导致性能下降。本文通过采用我们提出的基于损失的抽样方法，展示了通过减轻这种曝光偏差可以提高句子级和段落级 G2P 的性能。",
    "tldr": "本论文提出了一种用于减轻曝光偏差的损失抽样方法，以改善句子级和段落级音素转换的性能。"
}