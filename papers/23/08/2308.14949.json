{
    "title": "Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation. (arXiv:2308.14949v1 [cs.LG])",
    "abstract": "Graph Neural Network (GNN) training and inference involve significant challenges of scalability with respect to both model sizes and number of layers, resulting in degradation of efficiency and accuracy for large and deep GNNs. We present an end-to-end solution that aims to address these challenges for efficient GNNs in resource constrained environments while avoiding the oversmoothing problem in deep GNNs. We introduce a quantization based approach for all stages of GNNs, from message passing in training to node classification, compressing the model and enabling efficient processing. The proposed GNN quantizer learns quantization ranges and reduces the model size with comparable accuracy even under low-bit quantization. To scale with the number of layers, we devise a message propagation mechanism in training that controls layer-wise changes of similarities between neighboring nodes. This objective is incorporated into a Lagrangian function with constraints and a differential multiplie",
    "link": "http://arxiv.org/abs/2308.14949",
    "context": "Title: Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation. (arXiv:2308.14949v1 [cs.LG])\nAbstract: Graph Neural Network (GNN) training and inference involve significant challenges of scalability with respect to both model sizes and number of layers, resulting in degradation of efficiency and accuracy for large and deep GNNs. We present an end-to-end solution that aims to address these challenges for efficient GNNs in resource constrained environments while avoiding the oversmoothing problem in deep GNNs. We introduce a quantization based approach for all stages of GNNs, from message passing in training to node classification, compressing the model and enabling efficient processing. The proposed GNN quantizer learns quantization ranges and reduces the model size with comparable accuracy even under low-bit quantization. To scale with the number of layers, we devise a message propagation mechanism in training that controls layer-wise changes of similarities between neighboring nodes. This objective is incorporated into a Lagrangian function with constraints and a differential multiplie",
    "path": "papers/23/08/2308.14949.json",
    "total_tokens": 977,
    "translated_title": "带有平滑感知的深度图神经网络的低比特量化",
    "translated_abstract": "图神经网络（GNN）的训练和推理面临着模型规模和层数增加导致效率和准确性降低的挑战，特别是在资源受限环境下的大规模和深层GNN中更为明显。我们提出了一个端到端的解决方案，旨在解决这些挑战，以实现在资源受限环境下高效的GNN，并避免深层GNN中的过度平滑问题。我们引入了一种基于量化的方法，用于GNN的各个阶段，从训练中的消息传递到节点分类，压缩模型并实现高效处理。所提出的GNN量化器学习量化范围，即使在低比特量化下也能减小模型大小并获得可比的准确性。为了随着层数的增加而扩展，我们设计了一种训练中的消息传播机制，控制相邻节点之间相似性的逐层变化。通过将这个目标纳入带约束的Lagrangian函数和差分多元最优化问题中实现。",
    "tldr": "本文提出了一种带有平滑感知的深度图神经网络的低比特量化解决方案，用于解决大规模和深层GNN中的效率和准确性问题。该方法通过压缩模型、学习量化范围和控制层间相似性的变化来实现高效处理和避免过度平滑问题。"
}