{
    "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)",
    "abstract": "Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a r",
    "link": "http://arxiv.org/abs/2308.03549",
    "context": "Title: Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)\nAbstract: Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a r",
    "path": "papers/23/08/2308.03549.json",
    "total_tokens": 967,
    "translated_title": "Zhongjing: 通过专家反馈和真实的多轮对话增强大型语言模型的中医能力",
    "translated_abstract": "最近大型语言模型（LLM）的进展在理解和回应用户意图方面取得了显著突破。然而，在某些专业领域（如中医学）中，它们在常规使用中的表现仍然落后。目前将中医纳入LLM的方法主要依赖于使用单轮和精简对话数据进行监督微调（SFT）。这些模型缺乏医生一样的主动询问和多轮理解的能力，并且不能始终与专家的安全和专业性对齐回复。本研究中，我们介绍了中医基于LLaMA的中文医学LLM——中精，它实现了从预训练到强化学习的整个训练流程，并利用人类反馈（RLHF）。此外，我们还介绍了一个包含70,000个真实医患对话的中文多轮医学对话数据集CMtMedQA，它极大地增强了模型在复杂对话和主动询问发起方面的能力。",
    "tldr": "本研究介绍了中精，这是一种基于LLaMA的中文医学LLM，通过整个训练流程，并结合人类反馈，以及引入一个中文多轮医学对话数据集CMtMedQA，大大提升了模型在复杂对话和主动询问发起方面的能力。",
    "en_tdlr": "This paper presents Zhongjing, a Chinese medical LLaMA-based LLM, which incorporates a complete training pipeline and human feedback, as well as introduces a Chinese multi-turn medical dialogue dataset CMtMedQA, significantly enhancing the model's capability in complex dialogue and proactive inquiry initiation."
}