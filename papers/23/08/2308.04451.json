{
    "title": "Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])",
    "abstract": "In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.",
    "link": "http://arxiv.org/abs/2308.04451",
    "context": "Title: Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])\nAbstract: In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.",
    "path": "papers/23/08/2308.04451.json",
    "total_tokens": 732,
    "translated_title": "AI代码生成器的漏洞：探索针对数据毒化攻击的方法",
    "translated_abstract": "在这项工作中，我们通过数据毒化评估AI代码生成器的安全性，即通过将恶意样本注入训练数据来生成易受攻击的代码。我们通过注入包含安全漏洞的代码来毒化训练数据，并评估不同最先进的代码生成模型对攻击的成功程度。我们的分析表明，即使只有少量的数据毒化，AI代码生成器也容易受到攻击。此外，该攻击不会影响预训练模型生成的代码的正确性，使其难以检测。",
    "tldr": "本文评估了AI代码生成器的安全性，发现它们容易受到数据毒化攻击，即注入恶意样本来生成易受攻击的代码。攻击可以成功即使只有少量数据被毒化，而且不影响预训练模型生成的代码的正确性，使其难以被检测。",
    "en_tdlr": "This paper assesses the security of AI code generators and finds that they are susceptible to data poisoning attacks, where injecting malicious samples leads to the generation of vulnerable code. The attacks can succeed with even a small amount of poisoned data, and they do not impact the correctness of code generated by pre-trained models, making them hard to detect."
}