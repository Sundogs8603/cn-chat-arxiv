{
    "title": "ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])",
    "abstract": "Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even w",
    "link": "http://arxiv.org/abs/2308.03908",
    "context": "Title: ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])\nAbstract: Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even w",
    "path": "papers/23/08/2308.03908.json",
    "total_tokens": 948,
    "translated_title": "ViLP：使用视觉、语言和姿势嵌入进行视频动作识别的知识探索",
    "translated_abstract": "视频动作识别是一项具有挑战性的任务，由于其固有的复杂性。尽管文献中已经探索了不同的方法，但设计一个统一的框架来识别大量的人类动作仍然是一个具有挑战性的问题。最近，在这个领域中，多模态学习(MML)显示出了令人期待的结果。在文献中，2D骨骼或姿势模态经常被用于这个任务，要么独立使用，要么与视频中存在的视觉信息（RGB模态）结合使用。然而，尽管文本和姿势属性在许多计算机视觉任务中都被证明是有效的，但是尚未探索过姿势、视觉信息和文本属性的组合。在本文中，我们提出了第一个用于VAR的姿势增强的视觉语言模型(VLM)。值得注意的是，我们的方案在两个流行的人类视频动作识别基准数据集UCF-101和HMDB-51上分别达到了92.81%和73.02%的准确率。",
    "tldr": "本论文提出了一种基于姿势增强的视觉语言模型(VLM)，用于视频动作识别。实验结果表明，该模型能够取得令人期待的准确率，并在两个流行的动作识别数据集上取得了优异的性能。",
    "en_tdlr": "This paper proposes a pose augmented Vision-language model (VLM) for video action recognition, achieving promising accuracy and excellent performance on popular benchmark datasets."
}