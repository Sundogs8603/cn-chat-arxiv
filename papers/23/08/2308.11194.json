{
    "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data. (arXiv:2308.11194v1 [cs.CV])",
    "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up t",
    "link": "http://arxiv.org/abs/2308.11194",
    "context": "Title: ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data. (arXiv:2308.11194v1 [cs.CV])\nAbstract: Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up t",
    "path": "papers/23/08/2308.11194.json",
    "total_tokens": 890,
    "translated_title": "ViLLA:从真实世界数据中进行细粒度的视觉-语言表示学习",
    "translated_abstract": "视觉-语言模型（VLMs），如CLIP和ALIGN，通常是在从网络获取的图像-标题对的数据集上进行训练的。然而，真实世界的多模态数据集，如医疗数据，显著更加复杂：每个图像（如X光）通常与描述图像细粒度区域中发生的许多不同属性的文本（如医生报告）配对。我们将这些样本称为展示高配对复杂性，因为每个图像-文本对可以分解为大量的区域-属性配对。以往尚未评估VLMs在训练这种数据时能否捕捉到图像区域与文本属性之间的细粒度关系。此工作的第一个关键贡献是通过系统评估表明，随着训练数据集的配对复杂性增加，标准的VLMs在学习区域-属性关系方面面临困难，性能下降达到",
    "tldr": "本论文通过系统评估表明，当训练集的配对复杂性增加时，标准的视觉-语言模型在学习图像区域与文本属性之间的关系时表现较差，性能下降达到",
    "en_tdlr": "This paper demonstrates through systematic evaluations that standard vision-language models struggle to learn relationships between image regions and textual attributes, exhibiting performance degradations as the complexity of the training dataset increases."
}