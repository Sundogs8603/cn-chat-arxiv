{
    "title": "Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness. (arXiv:2308.13242v1 [cs.LG])",
    "abstract": "In learning-to-rank (LTR), optimizing only the relevance (or the expected ranking utility) can cause representational harm to certain categories of items. Moreover, if there is implicit bias in the relevance scores, LTR models may fail to optimize for true relevance. Previous works have proposed efficient algorithms to train stochastic ranking models that achieve fairness of exposure to the groups ex-ante (or, in expectation), which may not guarantee representation fairness to the groups ex-post, that is, after realizing a ranking from the stochastic ranking model. Typically, ex-post fairness is achieved by post-processing, but previous work does not train stochastic ranking models that are aware of this post-processing.  In this paper, we propose a novel objective that maximizes expected relevance only over those rankings that satisfy given representation constraints to ensure ex-post fairness. Building upon recent work on an efficient sampler for ex-post group-fair rankings, we propo",
    "link": "http://arxiv.org/abs/2308.13242",
    "context": "Title: Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness. (arXiv:2308.13242v1 [cs.LG])\nAbstract: In learning-to-rank (LTR), optimizing only the relevance (or the expected ranking utility) can cause representational harm to certain categories of items. Moreover, if there is implicit bias in the relevance scores, LTR models may fail to optimize for true relevance. Previous works have proposed efficient algorithms to train stochastic ranking models that achieve fairness of exposure to the groups ex-ante (or, in expectation), which may not guarantee representation fairness to the groups ex-post, that is, after realizing a ranking from the stochastic ranking model. Typically, ex-post fairness is achieved by post-processing, but previous work does not train stochastic ranking models that are aware of this post-processing.  In this paper, we propose a novel objective that maximizes expected relevance only over those rankings that satisfy given representation constraints to ensure ex-post fairness. Building upon recent work on an efficient sampler for ex-post group-fair rankings, we propo",
    "path": "papers/23/08/2308.13242.json",
    "total_tokens": 955,
    "translated_title": "优化关于相关性和后期公平性的群组公平Plackett-Luce排序模型",
    "translated_abstract": "在学习排名中，仅优化相关性（或预期排名效用）可能对某些类别的项目造成表现性损害。此外，如果相关性分数中存在隐性偏见，则学习排名模型可能无法优化真实的相关性。以前的研究提出了有效的算法来训练随机排名模型，以达到群组预期暴露的公平性（即期望值），但可能无法保证群组后期的表现公平性，即在从随机排序模型中实现排名之后。通常，通过后期处理实现后期公平性，但是以前的工作不训练意识到此后期处理的随机排序模型。在本文中，我们提出了一种新颖的目标函数，仅在满足给定表示约束的排名中最大化预期相关性，以确保后期公平性。基于最近关于后期群组公平排名的有效抽样器的工作，我们提出了一个新颖的目标函数，仅在满足给定表示约束的排名中最大化预期相关性，以确保后期公正性。建立在一个高效的抽样器的基础上，我们提出了一种新的目标函数，它最大化在满足给定的表示约束的排名中的预期相关性，以确保后期的公平性。",
    "tldr": "提出了一种优化群组公平Plackett-Luce排序模型的方法，该方法最大化预期相关性并满足表示约束以确保后期公平性。"
}