{
    "title": "Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models. (arXiv:2308.08977v1 [math.OC])",
    "abstract": "We analyze the dynamics of streaming stochastic gradient descent (SGD) in the high-dimensional limit when applied to generalized linear models and multi-index models (e.g. logistic regression, phase retrieval) with general data-covariance. In particular, we demonstrate a deterministic equivalent of SGD in the form of a system of ordinary differential equations that describes a wide class of statistics, such as the risk and other measures of sub-optimality. This equivalence holds with overwhelming probability when the model parameter count grows proportionally to the number of data. This framework allows us to obtain learning rate thresholds for stability of SGD as well as convergence guarantees. In addition to the deterministic equivalent, we introduce an SDE with a simplified diffusion coefficient (homogenized SGD) which allows us to analyze the dynamics of general statistics of SGD iterates. Finally, we illustrate this theory on some standard examples and show numerical simulations w",
    "link": "http://arxiv.org/abs/2308.08977",
    "context": "Title: Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models. (arXiv:2308.08977v1 [math.OC])\nAbstract: We analyze the dynamics of streaming stochastic gradient descent (SGD) in the high-dimensional limit when applied to generalized linear models and multi-index models (e.g. logistic regression, phase retrieval) with general data-covariance. In particular, we demonstrate a deterministic equivalent of SGD in the form of a system of ordinary differential equations that describes a wide class of statistics, such as the risk and other measures of sub-optimality. This equivalence holds with overwhelming probability when the model parameter count grows proportionally to the number of data. This framework allows us to obtain learning rate thresholds for stability of SGD as well as convergence guarantees. In addition to the deterministic equivalent, we introduce an SDE with a simplified diffusion coefficient (homogenized SGD) which allows us to analyze the dynamics of general statistics of SGD iterates. Finally, we illustrate this theory on some standard examples and show numerical simulations w",
    "path": "papers/23/08/2308.08977.json",
    "total_tokens": 1023,
    "translated_title": "打破高维音符：关于广义线性模型和多索引模型上 SGD 学习动力学的 ODE 分析",
    "translated_abstract": "本文分析了在应用于具有一般数据协方差的广义线性模型和多索引模型（例如逻辑回归、相位恢复）时，流式随机梯度下降（SGD）在高维限制下的动力学。具体而言，我们证明了 SGD 的确定性等效形式，即一组描述风险和其他次优性度量的普通微分方程系统。当模型参数数量与数据数量成正比增长时，该等效性以极大概率发生。该框架使我们能够获得 SGD 稳定性的学习率阈值以及收敛保证。除了确定性等效性外，我们引入了一个具有简化扩散系数的 SDE（均匀化 SGD），它使我们能够分析 SGD 迭代的常规统计动力学。最后，我们在一些标准示例上演示了该理论，并展示了数值模拟结果。",
    "tldr": "本文研究了应用于广义线性模型和多索引模型中的流式随机梯度下降（SGD）的学习动力学。通过建立了一个普通微分方程系统来描述风险和次优性度量等统计量，获得了稳定性学习率阈值和收敛保证。同时，引入了一个简化扩散系数的随机微分方程模型，用于分析SGD迭代的统计动力学。通过标准示例和数值模拟，验证了该理论的有效性。",
    "en_tdlr": "This paper investigates the learning dynamics of streaming stochastic gradient descent (SGD) in generalized linear models and multi-index models. It establishes a system of ordinary differential equations to describe various statistics, such as risk and sub-optimality measures, providing learning rate thresholds for stability and convergence guarantees. Additionally, a simplified stochastic differential equation (homogenized SGD) is introduced to analyze general statistics of SGD iterates. The effectiveness of the theory is demonstrated through standard examples and numerical simulations."
}