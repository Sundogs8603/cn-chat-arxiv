{
    "title": "Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])",
    "abstract": "While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime",
    "link": "http://arxiv.org/abs/2308.13453",
    "context": "Title: Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])\nAbstract: While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime",
    "path": "papers/23/08/2308.13453.json",
    "total_tokens": 936,
    "translated_title": "学习干预概念瓶颈",
    "translated_abstract": "传统的深度学习模型缺乏解释性，而概念瓶颈模型（CBM）通过其概念表示提供固有的解释。具体而言，它们允许用户通过更新概念值并纠正模型的预测输出来进行干预交互。然而，传统方法中这些干预仅应用于模型一次后即被丢弃。为了纠正这一问题，我们提出了概念瓶颈记忆模型（CB2M），这是CBM的一个扩展。具体而言，CB2M通过双折叠记忆学习将干预的推广到适当的新情境中，从而能够学习检测错误并重新应用先前的干预。通过这种方式，CB2M能够从最初获得的少量干预中自动提高模型的性能。如果没有先前的人类干预信息，CB2M可以检测到CBM瓶颈的潜在错误并请求有针对性的干预。",
    "tldr": "该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。",
    "en_tdlr": "This paper proposes a concept bottleneck memory model (CB2M) that extends the concept bottleneck model (CBM) by learning to generalize interventions to different situations and reapplying previous interventions to automatically improve model performance. When no prior human interventions are available, CB2M can detect mistakes and request targeted interventions."
}