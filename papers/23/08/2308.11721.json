{
    "title": "When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])",
    "abstract": "Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \\in [2, n-1]$ - that is, there are strict benefits to collaborating,",
    "link": "http://arxiv.org/abs/2308.11721",
    "context": "Title: When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])\nAbstract: Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \\in [2, n-1]$ - that is, there are strict benefits to collaborating,",
    "path": "papers/23/08/2308.11721.json",
    "total_tokens": 960,
    "translated_title": "两个列表什么时候比一个列表更好？合作决策中的益处和伤害",
    "translated_abstract": "在过去的机器学习研究中，很大一部分关注的是算法的性能，但最近更多地关注于优化人工和算法的联合性能。在这里，我们分析了一种特定类型的人工和算法合作，在这种合作中，算法可以访问一组n个项目，并将大小为k的一个子集呈现给人类，然后人类从这些k个项目中选择一个最终项目。这种情况可以模拟内容推荐、路径规划或任何类型的标注任务。由于人类和算法都对项目的真实排序有着不完美、有噪音的信息，关键问题是：哪个$k$值能最大化最终选择最佳项目的概率？对于$k=1$，算法单独行动时性能最优，而对于$k=n$，人类单独行动时性能最优。令人惊讶的是，我们发现对于多个噪音模型，将$k$设置在$[2, n-1]$范围内是最优的，也就是说，合作有明显的益处。",
    "tldr": "这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。",
    "en_tdlr": "This study analyzes a specific type of human-algorithm collaboration and finds that, for multiple noise models, setting the subset size $k$ for selecting items in the range $[2, n-1]$ maximizes the probability of selecting the best item in the end."
}