{
    "title": "Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])",
    "abstract": "Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc",
    "link": "http://arxiv.org/abs/2308.13985",
    "context": "Title: Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])\nAbstract: Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc",
    "path": "papers/23/08/2308.13985.json",
    "total_tokens": 944,
    "translated_title": "重新审视多任务学习中的标量化：一个理论的视角",
    "translated_abstract": "线性标量化，即通过加权总和来组合所有损失函数，自从多任务学习（MTL）的创立以来一直是文献中的默认选择。近年来，越来越多的人对开发专门的多任务优化器（SMTOs）来处理MTL作为多目标优化问题产生了兴趣。然而，目前还不清楚SMTOs是否比标量化有根本上的优势。实际上，社区中存在对比这两种算法的激烈讨论，主要是从经验角度出发。为了回答上述问题，本文从理论的角度重新审视了标量化。我们专注于线性MTL模型，并研究标量化是否能够充分探索帕累托前沿。我们的研究发现，与那些声称标量化具有经验优势的最近工作相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren",
    "tldr": "本论文重新审视了多任务学习中的标量化方法，并从理论的角度探讨了标量化是否能够充分探索帕累托前沿。结果显示，与最近的研究声称的经验优势相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren",
    "en_tdlr": "This paper revisits scalarization in multi-task learning and explores whether scalarization is capable of fully exploring the Pareto front from a theoretical perspective. The findings reveal that, contrary to recent claims of empirical advantages, scalarization is inherently incapable of complete exploration, especially for solutions that strike a balance on the Pareto front."
}