{
    "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
    "abstract": "arXiv:2308.07317v2 Announce Type: replace  Abstract: We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for othe",
    "link": "https://arxiv.org/abs/2308.07317",
    "context": "Title: Platypus: Quick, Cheap, and Powerful Refinement of LLMs\nAbstract: arXiv:2308.07317v2 Announce Type: replace  Abstract: We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for othe",
    "path": "papers/23/08/2308.07317.json",
    "total_tokens": 902,
    "translated_title": "Platypus: LLM的快速、廉价和强大的细化",
    "translated_abstract": "我们提出了Platypus，这是一组精细调节和合并的大型语言模型（LLMs），在发布本文时在HuggingFace的开放LLM排行榜上表现最出色，目前位列第一。在这项工作中，我们描述了（1）我们精心策划的数据集Open-Platypus，这是其他开放数据集的一个子集，我们向公众开放；（2）我们在微调和合并LoRA模块的过程中如何保留预训练LLMs的强先验，并将特定领域知识呈现出来；（3）我们在检查测试数据泄霢和训练数据中的污染方面的努力，这可以为未来的研究提供信息。具体来说，Platypus系列在各种模型大小上表现出色，以较少的微调数据和总体计算量获得了强大的定量LLM指标，位居全球开放LLM排行榜榜首。",
    "tldr": "Platypus是一组经过精细调节和合并的大型语言模型（LLMs），在全球开放LLM排行榜上表现最出色，在微调数据和计算量上仅需其他方法的一小部分。",
    "en_tdlr": "Platypus is a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance in the global Open LLM leaderboard, using just a fraction of the fine-tuning data and overall compute required by other methods."
}