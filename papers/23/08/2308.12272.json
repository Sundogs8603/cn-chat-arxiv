{
    "title": "Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])",
    "abstract": "Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guide",
    "link": "http://arxiv.org/abs/2308.12272",
    "context": "Title: Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])\nAbstract: Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guide",
    "path": "papers/23/08/2308.12272.json",
    "total_tokens": 941,
    "translated_title": "简单即是更好，大并不足够：走向基础语言模型的集成",
    "translated_abstract": "基础语言模型(FLMs)推动了自然语言处理(NLP)研究的发展。当前的研究者正在开发更大的FLMs（例如，XLNet、T5）以实现上下文化的语言表示、分类和生成。虽然开发更大的FLMs具有显著优势，但也存在虚构和预测不确定性的风险。从根本上说，更大的FLMs建立在较小的FLMs（例如，BERT）的基础之上；因此，人们必须认识到较小的FLMs的潜力，这可以通过集成来实现。在当前研究中，我们在基准和现实世界的数据集上对FLMs及其集成进行了实际检验。我们假设FLMs的集成可以影响其个体关注，并揭示不同FLMs之间的协调和合作的优势。我们利用BERT并定义了三种其他的集成技术：{浅层、半深层和深层}，其中深层集成引入了一个知识引导。",
    "tldr": "这项研究检验了基础语言模型(FLMs)及其集成在基准和真实数据集上的表现，发现集成可以影响FLMs的个体关注，并展示不同FLMs之间的协调和合作优势。"
}