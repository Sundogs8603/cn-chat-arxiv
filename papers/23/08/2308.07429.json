{
    "title": "Semantic Similarity Loss for Neural Source Code Summarization. (arXiv:2308.07429v1 [cs.SE])",
    "abstract": "This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each ",
    "link": "http://arxiv.org/abs/2308.07429",
    "context": "Title: Semantic Similarity Loss for Neural Source Code Summarization. (arXiv:2308.07429v1 [cs.SE])\nAbstract: This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each ",
    "path": "papers/23/08/2308.07429.json",
    "total_tokens": 895,
    "translated_title": "适用于神经源代码摘要的语义相似性损失",
    "translated_abstract": "本文提出了一种改进的损失函数用于神经源代码摘要。代码摘要是编写源代码的自然语言描述的任务。神经代码摘要是使用神经网络生成这些描述的自动化技术。几乎所有目前的方法都涉及神经网络作为独立模型或作为预训练的大型语言模型的一部分，例如GPT、Codex、LLaMA。然而，几乎所有方法都使用分类交叉熵（CCE）损失函数进行网络优化。CCE存在两个问题：1）它一次计算每个单词预测的损失，而不是评估整个句子；2）它要求完美预测，不允许对同义词给予部分信用。我们提出并评估了一种损失函数来缓解这个问题。实质上，我们建议使用语义相似性度量来计算整个输出句子预测的损失，而不仅仅是每个训练批次的损失。",
    "tldr": "本文提出了一种适用于神经源代码摘要的改进损失函数，通过使用语义相似性度量来评估整个输出句子预测的损失，解决了当前方法中基于分类交叉熵损失函数的两个问题。"
}