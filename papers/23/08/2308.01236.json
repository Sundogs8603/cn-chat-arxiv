{
    "title": "Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])",
    "abstract": "This paper introduces Grounded Image Text Matching with Mismatched Relation (GITM-MR), a novel visual-linguistic joint task that evaluates the relation understanding capabilities of transformer-based pre-trained models. GITM-MR requires a model to first determine if an expression describes an image, then localize referred objects or ground the mismatched parts of the text. We provide a benchmark for evaluating pre-trained models on this task, with a focus on the challenging settings of limited data and out-of-distribution sentence lengths. Our evaluation demonstrates that pre-trained models lack data efficiency and length generalization ability. To address this, we propose the Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates relation-aware reasoning via bi-directional message propagation guided by language structure. RCRN can be interpreted as a modular program and delivers strong performance in both length generalization and data efficiency.",
    "link": "http://arxiv.org/abs/2308.01236",
    "context": "Title: Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])\nAbstract: This paper introduces Grounded Image Text Matching with Mismatched Relation (GITM-MR), a novel visual-linguistic joint task that evaluates the relation understanding capabilities of transformer-based pre-trained models. GITM-MR requires a model to first determine if an expression describes an image, then localize referred objects or ground the mismatched parts of the text. We provide a benchmark for evaluating pre-trained models on this task, with a focus on the challenging settings of limited data and out-of-distribution sentence lengths. Our evaluation demonstrates that pre-trained models lack data efficiency and length generalization ability. To address this, we propose the Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates relation-aware reasoning via bi-directional message propagation guided by language structure. RCRN can be interpreted as a modular program and delivers strong performance in both length generalization and data efficiency.",
    "path": "papers/23/08/2308.01236.json",
    "total_tokens": 984,
    "translated_title": "使用不匹配关系推理的基于视觉和语言的图像文本匹配",
    "translated_abstract": "本文介绍了一种称为“使用不匹配关系推理的基于视觉和语言的图像文本匹配（GITM-MR）”的新型视觉-语言联合任务，用于评估基于Transformer预训练模型的关系理解能力。GITM-MR需要模型首先确定一个表达是否描述了一张图像，然后定位所指的对象或者对文本中的不匹配部分进行 grounding。我们提供了一个用于评估预训练模型在这个任务上表现的基准，重点关注有限数据和超出分布的句子长度的挑战性设置。我们的评估表明，预训练模型缺乏数据效率和长度泛化能力。为了解决这个问题，我们提出了一种称为关系敏感的对应推理网络（RCRN），通过双向信息传递和语言结构引导的关系感知推理，将RCRN解释为一个模块化程序，并在长度泛化和数据效率方面表现出较强的性能。",
    "tldr": "本文介绍了一种新颖的基于视觉和语言的图像文本匹配任务，要求模型能够确定图像与文本的关系，并定位所指的对象或者对不匹配的部分进行 grounding。为了解决预训练模型在此任务上的数据效率和长度泛化能力不足的问题，我们提出了一种关系敏感的对应推理网络（RCRN），该网络通过双向信息传递和语言结构引导的关系感知推理来实现。",
    "en_tdlr": "This paper introduces a novel visual-linguistic joint task, Grounded Image Text Matching with Mismatched Relation (GITM-MR), which evaluates the relation understanding capabilities of transformer-based pre-trained models. The proposed Relation-sensitive Correspondence Reasoning Network (RCRN) addresses the data efficiency and length generalization issues, achieving strong performance in both aspects."
}