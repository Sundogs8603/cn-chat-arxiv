{
    "title": "DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])",
    "abstract": "Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\\% while maintaining 98\\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met",
    "link": "http://arxiv.org/abs/2308.00108",
    "context": "Title: DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])\nAbstract: Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\\% while maintaining 98\\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met",
    "path": "papers/23/08/2308.00108.json",
    "total_tokens": 832,
    "translated_title": "DPBERT: 基于动态规划的高效BERT推理",
    "translated_abstract": "大规模预训练语言模型（如BERT）为自然语言处理的发展做出了重要贡献。然而，这些模型需要大量的计算资源，难以应用于计算能力有限的移动设备上。本文旨在解决现有自适应输入推理方法的缺点，这些方法未能充分利用BERT的结构。我们提出了DPBERT，一种新的微调策略，通过选择BERT的一部分transformer层作为计算路径，在推理过程中加速BERT的推理。为此，我们的方法在原始BERT模型中添加了一个规划模块，用于确定推理过程中是否包含或绕过某个层。在GLUE基准测试上的实验结果表明，我们的方法在保持98%准确性的情况下，将延迟降低到75%，相比最先进的自适应输入方法，获得了更好的准确度和速度权衡。",
    "tldr": "DPBERT是一个基于动态规划的高效BERT推理方法，通过选择一部分transformer层来加速推理过程，在保持高准确性的同时降低延迟。",
    "en_tdlr": "DPBERT is an efficient BERT inference method based on dynamic planning, which accelerates the inference process by selecting a subset of transformer layers while maintaining high accuracy."
}