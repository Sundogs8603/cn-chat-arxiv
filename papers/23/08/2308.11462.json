{
    "title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])",
    "abstract": "The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This p",
    "link": "http://arxiv.org/abs/2308.11462",
    "context": "Title: LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])\nAbstract: The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This p",
    "path": "papers/23/08/2308.11462.json",
    "total_tokens": 918,
    "translated_title": "LegalBench：一个用于衡量大型语言模型的法律推理的协同构建基准库",
    "translated_abstract": "大型语言模型（LLMs）的出现和法律界对其的采用引发了一个问题：LLMs能够执行哪些类型的法律推理？为了更深入地研究这个问题，我们提出了LegalBench：一个协同构建的法律推理基准库，包含162个任务，涵盖了六种不同类型的法律推理。LegalBench是通过跨学科的过程构建的，我们收集了由法律专业人员设计和手工制作的任务。因为这些专业人员在构建过程中起了主导作用，所以任务要么衡量了实际有用的法律推理能力，要么衡量了律师们感兴趣的推理技能。为了促进跨学科关于法律界LLMs的对话，我们还展示了流行的法律框架如何描述法律推理，这些框架区分了许多形式，与LegalBench的任务对应起来，从而给律师和LLM开发者提供了共同的词汇表。",
    "tldr": "LegalBench是一个协同构建的法律推理基准库，涵盖了162个任务，可用于衡量大型语言模型在法律推理方面的能力，为律师和LLM开发者提供了共同的词汇表。"
}