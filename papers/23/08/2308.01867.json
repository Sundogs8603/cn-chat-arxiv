{
    "title": "MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])",
    "abstract": "Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -> symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and",
    "link": "http://arxiv.org/abs/2308.01867",
    "context": "Title: MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])\nAbstract: Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -> symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and",
    "path": "papers/23/08/2308.01867.json",
    "total_tokens": 911,
    "translated_title": "MRQ:通过模型重新量化支持多种量化方案",
    "translated_abstract": "尽管各种硬件加速器（如NPU，TPU，DPU）的普及，但在固定点硬件上部署深度学习模型仍然具有挑战性，原因是复杂的模型量化和转换。现有的模型量化框架（如Tensorflow QAT，TFLite PTQ和Qualcomm AIMET）只支持有限的量化方案（如仅在TF1.x QAT中的非对称每张量量化）。因此，由于稍微不同的量化要求，深度学习模型不能轻松地为各种固定点硬件进行量化。在本文中，我们设想了一种新型的模型量化方法，称为MRQ（模型重新量化），它可以采用现有的量化模型，并快速将模型转换为满足不同量化要求（如非对称->对称，非2的幂次->2的幂次）。重新量化比从头开始进行量化要简单得多，因为它避免了昂贵的重新训练。",
    "tldr": "该论文提出了一种名为MRQ的模型重新量化方法，通过将现有的量化模型快速转换以满足不同的量化要求，解决了在固定点硬件上部署深度学习模型的挑战。"
}