{
    "title": "An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])",
    "abstract": "Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas",
    "link": "http://arxiv.org/abs/2308.11677",
    "context": "Title: An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])\nAbstract: Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas",
    "path": "papers/23/08/2308.11677.json",
    "total_tokens": 925,
    "translated_title": "无范例类增量学习的初始训练策略分析",
    "translated_abstract": "类增量学习旨在从数据流中构建分类模型。在类增量学习过程的每一步中，新的类别必须被整合到模型中。由于灾难性遗忘，当无法存储过去类别的样本时，类增量学习变得尤为具有挑战性，这正是我们在此研究的对象。迄今为止，大多数方法仅基于类增量学习过程的目标数据集。然而，最近在大量数据上通过自监督方式预训练模型的使用已经逐渐增多。类增量学习过程的初始模型可能仅使用目标数据集的第一批数据，或者还可以使用在辅助数据集上获得的预训练权重。这两种初始学习策略的选择可以极大地影响增量学习模型的性能，但目前还没有进行深入研究。性能还受到类增量学习算法的选择、神经网络结构、目标任务的性质、类别分布的影响。",
    "tldr": "本文分析了无范例类增量学习过程中的初始训练策略。研究发现，初始学习策略的选择会显著影响增量学习模型的性能，但目前还没有进行深入研究。",
    "en_tdlr": "This paper analyzes the initial training strategies for exemplar-free class-incremental learning. The study found that the choice of initial learning strategy significantly affects the performance of the incremental learning model, but has not been thoroughly studied."
}