{
    "title": "When hard negative sampling meets supervised contrastive learning. (arXiv:2308.14893v1 [cs.CV])",
    "abstract": "State-of-the-art image models predominantly follow a two-stage strategy: pre-training on large datasets and fine-tuning with cross-entropy loss. Many studies have shown that using cross-entropy can result in sub-optimal generalisation and stability. While the supervised contrastive loss addresses some limitations of cross-entropy loss by focusing on intra-class similarities and inter-class differences, it neglects the importance of hard negative mining. We propose that models will benefit from performance improvement by weighting negative samples based on their dissimilarity to positive counterparts. In this paper, we introduce a new supervised contrastive learning objective, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. Without requiring specialized architectures, additional data, or extra computational resources, experimental results indicate that SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various benchmarks, with signific",
    "link": "http://arxiv.org/abs/2308.14893",
    "context": "Title: When hard negative sampling meets supervised contrastive learning. (arXiv:2308.14893v1 [cs.CV])\nAbstract: State-of-the-art image models predominantly follow a two-stage strategy: pre-training on large datasets and fine-tuning with cross-entropy loss. Many studies have shown that using cross-entropy can result in sub-optimal generalisation and stability. While the supervised contrastive loss addresses some limitations of cross-entropy loss by focusing on intra-class similarities and inter-class differences, it neglects the importance of hard negative mining. We propose that models will benefit from performance improvement by weighting negative samples based on their dissimilarity to positive counterparts. In this paper, we introduce a new supervised contrastive learning objective, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. Without requiring specialized architectures, additional data, or extra computational resources, experimental results indicate that SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various benchmarks, with signific",
    "path": "papers/23/08/2308.14893.json",
    "total_tokens": 950,
    "translated_title": "当困难的负样本采样与监督对比学习相遇",
    "translated_abstract": "目前的图像模型通常遵循两阶段策略：在大数据集上进行预训练，然后使用交叉熵损失进行微调。许多研究表明，使用交叉熵可能导致次优的泛化和稳定性。监督对比损失通过关注类内相似性和类间差异来解决交叉熵损失的一些限制，但它忽视了困难负样本挖掘的重要性。我们提出，通过根据负样本与正样本的不相似程度进行加权，模型将从性能改进中受益。在本文中，我们引入了一种新的监督对比学习目标，SCHaNe，在微调阶段引入了困难负样本采样。实验结果表明，SCHaNe在各个基准测试中的Top-1准确率上优于强基线模型BEiT-3，而无需专门的架构、额外的数据或计算资源。",
    "tldr": "当前图像模型在训练时常用交叉熵损失，但其在泛化和稳定性方面存在问题。本文提出了一种新的监督对比学习目标，SCHaNe，通过加权困难的负样本挖掘来提高模型性能。实验结果显示，SCHaNe在各个基准测试中的Top-1准确率优于BEiT-3。"
}