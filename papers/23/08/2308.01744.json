{
    "title": "Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])",
    "abstract": "Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second k",
    "link": "http://arxiv.org/abs/2308.01744",
    "context": "Title: Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])\nAbstract: Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second k",
    "path": "papers/23/08/2308.01744.json",
    "total_tokens": 870,
    "translated_title": "具有无悔的多任务学习：从改进的置信区间到主动学习",
    "translated_abstract": "多任务学习是一个强大的框架，通过共享信息来同时学习多个相关任务。对估计任务的不确定性进行量化对许多下游应用非常重要，例如在线或主动学习。在这项工作中，我们在具有挑战性的不可知设置下提供了新颖的多任务置信区间，即当学习者无法获得任务之间的相似性和任务特征时。所得到的区间不需要独立同分布的数据，并且可以直接应用于在线学习中的遗憾边界。通过对多任务信息增益的精细分析，我们获得了新的遗憾保证，可以根据任务相似性参数明显改进独立处理任务的方法。我们进一步提出了一种新的在线学习算法，可以在不事先知道这个参数的情况下实现这种改进的遗憾，即自动适应任务相似性。",
    "tldr": "本文提出了一种在不可知情况下的多任务学习新方法，通过对多任务信息增益的分析，获得了新的遗憾保证，并提出了一种能够自动适应任务相似性的在线学习算法。",
    "en_tdlr": "This paper proposes a new method for multitask learning in the agnostic setting, provides new regret guarantees based on the analysis of multitask information gain, and proposes an online learning algorithm that can adapt to task similarity automatically."
}