{
    "title": "Where's the Liability in Harmful AI Speech?. (arXiv:2308.04635v1 [cs.CY])",
    "abstract": "Generative AI, in particular text-based \"foundation models\" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly \"red team\" models to identify and mitigate such problematic speech: from \"hallucinations\" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding mo",
    "link": "http://arxiv.org/abs/2308.04635",
    "context": "Title: Where's the Liability in Harmful AI Speech?. (arXiv:2308.04635v1 [cs.CY])\nAbstract: Generative AI, in particular text-based \"foundation models\" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly \"red team\" models to identify and mitigate such problematic speech: from \"hallucinations\" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding mo",
    "path": "papers/23/08/2308.04635.json",
    "total_tokens": 880,
    "translated_title": "AI有害言论的责任在哪里？",
    "translated_abstract": "生成式人工智能（特别是基于文本的“基础模型”）可以生成可能在广泛的责任制度下引发问题的言论。机器学习从业者经常对模型进行“红队”测试，以识别和减轻此类问题言论，从错误指责严重不端行为的“幻觉”到构造原子弹的食谱。一个关键问题是这些红队测试行为是否真的对模型创建者和部署者构成任何法律责任风险，从而激励投资于安全机制。我们研究了三种责任制度，并将其与红队测试模型行为的常见例子联系起来：诽谤、构成犯罪行为的言论和错误致死。我们发现，任何Section 230免责分析或下游责任分析都与算法设计的技术细节密切相关。而要真正找到解决这些问题的方法有很多障碍。",
    "tldr": "AI生成式模型可能会产生具有潜在责任风险的有害言论。解决模型创建者和部署者的法律责任问题的关键在于算法设计的技术细节。需要进行深入的Section 230免责分析以及下游责任分析。"
}