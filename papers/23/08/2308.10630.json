{
    "title": "A Homogenization Approach for Gradient-Dominated Stochastic Optimization",
    "abstract": "Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper com",
    "link": "https://arxiv.org/abs/2308.10630",
    "context": "Title: A Homogenization Approach for Gradient-Dominated Stochastic Optimization\nAbstract: Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper com",
    "path": "papers/23/08/2308.10630.json",
    "total_tokens": 898,
    "translated_title": "一个基于均匀化方法的梯度主导随机优化方法",
    "translated_abstract": "梯度主导性质是一种比强凸性条件更弱但足以确保全局收敛的条件，即使在非凸优化中也可以应用广泛。本文提出了一种基于最近提出的均匀化方法的梯度主导随机二阶下降方法（SHSODM），用于满足梯度主导性质的随机函数。从理论上讲，我们提供了其样本复杂度分析，并通过结合方差减少技术提供了进一步的增强结果。我们的发现表明，SHSODM与其他梯度主导随机优化的二阶方法相比，可以达到已知的最佳样本复杂度，而无需立方正则化。从经验上讲，由于均匀化方法仅依赖于每次迭代中解极值特征向量问题，而不是牛顿类型的系统，所以我们的方法具有更低的计算成本。",
    "tldr": "本文介绍了一种基于均匀化方法的梯度主导随机优化方法，通过满足梯度主导性质的随机函数，实现全局收敛。我们提供了样本复杂度分析，并通过方差减少技术提供了增强结果。实验结果表明，该方法在无需立方正则化的情况下达到了最佳样本复杂度。",
    "en_tdlr": "This paper presents a gradient-dominant stochastic optimization approach based on homogenization, which ensures global convergence by satisfying the gradient dominance property. The proposed method achieves the best-known sample complexity compared to other second-order methods for gradient-dominated stochastic optimization without cubic regularization."
}