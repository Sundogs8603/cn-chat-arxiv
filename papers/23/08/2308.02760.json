{
    "title": "Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks. (arXiv:2308.02760v1 [cs.LG])",
    "abstract": "Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separa",
    "link": "http://arxiv.org/abs/2308.02760",
    "context": "Title: Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks. (arXiv:2308.02760v1 [cs.LG])\nAbstract: Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separa",
    "path": "papers/23/08/2308.02760.json",
    "total_tokens": 995,
    "translated_title": "分类神经网络中间隐藏层的神经崩溃",
    "translated_abstract": "神经崩溃（NC）对分类神经网络中最终隐藏层中类别的表示提供了精确的描述。这个描述揭示了这些网络如何在训练超过零训练误差时学习特征和泛化。然而，迄今为止，（NC）只在这些网络的最终层中进行研究。在本论文中，我们首次对这些分类器的中间隐藏层中（NC）的出现进行了全面的实证分析。我们检查了各种网络架构、激活函数和数据集，并证明了在网络的大多数中间隐藏层中会出现某种程度的（NC），其中每个隐藏层的崩溃程度通常与该层在神经网络中的深度呈正相关。此外，我们还指出：（1）样本中的几乎所有类内方差的减小发生在更浅的层中，（2）夹角分离量随隐藏层深度的增加而增加。",
    "tldr": "本论文首次对分类神经网络中间隐藏层的神经崩溃进行了全面的实证分析。研究发现，大多数中间隐藏层都会出现某种程度的神经崩溃，而崩溃程度通常与层的深度呈正相关。此外，研究还发现类内方差的减小主要发生在较浅的层中，夹角分离量随隐藏层深度的增加而增加。",
    "en_tdlr": "This paper provides a comprehensive empirical analysis of neural collapse in the intermediate hidden layers of classification neural networks. It reveals that most of the intermediate hidden layers exhibit some degree of neural collapse, which is typically positively correlated with the depth of the layer. Furthermore, the study finds that the reduction in intra-class variance mainly occurs in the shallower layers, and the angular separability increases with the depth of the hidden layers."
}