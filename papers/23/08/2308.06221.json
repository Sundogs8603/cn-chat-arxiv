{
    "title": "Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])",
    "abstract": "We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d",
    "link": "http://arxiv.org/abs/2308.06221",
    "context": "Title: Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])\nAbstract: We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d",
    "path": "papers/23/08/2308.06221.json",
    "total_tokens": 838,
    "translated_title": "使用二阶算法自动调整和训练高效的深度自编码器",
    "translated_abstract": "我们提出了一种多步训练方法，用于设计广义线性分类器。首先，通过回归找到一个初始的多类线性分类器。然后通过修剪不必要的输入来最小化验证误差。同时，通过类似于Ho-Kashyap规则的方法改善期望输出。接下来，将输出判别式缩放为广义线性分类器中S型输出单元的网络函数。然后，我们开发了一族批量训练算法，用于优化多层感知机的隐藏层大小和训练时长。接着，我们将修剪与增长方法相结合。然后，将输入单元缩放为S型输出单元的网络函数，然后将其作为输入馈送到MLP中。最后，我们提出了深度学习模块中的改进，从而提高了深度架构的整体性能。我们讨论了关于d的学习算法的原则和公式。",
    "tldr": "该论文提出了一种多步训练方法，用于设计深度自编码器，并通过修剪和增长方法以及优化隐藏层大小和训练时长来改善其性能。",
    "en_tdlr": "This paper proposes a multi-step training method for designing deep autoencoders, improving their performance through pruning and growing techniques, and optimizing the size and duration of the hidden layers."
}