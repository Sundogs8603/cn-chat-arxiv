{
    "title": "Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])",
    "abstract": "Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequenc",
    "link": "http://arxiv.org/abs/2308.12097",
    "context": "Title: Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])\nAbstract: Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequenc",
    "path": "papers/23/08/2308.12097.json",
    "total_tokens": 838,
    "translated_title": "使用大型语言模型进行序列生成中，指令位置的重要性",
    "translated_abstract": "大型语言模型（LLMs）通过指令微调可以执行条件序列生成任务，如翻译或摘要。微调数据通常是从特定任务指令、输入句子和对应回应串联而成。考虑到LLMs的自注意机制建模的局部性，这些模型在为长输入句子生成回应时面临指令遗忘的风险。为了缓解这个问题，我们提出通过将任务指令的位置移动到输入句子之后，增强LLMs的遵循指令能力。理论分析表明，我们的简单方法可以改变模型的学习重点，从而强调遵循指令能力的训练。同时，实验结果表明我们的方法在不同的模型规模（1B / 7B / 13B）和不同序列长度下始终优于传统设置。",
    "tldr": "本研究提出了一种增强大型语言模型在序列生成任务中遵循指令能力的方法，通过调整任务指令的位置，从而弥补指令遗忘的问题。实验结果证明了该方法在不同规模和长度的序列上均优于传统设置。"
}