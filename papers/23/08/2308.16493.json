{
    "title": "Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. (arXiv:2308.16493v1 [cs.AI])",
    "abstract": "Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU",
    "link": "http://arxiv.org/abs/2308.16493",
    "context": "Title: Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. (arXiv:2308.16493v1 [cs.AI])\nAbstract: Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU",
    "path": "papers/23/08/2308.16493.json",
    "total_tokens": 923,
    "translated_title": "不重新训练的情况下扩展冻结的视觉-语言模型：对改进机器人感知的探索",
    "translated_abstract": "视觉-语言模型（VLM）通过将视觉表示与大型语言模型（LLMs）在预训练期间学习的抽象技能组合起来，在视觉问答和推理任务中展现了强大的能力。视觉是最受欢迎的方式之一，用于增强LLMs，但仅仅是场景的一个表示。在人机交互场景中，机器人感知需要对场景进行准确的理解。在本文中，我们定义并展示了一种通过监督和对比训练将不同模态（在此情况下是惯性测量单元（IMU）数据）的嵌入空间与视觉嵌入空间对齐的方法，使得VLM能够在不重新训练的情况下理解和推理这些额外的模态。我们选择直接提供模型IMU嵌入，而不是使用单独的人体活动识别模型直接输入提示，以便允许查询、图像和IMU之间的任何非线性交互。",
    "tldr": "本文提出一种方法，通过对不同模态的嵌入空间进行对齐，使得视觉-语言模型能够在不重新训练的情况下理解和推理额外的模态，并且在人机交互场景中改进机器人的感知能力。",
    "en_tdlr": "This paper presents a method to align embedding spaces of different modalities, enabling vision-language models to understand and reason about additional modalities without retraining, thus improving robot perception in human-robot interaction scenarios."
}