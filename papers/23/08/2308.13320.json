{
    "title": "Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])",
    "abstract": "Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon \"concept forgetting\" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS ",
    "link": "http://arxiv.org/abs/2308.13320",
    "context": "Title: Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])\nAbstract: Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon \"concept forgetting\" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS ",
    "path": "papers/23/08/2308.13320.json",
    "total_tokens": 849,
    "translated_title": "微调可能削弱基础模型；保留特征可能是解决方案",
    "translated_abstract": "预训练的基础模型主要由于其巨大的容量和对从互联网上爬取的大量训练数据的暴露，享有存储关于许多现实世界概念的知识的优势。这些模型通常在下游数据集上进行微调，以产生出色的最新性能。然而，我们观察到，与预训练模型相比，微调模型在与下游任务不同的任务上识别概念的能力显著降低。这显然是不可取的，因为在首次学习这些概念时，投入了大量的时间和金钱。我们将这种不可取的现象称为“概念遗忘”，通过实验证明大多数端到端微调方法都严重受到这种副作用的影响。为此，我们还提出了一个相当简单的解决方法，即设计了一种名为LDIFS的方法。",
    "tldr": "在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。",
    "en_tdlr": "Fine-tuning can lead to concept forgetting in foundation models. We propose a method called LDIFS to address this issue, which has been shown to be effective in experiments."
}