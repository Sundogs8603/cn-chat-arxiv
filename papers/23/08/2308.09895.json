{
    "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])",
    "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten",
    "link": "http://arxiv.org/abs/2308.09895",
    "context": "Title: Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])\nAbstract: Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten",
    "path": "papers/23/08/2308.09895.json",
    "total_tokens": 896,
    "translated_title": "从高资源语言到低资源编程语言的知识转移用于代码LLMs",
    "translated_abstract": "在过去几年中，代码LLMs（大规模语言模型）开始对编程实践产生重大影响。代码LLMs还成为编程语言和软件工程研究的重要组成部分。然而，代码LLMs生成的代码质量在不同编程语言之间存在显著差异。代码LLMs对训练数据充分的编程语言（如Java、Python或JavaScript）产生令人印象深刻的结果，但在像OCaml和Racket这样的低资源语言上表现困难。本文提出了一种有效的方法，通过使用半合成数据提高代码LLMs在低资源语言上的性能。我们的方法生成了高质量的低资源语言数据集，并可用于微调任何预训练的代码LLMs。我们的方法称为MultiPL-T，它将高资源语言的训练数据转化为低资源语言的训练数据。我们将该方法应用于生成十个数据集。",
    "tldr": "本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。"
}