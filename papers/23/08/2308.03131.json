{
    "title": "Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)",
    "abstract": "N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \\textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi",
    "link": "http://arxiv.org/abs/2308.03131",
    "context": "Title: Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)\nAbstract: N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \\textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi",
    "path": "papers/23/08/2308.03131.json",
    "total_tokens": 1035,
    "translated_title": "迈向多参考时代 —— 解决NLG评估中的数据泄漏和参考多样性有限问题",
    "translated_abstract": "N-gram匹配的评估指标，如BLEU和chrF，在各种自然语言生成（NLG）任务中被广泛使用。然而，最近的研究发现，这些基于匹配的指标与人类评估之间存在较弱的相关性，尤其是与基于神经网络的指标如BLEURT相比。在本文中，我们推测匹配指标性能瓶颈的原因可能是参考资料多样性有限。为了解决这个问题，我们提出利用\"多个参考\"来增强这些指标与人类评估之间的一致性。在WMT Metrics基准中，我们观察到多参考F200spBLEU相对于传统的单参考方法，准确率提高了7.2\\%。值得注意的是，它还超过了基于神经网络的BERTscore，准确率提高了3.9\\%。此外，我们观察到大型语言模型（LLMs）中的数据泄漏问题可以在很大程度上得到缓解通过我们的多参考方法。",
    "tldr": "本论文提出了使用多个参考来增强匹配指标与人类评估之间的一致性。在WMT Metrics基准中，多参考F200spBLEU相对于传统的单参考方法准确率提高了7.2\\%，超过了基于神经网络的BERTscore的3.9\\%的准确率提高。此外，该方法还可以在很大程度上缓解大型语言模型中的数据泄漏问题。",
    "en_tdlr": "This paper proposes using multiple references to enhance the consistency between matching-based metrics and human evaluations. In the WMT Metrics benchmark, the multi-reference F200spBLEU achieves an accuracy improvement of 7.2% compared to the traditional single-reference method, exceeding the accuracy enhancement of 3.9% of the neural-based BERTscore. Moreover, this method also mitigates the issue of data leakage in large language models to a large extent."
}