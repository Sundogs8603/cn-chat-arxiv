{
    "title": "Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters. (arXiv:2308.06088v1 [cs.AI])",
    "abstract": "Identifying logical errors in complex, incomplete or even contradictory and overall heterogeneous data like students' experimentation protocols is challenging. Recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. Our aim is to provide a foundation for productive, personalized feedback. Using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. Our results indicate varying levels of accuracy in error detection between the AI system and human raters. The AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investi",
    "link": "http://arxiv.org/abs/2308.06088",
    "context": "Title: Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters. (arXiv:2308.06088v1 [cs.AI])\nAbstract: Identifying logical errors in complex, incomplete or even contradictory and overall heterogeneous data like students' experimentation protocols is challenging. Recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. Our aim is to provide a foundation for productive, personalized feedback. Using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. Our results indicate varying levels of accuracy in error detection between the AI system and human raters. The AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investi",
    "path": "papers/23/08/2308.06088.json",
    "total_tokens": 996,
    "translated_title": "使用人工智能和大型语言模型评估学生实验错误：与人工评分比较研究",
    "translated_abstract": "辨别复杂、不完整甚至矛盾以及整体异质化的数据，如学生的实验方案，是具有挑战性的。鉴于当前评估方法的局限性，我们研究了大型语言模型（LLM）在自动识别学生错误和简化教师评估方面的潜力。我们的目标是为有效的个性化反馈打下基础。通过使用65份学生实验方案的数据集，我们开发了一个基于GPT-3.5和GPT-4系列的人工智能（AI）系统，并将其与人工评分员进行了测试。我们的结果表明，AI系统和人工评分员之间在错误检测方面的准确性存在不同水平。AI系统能够准确识别许多基本的学生错误，例如，当学生把假设重点放在独立变量而不是预期的观察上时，AI系统能够识别出来（准确度=0.90），当学生在进行中的调查中改变试验时。",
    "tldr": "本研究探究了大型语言模型（LLMs）在自动识别学生错误和简化教师评估中的潜力。使用65份学生实验方案的数据集，研发了基于GPT-3.5和GPT-4系列的人工智能（AI）系统，并与人工评分员进行测试。结果显示，AI系统在错误检测方面能够准确识别许多基本学生错误。",
    "en_tdlr": "This study investigates the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. By developing an AI system based on GPT-3.5 and GPT-4 series and testing it against human raters with a dataset of 65 student protocols, the results indicate accurate identification of fundamental student errors."
}