{
    "title": "CodeCoT and Beyond: Learning to Program and Test like a Developer. (arXiv:2308.08784v1 [cs.SE])",
    "abstract": "In natural language processing, transformer-based large language models (LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape. Despite their impressive capabilities, these models often encounter challenges when handling tasks that differ from their training data, resulting in compromised performance. To address this, few-shot learning has emerged as a valuable technique, allowing LLMs to adapt with minimal task-specific data. One innovative strategy, known as Chain-of-Thought Prompting (CoT), has been introduced to guide LLMs in revealing cognitive processes during multi-step reasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), which consists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. The latter incorporates self-examination, empowering the model to iteratively generate code, formulate test cases, and refine its outputs. Specifically, the process entails the generation of test examples by the model corresponding to the co",
    "link": "http://arxiv.org/abs/2308.08784",
    "context": "Title: CodeCoT and Beyond: Learning to Program and Test like a Developer. (arXiv:2308.08784v1 [cs.SE])\nAbstract: In natural language processing, transformer-based large language models (LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape. Despite their impressive capabilities, these models often encounter challenges when handling tasks that differ from their training data, resulting in compromised performance. To address this, few-shot learning has emerged as a valuable technique, allowing LLMs to adapt with minimal task-specific data. One innovative strategy, known as Chain-of-Thought Prompting (CoT), has been introduced to guide LLMs in revealing cognitive processes during multi-step reasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), which consists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. The latter incorporates self-examination, empowering the model to iteratively generate code, formulate test cases, and refine its outputs. Specifically, the process entails the generation of test examples by the model corresponding to the co",
    "path": "papers/23/08/2308.08784.json",
    "total_tokens": 924,
    "translated_title": "CodeCoT及其进展：学习像开发者一样编程和测试",
    "translated_abstract": "在自然语言处理领域，OpenAI开发的基于转换器的大型语言模型（LLM）如GPT-x模型已经彻底改变了现状。尽管这些模型具有令人印象深刻的能力，但它们在处理与其训练数据不同的任务时常常遇到挑战，造成性能下降。为了解决这个问题，出现了一种被称为少样本学习的有价值技术，允许LLM在最少的任务特定数据上进行适应。一种创新的策略，称为思维链提示（CoT），已被引入以指导LLM在多步推理过程中揭示认知过程。在本文中，我们提出了Code Chain-of-Thought（CodeCoT），它由两个组成部分组成：经典CodeCoT和自我检查CodeCoT。后者加入了自我检查，使模型能够迭代生成代码，制定测试用例并改善其输出。具体而言，该过程包括模型生成与分类别特征对应的测试示例。",
    "tldr": "本文介绍了CodeCoT和Beyond的学习方法，该方法可以帮助模型在处理任务时从少量特定数据中进行适应。通过链式思维引导，模型可以揭示多步推理过程中的认知过程，并通过自我检查不断优化输出。"
}