{
    "title": "Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)",
    "abstract": "Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w",
    "link": "http://arxiv.org/abs/2308.14306",
    "context": "Title: Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)\nAbstract: Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w",
    "path": "papers/23/08/2308.14306.json",
    "total_tokens": 903,
    "translated_title": "评估大型语言模型对指令的鲁棒性",
    "translated_abstract": "最近，指令微调已成为提升大型语言模型在新任务中零-shot能力的潜在方法。该技术显示出出色的能力，可以提升中等规模的语言模型的性能，有时甚至达到相当于更大模型变体的性能水平。本研究重点研究了经过指令微调的语言模型对已知任务和未知任务的鲁棒性。我们对六个模型进行了探索，包括Alpaca、Vicuna、WizardLM和传统的任务导向模型（Flan-T5-XL/XXL、T0++），以真实世界的关系提取数据集作为案例研究。我们对这些遵循指令的语言模型进行了全面评估，这些模型是基于开放域指令和任务导向指令进行微调的。主要讨论的是它们在处理指令时的性能和鲁棒性。我们观察到，在大多数情况下，模型在处理陌生指令方面的性能往往会受到影响。",
    "tldr": "本论文评估了大型语言模型对指令的鲁棒性。结果表明，指令微调可以提升中等规模模型的性能，并且模型对陌生指令的处理能力有待改进。"
}