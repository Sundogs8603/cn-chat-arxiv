{
    "title": "Composable Function-preserving Expansions for Transformer Architectures. (arXiv:2308.06103v1 [cs.LG])",
    "abstract": "Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.",
    "link": "http://arxiv.org/abs/2308.06103",
    "context": "Title: Composable Function-preserving Expansions for Transformer Architectures. (arXiv:2308.06103v1 [cs.LG])\nAbstract: Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.",
    "path": "papers/23/08/2308.06103.json",
    "total_tokens": 743,
    "translated_title": "Transformer架构的可组合函数保持扩展方法",
    "translated_abstract": "训练最新的神经网络需要大量的计算和时间成本。模型规模被认为是实现和提升最新技术的关键因素。增加神经网络的规模通常需要从头开始，通过随机初始化模型的所有参数，因为这意味着架构参数的变化，不允许从更小的模型中直接传递知识。在这项工作中，我们提出了六个可组合的转换方法，逐渐增加基于Transformer的神经网络的规模，同时保持功能，允许根据需要扩展模型的容量。我们提供了对每个转换的最小初始化约束下精确功能保持的证明。所提出的方法可以通过在训练过程中逐步扩展架构，实现更大和更强大模型的高效培训流程。",
    "tldr": "提出了六个可组合的转换方法，逐渐增加基于Transformer的神经网络的规模，同时保持功能，充分利用现有知识，实现更大和更强大模型的高效培训流程。"
}