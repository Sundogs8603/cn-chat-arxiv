{
    "title": "Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])",
    "abstract": "In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering \"sweet spots\" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m",
    "link": "http://arxiv.org/abs/2308.06017",
    "context": "Title: Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])\nAbstract: In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering \"sweet spots\" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m",
    "path": "papers/23/08/2308.06017.json",
    "total_tokens": 887,
    "translated_title": "优化单GPU训练的基于transformer的机器翻译模型：超参数消融研究",
    "translated_abstract": "在机器翻译任务中，模型复杂性和性能之间的关系通常被认为是线性的，驱动参数数量增加并对多个GPU等计算资源提出要求。为了探索这一假设，本研究通过超参数消融系统地研究了一个基于序列到序列的机器翻译管道在单个NVIDIA A100 GPU上的影响。与预期相反，我们的实验发现具有最多参数的组合未必是最有效的。这一意外的发现促使我们仔细减少参数大小，揭示了能够在单个GPU上训练复杂模型而不损害翻译质量的“甜点”。这些发现展示了超参数选择、模型大小和计算资源需求之间的复杂关系。这项研究的见解对于努力进行机器翻译的改进工作有所贡献。",
    "tldr": "本研究通过消融实验发现，在单个GPU上，参数数量最多的组合并不一定是最有效的，通过减少参数大小可以在不降低翻译质量的情况下训练复杂模型，揭示了超参数选择、模型大小和计算资源需求之间的关系。"
}