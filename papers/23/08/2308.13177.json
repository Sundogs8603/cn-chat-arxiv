{
    "title": "How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v1 [cs.CV])",
    "abstract": "Object detection (OD) in computer vision has made significant progress in recent years, transitioning from closed-set labels to open-vocabulary detection (OVD) based on large-scale vision-language pre-training (VLP). However, current evaluation methods and datasets are limited to testing generalization over object types and referral expressions, which do not provide a systematic, fine-grained, and accurate benchmark of OVD models' abilities. In this paper, we propose a new benchmark named OVDEval, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input. Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Su",
    "link": "http://arxiv.org/abs/2308.13177",
    "context": "Title: How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v1 [cs.CV])\nAbstract: Object detection (OD) in computer vision has made significant progress in recent years, transitioning from closed-set labels to open-vocabulary detection (OVD) based on large-scale vision-language pre-training (VLP). However, current evaluation methods and datasets are limited to testing generalization over object types and referral expressions, which do not provide a systematic, fine-grained, and accurate benchmark of OVD models' abilities. In this paper, we propose a new benchmark named OVDEval, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input. Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Su",
    "path": "papers/23/08/2308.13177.json",
    "total_tokens": 1039,
    "translated_title": "如何评估检测的泛化能力？一种用于全面开放词汇检测的基准测试方法",
    "translated_abstract": "计算机视觉中的目标检测在近年取得了显著的进展，从基于封闭集标签到基于大规模视觉语言预训练的开放词汇检测（OVD）。然而，当前的评估方法和数据集仅限于测试对象类型和引用表达的泛化能力，无法提供OVD模型能力的系统、细粒度和准确的基准。在本文中，我们提出了一个名为OVDEval的新基准测试方法，包括9个子任务，并引入了常识知识、属性理解、位置理解、对象关系理解等方面的评估。数据集被精心创建以提供具有挑战性的负例，考验模型对视觉和语言输入的真正理解。此外，我们还发现了在这些细粒度标签数据集上对模型进行基准测试时普遍使用的平均精确度（AP）指标存在的问题，并提出了一种名为非极大值抑制的新指标。",
    "tldr": "提出了一个名为OVDEval的新的基准测试方法，用于全面开放词汇检测。该方法包括9个子任务，并引入了常识知识、属性理解、位置理解、对象关系理解等方面的评估。数据集被精心创建以提供具有挑战性的负例，考验模型对视觉和语言输入的真正理解。此外，该方法发现了平均精确度（AP）指标在这些细粒度标签数据集上的问题，并提出了一种新指标。",
    "en_tdlr": "A new benchmark named OVDEval is proposed for comprehensive open-vocabulary detection, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset provides challenging examples to test models' true understanding of visual and linguistic input. Additionally, a new metric called Non-Maximum Su is proposed to address the problem with the popular Average Precision (AP) metric on fine-grained label datasets."
}