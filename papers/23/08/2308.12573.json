{
    "title": "Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])",
    "abstract": "Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our appr",
    "link": "http://arxiv.org/abs/2308.12573",
    "context": "Title: Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])\nAbstract: Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our appr",
    "path": "papers/23/08/2308.12573.json",
    "total_tokens": 837,
    "translated_title": "条件核模仿学习在连续状态环境中的应用",
    "translated_abstract": "仿真学习（IL）是在更广泛的强化学习（RL）方法中的重要范例。与大多数RL不同，它不假设回馈奖励的可用性。奖励推断和塑形已知是困难且容易出错的方法，特别是当演示数据来自人类专家时。传统方法如行为克隆和逆向强化学习对估计误差非常敏感，这在连续状态空间问题中尤为严重。与此同时，最先进的IL算法将行为策略学习问题转换为分布匹配问题，通常需要额外的在线交互数据才能发挥作用。本文考虑了在连续状态空间环境中仅基于观察到的行为进行仿真学习的问题，不需要访问转移动力学信息、奖励结构，或者最重要的是不需要与环境进行任何额外交互。",
    "tldr": "本研究以连续状态空间环境为基础，仅凭观察到的行为进行仿真学习，无需访问转移动力学信息、奖励结构，或者任何额外的交互。",
    "en_tdlr": "This study focuses on imitation learning in continuous state space environments based solely on observed behavior, without the need for transition dynamics information, reward structure, or any additional interactions."
}