{
    "title": "A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation. (arXiv:2308.06063v1 [cs.CL])",
    "abstract": "Recent studies have shown that the multi-encoder models are agnostic to the choice of context, and the context encoder generates noise which helps improve the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz, previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder generates noise. Our analysis shows that the context encoder provides sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this c",
    "link": "http://arxiv.org/abs/2308.06063",
    "context": "Title: A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation. (arXiv:2308.06063v1 [cs.CL])\nAbstract: Recent studies have shown that the multi-encoder models are agnostic to the choice of context, and the context encoder generates noise which helps improve the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz, previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder generates noise. Our analysis shows that the context encoder provides sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this c",
    "path": "papers/23/08/2308.06063.json",
    "total_tokens": 900,
    "translated_title": "多编码器的文档级神经机器翻译中上下文编码的案例研究",
    "translated_abstract": "最近的研究表明，多编码器模型对上下文的选择不敏感，并且上下文编码器产生的噪声有助于提高模型的BLEU分数。本文通过在三种不同上下文设置下训练多编码器模型，并评估上下文感知代词翻译测试集来进一步探索这个想法。具体来说，我们在ContraPro测试集上评估模型，以研究不同上下文对代词翻译准确性的影响。结果表明，即使上下文是随机的，该模型在ContraPro测试集上表现良好。我们还分析了源表示以研究上下文编码器是否产生噪声。我们的分析表明，上下文编码器提供了足够的信息来学习语篇级信息。此外，我们观察到将选择的上下文（即前两个句子）混合在一起时。",
    "tldr": "本文研究了多编码器的文档级神经机器翻译中上下文编码的案例，结果表明上下文对代词翻译的准确性影响不大，并且上下文编码器提供了足够的信息来学习语篇级信息。"
}