{
    "title": "Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])",
    "abstract": "Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their ",
    "link": "http://arxiv.org/abs/2308.04375",
    "context": "Title: Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])\nAbstract: Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their ",
    "path": "papers/23/08/2308.04375.json",
    "total_tokens": 935,
    "translated_title": "理解因果解释对于人工智能与人类协作临床决策中信任和依赖的影响",
    "translated_abstract": "人工智能（AI）越来越被考虑用于辅助高风险领域（如健康）中的人类决策。然而，研究人员讨论了一个问题，即人类可能会过度依赖错误的AI模型建议，而非实现人工智能与人类的互补性能。在这项工作中，我们利用显著特征解释和假设性因果解释，使人类能更分析地审查AI建议，以减少对AI的过度依赖，并探讨这些解释对临床决策中的信任和依赖的影响。我们在七位治疗师和十位非专业人士中进行了一项实验，任务是评估中风后幸存者的运动质量，并分析了他们的表现、任务上的一致性水平以及在无解释和有两种AI解释的情况下对AI的依赖。结果显示，具有显著特征和因果解释的AI模型帮助治疗师和非专业人士改善了他们的表现。",
    "tldr": "本研究利用因果解释方法帮助人类分析审查人工智能建议，减少对人工智能的过度依赖，提高了临床决策的质量和表现。"
}