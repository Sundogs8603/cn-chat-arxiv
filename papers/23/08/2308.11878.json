{
    "title": "Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])",
    "abstract": "The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be",
    "link": "http://arxiv.org/abs/2308.11878",
    "context": "Title: Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])\nAbstract: The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be",
    "path": "papers/23/08/2308.11878.json",
    "total_tokens": 822,
    "translated_title": "Cabrita: 弥合外语差距",
    "translated_abstract": "从头训练模型在特定语言或领域中有两个重要目的：i)增强在特定语言或领域背景下的性能，ii)确保有效的标记化。然而，这种方法的主要限制在于相关成本，这些成本可能达到六位数甚至七位数的美元金额，这取决于模型大小和涉及的参数数量。为了克服这个成本挑战，主要解决方案是依赖可用的预训练模型，尽管最近出现了像LLaMA和LLaMA-2模型这样的进展，但对于某些特定领域问题仍然表现低效，或者在涉及对话式记忆资源的场景中无效，因为表示文本所需的标记数量巨大。为了解决这个问题，我们提出了一种名为Cabrita的方法，我们的研究证明，它成功解决了性能和高效标记化的问题，而且成本可承受。",
    "tldr": "Cabrita是一种解决性能和高效标记化问题的方法，以可承受的成本解决了从头训练模型的限制。"
}