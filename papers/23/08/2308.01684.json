{
    "title": "Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed \"CoThought\" pipeline, which efficiently trains smaller \"baby\" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve",
    "link": "http://arxiv.org/abs/2308.01684",
    "context": "Title: Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])\nAbstract: Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed \"CoThought\" pipeline, which efficiently trains smaller \"baby\" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve",
    "path": "papers/23/08/2308.01684.json",
    "total_tokens": 965,
    "translated_title": "Baby's CoThought: 利用大型语言模型提高紧凑模型的推理能力",
    "translated_abstract": "大型语言模型(LLMs)在各种自然语言理解(NLU)任务中展示出出色的性能，主要是由于它们的上下文学习能力。我们提出的\"CoThought\"流水线利用LLMs的CoT提示，高效地训练较小的\"baby\"语言模型(BabyLMs)。我们使用GPT-3.5-turbo对少于100M大小的数据集进行重组，将其转化为面向任务的、可读性强的文本，类似于语言学习者的学校教材。然后，在这个重组后的数据集上，以RoBERTa(Liu等人，2019)的方式对BabyLM进行预训练。在4个基准测试中，我们的BabyLM在10个语言学、NLU和问答任务中的表现优于RoBERTa-base超过3个点，展现出更好的提取上下文信息的能力。这些结果表明，在小型的、由LLM重组的数据上预训练的紧凑语言模型能够更好地理解任务并实现",
    "tldr": "\"Baby's CoThought\" 提出了一种利用大型语言模型重组数据训练紧凑语言模型的方法，经过评估发现，在10个语言学、NLU和问答任务中，BabyLM的表现超过RoBERTa-base超过3个点，展现出更好的上下文信息提取能力。"
}