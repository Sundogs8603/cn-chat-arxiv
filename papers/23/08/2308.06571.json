{
    "title": "ModelScope Text-to-Video Technical Report. (arXiv:2308.06571v1 [cs.CV])",
    "abstract": "This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",
    "link": "http://arxiv.org/abs/2308.06571",
    "context": "Title: ModelScope Text-to-Video Technical Report. (arXiv:2308.06571v1 [cs.CV])\nAbstract: This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",
    "path": "papers/23/08/2308.06571.json",
    "total_tokens": 848,
    "translated_title": "ModelScope文本到视频技术报告",
    "translated_abstract": "本论文介绍了ModelScopeT2V，一种从文本到图像合成模型（即Stable Diffusion）演变而来的文本到视频合成模型。ModelScopeT2V采用时空块来确保一致的帧生成和平滑的运动过渡。该模型在训练和推断过程中能够适应不同的帧数量，适用于图像-文本和视频-文本数据集。ModelScopeT2V结合了三个组件（即VQGAN，文本编码器和去噪UNet），总共包含17亿个参数，其中5亿个参数用于时间能力。该模型在三个评估指标上表现出优越的性能，超过了现有方法。代码和在线演示可在\\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}获取。",
    "tldr": "本论文介绍了ModelScopeT2V，一种从文本到视频合成的模型。该模型采用时空块确保帧生成和运动过渡的一致性，并能适应不同的帧数量。ModelScopeT2V在三个评估指标上表现出优越性能，代码和在线演示可在指定链接获取。",
    "en_tdlr": "This paper introduces ModelScopeT2V, a text-to-video synthesis model. The model incorporates spatio-temporal blocks for consistent frame generation and movement transitions, and can adapt to varying frame numbers. ModelScopeT2V demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and online demo are available at the provided link."
}