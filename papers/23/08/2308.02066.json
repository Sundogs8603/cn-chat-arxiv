{
    "title": "Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])",
    "abstract": "Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networ",
    "link": "http://arxiv.org/abs/2308.02066",
    "context": "Title: Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])\nAbstract: Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networ",
    "path": "papers/23/08/2308.02066.json",
    "total_tokens": 922,
    "translated_title": "通过使用非可学习原语的显式任务路由来减轻多任务学习中的任务干扰",
    "translated_abstract": "多任务学习（MTL）通过利用任务之间的共享信息来学习一个模型来完成多个任务。然而，现有的MTL模型已经被发现存在负面干扰问题。为了减轻任务干扰，已有的努力主要集中在损失/梯度平衡或隐式参数划分上。在本文中，我们提出了ETR-NLP来通过非可学习原语（NLP）和显式任务路由（ETR）的协同组合来减轻任务干扰。我们的关键思想是使用非可学习原语来提取一组多样化的与任务无关的特征，并将它们重新组合成一个共享于所有任务的分支和专门为每个任务保留的显式任务特定分支。非可学习原语和可学习参数的显式解耦为最小化任务干扰提供了所需的灵活性。我们评估了ETR-NLP网络的有效性。",
    "tldr": "本文提出了ETR-NLP模型来减轻多任务学习中的任务干扰，通过使用非可学习原语和显式任务路由的协同组合，在共享分支和任务特定分支中显式地分离可学习参数，以实现任务之间的最小化干扰。",
    "en_tdlr": "This paper proposes ETR-NLP model to mitigate task interference in multi-task learning by using a synergistic combination of non-learnable primitives and explicit task routing, explicitly separating learnable parameters in shared branches and task-specific branches to minimize interference among tasks."
}