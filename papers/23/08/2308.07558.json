{
    "title": "Action Class Relation Detection and Classification Across Multiple Video Datasets. (arXiv:2308.07558v1 [cs.CV])",
    "abstract": "The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performan",
    "link": "http://arxiv.org/abs/2308.07558",
    "context": "Title: Action Class Relation Detection and Classification Across Multiple Video Datasets. (arXiv:2308.07558v1 [cs.CV])\nAbstract: The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performan",
    "path": "papers/23/08/2308.07558.json",
    "total_tokens": 934,
    "translated_title": "跨多个视频数据集检测和分类行动类别关系",
    "translated_abstract": "Meta Video Dataset (MetaVD)提供了主要视频人体动作识别数据集中行动类别之间的关系注释。虽然这些关系注释可以实现数据集扩充，但只适用于MetaVD覆盖的数据集。对于外部数据集要享受相同的好处，需要确定其行动类别与MetaVD中行动类别之间的关系。为了解决这个问题，我们考虑了两个新的机器学习任务：行动类别关系检测和分类。我们提出了一个统一的模型来预测行动类别之间的关系，利用与类别相关的语言和视觉信息。实验结果表明：(i) 针对文本和视频的预训练的最新神经网络模型对高预测性能有贡献，(ii) 基于行动标签文本的关系预测比基于视频更准确，(iii) 通过将两种模态的预测结果进行综合可以进一步提高预测性能",
    "tldr": "本论文提出了一种在多个视频数据集上检测和分类行动类别关系的方法，并通过语言和视觉信息来预测行动类别之间的关系。实验结果表明，预训练的神经网络模型对提高预测性能有贡献，基于行动标签文本的关系预测更准确，通过综合两种模态的预测结果可以进一步改进预测性能。",
    "en_tdlr": "This paper proposes a method to detect and classify the relations between action classes across multiple video datasets, using language and visual information. Experimental results show that pre-trained neural network models contribute to high predictive performance, relation prediction based on action label texts is more accurate than based on videos, and a blending approach can further improve the predictive performance."
}