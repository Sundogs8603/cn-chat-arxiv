{
    "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)",
    "abstract": "With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.",
    "link": "http://arxiv.org/abs/2308.10855",
    "context": "Title: LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)\nAbstract: With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.",
    "path": "papers/23/08/2308.10855.json",
    "total_tokens": 1055,
    "translated_title": "LatEval: 一个带有来自侧思维谜题的不完整信息的交互式LLMs评估基准",
    "translated_abstract": "随着LLMs的不断发展和改进，它们被赋予了令人印象深刻的逻辑推理或纵向思维能力。但它们能否跳出固定模式思考？它们是否具备高超的横向思考能力？在侧思维谜题的基础上，我们提出了一个新颖的评估基准，称为LatEval，它在一个交互式框架中评估模型的横向思考能力。在我们的基准测试中，我们向LLMs提出了两个方面的挑战：模型提出的问题的质量和模型在解决问题时整合信息的能力。我们发现几乎所有的LLMs在交互过程中都很难运用横向思考。例如，即使是最先进的模型GPT-4，在某种程度上也具有优势，但与人类相比仍存在明显差距。这个评估基准为LLMs提供了一个极具挑战性和独特的任务，对于有效的人工智能助理至关重要。",
    "tldr": "LatEval是一个新颖的LLMs评估基准，通过侧思维谜题挑战模型的横向思考能力，在交互过程中考验模型提出问题的质量和整合信息解决问题的能力。研究发现，几乎所有LLMs在横向思考方面存在困难，即使是最先进的GPT-4模型相比人类也有明显差距。这个基准测试对于有效的AI助理至关重要。",
    "en_tdlr": "LatEval is a novel benchmark for evaluating LLMs, challenging their lateral thinking abilities through lateral thinking puzzles. The benchmark assesses the quality of questions posed by the models and their ability to integrate information for problem-solving. The research finds that almost all LLMs struggle with lateral thinking, and even the most advanced model, GPT-4, has a noticeable gap compared to human performance. This evaluation benchmark is crucial for the development of effective AI assistants."
}