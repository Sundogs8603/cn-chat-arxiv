{
    "title": "FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])",
    "abstract": "Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce",
    "link": "http://arxiv.org/abs/2308.01006",
    "context": "Title: FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])\nAbstract: Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce",
    "path": "papers/23/08/2308.01006.json",
    "total_tokens": 909,
    "translated_title": "FusionAD: 自动驾驶预测和规划任务的多模态融合",
    "translated_abstract": "在自动驾驶感知任务中，构建一个多模态多任务神经网络以实现准确和稳健的性能已成为铁板一块的标准。然而，利用来自多个传感器的数据来联合优化预测和规划任务仍然几乎未被探索。本文提出了FusionAD，据我们所知，这是第一个将来自两个最关键传感器相机和激光雷达的信息融合起来超越感知任务的统一框架。具体来说，我们首先构建了一个基于转换器的多模态融合网络，以有效地产生基于融合的特征。然后，与基于相机的端到端方法UniAD相比，我们建立了一个融合辅助的模态感知预测和状态感知规划模块，称为FMSPnP，充分利用多模态特征的优势。我们在常用的nuScenes数据集上进行了大量实验，结果表明我们的FusionAD达到了最先进的性能，并优于基准线平均15%的性能。",
    "tldr": "FusionAD是第一个将来自相机和激光雷达的信息融合起来用于自动驾驶预测和规划任务的统一框架，在常用数据集上的实验中达到了最先进的性能。",
    "en_tdlr": "FusionAD is the first unified framework that fuses information from both camera and LiDAR sensors for prediction and planning tasks in autonomous driving, achieving state-of-the-art performance on a commonly used benchmark dataset."
}