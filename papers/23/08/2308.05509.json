{
    "title": "On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem. (arXiv:2308.05509v1 [cs.LG])",
    "abstract": "This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.",
    "link": "http://arxiv.org/abs/2308.05509",
    "context": "Title: On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem. (arXiv:2308.05509v1 [cs.LG])\nAbstract: This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.",
    "path": "papers/23/08/2308.05509.json",
    "total_tokens": 969,
    "translated_title": "对ReLU深度神经网络的最佳表达能力及其在用科尔莫哥洛夫叠加定理进行逼近中的应用的研究",
    "translated_abstract": "本文致力于研究ReLU深度神经网络（DNNs）的最佳表达能力及其在通过科尔莫哥洛夫叠加定理进行逼近的应用。我们首先构造性地证明了任何在$[0,1]$上由$O(N^2L)$个线性分段构成的连续函数都可以由具有$L$个隐藏层和每层$N$个神经元的ReLU DNNs表示。随后，我们通过研究ReLU DNNs的破碎容量，证明了这种构造在DNNs的参数计数方面是最优的。此外，通过引用科尔莫哥洛夫叠加定理，我们在处理高维空间中的连续函数时，实现了ReLU DNNs任意宽度和深度的提升逼近速率。",
    "tldr": "本文研究了ReLU DNNs的最佳表达能力及其在逼近中的应用。我们证明了ReLU DNNs可以表示任何在$[0,1]$上由$O(N^2L)$个线性分段构成的连续函数，并且这种表示方式是最优的。此外，通过科尔莫哥洛夫叠加定理，我们实现了在高维空间中处理连续函数时ReLU DNNs的提升逼近速率。"
}