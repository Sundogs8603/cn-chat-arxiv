{
    "title": "Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])",
    "abstract": "We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \\citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te",
    "link": "http://arxiv.org/abs/2308.15470",
    "context": "Title: Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])\nAbstract: We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \\citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te",
    "path": "papers/23/08/2308.15470.json",
    "total_tokens": 913,
    "translated_title": "强化学习中的政策组合通过多目标政策优化",
    "translated_abstract": "我们通过使用相关的预先存在的教师策略，使强化学习智能体能够学习成功的行为策略。教师策略被引入作为目标，除了任务目标以外，在多目标政策优化的设置中。我们使用多目标最大后验政策优化算法，展示了教师策略能够加速学习的过程，尤其是在缺少形状奖励的情况下。在具有连续观察和行动空间的两个域中，我们的智能体成功地按顺序和并行地组合教师策略，并且还能够进一步扩展教师的策略以解决任务。根据任务和教师的指定组合，教师可能自然地限制智能体的最终性能。智能体需要遵守教师策略的程度由超参数决定，这些超参数确定了教师策略的影响程度。",
    "tldr": "通过多目标政策优化方法，我们将预先存在的教师策略引入到强化学习中，证明了教师策略在无形状奖励下能够加速学习过程。我们的智能体成功地组合教师策略并扩展教师的策略以解决任务。"
}