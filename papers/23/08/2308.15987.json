{
    "title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])",
    "abstract": "In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt",
    "link": "http://arxiv.org/abs/2308.15987",
    "context": "Title: FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])\nAbstract: In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt",
    "path": "papers/23/08/2308.15987.json",
    "total_tokens": 951,
    "translated_title": "FPTQ：面向大型语言模型的细粒度训练后量化",
    "translated_abstract": "在大规模语言模型的时代中，庞大的参数大小给部署带来了巨大的挑战。作为一种流行的压缩技术，量化已经成为解决这个问题的主流实践，主要集中在两种方案W8A8和W4A16（即这两种位宽的权重和激活）。在本研究中，我们提出了一种新颖的W4A8训练后量化方法，用于现有的开放源码语言模型，结合了这两种方案的优点。因此，我们可以利用4位权重量化的I/O利用率优势和8位矩阵计算的加速效果。然而，W4A8面临着明显的性能下降问题。为了解决这个问题，我们采用了逐层激活量化策略，对于最棘手的层使用了一种新颖的对数均衡方法，并将其与细粒度权重量化相结合。在没有额外的调整的情况下，我们消除了进一步微调的必要性。",
    "tldr": "本研究提出了一种面向大型语言模型的细粒度训练后量化方法，结合了W4A8和W8A8两种方案的优点，通过逐层激活量化和细粒度权重量化来解决性能下降的问题。",
    "en_tdlr": "This paper proposes a fine-grained post-training quantization method for large language models, combining the advantages of W4A8 and W8A8. It addresses the performance degradation issue by using layerwise activation quantization and fine-grained weight quantization."
}