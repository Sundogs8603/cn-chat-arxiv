{
    "title": "Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])",
    "abstract": "Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.",
    "link": "http://arxiv.org/abs/2308.08886",
    "context": "Title: Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])\nAbstract: Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.",
    "path": "papers/23/08/2308.08886.json",
    "total_tokens": 821,
    "translated_title": "深度学习的双高斯牛顿方向",
    "translated_abstract": "受高斯牛顿方法的启发，我们研究了利用深度学习目标的结构，即由凸损失函数和非线性网络组成，以导出比随机梯度更好的方向预言，基于部分线性化的思想。与以前的工作不同，我们提出通过它们的对偶形式来计算这样的方向预言，从而获得计算上的好处和新的见解。我们证明了由此产生的预言定义了可以用作现有优化算法中随机梯度的替代品的下降方向。我们通过实验证明了使用对偶形式的优势以及涉及计算这些预言的计算权衡。",
    "tldr": "本文研究了深度学习目标的结构特点，提出了基于双重高斯牛顿方向预言的方法。通过对偶形式计算这些预言，得到了既具有计算好处又具有新见解的结果。通过实验证明了这些预言作为随机梯度的下降方向的优势，并研究了计算上的权衡。",
    "en_tdlr": "This paper studies the structure of deep learning objectives and proposes a method based on dual Gauss-Newton direction oracles. By computing these oracles in their dual formulation, both computational benefits and new insights are obtained. Empirical results demonstrate the advantage of using these oracles as descent directions instead of stochastic gradients, as well as the computational trade-offs involved in their computation."
}