{
    "title": "PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. (arXiv:2308.04791v1 [cs.LG])",
    "abstract": "Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-th",
    "link": "http://arxiv.org/abs/2308.04791",
    "context": "Title: PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. (arXiv:2308.04791v1 [cs.LG])\nAbstract: Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-th",
    "path": "papers/23/08/2308.04791.json",
    "total_tokens": 894,
    "translated_title": "PETformer: 通过增强占位符的Transformer实现长期时间序列预测",
    "translated_abstract": "最近，基于Transformer的模型在长期时间序列预测（LTSF）任务中显示出卓越的性能，这归因于它们能够建模长期依赖关系。然而，将Transformer应用于LTSF任务的有效性仍然存在争议，尤其是最近的研究表明简单的线性模型可以胜过许多基于Transformer的方法。这表明，在LTSF中应用Transformer存在局限性。因此，本文研究了将Transformer应用于LTSF时的三个关键问题：时间连续性，信息密度和多通道关系。因此，我们提出了三种创新性解决方案，包括占位符增强技术（PET），长子序列划分（LSD）和多通道分离与交互（MSI），共同构成了一个名为PETformer的新模型。这三个关键设计引入了适合LTSF任务的先验偏差。大量实验证明PETformer实现了最先进的性能。",
    "tldr": "PETformer是一个创新的模型，通过引入占位符增强技术，长子序列划分和多通道分离与交互的方法解决了将Transformer应用于长期时间序列预测时的关键问题。实验证明PETformer实现了最先进的性能。",
    "en_tdlr": "PETformer is an innovative model that addresses key issues when applying Transformer to long-term time series forecasting (LTSF) through the introduction of Placeholder Enhancement Technique, Long Sub-sequence Division, and Multi-channel Separation and Interaction. Experiments show that PETformer achieves state-of-the-art performance."
}