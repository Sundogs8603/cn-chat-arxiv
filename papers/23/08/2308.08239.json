{
    "title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])",
    "abstract": "We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative \"memorization-retrieval-response\" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.",
    "link": "http://arxiv.org/abs/2308.08239",
    "context": "Title: MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])\nAbstract: We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative \"memorization-retrieval-response\" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.",
    "path": "papers/23/08/2308.08239.json",
    "total_tokens": 824,
    "translated_title": "MemoChat: 通过调整LLMs使用备忘录以保持一致性的长距离开放领域对话",
    "translated_abstract": "我们提出了MemoChat，一个用于优化指令的流水线，使大规模语言模型（LLMs）能够有效地使用自行组织的备忘录来保持一致的长距离开放领域对话。我们通过迭代的“记忆-检索-响应”循环展示了一个长距离的开放领域对话。这要求我们为每个不同的阶段精心设计定制的调优指令。这些指令是从一系列公共数据集中重建的，以教导LLMs记忆和检索过去的对话，并通过结构化备忘录提高未来对话的一致性。我们邀请专家手动注释一个用于评估长距离对话一致性的测试集。在涉及开源和可访问API的聊天机器人的三种测试场景上进行的实验证实了MemoChat的有效性，它超越了强基线。",
    "tldr": "MemoChat提出了一种用于调优指令的流程，通过让大型语言模型使用备忘录来保持对话一致性。实验证实了其有效性。",
    "en_tdlr": "MemoChat proposes a pipeline for refining instructions to enable large language models to use memos for consistent long-range open-domain conversations. Experiments verify its efficacy."
}