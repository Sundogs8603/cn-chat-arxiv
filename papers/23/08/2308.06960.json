{
    "title": "Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks",
    "abstract": "arXiv:2308.06960v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs t",
    "link": "https://arxiv.org/abs/2308.06960",
    "context": "Title: Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks\nAbstract: arXiv:2308.06960v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs t",
    "path": "papers/23/08/2308.06960.json",
    "total_tokens": 830,
    "translated_title": "为图级任务调优预训练图神经网络的搜索",
    "translated_abstract": "最近，图神经网络（GNNs）在许多与图相关的任务中展现了前所未有的成功。然而，与其他神经网络一样，GNN面临着标签稀缺的问题。因此，最近的工作尝试在大规模未标记的图上预训练GNN，并将从未标记的图中获得的知识适应到目标下游任务中。该适应通常通过使用有限数量的标记数据对预训练的GNN进行微调来实现。尽管微调的重要性，当前的GNN预训练工作往往忽视了设计一个良好的微调策略以更好地利用转移的知识并提高下游任务的性能。只有少数工作开始研究预训练GNN的更好微调策略。但他们的设计要么有很强的假设，要么忽视了各种下游数据集的数据感知问题。因此，我们的目标是为预训练的GNN设计更好的微调策略。",
    "tldr": "设计更好的微调策略以在下游任务中更好地利用转移的知识并提高性能。",
    "en_tdlr": "Designing a better fine-tuning strategy to better leverage transferred knowledge and improve performance on downstream tasks."
}