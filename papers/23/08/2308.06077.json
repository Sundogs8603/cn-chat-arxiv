{
    "title": "Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])",
    "abstract": "Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w",
    "link": "http://arxiv.org/abs/2308.06077",
    "context": "Title: Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])\nAbstract: Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w",
    "path": "papers/23/08/2308.06077.json",
    "total_tokens": 894,
    "translated_title": "飞拍或大炮？通过元模型选择经济有效的语言模型",
    "translated_abstract": "生成式语言模型在数据科学领域中变得无处不在。对于各种任务，可以将输入作为自然语言提示，通过LM的输出来提取解决方案。LM的性能随着模型大小的增加而不断提高，但同时查询越来越大的模型的经济成本也在增加。然而，不是所有的输入都很难：有些输入需要更大的LM才能获得令人满意的解决方案，而对于其他输入，较小的LM就足够了。基于这个事实，我们设计了一个经济有效的语言模型选择框架（CELMOC）。给定一组输入和一组候选LM，CELMOC根据所谓的元模型聪明地将每个输入分配给一个在该输入上预测表现良好的LM，以期在低成本下实现高整体性能。用户可以灵活调整成本与性能的权衡。选项包括，最大化总体性能（或处理输入的数量）等。",
    "tldr": "本文提出了一种经济有效的语言模型选择框架（CELMOC），通过元模型预测在不同输入上表现良好的语言模型，从而在低成本下实现高整体性能。"
}