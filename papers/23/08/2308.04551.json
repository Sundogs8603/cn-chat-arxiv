{
    "title": "Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining. (arXiv:2308.04551v1 [eess.IV])",
    "abstract": "Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images. In this work, we explore contrastive and pretext task-based self-supervised pretr",
    "link": "http://arxiv.org/abs/2308.04551",
    "context": "Title: Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining. (arXiv:2308.04551v1 [eess.IV])\nAbstract: Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images. In this work, we explore contrastive and pretext task-based self-supervised pretr",
    "path": "papers/23/08/2308.04551.json",
    "total_tokens": 879,
    "translated_title": "仅使用自监督预训练改善噪声标签下的医学图像分类",
    "translated_abstract": "噪声标签会对基于深度学习的监督图像分类性能造成负面影响，因为模型可能会过度拟合噪声并学习到有损的特征提取器。使用对比自监督预训练权重来初始化模型已被证明可以减少特征损坏并提高分类性能。然而，目前尚未有研究探索1）其他自监督方法（如预训练任务）如何影响噪声标签下的学习，以及2）在噪声标签设置下，仅使用自监督预训练方法对医学图像的影响。医学图像往往具有较小的数据集和微小的类间变化，需要人类专业知识来保证正确分类。因此，尚不清楚改善噪声标签下自然图像数据集学习的方法是否也对医学图像有帮助。在本研究中，我们探索了对比和预训练任务为基础的自监督预训练方法。",
    "tldr": "本研究旨在探索在噪声标签下仅使用自监督预训练方法，以改善医学图像分类。通过对比和预训练任务为基础的自监督方法，我们发现这些方法在医学图像上也具有改进学习效果的潜力。"
}