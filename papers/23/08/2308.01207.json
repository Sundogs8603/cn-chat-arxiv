{
    "title": "BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])",
    "abstract": "Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves ",
    "link": "http://arxiv.org/abs/2308.01207",
    "context": "Title: BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])\nAbstract: Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves ",
    "path": "papers/23/08/2308.01207.json",
    "total_tokens": 882,
    "translated_title": "BiERL: 一种通过双层优化实现的元进化强化学习框架",
    "translated_abstract": "进化强化学习算法最近引起了人们的关注，因为它们能够处理复杂的强化学习问题，具有高并行性，但是在不仔细调整超参数（即元参数）的情况下，往往会面临不足的探索或模型崩溃的问题。在本文中，我们提出了一个通用的元进化强化学习框架，通过双层优化（BiERL）来同时更新超参数和训练强化学习模型，从而免去了在模型部署之前需要先有领域知识或昂贵优化过程的需求。我们设计了一个优雅的元级架构，将内部级别的进化经验嵌入到信息丰富的种群表示中，并引入了一个简单可行的元级适应度函数评估方法，以提高学习效率。我们在MuJoCo和Box2D任务上进行了大量实验，验证了作为一个通用框架，BiERL优于各种基线方法，并且持续改进。",
    "tldr": "BiERL是一个通用的元进化强化学习框架，通过双层优化来同时更新超参数和训练强化学习模型，从而提高学习效率和性能。",
    "en_tdlr": "BiERL is a general meta evolutionary reinforcement learning framework that improves learning efficiency and performance by jointly updating hyperparameters and training the reinforcement learning model through bilevel optimization."
}