{
    "title": "Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])",
    "abstract": "Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method ",
    "link": "http://arxiv.org/abs/2308.13278",
    "context": "Title: Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])\nAbstract: Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method ",
    "path": "papers/23/08/2308.13278.json",
    "total_tokens": 796,
    "translated_title": "将LLMs和Decision Transformers集成到语言驱动的生成质量多样性中",
    "translated_abstract": "质量多样性是一种用于解决强化学习和控制领域问题的随机优化分支，其目的是构建表现良好且在行为空间上具有多样性的政策/技能库。这样的存档通常由有限数量的反应代理组成，每个代理都与唯一的行为描述符相关联，而在粗略离散化的空间之外实例化行为描述符并不直观。虽然最近有一些工作提出了解决这个问题的方法，但生成的轨迹在目标行为描述符规定之外很难进行定制。我们提出在具有静态场景元素的环境中，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来共同解决这些问题。",
    "tldr": "本文提出了一种将LLMs和Decision Transformers集成到语言驱动的生成质量多样性问题中的方法，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来解决问题。"
}