{
    "title": "Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. (arXiv:2308.09595v2 [cs.AI] UPDATED)",
    "abstract": "Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, prior heuristic-based diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent polic",
    "link": "http://arxiv.org/abs/2308.09595",
    "context": "Title: Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. (arXiv:2308.09595v2 [cs.AI] UPDATED)\nAbstract: Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, prior heuristic-based diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent polic",
    "path": "papers/23/08/2308.09595.json",
    "total_tokens": 921,
    "translated_title": "最小覆盖集合用于训练鲁棒自组织团队协作代理",
    "translated_abstract": "由于合作伙伴可能采用各种不同的合作约定，与未知的代理和人类合作具有重大挑战。现有的自组织团队协作（AHT）方法通过训练代理与通过最大化特定多样性度量获得的多样的队友策略群进行合作，从而解决了这一挑战。然而，先前基于启发式的多样性度量并不总是在所有合作问题中最大化代理的鲁棒性。在这项工作中，我们首先提出最大化AHT代理的鲁棒性需要它模拟最小覆盖集（MCS）中的策略，即对环境中的任何合作伙伴策略的最优响应策略集合。然后，我们引入了L-BRDiv算法，该算法生成一组队友策略，用于AHT训练时鼓励代理模拟MCS中的策略。L-BRDiv通过解决约束优化问题来共同训练AHT训练的队友策略，以及近似AHT代理策略。",
    "tldr": "这篇论文提出了最小覆盖集合（MCS）概念，通过训练代理模拟MCS中的最佳策略，提高了自组织团队协作代理的鲁棒性。",
    "en_tdlr": "This paper introduces the concept of a minimum coverage set (MCS) and enhances the robustness of ad hoc teamwork agents by training them to emulate the best policies from the MCS."
}