{
    "title": "ReLU and Addition-based Gated RNN. (arXiv:2308.05629v1 [cs.LG])",
    "abstract": "We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while signifi",
    "link": "http://arxiv.org/abs/2308.05629",
    "context": "Title: ReLU and Addition-based Gated RNN. (arXiv:2308.05629v1 [cs.LG])\nAbstract: We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while signifi",
    "path": "papers/23/08/2308.05629.json",
    "total_tokens": 869,
    "translated_title": "ReLU和基于加法的门控循环神经网络",
    "translated_abstract": "我们将传统循环门的乘法和Sigmoid函数替换为加法和ReLU激活。该机制旨在以较低的计算成本来维护序列处理的长期记忆，从而在受限硬件上实现更高效的执行或更大型的模型。具有LSTM和GRU等门控机制的循环神经网络在学习序列数据方面取得了广泛成功，因为它们能够捕捉长期依赖关系。传统上，基于当前输入和先前状态历史的更新分别与动态权重相乘，并组合计算出下一个状态。然而，乘法在某些硬件架构或替代算术系统（如同态加密）中可能具有较高的计算开销。实验证明，这种新型门控机制可以对标准的合成序列学习任务捕捉到长期依赖关系。",
    "tldr": "这种新型的门控机制将循环神经网络中传统门的乘法和Sigmoid函数替换为加法和ReLU激活函数，以降低计算成本，并在受限硬件上实现更高效的执行或更大型的模型。通过实验证明，该机制能够捕捉到序列数据中的长期依赖关系。"
}