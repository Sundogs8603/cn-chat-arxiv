{
    "title": "Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])",
    "abstract": "The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it ",
    "link": "http://arxiv.org/abs/2308.08234",
    "context": "Title: Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])\nAbstract: The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it ",
    "path": "papers/23/08/2308.08234.json",
    "total_tokens": 866,
    "translated_title": "在自然语言处理中使用基于Transformer的多任务学习的挑战和机会：一项调研",
    "translated_abstract": "自然语言处理（NLP）模型在各个行业中的广泛应用导致从训练到在生产中运行这些模型的机器学习系统需要有效处理。然而，使用基于Transformer的预训练语言模型进行训练、部署和更新多个模型可能复杂、昂贵且耗时，特别是使用多任务学习（MTL）作为改进效率和性能的方法。本调研首先概述了NLP中基于Transformer的MTL方法。然后，我们讨论了在典型的机器学习生命周期中使用MTL方法面临的挑战和机会，重点关注数据工程、模型开发、部署和监控阶段的挑战。本项调研集中于基于Transformer的MTL架构，并据我们所知是首创的。",
    "tldr": "本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。"
}