{
    "title": "Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks. (arXiv:2308.04909v1 [cs.CR])",
    "abstract": "This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN). Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms. The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process. The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences. Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed. The games are played twice; first, without an active causative attack and se",
    "link": "http://arxiv.org/abs/2308.04909",
    "context": "Title: Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks. (arXiv:2308.04909v1 [cs.CR])\nAbstract: This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN). Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms. The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process. The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences. Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed. The games are played twice; first, without an active causative attack and se",
    "path": "papers/23/08/2308.04909.json",
    "total_tokens": 975,
    "translated_title": "软件定义网络中对抗性深度强化学习在网络安全中的应用",
    "translated_abstract": "本文研究了在软件定义网络（Software Defined Networks，SDN）中，利用自主攻击方法在深度强化学习（Deep Reinforcement Learning，DRL）中训练更加鲁棒的智能体的影响，探讨了将对抗性学习应用于DRL的自主安全性。比较了两种算法：Double Deep Q-Networks（DDQN）和Neural Episodic Control to Deep Q-Network（NEC2DQN或N2D）。攻击者对环境具有完全的可见性，并且可以利用状态操作进行因果攻击，试图破坏学习过程。攻击实施在白盒环境中进行，攻击者可以访问防御者的模型和经验。进行了两轮游戏：第一轮游戏中，DDQN是防御者，N2D是攻击者；第二轮游戏中，角色互换。两轮游戏分别进行了两次，第一次没有主动的因果攻击，第二次进行了有序的因果攻击。",
    "tldr": "本文研究了在软件定义网络中利用对抗性学习来训练更加鲁棒的深度强化学习智能体，探讨了两种算法的差异。攻击者利用因果攻击试图破坏学习过程。游戏中进行了有序的因果攻击。",
    "en_tdlr": "This paper focuses on leveraging adversarial learning in Software Defined Networks (SDN) to train more robust agents using Deep Reinforcement Learning (DRL). The paper compares two algorithms and explores the impact of causative attacks on the learning process."
}