{
    "title": "Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])",
    "abstract": "Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-gra",
    "link": "http://arxiv.org/abs/2308.16271",
    "context": "Title: Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])\nAbstract: Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-gra",
    "path": "papers/23/08/2308.16271.json",
    "total_tokens": 813,
    "translated_title": "最简白盒Transformer中的分割出现现象",
    "translated_abstract": "最近，用于视觉任务的Transformer模型已被证明在诸多下游应用（如分割和检测）中非常有效。先前的研究表明，通过使用DINO等自监督方法训练的视觉Transformer（ViTs）出现了分割特性，但通过在监督分类任务上训练的模型却没有出现这种特性。本研究探究了分割是否仅通过复杂的自监督学习机制在基于Transformer的模型中出现，或者通过适当设计模型架构可以在更广泛的条件下实现相同的出现。通过广泛的实验结果，我们证明了当采用CRATE这种被称为白盒Transformer的架构时，该架构明确地对数据分布中的低维结构进行建模，并通过最简化的监督训练方法已经实现了整体和局部层面的分割特性的出现。",
    "tldr": "本研究表明，在使用CRATE架构的最简监督训练方法下，Transformer模型可以实现分割特性的出现，无需复杂的自监督学习机制。"
}