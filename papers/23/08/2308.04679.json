{
    "title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have shown outstanding performance across wide range of downstream tasks. This competency is attributed to their substantial parameter size and pre-training on extensive corpus. Moreover, LLMs have exhibited enhanced reasoning capabilities in tackling complex reasoning tasks, owing to the utilization of a method named ``Chain-of-Thought (CoT) prompting''. This method is designed to generate intermediate reasoning steps that guide the inference of the final answer. However, it is essential to highlight that these advanced reasoning abilities appear to emerge in models with a minimum of 10 billion parameters, thereby limiting its efficacy in situations where computational resources are constrained. In this paper, we investigate the possibility of transferring the reasoning capabilities of LLMs to smaller models via knowledge distillation. Specifically, we propose Sci-CoT, a two-stage framework that separates the processes of generating rationales and inferrin",
    "link": "http://arxiv.org/abs/2308.04679",
    "context": "Title: Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have shown outstanding performance across wide range of downstream tasks. This competency is attributed to their substantial parameter size and pre-training on extensive corpus. Moreover, LLMs have exhibited enhanced reasoning capabilities in tackling complex reasoning tasks, owing to the utilization of a method named ``Chain-of-Thought (CoT) prompting''. This method is designed to generate intermediate reasoning steps that guide the inference of the final answer. However, it is essential to highlight that these advanced reasoning abilities appear to emerge in models with a minimum of 10 billion parameters, thereby limiting its efficacy in situations where computational resources are constrained. In this paper, we investigate the possibility of transferring the reasoning capabilities of LLMs to smaller models via knowledge distillation. Specifically, we propose Sci-CoT, a two-stage framework that separates the processes of generating rationales and inferrin",
    "path": "papers/23/08/2308.04679.json",
    "total_tokens": 868,
    "translated_title": "Sci-CoT: 利用大型语言模型改进小型科学问答中的知识蒸馏",
    "translated_abstract": "大型语言模型(LLMs)在广泛的下游任务中展现出了出色的性能。这种能力归功于它们庞大的参数规模和对大量语料库的预训练。此外，LLMs展现出增强的推理能力，能够应对复杂的推理任务，这归功于一种名为\"思维链 (CoT)提示\"的方法。该方法旨在生成引导最终答案推理的中间推理步骤。然而，需要强调的是，这些先进的推理能力似乎只在具有至少100亿参数的模型中出现，从而限制了其在计算资源有限的情况下的有效性。在本文中，我们探讨了通过知识蒸馏将LLMs的推理能力转移到较小模型的可能性。具体来说，我们提出了Sci-CoT，一个两阶段的框架，分离了生成理由和推理的过程。",
    "tldr": "本论文研究了通过知识蒸馏将大型语言模型(LLMs)的推理能力转移到较小模型的可能性，提出了Sci-CoT框架，分离了生成理由和推理的过程。"
}