{
    "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content. (arXiv:2308.13768v1 [cs.CL])",
    "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisat",
    "link": "http://arxiv.org/abs/2308.13768",
    "context": "Title: Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content. (arXiv:2308.13768v1 [cs.CL])\nAbstract: In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisat",
    "path": "papers/23/08/2308.13768.json",
    "total_tokens": 946,
    "translated_title": "通过对语言模型进行敌对微调：针对问题内容生成和检测的迭代优化方法",
    "translated_abstract": "本文采用一种新颖的双阶段优化技术，使用对抗性微调来应对大型语言模型（LLMs）中意外生成有害内容的挑战。我们的方法包括对抗模型和判别模型两个阶段，对抗模型被微调用于生成潜在有害提示，而判别模型则通过迭代优化来识别这些提示。通过对抗循环，两个模型在提示阶段争相超越对方，生成包含丰富示例的数据集，然后用于微调。这种迭代应用提示和微调的方法使得持续的改进和性能提升成为可能。我们通过在一个包含GPT-4未检测到的问题提示和一些有争议但无问题的提示的数据集上进行分类准确度的评估来验证我们的方法的性能。结果显示在这个具有挑战性的数据集上，判别模型的分类准确度有了显著提升。",
    "tldr": "本研究提出了一种新的双阶段优化技术，使用对抗性微调来解决大型语言模型中意外生成有害内容的问题。通过迭代的提示和微调，实现了持续的改进和性能提升，并在具有挑战性的数据集上展示了显著的分类准确度提升。",
    "en_tdlr": "This paper presents a novel dual-stage optimization technique called adversarial fine-tuning to address the problem of unintended harmful content generation in large language models. Through iterative prompting and fine-tuning, continuous improvement and performance enhancement are achieved, resulting in significant increase in classification accuracy on a challenging dataset."
}