{
    "title": "Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])",
    "abstract": "Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal proces",
    "link": "http://arxiv.org/abs/2308.02464",
    "context": "Title: Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])\nAbstract: Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal proces",
    "path": "papers/23/08/2308.02464.json",
    "total_tokens": 876,
    "translated_title": "通过RNNs实现线性时不变系统的通用逼近：随机性在储备计算中的作用",
    "translated_abstract": "循环神经网络(RNNs)以相对温和和普适的条件被认为是动态系统的通用近似器，使其成为处理时间信息的良好工具。然而，RNNs通常受到标准RNN训练中梯度消失和爆炸的问题的影响。储备计算(RC)是一种特殊的RNN，其中的循环权重是随机化并留在未经训练状态，它被引入用于克服这些问题，并在诸如自然语言处理和无线通信等领域展现出卓越的实证性能，特别是在训练样本极其有限的情况下。然而，支持这种观察到的性能的理论基础并未以相同的速度完全发展。在这项工作中，我们展示了RNNs可以提供线性时不变(LTI)系统的通用逼近。具体而言，我们展示了RC可以对一般LTI系统进行全面逼近。我们提出了一个明确的信号处理方法。",
    "tldr": "通过随机性的储备计算，RNNs可以通用逼近线性时不变系统，这一观察到的性能在理论上得到了支持。",
    "en_tdlr": "Through the randomness of reservoir computing, RNNs can universally approximate linear time-invariant systems, and this observed performance is supported theoretically."
}