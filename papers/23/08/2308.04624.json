{
    "title": "Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])",
    "abstract": "Autonomous conversational agents, i.e. chatbots, are becoming an increasingly common mechanism for enterprises to provide support to customers and partners. In order to rate chatbots, especially ones powered by Generative AI tools like Large Language Models (LLMs) we need to be able to accurately assess their performance. This is where chatbot benchmarking becomes important. In this paper, we propose the use of a novel benchmark that we call the E2E (End to End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy and usefulness of the answers provided by chatbots, especially ones powered by LLMs. We evaluate an example chatbot at different levels of sophistication based on both our E2E benchmark, as well as other available metrics commonly used in the state of art, and observe that the proposed benchmark show better results compared to others. In addition, while some metrics proved to be unpredictable, the metric associated with the E2E benchmark, which uses cosi",
    "link": "http://arxiv.org/abs/2308.04624",
    "context": "Title: Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])\nAbstract: Autonomous conversational agents, i.e. chatbots, are becoming an increasingly common mechanism for enterprises to provide support to customers and partners. In order to rate chatbots, especially ones powered by Generative AI tools like Large Language Models (LLMs) we need to be able to accurately assess their performance. This is where chatbot benchmarking becomes important. In this paper, we propose the use of a novel benchmark that we call the E2E (End to End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy and usefulness of the answers provided by chatbots, especially ones powered by LLMs. We evaluate an example chatbot at different levels of sophistication based on both our E2E benchmark, as well as other available metrics commonly used in the state of art, and observe that the proposed benchmark show better results compared to others. In addition, while some metrics proved to be unpredictable, the metric associated with the E2E benchmark, which uses cosi",
    "path": "papers/23/08/2308.04624.json",
    "total_tokens": 891,
    "translated_title": "基于LLM技术的聊天机器人的基准测试：方法和指标",
    "translated_abstract": "自主对话代理，即聊天机器人，正成为企业为客户和合作伙伴提供支持的越来越常见的机制。为了评估特别是由大型语言模型（LLM）驱动的聊天机器人的表现，我们需要能够准确评估其性能。这就是聊天机器人基准测试的重要性所在。在本文中，我们提出了一种称为E2E（端到端）基准测试的新型基准测试，并展示了如何使用E2E基准测试来评估由LLM驱动的聊天机器人提供的答案的准确性和实用性。我们根据我们的E2E基准测试以及其他常用的现有指标评估了一个示例聊天机器人的不同复杂程度，并观察到所提出的基准测试相比其他指标展现出更好的结果。此外，一些指标被证明是不可预测的，而与E2E基准测试相关的指标使用了余弦相似度。",
    "tldr": "本文提出了一种新型的E2E基准测试，用于评估由LLM驱动的聊天机器人的准确性和实用性，相比其他指标，该基准测试展现出更好的结果。",
    "en_tdlr": "This paper proposes a novel E2E benchmark for evaluating the accuracy and usefulness of chatbots powered by LLMs, showing better results compared to other metrics."
}