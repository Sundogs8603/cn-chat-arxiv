{
    "title": "AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])",
    "abstract": "Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher ",
    "link": "http://arxiv.org/abs/2308.00688",
    "context": "Title: AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])\nAbstract: Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher ",
    "path": "papers/23/08/2308.00688.json",
    "total_tokens": 906,
    "translated_title": "AnyLoc:朝着通用的视觉地点识别迈进",
    "translated_abstract": "视觉地点识别(VPR)对于机器人定位至关重要。迄今为止，最有效的VPR方法是针对特定环境和任务设计的：虽然它们在结构化环境（主要是城市驾驶）中表现出强大的性能，但在非结构化环境中它们的性能严重下降，使得大多数方法无法在真实世界中稳健部署。在这项工作中，我们开发了一种通用的VPR解决方案——一种在各种结构化和非结构化环境（城市、户外、室内、空中、水下和地下环境）中均可运行的技术，而无需重新训练或微调。我们证明了从现成的自监督模型中获得的通用特征表示是构建这种通用VPR解决方案的理想基础。通过将这些派生特征与无监督特征聚合相结合，我们的方法套件AnyLoc能够实现高达4倍的显著提高",
    "tldr": "AnyLoc实现了一个通用的视觉地点识别(VPR)解决方案，能够在各种结构化和非结构化环境中运行，并通过组合通用特征表示和无监督特征聚合实现了显著的性能提升。"
}