{
    "title": "Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])",
    "abstract": "Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob",
    "link": "http://arxiv.org/abs/2308.08291",
    "context": "Title: Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])\nAbstract: Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob",
    "path": "papers/23/08/2308.08291.json",
    "total_tokens": 892,
    "translated_title": "鲁棒贝叶斯满足。",
    "translated_abstract": "分布偏移对于实现当代机器学习的鲁棒性构成了重大挑战。为了克服这一挑战，鲁棒满足（RS）在实现超过期望阈值的效用的同时，寻求对于未指定的分布偏移的鲁棒解决方案。本文关注在上下文贝叶斯优化中存在真实和参考分布之间的差异时的鲁棒满足（RS）问题。我们提出了一种名为RoBOS的新型噪声黑箱优化的鲁棒贝叶斯满足算法。在某些关于分布偏移量的假设下，我们的算法保证亏得不严重的子线性遗憾。此外，我们定义了一种较弱的遗憾概念，称为鲁棒满足遗憾，其中我们的算法实现了与分布偏移量无关的子线性上界。为了展示我们方法的有效性，我们将其应用于各种学习问题，并与其他方法进行比较，如分布交换方法。",
    "tldr": "本文提出了一种名为RoBOS的鲁棒贝叶斯满足算法，用于解决在上下文贝叶斯优化中存在分布偏移时的问题。该算法能够在一定的分布偏移量下保证亏得不严重的子线性遗憾，并且具有与分布偏移量无关的较弱遗憾边界。"
}