{
    "title": "Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])",
    "abstract": "Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimatio",
    "link": "http://arxiv.org/abs/2308.04077",
    "context": "Title: Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])\nAbstract: Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimatio",
    "path": "papers/23/08/2308.04077.json",
    "total_tokens": 818,
    "translated_title": "使用轨迹信息的代理梯度进行联邦零阶优化",
    "translated_abstract": "联邦优化是一种新兴的范式，广泛应用于联邦学习等实际场景中，它使多个客户端（如边缘设备）能够共同优化一个全局函数。这些客户端不共享本地数据集，通常只共享本地梯度。然而，在许多联邦优化应用中，梯度信息是不可用的，因此引出了联邦零阶优化（ZOO）的范式。现有的联邦ZOO算法存在查询和通信效率的限制，这可以归因于：（a）它们对梯度估计需要大量的函数查询；（b）它们的实际本地更新与预期全局更新存在显著差异。为此，我们引入了基于轨迹信息的梯度代理，它能够利用优化过程中的函数查询历史进行准确且高效的梯度估计。",
    "tldr": "本论文提出了一种使用轨迹信息的代理梯度方法，用于解决联邦零阶优化中的查询和通信效率问题。"
}