{
    "title": "Training on Foveated Images Improves Robustness to Adversarial Attacks. (arXiv:2308.00854v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data.",
    "link": "http://arxiv.org/abs/2308.00854",
    "context": "Title: Training on Foveated Images Improves Robustness to Adversarial Attacks. (arXiv:2308.00854v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data.",
    "path": "papers/23/08/2308.00854.json",
    "total_tokens": 900,
    "translated_title": "训练焦点图像提高对对抗攻击的健壮性",
    "translated_abstract": "深度神经网络(DNNs)对对抗攻击具有脆弱性，即输入的细微、被感知不到的扰动会改变模型的响应。在视觉上，我们假设人类视觉感知的健壮性的一个重要因素是持续暴露于外围视觉中的低保真度视觉刺激。为了研究这个假设，我们开发了RBlur，一种图像转换方法，通过对图像进行模糊和降低颜色饱和度来模拟外围视觉的保真度损失，方法基于给定的注视点的距离。我们证明，与在原始图像上训练的DNN相比，通过RBlur转换的图像上训练的DNN对对抗攻击和其他非对抗性损坏具有更强的健壮性，在扰动数据上的准确性提高了高达25%。",
    "tldr": "本研究通过使用RBlur转换的图像来训练深度神经网络(DNN)，证明了相比于在原始图像上训练的DNN，使用RBlur方法训练的DNN对对抗攻击和其他损坏具有更强的健壮性，提高了高达25%的准确性。",
    "en_tdlr": "This study demonstrates that training deep neural networks (DNNs) on images transformed by RBlur improves robustness to adversarial attacks and other corruptions, achieving up to 25% higher accuracy compared to DNNs trained on original images."
}