{
    "title": "Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])",
    "abstract": "Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver",
    "link": "http://arxiv.org/abs/2308.01170",
    "context": "Title: Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])\nAbstract: Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver",
    "path": "papers/23/08/2308.01170.json",
    "total_tokens": 920,
    "translated_title": "直接梯度时间差分学习",
    "translated_abstract": "脱机学习使强化学习（RL）代理能够反事实地推理未执行的策略，是强化学习中最重要的思想之一。然而，当与函数逼近和自举这两个在大规模强化学习中不可或缺的因素结合时，会导致不稳定性。这就是臭名昭著的致命三元组。梯度时间差分（GTD）是解决这个致命三元组的一种强大工具。它的成功是通过使用权重复制或Fenchel对偶间接解决双重取样问题而实现的。在这篇论文中，我们提出了一种直接方法来解决双重取样问题，只需在逐渐增加的马尔可夫数据流中使用两个样本。所得到的算法与GTD一样计算效率高，但摒弃了GTD的额外权重。我们所付出的唯一代价是随着时间的推移，内存呈对数增长。我们提供了渐近和有限样本分析，其中收敛性可以得到保证。",
    "tldr": "本文提出了直接梯度时间差分学习的方法，通过使用马尔可夫数据流中的两个样本来解决双重取样问题，去除了传统方法中的额外权重，保证了计算效率，并提供了收敛性的分析。",
    "en_tdlr": "This paper proposes a method called Direct Gradient Temporal Difference Learning, which solves the double sampling issue in reinforcement learning by using two samples in a Markovian data stream, without the need for extra weights. It ensures computational efficiency and provides convergence analysis."
}