{
    "title": "Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])",
    "abstract": "Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like \"What is written on the signboard?\", the answer predicted by the model is always \"STOP\" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc",
    "link": "http://arxiv.org/abs/2308.00295",
    "context": "Title: Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])\nAbstract: Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like \"What is written on the signboard?\", the answer predicted by the model is always \"STOP\" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc",
    "path": "papers/23/08/2308.00295.json",
    "total_tokens": 896,
    "translated_title": "让Text-VQA中的V变得重要",
    "translated_abstract": "文本VQA旨在通过阅读图像中的文本来回答问题。与VQA任务相比，它需要大量的场景-文本关系理解。最近的研究表明，数据集中的问题-答案对更关注图像中的文本，而对于视觉特征则给予较少重视，而且有些问题不需要理解图像。由于缺乏对视觉上下文的理解，使用该数据集训练的模型会预测出有偏差的答案。例如，在类似“标牌上写着什么？”的问题中，模型预测的答案总是“STOP”，这使得模型忽略了图像。为了解决这些问题，我们提出了一种方法，通过使用VQA数据集作为Text-VQA的外部知识，学习视觉特征（让V在Text-VQA中变得重要），以及OCR特征和问题特征。具体来说，我们将TextVQA数据集和VQA数据集进行合并，并在这个合并的数据集上训练模型。",
    "tldr": "本论文针对文本VQA中对视觉特征理解不足的问题，提出通过学习视觉特征来解决这一问题，通过将TextVQA和VQA数据集相结合进行模型训练，从而提高模型的准确性。"
}