{
    "title": "Contrastive Learning for Cross-modal Artist Retrieval. (arXiv:2308.06556v1 [cs.IR])",
    "abstract": "Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate",
    "link": "http://arxiv.org/abs/2308.06556",
    "context": "Title: Contrastive Learning for Cross-modal Artist Retrieval. (arXiv:2308.06556v1 [cs.IR])\nAbstract: Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate",
    "path": "papers/23/08/2308.06556.json",
    "total_tokens": 884,
    "translated_title": "跨模式艺术家检索的对比学习",
    "translated_abstract": "音乐检索和推荐应用通常依赖于作为嵌入的内容特征，这些特征提供了音乐数据集中项目的向量表示。可以从处理原始表示为多种模式的项目中得到众多互补的嵌入，例如音频信号、用户互动数据或编辑数据。然而，任何给定模态的数据可能在音乐数据集中的所有项目中都不可用。在本文中，我们提出了一种基于对比学习的方法，用于组合来自多个模态的嵌入，并探索在艺术家相似性任务中多模态嵌入的存在与缺失的影响。对两个数据集的实验表明，我们的对比方法在艺术家检索准确度和覆盖范围方面优于单模态嵌入和基准算法。对于较不受欢迎的查询艺术家，与其他方法相比的改进尤为显著。我们展示了...",
    "tldr": "本研究提出了一种基于对比学习的方法，用于跨模式艺术家检索任务。实验证明，这种方法在艺术家检索准确度和覆盖范围方面优于单模态和基准算法，并且尤其适用于较不受欢迎的查询艺术家。",
    "en_tdlr": "This study proposes a contrastive learning method for cross-modal artist retrieval. Experiments show that this method outperforms single-modal and baseline algorithms in terms of artist retrieval accuracy and coverage, especially for less popular query artists."
}