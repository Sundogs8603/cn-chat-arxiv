{
    "title": "Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?",
    "abstract": "arXiv:2308.10168v2 Announce Type: replace  Abstract: Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?   To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",
    "link": "https://arxiv.org/abs/2308.10168",
    "context": "Title: Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?\nAbstract: arXiv:2308.10168v2 Announce Type: replace  Abstract: Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?   To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",
    "path": "papers/23/08/2308.10168.json",
    "total_tokens": 969,
    "translated_title": "从头到尾：大型语言模型（LLMs）有多富有知识？又称LLMs是否会取代知识图谱？",
    "translated_abstract": "自从大型语言模型（LLMs）近来繁荣以来，关于如何减少LLMs回应中的幻觉，如何提高LLMs的事实性以及符号化存储世界知识的知识图谱（KGs）是否会被LLMs取代等问题已经交织在一起进行讨论。在本文中，我们试图从一个新角度回答这些问题：LLMs有多富有知识？为了回答这个问题，我们构建了一个名为“从头到尾”的基准，其中包含18,000个有关头部、躯干和尾部事实的问答（QA）对，涉及流行度。我们设计了一种自动化评估方法和一组能够接近LLMs自信内化的知识的度量标准。通过对16个公开可用的LLMs进行全面评估，我们展示了现有LLMs在掌握事实知识方面仍然远未达到完美，特别是对于躯干到尾部实体的事实。",
    "tldr": "本文构建了Head-to-Tail基准，通过对18K个头部、躯干和尾部事实的问答对进行全面评估，揭示了现有大型语言模型在掌握事实知识方面尤其是对躯干到尾部实体的事实仍然远未完美。",
    "en_tdlr": "This paper introduces the Head-to-Tail benchmark, evaluates 18K question-answer pairs related to head, torso, and tail facts, and reveals that existing Large Language Models are still far from perfect in grasping factual knowledge, especially for torso-to-tail entities."
}