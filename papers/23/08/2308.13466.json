{
    "title": "Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])",
    "abstract": "Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr",
    "link": "http://arxiv.org/abs/2308.13466",
    "context": "Title: Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])\nAbstract: Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr",
    "path": "papers/23/08/2308.13466.json",
    "total_tokens": 930,
    "translated_title": "可通过在线动态嵌入预测减轻过时性的分布式GNN训练",
    "translated_abstract": "尽管图神经网络（GNNs）取得了近期的成功，但由于邻居扩散，仍然难以在大规模图上进行训练。分布式计算成为一种有希望的解决方案，通过利用丰富的计算资源（如GPU）。然而，在分布式GNN训练中，由于图数据的节点依赖性增加，实现高并发性变得困难，这会导致巨大的通信开销。为了解决这个问题，历史值近似被认为是一种有希望的分布式训练技术类别。它利用离线内存缓存历史信息（如节点嵌入），作为精确值的可承受的近似，并实现了高并发性。然而，这些好处是以使用过时的训练信息为代价的，导致过时性、不准确性和收敛性问题。为了克服这些挑战，本文提出了SAT（减轻过时性训练），这是一种新颖且可扩展的分布式GNN训练方法。",
    "tldr": "本文提出了一种新颖且可扩展的分布式GNN训练方法SAT，通过在线动态嵌入预测减轻了过时性带来的问题，解决了分布式GNN训练中的并发性和通信开销的困扰。",
    "en_tdlr": "This paper proposes SAT, a novel and scalable distributed GNN training method that alleviates staleness issues through online dynamic-embedding prediction, addressing concurrency and communication overhead challenges in distributed GNN training."
}