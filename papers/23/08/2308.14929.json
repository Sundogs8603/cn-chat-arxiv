{
    "title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition. (arXiv:2308.14929v1 [cs.LG])",
    "abstract": "Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take ",
    "link": "http://arxiv.org/abs/2308.14929",
    "context": "Title: Maestro: Uncovering Low-Rank Structures via Trainable Decomposition. (arXiv:2308.14929v1 [cs.LG])\nAbstract: Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take ",
    "path": "papers/23/08/2308.14929.json",
    "total_tokens": 898,
    "translated_title": "Maestro: 通过可训练分解揭示低秩结构",
    "translated_abstract": "深度神经网络(DNNs)近年来推动和促成了人工智能突破性进展。为了提高准确性并应对新兴的应用场景，包括AR/VR和智能助手，这些模型越来越大。然而，这些大模型的训练过程耗时费力，通常只能生成一个适应所有目标的模型。为了缓解这个问题，文献中提出了各种技术，包括模型权重和更新的剪枝、稀疏化或量化。虽然可以实现高压缩率，但往往会造成计算开销或准确性损失。另外，还通过因式分解方法将低秩压缩纳入训练过程中。然而，这些技术(例如SVD)常常依赖于计算昂贵的层次分解，对于非线性模型如DNNs可能不是最优解。在本研究中，我们采用了一种新的方法，通过可训练的分解揭示低秩结构。",
    "tldr": "本研究提出了一种通过可训练分解揭示低秩结构的方法，解决了深度神经网络训练大模型时的耗时和资源消耗的问题。",
    "en_tdlr": "This paper presents a method for uncovering low-rank structures through trainable decomposition, which addresses the issues of time and resource consumption in training large models of deep neural networks."
}