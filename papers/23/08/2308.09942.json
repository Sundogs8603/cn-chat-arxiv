{
    "title": "On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion. (arXiv:2308.09942v1 [cs.CV])",
    "abstract": "Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-ar",
    "link": "http://arxiv.org/abs/2308.09942",
    "context": "Title: On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion. (arXiv:2308.09942v1 [cs.CV])\nAbstract: Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-ar",
    "path": "papers/23/08/2308.09942.json",
    "total_tokens": 980,
    "translated_title": "关于开放世界测试时间训练的稳健性：自我训练与动态原型扩展",
    "translated_abstract": "将深度学习模型泛化到未知的目标领域分布与低延迟的问题促使了对测试时间训练/适应（TTT/TTA）的研究。现有的方法通常注重提高在经过精心策划的目标领域数据下的测试时间训练性能。在这项研究中发现，许多最先进的方法在目标领域受到强有力的外部分布（OOD）数据污染时无法保持性能，即开放世界测试时间训练（OWTTT）。这种失败主要是由于无法区分强ODD样本和常规弱OOD样本。为了提高OWTTT的稳健性，我们首先开发了自适应的强OOD修剪方法，以提高自我训练TTT方法的效能。我们进一步提出了一种动态扩展原型的方式，以表示强OOD样本，以改善弱/强OOD数据的分离。最后，我们用分布对齐来规范化自我训练，并将其与动态原型扩展相结合，实现了最先进的技术。",
    "tldr": "本研究提出了一种改进开放世界测试时间训练的方法，包括自适应强OOD修剪、动态原型扩展和分布对齐等技术。这些方法提高了模型在目标领域受到强OOD数据污染时的稳健性。",
    "en_tdlr": "This study proposes improvements for open-world test-time training, including adaptive strong OOD pruning, dynamic prototype expansion, and distribution alignment. These techniques enhance the robustness of models when exposed to strong OOD data contamination in the target domain."
}