{
    "title": "Resource Constrained Model Compression via Minimax Optimization for Spiking Neural Networks. (arXiv:2308.04672v1 [cs.CV])",
    "abstract": "Brain-inspired Spiking Neural Networks (SNNs) have the characteristics of event-driven and high energy-efficient, which are different from traditional Artificial Neural Networks (ANNs) when deployed on edge devices such as neuromorphic chips. Most previous work focuses on SNNs training strategies to improve model performance and brings larger and deeper network architectures. It is difficult to deploy these complex networks on resource-limited edge devices directly. To meet such demand, people compress SNNs very cautiously to balance the performance and the computation efficiency. Existing compression methods either iteratively pruned SNNs using weights norm magnitude or formulated the problem as a sparse learning optimization. We propose an improved end-to-end Minimax optimization method for this sparse learning problem to better balance the model performance and the computation efficiency. We also demonstrate that jointly applying compression and finetuning on SNNs is better than seq",
    "link": "http://arxiv.org/abs/2308.04672",
    "context": "Title: Resource Constrained Model Compression via Minimax Optimization for Spiking Neural Networks. (arXiv:2308.04672v1 [cs.CV])\nAbstract: Brain-inspired Spiking Neural Networks (SNNs) have the characteristics of event-driven and high energy-efficient, which are different from traditional Artificial Neural Networks (ANNs) when deployed on edge devices such as neuromorphic chips. Most previous work focuses on SNNs training strategies to improve model performance and brings larger and deeper network architectures. It is difficult to deploy these complex networks on resource-limited edge devices directly. To meet such demand, people compress SNNs very cautiously to balance the performance and the computation efficiency. Existing compression methods either iteratively pruned SNNs using weights norm magnitude or formulated the problem as a sparse learning optimization. We propose an improved end-to-end Minimax optimization method for this sparse learning problem to better balance the model performance and the computation efficiency. We also demonstrate that jointly applying compression and finetuning on SNNs is better than seq",
    "path": "papers/23/08/2308.04672.json",
    "total_tokens": 880,
    "translated_title": "资源受限下通过极小化最大化优化对脉冲神经网络进行模型压缩",
    "translated_abstract": "脑部启发的脉冲神经网络（SNNs）具有事件驱动和高能效的特点，与传统的人工神经网络（ANNs）在神经元仿真芯片等边缘设备上部署时有所不同。大部分之前的研究集中于SNNs训练策略以提高模型性能并引入更大更深的网络架构。但是在资源受限的边缘设备上直接部署这些复杂网络是困难的。为了满足这种需求，人们通过谨慎地对SNNs进行压缩，以平衡性能和计算效率。现有的压缩方法要么通过权重范数大小迭代地剪枝SNNs，要么将问题形式化为稀疏学习优化。我们提出了一种改进的端到端极小化最大化优化方法来更好地平衡模型性能和计算效率。我们还证明了在SNNs上联合应用压缩和微调优于顺序应用它们。",
    "tldr": "提出了一种资源受限下通过极小化最大化优化对脉冲神经网络进行模型压缩的方法，以平衡模型性能和计算效率。",
    "en_tdlr": "An improved end-to-end Minimax optimization method is proposed for compressing Spiking Neural Networks (SNNs) under resource constraints, to balance model performance and computation efficiency."
}