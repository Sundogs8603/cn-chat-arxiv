{
    "title": "Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])",
    "abstract": "Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge size causes significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for",
    "link": "http://arxiv.org/abs/2308.04269",
    "context": "Title: Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])\nAbstract: Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge size causes significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for",
    "path": "papers/23/08/2308.04269.json",
    "total_tokens": 908,
    "translated_title": "高压缩比训练后模型尺寸压缩的有损和无损方法",
    "translated_abstract": "深度神经网络在各种视觉任务中取得了显著的性能，并被广泛应用。然而，它们庞大的尺寸给传输和存储带来了很大不便。许多先前的研究探索了模型尺寸压缩的方法。然而，这些研究常常独立地处理各种有损和无损压缩方法，导致在高效实现高压缩比方面存在挑战。本文提出了一种将有损和无损压缩以统一方式结合的训练后模型尺寸压缩方法。我们首先提出了一个统一的参数化权重变换，确保可以以训练后的方式联合执行不同的有损压缩方法。然后，引入了一个专门的可微分计数器来引导有损压缩的优化，以达到更适合后续无损压缩的点。此外，我们的方法可以轻松控制所需的全局压缩比，并为不同的层分配自适应比例。",
    "tldr": "提出了一种将有损和无损压缩结合的训练后模型尺寸压缩方法，通过统一的参数化权重变换和引入可微分计数器来实现高效高压缩比的模型压缩。",
    "en_tdlr": "This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. The method utilizes a unified parametric weight transformation and a differentiable counter to achieve high compression ratios efficiently."
}