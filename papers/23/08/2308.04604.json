{
    "title": "A Survey on Decentralized Federated Learning. (arXiv:2308.04604v1 [cs.LG])",
    "abstract": "In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failur",
    "link": "http://arxiv.org/abs/2308.04604",
    "context": "Title: A Survey on Decentralized Federated Learning. (arXiv:2308.04604v1 [cs.LG])\nAbstract: In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failur",
    "path": "papers/23/08/2308.04604.json",
    "total_tokens": 808,
    "translated_title": "分散式联邦学习综述",
    "translated_abstract": "最近几年，联邦学习（FL）已经成为训练分布式、大规模、保护隐私的机器学习（ML）系统的流行范式。与标准ML不同，需要将数据收集在训练执行的确切位置，FL利用数百万边缘设备的计算能力来协同训练共享的全局模型，同时不会披露其本地私有数据。在典型的FL系统中，中央服务器只充当协调器的角色；它迭代地收集和汇总每个客户端在自己的私有数据上训练的本地模型，直到收敛。尽管FL在设计上具有许多优点（例如通过设计保护私有数据所有权），但也存在一些弱点。其中最关键的挑战之一是克服经典FL客户端-服务器架构的集中式编排，这被认为是易受单点故障攻击的。",
    "tldr": "最近几年，联邦学习成为训练分布式、大规模、保护隐私的机器学习系统的流行范式。然而，其中一个关键挑战是克服集中式编排的单点故障问题。"
}