{
    "title": "Automatic Task Parallelization of Dataflow Graphs in ML/DL models. (arXiv:2308.11192v1 [cs.LG])",
    "abstract": "Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\\bf Ramiel}. This allows us to be",
    "link": "http://arxiv.org/abs/2308.11192",
    "context": "Title: Automatic Task Parallelization of Dataflow Graphs in ML/DL models. (arXiv:2308.11192v1 [cs.LG])\nAbstract: Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\\bf Ramiel}. This allows us to be",
    "path": "papers/23/08/2308.11192.json",
    "total_tokens": 898,
    "translated_title": "自动并行化ML/DL模型中的数据流图",
    "translated_abstract": "现今存在多种方法用于加速机器学习(ML)或深度学习(DL)模型的训练和推断过程。然而，依赖于各种图形和运算符并行性方法的现代技术依赖于搜索空间优化，这在功耗和硬件使用方面代价高昂。特别是在推断过程中，当批量大小为1且在CPU上执行或用于功耗受限的边缘设备时，现有技术可能变得昂贵、复杂或不适用。为了改善这一问题，我们提出了一种基于关键路径的线性聚类方法，以利用ML数据流图中的内在并行路径。我们的任务并行化方法通过克隆优化图的结构，并通过常量传播和死代码消除对其进行修剪。与其他工作不同的是，我们通过一个名为Ramiel的新工具，从ONNX格式的输入ML模型生成可读和可执行的并行Pytorch+Python代码。这使我们能够...",
    "tldr": "通过对ML数据流图进行关键路径线性聚类和任务并行化，我们提出了一种优化ML/DL模型的自动并行化方法。与其他工作不同的是，我们通过一个新的工具Ramiel生成可读和可执行的并行Pytorch+Python代码。"
}