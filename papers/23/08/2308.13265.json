{
    "title": "Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])",
    "abstract": "Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons",
    "link": "http://arxiv.org/abs/2308.13265",
    "context": "Title: Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])\nAbstract: Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons",
    "path": "papers/23/08/2308.13265.json",
    "total_tokens": 844,
    "translated_title": "个性化生成网络实现异构联邦学习",
    "translated_abstract": "联邦学习允许多个客户端构建一个共同的全局机器学习模型，而无需共享数据。然而，联邦学习面临客户端数据的统计异质性的挑战，这降低了性能并减慢了向全局模型的收敛速度。本文提供了理论证明，最小化客户端之间的异质性有助于每个单独客户端的全局模型的收敛。这在客户端之间出现经验概念转变非常重要，而不仅仅是考虑到已被研究过的不平衡类别。因此，我们提出了一种知识传递方法，其中服务器训练客户端特定的生成器。每个生成器为相应的客户端生成样本，以消除与其他客户端模型的冲突。在合成和真实数据上进行的实验证明以及理论研究支持了我们方法的有效性。",
    "tldr": "本文通过个性化生成网络实现了异构联邦学习，解决了数据统计异质性的问题，并通过对客户端之间知识传递的方法，提高了全局模型的收敛效果。",
    "en_tdlr": "This paper proposes a method for heterogeneous federated learning using personalized generative networks, addressing the challenge of statistical heterogeneity between client data. The method involves knowledge transfer between clients through client-specific generators, leading to improved convergence of the global model."
}