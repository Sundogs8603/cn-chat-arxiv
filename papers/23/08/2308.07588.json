{
    "title": "High-Probability Risk Bounds via Sequential Predictors. (arXiv:2308.07588v1 [cs.LG])",
    "abstract": "Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper ",
    "link": "http://arxiv.org/abs/2308.07588",
    "context": "Title: High-Probability Risk Bounds via Sequential Predictors. (arXiv:2308.07588v1 [cs.LG])\nAbstract: Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper ",
    "path": "papers/23/08/2308.07588.json",
    "total_tokens": 798,
    "translated_title": "高概率风险上界通过顺序预测器实现",
    "translated_abstract": "在最小假设下，在线学习方法产生顺序遗憾上界，并为统计学习提供期望风险上界。然而，尽管在线保证相对于统计保证明显有优势，但最近的研究结果表明，在许多重要情况下，遗憾上界可能无法保证统计设置中的紧密高概率风险上界。在这项工作中，我们展示了应用于一般在线学习算法的在线到批次转换可以绕过此限制。通过对定义遗憾的损失函数进行一般的二阶校正，我们获得了几种经典统计估计问题的几乎最优高概率风险上界，例如离散分布估计，线性回归，逻辑回归和条件密度估计。我们的分析依赖于许多在线学习算法是不适当的事实，因为它们不受限于使用给定参考类别的预测器。",
    "tldr": "通过在线到批次转换和对损失函数的二阶校正，我们展示了一种方法可以在统计学习中获得几乎最优的高概率风险上界。"
}