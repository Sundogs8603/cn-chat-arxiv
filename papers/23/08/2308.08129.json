{
    "title": "Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?. (arXiv:2308.08129v1 [cs.LG])",
    "abstract": "The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target ",
    "link": "http://arxiv.org/abs/2308.08129",
    "context": "Title: Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?. (arXiv:2308.08129v1 [cs.LG])\nAbstract: The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target ",
    "path": "papers/23/08/2308.08129.json",
    "total_tokens": 855,
    "translated_title": "自我监督的预训练对分子性质预测的外推是否有效？",
    "translated_abstract": "材料性质的预测在电池、半导体、催化剂和药物等各种应用中起着至关重要的作用。最近，人们越来越关注使用机器学习技术结合常规理论计算的数据驱动方法。在材料科学中，未观测的值，通常称为外推，在性质预测中尤其关键，因为它使研究人员能够深入了解超出现有数据限制范围的材料。然而，即使在强大的机器学习模型的最新进展下，准确的外推仍被广泛认为是一个具有挑战性的问题。另一方面，自我监督的预训练是一种机器学习技术，模型首先通过使用相对简单的前提任务在无标签数据上进行训练，然后在带标签的数据上进行目标训练。",
    "tldr": "这项研究探讨了自我监督的预训练方法在分子性质预测中的外推能力，结果发现即使在强大的机器学习模型条件下，准确的外推仍然是一个具有挑战性的问题。",
    "en_tdlr": "This study investigates the extrapolation capability of self-supervised pretraining in molecular property prediction, and finds that accurate extrapolation remains a challenging problem even with powerful machine learning models."
}