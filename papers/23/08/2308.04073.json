{
    "title": "Learning Specialized Activation Functions for Physics-informed Neural Networks. (arXiv:2308.04073v1 [cs.LG])",
    "abstract": "Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which canno",
    "link": "http://arxiv.org/abs/2308.04073",
    "context": "Title: Learning Specialized Activation Functions for Physics-informed Neural Networks. (arXiv:2308.04073v1 [cs.LG])\nAbstract: Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which canno",
    "path": "papers/23/08/2308.04073.json",
    "total_tokens": 899,
    "translated_title": "为物理信息神经网络学习专门的激活函数",
    "translated_abstract": "众所周知，物理信息神经网络（PINNs）在优化过程中存在困难。本文揭示了PINNs的优化困难与激活函数之间的联系。具体而言，我们发现，在解决具有不同属性的偏微分方程时，PINNs对激活函数非常敏感。现有的方法通常通过低效的试错方式选择激活函数。为了避免低效的手动选择，并减轻PINNs的优化困难，我们引入了自适应激活函数，用于在解决不同问题时搜索最佳函数。我们比较了不同的自适应激活函数，并讨论了它们在PINNs中的局限性。此外，我们提出了将学习候选激活函数组合的思想应用于PINNs优化的方法，这对所学函数的平滑性和多样性有更高要求。通过移除不能满足要求的激活函数，我们实现了这一目标。",
    "tldr": "本文发现物理信息神经网络（PINNs）在解决偏微分方程时对激活函数非常敏感，为了避免低效的手动选择并减轻优化困难，我们引入了自适应激活函数，并提出了将学习候选激活函数组合的思想应用于PINNs优化的方法。",
    "en_tdlr": "This paper reveals that physics-informed neural networks (PINNs) exhibit high sensitivity to activation functions when solving partial differential equations. To avoid inefficient manual selection and alleviate optimization difficulties, the paper introduces adaptive activation functions and proposes learning combinations of candidate activation functions for PINNs optimization."
}