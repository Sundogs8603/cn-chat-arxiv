{
    "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])",
    "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g",
    "link": "http://arxiv.org/abs/2308.01140",
    "context": "Title: DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])\nAbstract: In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g",
    "path": "papers/23/08/2308.01140.json",
    "total_tokens": 857,
    "translated_title": "DySTreSS: 自监督对比学习中的动态温度调整",
    "translated_abstract": "在当代的自监督对比算法中，诸如SimCLR、MoCo等，平衡两个语义相似样本之间的吸引力和两个不同类别样本之间的排斥力的任务主要受到硬负样本的影响。虽然已经证明InfoNCE损失可以根据困难程度施加惩罚，但温度超参数是调节惩罚和均匀性与容忍度之间权衡的关键。在这项工作中，我们着眼于改进SSL中InfoNCE损失的性能，研究温度超参数值的影响。我们提出了一种余弦相似度依赖的温度调整函数，以有效优化特征空间中样本的分布。我们进一步分析均匀性和容忍度度量，以研究余弦相似度空间中更好优化的最佳区域。此外，我们还对局部和全局行为进行了全面的研究。",
    "tldr": "本研究提出了一种余弦相似度依赖的温度调整函数，用于改进自监督对比学习中的性能，并优化了样本在特征空间中的分布。",
    "en_tdlr": "This study proposes a cosine similarity-dependent temperature scaling function to improve the performance of self-supervised contrastive learning and optimize the distribution of samples in the feature space."
}