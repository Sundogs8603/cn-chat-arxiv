{
    "title": "Deep Inductive Logic Programming meets Reinforcement Learning. (arXiv:2308.16210v1 [cs.LG])",
    "abstract": "One approach to explaining the hierarchical levels of understanding within a machine learning model is the symbolic method of inductive logic programming (ILP), which is data efficient and capable of learning first-order logic rules that can entail data behaviour. A differentiable extension to ILP, so-called differentiable Neural Logic (dNL) networks, are able to learn Boolean functions as their neural architecture includes symbolic reasoning. We propose an application of dNL in the field of Relational Reinforcement Learning (RRL) to address dynamic continuous environments. This represents an extension of previous work in applying dNL-based ILP in RRL settings, as our proposed model updates the architecture to enable it to solve problems in continuous RL environments. The goal of this research is to improve upon current ILP methods for use in RRL by incorporating non-linear continuous predicates, allowing RRL agents to reason and make decisions in dynamic and continuous environments.",
    "link": "http://arxiv.org/abs/2308.16210",
    "context": "Title: Deep Inductive Logic Programming meets Reinforcement Learning. (arXiv:2308.16210v1 [cs.LG])\nAbstract: One approach to explaining the hierarchical levels of understanding within a machine learning model is the symbolic method of inductive logic programming (ILP), which is data efficient and capable of learning first-order logic rules that can entail data behaviour. A differentiable extension to ILP, so-called differentiable Neural Logic (dNL) networks, are able to learn Boolean functions as their neural architecture includes symbolic reasoning. We propose an application of dNL in the field of Relational Reinforcement Learning (RRL) to address dynamic continuous environments. This represents an extension of previous work in applying dNL-based ILP in RRL settings, as our proposed model updates the architecture to enable it to solve problems in continuous RL environments. The goal of this research is to improve upon current ILP methods for use in RRL by incorporating non-linear continuous predicates, allowing RRL agents to reason and make decisions in dynamic and continuous environments.",
    "path": "papers/23/08/2308.16210.json",
    "total_tokens": 917,
    "translated_title": "深层归纳逻辑编程与强化学习的结合",
    "translated_abstract": "解释机器学习模型中的层级理解水平的一种方法是归纳逻辑编程（ILP）的符号方法，它在数据效率和学习能力方面均具备，可以学习能够推导数据行为的一阶逻辑规则。可微扩展的ILP，即可微分的神经逻辑（dNL）网络，能够学习布尔函数，因为它的神经结构包括符号推理。我们提出了将dNL应用于关系强化学习（RRL）领域的方法，以解决动态连续环境问题。这是在RRL设置中应用基于dNL的ILP的扩展，因为我们的模型更新了架构，使其能够在连续RL环境中解决问题。本研究的目标是改进用于RRL的当前ILP方法，通过引入非线性连续谓词，使RRL代理能够在动态连续环境中进行推理和决策。",
    "tldr": "本研究将深层归纳逻辑编程与强化学习相结合，提出了一种在动态连续环境中应用可微分神经逻辑网络的方法，以改进当前关系强化学习中的ILP方法。该方法能够学习一阶逻辑规则并处理连续环境中的问题。",
    "en_tdlr": "This paper combines deep inductive logic programming with reinforcement learning, proposing a method that applies differentiable neural logic networks in dynamic continuous environments to improve ILP methods in relational reinforcement learning. It can learn first-order logic rules and handle problems in continuous environments."
}