{
    "title": "Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.",
    "link": "http://arxiv.org/abs/2308.00707",
    "context": "Title: Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])\nAbstract: Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.",
    "path": "papers/23/08/2308.00707.json",
    "total_tokens": 843,
    "translated_title": "安全强化学习的近似模型屏蔽",
    "translated_abstract": "强化学习 (RL) 在各个领域解决复杂任务方面展示了巨大的潜力。然而，将 RL 应用于现实世界的安全关键系统并不容易，因为许多算法在样本效率上存在问题，并且最大化标准 RL 目标不能保证最坏情况下的性能。在本文中，我们提出了近似模型屏蔽 (AMBS)，这是一种基于模型的理性前瞻屏蔽算法，用于验证学习的 RL 策略相对于一组给定的安全约束的性能。我们的算法与其他屏蔽方法不同，它不需要对系统的安全相关动态的先验知识。我们为 AMBS 提供了强大的理论基础，并在一组具有状态相关安全标签的 Atari 游戏上展示了优越的性能。",
    "tldr": "提出了一种近似模型屏蔽算法 (AMBS) 来验证学习的强化学习策略在给定安全约束下的性能，与其他屏蔽方法相比，AMBS不需要先验知识，并在具有状态相关安全标签的 Atari 游戏上展示了优越性能。"
}