{
    "title": "Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification. (arXiv:2308.15969v1 [cs.AI])",
    "abstract": "A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show t",
    "link": "http://arxiv.org/abs/2308.15969",
    "context": "Title: Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification. (arXiv:2308.15969v1 [cs.AI])\nAbstract: A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show t",
    "path": "papers/23/08/2308.15969.json",
    "total_tokens": 943,
    "translated_title": "使用人机反馈的迭代奖励塑造来纠正奖励规格化错误",
    "translated_abstract": "对于强化学习（RL）代理的成功训练，一个明确定义的奖励函数至关重要。然而，在复杂的多目标环境中定义一个合适的奖励函数是一项极具挑战性的任务。开发人员常常不得不从一个初始的、可能存在错误的奖励函数开始，并根据观察到的学习行为迭代地调整其参数。在这项工作中，我们旨在通过提出ITERS，一种使用人机反馈的迭代奖励塑造方法，来自动化这个过程，以减轻奖励函数规格化错误的影响。我们的方法允许用户在训练过程中提供基于轨迹的反馈，这些反馈可以作为下一个训练迭代中的奖励塑造信号进行整合。我们还允许用户提供对他们的反馈的解释，这些解释被用来增强反馈并减少用户的工作量和反馈频率。我们在三种环境中评估了ITERS并展示了其效果。",
    "tldr": "这项工作提出了ITERS，一种使用人机反馈的迭代奖励塑造方法，用于纠正奖励函数规格化错误。通过让用户提供基于轨迹的反馈，并结合用户的解释，我们实现了自动化的奖励调整过程，并在多种环境中对该方法进行了评估。",
    "en_tdlr": "This work proposes ITERS, an iterative reward shaping approach using human feedback, for correcting reward misspecification. It automates the process of reward adjustment by allowing users to provide trajectory-level feedback and explanations, and has been evaluated in multiple environments."
}