{
    "title": "Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization. (arXiv:2308.05749v1 [physics.chem-ph])",
    "abstract": "Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of",
    "link": "http://arxiv.org/abs/2308.05749",
    "context": "Title: Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization. (arXiv:2308.05749v1 [physics.chem-ph])\nAbstract: Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of",
    "path": "papers/23/08/2308.05749.json",
    "total_tokens": 963,
    "translated_title": "通过时间序列-Transformer引入混合建模：关于串行与并行方法在批结晶中的比较研究",
    "translated_abstract": "大多数现有的数字孪生依赖于数据驱动的黑箱模型，主要使用深度神经网络递归和卷积神经网络（DNN，RNN和CNN）来捕捉化学系统的动态。然而，考虑到安全和操作问题，这些模型尚未被实际部署。为了解决这个难题，将基于第一原理的物理动力学与机器学习（ML）模型相结合的混合模型因其被认为是一种“两全其美”的方法而越来越受欢迎。然而，现有简单的DNN模型在长期时间序列预测和利用过程动力学轨迹的上下文信息方面并不擅长。最近，基于注意力机制和位置编码的时间序列Transformer（TST）在捕捉过程状态的长期和短期变化方面表现出高预测性能。",
    "tldr": "本研究引入混合建模的方法，将基于物理原理的动力学与机器学习模型相结合，以解决直接部署黑箱模型的安全和操作问题。相比简单的深度神经网络模型，最近的时间序列Transformer模型利用注意力机制和位置编码，能更好地预测长期时间序列，并利用过程动力学轨迹的上下文信息。",
    "en_tdlr": "This study introduces a hybrid modeling approach that combines physics-based dynamics with machine learning models to address safety and operational issues in deploying black-box models directly. Compared to simple deep neural network models, recent time-series Transformers leverage attention mechanism and positional encoding to better predict long-term time series and utilize contextual information from process dynamics."
}