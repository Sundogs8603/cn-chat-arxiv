{
    "title": "Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation. (arXiv:2308.06457v1 [cs.CV])",
    "abstract": "The advent of ChatGPT has introduced innovative methods for information gathering and analysis. However, the information provided by ChatGPT is limited to text, and the visualization of this information remains constrained. Previous research has explored zero-shot text-to-video (TTV) approaches to transform text into videos. However, these methods lacked control over the identity of the generated audio, i.e., not identity-agnostic, hindering their effectiveness. To address this limitation, we propose a novel two-stage framework for person-agnostic video cloning, specifically focusing on TTV generation. In the first stage, we leverage pretrained zero-shot models to achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven talking head generation method is employed to produce compelling videos privided the audio generated in the first stage. This paper presents a comparative analysis of different TTS and audio-driven talking head generation methods, identifying the mo",
    "link": "http://arxiv.org/abs/2308.06457",
    "context": "Title: Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation. (arXiv:2308.06457v1 [cs.CV])\nAbstract: The advent of ChatGPT has introduced innovative methods for information gathering and analysis. However, the information provided by ChatGPT is limited to text, and the visualization of this information remains constrained. Previous research has explored zero-shot text-to-video (TTV) approaches to transform text into videos. However, these methods lacked control over the identity of the generated audio, i.e., not identity-agnostic, hindering their effectiveness. To address this limitation, we propose a novel two-stage framework for person-agnostic video cloning, specifically focusing on TTV generation. In the first stage, we leverage pretrained zero-shot models to achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven talking head generation method is employed to produce compelling videos privided the audio generated in the first stage. This paper presents a comparative analysis of different TTS and audio-driven talking head generation methods, identifying the mo",
    "path": "papers/23/08/2308.06457.json",
    "total_tokens": 933,
    "translated_title": "文本到视频：零样本身份不可知言语生成的两阶段框架",
    "translated_abstract": "ChatGPT的出现引入了创新的信息收集和分析方法。然而，ChatGPT提供的信息仅限于文本，其可视化仍受限。先前的研究探索了零样本文本到视频（TTV）的方法，将文本转换为视频。然而，这些方法缺乏对生成音频身份的控制，即不是身份不可知的，从而影响了其有效性。为了解决这个限制，我们提出了一个新颖的两阶段框架，用于面向人物不可知的视频克隆，特别关注TTV生成。在第一阶段，我们利用预训练的零样本模型实现了文本到语音（TTS）转换。在第二阶段，采用基于音频驱动的说话头生成方法来产生引人入胜的视频，提供了第一阶段生成的音频。本文对不同的TTS和基于音频驱动的说话头生成方法进行了比较分析，识别了最有效的方法。",
    "tldr": "本文提出了一个两阶段框架，用于零样本身份不可知言语生成，首先利用预训练模型进行文本到语音转换，然后使用音频驱动的说话头生成方法产生引人入胜的视频。这项研究提供了对不同方法的比较分析，识别出最有效的方法。"
}