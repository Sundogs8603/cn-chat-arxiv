{
    "title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])",
    "abstract": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe",
    "link": "http://arxiv.org/abs/2308.01546",
    "context": "Title: MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])\nAbstract: Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe",
    "path": "papers/23/08/2308.01546.json",
    "total_tokens": 991,
    "translated_title": "MusicLDM：使用节奏同步的混合策略增强文本转音乐生成中的新颖性",
    "translated_abstract": "扩散模型在跨模态生成任务中展现了令人期待的结果，包括文本到图像和文本到音频的生成。然而，由于音乐数据的有限可用性以及与版权和抄袭相关的敏感问题，生成音乐作为一种特殊类型的音频，面临着独特的挑战。在本文中，为了解决这些挑战，我们首先构建了一种最先进的文本到音乐模型MusicLDM，将稳定扩散和AudioLDM架构适应到音乐领域。我们通过对一系列音乐数据样本进行重新训练对比语言-音频预训练模型(CLAP)和Hifi-GAN声码器这些MusicLDM的组成部分进行了实现。然后，为了解决训练数据的限制并避免抄袭，我们利用节拍追踪模型并提出了两种不同的数据增广混合策略：节奏同步音频混合和节奏同步潜在混合，分别通过直接重新组合训练音频或通过潜在嵌入空间重新组合。",
    "tldr": "MusicLDM通过稳定扩散和AudioLDM架构，结合重新训练对比语言-音频预训练模型和Hifi-GAN声码器，以解决音乐生成中的挑战。通过节奏同步的混合策略，对训练数据进行增广，提高新颖性并避免抄袭问题。",
    "en_tdlr": "MusicLDM enhances novelty and tackles challenges in text-to-music generation by adapting diffusion models and retraining pretraining models with beat-synchronous mixup strategies for data augmentation."
}