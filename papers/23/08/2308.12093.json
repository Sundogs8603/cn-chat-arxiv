{
    "title": "Cached Operator Reordering: A Unified View for Fast GNN Training. (arXiv:2308.12093v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks i",
    "link": "http://arxiv.org/abs/2308.12093",
    "context": "Title: Cached Operator Reordering: A Unified View for Fast GNN Training. (arXiv:2308.12093v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks i",
    "path": "papers/23/08/2308.12093.json",
    "total_tokens": 891,
    "translated_title": "缓存操作重排：用于快速GNN训练的统一视图",
    "translated_abstract": "图神经网络（GNN）是处理结构化图数据和解决节点分类、图分类和聚类等任务的强大工具。然而，与传统深度神经网络相比，GNN计算的稀疏性提出了性能优化方面的新挑战。我们通过对GNN计算、输入/输出和内存进行统一视图来解决这些挑战。通过分析图卷积网络（GCN）和图注意力（GAT）两种广泛使用的GNN层的计算图，我们提出了替代的计算策略。我们提出了自适应操作重排和缓存，使GCN的速度提高了最多2.43倍，超过了当前最先进的技术。此外，对GAT的不同缓存方案的探索使速度提高了最多1.94倍。所提出的优化方法节省了内存，易于在各种硬件平台上实现，并有望缓解性能瓶颈。",
    "tldr": "本文通过统一视图提供了一种解决图神经网络计算稀疏性挑战的方法，并通过缓存和自适应操作重排实现了GCN和GAT的高速运行，有效节省内存并缓解性能瓶颈。",
    "en_tdlr": "This paper proposes a unified approach to address the challenges of sparse computation in Graph Neural Networks (GNNs) and achieves high-speed operation of GCN and GAT through caching and adaptive operator reordering, effectively saving memory and alleviating performance bottlenecks."
}