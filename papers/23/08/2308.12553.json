{
    "title": "Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. (arXiv:2308.12553v1 [cs.LG])",
    "abstract": "Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward ",
    "link": "http://arxiv.org/abs/2308.12553",
    "context": "Title: Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. (arXiv:2308.12553v1 [cs.LG])\nAbstract: Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward ",
    "path": "papers/23/08/2308.12553.json",
    "total_tokens": 896,
    "translated_title": "不要怪数据集转移！梯度和交叉熵导致了捷径学习",
    "translated_abstract": "常见对于捷径学习的解释认为捷径在训练分布下改善了预测结果，但在测试分布下却没有改善。因此，通过典型的基于梯度的交叉熵优化训练的模型（我们称其为默认-ERM）利用了这个捷径。然而，即使在训练分布中稳定特征决定了标签而捷径并没有提供额外的信息，比如在感知任务中，默认-ERM仍然表现出了捷径学习。为什么这样的解决方案更受青睐，当可以单独使用稳定特征将默认-ERM的损失驱动为零时？通过研究线性感知任务，我们展示了默认-ERM对于最大化间隔的偏好导致了更多依赖于捷径而非稳定特征的模型，即使没有过度参数化。这一发现表明，默认-ERM的隐性归纳偏好即最大间隔对于感知任务是不合适的。相反，我们提出了一种适合感知任务的归纳偏好。",
    "tldr": "默认-ERM模型通过最大化间隔来优化训练，导致模型更多依赖于捷径而非稳定特征，这对感知任务来说是不合适的。"
}