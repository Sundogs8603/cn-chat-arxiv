{
    "title": "FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models. (arXiv:2308.15324v1 [cs.AI])",
    "abstract": "Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the",
    "link": "http://arxiv.org/abs/2308.15324",
    "context": "Title: FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models. (arXiv:2308.15324v1 [cs.AI])\nAbstract: Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the",
    "path": "papers/23/08/2308.15324.json",
    "total_tokens": 831,
    "translated_title": "FedLogic: 可解释化的联邦多领域思维链选择方法用于大型语言模型",
    "translated_abstract": "利用“思维链（CoT）”推理从大型语言模型（LLM）中获取快速精确的回答正迅速引起研究界的兴趣。其中一个重要的挑战是如何设计或选择最佳提示。提示选择的过程依赖于用户根据LLM生成的相应新反应不断调整和组合输入提示的试错。此外，目前还没有研究探讨LLM如何利用从用户交互中学习到的数学问题求解能力来解决叙述写作中的问题。为了改进可解释性并在多领域CoT提示选择场景下探索通用性和个性化之间的平衡原则，我们提出了联邦逻辑规则学习方法（FedLogic）。我们在联邦LLM的背景下引入了多领域CoT提示选择困境的理论形式化和交互模拟。",
    "tldr": "本文提出了一种名为FedLogic的方法，用于解决大型语言模型在多领域思维链选择中的可解释性和平衡性问题。",
    "en_tdlr": "This paper proposes a method called FedLogic to address the interpretability and balance issues in multi-domain chain-of-thought prompt selection for large language models (LLMs)."
}