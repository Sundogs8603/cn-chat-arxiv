{
    "title": "Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its",
    "link": "http://arxiv.org/abs/2308.00890",
    "context": "Title: Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its",
    "path": "papers/23/08/2308.00890.json",
    "total_tokens": 886,
    "translated_title": "Tango: 重新思考在GPU上对图神经网络训练的量化方法",
    "translated_abstract": "图神经网络（GNN）因其在关键的图相关任务中表现出色而越来越受欢迎。虽然量化被广泛用于加速GNN计算，但量化训练面临前所未有的挑战。当前的量化GNN训练系统往往比其全精度对应物有更长的训练时间，原因有二：（一）解决准确度问题导致了过多的开销，（二）没有充分发挥量化所展示的优化潜力。本文介绍了Tango，它重新思考了在GPU上进行图神经网络训练的量化挑战和机会，并做出了三个贡献：首先，我们引入了有效的规则来在量化GNN训练过程中保持准确度。其次，我们设计并实现了量化感知的基本操作和基本操作之间的优化，可以加速GNN训练。最后，我们将Tango与流行的Deep Graph Library（DGL）系统集成，并展示其",
    "tldr": "Tango是一个重新思考在GPU上对图神经网络训练的量化方法的研究，通过引入新的规则来保持准确度，设计并实现量化感知的基本操作和基本操作之间的优化，以加速GNN训练。",
    "en_tdlr": "Tango is a research that rethinks quantization for training graph neural networks on GPUs, introducing new rules for accuracy maintenance, and designing and implementing quantization-aware primitives and inter-primitive optimizations to speed up GNN training."
}