{
    "title": "Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. (arXiv:2308.04617v1 [cs.LG])",
    "abstract": "Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation",
    "link": "http://arxiv.org/abs/2308.04617",
    "context": "Title: Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. (arXiv:2308.04617v1 [cs.LG])\nAbstract: Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation",
    "path": "papers/23/08/2308.04617.json",
    "total_tokens": 905,
    "translated_title": "改进的激活剪裁方法用于普适性后门缓解和测试时间检测",
    "translated_abstract": "深度神经网络易受到后门攻击（特洛伊），攻击者通过在训练集中注入后门触发器，使得神经网络在测试时将触发器分类到攻击者指定的目标类别。最近的研究表明，后门注入在受攻击模型中引起过拟合（异常大的激活），这促使我们提出了一种通用的、训练后的剪裁方法来缓解后门，即通过用一小组干净样本学习内部层的激活边界。我们设计了一种新的方法，选择激活边界来显式地限制分类边界。这种方法在CIFAR-10图像分类的对标方法中表现出优越性能。我们还展示了这种方法对于自适应攻击、X2X攻击和不同数据集的强大鲁棒性。最后，我们展示了基于原始和激活之间的输出差异的测试时间检测和纠正方法的扩展。",
    "tldr": "该论文提出了一种改进的激活剪裁方法用于普适性后门缓解和测试时间检测，该方法通过限制激活边界来提高对后门攻击的鲁棒性并在图像分类任务中表现出优越性能。",
    "en_tdlr": "This paper presents an improved activation clipping method for universal backdoor mitigation and test-time detection. The method improves robustness against backdoor attacks by limiting activation bounds and achieves superior performance in image classification tasks."
}