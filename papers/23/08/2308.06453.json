{
    "title": "Multi-Label Knowledge Distillation. (arXiv:2308.06453v1 [cs.LG])",
    "abstract": "Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the pr",
    "link": "http://arxiv.org/abs/2308.06453",
    "context": "Title: Multi-Label Knowledge Distillation. (arXiv:2308.06453v1 [cs.LG])\nAbstract: Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the pr",
    "path": "papers/23/08/2308.06453.json",
    "total_tokens": 868,
    "translated_title": "多标签知识蒸馏",
    "translated_abstract": "现有的知识蒸馏方法通常通过将教师网络的输出逻辑回归或中间特征映射传授给学生网络来工作，在多类单标签学习中非常成功。然而，在多标签学习场景下，这些方法很难推广，每个实例都与多个语义标签相关，因为预测概率不等于一，并且在这种情况下，整个示例的特征映射可能会忽略小类别。在本文中，我们提出了一种新颖的多标签知识蒸馏方法。一方面，它通过将多标签学习问题分解为一组二分类问题来利用逻辑回归中的信息性语义知识；另一方面，它通过利用标签嵌入的结构信息来增强学习到的特征表示的独特性。多个基准数据集上的实验结果验证了该方法的有效性。",
    "tldr": "这篇论文提出了一种专门针对多标签学习的新颖知识蒸馏方法，通过将多标签问题分解为一组二分类问题以利用逻辑回归的信息性语义知识，并通过利用标签嵌入的结构信息增强学习到的特征表示的独特性。"
}