{
    "title": "Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models. (arXiv:2308.14850v1 [cs.CL])",
    "abstract": "This report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.",
    "link": "http://arxiv.org/abs/2308.14850",
    "context": "Title: Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models. (arXiv:2308.14850v1 [cs.CL])\nAbstract: This report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.",
    "path": "papers/23/08/2308.14850.json",
    "total_tokens": 699,
    "translated_title": "Attention Visualizer Package:揭示编码器-只有的Transformer模型中单词重要性的注意力可视化工具",
    "translated_abstract": "本文介绍了Attention Visualizer包，该包用于视觉化展示编码器-只有的Transformer模型中个别单词的重要性。与其他关注标记和自注意力分数的方法相比，我们的方法将研究单词及其对最终嵌入表示的影响。这样的库在增强神经网络的解释性和可解释性方面起着关键作用。它们提供了了解其内部机制、提高其性能的更好理解的机会。您可以访问以下GitHub存储库获取代码并查看示例：https://github.com/AlaFalaki/AttentionVisualizer。",
    "tldr": "这个论文介绍了Attention Visualizer包，通过可视化展示单词在编码器-只有的Transformer模型中的重要性，提高了对神经网络的解释性和可解释性。",
    "en_tdlr": "This paper introduces the Attention Visualizer package, which enhances the interpretability and explainability of neural networks by visually illustrating the importance of words in encoder-only transformer models."
}