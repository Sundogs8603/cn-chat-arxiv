{
    "title": "LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying. (arXiv:2308.13542v1 [cs.AI])",
    "abstract": "Large language models (LLMs) have recently demonstrated their impressive ability to provide context-aware responses via text. This ability could potentially be used to predict plausible solutions in sequential decision making tasks pertaining to pattern completion. For example, by observing a partial stack of cubes, LLMs can predict the correct sequence in which the remaining cubes should be stacked by extrapolating the observed patterns (e.g., cube sizes, colors or other attributes) in the partial stack. In this work, we introduce LaGR (Language-Guided Reinforcement learning), which uses this predictive ability of LLMs to propose solutions to tasks that have been partially completed by a primary reinforcement learning (RL) agent, in order to subsequently guide the latter's training. However, as RL training is generally not sample-efficient, deploying this approach would inherently imply that the LLM be repeatedly queried for solutions; a process that can be expensive and infeasible. T",
    "link": "http://arxiv.org/abs/2308.13542",
    "context": "Title: LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying. (arXiv:2308.13542v1 [cs.AI])\nAbstract: Large language models (LLMs) have recently demonstrated their impressive ability to provide context-aware responses via text. This ability could potentially be used to predict plausible solutions in sequential decision making tasks pertaining to pattern completion. For example, by observing a partial stack of cubes, LLMs can predict the correct sequence in which the remaining cubes should be stacked by extrapolating the observed patterns (e.g., cube sizes, colors or other attributes) in the partial stack. In this work, we introduce LaGR (Language-Guided Reinforcement learning), which uses this predictive ability of LLMs to propose solutions to tasks that have been partially completed by a primary reinforcement learning (RL) agent, in order to subsequently guide the latter's training. However, as RL training is generally not sample-efficient, deploying this approach would inherently imply that the LLM be repeatedly queried for solutions; a process that can be expensive and infeasible. T",
    "path": "papers/23/08/2308.13542.json",
    "total_tokens": 899,
    "translated_title": "LaGR-SEQ: 语言引导的强化学习与高效查询",
    "translated_abstract": "最近，大型语言模型（LLMs）展示出了他们通过文本提供上下文感知的印象深刻能力。这种能力可以潜在地用于在序列决策任务中预测可能的解决方案，这些任务与模式完成有关。例如，通过观察部分堆叠的立方体，LLMs可以通过推断观察到的模式（如立方体的大小、颜色或其他属性）来预测剩余立方体应该堆叠的正确顺序。在这项工作中，我们介绍了LaGR（语言引导的强化学习），它利用LLMs的这种预测能力，提出由主强化学习（RL）代理部分完成的任务的解决方案，随后引导后者的训练。然而，由于RL训练通常不具备高效样本利用率，采用这种方法将固有地意味着需要反复查询LLMs来获取解决方案；这个过程可能是昂贵的和不可行的。",
    "tldr": "这个论文介绍了LaGR（语言引导的强化学习），它利用大型语言模型（LLMs）的预测能力，提出解决部分完成任务的解决方案，以引导强化学习代理的训练。",
    "en_tdlr": "This paper introduces LaGR (Language-Guided Reinforcement learning), which utilizes the predictive ability of large language models (LLMs) to propose solutions for partially completed tasks, guiding the training of reinforcement learning agents."
}