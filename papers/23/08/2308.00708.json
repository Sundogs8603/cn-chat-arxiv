{
    "title": "VeriGen: A Large Language Model for Verilog Code Generation. (arXiv:2308.00708v1 [cs.PL])",
    "abstract": "In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of small",
    "link": "http://arxiv.org/abs/2308.00708",
    "context": "Title: VeriGen: A Large Language Model for Verilog Code Generation. (arXiv:2308.00708v1 [cs.PL])\nAbstract: In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of small",
    "path": "papers/23/08/2308.00708.json",
    "total_tokens": 919,
    "translated_title": "VeriGen: 一种用于Verilog代码生成的大型语言模型",
    "translated_abstract": "在这项研究中，我们探索了大型语言模型（LLMs）在自动化硬件设计方面的能力，通过生成高质量的Verilog代码，Verilog是一种常用的用于设计和建模数字系统的语言。我们在GitHub和Verilog教材编译的Verilog数据集上对预先存在的LLMs进行了微调。我们使用特殊设计的测试套件评估了生成的Verilog代码的功能正确性，该套件包含自定义问题集和测试工作台。在这里，我们的开源CodeGen-16B模型在功能正确性方面优于商业领先的GPT-3.5-turbo模型，整体提高了1.1％。在对更多样化和复杂的问题集进行测试时，我们发现微调模型在与GPT-3.5-turbo模型相比具有竞争性的性能，在特定场景中表现出色。值得注意的是，与预训练模型相比，它在生成各种问题类别的语法正确Verilog代码方面提高了41％，突显了小型模型的潜力。",
    "tldr": "VeriGen是一种大型语言模型，用于自动化生成高质量的Verilog代码。它在生成的功能正确性和语法正确性方面优于商业GPT-3.5-turbo模型并且显示出竞争性的性能。",
    "en_tdlr": "VeriGen is a large language model for automating high-quality Verilog code generation. It outperforms the commercial GPT-3.5-turbo model in both functional correctness and syntactic correctness, showing competitive performance."
}