{
    "title": "Optimizing a Transformer-based network for a deep learning seismic processing workflow. (arXiv:2308.04739v1 [physics.geo-ph])",
    "abstract": "StorSeismic is a recently introduced model based on the Transformer to adapt to various seismic processing tasks through its pretraining and fine-tuning training strategy. In the original implementation, StorSeismic utilized a sinusoidal positional encoding and a conventional self-attention mechanism, both borrowed from the natural language processing (NLP) applications. For seismic processing they admitted good results, but also hinted to limitations in efficiency and expressiveness. We propose modifications to these two key components, by utilizing relative positional encoding and low-rank attention matrices as replacements to the vanilla ones. The proposed changes are tested on processing tasks applied to a realistic Marmousi and offshore field data as a sequential strategy, starting from denoising, direct arrival removal, multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$) prediction for normal moveout (NMO) correction. We observe faster pretraining and competi",
    "link": "http://arxiv.org/abs/2308.04739",
    "context": "Title: Optimizing a Transformer-based network for a deep learning seismic processing workflow. (arXiv:2308.04739v1 [physics.geo-ph])\nAbstract: StorSeismic is a recently introduced model based on the Transformer to adapt to various seismic processing tasks through its pretraining and fine-tuning training strategy. In the original implementation, StorSeismic utilized a sinusoidal positional encoding and a conventional self-attention mechanism, both borrowed from the natural language processing (NLP) applications. For seismic processing they admitted good results, but also hinted to limitations in efficiency and expressiveness. We propose modifications to these two key components, by utilizing relative positional encoding and low-rank attention matrices as replacements to the vanilla ones. The proposed changes are tested on processing tasks applied to a realistic Marmousi and offshore field data as a sequential strategy, starting from denoising, direct arrival removal, multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$) prediction for normal moveout (NMO) correction. We observe faster pretraining and competi",
    "path": "papers/23/08/2308.04739.json",
    "total_tokens": 862,
    "translated_title": "优化基于Transformer的深度学习地震处理工作流程的网络",
    "translated_abstract": "StorSeismic是一个基于Transformer的模型，通过其预训练和微调训练策略适应各种地震处理任务。在原始实现中，StorSeismic利用了从自然语言处理(NLP)应用中借鉴的正弦位置编码和传统的自注意机制。对于地震处理，它们取得了良好的结果，但也提到了效率和表现力的限制。我们提出了对这两个关键组件的修改，通过使用相对位置编码和低秩注意力矩阵来替代原始组件。我们在实际的Marmousi和海上场数据上对这些改进进行了测试，作为一个序列策略，从降噪、直达波去除、多次衰减，最后到均方根速度(VRMS)预测用于正常移动补偿(NMO)校正。我们观察到预训练速度更快，并且能够达到竞争性的效果。",
    "tldr": "这项研究对基于Transformer的地震处理网络StorSeismic进行优化，通过改进位置编码和注意力机制，提高了其效率和表现力，并在实际数据上取得了良好的结果。"
}