{
    "title": "Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])",
    "abstract": "Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren",
    "link": "http://arxiv.org/abs/2308.09726",
    "context": "Title: Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])\nAbstract: Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren",
    "path": "papers/23/08/2308.09726.json",
    "total_tokens": 925,
    "translated_title": "公平的不躁动多臂赌博机：受数字健康启发的通用框架",
    "translated_abstract": "不躁动多臂赌博机（RMABs）是一种在资源有限的连续环境中进行算法决策的流行框架。RMABs越来越被用于公共卫生、治疗安排、反偷猎等敏感决策，而本研究的动机正是数字健康。在这些高风险环境中，决策必须改善结果并避免不同群体之间的差距（例如，确保健康公平）。我们首次研究了RMABs的公平目标（ERMABs）。我们考虑了公平文献中两种公平性相关的目标，最小化最大化奖励和最大化纳什福利。我们为解决这两个目标开发了高效的算法——对于前者使用了水位填充算法，对于后者使用了理论上有动机的贪心算法来平衡不同群体大小。最后，我们通过三个模拟领域（包括一个新的数字健康模型）的实验证明，我们的方法在公平性方面可以比当前方法提高数倍。",
    "tldr": "这项研究提出了不躁动多臂赌博机的公平目标，并开发了相应的算法，证明在数字健康等领域中可以提高数倍的公平性。"
}