{
    "title": "Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series. (arXiv:2308.03210v2 [cs.LG] UPDATED)",
    "abstract": "Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares sim",
    "link": "http://arxiv.org/abs/2308.03210",
    "context": "Title: Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series. (arXiv:2308.03210v2 [cs.LG] UPDATED)\nAbstract: Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares sim",
    "path": "papers/23/08/2308.03210.json",
    "total_tokens": 877,
    "translated_title": "不规则采样时间序列的时间参数化卷积神经网络",
    "translated_abstract": "不规则采样的多变量时间序列在许多应用领域中普遍存在，导致观测值稀疏、不完全且不对齐。标准的顺序神经网络架构，如循环神经网络（RNN）和卷积神经网络（CNN），考虑了观测时间之间的正则间隙，对不规则时间序列建模带来了重大挑战。虽然大多数提出的架构都采用RNN变体来处理不规则时间间隔，但卷积神经网络在不规则采样环境下的研究还不充分。在本文中，我们通过使用时间显式初始化的核函数来参数化卷积层。时间的这种通用函数增强了连续时间隐藏动态的学习过程，并且可以高效地合并到卷积核权重中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它共享了相同的...",
    "tldr": "本研究提出了时间参数化卷积神经网络（TPCNN），通过时间显式初始化的核函数来参数化卷积层，以处理不规则采样的多变量时间序列。这一方法增强了对连续时间隐藏动态的学习。",
    "en_tdlr": "This paper introduces the time-parameterized convolutional neural network (TPCNN), which leverages time-explicitly initialized kernels to handle irregularly sampled multivariate time series. This approach enhances the learning of continuous-time hidden dynamics."
}