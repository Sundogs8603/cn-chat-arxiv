{
    "title": "Learning to Paraphrase Sentences to Different Complexity Levels. (arXiv:2308.02226v1 [cs.CL])",
    "abstract": "While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
    "link": "http://arxiv.org/abs/2308.02226",
    "context": "Title: Learning to Paraphrase Sentences to Different Complexity Levels. (arXiv:2308.02226v1 [cs.CL])\nAbstract: While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
    "path": "papers/23/08/2308.02226.json",
    "total_tokens": 880,
    "translated_title": "学习将句子改写为不同的复杂程度",
    "translated_abstract": "虽然句子简化是自然语言处理中的一个研究热点，但句子复杂化和同级改写这两个相邻任务却没有得到足够的关注。为了训练模型在这三个任务上，我们提出了两个新的无监督数据集。我们将这两个数据集与一个单一的有监督数据集进行比较，一个数据集由一个弱分类器标注，另一个数据集由一种基于规则的方法标注。使用这三个数据集进行训练，在多任务和提示策略上进行了大量实验。与其他使用无监督平行数据训练的系统相比，我们训练的模型在ASSET简化基准测试中取得了最先进的性能。我们的模型也优于先前的句子级目标化工作。最后，我们还确定了一些大型语言模型在零-shot设置下在这些任务上的表现。",
    "tldr": "本论文提出了学习将句子改写为不同复杂程度的方法，通过比较不同的数据集和采用多任务和提示策略，实验结果表明我们的模型在句子简化和同级改写任务上取得了最先进的性能，并在没有训练数据的情况下评估了一些大型语言模型的效果。",
    "en_tdlr": "This paper proposes a method for learning to paraphrase sentences to different complexity levels. Through comparing different datasets and employing multitasking and prompting strategies, the experimental results demonstrate that our models achieve state-of-the-art performance on sentence simplification and same-level paraphrasing tasks, and evaluate the effectiveness of several large language models under a zero-shot setting."
}