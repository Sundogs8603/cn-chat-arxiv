{
    "title": "Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])",
    "abstract": "Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ",
    "link": "http://arxiv.org/abs/2308.06093",
    "context": "Title: Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])\nAbstract: Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ",
    "path": "papers/23/08/2308.06093.json",
    "total_tokens": 887,
    "translated_title": "专家权重平均化: 一种视觉Transformer的新通用训练方案",
    "translated_abstract": "结构重参数化是一种用于卷积神经网络（CNNs）的通用训练方案，可以在不增加推断成本的情况下提高性能。随着视觉Transformer (ViTs)在各种视觉任务中逐渐超越CNNs，一个问题出现了: 是否存在一种专门用于ViTs的训练方案，可以在不增加推断成本的情况下提高性能？最近，混合专家（MoE）引起了越来越多的关注，因为它可以通过稀疏激活的专家有效地扩展Transformer的容量，而成本保持不变。考虑到MoE也可以看作是一种多支系结构，我们能否利用MoE来实现类似结构重参数化的ViT训练方案呢？在本文中，我们肯定地回答了这些问题，并提出了一种新的ViTs通用训练策略。具体来说，我们分离了ViTs的训练和推断阶段。在训练阶段，我们替换了一些前馈网络（FFNs）",
    "tldr": "本文提出了一种新的通用训练策略，利用专家权重平均化实现了对ViTs的性能提升，而不增加推断成本。",
    "en_tdlr": "This paper proposes a new general training strategy that utilizes expert weights averaging to improve the performance of Vision Transformers (ViTs) without increasing inference cost."
}