{
    "title": "POLCA: Power Oversubscription in LLM Cloud Providers. (arXiv:2308.12908v1 [cs.DC])",
    "abstract": "Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.  We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the avera",
    "link": "http://arxiv.org/abs/2308.12908",
    "context": "Title: POLCA: Power Oversubscription in LLM Cloud Providers. (arXiv:2308.12908v1 [cs.DC])\nAbstract: Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.  We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the avera",
    "path": "papers/23/08/2308.12908.json",
    "total_tokens": 952,
    "translated_title": "POLCA：LLM云服务提供商中的功率超额使用",
    "translated_abstract": "近期大型语言模型（LLM）的创新及其各种用例迅速推高了数据中心GPU的计算能力需求。几家云服务提供商和其他企业已经制定了大规模扩张计划，以支持这些新工作负载。数据中心的关键瓶颈资源之一是电力，而随着LLM模型规模的增大，它们的功耗也越来越高。本文展示了在LLM集群中存在显著的功率超额使用机会。功率超额使用提高了这些数据中心的功率效率，允许每个数据中心部署更多的服务器，并减少了部署时间，因为建设新的数据中心很慢。我们广泛地表征了各种LLM及其配置的功耗模式。我们确定了推理和训练功耗模式之间的差异。根据我们对这些LLM的分析，我们声称平均开销程度的功耗超额使用可以提高数据中心的能效。",
    "tldr": "本文研究了在大型语言模型（LLM）云服务提供商中的功率超额使用问题。通过对多种LLM及其不同配置的功耗模式进行分析，我们发现在LLM集群中存在显著的功率超额使用机会，这可以提高数据中心的功率效率，并且允许更多的服务器部署，同时减少部署时间。",
    "en_tdlr": "This paper investigates the power oversubscription problem in LLM cloud providers. By analyzing the power consumption patterns of various LLMs and their configurations, we find significant opportunities for power oversubscription in LLM clusters, which can improve power efficiency in data centers, allow for more server deployments, and reduce deployment time."
}