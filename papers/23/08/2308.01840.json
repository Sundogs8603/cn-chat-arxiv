{
    "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])",
    "abstract": "Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi",
    "link": "http://arxiv.org/abs/2308.01840",
    "context": "Title: URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])\nAbstract: Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi",
    "path": "papers/23/08/2308.01840.json",
    "total_tokens": 863,
    "translated_title": "URET: 通用鲁棒性评估工具包（用于逃避攻击）",
    "translated_abstract": "众所周知，机器学习模型容易受到逃避攻击的影响，如图像分类模型所示。充分了解这种攻击对于确保关键AI任务的安全和鲁棒性至关重要。然而，大多数逃避攻击很难对大多数AI系统进行部署，因为它们仅集中在图像领域并具有少数约束。与实践中使用的其他输入类型不同，图像由均匀的、数值的、连续的和独立的特征组成。此外，某些输入类型包含额外的语义和功能约束，必须遵守以生成逼真的对抗性输入。在这项工作中，我们提出了一个新的框架，可以生成与输入类型和任务领域无关的对抗性输入。给定一个输入和一组预定义的输入转换，我们的框架可以发现一系列转换，从而得到一个符合语义和功能要求的正确结果。",
    "tldr": "本论文提出了一个名为URET的通用鲁棒性评估工具包，该工具包可以生成各种类型和任务领域下的对抗性输入，以确保关键AI任务的安全和鲁棒性。",
    "en_tdlr": "This paper proposes a universal robustness evaluation toolkit (URET) that can generate adversarial inputs for various types and tasks, aiming to ensure the safety and robustness of critical AI tasks."
}