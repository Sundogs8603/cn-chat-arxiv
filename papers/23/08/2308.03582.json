{
    "title": "WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset. (arXiv:2308.03582v2 [cs.CL] UPDATED)",
    "abstract": "A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to 'learn' new information. While model-centric solutions like continual learning or parameter-efficient fine tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resource can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic, and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, sh",
    "link": "http://arxiv.org/abs/2308.03582",
    "context": "Title: WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset. (arXiv:2308.03582v2 [cs.CL] UPDATED)\nAbstract: A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to 'learn' new information. While model-centric solutions like continual learning or parameter-efficient fine tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resource can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic, and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, sh",
    "path": "papers/23/08/2308.03582.json",
    "total_tokens": 915,
    "translated_title": "WIKITIDE: 一个基于维基百科时间戳定义对数据集的研究",
    "translated_abstract": "目前自然语言处理领域的一个基本挑战是，由语言模型主导的情况下，现有体系架构对于学习新信息的灵活度不够。虽然存在基于模型的解决方案，如连续学习或参数高效微调，但仍然存在如何可靠地识别语言或世界变化的问题。本文提出了WikiTiDe，这是一个从维基百科中提取的时间戳定义对数据集。我们认为这样的资源有助于加速历时性自然语言处理，并用于训练模型，使其能够从知识资源中扫描与概念、事件或命名实体相关的核心更新。我们提出的端到端方法是完全自动的，并利用自举算法逐步创建高质量的数据集。我们的研究结果表明，通过自举WikiTiDe的种子版本可以获得更好的微调模型。我们还在许多下游任务中应用了微调模型。",
    "tldr": "本文提出了WikiTiDe，这是一个从维基百科中提取的时间戳定义对数据集，该数据集可用于加速历时性自然语言处理，并训练模型以扫描核心更新。通过自举种子版本，可以获得更好的微调模型。",
    "en_tdlr": "This paper introduces WikiTiDe, a dataset extracted from Wikipedia that consists of pairs of timestamped definitions. The dataset can be used to accelerate diachronic NLP and train models to scan for core updates. Bootstraping the seed version leads to better fine-tuned models."
}