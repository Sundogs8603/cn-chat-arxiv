{
    "title": "FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)",
    "abstract": "Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied. However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. To address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of LLMs, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. Additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. Using the education sector as a case study, we constructed the Edu-FairMonitor based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensit",
    "link": "http://arxiv.org/abs/2308.10397",
    "context": "Title: FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)\nAbstract: Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied. However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. To address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of LLMs, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. Additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. Using the education sector as a case study, we constructed the Edu-FairMonitor based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensit",
    "path": "papers/23/08/2308.10397.json",
    "total_tokens": 1020,
    "translated_title": "FairMonitor: 一种检测大型语言模型中刻板印象和偏见的四阶段自动框架",
    "translated_abstract": "检测大型语言模型（LLMs）中的刻板印象和偏见可以增强公平性并减少这些LLMs应用时对个人或群体的不利影响。然而，现有方法大多关注于测量模型对包含偏见和刻板印象的句子的偏好，这种方法缺乏可解释性且无法检测真实世界中的隐含偏见和刻板印象。为填补这一空白，本文引入了一个四阶段框架，直接评估LLMs生成内容中的刻板印象和偏见，包括直接询问测试、串行或适应性故事测试、隐性关联测试和未知情境测试。此外，本文还提出了多维评估指标和可解释的零样本提示，用于自动评估。以教育部门为案例研究，我们基于这个四阶段框架构建了Edu-FairMonitor，该框架包括12,632个开放式问题，涵盖九个敏感议题。",
    "tldr": "本文引入了一个四阶段框架，可以直接评估大型语言模型（LLMs）生成内容中的刻板印象和偏见，包括直接询问测试、串行或适应性故事测试、隐性关联测试和未知情境测试。此外，还提出了多维评估指标和可解释的零样本提示，并以教育部门为案例研究构建了Edu-FairMonitor框架。"
}