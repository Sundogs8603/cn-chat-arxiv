{
    "title": "Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])",
    "abstract": "Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level ",
    "link": "http://arxiv.org/abs/2308.06488",
    "context": "Title: Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])\nAbstract: Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level ",
    "path": "papers/23/08/2308.06488.json",
    "total_tokens": 824,
    "translated_title": "无噪声参考文本的知识图生成忠实文本",
    "translated_abstract": "知识图（KG）到文本的生成旨在生成流畅的自然语言文本，准确地表示给定知识图的信息。虽然通过利用预训练语言模型（PLMs）与适当的图结构感知模块在这个任务上取得了显著的进展，但现有模型在生成忠实文本方面仍存在不足，特别是当地面真实语言文本中包含图中不存在的额外信息时。本文提出了一种KG到文本生成模型，能够在存在噪声参考文本的情况下，从给定的图生成忠实的自然语言文本。我们的框架融合了两个核心思想：首先，我们利用对比学习增强模型区分文本中的忠实和虚构信息的能力，从而鼓励解码器生成与输入图对齐的文本。其次，我们赋予解码器控制文本生成过程的能力，",
    "tldr": "本文提出了一种KG到文本生成模型，能够在存在噪声参考文本的情况下，从给定的图生成忠实的自然语言文本"
}