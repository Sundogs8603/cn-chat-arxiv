{
    "title": "Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])",
    "abstract": "Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.",
    "link": "http://arxiv.org/abs/2308.00989",
    "context": "Title: Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])\nAbstract: Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.",
    "path": "papers/23/08/2308.00989.json",
    "total_tokens": 887,
    "translated_title": "基于Wasserstein多样性增强正则化器的层次化强化学习",
    "translated_abstract": "层次化强化学习通过将不同层次的子策略组合起来完成复杂任务。自动发现子策略是一种不依赖于领域知识的生成子策略的有前景的方法。然而，存在方法很难处理的退化问题，这是由于缺乏对多样性的考虑或使用弱正则化器。在本文中，我们提出了一种新的与任务无关的正则化器，称为Wasserstein多样性增强正则化器（WDER），通过最大化动作分布之间的Wasserstein距离来增加子策略的多样性。所提出的WDER可以轻松地融入到现有方法的损失函数中，进一步提高它们的性能。实验结果表明，相比于之前的工作，在不修改超参数的情况下，我们的WDER提高了性能和样本效率，这表明了WDER的适用性和鲁棒性。",
    "tldr": "本文中，我们提出了一种新的任务无关正则化器WDER，通过增加子策略的多样性来解决层次化强化学习中的退化问题。实验证明，WDER能够提高性能和样本效率，并且不需要修改超参数。"
}