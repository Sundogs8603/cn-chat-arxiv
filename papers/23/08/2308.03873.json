{
    "title": "Evaluating and Explaining Large Language Models for Code Using Syntactic Structures. (arXiv:2308.03873v1 [cs.SE])",
    "abstract": "Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.  To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enab",
    "link": "http://arxiv.org/abs/2308.03873",
    "context": "Title: Evaluating and Explaining Large Language Models for Code Using Syntactic Structures. (arXiv:2308.03873v1 [cs.SE])\nAbstract: Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.  To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enab",
    "path": "papers/23/08/2308.03873.json",
    "total_tokens": 905,
    "translated_title": "评估和解释大型语言模型在代码中使用语法结构的效果",
    "translated_abstract": "代码的大型语言模型（LLM）是一类基于变换器的高参数神经网络，预训练于海量自然语言和编程语言数据集。这些模型正在快速应用于商业化的基于人工智能的开发者工具，如GitHub CoPilot。然而，鉴于其规模和复杂性，衡量和解释它们在程序任务上的有效性是一项具有挑战性的任务。评估和解释代码的LLM方法是密不可分的。也就是说，为了解释模型的预测，必须将其可靠地映射到细粒度、可理解的概念上。一旦实现了这种映射，就可以开展新的详细模型评估方法。然而，目前大多数的解释能力技术和评估基准都集中在模型的健壮性或单个任务性能上，而非解释模型预测。为此，本文介绍了ASTxplainer，一种针对代码的LLM的解释方法，可以实现模型的可解释性。",
    "tldr": "本文介绍了一种针对代码的大型语言模型（LLM）的解释方法ASTxplainer，该方法能够可靠地将模型的预测映射到可理解的概念上，从而实现模型的可解释性。",
    "en_tdlr": "This paper introduces ASTxplainer, an explainability method specific to large language models (LLMs) for code, which reliably maps the model's predictions to understandable concepts, enabling interpretability of the model."
}