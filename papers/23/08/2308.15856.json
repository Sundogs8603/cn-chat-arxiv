{
    "title": "Domain Generalization without Excess Empirical Risk. (arXiv:2308.15856v1 [cs.LG])",
    "abstract": "Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, i.e., in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization",
    "link": "http://arxiv.org/abs/2308.15856",
    "context": "Title: Domain Generalization without Excess Empirical Risk. (arXiv:2308.15856v1 [cs.LG])\nAbstract: Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, i.e., in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization",
    "path": "papers/23/08/2308.15856.json",
    "total_tokens": 867,
    "translated_title": "不需要过量经验风险的域泛化",
    "translated_abstract": "在给定不同分布的多样数据集的情况下，域泛化旨在学习可以推广到未见分布的模型。一种常见的方法是设计一个数据驱动的替代惩罚来捕捉泛化性能，并与惩罚一起最小化经验风险。我们认为这种方法的一个重要失败模式是由于错误的惩罚或联合优化的困难而导致的过量风险。我们提出了一种解决这个问题的方法。我们不是将经验风险和惩罚联合最小化，而是在保证经验风险最优的约束下最小化惩罚。这种改变保证了域泛化惩罚不会影响对经验风险的优化，即在分布内的性能。为了解决这个优化问题，我们展示了与率失真理论的令人兴奋的联系，并利用其工具设计了一个高效的方法。我们的方法可以应用于任何基于惩罚的域泛化问题。",
    "tldr": "本论文提出了一种解决域泛化中过量风险问题的方法，通过在保证经验风险最优的约束下最小化惩罚，避免了惩罚对经验风险优化的影响。"
}