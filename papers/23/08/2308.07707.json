{
    "title": "Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])",
    "abstract": "Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require lon",
    "link": "http://arxiv.org/abs/2308.07707",
    "context": "Title: Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])\nAbstract: Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require lon",
    "path": "papers/23/08/2308.07707.json",
    "total_tokens": 875,
    "translated_title": "通过选择性突触减弱实现快速的机器遗忘无需重新训练",
    "translated_abstract": "机器遗忘，即机器学习模型的遗忘能力，正在越来越重要，以便符合数据隐私法规，并删除有害、篡改或过时的信息。关键挑战在于在保护模型在其余数据上的性能的同时遗忘特定信息。虽然目前的最先进方法表现良好，但通常需要在保留数据上进行某种程度的重新训练，以保护或恢复模型性能。这增加了计算开销，并要求训练数据保持可用和可访问，这可能并不可行。相反，其他方法采用无需重新训练的范式，但这些方法在计算上代价过高，并且性能不及重新训练的对应方法。我们提出选择性突触减弱（SSD），一种新颖的两步后验、无需重新训练的机器遗忘方法，快速、性能优越，并且不需要长时间的模型训练。",
    "tldr": "选择性突触减弱（SSD）是快速、性能优越且无需重新训练的机器遗忘方法，能够同时保护模型的性能并遗忘特定信息。"
}