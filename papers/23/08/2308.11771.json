{
    "title": "3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network. (arXiv:2308.11771v1 [cs.CV])",
    "abstract": "This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\\times$ without losing accuracy when tested on a \\texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \\url{https://github.com/qinche106/cb-convlstm-eyetracking}.",
    "link": "http://arxiv.org/abs/2308.11771",
    "context": "Title: 3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network. (arXiv:2308.11771v1 [cs.CV])\nAbstract: This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\\times$ without losing accuracy when tested on a \\texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \\url{https://github.com/qinche106/cb-convlstm-eyetracking}.",
    "path": "papers/23/08/2308.11771.json",
    "total_tokens": 996,
    "translated_title": "3ET: 使用基于变化的ConvLSTM网络的高效事件驱动眼球追踪",
    "translated_abstract": "本文提出了一种稀疏的基于变化的Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于事件驱动眼球追踪，这是下一代可穿戴医疗技术（如AR/VR头盔）的关键。我们利用视网膜灵感的事件相机的低延迟响应和稀疏输出事件流的优势，超过传统的基于帧的相机。我们的CB-ConvLSTM架构能够高效地从事件流中提取时空特征，用于瞳孔追踪，优于传统的CNN结构。通过利用增强激活稀疏性的增量编码循环路径，CB-ConvLSTM在经过标记的瞳孔数据集上的测试中，在不损失准确性的情况下，将算术操作减少了约4.7倍。这种提高效率的增加使其非常适合在资源有限的设备上进行实时眼球追踪。项目代码和数据集可在\\url{https://github.com/qinche106/cb-convlstm-eyetracking}中公开获取。",
    "tldr": "本文介绍了一种3ET方法，即使用基于变化的ConvLSTM网络的高效事件驱动眼球追踪的模型。该模型利用事件相机的优势，在标记的瞳孔数据集上实现了高效的时空特征提取和准确的瞳孔追踪，适用于资源有限的设备。",
    "en_tdlr": "This paper presents the 3ET method, an efficient event-based eye tracking model using a change-based ConvLSTM network. The model leverages the advantages of event cameras for efficient spatio-temporal feature extraction and accurate pupil tracking on labeled datasets, making it suitable for resource-constrained devices."
}