{
    "title": "Bayesian Inverse Transition Learning for Offline Settings. (arXiv:2308.05075v1 [cs.LG])",
    "abstract": "Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with th",
    "link": "http://arxiv.org/abs/2308.05075",
    "context": "Title: Bayesian Inverse Transition Learning for Offline Settings. (arXiv:2308.05075v1 [cs.LG])\nAbstract: Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with th",
    "path": "papers/23/08/2308.05075.json",
    "total_tokens": 896,
    "translated_title": "具有贝叶斯逆转换学习的离线设置",
    "translated_abstract": "离线强化学习通常用于在诸如医疗保健和教育等领域进行顺序决策，其中奖励是已知的，并且必须基于批量数据估计过渡动态T。对于所有任务而言，一个关键挑战是如何学习一个可靠的过渡动态T估计，这些估计能够产生接近最优策略，并且足够安全，以至于它们从未采取远离最佳动作的行动，并且足够信息丰富，以传达其所具有的不确定性。使用来自专家的数据，我们提出了一种新的基于约束的方法，用于捕捉我们可靠地学习过渡动态T后验分布的需要，而这些分布又不涉及梯度。我们的结果表明，通过使用我们的约束条件，我们可以学习一个高性能政策，同时在不同数据集上显著减少政策的方差。我们还解释了如何将不确定性估计与…",
    "tldr": "该论文提出了一种用于离线设置的贝叶斯逆转换学习方法，通过结合专家数据，可可靠地学习过渡动态T的后验分布，并且在不同数据集上显著减少政策的方差。"
}