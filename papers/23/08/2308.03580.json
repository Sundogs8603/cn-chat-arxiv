{
    "title": "Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization. (arXiv:2308.03580v2 [cs.CV] UPDATED)",
    "abstract": "Supervised deep learning models require significant amount of labelled data to achieve an acceptable performance on a specific task. However, when tested on unseen data, the models may not perform well. Therefore, the models need to be trained with additional and varying labelled data to improve the generalization. In this work, our goal is to understand the models, their performance and generalization. We establish image-image, dataset-dataset, and image-dataset distances to gain insights into the model's behavior. Our proposed distance metric when combined with model performance can help in selecting an appropriate model/architecture from a pool of candidate architectures. We have shown that the generalization of these models can be improved by only adding a small number of unseen images (say 1, 3 or 7) into the training set. Our proposed approach reduces training and annotation costs while providing an estimate of model performance on unseen data in dynamic environments.",
    "link": "http://arxiv.org/abs/2308.03580",
    "context": "Title: Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization. (arXiv:2308.03580v2 [cs.CV] UPDATED)\nAbstract: Supervised deep learning models require significant amount of labelled data to achieve an acceptable performance on a specific task. However, when tested on unseen data, the models may not perform well. Therefore, the models need to be trained with additional and varying labelled data to improve the generalization. In this work, our goal is to understand the models, their performance and generalization. We establish image-image, dataset-dataset, and image-dataset distances to gain insights into the model's behavior. Our proposed distance metric when combined with model performance can help in selecting an appropriate model/architecture from a pool of candidate architectures. We have shown that the generalization of these models can be improved by only adding a small number of unseen images (say 1, 3 or 7) into the training set. Our proposed approach reduces training and annotation costs while providing an estimate of model performance on unseen data in dynamic environments.",
    "path": "papers/23/08/2308.03580.json",
    "total_tokens": 977,
    "translated_title": "揭示潜在模式：研究数据集的相似性、性能和泛化能力",
    "translated_abstract": "监督深度学习模型需要大量标记数据才能在特定任务上取得可接受的性能。然而，当在未见过的数据上进行测试时，模型可能表现不佳。因此，需要用额外和多样化的标记数据来训练模型以提高泛化能力。本研究旨在理解模型、它们的性能和泛化能力。我们建立了图像-图像、数据集-数据集和图像-数据集距离，以洞察模型的行为。我们提出的距离度量方法结合模型性能可以帮助从候选架构中选择一个合适的模型/架构。我们发现，只需将少量未见过的图像（如1、3或7个）添加到训练集中即可改善这些模型的泛化能力。我们提出的方法可以在动态环境中减少训练和标注成本，并提供模型在未见数据上的性能估计。",
    "tldr": "该研究探索了监督深度学习模型的泛化能力和性能，并提出了一种结合距离度量和模型性能的方法，从候选架构中选择适当的模型/架构。结果显示，通过添加少量未见过的图像，可以改善模型的泛化能力。这种方法可以降低训练和标注成本，并在动态环境中提供模型在未见数据上的性能估计。",
    "en_tdlr": "This study investigates the generalization and performance of supervised deep learning models and proposes a method that combines distance metrics and model performance to select appropriate architectures. The results show that adding a small number of unseen images can improve the models' generalization, reducing training and annotation costs while providing estimates of model performance on unseen data in dynamic environments."
}