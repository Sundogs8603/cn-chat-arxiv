{
    "title": "Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])",
    "abstract": "There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of",
    "link": "http://arxiv.org/abs/2308.10905",
    "context": "Title: Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])\nAbstract: There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of",
    "path": "papers/23/08/2308.10905.json",
    "total_tokens": 793,
    "translated_title": "在TVM中分析量化",
    "translated_abstract": "在深度学习模型中对权重张量进行量化以减少推理延迟和内存占用的研究已经有很多。TVM也具备支持低比特计算和量化权重的能力。尽管通常期望通过量化来提高推理时间，在TVM中，8位量化的性能却不能满足期望。通常在将8位量化应用于深度学习模型时，通常期望达到全精度推理时间的50%左右。然而，在这种特殊情况下，量化版本不仅未能实现所期望的性能提升，而且实际上性能更差，导致推理时间约为非量化版本的两倍慢。在这个项目中，我们深入探讨了性能不佳的原因，评估了8位量化在TVM中的兼容性和优化机会。我们讨论了两种不同类型的优化方法",
    "tldr": "该论文研究了在TVM中8位量化的性能问题，并讨论了兼容性和优化机会。"
}