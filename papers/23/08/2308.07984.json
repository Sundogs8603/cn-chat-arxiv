{
    "title": "Anaphoric Structure Emerges Between Neural Networks. (arXiv:2308.07984v1 [cs.CL])",
    "abstract": "Pragmatics is core to natural language, enabling speakers to communicate efficiently with structures like ellipsis and anaphora that can shorten utterances without loss of meaning. These structures require a listener to interpret an ambiguous form - like a pronoun - and infer the speaker's intended meaning - who that pronoun refers to. Despite potential to introduce ambiguity, anaphora is ubiquitous across human language. In an effort to better understand the origins of anaphoric structure in natural language, we look to see if analogous structures can emerge between artificial neural networks trained to solve a communicative task. We show that: first, despite the potential for increased ambiguity, languages with anaphoric structures are learnable by neural models. Second, anaphoric structures emerge between models 'naturally' without need for additional constraints. Finally, introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. We con",
    "link": "http://arxiv.org/abs/2308.07984",
    "context": "Title: Anaphoric Structure Emerges Between Neural Networks. (arXiv:2308.07984v1 [cs.CL])\nAbstract: Pragmatics is core to natural language, enabling speakers to communicate efficiently with structures like ellipsis and anaphora that can shorten utterances without loss of meaning. These structures require a listener to interpret an ambiguous form - like a pronoun - and infer the speaker's intended meaning - who that pronoun refers to. Despite potential to introduce ambiguity, anaphora is ubiquitous across human language. In an effort to better understand the origins of anaphoric structure in natural language, we look to see if analogous structures can emerge between artificial neural networks trained to solve a communicative task. We show that: first, despite the potential for increased ambiguity, languages with anaphoric structures are learnable by neural models. Second, anaphoric structures emerge between models 'naturally' without need for additional constraints. Finally, introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. We con",
    "path": "papers/23/08/2308.07984.json",
    "total_tokens": 904,
    "translated_title": "神经网络之间出现了语用性结构",
    "translated_abstract": "语用学对于自然语言至关重要，使得说话者能够通过省略和回指等结构高效地进行交流，而不会丧失意义。这些结构要求听者解释一个多义的形式，比如代词，并且推断说话者的意图来确定代词所指的对象。尽管会引入歧义，回指在人类语言中普遍存在。为了更好地理解自然语言中回指结构的起源，我们研究了是否可以在训练用于解决交流任务的人工神经网络之间出现类似的结构。我们的研究结果表明：首先，尽管可能增加歧义，带有回指结构的语言对神经模型来说是可学习的。其次，回指结构在模型之间“自然”地出现，不需要额外的约束条件。最后，引入对说话者明确的效率压力会增加这些结构的普遍性。",
    "tldr": "本研究通过研究神经网络之间是否能够出现类似于自然语言中回指结构的现象，发现带有回指结构的语言对神经模型来说是可学习的，这些结构会在模型之间自然地出现，并且增加对说话者效率的压力会增加这些结构的普遍性。",
    "en_tdlr": "In this study, we investigate the emergence of anaphoric structure between artificial neural networks and find that languages with anaphoric structures are learnable, these structures emerge naturally between models without constraints, and introducing speaker efficiency pressure increases the prevalence of these structures."
}