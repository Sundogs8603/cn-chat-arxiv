{
    "title": "FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])",
    "abstract": "Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par",
    "link": "http://arxiv.org/abs/2308.16835",
    "context": "Title: FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])\nAbstract: Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par",
    "path": "papers/23/08/2308.16835.json",
    "total_tokens": 827,
    "translated_title": "FedDD: 通过差分参数丢弃实现通信高效的联邦学习",
    "translated_abstract": "联邦学习（FL）需要频繁交换模型参数，这导致了长时间的通信延迟，尤其是当客户端的网络环境差异很大时。此外，参数服务器需要等待最慢的客户端（即straggler，可能具有最大的模型大小、最低的计算能力或最差的网络条件）上传参数，这可能会严重影响通信效率。常用的客户端选择方法，如部分客户端选择，会导致计算资源的浪费并削弱全局模型的泛化能力。为了解决这个问题，在本文中，我们提出了一种不同的方法，即使用模型参数丢弃而不是客户端选择，并据此提出了一种新的差分参数丢弃的联邦学习方案（FedDD）框架。FedDD包括两个关键模块：丢弃率分配和上传参数选择，将优化模型参数。",
    "tldr": "本文提出了一种通过差分参数丢弃实现通信高效的联邦学习方案（FedDD）。这种方案避免了频繁交换模型参数的通信延迟问题，并通过模型参数丢弃而不是客户端选择来优化全局模型泛化能力。"
}