{
    "title": "Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])",
    "abstract": "How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our ne",
    "link": "http://arxiv.org/abs/2308.01746",
    "context": "Title: Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])\nAbstract: How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our ne",
    "path": "papers/23/08/2308.01746.json",
    "total_tokens": 961,
    "translated_title": "神经崩溃终止点：一种统一的解决方案，用于类别增量学习及其变体",
    "translated_abstract": "如何在保持对旧类别的能力的同时使新类别具有可学习性，一直是类别增量学习的一项重要挑战。除了常规情况之外，还提出了长尾类别增量学习和少样本类别增量学习，以考虑数据不平衡和数据稀缺性问题，这在实际应用中很常见，进一步加剧了灾难性遗忘问题。现有的方法专门针对这三个任务中的一个进行提出。本文提出了一种统一的解决方案来解决这三个任务中的问题。具体来说，我们提出了神经崩溃终止点，它是一个具有整个标签空间中最大等角互类间隔的固定结构。它在整个增量训练过程中充当一致的目标，以避免逐步划分特征空间。针对类别增量学习和长尾类别增量学习，我们还提出了一种原型演化方案，将主干特征引入我们的神经崩溃终止点。",
    "tldr": "本文提出了一种神经崩溃终止点的统一解决方案，用于解决类别增量学习及其变体中的问题，通过固定的结构和原型演化方案来保持对新类别的可学习性，同时避免灾难性遗忘问题。",
    "en_tdlr": "This paper proposes a unified solution called neural collapse terminus for addressing the challenges in class incremental learning and its variants. The solution involves a fixed structure with maximal inter-class separation and a prototype evolving scheme to maintain learnability for new classes while preventing catastrophic forgetting."
}