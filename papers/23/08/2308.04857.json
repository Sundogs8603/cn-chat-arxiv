{
    "title": "Emotion-Conditioned Text Generation through Automatic Prompt Optimization. (arXiv:2308.04857v1 [cs.CL])",
    "abstract": "Conditional natural language generation methods often require either expensive fine-tuning or training a large language model from scratch. Both are unlikely to lead to good results without a substantial amount of data and computational resources. Prompt learning without changing the parameters of a large language model presents a promising alternative. It is a cost-effective approach, while still achieving competitive results. While this procedure is now established for zero- and few-shot text classification and structured prediction, it has received limited attention in conditional text generation. We present the first automatic prompt optimization approach for emotion-conditioned text generation with instruction-fine-tuned models. Our method uses an iterative optimization procedure that changes the prompt by adding, removing, or replacing tokens. As objective function, we only require a text classifier that measures the realization of the conditional variable in the generated text. ",
    "link": "http://arxiv.org/abs/2308.04857",
    "context": "Title: Emotion-Conditioned Text Generation through Automatic Prompt Optimization. (arXiv:2308.04857v1 [cs.CL])\nAbstract: Conditional natural language generation methods often require either expensive fine-tuning or training a large language model from scratch. Both are unlikely to lead to good results without a substantial amount of data and computational resources. Prompt learning without changing the parameters of a large language model presents a promising alternative. It is a cost-effective approach, while still achieving competitive results. While this procedure is now established for zero- and few-shot text classification and structured prediction, it has received limited attention in conditional text generation. We present the first automatic prompt optimization approach for emotion-conditioned text generation with instruction-fine-tuned models. Our method uses an iterative optimization procedure that changes the prompt by adding, removing, or replacing tokens. As objective function, we only require a text classifier that measures the realization of the conditional variable in the generated text. ",
    "path": "papers/23/08/2308.04857.json",
    "total_tokens": 824,
    "translated_title": "通过自动提示优化进行情感条件文本生成",
    "translated_abstract": "条件自然语言生成方法通常需要耗费巨大的微调或从头开始训练一个大型语言模型。这两种方法都很难在没有大量数据和计算资源的情况下得到好的结果。不改变大型语言模型的参数而进行提示学习则是一个具有潜力的替代方法。这是一种经济高效的方法，同时又能够达到竞争力的结果。虽然这种方法在零样本和少样本文本分类和结构化预测中已经得到了广泛应用，但在条件文本生成中受到的关注却有限。我们提出了第一个用于情感条件文本生成的自动提示优化方法，采用了指令微调模型。我们的方法使用迭代优化过程，通过添加、删除或替换标记来改变提示。作为目标函数，我们只需要一个文本分类器来衡量生成文本中条件变量的实现情况。",
    "tldr": "本论文提出了一种通过自动提示优化的方法，用于情感条件文本生成。通过改变提示，我们的方法能够实现生成文本中条件变量的实现情况。这种方法经济高效且具有竞争力。",
    "en_tdlr": "This paper presents an automatic prompt optimization approach for emotion-conditioned text generation. By changing the prompt, our method achieves the realization of the conditional variable in the generated text. This method is cost-effective and competitive."
}