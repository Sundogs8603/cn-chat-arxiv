{
    "title": "Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])",
    "abstract": "Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at",
    "link": "http://arxiv.org/abs/2308.06095",
    "context": "Title: Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])\nAbstract: Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at",
    "path": "papers/23/08/2308.06095.json",
    "total_tokens": 953,
    "translated_title": "神经对话模型及其控制方法：失败和修复的综述",
    "translated_abstract": "最近，以强大语言模型为基础的条件语言模型能够以看似流利的方式延续任何类型的文本来源。这个事实促进了对基于强大语言模型的开放领域对话系统的研究，旨在通过生成适当的对话内容来模仿对话方的行为。然而，从语言学的角度来看，参与对话的复杂性很高。在这项综述中，我们从这一特定研究领域的角度解释了Grice的合作性对话最大规则，并将文献系统化地归纳为一个贡献何种内容是适当的方面：神经对话模型必须流畅、信息丰富、一致、连贯，并遵循社会准则。为了确保这些特性，最近的方法尝试在数据、训练制度或解码等各个干预点上控制底层语言模型。按照这些类别和干预点进行排序，我们讨论了一些有希望的方法。",
    "tldr": "这项综述研究了以强大语言模型为基础的开放领域对话系统，并探讨了如何通过干预底层语言模型的不同方面，如数据、训练制度或解码，来保证模型的流畅性、信息丰富性、一致性、连贯性以及遵循社会准则的特性。",
    "en_tdlr": "This survey examines open-domain conversational systems based on powerful language models and explores how to ensure the qualities of fluency, informativeness, consistency, coherence, and adherence to social norms by intervening in various aspects of the underlying language models, such as data, training regime, or decoding."
}