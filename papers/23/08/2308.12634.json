{
    "title": "Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])",
    "abstract": "The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.",
    "link": "http://arxiv.org/abs/2308.12634",
    "context": "Title: Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])\nAbstract: The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.",
    "path": "papers/23/08/2308.12634.json",
    "total_tokens": 960,
    "translated_title": "面向分层区域变压器的多实例学习",
    "translated_abstract": "在数字病理学和精确医学中，使用深度多实例学习模型对巨像素组织病理学图像进行分类已成为一项关键任务。本文提出了一种基于变压器的多实例学习方法，该方法用区域性的、受到视觉变压器启发的自注意力机制替代了传统的学习注意力机制。我们提出了一种融合区域补丁信息以得出滑片级别预测的方法，并展示了如何堆叠这种区域聚合以分层地处理不同距离水平上的特征。为了提高预测准确性，特别是对于具有小的局部形态特征的数据集，我们引入了一种方法，在推理期间将图像处理集中在高关注区域。我们的方法能够显著改善两个组织病理学数据集的性能，并指向进一步研究的有希望的方向。",
    "tldr": "本文提出了一种基于变压器的多实例学习方法，通过使用区域自注意力机制，融合区域补丁信息以得出滑片级别预测，并通过堆叠区域聚合来分层处理特征。此外，引入了一种方法来聚焦于高关注区域，从而提高预测准确性。这种方法在组织病理学图像分类任务上表现出了显著的性能改进，并为进一步研究提供了有希望的方向。",
    "en_tdlr": "This paper presents a Transformer-based multiple instance learning approach that utilizes regional self-attention mechanism to fuse regional patch information for slide-level predictions. The approach also introduces a method to focus on high attention regions during inference, leading to improved predictive accuracy. The method shows significant performance improvement on histopathology image classification tasks, indicating promising directions for further research."
}