{
    "title": "Max-affine regression via first-order methods. (arXiv:2308.08070v1 [stat.ML])",
    "abstract": "We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alt",
    "link": "http://arxiv.org/abs/2308.08070",
    "context": "Title: Max-affine regression via first-order methods. (arXiv:2308.08070v1 [stat.ML])\nAbstract: We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alt",
    "path": "papers/23/08/2308.08070.json",
    "total_tokens": 929,
    "translated_title": "最大仿射回归及其一阶方法",
    "translated_abstract": "本文考虑了最大仿射模型的回归问题，该模型通过使用最大化函数将仿射模型组合成分段线性模型。最大仿射模型广泛应用于信号处理和统计学中，包括多类别分类、拍卖问题和凸回归。它还推广了相位恢复和学习整流线性单元激活函数。我们对梯度下降（GD）和小批量随机梯度下降（SGD）方法在最大仿射回归中的非渐近收敛性进行了分析，假设模型以随机位置观测，遵循次高斯分布和具有加性次高斯噪声的反浓度。在这些假设下，适当初始化的GD和SGD能够线性收敛到由相应误差界限确定的目标区域附近。我们提供了与理论发现相一致的数值结果。值得注意的是，SGD不仅在运行时间上收敛更快，而且在观测次数较少时也能获得较好的效果。",
    "tldr": "本文研究了最大仿射回归问题，并提出了基于梯度下降和随机梯度下降的收敛分析方法。数值实验验证了理论结果的有效性。该方法在运行时间和观测次数较少时都能取得较好的效果。",
    "en_tdlr": "This paper studies the problem of max-affine regression and presents convergence analysis methods based on gradient descent and stochastic gradient descent. Numerical experiments confirm the effectiveness of the theoretical results. The proposed methods show good performance in terms of both runtime and the number of observations."
}