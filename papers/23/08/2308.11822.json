{
    "title": "PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (arXiv:2308.11822v1 [cs.LG])",
    "abstract": "Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate th",
    "link": "http://arxiv.org/abs/2308.11822",
    "context": "Title: PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (arXiv:2308.11822v1 [cs.LG])\nAbstract: Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate th",
    "path": "papers/23/08/2308.11822.json",
    "total_tokens": 821,
    "translated_title": "PatchBackdoor: 无需修改模型的深度神经网络后门攻击",
    "translated_abstract": "后门攻击是对安全关键场景下深度学习系统的主要威胁，旨在在受攻击者控制的条件下触发神经网络模型的错误行为。然而，大多数后门攻击需要通过训练带有有毒数据和/或直接修改模型来修改神经网络模型，这导致了一种普遍但错误的信念，即通过正确保护模型可以轻松避免后门攻击。在本文中，我们展示了无需任何模型修改即可实现后门攻击。我们提出将一个精心设计的补丁（称为后门补丁）放置在摄像头前面，与输入图像一起输入模型。该补丁可以在大多数时候表现正常，但当输入图像包含受攻击者控制的触发物体时会产生错误预测。我们的主要技术包括一种有效的训练方法来生成补丁。",
    "tldr": "本论文提出了一种无需修改模型的深度神经网络后门攻击方法，通过在摄像头前放置一个精心设计的补丁，即可触发模型的错误行为，而无需修改模型。"
}