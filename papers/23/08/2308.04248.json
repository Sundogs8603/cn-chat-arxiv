{
    "title": "Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])",
    "abstract": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective",
    "link": "http://arxiv.org/abs/2308.04248",
    "context": "Title: Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])\nAbstract: Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective",
    "path": "papers/23/08/2308.04248.json",
    "total_tokens": 837,
    "translated_title": "使用词嵌入进行词汇对齐",
    "translated_abstract": "捕捉和注释手语数据集是一项耗时且昂贵的过程。目前的数据集规模远远不足以成功训练无约束的手语翻译模型。因此，研究已转向电视广播内容作为大规模训练数据的来源，包括手语翻译者和相关音频字幕。然而，缺乏手语注释限制了这些数据的可用性，并导致了自动注释技术（如手语定位）的发展。这些定位与视频对齐，而不是与字幕对齐，这往往导致字幕和定位的手势不匹配。在本文中，我们提出了一种使用大型口语语言模型将定位与其对应字幕对齐的方法。使用单一模态意味着我们的方法在计算上开销小，并且可以与现有的定位技术结合使用。我们定量地证明了该方法的有效性。",
    "tldr": "这篇论文提出了一种使用大型口语语言模型对定位结果与字幕进行对齐的方法，解决了手语翻译数据中字幕和手势不匹配的问题。",
    "en_tdlr": "This paper proposes a method for aligning sign spottings with their corresponding subtitles using a large spoken language model, addressing the issue of mismatch between subtitles and gestures in sign language translation data."
}