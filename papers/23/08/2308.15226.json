{
    "title": "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation. (arXiv:2308.15226v1 [cs.CV])",
    "abstract": "There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-",
    "link": "http://arxiv.org/abs/2308.15226",
    "context": "Title: CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation. (arXiv:2308.15226v1 [cs.CV])\nAbstract: There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-",
    "path": "papers/23/08/2308.15226.json",
    "total_tokens": 903,
    "translated_title": "CLIPTrans：使用预训练模型转移视觉知识进行多模态机器翻译",
    "translated_abstract": "近年来，开发增强神经机器翻译（NMT）的多模态机器翻译（MMT）系统以提高翻译质量的兴趣逐渐增长。这一问题设置涉及在训练过程中使用图像作为辅助信息，并且最近在推理过程中消除它们的使用。然而，之前的工作在从头开始训练强大的MMT模型时面临了一个挑战，因为多语言视觉语言数据的标注稀缺，尤其是对于低资源语言。与此同时，针对NMT的多语言预训练模型和针对视觉语言任务的多模态预训练模型大量涌现，主要针对英文，它们展现了出色的泛化能力。然而，这些模型对于MMT并不直接适用，因为它们没有为生成任务提供对齐的多模态多语言特征。为了解决这个问题，我们提出了CLIPTrans，它不像设计复杂的MMT模块，而是简单地适应独立预训练模型，",
    "tldr": "本研究提出了CLIPTrans，它通过简单地适应独立预训练模型，实现了多模态机器翻译中视觉知识的转移。"
}