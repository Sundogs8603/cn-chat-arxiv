{
    "title": "Comparing the quality of neural network uncertainty estimates for classification problems. (arXiv:2308.05903v1 [cs.LG])",
    "abstract": "Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' resu",
    "link": "http://arxiv.org/abs/2308.05903",
    "context": "Title: Comparing the quality of neural network uncertainty estimates for classification problems. (arXiv:2308.05903v1 [cs.LG])\nAbstract: Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' resu",
    "path": "papers/23/08/2308.05903.json",
    "total_tokens": 873,
    "translated_title": "比较神经网络分类问题的不确定性估计质量",
    "translated_abstract": "传统的深度学习模型是强大的分类器，但许多方法没有提供对其估计结果的不确定性。不确定性量化（UQ）方法对于深度学习模型在决策中的有用性引起了文献中的关注，尤其是对于高风险决策。然而，目前对这些方法的质量评估研究很少。我们使用经验主义置信区间覆盖率和区间宽度的统计方法来评估置信区间的质量，并使用期望校准误差评估分类预测的置信度。我们将这些不同的UQ方法应用于使用马尔科夫链蒙特卡洛（MCMC）和变分推断（VI）拟合的贝叶斯神经网络（BNN），自助式神经网络（NN），深度集成（DE）和蒙特卡洛（MC）dropout的高光谱图像目标检测问题，并展示了不同方法结果的一致性问题。",
    "tldr": "本研究比较了用于分类问题的神经网络的不确定性估计质量，并通过统计方法和指标对不同方法进行了评估。研究展示了这些估计方法的一致性问题。",
    "en_tdlr": "This study compares the quality of uncertainty estimates for neural networks in classification problems and evaluates different methods using statistical metrics. The study demonstrates the inconsistency of these estimation methods."
}