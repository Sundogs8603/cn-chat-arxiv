{
    "title": "CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])",
    "abstract": "The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to \"see\" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the sema",
    "link": "http://arxiv.org/abs/2308.12045",
    "context": "Title: CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])\nAbstract: The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to \"see\" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the sema",
    "path": "papers/23/08/2308.12045.json",
    "total_tokens": 852,
    "translated_title": "CgT-GAN: CLIP引导的文本生成对抗网络用于图像描述",
    "translated_abstract": "大规模的视觉语言预训练模型CLIP在没有人工注释的图像描述场景中显著提升了图像描述的效果。最近的CLIP-based图像描述方法采用了纯文本训练范式，即从共享的嵌入空间中重建文本。然而，这些方法在训练和推理差距或者文本嵌入的存储需求上存在一定限制。考虑到在现实世界中获取图像是微不足道的，我们提出了CLIP引导的文本生成对抗网络（CgT-GAN），将图像加入到训练过程中使得模型能够“看到”真实的视觉模态。特别地，我们采用对抗训练使得CgT-GAN能够模仿外部文本语料库的短语，并使用基于CLIP的奖励提供语义引导。生成的图像描述器通过GAN的鉴别器计算的自然度和基于人类语言的语义从而获得联合奖励。",
    "tldr": "本文提出了CgT-GAN，通过引入图像为模型提供视觉信息，结合对抗训练和基于CLIP的奖励，实现了在没有人工注释的情况下进行图像描述。"
}