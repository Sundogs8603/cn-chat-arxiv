{
    "title": "Understanding Hessian Alignment for Domain Generalization. (arXiv:2308.11778v1 [cs.LG])",
    "abstract": "Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis expl",
    "link": "http://arxiv.org/abs/2308.11778",
    "context": "Title: Understanding Hessian Alignment for Domain Generalization. (arXiv:2308.11778v1 [cs.LG])\nAbstract: Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis expl",
    "path": "papers/23/08/2308.11778.json",
    "total_tokens": 931,
    "translated_title": "理解Hessian对领域泛化的对齐",
    "translated_abstract": "在许多实际场景中，包括医疗保健和自动驾驶，超出分布（OOD）泛化是深度学习模型的关键能力。最近，已经提出了不同的技术来改进OOD泛化。在这些方法中，基于梯度的正则化器与其他竞争对手相比显示出有希望的性能。尽管取得了这样的成功，我们对Hessian和梯度对领域泛化的作用的认识仍然有限。为了解决这个缺点，我们使用最近的OOD转移性理论分析了分类器头部Hessian矩阵和梯度在领域泛化中的作用。从理论上讲，我们表明，跨领域的分类器头部Hessian矩阵之间的谱范数是传输度量的上界，传输度量是目标领域和源领域之间的距离的概念。此外，我们分析了在鼓励Hessian和梯度之间的相似性时所有的对齐属性。",
    "tldr": "本论文通过分析分类器的Hessian矩阵和梯度在领域泛化中的作用，发现了跨领域的Hessian矩阵之间的谱范数是传输度量的上界，并分析了在鼓励Hessian和梯度之间的相似性时的所有对齐属性。",
    "en_tdlr": "This paper analyzes the role of the classifier's Hessian matrix and gradient in domain generalization, finding that the spectral norm between the Hessian matrices across domains is an upper bound of the transfer measure, and identifies all the alignment attributes when encouraging similarity between Hessians and gradients."
}