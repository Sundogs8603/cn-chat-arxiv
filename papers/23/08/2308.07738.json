{
    "title": "Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods. (arXiv:2308.07738v1 [cs.AI])",
    "abstract": "We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments --",
    "link": "http://arxiv.org/abs/2308.07738",
    "context": "Title: Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods. (arXiv:2308.07738v1 [cs.AI])\nAbstract: We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments --",
    "path": "papers/23/08/2308.07738.json",
    "total_tokens": 992,
    "translated_title": "使用形式方法的DAgger算法实现低延迟的蒙特卡罗树搜索",
    "translated_abstract": "本文研究如何高效地结合形式方法、蒙特卡罗树搜索（MCTS）和深度学习，以在大规模马尔可夫决策过程（MDP）中生成高质量的突前视界策略。我们使用模型检测技术指导MCTS算法，在MDP的一组代表性状态上生成高质量决策的离线样本。这些样本可以用来训练一个神经网络，该网络模仿用于生成它们的策略。这个神经网络可以作为低延迟的MCTS在线搜索的指导，也可以在需要最小延迟时作为一个完整的策略使用。我们使用统计模型检测来检测需要额外样本的情况，并将这些额外样本集中在学到的神经网络策略与（计算昂贵的）离线策略不一致的配置上。我们在模拟冰湖和吃豆人环境的MDP上展示了我们方法的应用。",
    "tldr": "本研究提出了一种使用形式方法指导蒙特卡罗树搜索以生成高质量决策的算法。通过训练神经网络来模仿生成的策略，并在低延迟搜索或最小延迟情况下作为完整策略使用。模拟实验结果表明该方法的有效性和性能提升。",
    "en_tdlr": "This paper presents an algorithm that combines formal methods and Monte Carlo Tree Search (MCTS) to generate high-quality decisions. A neural network is trained to imitate the policy used by MCTS, and can be used as a guide in lower-latency MCTS search or as a full-fledged policy for minimal latency. Experimental results on simulated environments show the effectiveness and performance improvement of this method."
}