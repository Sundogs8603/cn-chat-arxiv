{
    "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])",
    "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning fra",
    "link": "http://arxiv.org/abs/2308.12305",
    "context": "Title: FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])\nAbstract: Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning fra",
    "path": "papers/23/08/2308.12305.json",
    "total_tokens": 914,
    "translated_title": "FedDAT: 一种多模态异构联邦学习中基础模型微调的方法",
    "translated_abstract": "近年来，在多模态学习中，基础模型取得了显著的进展。这些模型通常配备了数百万（或数十亿）个参数，需要大量数据进行微调。然而，由于不同的隐私法规，从不同领域收集和集中训练数据变得具有挑战性。联邦学习（FL）作为一种有前途的解决方案出现，可以让多个客户端在不集中其本地数据的情况下共同训练神经网络。为了减轻客户端的计算负担和通信开销，之前的工作已经采用了参数高效微调（PEFT）方法用于FL。在此过程中，只有少量模型参数在联邦通信期间进行优化和传输。然而，大多数之前的工作都专注于单一形态，并忽略了一种常见现象，即客户端之间存在数据异构性。因此，在本文中，我们提出了一个微调框架，用于解决多模态异构联邦学习中的问题。",
    "tldr": "FedDAT是一种在多模态异构联邦学习中进行基础模型微调的方法，通过采用参数高效微调（PEFT）方法来减轻客户端计算负担和通信开销，并解决了数据异构性的问题。"
}