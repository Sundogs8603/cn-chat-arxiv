{
    "title": "Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that languag",
    "link": "http://arxiv.org/abs/2308.15727",
    "context": "Title: Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])\nAbstract: Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that languag",
    "path": "papers/23/08/2308.15727.json",
    "total_tokens": 868,
    "translated_title": "在大型语言模型中量化和分析实体级记忆",
    "translated_abstract": "大型语言模型（LLMs）被证明能够记忆其训练数据，这可以通过特定设计的提示提取出来。随着数据集规模的不断增长，由记忆引起的隐私风险引起了越来越多的关注。量化语言模型的记忆能力有助于评估潜在的隐私风险。然而，以往关于量化记忆的研究需要访问精确的原始数据或产生相当大的计算开销，这对于实际应用中的语言模型来说很困难。因此，我们提出了一种细粒度的、实体级的定义，用于以更接近实际场景的条件和度量方式来量化记忆。此外，我们还提出了一种从自回归语言模型中高效提取敏感实体的方法。我们基于提出的方法进行了大量的实验证明了语言模型在不同设置下的重构敏感实体的能力。",
    "tldr": "本研究提出了一种细粒度的、实体级的定义来量化大型语言模型中的记忆能力，并提出了一种高效提取敏感实体的方法。实验证明了语言模型在不同设置下重构敏感实体的能力。"
}