{
    "title": "Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])",
    "abstract": "Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.",
    "link": "http://arxiv.org/abs/2308.16259",
    "context": "Title: Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])\nAbstract: Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.",
    "path": "papers/23/08/2308.16259.json",
    "total_tokens": 864,
    "translated_title": "材料信息学变压器：一种用于可解释材料性能预测的语言模型",
    "translated_abstract": "最近，大型语言模型(LLMs)在自然语言处理、计算机视觉和分子建模等多个研究领域展示了显著的能力。我们通过引入我们的模型材料信息学变压器(MatInFormer)，将LLMs的这种范式扩展到材料性能预测领域。具体而言，我们引入了一种新颖的方法，通过对相关的空间群信息进行标记化，学习了晶体学的语法。我们进一步展示了MatInFormer的适应性，通过 incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs)。通过注意力可视化，我们揭示了模型在属性预测过程中优先考虑的关键特征。我们的提议模型在14个不同的数据集中经过实证验证，从而强调了其在通过准确的材料属性预测进行高通量筛选方面的潜力。",
    "tldr": "本研究提出了一种名为材料信息学变压器（MatInFormer）的语言模型，通过学习晶体学语法和引入MOFs数据，实现了对材料性能的准确预测，并通过注意力可视化揭示了模型的关键特征。",
    "en_tdlr": "This study proposes a language model called Materials Informatics Transformer (MatInFormer), which learns crystallography grammar and incorporates MOFs data to accurately predict material properties. Attention visualization is used to reveal key features of the model during property prediction."
}