{
    "title": "Evaluating the Robustness of Test Selection Methods for Deep Neural Networks. (arXiv:2308.01314v1 [cs.LG])",
    "abstract": "Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suf",
    "link": "http://arxiv.org/abs/2308.01314",
    "context": "Title: Evaluating the Robustness of Test Selection Methods for Deep Neural Networks. (arXiv:2308.01314v1 [cs.LG])\nAbstract: Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suf",
    "path": "papers/23/08/2308.01314.json",
    "total_tokens": 876,
    "translated_title": "评估深度神经网络测试选择方法的稳定性",
    "translated_abstract": "由于对收集的原始数据进行标记所需的时间和劳动力，测试基于深度学习的系统是至关重要的，但也具有挑战性。为了减轻标记工作量，已经提出了多种测试选择方法，只需对测试数据的子集进行标记即可满足测试要求。然而，我们观察到，这些报道有希望的结果的方法只在简单情景下进行评估，例如，在原始测试数据上进行测试。这让我们产生了一个问题：它们总是可靠的吗？本文探讨了测试选择方法在测试中失败的时间和程度。具体而言，首先，我们基于其构建方法，确定了来自顶级会议的11种选择方法的潜在陷阱。其次，我们对五个数据集进行了研究，每个数据集有两个模型架构，以从经验上确认这些陷阱的存在。此外，我们还演示了陷阱如何破坏这些方法的可靠性。具体来说，故障检测方法的缺陷。",
    "tldr": "本文评估了测试选择方法在深度神经网络测试中的稳定性，通过探讨这些方法的潜在陷阱并进行实证研究，揭示了它们的可靠性问题。",
    "en_tdlr": "This paper evaluates the robustness of test selection methods for deep neural network testing. Through exploring potential pitfalls and conducting empirical studies, it reveals the reliability issues of these methods."
}