{
    "title": "Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models. (arXiv:2308.13812v1 [cs.AI])",
    "abstract": "Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Takin",
    "link": "http://arxiv.org/abs/2308.13812",
    "context": "Title: Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models. (arXiv:2308.13812v1 [cs.AI])\nAbstract: Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Takin",
    "path": "papers/23/08/2308.13812.json",
    "total_tokens": 908,
    "translated_title": "通过大型语言模型增强面向动态感知的文本到视频扩散",
    "translated_abstract": "文本到视频（T2V）合成在社区中引起了越来越多的关注，其中最近出现的扩散模型（DMs）比过去的方法表现更强大。尽管现有的最先进的DMs能够实现高分辨率的视频生成，但它们在复杂的时间动态建模方面存在一些主要限制（例如，动作出现障碍，粗糙的视频运动）。在这项工作中，我们研究了如何加强DMs对视频动态的感知，以实现高质量的T2V生成。受到人类直觉的启发，我们设计了一种创新的动态场景管理器（称为Dysen）模块，其中包括（步骤1）从输入文本中提取具有适当时间顺序安排的关键动作，（步骤2）将动作时间表转化为动态场景图（DSG）表示，以及（步骤3）丰富DSG中的场景并提供充分和合理的细节。",
    "tldr": "本文提出了一种新的方法（称为Dysen）来增强面向动态感知的文本到视频扩散，通过提取关键动作、建立动态场景图和丰富细节，以实现高质量的T2V生成。",
    "en_tdlr": "This paper proposes a novel approach (called Dysen) to enhance dynamics-aware text-to-video diffusion by extracting key actions, constructing dynamic scene graphs, and enriching details, enabling high-quality T2V generation."
}