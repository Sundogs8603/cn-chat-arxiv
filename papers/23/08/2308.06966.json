{
    "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce. (arXiv:2308.06966v2 [cs.CL] UPDATED)",
    "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the ",
    "link": "http://arxiv.org/abs/2308.06966",
    "context": "Title: EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce. (arXiv:2308.06966v2 [cs.CL] UPDATED)\nAbstract: Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the ",
    "path": "papers/23/08/2308.06966.json",
    "total_tokens": 832,
    "translated_title": "EcomGPT: 使用任务链调节大型语言模型以适应电子商务领域",
    "translated_abstract": "最近，指令跟随的大型语言模型（LLM），例如ChatGPT，在通用自然语言处理（NLP）任务中展现了出色的性能。然而，电子商务数据的独特特点对于通用LLM来说带来了重大挑战。为了解决这个问题，我们提出了第一个针对电子商务场景的LLM，名为EcomGPT。通过构建以电子商务基本数据类型（例如产品信息、用户评价）为基础的原子任务，EcomInstruct数据集扩大了数据规模和任务多样性。原子任务是隐含在解决最终任务中的中间任务，我们也称之为任务链任务。通过使用EcomInstruct训练骨干模型BLOOMZ，我们开发了具有不同参数规模的EcomGPT。",
    "tldr": "EcomGPT是针对电子商务领域的任务链调节语言模型，通过构建原子任务链并使用EcomInstruct数据集进行训练，具有较强的通用化能力。"
}