{
    "title": "An Examination of the Compositionality of Large Generative Vision-Language Models",
    "abstract": "arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D",
    "link": "https://arxiv.org/abs/2308.10509",
    "context": "Title: An Examination of the Compositionality of Large Generative Vision-Language Models\nAbstract: arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D",
    "path": "papers/23/08/2308.10509.json",
    "total_tokens": 866,
    "translated_title": "对大规模生成视觉-语言模型的组合性的研究",
    "translated_abstract": "随着大型语言模型（LLMs）的成功，许多生成视觉-语言模型（GVLMs）通过多模态指导调整得以构建。然而，GVLMs在多模态组合推理中的性能还未得到充分探索。本文旨在检验评估GVLMs组合性的评估指标（VisualGPTScore等）和当前基准。我们发现当前基准中的句法偏见，被GVLMs的语言能力所利用。这种偏见使得VisualGPTScore成为评估GVLMs的不足指标。为了解决这一问题，我们首先引入了一个SyntaxBias Score，利用LLMs量化此类偏见以进行缓解。随后添加了一个具有挑战性的新任务，以评估GVLMs对固有倾向于句法正确性的健壮性。利用缓解偏见的数据集和新任务，我们提出了一个新的基准，即SyntActically D",
    "tldr": "本文检验了生成视觉-语言模型的组合性，发现当前基准中存在句法偏见，提出了新的评估指标SyntaxBias Score和新的基准SyntActically D。",
    "en_tdlr": "This paper examines the compositionality of generative vision-language models, identifies a syntactical bias in current benchmarks exploited by the models, proposes new evaluation metrics SyntaxBias Score and a new benchmark named SyntActically D."
}