{
    "title": "Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])",
    "abstract": "Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .",
    "link": "http://arxiv.org/abs/2308.12510",
    "context": "Title: Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])\nAbstract: Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .",
    "path": "papers/23/08/2308.12510.json",
    "total_tokens": 925,
    "translated_title": "Masked Autoencoders是高效的分类增量学习器",
    "translated_abstract": "分类增量学习(CIL)旨在在学习新类别的同时避免对之前知识的灾难性遗忘。我们提出使用掩蔽自编码器(MAEs)作为CIL的高效学习器。MAEs最初是为了通过重建无监督学习来学习有用的表示，并且它们可以轻松地与监督损失结合以用于分类。此外，MAEs可以可靠地从随机选择的图像补丁中重建原始输入图像，我们使用这种方法更高效地存储过去任务的范例用于CIL。我们还提出了双边MAE框架来学习图像级和嵌入级融合，它可以产生更高质量的重建图像和更稳定的表示。我们的实验证实，我们的方法在CIFAR-100，ImageNet-Subset和ImageNet-Full上优于现有最先进的方法。代码可在https://github.com/scok30/MAE-CIL上获取。",
    "tldr": "本论文提出了使用掩蔽自编码器(MAEs)作为高效的分类增量学习器，通过重建原始输入图像和学习图像级和嵌入级融合来存储和学习过去任务的表示。实验证实，在CIFAR-100，ImageNet-Subset和ImageNet-Full上，该方法优于现有最先进的方法。"
}