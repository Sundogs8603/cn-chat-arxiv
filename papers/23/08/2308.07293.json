{
    "title": "DiffSED: Sound Event Detection with Denoising Diffusion. (arXiv:2308.07293v2 [cs.SD] UPDATED)",
    "abstract": "Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outpe",
    "link": "http://arxiv.org/abs/2308.07293",
    "context": "Title: DiffSED: Sound Event Detection with Denoising Diffusion. (arXiv:2308.07293v2 [cs.SD] UPDATED)\nAbstract: Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outpe",
    "path": "papers/23/08/2308.07293.json",
    "total_tokens": 869,
    "translated_title": "使用去噪扩散的声音事件检测（DiffSED）",
    "translated_abstract": "声音事件检测（SED）旨在在给定无约束音频样本的情况下，预测感兴趣事件的时间边界和类别标签。现有的所有方法都是从鉴别学习的角度考虑SED问题，采用分割和分类（即帧级）策略或更有原则的事件级建模方法。在这项工作中，我们通过采取生成学习的角度来重新定义SED问题。具体而言，我们的目标是在密集扩散过程中，基于目标音频样本，从噪声提议中生成准确的声音时间边界。在训练过程中，我们的模型通过在优雅的Transformer解码器框架中将噪声潜在查询转换为地面真实版本，从而学习逆转噪声过程。通过这样做，即使在推断过程中使用噪声查询，模型也能生成准确的事件边界。在Urban-SED和EPIC-Sounds数据集上进行了大量实验证明，我们的模型明显优于现有方法。",
    "tldr": "本论文提出了一种使用去噪扩散的声音事件检测方法，通过从噪声查询中生成准确的事件边界，实验表明该方法明显优于现有方法。",
    "en_tdlr": "This paper proposes a sound event detection method using denoising diffusion, which generates accurate event boundaries from noisy queries. Experimental results show that this method significantly outperforms existing methods."
}