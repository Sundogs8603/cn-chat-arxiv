{
    "title": "Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])",
    "abstract": "The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces \"diffmotion-v2,\" a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou",
    "link": "http://arxiv.org/abs/2308.05995",
    "context": "Title: Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])\nAbstract: The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces \"diffmotion-v2,\" a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou",
    "path": "papers/23/08/2308.05995.json",
    "total_tokens": 875,
    "translated_title": "一体化音频：利用WavLM预训练模型进行基于语音驱动的手势合成",
    "translated_abstract": "在虚拟人创作领域，生成与语音配套的手势是一个正在兴起的研究方向。先前的研究通过以声学和语义信息作为输入，采用分类方法来识别人物的ID和情感，以驱动与语音配套的手势生成。然而，这项工作仍然面临着重大挑战。这些挑战不仅涉及手势、语音声学和语义之间错综复杂的相互作用，还包括与个性、情感和其他不明确但重要的因素相关的复杂性。本文介绍了“diffmotion-v2”，这是一种基于语音条件扩散和基于非自回归Transformer的生成模型，采用WavLM预训练模型。它可以仅使用原始语音音频生成个性化的全身手势，消除了复杂的多模态处理和手动注释的需求。",
    "tldr": "本文介绍了一种利用WavLM预训练模型的基于语音驱动的手势合成方法，实现只使用原始语音音频生成个性化全身手势，消除了复杂的多模态处理和手动注释的需求。"
}