{
    "title": "Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])",
    "abstract": "The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro",
    "link": "http://arxiv.org/abs/2308.12919",
    "context": "Title: Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])\nAbstract: The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro",
    "path": "papers/23/08/2308.12919.json",
    "total_tokens": 866,
    "translated_title": "用CLIP实现真实的无监督微调",
    "translated_abstract": "视觉-语言模型（VLM）如CLIP的出现推动了人们在下游监督学习任务中的应用研究。尽管一些之前的研究探索了CLIP的无监督微调，但它们常常依赖于与真实标签相关的类名等先验知识。本文中，我们探讨了一种真实的无监督微调情景，假设未标记的数据可能包含来自未知类别的超出分布范围的样本。此外，我们强调了在预定义类标签的识别之外，同时提高对超出分布检测能力的重要性。为了解决这个问题，我们提出了一种简单、高效、有效的微调方法，称为Universal Entropy Optimization (UEO)。UEO利用样本级置信度，以近似方式最小化置信实例的条件熵并最大化边缘熵。",
    "tldr": "本论文针对无监督微调中可能出现的未知类别和超出分布范围的问题，提出了一种称为UEO的简单、高效、有效的微调方法，该方法能够同时提高对超出分布样本的检测能力和预定义类别实例的识别能力。"
}