{
    "title": "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval. (arXiv:2308.08285v1 [cs.IR])",
    "abstract": "In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.",
    "link": "http://arxiv.org/abs/2308.08285",
    "context": "Title: Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval. (arXiv:2308.08285v1 [cs.IR])\nAbstract: In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.",
    "path": "papers/23/08/2308.08285.json",
    "total_tokens": 842,
    "translated_title": "基于大型语言模型的文档扩展预训练用于稠密通道检索",
    "translated_abstract": "本文系统地研究了基于大型语言模型（LLM）的文档扩展预训练在稠密通道检索中的潜力。具体来说，我们利用LLMs的能力进行文档扩展，即查询生成，并通过针对通道检索的预训练策略有效地将扩展的知识传递给检索器。这些策略包括对比学习和瓶颈查询生成。此外，我们还采用了课程学习策略来减少对LLM推理的依赖。实验结果表明，基于LLM的文档扩展预训练显著提高了大规模网络搜索任务的检索性能。我们的工作展示了强大的零-shot和跨领域检索能力，在没有人工标注数据的情况下更具广泛的应用性。",
    "tldr": "本文研究了基于大型语言模型的文档扩展预训练对稠密通道检索的潜力，通过利用该方法进行查询生成并传递扩展的知识给检索器，实验证明这种方法显著提高了大规模网络搜索任务的检索性能。",
    "en_tdlr": "This paper investigates the potential of pre-training with Large Language Model-based document expansion for dense passage retrieval. Experimental results show that this method significantly improves retrieval performance on large-scale web-search tasks by utilizing query generation and transferring expanded knowledge to retrievers."
}