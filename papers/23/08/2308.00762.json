{
    "title": "Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])",
    "abstract": "As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrasti",
    "link": "http://arxiv.org/abs/2308.00762",
    "context": "Title: Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])\nAbstract: As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrasti",
    "path": "papers/23/08/2308.00762.json",
    "total_tokens": 858,
    "translated_title": "自监督对比BERT微调用于基于融合的评论项检索",
    "translated_abstract": "随着自然语言界面使用户能够表达越来越复杂的自然语言查询，用户评论内容也呈爆炸式增长，这可以使用户更好地找到与这些表达性查询匹配的餐厅、书籍或电影等物品。虽然神经信息检索(IR)方法为查询与文档之间的匹配提供了最先进的结果，但它们尚未扩展到评估项检索(RIR)任务，其中查询-评论得分必须聚合(或融合)成物品级得分进行排名。在没有标记的RIR数据集的情况下，我们通过利用自监督方法对BERT嵌入进行对比学习来将神经IR方法扩展到RIR。具体而言，对比学习需要选择正样本和负样本，而我们的项-评论数据的独特二级结构结合元数据为我们提供了丰富的样本选择结构。",
    "tldr": "本文介绍了一种扩展神经信息检索方法到评论项检索任务的方法，通过利用自监督对比学习来学习BERT嵌入，以融合查询和评论得分并进行排名。",
    "en_tdlr": "This paper presents a method to extend neural information retrieval methods to the task of reviewed-item retrieval by leveraging self-supervised contrastive learning of BERT embeddings, allowing for the fusion of query-review scores for ranking."
}