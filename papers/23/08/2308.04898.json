{
    "title": "An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures. (arXiv:2308.04898v1 [cs.CR])",
    "abstract": "As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts",
    "link": "http://arxiv.org/abs/2308.04898",
    "context": "Title: An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures. (arXiv:2308.04898v1 [cs.CR])\nAbstract: As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts",
    "path": "papers/23/08/2308.04898.json",
    "total_tokens": 868,
    "translated_title": "使用大语言模型分析软件供应链安全失败的实证研究",
    "translated_abstract": "随着我们越来越依赖软件系统，软件供应链被攻破的后果变得更加严重。像SolarWinds和ShadowHammer这样的高调网络攻击导致了重大的财务和数据损失，凸显了加强网络安全的需求。防止未来的破坏的一种方法是研究过去的失败案例。然而，传统的分析方法需要手动阅读和总结报告。自动化的支持可以降低成本并允许分析更多的失败案例。自然语言处理（NLP）技术如大语言模型（LLM）可以用来辅助分析失败。在这项研究中，我们评估了大语言模型（LLM）分析历史软件供应链违规的能力。我们使用LLM复制了Cloud Native Computing Foundation（CNCF）成员对69个软件供应链安全失败的手动分析。",
    "tldr": "本研究通过使用大语言模型（LLMs）对历史软件供应链安全失败进行分析，评估了其能力。通过自动化分析，可以降低成本并实现对更多失败案例的研究。",
    "en_tdlr": "This study empirically examines the use of large language models (LLMs) to analyze historical software supply chain security failures. The findings demonstrate the potential of automated analysis using LLMs to reduce costs and enable the study of a larger number of failure cases."
}