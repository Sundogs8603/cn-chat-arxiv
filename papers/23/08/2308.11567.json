{
    "title": "Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])",
    "abstract": "Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie",
    "link": "http://arxiv.org/abs/2308.11567",
    "context": "Title: Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])\nAbstract: Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie",
    "path": "papers/23/08/2308.11567.json",
    "total_tokens": 974,
    "translated_title": "神经动力学的低阶张量秩学习",
    "translated_abstract": "学习依赖于神经元群体中的协调突触变化。因此，了解学习过程中突触连接的集体演化是神经科学和机器学习中的一个关键挑战。近期的研究表明，经过训练的递归神经网络（RNN）的权重矩阵通常是低秩的，但是这种低秩结构如何在学习过程中展开还不清楚。为了解决这个问题，我们研究了整个学习过程中由权重矩阵形成的3阶张量的秩。通过用不同秩的RNN拟合大规模神经记录的运动学习任务，我们发现推断的权重是低阶张量秩的，因此在整个学习过程中在一个固定的低维子空间中演化。接下来，我们通过在真实权重上直接进行低阶张量秩分解，并展示我们所使用的方法，验证了低阶张量秩学习的观察结论。",
    "tldr": "研究发现通过学习过程中的张量秩演化来理解神经元连接在学习中的协调变化。研究表明训练过的递归神经网络的权重矩阵通常具有低秩结构，而这种结构在整个学习过程中保持在一个固定的低维子空间中。对真实权重进行低秩分解验证了这一观察结果。",
    "en_tdlr": "Understanding the coordinated synaptic changes in learning through the evolution of tensor rank. Low rank structure in weight matrices of trained RNNs is maintained in a fixed low-dimensional subspace throughout the entire learning process. Verification of this observation is done through low rank decomposition of ground truth weights."
}