{
    "title": "ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])",
    "abstract": "Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses b",
    "link": "http://arxiv.org/abs/2308.00400",
    "context": "Title: ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])\nAbstract: Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses b",
    "path": "papers/23/08/2308.00400.json",
    "total_tokens": 902,
    "translated_title": "ZRIGF：一种用于无资源图像驱动对话生成的创新多模态框架",
    "translated_abstract": "图像驱动的对话系统通过整合视觉信息，在生成高质量的回应方面具有很大优势。然而，当前的模型在无资源情境中难以有效利用这些信息，主要原因是图像和文本模态之间的差异。为了克服这一挑战，我们提出了一种创新的多模态框架，称为ZRIGF，它在无资源情境中融合了图像驱动信息来生成对话。ZRIGF采用两阶段学习策略，包括对比预训练和生成预训练。对比预训练包括一个文本-图像匹配模块，将图像和文本映射到统一的编码向量空间中，以及一个文本辅助的遮蔽图像建模模块，用于保存预训练的视觉特征并促进进一步的多模态特征对齐。生成预训练使用多模态融合模块和信息传递模块来生成有洞察力的回应。",
    "tldr": "ZRIGF是一种创新的多模态框架，用于无资源情境下的图像驱动对话生成。它通过对比预训练和生成预训练实现了视觉特征的对齐，生成有洞察力的回应。"
}