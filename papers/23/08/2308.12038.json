{
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])",
    "abstract": "Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP",
    "link": "http://arxiv.org/abs/2308.12038",
    "context": "Title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])\nAbstract: Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP",
    "path": "papers/23/08/2308.12038.json",
    "total_tokens": 930,
    "translated_title": "大型多语言模型在跨语种零样本多模式学习中的作用。",
    "translated_abstract": "最近，在图像到文本和文本到图像的生成方面，多模式学习出现了显著增长。然而，成功通常仅限于英语，其他语言则相对落后。在其他语言中构建具有竞争力的对应物是非常具有挑战性的，因为非英语多模式数据具有低资源特性（即缺乏大规模、高质量的图像-文本数据）。在这项工作中，我们提出了MPM，一种在低资源语言中训练大型多模式模型的有效训练范例。MPM表明，多语言模型可以在跨语种零样本多模式学习中起到关键作用。具体而言，基于强大的多语言大语言模型，仅在英语图像-文本数据上预训练的多模式模型可以以零样本的方式很好地泛化到其他语言，用于图像到文本和文本到图像的生成，甚至超过在本地语言的图像-文本数据上训练的模型。以中文作为MPM实践的一个练习。",
    "tldr": "本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。"
}