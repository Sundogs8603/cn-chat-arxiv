{
    "title": "Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval. (arXiv:2308.11474v1 [cs.IR])",
    "abstract": "Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text. In contrast, there has been little research on retrieving data with multiple aspects using dense models. In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects. However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently. To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs. To facilitate effective retrieval with the aspect strings, we p",
    "link": "http://arxiv.org/abs/2308.11474",
    "context": "Title: Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval. (arXiv:2308.11474v1 [cs.IR])\nAbstract: Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text. In contrast, there has been little research on retrieving data with multiple aspects using dense models. In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects. However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently. To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs. To facilitate effective retrieval with the aspect strings, we p",
    "path": "papers/23/08/2308.11474.json",
    "total_tokens": 877,
    "translated_title": "使用方面-内容文本相互预测的预训练方法进行多方面密集检索",
    "translated_abstract": "基于预训练语言模型（PLMs），已经对纯文本上的密集检索进行了广泛研究。然而，对于使用密集模型进行多方面数据检索的研究却很少。在产品搜索等场景中，方面信息在相关匹配中起着重要作用，例如类别：电子设备、计算机和宠物用品等。利用方面信息进行多方面检索的常见方法是引入辅助分类目标，即使用物品内容预测物品方面的注释值ID。然而，通过从头学习值嵌入，这种方法可能无法充分捕捉值之间的各种语义相似性。为解决这一局限性，我们在预训练过程中将方面信息作为文本字符串而不是类别ID，以便在PLMs中自然地捕捉它们的语义相似性。为了便于使用方面字符串进行有效的检索，我们使用了一种新的方法。",
    "tldr": "这项研究基于预训练语言模型，提出了一种使用方面-内容文本相互预测的方法，以实现多方面密集检索，解决了现有方法通常无法捕捉值之间语义相似性的问题。",
    "en_tdlr": "This study propose a method for multi-aspect dense retrieval using pre-training language models, by predicting aspect-content text mutualy, which addresses the problem of existing methods failing to capture the semantic similarities between values sufficiently."
}