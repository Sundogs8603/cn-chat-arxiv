{
    "title": "TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])",
    "abstract": "Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This",
    "link": "http://arxiv.org/abs/2308.16890",
    "context": "Title: TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])\nAbstract: Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This",
    "path": "papers/23/08/2308.16890.json",
    "total_tokens": 882,
    "translated_title": "TouchStone: 用语言模型评估视觉-语言模型",
    "translated_abstract": "大规模视觉-语言模型（LVLMs）近年来取得了快速进展，通过将视觉接收器与大型语言模型（LLMs）相连接，展现出了惊人的感知、理解和处理视觉信息的能力。然而，目前的评估主要关注识别和推理能力，缺乏对对话能力和视觉叙事能力的直接评估。本文提出了一种评估方法，使用强大的LLMs作为评委来全面评估LVLMs的各种能力。首先，我们构建了一个包含开放世界图像和问题的综合视觉对话数据集TouchStone，涵盖了五个主要能力和27个子任务。该数据集不仅涵盖了基础的识别和理解，还扩展到文学创作。其次，通过整合详细的图像注释，我们有效地将多模态输入内容转化为LLMs可以理解的形式。",
    "tldr": "TouchStone提出了一种评估方法，使用强大的语言模型作为评委来全面评估大规模视觉-语言模型的各种能力。通过构建综合的视觉对话数据集和整合图像注释，评估包括识别、理解、对话和叙事等多个能力。"
}