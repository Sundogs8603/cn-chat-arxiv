{
    "title": "Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])",
    "abstract": "Traditional neural word embeddings are usually dependent on a richer diversity of vocabulary. However, the language models recline to cover major vocabularies via the word embedding parameters, in particular, for multilingual language models that generally cover a significant part of their overall learning parameters. In this work, we present a new compact embedding structure to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models. The subspace embedding structure calibrates to masked language models, to evaluate our compact embedding structure on similarity and textual entailment tasks, sentence and paraphrase tasks. Our experimental evaluation shows that the subspace embeddings achieve compression rates beyond 99.8% in comparison with the original e",
    "link": "http://arxiv.org/abs/2308.08688",
    "context": "Title: Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])\nAbstract: Traditional neural word embeddings are usually dependent on a richer diversity of vocabulary. However, the language models recline to cover major vocabularies via the word embedding parameters, in particular, for multilingual language models that generally cover a significant part of their overall learning parameters. In this work, we present a new compact embedding structure to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models. The subspace embedding structure calibrates to masked language models, to evaluate our compact embedding structure on similarity and textual entailment tasks, sentence and paraphrase tasks. Our experimental evaluation shows that the subspace embeddings achieve compression rates beyond 99.8% in comparison with the original e",
    "path": "papers/23/08/2308.08688.json",
    "total_tokens": 887,
    "translated_title": "轻量级神经语言模型通过子空间嵌入进行适应性处理",
    "translated_abstract": "传统的神经词嵌入通常依赖于更丰富的词汇多样性。然而，语言模型倾向于通过单词嵌入参数来覆盖主要词汇，特别是对于通常覆盖其整体学习参数中的重要部分的多语言语言模型来说。在这项工作中，我们提出了一种新的紧凑嵌入结构，通过牺牲高达4%的绝对准确度来减少预训练语言模型的内存占用。嵌入向量的重建遵循一组子空间嵌入和通过从预训练语言模型中的标记之间的上下文关系进行的分配过程。子空间嵌入结构适应了掩码语言模型，以在相似性和文本蕴涵任务、句子和释义任务中评估我们的紧凑嵌入结构。我们的实验评估表明，与原始e相比，子空间嵌入实现了超过99.8%的压缩率。",
    "tldr": "本论文提出了一种新的紧凑嵌入结构，通过牺牲部分准确度，减少预训练语言模型的内存占用。实验证明，该结构可以实现超过99.8%的压缩率。",
    "en_tdlr": "This paper proposes a new compact embedding structure to reduce the memory footprint of pre-trained language models with a sacrifice of partial accuracy, achieving compression rates beyond 99.8%."
}