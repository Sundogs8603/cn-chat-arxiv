{
    "title": "NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search. (arXiv:2308.05600v1 [cs.LG])",
    "abstract": "Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar m",
    "link": "http://arxiv.org/abs/2308.05600",
    "context": "Title: NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search. (arXiv:2308.05600v1 [cs.LG])\nAbstract: Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar m",
    "path": "papers/23/08/2308.05600.json",
    "total_tokens": 862,
    "translated_title": "NUPES：通过指数搜索进行非均匀后训练量化",
    "translated_abstract": "深度神经网络（DNN）的部署由于其昂贵的计算需求而局限于较大的硬件设备。随着大型语言模型（LLM）的出现，这一挑战最近已经达到了另一个层面。为了减少内存占用和延迟，一种有希望的技术是量化。它通过将浮点表示转换为低位宽定点表示来实现，通常假设均匀映射到正则网格上。然而，这个过程，在文献中被称为均匀量化，可能不适合，因为大多数DNN的权重和激活遵循钟形分布。在LLM中更糟糕的是，其权重分布被认为具有大量、高影响力的异常值。在这项工作中，我们提出了对解决深度学习模型量化限制最常用方法的改进，即非均匀量化。NUPES利用自同构保持标量m",
    "tldr": "NUPES提出一种改进的非均匀量化方法，用于解决深度学习模型量化中的局限性，并通过利用自同构来保持标量m。",
    "en_tdlr": "NUPES proposes an improved non-uniform quantization method to address the limitations of quantizing deep learning models, and preserves the scalar m using automorphisms."
}