{
    "title": "BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition. (arXiv:2308.08625v1 [cs.CL])",
    "abstract": "Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-",
    "link": "http://arxiv.org/abs/2308.08625",
    "context": "Title: BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition. (arXiv:2308.08625v1 [cs.CL])\nAbstract: Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-",
    "path": "papers/23/08/2308.08625.json",
    "total_tokens": 924,
    "translated_title": "BIOptimus：使用课程学习为命名实体识别预训练了一种最优生物医学语言模型",
    "translated_abstract": "在大型语料库上进行自监督预训练并对下游任务进行微调已经成为处理监督学习任务（如命名实体识别）中有限标注数据问题的一种方法。最近在生物医学语言处理领域的研究中，提出了多种使用不同方法和技术进行预训练的生物医学语言模型，这些模型在许多BioNLP任务中，包括命名实体识别方面取得了进展。然而，目前还缺乏对预训练方法进行全面比较，以找到在生物医学领域更优的方法。本文旨在研究不同的预训练方法，如从头开始预训练生物医学语言模型和连续预训练生物医学语言模型。我们将现有方法与我们提出的预训练方法进行比较，该方法通过将BERT模型中的现有权重提炼到上下文中找到的新标记的权重来初始化权重。该方法有助于加快预训练速度。",
    "tldr": "本文通过比较不同的预训练方法，提出了一种初始化权重的新方法，该方法通过从BERT模型中提取现有权重来加速预训练，以探索在生物医学领域更优的预训练方法。",
    "en_tdlr": "This paper proposes a new method for initializing weights by distilling existing weights from the BERT model, which accelerates pre-training, in order to explore more optimal pre-training approaches in the biomedical domain."
}