{
    "title": "Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])",
    "abstract": "Interpreting a seemingly-simple function word like \"or\", \"behind\", or \"more\" can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learnt by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives \"and\" and \"or\" without any prior knowledge of logical reasoning, as well as early evidence that they can develop the ability to reason about alte",
    "link": "http://arxiv.org/abs/2308.08628",
    "context": "Title: Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])\nAbstract: Interpreting a seemingly-simple function word like \"or\", \"behind\", or \"more\" can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learnt by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives \"and\" and \"or\" without any prior knowledge of logical reasoning, as well as early evidence that they can develop the ability to reason about alte",
    "path": "papers/23/08/2308.08628.json",
    "total_tokens": 1065,
    "translated_title": "从视觉问答模型中以基于语境语言学习功能词的意义",
    "translated_abstract": "解释一个看似简单的功能词，如“或者”，“在......后面”，或“更多”可能需要逻辑、数字和关系推理。儿童如何学习这样的词汇？既往的习得理论通常依赖于认为具有先天知识的基础。然而，最近基于神经网络的视觉问答模型显然可以通过使用功能词来回答关于复杂视觉场景的问题而进行学习。在本文中，我们研究了这些模型对功能词的学习，并希望更好地了解这些词汇的意义如何被模型和儿童所学习。我们展示了在以视觉为基础的语言上训练的递归模型学习了需要空间和数字推理的功能词的梯度语义。此外，我们发现这些模型可以在没有任何逻辑推理的先验知识下学习到\"和\"和\"或\"的意义，并迅速发展出进行替换推论的能力的早期证据。",
    "tldr": "本研究通过研究基于视觉问答模型学习到的功能词的意义，旨在更好地了解模型和儿童如何学习这些词汇。研究发现，在以视觉为基础的语言上训练的递归模型能够学习到需要空间和数字推理的功能词的梯度语义，并且可以在没有逻辑推理先验知识的情况下学习到\"和\"和\"或\"的意义，以及迅速发展出替换推论的能力的早期证据。",
    "en_tdlr": "This study investigates the learning of the meanings of function words by visual question answering models, aiming to understand how both models and children acquire these vocabulary. The findings show that recurrent models trained on visually grounded language can learn the gradient semantics for function words requiring spatial and numerical reasoning. Moreover, the study reveals that these models can learn the meanings of logical connectives \"and\" and \"or\" without prior knowledge of logical reasoning, and provide early evidence of their ability to reason about replacements."
}