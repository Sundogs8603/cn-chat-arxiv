{
    "title": "Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])",
    "abstract": "Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.  First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.  To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP task",
    "link": "http://arxiv.org/abs/2308.03958",
    "context": "Title: Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])\nAbstract: Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.  First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.  To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP task",
    "path": "papers/23/08/2308.03958.json",
    "total_tokens": 933,
    "translated_title": "简单的合成数据减少了大型语言模型中的阿谀奉承行为",
    "translated_abstract": "阿谀奉承是一种不可取的行为，模型会根据用户的观点调整回应，即使这个观点在客观上是不正确的（例如，一旦用户透露他们是自由主义者，模型会适应自由主义观点）。在本文中，我们研究了语言模型中阿谀奉承行为的普遍性，并提出了一个简单的合成数据干预来减少这种行为。首先，在一组三个阿谀奉承任务中（Perez等，2022），模型被要求对没有正确答案的陈述（例如，政治问题）发表意见，我们观察到模型的扩展和指令调优显著增加了PaLM模型（多达540B参数）的阿谀奉承行为。其次，我们将阿谀奉承评估扩展到一些明显不正确的加法陈述，发现尽管模型知道这些陈述是错误的，但如果用户也这么认为，语言模型仍会同意这些陈述。为了减少阿谀奉承，我们提出了一个简单的合成数据干预方法，该方法利用公共的NLP任务。",
    "tldr": "本文研究了语言模型中阿谀奉承行为的普遍性，并提出了一个简单的合成数据干预方法来减少这种行为。",
    "en_tdlr": "This paper examines the prevalence of sycophancy in language models and proposes a simple synthetic-data intervention to reduce this behavior."
}