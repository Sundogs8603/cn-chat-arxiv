{
    "title": "FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])",
    "abstract": "Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo",
    "link": "http://arxiv.org/abs/2308.04265",
    "context": "Title: FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])\nAbstract: Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo",
    "path": "papers/23/08/2308.04265.json",
    "total_tokens": 1025,
    "translated_title": "FLIRT: 反馈循环背景下的红队行动",
    "translated_abstract": "警告：本论文内容可能不合适或冒犯人。随着生成模型在各种应用中可供公众使用，测试和分析这些模型的漏洞已成为一项优先任务。在这里，我们提出了一个自动红队行动框架，对给定模型进行评估，并暴露其对不安全和不适当内容生成的漏洞。我们的框架使用反馈循环中的背景学习来进行红队行动，并激发模型生成不安全内容。我们提出了不同的背景攻击策略，用于自动学习用于文本到图像模型的有效和多样化的对抗提示。我们的实验表明，与基线方法相比，我们提出的策略在暴露稳定扩散(SD)模型的漏洞时显著更有效，即使后者采用了安全功能的增强。此外，我们证明了所提出的框架对于红队行动文本到文本模型也是有效的。",
    "tldr": "本论文提出了一个自动红队行动框架，通过反馈循环和背景学习来评估和暴露生成模型的漏洞，特别是对不安全和不适当内容的生成。对于文本到图像模型，采用不同的背景攻击策略可以学习出有效和多样化的对抗提示。相比基线方法，该策略在暴露漏洞方面更加有效，甚至在模型增加安全功能的情况下仍然能够发现漏洞。该框架也适用于红队行动文本到文本模型。",
    "en_tdlr": "This paper proposes an automatic red teaming framework that evaluates and exposes vulnerabilities in generative models, particularly in generating unsafe and inappropriate content. Different in-context attack strategies are used to learn effective and diverse adversarial prompts for text-to-image models. The proposed strategy is more effective in exposing vulnerabilities compared to baseline approaches, even when the model is enhanced with safety features. The framework is also effective for red teaming text-to-text models."
}