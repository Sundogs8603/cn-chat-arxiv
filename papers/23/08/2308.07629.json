{
    "title": "Learning from All Sides: Diversified Positive Augmentation via Self-distillation in Recommendation. (arXiv:2308.07629v1 [cs.IR])",
    "abstract": "Personalized recommendation relies on user historical behaviors to provide user-interested items, and thus seriously struggles with the data sparsity issue. A powerful positive item augmentation is beneficial to address the sparsity issue, while few works could jointly consider both the accuracy and diversity of these augmented training labels. In this work, we propose a novel model-agnostic Diversified self-distillation guided positive augmentation (DivSPA) for accurate and diverse positive item augmentations. Specifically, DivSPA first conducts three types of retrieval strategies to collect high-quality and diverse positive item candidates according to users' overall interests, short-term intentions, and similar users. Next, a self-distillation module is conducted to double-check and rerank these candidates as the final positive augmentations. Extensive offline and online evaluations verify the effectiveness of our proposed DivSPA on both accuracy and diversity. DivSPA is simple and ",
    "link": "http://arxiv.org/abs/2308.07629",
    "context": "Title: Learning from All Sides: Diversified Positive Augmentation via Self-distillation in Recommendation. (arXiv:2308.07629v1 [cs.IR])\nAbstract: Personalized recommendation relies on user historical behaviors to provide user-interested items, and thus seriously struggles with the data sparsity issue. A powerful positive item augmentation is beneficial to address the sparsity issue, while few works could jointly consider both the accuracy and diversity of these augmented training labels. In this work, we propose a novel model-agnostic Diversified self-distillation guided positive augmentation (DivSPA) for accurate and diverse positive item augmentations. Specifically, DivSPA first conducts three types of retrieval strategies to collect high-quality and diverse positive item candidates according to users' overall interests, short-term intentions, and similar users. Next, a self-distillation module is conducted to double-check and rerank these candidates as the final positive augmentations. Extensive offline and online evaluations verify the effectiveness of our proposed DivSPA on both accuracy and diversity. DivSPA is simple and ",
    "path": "papers/23/08/2308.07629.json",
    "total_tokens": 994,
    "translated_title": "从各个方面学习：通过自我蒸馏实现多样化的正向增强在推荐中的应用",
    "translated_abstract": "个性化推荐依赖于用户的历史行为来提供用户感兴趣的物品，因此严重受到数据稀疏问题的困扰。一个强大的正向物品增强有助于解决稀疏问题，然而很少有工作能够同时考虑这些增强训练标签的准确性和多样性。在这项工作中，我们提出了一种新颖的模型无关的多样化自我蒸馏引导的正向增强（DivSPA），用于准确和多样化的正向物品增强。具体而言，DivSPA首先通过三种检索策略收集与用户的整体兴趣、短期意图和相似用户相对应的高质量和多样化的正向候选物品。接下来，进行自我蒸馏模块来重新检查和重新排名这些候选者作为最终的正向增强。广泛的离线和在线评估验证了我们提出的DivSPA在准确性和多样性方面的有效性。DivSPA简单且有效。",
    "tldr": "本文介绍了一种新的模型无关的多样化自我蒸馏引导的正向增强方法（DivSPA），用于解决个性化推荐中数据稀疏问题。DivSPA通过多种检索策略收集高质量和多样化的正向候选物品，并通过自我蒸馏模块重新排名这些候选者，从而提高了推荐准确性和多样性。",
    "en_tdlr": "This paper introduces a novel model-agnostic approach called DivSPA for diversified positive augmentation in personalized recommendation. DivSPA addresses the data sparsity issue by collecting high-quality and diverse positive item candidates through multiple retrieval strategies and reranking them using a self-distillation module, leading to improved accuracy and diversity in the recommendation process."
}