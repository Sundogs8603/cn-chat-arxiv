{
    "title": "SGDiff: A Style Guided Diffusion Model for Fashion Synthesis. (arXiv:2308.07605v1 [cs.CV])",
    "abstract": "This paper reports on the development of \\textbf{a novel style guided diffusion model (SGDiff)} which overcomes certain weaknesses inherent in existing models for image synthesis. The proposed SGDiff combines image modality with a pretrained text-to-image diffusion model to facilitate creative fashion image synthesis. It addresses the limitations of text-to-image diffusion models by incorporating supplementary style guidance, substantially reducing training costs, and overcoming the difficulties of controlling synthesized styles with text-only inputs. This paper also introduces a new dataset -- SG-Fashion, specifically designed for fashion image synthesis applications, offering high-resolution images and an extensive range of garment categories. By means of comprehensive ablation study, we examine the application of classifier-free guidance to a variety of conditions and validate the effectiveness of the proposed model for generating fashion images of the desired categories, product at",
    "link": "http://arxiv.org/abs/2308.07605",
    "context": "Title: SGDiff: A Style Guided Diffusion Model for Fashion Synthesis. (arXiv:2308.07605v1 [cs.CV])\nAbstract: This paper reports on the development of \\textbf{a novel style guided diffusion model (SGDiff)} which overcomes certain weaknesses inherent in existing models for image synthesis. The proposed SGDiff combines image modality with a pretrained text-to-image diffusion model to facilitate creative fashion image synthesis. It addresses the limitations of text-to-image diffusion models by incorporating supplementary style guidance, substantially reducing training costs, and overcoming the difficulties of controlling synthesized styles with text-only inputs. This paper also introduces a new dataset -- SG-Fashion, specifically designed for fashion image synthesis applications, offering high-resolution images and an extensive range of garment categories. By means of comprehensive ablation study, we examine the application of classifier-free guidance to a variety of conditions and validate the effectiveness of the proposed model for generating fashion images of the desired categories, product at",
    "path": "papers/23/08/2308.07605.json",
    "total_tokens": 974,
    "translated_title": "SGDiff: 一种用于时尚合成的风格引导扩散模型",
    "translated_abstract": "本文介绍了一种新颖的风格引导扩散模型（SGDiff），它克服了现有图像合成模型固有的一些弱点。所提出的SGDiff将图像模态与预训练的文本到图像扩散模型相结合，以促进创造性的时尚图像合成。它通过引入辅助的风格引导来解决文本到图像扩散模型的局限性，大大降低了训练成本，并克服了只有文本输入时控制合成样式的困难。本文还介绍了一个新的数据集-SG-Fashion，专门设计用于时尚图像合成应用，提供高分辨率的图像和广泛的服装类别。通过全面的消融实验，我们研究了在各种条件下应用无分类器引导的效果，并验证了所提出模型生成所需类别的时尚图像的有效性。",
    "tldr": "这个论文介绍了一种名为SGDiff的风格引导扩散模型，通过结合图像模态和预训练的文本到图像扩散模型，成功地实现了创造性时尚图像合成。它通过引入辅助风格引导、减少训练成本以及克服文本输入限制等方式，克服了现有模型的局限性。此外，还提出了一个专门为时尚图像合成设计的数据集SG-Fashion，通过全面的实验证明了所提出模型的有效性。",
    "en_tdlr": "This paper presents SGDiff, a style guided diffusion model that combines image modality with a pretrained text-to-image diffusion model for creative fashion image synthesis. It overcomes limitations of existing models by incorporating supplementary style guidance, reducing training costs, and addressing difficulties in controlling synthesized styles. The paper also introduces the SG-Fashion dataset and validates the effectiveness of the proposed model through comprehensive experiments."
}