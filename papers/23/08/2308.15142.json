{
    "title": "A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information. (arXiv:2308.15142v1 [cs.CV])",
    "abstract": "Biological research has revealed that the verbal semantic information in the brain cortex, as an additional source, participates in nonverbal semantic tasks, such as visual encoding. However, previous visual encoding models did not incorporate verbal semantic information, contradicting this biological finding. This paper proposes a multimodal visual information encoding network model based on stimulus images and associated textual information in response to this issue. Our visual information encoding network model takes stimulus images as input and leverages textual information generated by a text-image generation model as verbal semantic information. This approach injects new information into the visual encoding model. Subsequently, a Transformer network aligns image and text feature information, creating a multimodal feature space. A convolutional network then maps from this multimodal feature space to voxel space, constructing the multimodal visual information encoding network model",
    "link": "http://arxiv.org/abs/2308.15142",
    "context": "Title: A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information. (arXiv:2308.15142v1 [cs.CV])\nAbstract: Biological research has revealed that the verbal semantic information in the brain cortex, as an additional source, participates in nonverbal semantic tasks, such as visual encoding. However, previous visual encoding models did not incorporate verbal semantic information, contradicting this biological finding. This paper proposes a multimodal visual information encoding network model based on stimulus images and associated textual information in response to this issue. Our visual information encoding network model takes stimulus images as input and leverages textual information generated by a text-image generation model as verbal semantic information. This approach injects new information into the visual encoding model. Subsequently, a Transformer network aligns image and text feature information, creating a multimodal feature space. A convolutional network then maps from this multimodal feature space to voxel space, constructing the multimodal visual information encoding network model",
    "path": "papers/23/08/2308.15142.json",
    "total_tokens": 899,
    "translated_title": "受文字语义信息辅助的多模态视觉编码模型",
    "translated_abstract": "生物学研究发现，在大脑皮层中，文字语义信息作为一种额外的来源参与非语言语义任务，比如视觉编码。然而，先前的视觉编码模型没有整合文字语义信息，与这一生物学发现相矛盾。本文针对这个问题提出了一种基于刺激图像和相关文本信息的多模态视觉信息编码网络模型。我们的视觉信息编码网络模型以刺激图像为输入，并利用由文本-图像生成模型生成的文本信息作为文字语义信息。这种方法将新的信息注入到视觉编码模型中。随后，一个Transformer网络对图像和文本特征信息进行对齐，创建一个多模态特征空间。然后，一个卷积网络将从这个多模态特征空间映射到体素空间，构建多模态视觉信息编码网络模型。",
    "tldr": "本文提出了一种多模态视觉信息编码网络模型，通过整合文字语义信息和图像特征，实现了对视觉信息的编码。该模型通过Transformer网络对图像和文本特征进行对齐，构建了一个多模态特征空间，并通过卷积网络将特征映射到体素空间，从而实现了多模态视觉信息编码。",
    "en_tdlr": "This paper proposes a multimodal visual information encoding network model that integrates verbal semantic information and visual features to encode visual information. The model aligns image and text features using a Transformer network to create a multimodal feature space, and maps the features to voxel space using a convolutional network, enabling multimodal visual information encoding."
}