{
    "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v1 [cs.CL])",
    "abstract": "Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue Systems\", proving the evaluation capabilities of prompted LLMs.",
    "link": "http://arxiv.org/abs/2308.16797",
    "context": "Title: Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v1 [cs.CL])\nAbstract: Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue Systems\", proving the evaluation capabilities of prompted LLMs.",
    "path": "papers/23/08/2308.16797.json",
    "total_tokens": 904,
    "translated_title": "简单的LLM提示是稳健且多语言对话评价的最先进技术",
    "translated_abstract": "尽管在自动对话评价指标的开发上已经付出了大量的研究工作，但对评价非英语对话的思考却很少。与此同时，确保指标对语义相似的回答不变也是一个被忽视的问题。为了实现稳健性和多语言性对话评估指标的期望属性，我们提出了一个新颖的框架，利用当前评估模型的优势和新建立的提示大型语言模型(LLM)范式。实证结果表明，我们的框架在多个基准测试中以平均斯皮尔曼相关得分创造了最先进的结果，并在DSTC11 Track 4“开放域对话系统的自动评价指标”中的稳健和多语言任务中排名第一，证明了提示LLM的评估能力。",
    "tldr": "该论文提出了一个新颖的框架，通过利用当前评估模型的优势和新建立的提示大型语言模型(LLM)范式，实现了稳健的、多语言的对话评估指标。实证结果表明，这个框架在多个基准测试中取得了最先进的结果，并在DSTC11 Track 4中的稳健和多语言任务中排名第一。",
    "en_tdlr": "This paper proposes a novel framework that achieves robust and multilingual dialogue evaluation metrics by leveraging the strengths of current evaluation models and the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results demonstrate that this framework achieves state-of-the-art results across multiple benchmarks, ranking first in the Robust and Multilingual tasks of the DSTC11 Track 4."
}