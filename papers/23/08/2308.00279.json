{
    "title": "Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction. (arXiv:2308.00279v1 [cs.LG])",
    "abstract": "Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the ",
    "link": "http://arxiv.org/abs/2308.00279",
    "context": "Title: Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction. (arXiv:2308.00279v1 [cs.LG])\nAbstract: Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the ",
    "path": "papers/23/08/2308.00279.json",
    "total_tokens": 903,
    "translated_title": "基于噪声负样本自校正的鲁棒正负样本学习",
    "translated_abstract": "文献中将从正样本和无标签数据中学习称为正负样本（PU）学习，并在最近几年引起了很大关注。PU学习中一种常见的方法是使用临时阈值从无标签数据中采样一组伪负样本，以便可以应用传统的监督方法来包括正样本和负样本。由于无标签数据中标签的不确定性，将无标签的正样本错误分类为负样本的错误不可避免地出现，并且可能在训练过程中累积。这些错误经常导致性能下降和模型不稳定。为了减轻标签的不确定性影响，提高正负样本学习的鲁棒性，我们提出了一种新的鲁棒PU学习方法，其中训练策略受到人类学习的自然启示：首先学习简单的案例。相似的直觉已经在课程学习中被利用，只使用更简单的案例。",
    "tldr": "本论文提出了一种基于噪声负样本自校正的鲁棒正负样本学习方法，通过首先学习简单的案例来减轻标签的不确定性，提高学习的鲁棒性。",
    "en_tdlr": "This paper proposes a robust positive-unlabeled (PU) learning method based on noise negative sample self-correction. By learning easy cases first to mitigate label uncertainty, it improves the robustness of learning from positive and unlabeled data."
}