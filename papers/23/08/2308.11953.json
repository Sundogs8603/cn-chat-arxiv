{
    "title": "When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])",
    "abstract": "Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \\textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-s",
    "link": "http://arxiv.org/abs/2308.11953",
    "context": "Title: When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])\nAbstract: Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \\textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-s",
    "path": "papers/23/08/2308.11953.json",
    "total_tokens": 893,
    "translated_title": "当MiniBatch SGD遇上SplitFed Learning: 收敛性分析和性能评估",
    "translated_abstract": "联邦学习（FL）使得分布式客户端（例如边缘设备）能够协同进行模型训练，而不需要共享原始数据。然而，FL在计算上可能会很昂贵，因为客户端需要训练整个模型多次。SplitFed学习（SFL）是一种最近的分布式方法，它通过在一个切割层将模型分成两部分来减轻客户设备上的计算负载，其中客户端只需要训练模型的一部分。然而，当客户端数据高度不均衡时，SFL仍然面临着“客户端漂移”问题。为了解决这个问题，我们提出了MiniBatch-SFL。该算法将MiniBatch SGD引入SFL中，其中客户端以FL方式训练客户端模型，而服务器则类似于MiniBatch SGD训练服务器端模型。我们分析了MiniBatch-SFL的收敛性，并通过分析预期的服务器端和客户端模型更新来得到期望损失的界限。",
    "tldr": "当MiniBatch SGD遇上SplitFed Learning：本文提出了MiniBatch-SFL算法，通过引入MiniBatch SGD到SplitFed Learning中，解决了非均衡数据导致的客户端漂移问题。",
    "en_tdlr": "When MiniBatch SGD Meets SplitFed Learning: This paper proposes the MiniBatch-SFL algorithm which addresses the client drift problem caused by non-IID data by incorporating MiniBatch SGD into SplitFed Learning."
}