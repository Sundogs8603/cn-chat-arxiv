{
    "title": "Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])",
    "abstract": "Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the m",
    "link": "http://arxiv.org/abs/2308.02746",
    "context": "Title: Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])\nAbstract: Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the m",
    "path": "papers/23/08/2308.02746.json",
    "total_tokens": 907,
    "translated_title": "元-Tsallis-熵最小化：一种新的领域自适应文本分类自训练方法",
    "translated_abstract": "文本分类是自然语言处理的基本任务，跨领域适应文本分类模型具有广泛应用。自训练通过从模型的预测结果中生成伪样本，并迭代在伪样本上进行训练，即在源域上最小化损失，在目标域上最小化Gibbs熵。然而，Gibbs熵对预测误差非常敏感，因此当领域转移较大时，自训练往往会失败。在本文中，我们提出了元-Tsallis-熵最小化（MTEM）方法，该方法应用元学习算法来优化目标域上的实例自适应Tsallis熵。为了降低MTEM的计算成本，我们提出了一种近似技术来近似元学习中涉及的二阶导数。为了高效生成伪标签，我们提出了一种退火采样机制来探索模型的预测概率。从理论上讲，我们证明了m的收敛性",
    "tldr": "元-Tsallis-熵最小化是一种新的自适应文本分类领域自适应方法，通过优化目标域上的实例自适应Tsallis熵来解决自训练在大领域转移时失败的问题。",
    "en_tdlr": "Meta-Tsallis-Entropy Minimization is a new self-training approach for domain adaptation in text classification. It addresses the failure of self-training in large domain shifts by optimizing the instance adaptive Tsallis entropy on the target domain."
}