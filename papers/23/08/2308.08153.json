{
    "title": "Fast Training of NMT Model with Data Sorting. (arXiv:2308.08153v1 [cs.CL])",
    "abstract": "The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated ",
    "link": "http://arxiv.org/abs/2308.08153",
    "context": "Title: Fast Training of NMT Model with Data Sorting. (arXiv:2308.08153v1 [cs.CL])\nAbstract: The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated ",
    "path": "papers/23/08/2308.08153.json",
    "total_tokens": 850,
    "translated_title": "用数据排序进行快速训练的NMT模型",
    "translated_abstract": "Transformer模型已经为神经机器翻译等自然语言处理任务带来了革命，并且已经有很多研究致力于提高Transformer框架的效率和准确性。改进的潜在领域之一是解决Transformer计算并且后续舍弃空标记的计算问题，从而减轻不必要的计算负担。为了解决这个问题，我们提出了一种在批处理之前根据句子长度排序翻译句子对的算法，最小化计算能量的浪费。由于排序的数量可能违反相互独立和独立同分布（IID）数据假设，我们对数据进行部分排序。在实验中，我们将提出的方法应用于英文-韩文和英文-卢干达语对的机器翻译，并显示出在保持性能的同时节省了计算时间。我们的方法与架构无关，因此可以轻松集成。",
    "tldr": "本论文提出了一种通过对翻译句子对进行排序来节省计算能力的算法，用于提高Transformer模型的训练速度。实验证明，在保持性能的同时，通过这种方法可以显著减少计算时间。"
}