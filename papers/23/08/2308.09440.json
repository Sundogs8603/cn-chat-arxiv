{
    "title": "Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)",
    "abstract": "With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a c",
    "link": "http://arxiv.org/abs/2308.09440",
    "context": "Title: Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)\nAbstract: With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a c",
    "path": "papers/23/08/2308.09440.json",
    "total_tokens": 968,
    "translated_title": "Scope is all you need: Transforming LLMs for HPC Code (arXiv:2308.09440v2 [cs.CL] UPDATED)",
    "translated_abstract": "随着强大计算资源的更容易获取，AI领域的软件开发越来越倾向于开发更大的语言模型（LLMs）来解决各种编程任务。即使应用于高性能计算（HPC）领域的LLMs也非常庞大（例如，数十亿个参数），并且需要昂贵的计算资源进行训练。我们发现这种设计选择令人困惑-为什么我们需要在与HPC不相关的自然语言和编程语言上训练的大型LLMs来处理HPC特定任务？在这一系列工作中，我们旨在质疑现有LLMs所做的设计选择，通过为特定领域开发更小的LLMs-我们称之为领域特定LLMs。具体而言，我们以HPC为领域开始，并提出了一种名为Tokompiler的新型标记器，专门用于HPC中的代码预处理和编译中心任务。Tokompiler利用语言原语的知识生成面向语言的标记，提供了一种新的预处理方法。",
    "tldr": "本研究旨在质疑现有大型语言模型（LLMs）在高性能计算（HPC）领域中的应用，并通过开发针对特定领域的小型LLMs来解决该问题。具体而言，我们为HPC开发了一种名为Tokompiler的新型标记器，用于代码预处理和编译任务。",
    "en_tdlr": "This research questions the application of large language models (LLMs) in the field of high-performance computing (HPC) and proposes the development of smaller domain-specific LLMs. Specifically, a novel tokenizer named Tokompiler is introduced for code preprocessing and compilation tasks in the HPC domain."
}