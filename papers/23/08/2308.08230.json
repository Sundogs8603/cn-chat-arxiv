{
    "title": "Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance. (arXiv:2308.08230v1 [cs.LG])",
    "abstract": "Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead b",
    "link": "http://arxiv.org/abs/2308.08230",
    "context": "Title: Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance. (arXiv:2308.08230v1 [cs.LG])\nAbstract: Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead b",
    "path": "papers/23/08/2308.08230.json",
    "total_tokens": 899,
    "translated_title": "探索Winograd卷积用于成本效益的神经网络容错性",
    "translated_abstract": "Winograd通常用于优化卷积性能和计算效率，因为它减少了乘法运算，但通常忽略了Winograd带来的可靠性问题。在这项工作中，我们观察到Winograd卷积在提高神经网络容错性方面具有巨大潜力。基于这一观察，我们首次全面评估了Winograd卷积容错性，从模型、层和操作类型等不同粒度进行评估。然后，我们探索了Winograd卷积的内在容错性如何与经典的容错设计方法（包括三重模块冗余、容错重训练和约束激活函数）有效结合，实现对软错误的成本效益的神经网络保护。根据我们的实验，Winograd卷积可以降低容错设计开销。",
    "tldr": "本论文探索了Winograd卷积在神经网络容错性方面的潜力，并评估了从不同粒度（模型、层、操作类型）进行的综合容错评估。研究发现Winograd卷积能够降低容错设计开销，并与经典的容错设计方法有效结合，实现对软错误的成本效益的神经网络保护。",
    "en_tdlr": "This paper explores the potential of Winograd convolution in neural network fault tolerance and evaluates its fault tolerance comprehensively from different granularities (models, layers, operation types). It is found that Winograd convolution can reduce the fault-tolerant design overhead and effectively combine with classical fault-tolerant design methods for cost-effective protection against soft errors in neural networks."
}