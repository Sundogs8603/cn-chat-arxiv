{
    "title": "Planning with Logical Graph-based Language Model for Instruction Generation. (arXiv:2308.13782v1 [cs.CL])",
    "abstract": "Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generat",
    "link": "http://arxiv.org/abs/2308.13782",
    "context": "Title: Planning with Logical Graph-based Language Model for Instruction Generation. (arXiv:2308.13782v1 [cs.CL])\nAbstract: Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generat",
    "path": "papers/23/08/2308.13782.json",
    "total_tokens": 905,
    "translated_title": "使用基于逻辑图的语言模型进行指令生成的规划",
    "translated_abstract": "尽管大型语言模型在生成自然语言文本方面表现出优越性能，但由于神经模型难以从自由形式的文本中捕捉到隐含的规则，因此很难生成具有正确逻辑的文本。在本文中，我们提出了一种新颖的基于图的语言模型，Logical-GLM，将逻辑注入语言模型以进行更有效的文本生成和可解释性。具体而言，我们首先从自然语言指令中提取信息并构建通常描述领域的逻辑贝叶斯图。接下来，我们生成逻辑骨架以指导语言模型训练，将领域知识注入语言模型。最后，我们交替优化图的搜索策略和语言模型，直至收敛。实验结果表明，Logical-GLM与传统语言模型相比，尽管使用规模较小的训练数据和较少的参数，仍然具有有效和高效的性能。我们的方法可以生成有效的指令。",
    "tldr": "本文提出了一种基于逻辑图的语言模型，Logical-GLM，用于指导语言模型生成具有正确逻辑的文本，并以提高文本生成的有效性和可解释性。实验结果表明，Logical-GLM在使用较少数据和参数的情况下仍然有效和高效。",
    "en_tdlr": "This paper proposes a logical graph-based language model, Logical-GLM, to guide language models in generating text with correct logic, improving the effectiveness and interpretability of text generation. The experimental results show that Logical-GLM is effective and efficient even with less data and parameters."
}