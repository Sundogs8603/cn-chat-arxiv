{
    "title": "Language Reward Modulation for Pretraining Reinforcement Learning. (arXiv:2308.12270v1 [cs.LG])",
    "abstract": "Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with rei",
    "link": "http://arxiv.org/abs/2308.12270",
    "context": "Title: Language Reward Modulation for Pretraining Reinforcement Learning. (arXiv:2308.12270v1 [cs.LG])\nAbstract: Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with rei",
    "path": "papers/23/08/2308.12270.json",
    "total_tokens": 918,
    "translated_title": "使用语言奖励调制预训练强化学习",
    "translated_abstract": "通过使用学习到的奖励函数（LRF）来解决稀疏奖励强化学习任务，近年来在任务复杂性方面取得了一些稳定的进展。在这项工作中，我们质疑当今的LRF是否最适合作为任务奖励的直接替代。相反，我们提出利用LRF作为RL的预训练信号。具体来说，我们提出了Language Reward Modulated Pretraining（LAMP），它利用视觉-语言模型（VLM）的零射能力作为预训练的效用来代替下游任务奖励。LAMP使用一个冻结的、预训练的VLM来可扩展地生成噪声的探索奖励，通过计算一个高度多样化的语言指令集和一个代理在预训练环境中的图像观测之间的对比对齐。LAMP与标准的探索奖励一起优化这些奖励。",
    "tldr": "这项工作提出了一种名为LAMP的方法，通过利用语言奖励函数作为强化学习的预训练信号，代替传统的任务奖励，以解决稀疏奖励问题，并在实验中取得了稳定的进展。",
    "en_tdlr": "This work proposes a method called LAMP, which uses language reward functions as a pretraining signal for reinforcement learning. It addresses the issue of sparse rewards and achieves steady progress in experiments by replacing traditional task rewards with language rewards."
}