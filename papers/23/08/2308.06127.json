{
    "title": "Learning Control Policies for Variable Objectives from Offline Data. (arXiv:2308.06127v1 [cs.LG])",
    "abstract": "Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.",
    "link": "http://arxiv.org/abs/2308.06127",
    "context": "Title: Learning Control Policies for Variable Objectives from Offline Data. (arXiv:2308.06127v1 [cs.LG])\nAbstract: Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.",
    "path": "papers/23/08/2308.06127.json",
    "total_tokens": 782,
    "translated_title": "从离线数据中学习可变目标的控制策略",
    "translated_abstract": "离线强化学习为获取动态系统的先进控制策略提供了一种可行的方法，尤其是在无法与环境直接交互时。本文介绍了一种基于模型的策略搜索方法的概念扩展，称为可变目标策略（VOP）。通过这种方法，策略被训练以有效地泛化多种参数化奖励函数的目标。我们证明，通过改变作为策略输入的目标，用户可以在运行时自由调整其行为或重新平衡优化目标，而无需收集额外的观测数据批次或重新训练。",
    "tldr": "本文介绍了一种基于模型的策略搜索方法的扩展——可变目标策略（VOP），通过该方法，策略可以有效地泛化多种参数化奖励函数的目标。通过改变策略的输入目标，用户可以在运行时自由调整其行为或重新平衡优化目标，无需收集额外的观测数据批次或重新训练。",
    "en_tdlr": "This paper introduces a conceptual extension for model-based policy search methods called variable objective policy (VOP), which allows policies to efficiently generalize over a variety of parameterized reward function objectives. By altering the input objectives, users can adjust the policy's behavior or re-balance optimization targets at runtime without the need for additional observation batches or re-training."
}