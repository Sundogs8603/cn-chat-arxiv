{
    "title": "HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation. (arXiv:2308.08497v1 [cs.IR])",
    "abstract": "In real-world streaming recommender systems, user preferences often dynamically change over time (e.g., a user may have different preferences during weekdays and weekends). Existing bandit-based streaming recommendation models only consider time as a timestamp, without explicitly modeling the relationship between time variables and time-varying user preferences. This leads to recommendation models that cannot quickly adapt to dynamic scenarios. To address this issue, we propose a contextual bandit approach using hypernetwork, called HyperBandit, which takes time features as input and dynamically adjusts the recommendation model for time-varying user preferences. Specifically, HyperBandit maintains a neural network capable of generating the parameters for estimating time-varying rewards, taking into account the correlation between time features and user preferences. Using the estimated time-varying rewards, a bandit policy is employed to make online recommendations by learning the laten",
    "link": "http://arxiv.org/abs/2308.08497",
    "context": "Title: HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation. (arXiv:2308.08497v1 [cs.IR])\nAbstract: In real-world streaming recommender systems, user preferences often dynamically change over time (e.g., a user may have different preferences during weekdays and weekends). Existing bandit-based streaming recommendation models only consider time as a timestamp, without explicitly modeling the relationship between time variables and time-varying user preferences. This leads to recommendation models that cannot quickly adapt to dynamic scenarios. To address this issue, we propose a contextual bandit approach using hypernetwork, called HyperBandit, which takes time features as input and dynamically adjusts the recommendation model for time-varying user preferences. Specifically, HyperBandit maintains a neural network capable of generating the parameters for estimating time-varying rewards, taking into account the correlation between time features and user preferences. Using the estimated time-varying rewards, a bandit policy is employed to make online recommendations by learning the laten",
    "path": "papers/23/08/2308.08497.json",
    "total_tokens": 914,
    "translated_title": "HyperBandit: 基于超网络的时间变化用户偏好的上下文强化学习算法在流媒体推荐系统中的应用",
    "translated_abstract": "在现实世界的流媒体推荐系统中，用户偏好经常在时间上动态变化（例如，在工作日和周末用户可能有不同的偏好）。现有的基于强化学习的流媒体推荐模型只将时间视为时间戳，没有明确地建模时间变量与时间变化的用户偏好之间的关系。这导致推荐模型无法快速适应动态场景。为了解决这个问题，我们提出了一种使用超网络的上下文强化学习方法，称为HyperBandit，其将时间特征作为输入，并动态调整推荐模型以适应时间变化的用户偏好。具体而言，HyperBandit维护了一个能够生成用于估计时间变化奖励的参数的神经网络，考虑了时间特征和用户偏好之间的相关性。使用估计的时间变化奖励，我们采用强化学习策略来进行在线推荐。",
    "tldr": "HyperBandit是一种基于超网络的上下文强化学习方法，用于处理流媒体推荐系统中时间变化的用户偏好。它通过建立时间特征和用户偏好之间的关联，动态调整推荐模型以适应动态场景。",
    "en_tdlr": "HyperBandit is a contextual bandit approach using hypernetwork, which adapts the recommendation model based on time-varying user preferences in streaming recommendation systems. It dynamically adjusts the model by considering the correlation between time features and user preferences, allowing it to quickly adapt to dynamic scenarios."
}