{
    "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v1 [cs.CR])",
    "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduc",
    "link": "http://arxiv.org/abs/2308.00958",
    "context": "Title: Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v1 [cs.CR])\nAbstract: Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduc",
    "path": "papers/23/08/2308.00958.json",
    "total_tokens": 1005,
    "translated_title": "隔离和诱导：针对模型窃取攻击的鲁棒深度神经网络训练",
    "translated_abstract": "尽管机器学习模型作为服务（MLaaS）广泛应用，但它们容易受到模型窃取攻击的威胁。这些攻击可以通过黑盒查询过程复制模型功能，而不需要任何关于目标受害模型的先前知识。现有的窃取防御方法通过向受害者的后验概率添加欺骗性扰动来误导攻击者。然而，这些防御方法现在面临着推理计算开销高和良好准确性与防窃鲁棒性之间不利权衡的问题，这挑战了在实践中部署这些模型的可行性。为了解决这些问题，本文提出了一种新颖有效的模型窃取防御训练框架Isolation and Induction（InI）。InI不像部署辅助防御模块那样引入冗余推理时间，而是通过将对手的训练梯度与预期梯度隔离来直接训练防御模型，可以有效地减少推理计算开销。",
    "tldr": "这项研究提出了一种名为隔离和诱导（InI）的训练框架，用于对抗模型窃取攻击。该框架通过隔离对手的训练梯度，并直接训练一个防御模型，有效地解决了现有防御方法中推理计算开销高和准确性与防窃鲁棒性之间的不利权衡问题。",
    "en_tdlr": "This research proposes a training framework called Isolation and Induction (InI) to combat model stealing attacks. By isolating the adversary's training gradient and directly training a defense model, InI effectively addresses the challenges of high inference computational overhead and the trade-off between accuracy and stealing robustness in existing defense methods."
}