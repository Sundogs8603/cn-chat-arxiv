{
    "title": "Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks. (arXiv:2308.02836v1 [cs.LG])",
    "abstract": "We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\\lambda x) = \\lambda f(x)$ for all non-negative $\\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functio",
    "link": "http://arxiv.org/abs/2308.02836",
    "context": "Title: Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks. (arXiv:2308.02836v1 [cs.LG])\nAbstract: We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\\lambda x) = \\lambda f(x)$ for all non-negative $\\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functio",
    "path": "papers/23/08/2308.02836.json",
    "total_tokens": 947,
    "translated_title": "使用尺度不变神经网络逼近正齐次函数",
    "translated_abstract": "本文研究使用ReLU网络解决线性反问题的可能性。由于线性关系带来的尺度不变性，对于这类问题的最优重构函数f是正齐次函数，即满足对于所有非负的λ，有f(λx) = λf(x)。在ReLU网络中，这个条件转化为在网络中不考虑偏置项。我们首先考虑从少量线性测量中恢复稀疏向量的问题。我们证明单隐藏层的ReLU网络无法恢复1-稀疏向量，即使是近似恢复，而且不论网络的宽度如何。然而，通过两个隐藏层，可以以稳定的方式近似地恢复任意精度的、任意稀疏程度为s的向量。然后，我们将结果推广到包括低秩矩阵恢复和相位恢复在内的更广泛的恢复问题。此外，我们还考虑了对一般正齐次函数的逼近问题。",
    "tldr": "本文研究使用尺度不变神经网络近似解决线性反问题的可行性，证明了单隐藏层ReLU网络无法恢复稀疏向量，但通过两个隐藏层可以稳定且精确地恢复任意稀疏程度的向量，此外还推广到了其他恢复问题。",
    "en_tdlr": "This paper investigates the feasibility of using scale invariant neural networks to approximately solve linear inverse problems, demonstrates the limitations of single hidden layer ReLU networks in recovering sparse vectors, and shows that recovery with arbitrary sparsity levels can be achieved in a stable manner with two hidden layers. The results are also extended to a wider class of recovery problems."
}