{
    "title": "Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift. (arXiv:2308.09565v1 [cs.LG])",
    "abstract": "Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to unde",
    "link": "http://arxiv.org/abs/2308.09565",
    "context": "Title: Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift. (arXiv:2308.09565v1 [cs.LG])\nAbstract: Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to unde",
    "path": "papers/23/08/2308.09565.json",
    "total_tokens": 964,
    "translated_title": "规范化就是你所需要的：理解极端标签偏移下的层归一化联邦学习",
    "translated_abstract": "层归一化（LN）是一个广泛采用的深度学习技术，特别在基础模型的时代。最近，已经证明LN在非独立同分布数据上的联邦学习（FL）中非常有效。然而，它为什么以及如何起作用仍然是个谜。在这项工作中，我们揭示了层归一化和联邦学习中的标签偏移问题之间的深刻联系。为了更好地理解FL中的层归一化，我们确定了规范化方法在FL中的关键贡献机制，称之为特征归一化（FN），它在分类器头之前将归一化应用于潜在特征表示。虽然LN和FN不会提高表达能力，但它们控制特征崩溃和局部过拟合，使得对严重倾斜的数据集进行加速全局训练。经验证明，规范化在极端标签偏移下可以引起标准基准的显著改进。此外，我们还进行了大量的割除研究。",
    "tldr": "本论文揭示了层归一化和联邦学习中的标签偏移问题之间的深刻联系，通过在联邦学习中应用特征归一化，使得对严重倾斜的数据集进行加速全局训练，从而在极端标签偏移下获得显著改进。",
    "en_tdlr": "This paper reveals the profound connection between layer normalization and the label shift problem in federated learning, and shows that by applying feature normalization in federated learning, drastic improvements can be achieved on heavily skewed datasets under extreme label shift."
}