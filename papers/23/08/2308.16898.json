{
    "title": "Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])",
    "abstract": "Since its inception in \"Attention Is All You Need\", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. ",
    "link": "http://arxiv.org/abs/2308.16898",
    "context": "Title: Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])\nAbstract: Since its inception in \"Attention Is All You Need\", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. ",
    "path": "papers/23/08/2308.16898.json",
    "total_tokens": 909,
    "translated_title": "Transformers作为支持向量机",
    "translated_abstract": "自从\"Attention Is All You Need\"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\\top X^\\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。",
    "tldr": "这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。",
    "en_tdlr": "This work establishes a formal equivalence between self-attention and hard-margin SVM problems, solving natural language processing tasks through optimization geometry of transformer architecture, and revealing the implicit bias of gradient descent optimization in transformers."
}