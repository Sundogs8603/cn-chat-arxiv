{
    "title": "LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])",
    "abstract": "In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images. However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation. Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually. In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance. Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation. Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language M",
    "link": "http://arxiv.org/abs/2308.05095",
    "context": "Title: LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])\nAbstract: In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images. However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation. Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually. In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance. Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation. Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language M",
    "path": "papers/23/08/2308.05095.json",
    "total_tokens": 954,
    "translated_title": "LayoutLLM-T2I：从LLM中获取布局指导以用于文本到图像生成",
    "translated_abstract": "在文本到图像生成领域，近期稳定扩散技术的显著进展使得生成各种新颖的逼真图像成为可能。然而，当前的模型在复杂自然场景中仍然面临着错位问题（例如，空间关系理解和数字化失败），这阻碍了高保真度的文本到图像生成。尽管最近已经进行了一些改进，通过提供细粒度的指导（例如，草图和涂鸦）来改善可控性，但是由于用户必须手动提供这些指导信息，因此这个问题尚未根本解决。在这项工作中，我们努力合成与给定文本提示语义对齐且不需要任何指导的高保真度图像。为了达到这个目标，我们提出了一个由粗到细的范例，用于布局规划和图像生成。具体而言，我们首先通过基于大型语言模型的上下文学习，在给定的文本提示条件下生成粗粒度的布局。",
    "tldr": "本论文提出了LayoutLLM-T2I的方法，用于从LLM中获取布局指导以用于文本到图像生成。该方法采用由粗到细的范例，通过基于大型语言模型的上下文学习，在给定文本提示的条件下合成与文本语义对齐的高保真度图像。",
    "en_tdlr": "This paper proposes a method called LayoutLLM-T2I for eliciting layout guidance from LLM for text-to-image generation. The method utilizes a coarse-to-fine paradigm and in-context learning based on large language models to synthesize high-fidelity images that are semantically aligned with a given textual prompt."
}