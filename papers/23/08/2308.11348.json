{
    "title": "Careful at Estimation and Bold at Exploration. (arXiv:2308.11348v1 [cs.LG])",
    "abstract": "Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two d",
    "link": "http://arxiv.org/abs/2308.11348",
    "context": "Title: Careful at Estimation and Bold at Exploration. (arXiv:2308.11348v1 [cs.LG])\nAbstract: Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two d",
    "path": "papers/23/08/2308.11348.json",
    "total_tokens": 899,
    "translated_title": "谨慎估计，大胆探索",
    "translated_abstract": "由于无限的动作空间，连续动作空间中的探索策略往往是启发式的，这些方法无法得出一般性结论。在先前的研究中，已经展示了对确定性策略强化学习中的连续动作空间进行基于策略的探索的好处。然而，在确定性策略强化学习中，基于策略的探索存在两个主要问题：无目标的探索和策略发散，并且由于估计不准确，策略梯度对探索的帮助仅在某些情况下有效。基于双Q函数框架，我们引入了一种新颖的探索策略来缓解这些问题，与策略梯度分离。我们首先提出了贪婪的Q softmax更新方案来更新Q值。期望Q值通过在动作上加权求和保守Q值得到，权重为相应的贪婪Q值。贪婪Q取两个Q函数的最大值，保守Q取两个Q函数的最小值。",
    "tldr": "本文中提出了一个基于双Q函数框架的新颖探索策略，以解决连续动作空间中基于策略的探索中的问题，并提出了贪婪的Q softmax更新方案来更新Q值。"
}