{
    "title": "DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning. (arXiv:2308.08290v1 [cs.LG])",
    "abstract": "To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models",
    "link": "http://arxiv.org/abs/2308.08290",
    "context": "Title: DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning. (arXiv:2308.08290v1 [cs.LG])\nAbstract: To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models",
    "path": "papers/23/08/2308.08290.json",
    "total_tokens": 859,
    "translated_title": "DFedADMM：用于分散式联邦学习的双重约束控制模型不一致性",
    "translated_abstract": "为了解决联邦学习中的通信负担问题，分散式联邦学习（DFL）舍弃中央服务器，建立分散式通信网络，每个客户端仅与相邻客户端通信。然而，现有的DFL方法仍然面临两个主要挑战：局部不一致性和局部异构过拟合，这些问题尚未从根本上得到解决。为了解决这些问题，我们提出了新的DFL算法DFedADMM及其改进版本DFedADMM-SAM，以提高DFL的性能。DFedADMM算法通过利用双变量来控制由分散的异构数据分布导致的模型不一致性来使用原始-对偶优化（ADMM）。DFedADMM-SAM算法通过使用梯度扰动来生成局部平坦模型并寻找模型来进一步改进DFedADMM。",
    "tldr": "我们提出了一种新的分散式联邦学习算法DFedADMM和其改进版本DFedADMM-SAM，用于解决局部不一致性和局部异构过拟合的问题。",
    "en_tdlr": "We propose novel DFL algorithms, DFedADMM and DFedADMM-SAM, to address the challenges of local inconsistency and local heterogeneous overfitting in decentralized federated learning."
}