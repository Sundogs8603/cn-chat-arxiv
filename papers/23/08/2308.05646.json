{
    "title": "AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])",
    "abstract": "Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.  To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and",
    "link": "http://arxiv.org/abs/2308.05646",
    "context": "Title: AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])\nAbstract: Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.  To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and",
    "path": "papers/23/08/2308.05646.json",
    "total_tokens": 815,
    "translated_title": "AST-MHSA: 利用多头自注意力进行代码摘要",
    "translated_abstract": "代码摘要旨在为源代码生成简明的自然语言描述。现有的方法采用基于Transformer的编码器-解码器架构，其中利用源代码的抽象语法树（AST）来编码结构信息。然而，AST比对应的源代码要长得多，现有方法通过直接将整个线性化的AST输入到编码器中来忽略这个大小约束。这种简化的方法使得从过长的输入序列中提取真正有价值的依赖关系变得具有挑战性，并且由于对AST中的所有节点应用自注意力，导致了显著的计算开销。为了有效而高效地解决这个问题，我们提出了一个模型AST-MHSA，利用多头注意力从AST中提取重要的语义信息。该模型由两个主要组件组成：一个编码器和一个解码器。编码器以代码的抽象语法树（AST）作为输入，并",
    "tldr": "AST-MHSA模型通过多头自注意力从AST中提取重要的语义信息，并利用编码器-解码器架构生成源代码的简明自然语言描述。",
    "en_tdlr": "The AST-MHSA model extracts important semantic information from the AST using multi-head self-attention, and generates concise natural language descriptions for source code using an encoder-decoder architecture."
}