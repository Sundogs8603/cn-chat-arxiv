{
    "title": "Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])",
    "abstract": "Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e",
    "link": "http://arxiv.org/abs/2308.02001",
    "context": "Title: Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])\nAbstract: Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e",
    "path": "papers/23/08/2308.02001.json",
    "total_tokens": 969,
    "translated_title": "具有平滑激活函数的两层神经网络的存储容量研究",
    "translated_abstract": "确定具有m个隐藏神经元和输入维数d（即md+m个训练参数）的两层神经网络的存储容量，即网络能够记忆的一般数据的最大尺寸，是一个基本的机器学习问题。对于非多项式实解析激活函数，如sigmoid和平滑的修正线性单元（平滑ReLU），我们建立了md/2的下界，并且准确性大约为2倍。类似的先前结果仅限于阶跃函数和ReLU激活函数，对于平滑激活函数的结果受到对数因子和随机数据的限制。为了分析存储容量，我们通过计算涉及Hadamard幂和Khati-Rao积的矩阵的秩来考察网络雅可比矩阵的秩。我们的计算扩展了关于Hadamard幂秩的经典线性代数事实。总体而言，我们的方法与之前关于存储容量的研究不同，并有希望实现。",
    "tldr": "研究发现，具有平滑激活函数的两层神经网络的存储容量下界为md/2，并且准确性大致为2倍。分析存储容量的方法包括计算网络雅可比矩阵的秩，并扩展了有关Hadamard幂秩的经典线性代数事实。"
}