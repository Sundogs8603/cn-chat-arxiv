{
    "title": "Quantization Aware Factorization for Deep Neural Network Compression. (arXiv:2308.04595v1 [cs.LG])",
    "abstract": "Tensor decomposition of convolutional and fully-connected layers is an effective way to reduce parameters and FLOP in neural networks. Due to memory and power consumption limitations of mobile or embedded devices, the quantization step is usually necessary when pre-trained models are deployed. A conventional post-training quantization approach applied to networks with decomposed weights yields a drop in accuracy. This motivated us to develop an algorithm that finds tensor approximation directly with quantized factors and thus benefit from both compression techniques while keeping the prediction quality of the model. Namely, we propose to use Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition with factors whose elements lie on a specified quantization grid. We compress neural network weights with a devised algorithm and evaluate it's prediction quality and performance. We compare our approach to state-of-the-art post-training quantization method",
    "link": "http://arxiv.org/abs/2308.04595",
    "context": "Title: Quantization Aware Factorization for Deep Neural Network Compression. (arXiv:2308.04595v1 [cs.LG])\nAbstract: Tensor decomposition of convolutional and fully-connected layers is an effective way to reduce parameters and FLOP in neural networks. Due to memory and power consumption limitations of mobile or embedded devices, the quantization step is usually necessary when pre-trained models are deployed. A conventional post-training quantization approach applied to networks with decomposed weights yields a drop in accuracy. This motivated us to develop an algorithm that finds tensor approximation directly with quantized factors and thus benefit from both compression techniques while keeping the prediction quality of the model. Namely, we propose to use Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition with factors whose elements lie on a specified quantization grid. We compress neural network weights with a devised algorithm and evaluate it's prediction quality and performance. We compare our approach to state-of-the-art post-training quantization method",
    "path": "papers/23/08/2308.04595.json",
    "total_tokens": 889,
    "translated_title": "深度神经网络压缩中的量化感知分解方法",
    "translated_abstract": "张量分解是在神经网络中减少参数和FLOP的有效方法。由于移动或嵌入式设备的内存和功耗限制，当部署预训练模型时，通常需要量化步骤。对于具有分解权重的网络应用传统的后训练量化方法会导致准确度下降。针对这个问题，我们提出了一种算法，直接在量化因子上找到张量逼近，从而在保持模型预测品质的同时，同时从压缩技术中受益。具体而言，我们提出使用交替方向乘子法(ADMM)进行具有量化格点元素的规范多线性(CP)分解。我们使用一种设计的算法压缩神经网络权重，并评估其预测品质和性能。我们将我们的方法与最新的后训练量化方法进行比较。",
    "tldr": "本论文提出了一种量化感知分解方法，通过在量化因子上找到张量逼近，既实现了神经网络压缩，又保持了模型预测的准确性。该方法使用交替方向乘子法进行规范多线性分解，并与最新的后训练量化方法进行比较分析。",
    "en_tdlr": "This paper proposes a quantization aware factorization method that achieves neural network compression while maintaining prediction accuracy by finding tensor approximation directly on quantized factors. The method utilizes the Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition and is compared to state-of-the-art post-training quantization methods."
}