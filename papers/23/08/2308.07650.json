{
    "title": "EQ-Net: Elastic Quantization Neural Networks. (arXiv:2308.07650v1 [cs.CV])",
    "abstract": "Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we",
    "link": "http://arxiv.org/abs/2308.07650",
    "context": "Title: EQ-Net: Elastic Quantization Neural Networks. (arXiv:2308.07650v1 [cs.CV])\nAbstract: Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we",
    "path": "papers/23/08/2308.07650.json",
    "total_tokens": 963,
    "translated_title": "EQ-Net：弹性量化神经网络",
    "translated_abstract": "当前的模型量化方法在减少存储空间和计算复杂度方面显示出了巨大的潜力。然而，由于不同硬件所支持的量化形式的多样性，现有解决方案的一个局限性通常是需要针对不同场景进行重复优化。如何构建一个具有灵活量化形式的模型还未被充分研究。在本文中，我们探索了一种一次性网络量化方案，称为弹性量化神经网络（EQ-Net），旨在训练一个强大的权重共享量化超网络。首先，我们提出了一个弹性量化空间（包括弹性位宽、粒度和对称性）以适应各种主流量化形式。其次，我们提出了权重分布正则化损失（WDR-Loss）和分组渐进引导损失（GPG-Loss）来弥合弹性量化空间差异中权重和输出标准差的不一致性。最后，我们通过一阶搜索算法和二分查找算法确定最优量化策略，并在多个数据集上进行了广泛的实验验证。",
    "tldr": "本文提出了EQ-Net，一种弹性量化神经网络，旨在训练一个灵活的权重共享量化超网络。通过探索弹性量化空间和引入权重分布正则化损失和分组渐进引导损失，本文解决了现有模型量化方法中对不同场景重复优化的局限性。"
}