{
    "title": "Solving Attention Kernel Regression Problem via Pre-conditioner",
    "abstract": "arXiv:2308.14304v2 Announce Type: replace  Abstract: The attention mechanism is the key to large language models, and the attention matrix serves as an algorithmic and computational bottleneck for such a scheme. In this paper, we define two problems, motivated by designing fast algorithms for proxy of attention matrix and solving regressions against them. Given an input matrix $A\\in \\mathbb{R}^{n\\times d}$ with $n\\gg d$ and a response vector $b$, we first consider the matrix exponential of the matrix $A^\\top A$ as a proxy, and we in turn design algorithms for two types of regression problems: $\\min_{x\\in \\mathbb{R}^d}\\|(A^\\top A)^jx-b\\|_2$ and $\\min_{x\\in \\mathbb{R}^d}\\|A(A^\\top A)^jx-b\\|_2$ for any positive integer $j$. Studying algorithms for these regressions is essential, as matrix exponential can be approximated term-by-term via these smaller problems. The second proxy is applying exponential entrywise to the Gram matrix, denoted by $\\exp(AA^\\top)$ and solving the regression $\\min",
    "link": "https://arxiv.org/abs/2308.14304",
    "context": "Title: Solving Attention Kernel Regression Problem via Pre-conditioner\nAbstract: arXiv:2308.14304v2 Announce Type: replace  Abstract: The attention mechanism is the key to large language models, and the attention matrix serves as an algorithmic and computational bottleneck for such a scheme. In this paper, we define two problems, motivated by designing fast algorithms for proxy of attention matrix and solving regressions against them. Given an input matrix $A\\in \\mathbb{R}^{n\\times d}$ with $n\\gg d$ and a response vector $b$, we first consider the matrix exponential of the matrix $A^\\top A$ as a proxy, and we in turn design algorithms for two types of regression problems: $\\min_{x\\in \\mathbb{R}^d}\\|(A^\\top A)^jx-b\\|_2$ and $\\min_{x\\in \\mathbb{R}^d}\\|A(A^\\top A)^jx-b\\|_2$ for any positive integer $j$. Studying algorithms for these regressions is essential, as matrix exponential can be approximated term-by-term via these smaller problems. The second proxy is applying exponential entrywise to the Gram matrix, denoted by $\\exp(AA^\\top)$ and solving the regression $\\min",
    "path": "papers/23/08/2308.14304.json",
    "total_tokens": 881,
    "translated_title": "通过预条件器解决注意力核回归问题",
    "translated_abstract": "注意力机制是大型语言模型的关键，注意力矩阵作为这种方案的算法和计算瓶颈。本文通过定义两个问题，旨在设计用于代理注意力矩阵的快速算法以及解决回归问题。首先考虑矩阵$A\\in \\mathbb{R}^{n\\times d}$的矩阵指数$A^\\top A$作为一个代理，然后设计两种类型的回归问题的算法：对于任意正整数$j$，分别是$\\min_{x\\in \\mathbb{R}^d}\\|(A^\\top A)^jx-b\\|_2$和$\\min_{x\\in \\mathbb{R}^d}\\|A(A^\\top A)^jx-b\\|_2$。研究这些回归的算法是至关重要的，因为矩阵指数可以通过这些较小的问题逐项逼近。第二个代理是将指数逐项应用于Gram矩阵，记为$\\exp(AA^\\top)$，并解决回归问题$\\min",
    "tldr": "本文通过定义两个问题，设计了用于注意力矩阵的代理快速算法及其回归问题求解算法。",
    "en_tdlr": "This paper defines two problems, designs fast algorithms for proxy of attention matrix, and solves regression problems against them."
}