{
    "title": "Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)",
    "abstract": "Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and ",
    "link": "http://arxiv.org/abs/2308.10154",
    "context": "Title: Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)\nAbstract: Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and ",
    "path": "papers/23/08/2308.10154.json",
    "total_tokens": 844,
    "translated_title": "分布式学习中的资源自适应牛顿法",
    "translated_abstract": "基于牛顿方法的分布式随机优化方法通过利用曲率信息提供了比一阶方法更好的性能。然而，在大规模和异构学习环境中，牛顿方法的实际适用性受到了诸多挑战的限制，例如与Hessian矩阵相关的高计算和通信成本、子模型多样性、训练的过时性和数据的异构性。为了解决这些挑战，本文引入了一种名为RANL的新颖高效算法，通过采用简单的Hessian初始化和自适应的训练区域分配来克服牛顿方法的限制。该算法表现出令人印象深刻的收敛性质，在随机优化的标准假设下进行了严格分析。理论分析证明了RANL实现了线性收敛率，同时有效地适应了可用资源。",
    "tldr": "本文介绍了一种名为RANL的新颖和高效的算法，通过使用简单的Hessian初始化和自适应的训练区域分配，克服了牛顿法在大规模和异构学习环境中的限制，并实现了线性收敛率。",
    "en_tdlr": "This paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method in large-scale and heterogeneous learning environments by employing a simple Hessian initialization and adaptive assignments of training regions, achieving linear convergence rate."
}