{
    "title": "Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model. (arXiv:2308.07749v1 [cs.CV])",
    "abstract": "The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT",
    "link": "http://arxiv.org/abs/2308.07749",
    "context": "Title: Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model. (arXiv:2308.07749v1 [cs.CV])\nAbstract: The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT",
    "path": "papers/23/08/2308.07749.json",
    "total_tokens": 985,
    "translated_title": "舞蹈化身：通过图像扩散模型合成受姿势和文本引导的人体动作视频",
    "translated_abstract": "在数字领域中创建逼真化身的需求日益增长，需要通过文本描述和姿势来生成高质量的人体视频。我们提出了舞蹈化身，旨在通过姿势和文本提示生成人体运动视频。我们的方法使用经过预训练的T2I扩散模型来自回归地生成每个视频帧。创新的关键在于我们巧妙地利用T2I扩散模型来连续生成视频帧，同时保持上下文相关性。我们克服了在不同姿势下保持人物特征和服装一致性以及在各种人体运动中保持背景连续性的困难。为了确保整个视频中人物外观一致，我们设计了一个帧内对齐模块。这个模块将文本引导的合成人物知识融合到预训练的T2I扩散模型中，结合了ChatGPT的见解。",
    "tldr": "该论文提出了一种名为舞蹈化身的方法，通过姿势和文本引导生成高质量的人体动作视频。创新之处在于巧妙地利用T2I扩散模型连续生成视频帧，并解决了保持人物特征和服装一致性以及背景连续性的困难。通过设计帧内对齐模块，确保整个视频中人物外观一致。",
    "en_tdlr": "This paper presents a method called Dancing Avatar that generates high-quality human motion videos guided by poses and textual cues. The key innovation lies in the clever utilization of a T2I diffusion model to generate video frames successively, addressing challenges in maintaining human character and clothing consistency as well as background continuity. Through the design of an intra-frame alignment module, consistent human appearances are ensured throughout the entire video."
}