{
    "title": "Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])",
    "abstract": "I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to \"grok\" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.",
    "link": "http://arxiv.org/abs/2308.15594",
    "context": "Title: Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])\nAbstract: I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to \"grok\" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.",
    "path": "papers/23/08/2308.15594.json",
    "total_tokens": 901,
    "translated_title": "变形金刚是否能学会最大公约数？",
    "translated_abstract": "本文研究小型变形金刚模型计算两个正整数的最大公约数（GCD）的能力。当训练分布和表示基准仔细选择时，模型可以达到98%的准确率，并且正确预测前100个GCD中的91个。模型的预测是确定性的，并且完全可解释的。在训练过程中，模型学会将具有相同GCD的输入对聚类，并通过其除数进行分类。基本模型通过使用小型基数编码的均匀操作数仅计算少数GCD（最多100个中的38个）：基数的除数乘积。更长的训练时间和更大的基数允许一些模型“了解”小的素数GCD。使用对数均匀操作数进行训练将性能提升到正确的73个GCD，并通过从倒数平方到对数均匀的GCD训练分布的平衡，使性能达到91个GCD。从GCD的均匀分布进行训练模型破坏了确定性模型行为。",
    "tldr": "本文研究了小型变形金刚模型计算最大公约数的能力。通过选择合适的训练分布和表示基准，模型可以达到高准确率，并在预测中表现出明确的模式。"
}