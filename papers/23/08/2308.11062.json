{
    "title": "UnLoc: A Unified Framework for Video Localization Tasks. (arXiv:2308.11062v1 [cs.CV])",
    "abstract": "While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \\url{https://github.com/google-research/scenic}.",
    "link": "http://arxiv.org/abs/2308.11062",
    "context": "Title: UnLoc: A Unified Framework for Video Localization Tasks. (arXiv:2308.11062v1 [cs.CV])\nAbstract: While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \\url{https://github.com/google-research/scenic}.",
    "path": "papers/23/08/2308.11062.json",
    "total_tokens": 884,
    "translated_title": "UnLoc：用于视频定位任务的统一框架",
    "translated_abstract": "虽然像CLIP这样的大规模图像-文本预训练模型已经被用于修剪视频上的多个视频级任务，但它们在未修剪视频的时间定位方面仍然是一个相对未被探索的任务。我们设计了一种称为UnLoc的新方法，它使用预训练的图像和文本模型，并将令牌提供给视频-文本融合模型。融合模块的输出然后用于构建一个特征金字塔，其中每个级别都连接到一个头部以预测每帧的相关性得分和开始/结束时间的偏移。与以前的工作不同，我们的架构使用一个单一阶段模型实现了Moment Retrieval、Temporal Localization和Action Segmentation，而无需行动建议、基于动作的预训练特征或表示掩码。与专门的模型不同，我们在所有三个不同的定位任务上都取得了最先进的结果，采用了统一的方法。代码可在\\url{https://github.com/google-research/scenic}上找到。",
    "tldr": "UnLoc是一个用于视频定位任务的统一框架，通过使用预训练的图像和文本模型以及视频-文本融合模型，实现了Moment Retrieval、Temporal Localization和Action Segmentation三个定位任务的最先进结果。",
    "en_tdlr": "UnLoc is a unified framework for video localization tasks, achieving state of the art results in Moment Retrieval, Temporal Localization, and Action Segmentation by using pretrained image and text models as well as a video-text fusion model."
}