{
    "title": "Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])",
    "abstract": "Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled squ",
    "link": "http://arxiv.org/abs/2308.00710",
    "context": "Title: Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])\nAbstract: Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled squ",
    "path": "papers/23/08/2308.00710.json",
    "total_tokens": 891,
    "translated_title": "实现聚合的类激活图可用于分析类特征的整体贡献",
    "translated_abstract": "深度学习模型在分类任务中取得了显著的性能，但是高复杂度的模型在许多风险敏感的应用中无法使用，除非提供了可理解的解释。可解释的人工智能（xAI）关注于解释类似深度学习的AI系统的决策。我们扩展了最近的类激活图（CAMs）方法，该方法可可视化数据样本中每个特征对分类的贡献重要性。在本文中，我们聚合了来自多个样本的CAMs，以展示语义结构化数据的分类的全局解释。聚合使分析师能够进行复杂的假设，并通过进一步的钻取可视化进行分析。我们的全局CAM的可视化表示以一个方形标记表示每个特征的影响力，方形的颜色表示该特征的分类影响力，填充方形的大小表示该特征的重要性。",
    "tldr": "本文扩展了类激活图的方法，将多个样本的类激活图聚合起来，以展示语义结构化数据的分类的全局解释。聚合过程使得分析师可以进行复杂的假设和进一步的钻取可视化分析。",
    "en_tdlr": "This paper extends the method of Class Activation Maps (CAMs) by aggregating CAMs from multiple samples to provide a global explanation of classification for semantically structured data. The aggregation allows analysts to make complex assumptions and analyze them with further drill-down visualizations."
}