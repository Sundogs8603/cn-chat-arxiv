{
    "title": "Multi-Objective Decision Transformers for Offline Reinforcement Learning. (arXiv:2308.16379v1 [cs.LG])",
    "abstract": "Offline Reinforcement Learning (RL) is structured to derive policies from static trajectory data without requiring real-time environment interactions. Recent studies have shown the feasibility of framing offline RL as a sequence modeling task, where the sole aim is to predict actions based on prior context using the transformer architecture. However, the limitation of this single task learning approach is its potential to undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the ",
    "link": "http://arxiv.org/abs/2308.16379",
    "context": "Title: Multi-Objective Decision Transformers for Offline Reinforcement Learning. (arXiv:2308.16379v1 [cs.LG])\nAbstract: Offline Reinforcement Learning (RL) is structured to derive policies from static trajectory data without requiring real-time environment interactions. Recent studies have shown the feasibility of framing offline RL as a sequence modeling task, where the sole aim is to predict actions based on prior context using the transformer architecture. However, the limitation of this single task learning approach is its potential to undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the ",
    "path": "papers/23/08/2308.16379.json",
    "total_tokens": 854,
    "translated_title": "离线强化学习的多目标决策Transformer",
    "translated_abstract": "离线强化学习 (RL) 结构化地从静态轨迹数据中提取策略，而无需实时环境交互。最近的研究表明，将离线 RL 架构化为序列建模任务是可行的，其中唯一的目标是使用 Transformer 架构基于先前上下文预测动作。然而，这种单任务学习方法的局限性在于可能削弱 Transformer 模型的注意力机制，这理想情况下应该在输入上下文中为最佳预测分配不同的注意力权重。为了解决这个问题，我们将离线 RL 重新构造为多目标优化问题，其中预测扩展到状态和回报。我们还强调了用于序列建模的轨迹表示中可能存在的一个缺陷，这可能在建模状态和回报分布时产生不准确性。这是由于操作分布的不平滑性造成的。",
    "tldr": "这项研究将离线强化学习问题视为多目标优化问题，通过扩展预测到状态和回报，解决了传统单任务学习方法中 Transformer 模型注意力机制的局限性，并指出了轨迹表示中可能存在的缺陷。",
    "en_tdlr": "This research reformulates offline reinforcement learning as a multi-objective optimization problem, extending the prediction to states and returns. It addresses the limitation of the attention mechanism in the transformer model used in traditional single-task learning and highlights a potential flaw in the trajectory representation."
}