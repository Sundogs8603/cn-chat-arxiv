{
    "title": "Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])",
    "abstract": "Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved per",
    "link": "http://arxiv.org/abs/2308.04712",
    "context": "Title: Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])\nAbstract: Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved per",
    "path": "papers/23/08/2308.04712.json",
    "total_tokens": 953,
    "translated_title": "通过预训练语言模型探测和多层对比学习进行槽位归纳",
    "translated_abstract": "最近在面向任务的对话系统（如意图识别和槽位填充）的自然语言理解中，需要大量的标注数据才能达到竞争性的性能。在现实中，标记的时间级别（槽位标签）耗时且难以获取。在本文中，我们研究了槽位归纳(SI)任务，其目标是在没有显式了解的情况下诱导槽位边界。我们提出了利用无监督预训练语言模型(PLM)探测和对比学习机制来利用(1)从PLM中提取的无监督语义知识，和(2)从TOD中可用的额外句子级意图标签信号。我们的方法在槽位归纳任务中证明了其有效性，并能够弥补与基于令牌级监督模型在两个NLU基准数据集上的差距。当推广到新出现的意图时，我们的SI目标也提供了增强的槽位标签表示，从而提高了性能。",
    "tldr": "本研究通过利用预训练语言模型的探测和对比学习机制，在槽位归纳任务中，成功地诱导了槽位边界，并在两个NLU基准数据集上表现出与令牌级监督模型相当的性能，同时也能提供增强的槽位标签表示。",
    "en_tdlr": "This study successfully induces slot boundaries in the Slot Induction task through the use of language model probing and contrastive learning mechanism, achieving performance comparable to token-level supervised models on two NLU benchmark datasets and providing enhanced slot label representations."
}