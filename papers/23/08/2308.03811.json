{
    "title": "Non-Convex Bilevel Optimization with Time-Varying Objective Functions. (arXiv:2308.03811v1 [math.OC])",
    "abstract": "Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for ",
    "link": "http://arxiv.org/abs/2308.03811",
    "context": "Title: Non-Convex Bilevel Optimization with Time-Varying Objective Functions. (arXiv:2308.03811v1 [math.OC])\nAbstract: Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for ",
    "path": "papers/23/08/2308.03811.json",
    "total_tokens": 918,
    "translated_title": "非凸双层优化和时变目标函数",
    "translated_abstract": "双层优化已经成为机器学习问题中的一个强大工具，然而，当前的非凸双层优化考虑的是离线数据集和静态函数，在新兴的在线应用中，这可能无法很好地处理流式数据和时变函数。在这项工作中，我们研究了在线双层优化（OBO），其中函数可以时变，并且代理不断根据在线流数据更新决策。为了处理OBO中的函数变化和真实超梯度不可用的问题，我们提出了一种基于窗口平均的单循环在线双层优化器（SOBOW），它根据内层窗口平均的最近超梯度估计值来更新外层的决策。与现有算法相比，SOBOW计算效率高，并且不需要知道先前的函数。为了处理单循环更新和函数变化带来的独特技术困难，我们还提出了一种深度学习方法，重点是通过增加稀疏性以及结构和全局信息来减少预测误差。",
    "tldr": "本论文研究了在线双层优化问题，提出了一种基于窗口平均的单循环在线双层优化器（SOBOW），它能在处理函数变化和真实超梯度不可用的情况下高效更新决策。",
    "en_tdlr": "This paper studies the problem of online bilevel optimization, proposing a single-loop online bilevel optimizer called SOBOW, which efficiently updates decisions in the presence of function variations and unavailable true hypergradients."
}