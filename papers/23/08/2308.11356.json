{
    "title": "Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])",
    "abstract": "Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. ",
    "link": "http://arxiv.org/abs/2308.11356",
    "context": "Title: Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])\nAbstract: Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. ",
    "path": "papers/23/08/2308.11356.json",
    "total_tokens": 901,
    "translated_title": "语义RGB-D图像合成",
    "translated_abstract": "在RGB-D语义图像分割的训练集中收集各种各样的图像并不总是可能的。尤其是当机器人需要在隐私敏感区域如家庭中操作时，收集受限于一小部分地点。因此，标注图像在外观上缺乏多样性，RGB-D语义图像分割的方法往往对训练数据过度拟合。因此本文提出了语义RGB-D图像合成来解决这个问题。这需要为给定的语义标签图合成一个逼真的RGB-D图像。然而，目前的方法是单模态的，不能处理多模态数据。事实上，我们展示了将单模态方法扩展到多模态数据时效果不佳。因此，在本文中，我们提出了一个用于多模态数据的生成器，将语义布局中与模态无关的信息与生成RGB和深度图像所需的与模态相关的信息分离开来。",
    "tldr": "本文提出了语义RGB-D图像合成方法，用于解决RGB-D语义图像分割训练集缺乏多样性的问题。通过生成逼真的RGB-D图像来实现给定语义标签图的合成。本文的主要创新是提出了一种生成器，可以处理多模态数据，将与模态无关的信息与与模态相关的信息分离开来。",
    "en_tdlr": "This paper proposes a semantic RGB-D image synthesis method to address the lack of diversity in RGB-D semantic image segmentation training sets. It generates realistic RGB-D images for a given semantic label map. The main contribution of this paper is the introduction of a generator that can handle multi-modal data and separate modal-independent information from modal-dependent information."
}