{
    "title": "Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error. (arXiv:2308.05292v1 [cs.LG])",
    "abstract": "This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LS",
    "link": "http://arxiv.org/abs/2308.05292",
    "context": "Title: Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error. (arXiv:2308.05292v1 [cs.LG])\nAbstract: This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LS",
    "path": "papers/23/08/2308.05292.json",
    "total_tokens": 949,
    "translated_title": "具有随机梯度噪声独立学习错误的拜占庭鲁棒分散随机优化",
    "translated_abstract": "本文研究了在分散网络中拜占庭鲁棒的随机优化，其中每个代理定期与其邻居进行通信以交换本地模型，然后通过随机梯度下降（SGD）更新自己的本地模型。这种方法的性能受到在优化过程中进行敌对行为的未知数量的拜占庭代理的影响。据我们所知，目前尚没有现有的工作能够同时实现线性收敛速度和小的学习错误。我们观察到学习错误在很大程度上取决于内在的随机梯度噪声。受到这一观察的启发，我们引入了两种方差减少方法，随机平均梯度算法（SAGA）和无循环随机方差减少梯度（LSVRG），用于拜占庭鲁棒的分散随机优化，以消除随机梯度噪声的负面效应。最终得到的两种方法是BRAVO-SAGA和BRAVO-LS。",
    "tldr": "本文研究了在分散网络中拜占庭鲁棒的随机优化问题，提出了两种方差减少方法来消除随机梯度噪声的负面效应，并取得了具有线性收敛速度和小的学习错误的结果。"
}