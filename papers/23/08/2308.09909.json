{
    "title": "Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])",
    "abstract": "In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the \"revisitation\" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.",
    "link": "http://arxiv.org/abs/2308.09909",
    "context": "Title: Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])\nAbstract: In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the \"revisitation\" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.",
    "path": "papers/23/08/2308.09909.json",
    "total_tokens": 845,
    "translated_title": "不再重复探索的多智能体强化学习方法",
    "translated_abstract": "在多智能体强化学习领域中，内在动机已经成为探索的重要工具。尽管计算许多内在奖励依赖于使用神经网络逼近器估计变分后验，但由于这些神经统计逼近器的表达能力有限，出现了一个明显的挑战。我们将这个挑战定为\"回访\"问题，即智能体反复探索任务空间中的有限区域。为了解决这个问题，我们提出了一种动态奖励缩放方法。这种方法旨在稳定先前探索区域内的内在奖励的明显波动，并促进更广泛的探索，有效地抑制回访现象。我们的实验结果强调了我们方法的有效性，在像Google Research Football和StarCraft II微操作任务这样的复杂环境中表现出了更好的性能，特别是在稀疏奖励设置下。",
    "tldr": "该论文提出了一种动态奖励缩放方法，以解决多智能体强化学习中的回访问题。实验结果表明，在复杂环境中，特别是在稀疏奖励设置下，该方法能够提高性能。",
    "en_tdlr": "This paper proposes a dynamic reward scaling approach to address the revisitation issue in multi-agent reinforcement learning. Experimental results demonstrate improved performance, particularly in demanding environments and sparse reward settings."
}