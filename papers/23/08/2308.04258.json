{
    "title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets. (arXiv:2308.04258v1 [eess.AS])",
    "abstract": "This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
    "link": "http://arxiv.org/abs/2308.04258",
    "context": "Title: Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets. (arXiv:2308.04258v1 [eess.AS])\nAbstract: This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
    "path": "papers/23/08/2308.04258.json",
    "total_tokens": 978,
    "translated_title": "使用PaSST和大规模音频字幕数据集推进基于自然语言的音频检索",
    "translated_abstract": "本研究提出了一种基于预训练文本和频谱图转换器的文本到音频检索系统。我们的方法将录音和文字描述映射到共享的音频字幕空间中，使得不同模态的相关示例靠近。通过系统分析，我们研究了系统的每个组件对检索性能的影响。结果表明，我们确认了两个关键组件在提高性能方面起到了至关重要的作用：基于自注意力的音频编码器用于音频嵌入和利用额外的人工生成和合成数据集进行预训练。我们进一步尝试通过添加可用关键字来增加ClothoV2字幕的多样性，但这只带来了较小的改进。我们的系统在2023年DCASE挑战中名列第一，并且在ClothoV2基准测试中的平均准确率（mAP@10）较当前最先进方法提升了5.6个百分点。",
    "tldr": "本研究提出了一个基于预训练转换器的文本到音频检索系统，通过在共享空间中映射音频和文字描述来提高检索性能，其中自注意力的音频编码器和额外的人工生成和合成数据集是关键组件，取得了在DCASE挑战中的第一名，并在ClothoV2基准测试中超过当前最先进方法5.6个百分点。",
    "en_tdlr": "This study presents a text-to-audio retrieval system based on pre-trained transformers, which improves retrieval performance by mapping audio and textual descriptions in a shared space. The self-attention-based audio encoder and the utilization of additional human-generated and synthetic data sets are identified as crucial components, achieving first place in the DCASE Challenge and outperforming the current state of the art by 5.6 percentage points in the ClothoV2 benchmark."
}