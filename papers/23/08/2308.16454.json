{
    "title": "Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])",
    "abstract": "This paper addresses the tradeoff between standard accuracy on clean examples and robustness against adversarial examples in deep neural networks (DNNs). Although adversarial training (AT) improves robustness, it degrades the standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we propose a novel AT method called ARREST, which comprises three components: (i) adversarial finetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples by initializing its parameters with a DNN that is standardly pretrained on clean examples. RGKD and NR respectively entail a regularization term and an algorithm to preserve latent representations of clean examples during AFT. RGKD penalizes the distance between the representations of the standardly pretrained and AFT DNNs. NR switches input adversarial examples to nonadversarial ones when the representation changes significantly during AFT. By combining t",
    "link": "http://arxiv.org/abs/2308.16454",
    "context": "Title: Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])\nAbstract: This paper addresses the tradeoff between standard accuracy on clean examples and robustness against adversarial examples in deep neural networks (DNNs). Although adversarial training (AT) improves robustness, it degrades the standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we propose a novel AT method called ARREST, which comprises three components: (i) adversarial finetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples by initializing its parameters with a DNN that is standardly pretrained on clean examples. RGKD and NR respectively entail a regularization term and an algorithm to preserve latent representations of clean examples during AFT. RGKD penalizes the distance between the representations of the standardly pretrained and AFT DNNs. NR switches input adversarial examples to nonadversarial ones when the representation changes significantly during AFT. By combining t",
    "path": "papers/23/08/2308.16454.json",
    "total_tokens": 1015,
    "translated_title": "通过隐含表示约束的对抗微调来减少准确性与鲁棒性的权衡",
    "translated_abstract": "本文针对深度神经网络中干净样本的标准准确性和对抗样本的鲁棒性之间的权衡问题提出了一个解决方案。尽管对抗训练可以提高网络的鲁棒性，但会降低其标准准确性，从而产生权衡问题。为了减轻这种权衡，我们提出了一种新的对抗训练方法ARREST，包括三个组成部分：（i）对抗微调（AFT），（ii）基于表示的知识蒸馏（RGKD），和（iii）嘈杂重播（NR）。AFT通过将参数初始化为在干净样本上标准预训练的DNN，对DNN进行对抗样本的训练。RGKD和NR分别利用正则化项和算法在AFT期间保留干净样本的隐含表示。RGKD惩罚标准预训练和AFT DNN之间的表示距离。当AFT期间表示发生显著变化时，NR将输入对抗样本切换为非对抗样本。通过组合这些组件，我们实现了同时提高标准准确性和对抗鲁棒性的效果。",
    "tldr": "本文提出了一种名为ARREST的新型对抗训练方法，通过对抗微调、基于表示的知识蒸馏和嘈杂重播来减少标准准确性和对抗鲁棒性之间的权衡问题。",
    "en_tdlr": "This paper proposes a novel adversarial training method called ARREST, which mitigates the tradeoff between standard accuracy and robustness in deep neural networks. By combining adversarial finetuning, representation-guided knowledge distillation, and noisy replay, the method achieves improvements in both standard accuracy and adversarial robustness."
}