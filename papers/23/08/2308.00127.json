{
    "title": "DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])",
    "abstract": "Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di",
    "link": "http://arxiv.org/abs/2308.00127",
    "context": "Title: DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])\nAbstract: Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di",
    "path": "papers/23/08/2308.00127.json",
    "total_tokens": 894,
    "translated_title": "DiviML: 一种用于在异构平台上映射神经网络的基于模块的启发式方法",
    "translated_abstract": "数据中心越来越多地采用异构架构，并开始包括用于网络、视频处理和深度学习的专用硬件。为了利用现代数据中心的异构计算能力，我们开发了一种在编译器级别将深度神经网络(DNNs)分区映射到多个互联硬件设备的方法。我们提出了一种用于异构DNN编译的通用框架，提供自动分区和设备映射。我们的调度器集成了一个精确求解器，通过混合整数线性规划(MILP)的形式，并使用基于模块性的启发式方法实现可扩展性。此外，我们提出了一个理论下界公式来评估启发式解的质量。我们在一个由CPU和两个设备组成的异构系统上评估了我们的调度器，优化传统的DNNs和随机连接的神经网络，考虑到延迟和吞吐量的约束。",
    "tldr": "DiviML是一种基于模块的启发式方法，用于在异构平台上将神经网络映射，通过自动分区和设备映射，实现了编译器级别的分布式神经网络编译，具有较好的可扩展性和质量评估能力。",
    "en_tdlr": "DiviML is a module-based heuristic method for mapping neural networks onto heterogeneous platforms. It achieves compiler-level partitioning of deep neural networks, providing automatic partitioning and device mapping, with good scalability and quality assessment capability."
}