{
    "title": "Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])",
    "abstract": "This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho",
    "link": "http://arxiv.org/abs/2308.00129",
    "context": "Title: Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])\nAbstract: This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho",
    "path": "papers/23/08/2308.00129.json",
    "total_tokens": 865,
    "translated_title": "语音表示学习：使用单视图、多视图和多任务方法学习双向编码器",
    "translated_abstract": "本文集中研究了针对时间或空间序列数据的表示学习，旨在通过使用学习到的表示改进下游序列预测任务。有监督学习一直是训练深度神经网络学习良好顺序表示的主要方法。然而，扩展有监督学习的一个限制因素是缺乏足够的注释数据。受到这一挑战的启发，自然而然地探索能够利用大量无标签和弱标签数据以及附加数据模态的表示学习方法。本文描述了对语音数据的广泛研究。与大多数关注单一学习设置的其他作品不同，本文研究了多种设置：带辅助损失的有监督学习、无监督学习、半监督学习和多视图学习。除了不同的学习问题，本文还探讨了多种表示学习方法。",
    "tldr": "本文研究了语音数据的表示学习，主要关注多种设置和多种方法，旨在通过利用大量无标签和弱标签数据以及附加数据模态，改进下游序列预测任务。",
    "en_tdlr": "This thesis investigates representation learning for speech data, focusing on multiple settings and approaches to improve downstream sequence prediction tasks by utilizing large amounts of unlabeled and weakly labeled data, as well as an additional data modality."
}