{
    "title": "Universal Graph Continual Learning. (arXiv:2308.13982v1 [cs.LG])",
    "abstract": "We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.",
    "link": "http://arxiv.org/abs/2308.13982",
    "context": "Title: Universal Graph Continual Learning. (arXiv:2308.13982v1 [cs.LG])\nAbstract: We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.",
    "path": "papers/23/08/2308.13982.json",
    "total_tokens": 771,
    "translated_title": "通用图形连续学习",
    "translated_abstract": "我们解决了图形学习中的灾难性遗忘问题，这是因为传入的数据从一个图形分布转变为另一个图形分布。以往的研究主要解决了图形连续学习的一种情况，比如增量节点分类，而我们专注于一种通用方法，其中每个任务中的数据点可以是一个节点或一个图形，并且任务从节点到图形分类不同。我们提出了一种新颖的方法，使得图形神经网络在这种通用设置中表现出色。我们的方法通过维持图形间的局部和全局结构一致性来保持对过去任务的知识，并在真实世界的图形数据集中将其与各种连续学习基线进行对比，在平均性能和遗忘性上取得了显着的改善。",
    "tldr": "本研究解决了图形学习中的灾难性遗忘问题，通过维持图形间的结构一致性和一种新的方法，实现了通用图形连续学习，并在真实世界的图形数据集上取得了显著的改进。"
}