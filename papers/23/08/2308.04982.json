{
    "title": "Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])",
    "abstract": "With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillatio",
    "link": "http://arxiv.org/abs/2308.04982",
    "context": "Title: Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])\nAbstract: With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillatio",
    "path": "papers/23/08/2308.04982.json",
    "total_tokens": 883,
    "translated_title": "探索多语言文本数据提炼",
    "translated_abstract": "随着深度学习的兴起，大规模数据集和复杂模型已经成为常见的需求，需要大量的计算资源。为了解决这个问题，数据提炼技术应运而生，能够用更低的内存和时间要求快速训练模型。然而，在文本数据集上进行数据提炼并没有得到很好的研究，因为其离散性带来了挑战。此外，现有的数据集提炼方法通常难以泛化到新的架构上。在这篇论文中，我们提出了几种基于语言模型的学习方法，用于多语言文本分类数据集的数据提炼。我们进行了实验，分析了其在分类强度和跨架构泛化方面的性能。此外，我们还研究了这些方法生成的数据摘要的语言特定公平性。我们的方法建立在现有技术的基础上，增强了文本数据提炼中的跨架构泛化能力。",
    "tldr": "本论文提出了基于语言模型的学习方法，用于多语言文本分类数据集的数据提炼。通过实验分析，我们发现这些方法在分类强度和跨架构泛化方面的性能良好，并考虑了数据摘要的语言特定公平性。",
    "en_tdlr": "This paper proposes language-model-based learning methods for data distillation on multilingual text classification datasets. Experimental analysis shows that these methods perform well in terms of classification strength and cross-architecture generalization, while considering the language-specific fairness of the generated data summaries."
}