{
    "title": "Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])",
    "abstract": "Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2\\% by conditioning on intent, 10.8\\% by conditi",
    "link": "http://arxiv.org/abs/2308.06165",
    "context": "Title: Task Conditioned BERT for Joint Intent Detection and Slot-filling. (arXiv:2308.06165v1 [cs.CL])\nAbstract: Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2\\% by conditioning on intent, 10.8\\% by conditi",
    "path": "papers/23/08/2308.06165.json",
    "total_tokens": 867,
    "translated_title": "任务条件下的BERT用于联合意图检测和槽位填充",
    "translated_abstract": "对话系统需要解决用户意图的不可预测性以跟踪对话状态，并且需要处理槽位的异质性以理解用户的偏好。本文研究的假设是将这些挑战作为一个统一的模型来解决，将允许在不同的任务之间传递参数支持数据。提出的基于Transformer编码器的模型，通过丰富的输入来将模型条件于目标推理。通过对相同语料库上的多个目标推理（即意图和多个槽位类型）对Transformer编码器进行条件控制，允许学习到比单任务模型更丰富的语言交互。实验结果表明，将模型条件于越来越多的对话推理任务可以改善结果：在MultiWOZ数据集上，通过条件控制意图，联合意图和槽位检测的性能提高了3.2%，通过条件控制多个槽位类型，提高了10.8%。",
    "tldr": "本文研究了通过一个统一的模型来解决对话系统中意图跟踪和槽位理解的挑战，并通过在同一语料库上进行多个目标推理的条件控制，提高了模型的性能。"
}