{
    "title": "G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima. (arXiv:2308.03236v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based ",
    "link": "http://arxiv.org/abs/2308.03236",
    "context": "Title: G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima. (arXiv:2308.03236v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based ",
    "path": "papers/23/08/2308.03236.json",
    "total_tokens": 970,
    "translated_title": "G-Mix: 一种通用的混合学习框架，用于平面极小值",
    "translated_abstract": "深度神经网络(DNNs)在各种复杂任务中取得了良好的结果。然而，在有限的训练数据可用时，当前的DNNs在过参数化方面面临挑战。为了增强DNNs的泛化能力，Mixup技术变得越来越受欢迎。然而，它仍然产生了次优的结果。受到成功的锐度感知极小化(SAM)方法的启发，该方法建立了训练损失平面的锐度与模型泛化之间的联系，我们提出了一种新的学习框架称为通用混合(G-Mix)，它结合了Mixup和SAM的优点来训练DNN模型。提供的理论分析证明了开发的G-Mix框架如何增强泛化能力。此外，为了进一步优化DNN性能与G-Mix框架，我们介绍了两种新算法：二进制G-Mix和分解G-Mix。这些算法根据训练数据将其分成两个子集。",
    "tldr": "G-Mix是一种通用的混合学习框架，通过结合Mixup和锐度感知极小化（SAM）方法的优点，来增强深度神经网络（DNN）的泛化能力。本文还介绍了两种新的算法：二进制G-Mix和分解G-Mix，用于进一步优化DNN性能。",
    "en_tdlr": "G-Mix is a generalized mixup learning framework that enhances the generalization capability of deep neural networks (DNNs) by combining the strengths of Mixup and Sharpness-Aware Minimization (SAM) approaches. This paper also introduces two novel algorithms, Binary G-Mix and Decomposed G-Mix, to further optimize DNN performance."
}