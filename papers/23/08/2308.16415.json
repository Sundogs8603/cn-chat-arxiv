{
    "title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer. (arXiv:2308.16415v1 [cs.CL])",
    "abstract": "Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods.",
    "link": "http://arxiv.org/abs/2308.16415",
    "context": "Title: Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer. (arXiv:2308.16415v1 [cs.CL])\nAbstract: Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods.",
    "path": "papers/23/08/2308.16415.json",
    "total_tokens": 927,
    "translated_title": "从非流式到流式ASR编码器的知识蒸馏，使用辅助的非流式层",
    "translated_abstract": "流式自动语音识别（ASR）模型由于无法访问未来的上下文，导致性能比非流式模型差。为了提高流式ASR的性能，已经研究了从非流式到流式模型的知识蒸馏，主要关注输出标记概率的对齐。本文提出了一种从教师编码器到学生编码器的逐层知识蒸馏方法。为了确保使用相同的上下文进行特征提取，我们在学生模型中插入辅助的非流式分支，并从非流式教师层向非流式辅助层进行知识蒸馏。我们设计了一种特殊的蒸馏损失，利用自回归预测编码（APC）机制，鼓励流式模型预测看不见的未来上下文。实验结果表明，与之前的标记概率蒸馏方法相比，所提出的方法可以显著降低词错误率。",
    "tldr": "本文提出了一种从非流式到流式ASR编码器的知识蒸馏方法，通过逐层蒸馏和引入辅助的非流式层，以及特定的蒸馏损失函数设计，显著降低了流式ASR的词错误率。"
}