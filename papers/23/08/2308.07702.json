{
    "title": "Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])",
    "abstract": "Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, a",
    "link": "http://arxiv.org/abs/2308.07702",
    "context": "Title: Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])\nAbstract: Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, a",
    "path": "papers/23/08/2308.07702.json",
    "total_tokens": 877,
    "translated_title": "通过角色扮演提示提高零-shot推理能力",
    "translated_abstract": "现代大型语言模型（LLM），如ChatGPT，展示了出色的角色扮演能力，使其能够扮演不仅是人类角色，还包括像Linux终端这样的非人角色。这种多功能性使它们能够在不同的上下文中模拟复杂的人类交互和行为，并仿真特定的对象或系统。尽管这些能力增强了用户参与度并引入了新的交互模式，但角色扮演对LLM的推理能力的影响仍有待深入探究。在本研究中，我们引入了一种策略性设计的角色扮演提示方法，并在十二个不同的推理基准测试中评估其在零-shot设置下的性能，涵盖了算术、常识推理、符号推理等多个领域。利用ChatGPT和Llama 2等模型，我们的实证结果表明，角色扮演提示在大多数数据集上始终优于标准的零-shot方法。值得注意的是，一个",
    "tldr": "通过角色扮演提示，研究评估了现代大型语言模型在零-shot推理场景下的表现，并发现角色扮演提示在多个推理基准测试中都超越了标准的零-shot方法。",
    "en_tdlr": "Better zero-shot reasoning of modern large language models was achieved through role-play prompting, which consistently outperformed the standard zero-shot approach in various reasoning benchmarks."
}