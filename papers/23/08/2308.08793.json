{
    "title": "Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition. (arXiv:2308.08793v1 [cs.CL])",
    "abstract": "Incremental Named Entity Recognition (INER) involves the sequential learning of new entity types without accessing the training data of previously learned types. However, INER faces the challenge of catastrophic forgetting specific for incremental learning, further aggravated by background shift (i.e., old and future entity types are labeled as the non-entity type in the current task). To address these challenges, we propose a method called task Relation Distillation and Prototypical pseudo label (RDP) for INER. Specifically, to tackle catastrophic forgetting, we introduce a task relation distillation scheme that serves two purposes: 1) ensuring inter-task semantic consistency across different incremental learning tasks by minimizing inter-task relation distillation loss, and 2) enhancing the model's prediction confidence by minimizing intra-task self-entropy loss. Simultaneously, to mitigate background shift, we develop a prototypical pseudo label strategy that distinguishes old entit",
    "link": "http://arxiv.org/abs/2308.08793",
    "context": "Title: Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition. (arXiv:2308.08793v1 [cs.CL])\nAbstract: Incremental Named Entity Recognition (INER) involves the sequential learning of new entity types without accessing the training data of previously learned types. However, INER faces the challenge of catastrophic forgetting specific for incremental learning, further aggravated by background shift (i.e., old and future entity types are labeled as the non-entity type in the current task). To address these challenges, we propose a method called task Relation Distillation and Prototypical pseudo label (RDP) for INER. Specifically, to tackle catastrophic forgetting, we introduce a task relation distillation scheme that serves two purposes: 1) ensuring inter-task semantic consistency across different incremental learning tasks by minimizing inter-task relation distillation loss, and 2) enhancing the model's prediction confidence by minimizing intra-task self-entropy loss. Simultaneously, to mitigate background shift, we develop a prototypical pseudo label strategy that distinguishes old entit",
    "path": "papers/23/08/2308.08793.json",
    "total_tokens": 1032,
    "translated_title": "增量式命名实体识别的任务关系蒸馏和原型伪标签",
    "translated_abstract": "增量命名实体识别（Incremental Named Entity Recognition, INER）涉及在不访问先前学习类型的训练数据的情况下，对新实体类型进行顺序学习。然而，INER面临着增量学习中的灾难性遗忘挑战，并进一步受到背景转换的影响（即，旧和未来的实体类型在当前任务中被标记为非实体类型）。为了解决这些挑战，我们提出了一种称为任务关系蒸馏和原型伪标签（RDP）的方法，用于INER。具体而言，为了解决灾难性遗忘问题，我们引入了任务关系蒸馏方案，该方案具有两个目的：1）通过最小化任务间关系蒸馏损失来确保不同增量学习任务之间的任务间语义一致性；2）通过最小化任务内自熵损失来增强模型的预测置信度。同时，为了减轻背景转换，我们开发了一种原型伪标签策略，可以区分旧实体类型和当前任务中的非实体类型。",
    "tldr": "本论文提出了一种名为任务关系蒸馏和原型伪标签的方法，用于解决增量命名实体识别中的灾难性遗忘和背景转换问题。任务关系蒸馏通过最小化任务间关系蒸馏损失确保任务间语义一致性，同时通过最小化任务内自熵损失增强模型的预测置信度。原型伪标签策略能够区分旧实体类型和当前任务中的非实体类型。",
    "en_tdlr": "This paper proposes a method called task Relation Distillation and Prototypical pseudo label (RDP) for addressing the challenges of catastrophic forgetting and background shift in Incremental Named Entity Recognition (INER). The task relation distillation ensures inter-task semantic consistency while the prototypical pseudo label strategy mitigates the impact of background shift."
}