{
    "title": "CLIP Multi-modal Hashing: A new baseline CLIPMH. (arXiv:2308.11797v1 [cs.CV])",
    "abstract": "The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks ",
    "link": "http://arxiv.org/abs/2308.11797",
    "context": "Title: CLIP Multi-modal Hashing: A new baseline CLIPMH. (arXiv:2308.11797v1 [cs.CV])\nAbstract: The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks ",
    "path": "papers/23/08/2308.11797.json",
    "total_tokens": 922,
    "translated_title": "CLIP多模哈希：一种新的基准CLIPMH",
    "translated_abstract": "多模哈希方法被广泛应用于多媒体检索中，可以将多源数据融合生成二进制哈希码。然而，当前的多模方法存在检索精度低的问题，原因在于各个主干网络的特征表达能力有限，并且未经过大规模无监督多模数据的联合预训练。为了解决这个问题，我们提出了一种新的基准CLIP多模哈希（CLIPMH）方法。它利用CLIP模型提取文本和图像特征，然后融合生成哈希码。CLIP改善了每个模态特征的表达能力，从而极大地提高了多模哈希方法的检索性能。与最先进的无监督和有监督多模哈希方法进行比较，实验证明，所提出的CLIPMH可以显著提高性能（最大增加8.38%）。CLIP还在文本和视觉主干网络方面具有很大优势。",
    "tldr": "CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks.",
    "en_tdlr": "CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks."
}