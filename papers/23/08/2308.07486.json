{
    "title": "O-1: Self-training with Oracle and 1-best Hypothesis. (arXiv:2308.07486v1 [cs.LG])",
    "abstract": "We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\\% relative compared to EMBR which bridges the gap by 43\\% relative. O-1 achieves 13\\% to 25\\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.",
    "link": "http://arxiv.org/abs/2308.07486",
    "context": "Title: O-1: Self-training with Oracle and 1-best Hypothesis. (arXiv:2308.07486v1 [cs.LG])\nAbstract: We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\\% relative compared to EMBR which bridges the gap by 43\\% relative. O-1 achieves 13\\% to 25\\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.",
    "path": "papers/23/08/2308.07486.json",
    "total_tokens": 994,
    "translated_title": "O-1: 自动标注与1-best假设的自我训练",
    "translated_abstract": "我们引入了O-1，这是一个新的自我训练目标，旨在减少训练偏差，统一语音识别的训练和评估指标。O-1是期望最小贝叶斯风险（EMBR）的一种更快速的变体，它增强了神谕假设，并且可以适应有监督和无监督数据。我们通过对公开可用的SpeechStew数据集和大规模内部数据集进行识别效果的实证，证明了我们方法的有效性。在Speechstew上，相对于将实际性能与神谕性能之间的差距缩小43%的EMBR，O-1目标通过80%的相对缩小了这一差距。在SpeechStew的各个数据集上，O-1相对于EMBR实现了13%到25%的相对改进，并且相对于在内部数据集上使用EMBR训练的神谕词错误率相对缩小了12%的差距。总的来说，O-1相对于EMBR在词错误率上实现了9%的相对改进，这表明了该目标在大规模数据集上的可扩展性。",
    "tldr": "提出了一种新的自我训练目标O-1，通过增强神谕假设来减少训练偏差，并统一训练和评估指标。通过实验证明了O-1在语音识别任务中的有效性，并取得了相对于EMBR的显著改进。",
    "en_tdlr": "Introduced a new self-training objective, O-1, to reduce training bias and unify training and evaluation metrics for speech recognition. Demonstrated its effectiveness in terms of recognition on various datasets and achieved significant improvements compared to EMBR."
}