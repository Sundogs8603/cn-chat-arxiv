{
    "title": "Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation. (arXiv:2308.13116v1 [cs.CL])",
    "abstract": "Contextual language models have been trained on Classical languages, including Ancient Greek and Latin, for tasks such as lemmatization, morphological tagging, part of speech tagging, authorship attribution, and detection of scribal errors. However, high-quality sentence embedding models for these historical languages are significantly more difficult to achieve due to the lack of training data. In this work, we use a multilingual knowledge distillation approach to train BERT models to produce sentence embeddings for Ancient Greek text. The state-of-the-art sentence embedding approaches for high-resource languages use massive datasets, but our distillation approach allows our Ancient Greek models to inherit the properties of these models while using a relatively small amount of translated sentence data. We build a parallel sentence dataset using a sentence-embedding alignment method to align Ancient Greek documents with English translations, and use this dataset to train our models. We ",
    "link": "http://arxiv.org/abs/2308.13116",
    "context": "Title: Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation. (arXiv:2308.13116v1 [cs.CL])\nAbstract: Contextual language models have been trained on Classical languages, including Ancient Greek and Latin, for tasks such as lemmatization, morphological tagging, part of speech tagging, authorship attribution, and detection of scribal errors. However, high-quality sentence embedding models for these historical languages are significantly more difficult to achieve due to the lack of training data. In this work, we use a multilingual knowledge distillation approach to train BERT models to produce sentence embeddings for Ancient Greek text. The state-of-the-art sentence embedding approaches for high-resource languages use massive datasets, but our distillation approach allows our Ancient Greek models to inherit the properties of these models while using a relatively small amount of translated sentence data. We build a parallel sentence dataset using a sentence-embedding alignment method to align Ancient Greek documents with English translations, and use this dataset to train our models. We ",
    "path": "papers/23/08/2308.13116.json",
    "total_tokens": 725,
    "translated_title": "使用多语种知识蒸馏的句子嵌入模型用于古希腊语",
    "translated_abstract": "针对古希腊语等历史语言，高质量的句子嵌入模型难以实现，主要是因为缺乏训练数据。本文使用多语种知识蒸馏的方法，训练BERT模型来生成古希腊文本的句子嵌入。我们通过句子嵌入对齐方法构建了一个平行语料库，将古希腊文档与英文翻译对齐，并利用这个数据集来训练我们的模型。",
    "tldr": "本文提出了一种使用多语种知识蒸馏方法来训练古希腊文本的句子嵌入模型，克服了缺乏训练数据的困难。"
}