{
    "title": "MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])",
    "abstract": "Adapter tuning, which updates only a few parameters, has become a mainstream method for fine-tuning pretrained language models to downstream tasks. However, it often yields subpar results in few-shot learning. AdapterFusion, which assembles pretrained adapters using composition layers tailored to specific tasks, is a possible solution but significantly increases trainable parameters and deployment costs. Despite this, our preliminary study reveals that even single adapters can outperform Adapterfusion in few-shot learning, urging us to propose \\textbf{\\texttt{Merging Pretrained Adapters}} (MerA) that efficiently incorporates pretrained adapters to a single model through model fusion. Extensive experiments on two PLMs demonstrate that MerA achieves substantial improvements compared to both single adapters and AdapterFusion. To further enhance the capacity of MerA, we also introduce a simple yet effective technique, referred to as the \"\\textit{same-track}\" setting, that merges adapters f",
    "link": "http://arxiv.org/abs/2308.15982",
    "context": "Title: MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])\nAbstract: Adapter tuning, which updates only a few parameters, has become a mainstream method for fine-tuning pretrained language models to downstream tasks. However, it often yields subpar results in few-shot learning. AdapterFusion, which assembles pretrained adapters using composition layers tailored to specific tasks, is a possible solution but significantly increases trainable parameters and deployment costs. Despite this, our preliminary study reveals that even single adapters can outperform Adapterfusion in few-shot learning, urging us to propose \\textbf{\\texttt{Merging Pretrained Adapters}} (MerA) that efficiently incorporates pretrained adapters to a single model through model fusion. Extensive experiments on two PLMs demonstrate that MerA achieves substantial improvements compared to both single adapters and AdapterFusion. To further enhance the capacity of MerA, we also introduce a simple yet effective technique, referred to as the \"\\textit{same-track}\" setting, that merges adapters f",
    "path": "papers/23/08/2308.15982.json",
    "total_tokens": 943,
    "translated_title": "MerA: 合并预训练的适配器用于少样本学习",
    "translated_abstract": "适配器调优是一种主流方法，仅更新少量参数，用于将预训练语言模型微调到下游任务。然而，它经常在少样本学习中产生次优结果。AdapterFusion是一种可能的解决方案，它使用定制的组合层将预训练的适配器组合到特定任务中，但会显著增加可训练参数和部署成本。尽管如此，我们的初步研究发现，即使是单个适配器在少样本学习中也可以胜过AdapterFusion，这促使我们提出了\\textbf{\\texttt{合并预训练的适配器}} (MerA)，通过模型融合将预训练的适配器高效地融入到单个模型中。在两个PLMs上进行的广泛实验表明，MerA相比单个适配器和AdapterFusion都取得了实质性的改进。为了进一步增强MerA的能力，我们还引入了一种简单而有效的技术，称为“\\textit{same-track}”设置，将适配器融合到同一个通道中。",
    "tldr": "本文提出了一种名为MerA的方法，通过模型融合高效地将预训练的适配器合并到单个模型中，以解决少样本学习问题。实验证明，MerA相比于单个适配器和AdapterFusion都取得了实质性的改进。",
    "en_tdlr": "This paper proposes a method called MerA, which efficiently merges pretrained adapters into a single model through model fusion to address the problem of few-shot learning. Experiments show that MerA achieves substantial improvements compared to both single adapters and AdapterFusion."
}