{
    "title": "Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models. (arXiv:2308.02022v1 [cs.CL])",
    "abstract": "While reaching for NLP systems that maximize accuracy, other important metrics of system performance are often overlooked. Prior models are easily forgotten despite their possible suitability in settings where large computing resources are unavailable or relatively more costly. In this paper, we perform a broad comparative evaluation of document-level sentiment analysis models with a focus on resource costs that are important for the feasibility of model deployment and general climate consciousness. Our experiments consider different feature extraction techniques, the effect of ensembling, task-specific deep learning modeling, and domain-independent large language models (LLMs). We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller datasets, the differences in accuracy shrink while the difference in resource consumption gro",
    "link": "http://arxiv.org/abs/2308.02022",
    "context": "Title: Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models. (arXiv:2308.02022v1 [cs.CL])\nAbstract: While reaching for NLP systems that maximize accuracy, other important metrics of system performance are often overlooked. Prior models are easily forgotten despite their possible suitability in settings where large computing resources are unavailable or relatively more costly. In this paper, we perform a broad comparative evaluation of document-level sentiment analysis models with a focus on resource costs that are important for the feasibility of model deployment and general climate consciousness. Our experiments consider different feature extraction techniques, the effect of ensembling, task-specific deep learning modeling, and domain-independent large language models (LLMs). We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller datasets, the differences in accuracy shrink while the difference in resource consumption gro",
    "path": "papers/23/08/2308.02022.json",
    "total_tokens": 932,
    "translated_title": "高效情感分析：特征提取技术、集成和深度学习模型的资源可行性评估",
    "translated_abstract": "在追求最大化准确性的自然语言处理系统时，常常忽视其他重要的系统性能指标。先前的模型很容易被遗忘，尽管它们可能在计算资源有限或相对更昂贵的设置中适用。在本文中，我们对文档级情感分析模型进行广泛的比较评估，主要关注对于模型部署可行性和环境意识的资源成本。我们的实验考虑了不同的特征提取技术、集成效果、任务特定的深度学习建模以及领域无关的大型语言模型。我们发现，虽然经过微调的大型语言模型达到了最高的准确性，但某些替代配置在资源消耗方面提供了巨大的（最高达24,283*）节省，而准确性损失只有较小的(<1%)。此外，我们发现对于较小的数据集，准确性的差异会缩小，而资源消耗的差异会增大。",
    "tldr": "本文对于文档级情感分析模型进行了综合评估，重点考虑了资源成本和环境意识。研究发现，在资源消耗较低的配置下，准确性损失较小。这对于资源有限的环境下的模型部署具有重要意义。",
    "en_tdlr": "This paper presents a comprehensive evaluation of document-level sentiment analysis models, with a focus on resource costs and environmental consciousness. The study finds that certain configurations can achieve significant resource savings without sacrificing much accuracy, which is crucial for deployment in resource-constrained environments."
}