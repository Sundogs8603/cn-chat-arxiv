{
    "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v1 [cs.CL])",
    "abstract": "Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reaso",
    "link": "http://arxiv.org/abs/2308.01825",
    "context": "Title: Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v1 [cs.CL])\nAbstract: Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reaso",
    "path": "papers/23/08/2308.01825.json",
    "total_tokens": 885,
    "translated_title": "使用大型语言模型学习数学推理的规模关系",
    "translated_abstract": "数学推理是大型语言模型（LLMs）的一项具有挑战性的任务，然而关于LLM容量与数学推理之间的规模关系尚未充分探索。本文研究了预训练损失、监督数据量和增强数据量对监督LLM的推理性能的影响。我们发现预训练损失是模型性能的更好指标，而不是模型参数数量。我们使用不同数量的监督数据进行监督微调（SFT），并实证发现数据量与模型性能之间存在对数线性关系，而较好的模型在扩大的监督数据集上改进较小。为了在不需要人工努力的情况下增加更多的数据样本以提高模型性能，我们提出了拒绝采样微调（RFT）。RFT使用监督模型生成和收集正确的推理路径作为增强的微调数据集。我们发现，使用更多不同的推理路径作为增强样本可以提高模型的性能。",
    "tldr": "本文研究了大型语言模型在学习数学推理时的规模关系，发现预训练损失更好地预测模型性能，并提出了一种使用拒绝采样微调技术来增强数据集的方法。",
    "en_tdlr": "This paper investigates the scaling relationship of large language models in learning mathematical reasoning. It finds that pre-training loss is a better indicator of model performance and proposes using rejection sampling fine-tuning technique to augment datasets."
}