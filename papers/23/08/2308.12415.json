{
    "title": "Benchmarking Causal Study to Interpret Large Language Models for Source Code. (arXiv:2308.12415v1 [cs.SE])",
    "abstract": "One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evalu",
    "link": "http://arxiv.org/abs/2308.12415",
    "context": "Title: Benchmarking Causal Study to Interpret Large Language Models for Source Code. (arXiv:2308.12415v1 [cs.SE])\nAbstract: One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evalu",
    "path": "papers/23/08/2308.12415.json",
    "total_tokens": 855,
    "translated_title": "用于源代码的大型语言模型的因果研究基准评估",
    "translated_abstract": "软件研究人员常采用的解决代码生成的常见方法是通过对大量源代码进行大型语言模型（LLM）的训练。尽管许多研究已经表明LLM在流行的精确度指标（例如BLEU，CodeBleu）上已经得到了有效评估，但以前的研究在LLM性能的可解释性的根本组成部分——因果推断的作用方面主要被忽视了。现有的基准和数据集旨在突出预期结果与生成结果之间的差异，但没有考虑同样影响精确度指标的混淆变量（例如代码行数，提示大小）。事实上，在处理LLM的生成软件任务时，没有基准可以告诉研究人员如何量化基于软件工程的处理的因果效应以及混淆变量与模型性能的相关性。为了在评估LLM时引入统计严谨性。",
    "tldr": "这项研究提出了一种用于源代码的大型语言模型的因果研究基准评估，以解决包括因果推断在内的模型性能可解释性的问题。",
    "en_tdlr": "This study proposes a benchmarking approach for large language models in source code, addressing the interpretability of model performance including causal inference."
}