{
    "title": "Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (arXiv:2308.04373v1 [cs.LG])",
    "abstract": "The main premise of federated learning is that machine learning model updates are computed locally, in particular to preserve user data privacy, as those never leave the perimeter of their device. This mechanism supposes the general model, once aggregated, to be broadcast to collaborating and non malicious nodes. However, without proper defenses, compromised clients can easily probe the model inside their local memory in search of adversarial examples. For instance, considering image-based applications, adversarial examples consist of imperceptibly perturbed images (to the human eye) misclassified by the local model, which can be later presented to a victim node's counterpart model to replicate the attack. To mitigate such malicious probing, we introduce Pelta, a novel shielding mechanism leveraging trusted hardware. By harnessing the capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the back-propagation chain rule, otherwise typically exploited by attackers fo",
    "link": "http://arxiv.org/abs/2308.04373",
    "context": "Title: Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (arXiv:2308.04373v1 [cs.LG])\nAbstract: The main premise of federated learning is that machine learning model updates are computed locally, in particular to preserve user data privacy, as those never leave the perimeter of their device. This mechanism supposes the general model, once aggregated, to be broadcast to collaborating and non malicious nodes. However, without proper defenses, compromised clients can easily probe the model inside their local memory in search of adversarial examples. For instance, considering image-based applications, adversarial examples consist of imperceptibly perturbed images (to the human eye) misclassified by the local model, which can be later presented to a victim node's counterpart model to replicate the attack. To mitigate such malicious probing, we introduce Pelta, a novel shielding mechanism leveraging trusted hardware. By harnessing the capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the back-propagation chain rule, otherwise typically exploited by attackers fo",
    "path": "papers/23/08/2308.04373.json",
    "total_tokens": 799,
    "translated_title": "Pelta：用于缓解联邦学习中逃避攻击的变压器保护机制",
    "translated_abstract": "联邦学习的主要前提是机器学习模型的更新是在本地计算的，以保护用户数据的隐私，因为这些数据不会离开设备的范围。然而，如果没有适当的防御措施，受 compromise 的客户端可以轻易地在其本地内存中探测模型，寻找对抗样本。为了缓解这种恶意探测，我们引入了一种新颖的 Pelta 护盾机制，利用可信硬件的能力。通过利用可信执行环境 (TEE) 的能力，Pelta 遮蔽了反向传播的部分链规则，这通常是攻击者利用的要点。",
    "tldr": "Pelta是一种新颖的联邦学习机制，利用可信硬件的能力来保护模型免受恶意探测攻击。它遮蔽了反向传播的部分链规则，提供更高的安全性。"
}