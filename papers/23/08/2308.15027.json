{
    "title": "Improving Neural Ranking Models with Traditional IR Methods. (arXiv:2308.15027v1 [cs.IR])",
    "abstract": "Neural ranking methods based on large transformer models have recently gained significant attention in the information retrieval community, and have been adopted by major commercial solutions. Nevertheless, they are computationally expensive to create, and require a great deal of labeled data for specialized corpora. In this paper, we explore a low resource alternative which is a bag-of-embedding model for document retrieval and find that it is competitive with large transformer models fine tuned on information retrieval tasks. Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models on 3 datasets. Furthermore, adding TF-IDF measures improves the performance of large-scale fine tuned models on these tasks.",
    "link": "http://arxiv.org/abs/2308.15027",
    "context": "Title: Improving Neural Ranking Models with Traditional IR Methods. (arXiv:2308.15027v1 [cs.IR])\nAbstract: Neural ranking methods based on large transformer models have recently gained significant attention in the information retrieval community, and have been adopted by major commercial solutions. Nevertheless, they are computationally expensive to create, and require a great deal of labeled data for specialized corpora. In this paper, we explore a low resource alternative which is a bag-of-embedding model for document retrieval and find that it is competitive with large transformer models fine tuned on information retrieval tasks. Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models on 3 datasets. Furthermore, adding TF-IDF measures improves the performance of large-scale fine tuned models on these tasks.",
    "path": "papers/23/08/2308.15027.json",
    "total_tokens": 790,
    "translated_title": "用传统IR方法提高神经排名模型",
    "translated_abstract": "基于大型Transformer模型的神经排名方法近年来在信息检索领域引起了极大关注，并被主要商业解决方案采用。然而，它们在创建过程中计算成本高昂，并需要大量标记数据来适应特定的语料库。在本文中，我们探索了低资源替代方案，即基于嵌入模型的文档检索方法，并发现它在信息检索任务上与细调的大型Transformer模型相竞争。我们的结果表明，将传统关键字匹配方法TF-IDF与浅层嵌入模型简单结合可以以低成本追赶到复杂神经排名模型在三个数据集上的性能。此外，在这些任务上添加TF-IDF度量可以提高大规模细调模型的性能。",
    "tldr": "本文研究了一种低成本的替代方法，通过将传统的TF-IDF和浅层嵌入模型结合使用，可以与基于大型Transformer模型的神经排名模型竞争，并可以提高这些模型在大规模任务上的性能。",
    "en_tdlr": "This paper investigates a low-cost alternative that combines traditional TF-IDF and shallow embedding models to compete with neural ranking models based on large Transformer models, and improve their performance on large-scale tasks."
}