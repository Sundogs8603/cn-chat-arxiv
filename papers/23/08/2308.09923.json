{
    "title": "East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])",
    "abstract": "Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \\emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\\times$ and 2.5$\\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain ",
    "link": "http://arxiv.org/abs/2308.09923",
    "context": "Title: East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])\nAbstract: Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \\emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\\times$ and 2.5$\\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain ",
    "path": "papers/23/08/2308.09923.json",
    "total_tokens": 865,
    "translated_title": "East: 高效准确的安全Transformer推理框架",
    "translated_abstract": "Transformer已经成功应用于实际应用中，例如ChatGPT，由于其强大的优势。然而，在服务过程中，用户的输入会泄漏给模型提供商。随着人们对隐私的关注，隐私保护的Transformer推理在这类服务中需求量大。对于隐私保护的Transformer推理来说，非线性函数的安全协议至关重要，但研究得不多。因此，设计实用的非线性函数安全协议对于模型性能而言很困难但很重要。在这项工作中，我们提出了一个名为\\emph{East}的框架，以实现高效准确的安全Transformer推理。首先，我们提出了一个新的忘却分段多项式求值算法，并应用于激活函数，与之前的方法相比，它将GELU的运行时间和通信量分别减少了1.5倍和2.5倍以上。其次，我们精心设计了用于softmax和层归一化的安全协议，以忠实地保持模型的性能。",
    "tldr": "我们提出了一个名为East的框架，以实现高效准确的安全Transformer推理。我们通过设计新的忘却分段多项式求值算法来优化激活函数的运行时间和通信量。此外，我们还设计了安全协议来处理softmax和层归一化。"
}