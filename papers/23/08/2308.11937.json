{
    "title": "Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification. (arXiv:2308.11937v1 [cs.CV])",
    "abstract": "Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art",
    "link": "http://arxiv.org/abs/2308.11937",
    "context": "Title: Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification. (arXiv:2308.11937v1 [cs.CV])\nAbstract: Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art",
    "path": "papers/23/08/2308.11937.json",
    "total_tokens": 821,
    "translated_title": "学习瓶颈Transformer进行基于事件图像-体素特征融合的分类",
    "translated_abstract": "近年来，使用事件相机识别目标物体引起了越来越多的关注。现有的工作通常将事件流表示为点云、体素、图像等，并使用各种深度神经网络学习特征表示。然而，他们的最终结果可能受到以下因素的限制：单调的模态表达和网络结构的设计。为了解决上述挑战，本文提出了一个新颖的双流框架，用于事件表示、提取和融合。该框架同时模拟了两种常见的表示形式：事件图像和事件体素。通过利用Transformer和结构化图神经网络（GNN）架构，可以分别学习空间信息和三维立体信息。此外，引入了瓶颈Transformer来促进双流信息的融合。大量实验证明，我们提出的框架达到了最先进的水平。",
    "tldr": "本文提出了一个学习瓶颈Transformer的双流框架，用于事件图像和体素特征融合的分类任务，并通过实验证明了其在这一领域的优越性能。",
    "en_tdlr": "This paper proposes a dual-stream framework with bottleneck Transformer for event image-voxel feature fusion based classification, which achieves state-of-the-art performance in this field, as demonstrated by extensive experiments."
}