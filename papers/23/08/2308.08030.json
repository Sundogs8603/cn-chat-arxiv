{
    "title": "Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks. (arXiv:2308.08030v1 [stat.ML])",
    "abstract": "This paper studies the binary classification of unbounded data from ${\\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\\unicode{x2013}$ for the first time $\\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising",
    "link": "http://arxiv.org/abs/2308.08030",
    "context": "Title: Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks. (arXiv:2308.08030v1 [stat.ML])\nAbstract: This paper studies the binary classification of unbounded data from ${\\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\\unicode{x2013}$ for the first time $\\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising",
    "path": "papers/23/08/2308.08030.json",
    "total_tokens": 927,
    "translated_title": "使用深度ReLU网络对由高斯混合模型生成的数据进行分类",
    "translated_abstract": "本文研究了使用深度ReLU神经网络对由高斯混合模型生成的无界数据进行二分类。我们首次获得了对于没有对模型参数施加限制的分类任务中超出风险（超出错误分类率）的非渐近上界和收敛速率。我们得到的收敛速率不依赖于维度$d$，证明了深度ReLU网络可以克服在分类中的维度诅咒。虽然现有的分类算法的广义分析大多依赖于有界域，但我们通过利用高斯分布的解析性和快速衰减将其应用于无界域。为了便于我们的分析，我们给出了一个使用ReLU网络对一般解析函数的新近似误差界，这可能具有独立的研究价值。高斯分布可以很好地用于建模产生的数据。",
    "tldr": "本文研究了如何使用深度ReLU神经网络在没有对模型参数施加限制的情况下，对由高斯混合模型生成的无界数据进行二分类。我们首次获得了收敛速度不受维度诅咒影响的非渐近上界，并通过使用高斯分布的特性在无限域上进行了分类分析。",
    "en_tdlr": "This paper presents a study on binary classification of unbounded data generated by Gaussian Mixture Models using deep ReLU neural networks. The paper provides non-asymptotic upper bounds and convergence rates of the excess risk for classification without restrictions on model parameters, demonstrating that deep ReLU networks can overcome the curse of dimensionality. The analysis is done by leveraging the analyticity and fast decay of Gaussian distributions."
}