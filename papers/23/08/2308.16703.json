{
    "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])",
    "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of",
    "link": "http://arxiv.org/abs/2308.16703",
    "context": "Title: Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])\nAbstract: Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of",
    "path": "papers/23/08/2308.16703.json",
    "total_tokens": 910,
    "translated_title": "故障注入和安全错误攻击用于提取嵌入式神经网络模型",
    "translated_abstract": "模型提取作为一种关键的安全威胁而出现，攻击向量利用了算法和实现方面的方法。攻击者的主要目标是尽可能多地窃取受保护的受害者模型的信息，以便他可以用替代模型来模仿它，即使只有有限的访问相似的训练数据。最近，物理攻击，如故障注入，已经显示出对嵌入式模型的完整性和机密性的令人担忧的效果。我们的重点是32位微控制器上的嵌入式深度神经网络模型，这是物联网中广泛使用的硬件平台系列，以及使用标准故障注入策略-安全错误攻击（SEA）来进行具有有限训练数据访问的模型提取攻击。由于攻击强烈依赖于输入查询，我们提出了一种黑盒方法来构建一个成功的攻击集。对于一个经典的卷积神经网络，我们成功地恢复了至少90%的",
    "tldr": "本文介绍了故障注入和安全错误攻击用于提取嵌入式神经网络模型的方法，并阐述了对32位微控制器上的深度神经网络进行模型提取攻击的实验结果。",
    "en_tdlr": "This paper presents a method using fault injection and safe-error attack to extract embedded neural network models, and discusses the experimental results of model extraction attack on deep neural network models on 32-bit microcontrollers."
}