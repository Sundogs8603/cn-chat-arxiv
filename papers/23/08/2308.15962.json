{
    "title": "WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v1 [cs.RO])",
    "abstract": "Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the",
    "link": "http://arxiv.org/abs/2308.15962",
    "context": "Title: WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v1 [cs.RO])\nAbstract: Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the",
    "path": "papers/23/08/2308.15962.json",
    "total_tokens": 960,
    "translated_title": "WALL-E: 具有大型语言模型的实体机器人服务员举重",
    "translated_abstract": "让机器人能够理解语言指令并根据视觉感知做出反应一直以来都是机器人研究界的一个长期目标。实现这一目标需要在自然语言处理、计算机视觉和机器人工程方面取得前沿进展。因此，本文主要研究了将最新的大型语言模型（LLMs）与现有的视觉定位和机器人抓取系统集成以增强人机交互效果的潜力。我们以WALL-E（具有大型语言模型的实体机器人服务员举重）作为集成的示例。系统利用ChatGPT的LLM通过多轮交互式对话将用户的偏好物体总结为目标指令。然后将目标指令传递给视觉定位系统进行物体姿势和大小估计，然后机器人相应地抓取物体。我们将这个LLM增强系统部署在",
    "tldr": "本文研究了将大型语言模型与视觉定位和机器人抓取系统集成，通过使用WALL-E实现了在餐厅场景中提高人机交互准确性和效率的目标。通过实验和评估，证明了这种集成可以使WALL-E成为一位更有能力和智能的机器人服务员。",
    "en_tdlr": "This paper investigates the integration of large language models with visual grounding and robotic grasping systems, demonstrating the improvement in accuracy and efficiency of human-robot interaction in a restaurant scenario using WALL-E."
}