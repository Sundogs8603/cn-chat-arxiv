{
    "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition. (arXiv:2308.03279v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming gen",
    "link": "http://arxiv.org/abs/2308.03279",
    "context": "Title: UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition. (arXiv:2308.03279v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming gen",
    "path": "papers/23/08/2308.03279.json",
    "total_tokens": 1025,
    "translated_title": "UniversalNER：从大型语言模型中进行目标蒸馏，用于开放式命名实体识别",
    "translated_abstract": "大型语言模型（LLMs）展示了卓越的泛化能力，例如理解任意实体和关系。指导调整已被证明可以有效地将LLMs蒸馏成更高效的模型，如Alpaca和Vicuna。然而，这种学生模型在下游应用中仍然远远落后于原始LLMs。在本文中，我们探索了定向蒸馏与面向任务的指导调整，以训练能够在开放式信息提取等广泛应用类中表现出色的学生模型。通过以命名实体识别（NER）为案例研究，我们展示了如何将ChatGPT蒸馏成更小的UniversalNER模型用于开放式NER。为了评估，我们组合了迄今为止最大的NER基准，包括9个多样领域的43个数据集，如生物医学、编程、社交媒体、法律、金融。在不使用任何直接监督的情况下，UniversalNER在数万种实体类型上实现了卓越的NER准确性，超过了gen。",
    "tldr": "本文提出了一种从大型语言模型中进行目标蒸馏的方法，用于训练能够在开放式命名实体识别中表现出色的学生模型。通过使用指导调整和面向任务的方法，将ChatGPT蒸馏成更小的UniversalNER模型。实验结果表明，UniversalNER在各种领域的命名实体识别任务上取得了卓越的准确性，超过了原始的大型语言模型。",
    "en_tdlr": "This paper proposes a method for targeted distillation from large language models to train student models that excel in open named entity recognition. Using mission-focused instruction tuning, ChatGPT is distilled into smaller UniversalNER models. Evaluation results show that UniversalNER achieves remarkable accuracy in named entity recognition across diverse domains, outperforming the original large language models."
}