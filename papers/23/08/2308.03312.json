{
    "title": "Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)",
    "abstract": "Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv",
    "link": "http://arxiv.org/abs/2308.03312",
    "context": "Title: Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)\nAbstract: Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv",
    "path": "papers/23/08/2308.03312.json",
    "total_tokens": 811,
    "translated_title": "保持对称性的程序表示法用于学习代码语义",
    "translated_abstract": "大型语言模型(LLMs)在自动程序推理方面显示出潜力，这是许多安全任务的关键方面。然而，现有的用于代码的LLM架构通常从其他领域（如自然语言处理）借用，引发对其泛化能力和对未知代码的健壮性的担忧。一个关键的泛化挑战是将代码语义的知识，包括控制和数据流，纳入LLM架构中。受到利用平移对称性的卷积层的启发，我们探索了代码对称性如何增强程序分析和建模的LLM架构。我们提供了一个严格的群论框架，形式化地定义了代码对称性作为保持语义的变换，并提供了在LLM架构中精确推理对称性保持的技术。利用这个框架，我们引入了一种保持程序对称性的新型自注意力变体，并展示了其有效性。",
    "tldr": "本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。",
    "en_tdlr": "This paper introduces a novel approach to preserve symmetry in code representations, improving the effectiveness in program analysis and modeling by incorporating convolution layers."
}