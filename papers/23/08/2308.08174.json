{
    "title": "Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design. (arXiv:2308.08174v1 [cs.AR])",
    "abstract": "Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.  In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent",
    "link": "http://arxiv.org/abs/2308.08174",
    "context": "Title: Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design. (arXiv:2308.08174v1 [cs.AR])\nAbstract: Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.  In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent",
    "path": "papers/23/08/2308.08174.json",
    "total_tokens": 877,
    "translated_title": "通过架构、编译器和分区方法的协同设计加速通用图神经网络",
    "translated_abstract": "图神经网络（GNN）在各种图学习领域中显示出了显著的准确性改进，引发了相当大的研究兴趣。为了将这些准确性改进转化为实际应用，开发高性能和高效的硬件加速器对于GNN模型至关重要。然而，设计GNN加速器面临两个基本挑战：GNN模型的高带宽需求和GNN模型的多样性。先前的工作通过使用更昂贵的内存接口来实现更高的带宽来解决第一个挑战。而对于第二个挑战，现有的工作要么支持特定的GNN模型，要么具有硬件利用率低的通用设计。在这项工作中，我们同时解决了这两个挑战。首先，我们确定了一种新型的分区级运算符融合方法，利用它在内部减少GNN的高带宽需求。然后，我们引入了分区级多线程来调度并发操作。",
    "tldr": "该论文提出了一种加速通用图神经网络的方法，通过协同设计架构、编译器和分区方法来解决GNN模型的高带宽需求和多样性的挑战。",
    "en_tdlr": "This paper proposes an approach to accelerate generic graph neural networks by co-designing architecture, compiler, and partition methods to address the challenges of high bandwidth requirement and diversity of GNN models."
}