{
    "title": "Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])",
    "abstract": "Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\\mathbf{S}$elf-supervised s$\\mathbf{C}$alable deep CS method, comprising a $\\mathbf{L}$earning scheme called $\\mathbf{SCL}$ and a family of $\\mathbf{Net}$works named $\\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur",
    "link": "http://arxiv.org/abs/2308.13777",
    "context": "Title: Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])\nAbstract: Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\\mathbf{S}$elf-supervised s$\\mathbf{C}$alable deep CS method, comprising a $\\mathbf{L}$earning scheme called $\\mathbf{SCL}$ and a family of $\\mathbf{Net}$works named $\\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur",
    "path": "papers/23/08/2308.13777.json",
    "total_tokens": 935,
    "translated_title": "自监督可扩展深度压缩感知",
    "translated_abstract": "压缩感知（CS）是降低采样成本的一种有前景的工具。当前基于深度神经网络（NN）的CS方法在收集标记的测量-地面真实（GT）数据和推广到实际应用方面面临挑战。本文提出了一种新颖的自监督可扩展深度CS方法，包括一个称为SCL的学习方案和一个名为SCNet的网络系列，它不需要GT并且可以处理一旦在部分测量集上训练完毕就可以处理任意的采样比率和矩阵。我们的SCL包含一个双域损失和一个四阶段恢复策略。前者鼓励两个测量部分的交叉一致性以及采样-重构循环一致性，从而最大化数据/信息利用率。后者可以逐步利用外部测量中的常见信号先验和测试样本以及学习的NN的内部特征来提高准确性。",
    "tldr": "本文提出了一种自监督的可扩展深度压缩感知方法，不需要标记的测量-地面真实数据，并且可以处理任意的采样比率和矩阵。该方法包括一个双域损失和四个恢复阶段，通过最大化数据/信息利用率来提高准确性。",
    "en_tdlr": "This paper proposes a self-supervised scalable deep compressed sensing method that does not require labeled measurement-ground truth data and can handle arbitrary sampling ratios and matrices. The method includes a dual-domain loss and four recovery stages to improve accuracy by maximizing data/information utilization."
}