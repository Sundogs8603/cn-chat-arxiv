{
    "title": "Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])",
    "abstract": "Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context vari",
    "link": "http://arxiv.org/abs/2308.15670",
    "context": "Title: Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])\nAbstract: Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context vari",
    "path": "papers/23/08/2308.15670.json",
    "total_tokens": 980,
    "translated_title": "用于心脏超声解读的多模式基础模型",
    "translated_abstract": "多模式深度学习基础模型可以学习图像和文本之间的关系。在医学成像的背景下，将图像映射到语言概念反映了诊断图像解释的临床任务，然而当前通用的基础模型在这个背景下表现不佳，因为它们的训练文本和图像数据有限。为了解决这个挑战并考虑到心脏生理的范围，我们利用1,032,975个心脏超声视频和相应的专家解读开发了EchoCLIP，一个用于心脏超声的多模式基础模型。EchoCLIP在心脏功能评估（外部验证左室射血分数的平均绝对误差（MAE）为7.1%）和植入心脏内器件的识别方面表现出强大的零样本（未经过显式训练）性能（起搏器和人工心脏瓣膜的曲线下面积（AUC）在0.84至0.98之间）。我们还开发了一个长上下文可变模型来实现更好的模型性能。",
    "tldr": "该论文提出了一个名为EchoCLIP的多模式基础模型，用于心脏超声解读。该模型利用大量的心脏超声视频和专家解读来实现强大的零样本性能，并在心脏功能评估和植入心脏内器件识别方面表现出良好的性能。"
}