{
    "title": "Parallel Learning by Multitasking Neural Networks. (arXiv:2308.04106v1 [cond-mat.dis-nn])",
    "abstract": "A modern challenge of Artificial Intelligence is learning multiple patterns at once (i.e.parallel learning). While this can not be accomplished by standard Hebbian associative neural networks, in this paper we show how the Multitasking Hebbian Network (a variation on theme of the Hopfield model working on sparse data-sets) is naturally able to perform this complex task. We focus on systems processing in parallel a finite (up to logarithmic growth in the size of the network) amount of patterns, mirroring the low-storage level of standard associative neural networks at work with pattern recognition. For mild dilution in the patterns, the network handles them hierarchically, distributing the amplitudes of their signals as power-laws w.r.t. their information content (hierarchical regime), while, for strong dilution, all the signals pertaining to all the patterns are raised with the same strength (parallel regime). Further, confined to the low-storage setting (i.e., far from the spin glass ",
    "link": "http://arxiv.org/abs/2308.04106",
    "context": "Title: Parallel Learning by Multitasking Neural Networks. (arXiv:2308.04106v1 [cond-mat.dis-nn])\nAbstract: A modern challenge of Artificial Intelligence is learning multiple patterns at once (i.e.parallel learning). While this can not be accomplished by standard Hebbian associative neural networks, in this paper we show how the Multitasking Hebbian Network (a variation on theme of the Hopfield model working on sparse data-sets) is naturally able to perform this complex task. We focus on systems processing in parallel a finite (up to logarithmic growth in the size of the network) amount of patterns, mirroring the low-storage level of standard associative neural networks at work with pattern recognition. For mild dilution in the patterns, the network handles them hierarchically, distributing the amplitudes of their signals as power-laws w.r.t. their information content (hierarchical regime), while, for strong dilution, all the signals pertaining to all the patterns are raised with the same strength (parallel regime). Further, confined to the low-storage setting (i.e., far from the spin glass ",
    "path": "papers/23/08/2308.04106.json",
    "total_tokens": 852,
    "translated_title": "并行学习的多任务神经网络",
    "translated_abstract": "人工智能的一个现代挑战是同时学习多种模式（即并行学习）。本文中我们展示了多任务Hebbian网络（在稀疏数据集上工作的Hopfield模型变体）如何自然地执行这一复杂任务。我们关注的是并行处理有限数量（与网络大小的对数增长相对应）模式的系统，类似于标准关联神经网络在模式识别中的低存储水平。对于模式的轻度稀释，网络以层次方式处理它们，根据其信息内容以幂律分布其信号的幅度（层次制度），而对于强稀释，所有与所有模式相关的信号均具有相同的强度（并行制度）。此外，仅限于低存储设置（即远离自旋玻璃）。",
    "tldr": "本文介绍了一种名为多任务Hebbian网络的神经网络模型，能够自然地实现并行学习，并处理各种模式的信号幅度分布。这对于人工智能中学习多个模式的挑战具有重要意义。",
    "en_tdlr": "This paper introduces a neural network model called Multitasking Hebbian Network that can naturally achieve parallel learning and handle the distribution of signal amplitudes for various patterns. This has significant implications for the challenge of learning multiple patterns in Artificial Intelligence."
}