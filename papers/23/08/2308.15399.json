{
    "title": "Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?. (arXiv:2308.15399v1 [cs.CL])",
    "abstract": "Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. In contrast, top-down approaches make moral judgments grounded in a set of principles. However, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. In this study, we propose a flexible framework to steer Large Language Models (LLMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore,",
    "link": "http://arxiv.org/abs/2308.15399",
    "context": "Title: Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?. (arXiv:2308.15399v1 [cs.CL])\nAbstract: Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. In contrast, top-down approaches make moral judgments grounded in a set of principles. However, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. In this study, we propose a flexible framework to steer Large Language Models (LLMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore,",
    "path": "papers/23/08/2308.15399.json",
    "total_tokens": 891,
    "translated_title": "重新思考机器伦理 - LLM能否通过道德理论进行道德推理？",
    "translated_abstract": "进行道德判断是发展伦理人工智能系统的重要一步。目前的方法大多数以自下而上的方式实施，通过使用大量的注释数据来训练基于众包意见的模型，来判断道德问题。这些方法因潜在过度普遍化有限的注释者道德立场并且缺乏可解释性而受到批评。相反，自上而下的方法是基于一套原则进行道德判断。然而，这个方法在概念上存在问题，因为之前的语言模型无法胜任，且道德原则之间存在未解决的辩论。在本研究中，我们提出了一个灵活的框架，可以引导大型语言模型（LLMs）根据跨学科研究中建立的道德理论进行道德推理。这个自上而下的理论引导框架可以融入各种道德理论。我们的实验验证了这个提出的框架在基于道德理论的数据集上的有效性。",
    "tldr": "本研究提出了一个灵活的框架，引导大型语言模型根据跨学科研究中建立的道德理论进行道德推理，解决了现有方法面临的问题。",
    "en_tdlr": "This study proposes a flexible framework to guide Large Language Models (LLMs) in performing moral reasoning based on well-established moral theories from interdisciplinary research, addressing the limitations of current approaches."
}