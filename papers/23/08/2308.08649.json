{
    "title": "Towards Zero Memory Footprint Spiking Neural Network Training. (arXiv:2308.08649v1 [cs.NE])",
    "abstract": "Biologically-inspired Spiking Neural Networks (SNNs), processing information using discrete-time events known as spikes rather than continuous values, have garnered significant attention due to their hardware-friendly and energy-efficient characteristics. However, the training of SNNs necessitates a considerably large memory footprint, given the additional storage requirements for spikes or events, leading to a complex structure and dynamic setup. In this paper, to address memory constraint in SNN training, we introduce an innovative framework, characterized by a remarkably low memory footprint. We \\textbf{(i)} design a reversible SNN node that retains a high level of accuracy. Our design is able to achieve a $\\mathbf{58.65\\times}$ reduction in memory usage compared to the current SNN node. We \\textbf{(ii)} propose a unique algorithm to streamline the backpropagation process of our reversible SNN node. This significantly trims the backward Floating Point Operations Per Second (FLOPs), ",
    "link": "http://arxiv.org/abs/2308.08649",
    "context": "Title: Towards Zero Memory Footprint Spiking Neural Network Training. (arXiv:2308.08649v1 [cs.NE])\nAbstract: Biologically-inspired Spiking Neural Networks (SNNs), processing information using discrete-time events known as spikes rather than continuous values, have garnered significant attention due to their hardware-friendly and energy-efficient characteristics. However, the training of SNNs necessitates a considerably large memory footprint, given the additional storage requirements for spikes or events, leading to a complex structure and dynamic setup. In this paper, to address memory constraint in SNN training, we introduce an innovative framework, characterized by a remarkably low memory footprint. We \\textbf{(i)} design a reversible SNN node that retains a high level of accuracy. Our design is able to achieve a $\\mathbf{58.65\\times}$ reduction in memory usage compared to the current SNN node. We \\textbf{(ii)} propose a unique algorithm to streamline the backpropagation process of our reversible SNN node. This significantly trims the backward Floating Point Operations Per Second (FLOPs), ",
    "path": "papers/23/08/2308.08649.json",
    "total_tokens": 860,
    "translated_title": "实现零内存占用的脉冲神经网络训练",
    "translated_abstract": "受生物启发的脉冲神经网络（SNN）以离散时间事件（脉冲）而非连续值处理信息，由于其对硬件友好和高能效的特点，引起了广泛关注。然而，SNN的训练需要大量的内存占用，因为需要额外存储脉冲或事件，导致复杂的结构和动态设置。为了解决SNN训练中的内存限制，本文引入了一种创新的框架，具有异常低的内存占用。我们设计了一个可逆的SNN节点，保持高精度，并实现了与当前SNN节点相比的58.65倍的内存使用减少。我们提出了一种独特的算法来简化我们可逆SNN节点的反向传播过程。这显著减少了反向浮点数运算。",
    "tldr": "这项研究提出了一种具有极低内存占用的脉冲神经网络训练框架，并设计了可逆SNN节点和简化的反向传播算法，显著降低了内存使用和计算复杂性。"
}