{
    "title": "Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. T",
    "link": "http://arxiv.org/abs/2308.06051",
    "context": "Title: Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. T",
    "path": "papers/23/08/2308.06051.json",
    "total_tokens": 909,
    "translated_title": "面向实例自适应推理的联邦学习",
    "translated_abstract": "联邦学习是一种分布式学习范式，通过汇集本地训练来使多个客户端学习一个强大的全局模型。然而，全局模型的性能通常受到客户端之间的非独立同分布分布的影响，需要大量的努力来减轻客户端数据异质性。超越客户端数据异质性，我们注意到在复杂的现实世界数据中也可以观察到客户端内部的异质性，严重影响联邦学习的性能。在本文中，我们提出了一种新颖的联邦学习算法，即FedIns，在联邦学习框架中实现了实例自适应推理来处理客户端数据的异质性。我们不使用庞大的实例自适应模型，而是采用了一种参数高效的精细调整方法——缩放和位移深度特征（SSF），并在预训练模型上进行。具体而言，我们首先为每个客户端训练一个SSF池，在服务器端汇集这些SSF池，从而仍然保持低通信成本。",
    "tldr": "本文提出了一种面向实例自适应推理的联邦学习算法，通过使用缩放和位移深度特征（SSF）实现了处理客户端数据异质性的能力。",
    "en_tdlr": "This paper proposes a federated learning algorithm that enables instance-adaptive inference by using scale and shift deep features (SSF) to handle client data heterogeneity."
}