{
    "title": "Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])",
    "abstract": "In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o",
    "link": "http://arxiv.org/abs/2308.01421",
    "context": "Title: Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])\nAbstract: In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o",
    "path": "papers/23/08/2308.01421.json",
    "total_tokens": 869,
    "translated_title": "正则化、提前停止和梦想：一种处理泛化和过拟合的类Hopfield设置",
    "translated_abstract": "在这项工作中，我们从机器学习的角度来处理吸引子神经网络：通过对正则化损失函数进行梯度下降来寻找最优网络参数。在这个框架中，最优的神经元交互矩阵被证明是一类通过迭代应用某些取消学习协议修订的Hebbian核矩阵。值得注意的是，取消学习步骤的数量被证明与损失函数的正则化超参数和训练时间有关。因此，我们可以设计避免过拟合的策略，这些策略可以用交互矩阵的代数性质来描述，或者等价地用正则化调整和提前停止策略来描述。还研究了这些吸引子网络的泛化能力：针对随机合成数据集获得了分析结果，随后用数值实验来验证了所得到的整体情况。",
    "tldr": "这项工作提出了一种处理神经网络泛化和过拟合的新方法，通过正则化损失函数和提前停止策略来优化网络参数，并通过数值实验验证了该方法的有效性。",
    "en_tdlr": "This work presents a novel approach to address neural network generalization and overfitting, optimizing network parameters using regularized loss functions and early-stopping strategies, and validating the effectiveness of the method through numerical experiments."
}