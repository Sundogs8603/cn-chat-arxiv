{
    "title": "FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. (arXiv:2308.05170v1 [cs.AR])",
    "abstract": "Neural networks achieve state-of-the-art performance in image classification, speech recognition, scientific analysis and many more application areas. With the ever-increasing need for faster computation and lower power consumption, driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have emerged as suitable devices for deep learning inference. Due to the high computational complexity and memory footprint of neural networks, various compression techniques, such as pruning, quantization and knowledge distillation, have been proposed in literature. Pruning sparsifies a neural network, reducing the number of multiplications and memory. However, pruning often fails to capture properties of the underlying hardware, causing unstructured sparsity and load-balance inefficiency, thus bottlenecking resource improvements. We propose a hardware-centric formulation of pruning, by formulating it as a knapsack problem with resource-aware tensor structures. The primary emphasis is ",
    "link": "http://arxiv.org/abs/2308.05170",
    "context": "Title: FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. (arXiv:2308.05170v1 [cs.AR])\nAbstract: Neural networks achieve state-of-the-art performance in image classification, speech recognition, scientific analysis and many more application areas. With the ever-increasing need for faster computation and lower power consumption, driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have emerged as suitable devices for deep learning inference. Due to the high computational complexity and memory footprint of neural networks, various compression techniques, such as pruning, quantization and knowledge distillation, have been proposed in literature. Pruning sparsifies a neural network, reducing the number of multiplications and memory. However, pruning often fails to capture properties of the underlying hardware, causing unstructured sparsity and load-balance inefficiency, thus bottlenecking resource improvements. We propose a hardware-centric formulation of pruning, by formulating it as a knapsack problem with resource-aware tensor structures. The primary emphasis is ",
    "path": "papers/23/08/2308.05170.json",
    "total_tokens": 877,
    "translated_title": "FPGA资源感知的实时神经网络结构修剪",
    "translated_abstract": "神经网络在图像分类、语音识别、科学分析等众多应用领域中取得了最先进的性能。随着实时系统和物联网设备对更快的计算速度和更低的功耗需求的不断增加，FPGA已成为深度学习推断的合适设备。由于神经网络的高计算复杂性和内存占用，文献中提出了各种压缩技术，如修剪、量化和知识蒸馏。修剪稀疏化神经网络，减少了乘法和内存的数量。然而，修剪通常无法捕捉底层硬件的特性，导致非结构化稀疏和负载平衡效率低下，从而限制了资源改进。我们提出了一种以硬件为中心的修剪公式，将其形式化为具有资源感知张量结构的背包问题。",
    "tldr": "本文提出了一种FPGA资源感知的实时神经网络结构修剪方法，通过将修剪问题形式化为具有资源感知张量结构的背包问题，解决了修剪过程中非结构化稀疏和负载平衡效率低下的问题。",
    "en_tdlr": "This paper proposes a FPGA resource-aware structured pruning method for real-time neural networks by formulating the pruning problem as a knapsack problem with resource-aware tensor structures, addressing the issues of unstructured sparsity and load-balance inefficiency in the pruning process."
}