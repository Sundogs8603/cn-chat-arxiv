{
    "title": "Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])",
    "abstract": "Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.",
    "link": "http://arxiv.org/abs/2308.12964",
    "context": "Title: Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])\nAbstract: Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.",
    "path": "papers/23/08/2308.12964.json",
    "total_tokens": 814,
    "translated_title": "带有注意力调节的稠密文本到图像生成",
    "translated_abstract": "现有的文本到图像扩散模型在给定稠密标题的情况下很难合成逼真的图像，其中每个文本提示为特定图像区域提供了详细的描述。为了解决这个问题，我们提出了DenseDiffusion，一种无需训练的方法，它能够使预训练的文本到图像模型能够处理这些稠密标题，并同时提供对场景布局的控制。我们首先分析生成图像布局与预训练模型的中间注意力图之间的关系。接下来，我们开发了一种注意力调节方法，根据布局指导将对象引导到特定区域显示。在不需要额外的微调或数据集的情况下，我们在自动评估和人工评估分数方面改进了给定稠密标题的图像生成性能。此外，我们实现了与专门使用布局条件进行训练的模型相似质量的视觉结果。",
    "tldr": "提出了DenseDiffusion方法，该方法可以使预训练的文本到图像模型处理稠密标题并具有场景布局控制，无需额外微调或数据集，提升了图像生成性能。",
    "en_tdlr": "DenseDiffusion is proposed to adapt a pre-trained text-to-image model to handle dense captions while offering control over the scene layout, without requiring additional fine-tuning or datasets, improving image generation performance."
}