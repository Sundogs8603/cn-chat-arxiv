{
    "title": "Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])",
    "abstract": "After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc",
    "link": "http://arxiv.org/abs/2308.11578",
    "context": "Title: Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])\nAbstract: After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc",
    "path": "papers/23/08/2308.11578.json",
    "total_tokens": 917,
    "translated_title": "改变情绪识别建模方式:通用大型模型的出现",
    "translated_abstract": "在情绪识别或情感计算的诞生之后，由于其广泛应用，它已经成为一个越来越活跃的研究课题。在过去的几十年里，情绪识别模型逐渐从统计浅层模型迁移到基于神经网络的深度模型，可以显著提升情绪识别模型的性能，并在不同的基准测试中始终取得最佳结果。因此，近年来，深度模型一直被视为情绪识别的首选。然而，大型语言模型（LLMs）的出现，如ChatGPT，由于它们具备的零/少样本学习、上下文学习、连贯思维等能力，在情绪识别方面引起了巨大的惊讶，而这些能力在以前的深度模型中从未出现。在本文中，我们全面调查了LLMs在情绪识别方面的表现，包括上下文学习、少样本学习、准确度等各方面。",
    "tldr": "本论文综合调查了大型语言模型（LLMs）在情绪识别中表现的各个方面，包括上下文学习、少样本学习和准确度等。LLMs的出现为情绪识别建模带来了新的潜力和机会。",
    "en_tdlr": "This paper comprehensively investigates the performance of large language models (LLMs) in emotion recognition, including in-context learning, few-shot learning, and accuracy. The advent of LLMs brings new potential and opportunities for emotion recognition modeling."
}