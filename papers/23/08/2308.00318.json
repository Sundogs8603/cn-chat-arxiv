{
    "title": "Pixel to policy: DQN Encoders for within & cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])",
    "abstract": "Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi",
    "link": "http://arxiv.org/abs/2308.00318",
    "context": "Title: Pixel to policy: DQN Encoders for within & cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])\nAbstract: Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi",
    "path": "papers/23/08/2308.00318.json",
    "total_tokens": 898,
    "translated_title": "像素到策略：用于内部和跨游戏强化学习的DQN编码器",
    "translated_abstract": "强化学习可以应用于各种任务和环境。许多这些环境具有相似的共享结构，可以利用这些结构提高其他任务上的RL性能。迁移学习可以利用这种共享结构，通过学习可在不同任务和环境之间转移的策略，可以更有效地学习并在各种任务上改进性能。本研究探讨并比较了从零开始训练的RL模型和迁移学习不同方法的性能。此外，该研究还探讨了在多个游戏环境中训练的模型的性能，旨在开发通用的游戏代理以及使用DQN对预训练的编码器进行迁移学习，并在相同或不同游戏上进行训练。我们的DQN模型在平均回合奖励上达到了46.16，甚至超过了人类水平表现，仅仅使用了20,000次回合。",
    "tldr": "本文研究了在强化学习中利用共享结构的DQN编码器的性能。通过迁移学习，该模型在不同任务和环境中学习可转移的策略，实现了更高效的学习和更好的性能。在多个游戏环境中，该模型的平均回合奖励为46.16，在20,000次回合内甚至超过了人类水平表现。"
}