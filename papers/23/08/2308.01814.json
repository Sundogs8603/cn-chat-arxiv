{
    "title": "Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])",
    "abstract": "Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of \"kernel.\" We derive the corresponding \"neural tangent\" and \"maximal update\" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.",
    "link": "http://arxiv.org/abs/2308.01814",
    "context": "Title: Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])\nAbstract: Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of \"kernel.\" We derive the corresponding \"neural tangent\" and \"maximal update\" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.",
    "path": "papers/23/08/2308.01814.json",
    "total_tokens": 863,
    "translated_title": "Tensor程序IVb：无限宽度极限中的自适应优化",
    "translated_abstract": "超越随机梯度下降（SGD），当使用Adam等自适应优化器训练宽神经网络时，会出现新的现象吗？在这里，我们展示了以下结果：与SGD一样，对于包括Adam在内的一般优化器，特征学习和核行为之间存在着相同的二分法 - 尽管有一种非线性的“核”概念。我们推导了对于任何架构的相应的“神经切线”和“最大更新”极限。上述结果的两个基础性进展是：1）一种新的Tensor程序语言，NEXORT，可以表达自适应优化器如何将梯度处理为更新。2）引入bra-ket符号来极大地简化Tensor程序中的表达式和计算。该工作总结并概括了Tensor程序系列论文中的所有先前结果。",
    "tldr": "本论文研究了在无限宽度极限中使用自适应优化器训练宽神经网络时的新现象，通过推导相应的“神经切线”和“最大更新”极限，展示了特征学习和核行为之间的二分法，同时引入了简化计算的bra-ket符号。",
    "en_tdlr": "This paper investigates new phenomena in training wide neural networks using adaptive optimizers in the infinite-width limit. By deriving the corresponding limits of \"neural tangent\" and \"maximal update\", the paper demonstrates the dichotomy between feature learning and kernel behaviors, while introducing a simplified calculation notation using bra-ket symbols."
}