{
    "title": "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval. (arXiv:2308.04343v1 [cs.CV])",
    "abstract": "Most existing cross-modal retrieval methods employ two-stream encoders with different architectures for images and texts, \\textit{e.g.}, CNN for images and RNN/Transformer for texts. Such discrepancy in architectures may induce different semantic distribution spaces and limit the interactions between images and texts, and further result in inferior alignment between images and texts. To fill this research gap, inspired by recent advances of Transformers in vision tasks, we propose to unify the encoder architectures with Transformers for both modalities. Specifically, we design a cross-modal retrieval framework purely based on two-stream Transformers, dubbed \\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image Transformer, a text Transformer, and a hierarchical alignment module. With such identical architectures, the encoders could produce representations with more similar characteristics for images and texts, and make the interactions and alignments between th",
    "link": "http://arxiv.org/abs/2308.04343",
    "context": "Title: Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval. (arXiv:2308.04343v1 [cs.CV])\nAbstract: Most existing cross-modal retrieval methods employ two-stream encoders with different architectures for images and texts, \\textit{e.g.}, CNN for images and RNN/Transformer for texts. Such discrepancy in architectures may induce different semantic distribution spaces and limit the interactions between images and texts, and further result in inferior alignment between images and texts. To fill this research gap, inspired by recent advances of Transformers in vision tasks, we propose to unify the encoder architectures with Transformers for both modalities. Specifically, we design a cross-modal retrieval framework purely based on two-stream Transformers, dubbed \\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image Transformer, a text Transformer, and a hierarchical alignment module. With such identical architectures, the encoders could produce representations with more similar characteristics for images and texts, and make the interactions and alignments between th",
    "path": "papers/23/08/2308.04343.json",
    "total_tokens": 846,
    "translated_title": "用Transformer将双流编码器统一起来进行跨模态检索",
    "translated_abstract": "大多数现有的跨模态检索方法使用不同的架构的双流编码器，例如，用于图像的CNN和用于文本的RNN / Transformer。这种架构上的差异可能会导致不同的语义分布空间，并限制图像和文本之间的交互，进而导致图像和文本之间的对齐效果较差。为填补这一研究空白，受Transformer在视觉任务中的最新进展的启发，我们提出使用Transformer统一两种模态的编码器架构，用于跨模态检索。具体而言，我们设计了一个完全基于双流Transformer的跨模态检索框架，称为“Hierarchical Alignment Transformers (HAT)”，其中包括一种图像Transformer、一种文本Transformer和一个层次对齐模块。通过这种相同的架构，编码器可以为图像和文本生成更相似的表示，并实现更好的交互和对齐。",
    "tldr": "本论文提出了一种统一双流编码器架构的跨模态检索方法，使用Transformer进行图像和文本的编码，并通过层次对齐模块实现更好的交互和对齐效果。",
    "en_tdlr": "This paper proposes a cross-modal retrieval method that unifies the architectures of two-stream encoders using Transformers for encoding images and texts, resulting in better interactions and alignment through a hierarchical alignment module."
}