{
    "title": "CAME: Contrastive Automated Model Evaluation. (arXiv:2308.11111v1 [cs.CV])",
    "abstract": "The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior wo",
    "link": "http://arxiv.org/abs/2308.11111",
    "context": "Title: CAME: Contrastive Automated Model Evaluation. (arXiv:2308.11111v1 [cs.CV])\nAbstract: The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior wo",
    "path": "papers/23/08/2308.11111.json",
    "total_tokens": 880,
    "translated_title": "CAME: 对比自动化模型评估",
    "translated_abstract": "自动化模型评估（AutoEval）框架探索了在没有标记的测试集的情况下评估训练好的机器学习模型的可能性。尽管有一些不错的结果和承诺，但现有的AutoEval方法主要依赖于计算未标记测试集与训练集之间的分布偏移。我们认为这种对训练集的依赖成为将这项技术应用于真实世界机器学习开发中的另一个障碍。在这项工作中，我们提出了对比自动模型评估（CAME）一个新的AutoEval框架，该框架不需要依赖训练集。CAME的核心思想基于理论分析，将模型性能与对比损失相联系。此外，通过大量的实证验证，我们成功建立了两者之间的可预测关系，只需在未标记/未见的测试集上进行推导。由此产生的框架CAME通过超越以前的工作建立了新的SOTA结果。",
    "tldr": "CAME是一个不依赖训练集的对比自动化模型评估框架，通过理论分析和实证验证，建立了模型性能与对比损失之间的可预测关系，并取得了新的SOTA结果。",
    "en_tdlr": "CAME is a contrastive automated model evaluation framework that does not rely on the training set. It establishes a predictable relationship between model performance and contrastive loss through theoretical analysis and empirical validation, achieving new state-of-the-art results."
}