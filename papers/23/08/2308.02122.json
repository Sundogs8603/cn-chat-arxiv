{
    "title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])",
    "abstract": "Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W",
    "link": "http://arxiv.org/abs/2308.02122",
    "context": "Title: ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])\nAbstract: Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W",
    "path": "papers/23/08/2308.02122.json",
    "total_tokens": 929,
    "translated_title": "ParaFuzz：一种基于可解释性的技术用于检测自然语言处理中的毒样本",
    "translated_abstract": "傍门攻击已成为自然语言处理（NLP）模型的重要威胁，其中在输入中存在特定触发器可以导致被污染的模型将这些输入误分类为预定的目标类别。当前的检测机制受到限制，无法应对更隐蔽的傍门策略，如基于风格的攻击。在这项工作中，我们提出了一种创新的测试时污染样本检测框架，该框架依赖于模型预测的可解释性，并与输入的语义含义有关。我们认为，触发器（例如，不常见的单词）不应该从根本上改变被污染样本的基本语义含义，因为它们想保持潜伏。基于这个观察，我们假设在改写过程中，模型对于重写过的干净样本的预测应该保持稳定，而对于污染样本的预测在触发器的突变过程中应该恢复到真实标签。",
    "tldr": "ParaFuzz是一种基于可解释性的技术，用于检测自然语言处理中的毒样本。该技术通过观察模型在重写过的干净样本和污染样本上的预测稳定性，来判断样本是否被污染。",
    "en_tdlr": "ParaFuzz is an interpretability-driven technique for detecting poisoned samples in natural language processing (NLP). By observing the stability of model predictions on paraphrased clean samples and poisoned samples, the technique determines if a sample has been poisoned."
}