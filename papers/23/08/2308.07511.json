{
    "title": "Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach. (arXiv:2308.07511v1 [cs.LG])",
    "abstract": "As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers\" to assist the training of NNs, which are ``students\", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining o",
    "link": "http://arxiv.org/abs/2308.07511",
    "context": "Title: Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach. (arXiv:2308.07511v1 [cs.LG])\nAbstract: As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers\" to assist the training of NNs, which are ``students\", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining o",
    "path": "papers/23/08/2308.07511.json",
    "total_tokens": 865,
    "translated_title": "将资源管理算法的知识融入神经网络：一种统一的训练辅助方法",
    "translated_abstract": "作为一个基本问题，在多用户环境中，有很多方法致力于优化信干噪比（SINR）。尽管传统的基于模型的优化方法取得了很好的性能，但高复杂性催生了基于神经网络（NN）的方法来权衡性能和复杂性。为了充分利用传统基于模型的方法的高性能和NN方法的低复杂性，本文提出了一种基于知识蒸馏（KD）的算法蒸馏（AD）方法，以提高NN方法的性能和收敛速度。在这个方法中，采用传统的SINR优化方法作为“老师”来辅助训练NNs，即“学生”，从而增强无监督和强化学习技术的性能。该方法旨在缓解每种训练范例中遇到的常见问题，包括无法获得准确解的问题。",
    "tldr": "本文提出了一种将传统基于模型的信干噪比优化方法应用于神经网络训练的知识蒸馏方法，以提高神经网络的性能和收敛速度。",
    "en_tdlr": "A knowledge distillation method is proposed in this paper to improve the performance and convergence speed of neural networks by incorporating traditional model-based optimization methods for signal-to-interference-plus-noise ratio (SINR)."
}