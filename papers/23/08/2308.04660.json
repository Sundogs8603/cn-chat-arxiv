{
    "title": "Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets. (arXiv:2308.04660v1 [cs.LG])",
    "abstract": "Bayesian optimization (BO) is widely adopted in black-box optimization problems and it relies on a surrogate model to approximate the black-box response function. With the increasing number of black-box optimization tasks solved and even more to solve, the ability to learn from multiple prior tasks to jointly pre-train a surrogate model is long-awaited to further boost optimization efficiency. In this paper, we propose a simple approach to pre-train a surrogate, which is a Gaussian process (GP) with a kernel defined on deep features learned from a Transformer-based encoder, using datasets from prior tasks with possibly heterogeneous input spaces. In addition, we provide a simple yet effective mix-up initialization strategy for input tokens corresponding to unseen input variables and therefore accelerate new tasks' convergence. Experiments on both synthetic and real benchmark problems demonstrate the effectiveness of our proposed pre-training and transfer BO strategy over existing metho",
    "link": "http://arxiv.org/abs/2308.04660",
    "context": "Title: Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets. (arXiv:2308.04660v1 [cs.LG])\nAbstract: Bayesian optimization (BO) is widely adopted in black-box optimization problems and it relies on a surrogate model to approximate the black-box response function. With the increasing number of black-box optimization tasks solved and even more to solve, the ability to learn from multiple prior tasks to jointly pre-train a surrogate model is long-awaited to further boost optimization efficiency. In this paper, we propose a simple approach to pre-train a surrogate, which is a Gaussian process (GP) with a kernel defined on deep features learned from a Transformer-based encoder, using datasets from prior tasks with possibly heterogeneous input spaces. In addition, we provide a simple yet effective mix-up initialization strategy for input tokens corresponding to unseen input variables and therefore accelerate new tasks' convergence. Experiments on both synthetic and real benchmark problems demonstrate the effectiveness of our proposed pre-training and transfer BO strategy over existing metho",
    "path": "papers/23/08/2308.04660.json",
    "total_tokens": 930,
    "translated_title": "高效的贝叶斯优化方法：使用基于变换器预训练的深度核学习的多元异构数据集",
    "translated_abstract": "贝叶斯优化广泛应用于黑盒优化问题，它依赖于一个替代模型来近似黑盒响应函数。随着解决黑盒优化任务的数量增加，学习多个先前任务以共同预训练替代模型，以进一步提高优化效率的能力备受期待。本文提出了一种简单的方法来预训练替代模型，该模型是一个基于变换器编码器学习的深度特征上定义的高斯过程（GP），使用来自可能存在异构输入空间的先前任务的数据集。此外，我们提供了一种简单而有效的混合初始化策略，用于对应于未见过的输入变量的输入令牌，从而加速新任务的收敛。在合成和真实基准问题上的实验证明了我们提出的预训练和迁移贝叶斯优化策略相对于现有方法的有效性",
    "tldr": "本文提出了一种高效的贝叶斯优化方法，使用变换器预训练的深度核学习和多元异构数据集。通过从先前任务中学习来共同预训练替代模型，并通过简单有效的混合初始化策略加速新任务的收敛，实验结果证明了方法的有效性。",
    "en_tdlr": "This paper proposes an efficient Bayesian optimization method that utilizes deep kernel learning and a transformer pre-trained on multiple heterogeneous datasets. The approach involves jointly pre-training a surrogate model using prior tasks' data and a simple yet effective mix-up initialization strategy to accelerate convergence for new tasks. Experimental results demonstrate the effectiveness of the proposed method."
}