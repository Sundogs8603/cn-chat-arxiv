{
    "title": "Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])",
    "abstract": "Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks ",
    "link": "http://arxiv.org/abs/2308.14897",
    "context": "Title: Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])\nAbstract: Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks ",
    "path": "papers/23/08/2308.14897.json",
    "total_tokens": 887,
    "translated_title": "高效统计方差缩减的双策略评估用于序列建模强化学习中的离线策略评估",
    "translated_abstract": "离线强化学习旨在利用先前收集的环境-动作交互记录的数据集来学习一个无需访问真实环境的策略。最近的研究表明，离线强化学习可以被形式化为一个序列建模问题，并通过决策转换器等方法进行监督学习来解决。尽管这些基于序列的方法在回报率方法上取得了有竞争力的结果，尤其是在需要较长的回合或回报稀缺的任务上，但在处理离线数据时并未考虑重要性采样来校正策略偏差，主要原因是缺乏行为策略和使用确定性评估策略。为此，我们提出了DPE：一种将离线序列建模和离线强化学习与统计上证明具有方差缩减性质的双策略评估（DPE）融合在一个统一框架中的RL算法。我们在多个任务中验证了我们的方法。",
    "tldr": "本论文提出了一种使用双策略评估的离线序列建模和离线强化学习的RL算法，通过统计方法证明具有方差缩减的特性。"
}