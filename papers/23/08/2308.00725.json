{
    "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])",
    "abstract": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
    "link": "http://arxiv.org/abs/2308.00725",
    "context": "Title: Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])\nAbstract: End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
    "path": "papers/23/08/2308.00725.json",
    "total_tokens": 782,
    "translated_title": "潜在漂移:熵梯度有助于神经编解码器",
    "translated_abstract": "相比通过几十年的手工工程努力开发的传统压缩技术，端到端图像/视频编解码器变得具有竞争力。这些可训练的编解码器相对传统技术具有许多优势，例如易于适应感知失真度量标准和在特定领域具有高性能的学习能力。然而，现有的神经编解码器没有利用解码器中熵的梯度的存在。本文理论上证明了熵梯度（解码器端可用）与重构误差的梯度（解码器端不可用）之间的相关性。然后，我们通过实验证明，可以在各种压缩方法上使用这个梯度，从而在相同质量下实现1-2％的速率节省。我们的方法与其他改进方法正交，并带来独立的速率节省。",
    "tldr": "本文研究了梯度熵与重构误差梯度的相关性，在神经编解码器中利用梯度熵能够实现1-2％的速率节省，这是独立于其他改进方法的。"
}