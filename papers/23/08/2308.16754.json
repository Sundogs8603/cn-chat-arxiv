{
    "title": "Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction. (arXiv:2308.16754v1 [math.FA])",
    "abstract": "We introduce and study the theory of training neural networks using interpolation techniques from reproducing kernel Hilbert space theory. We generalize the method to Krein spaces, and show that widely-used neural network architectures are subsets of reproducing kernel Krein spaces (RKKS). We study the concept of \"associated Hilbert spaces\" of RKKS and develop techniques to improve upon the expressivity of various activation functions. Next, using concepts from the theory of functions of several complex variables, we prove a computationally applicable, multidimensional generalization of the celebrated Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural networks, called Prolongation Neural Networks (PNN). We demonstrate that, by applying the multidimensional AAK theorem to gain a PNN, one can gain performance superior to both our interpolatory methods and current state-of-the-art methods in noisy environments. We provide useful illustrations of our methods in p",
    "link": "http://arxiv.org/abs/2308.16754",
    "context": "Title: Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction. (arXiv:2308.16754v1 [math.FA])\nAbstract: We introduce and study the theory of training neural networks using interpolation techniques from reproducing kernel Hilbert space theory. We generalize the method to Krein spaces, and show that widely-used neural network architectures are subsets of reproducing kernel Krein spaces (RKKS). We study the concept of \"associated Hilbert spaces\" of RKKS and develop techniques to improve upon the expressivity of various activation functions. Next, using concepts from the theory of functions of several complex variables, we prove a computationally applicable, multidimensional generalization of the celebrated Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural networks, called Prolongation Neural Networks (PNN). We demonstrate that, by applying the multidimensional AAK theorem to gain a PNN, one can gain performance superior to both our interpolatory methods and current state-of-the-art methods in noisy environments. We provide useful illustrations of our methods in p",
    "path": "papers/23/08/2308.16754.json",
    "total_tokens": 964,
    "translated_title": "使用再生核空间插值和模型简化训练神经网络的理论研究",
    "translated_abstract": "我们引入并研究了使用再生核希尔伯特空间理论中的插值技术训练神经网络的理论。我们将该方法推广到克莱因空间，并证明了广泛使用的神经网络架构是再生核克莱因空间（RKKS）的子集。我们研究了与RKKS关联的希尔伯特空间的概念，并开发了改进各种激活函数表达能力的技术。接下来，我们使用几个复变函数的理论概念证明了一个可计算的、多维的亚当扬-阿罗夫-克雷因（AAK）定理的推广。该定理产生了一类新型神经网络，称为延拓神经网络（PNN）。我们证明，通过将多维AAK定理用于获得PNN，可以在噪声环境中获得优于我们的插值方法和当前最先进方法的性能。我们在实例中提供了我们方法的有用说明。",
    "tldr": "这篇论文研究了使用再生核空间插值和模型简化训练神经网络的理论，并推导了一个多维的亚当扬-阿罗夫-克雷因定理，提出了一种新型的神经网络架构（PNN），在噪声环境中表现出比传统方法更优越的性能。",
    "en_tdlr": "This paper studies the theory of training neural networks using reproducing kernel space interpolation and model reduction. It introduces a multidimensional generalization of the Adamjan-Arov-Krein theorem, which yields a novel class of neural networks called Prolongation Neural Networks (PNN). The PNN architecture outperforms traditional methods in noisy environments."
}