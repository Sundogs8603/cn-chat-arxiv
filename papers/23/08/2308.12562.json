{
    "title": "Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])",
    "abstract": "Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bot",
    "link": "http://arxiv.org/abs/2308.12562",
    "context": "Title: Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])\nAbstract: Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bot",
    "path": "papers/23/08/2308.12562.json",
    "total_tokens": 906,
    "translated_title": "利用大型语言和多模态模型进行可解释预测的变分信息追求",
    "translated_abstract": "变分信息追求(V-IP)是一个通过顺序选择与任务相关、用户定义和可解释的数据查询来设计可解释预测的框架。虽然这使得预测模型具有内置的可解释性，但将V-IP应用于任何任务都需要具有由领域专家进行密集概念标注的数据样本，限制了V-IP在手动数据注释可行的小规模任务中的应用。本文中，我们通过引入基础模型(FMs)来扩展V-IP框架，以解决这个限制。具体而言，我们使用了一个两步流程，首先利用大型语言模型(LLMs)生成足够大的候选任务相关可解释概念集，然后利用大型多模态模型通过与生成的概念集中的每个概念的语义相似性对每个数据样本进行注释。虽然还有其他可解释设计框架，比如Concept Bot，但这些框架不适合处理大规模任务。",
    "tldr": "本文提出了一种利用大型语言和多模态模型的变分信息追求(V-IP)框架，通过顺序选择任务相关的可解释查询，实现可解释预测。为了解决数据标注的限制，引入了基础模型(FMs)，使用大型语言模型(LLMs)生成候选可解释概念集，并使用大型多模态模型注释每个数据样本。此方法适用于大规模任务。"
}