{
    "title": "IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. (arXiv:2308.11473v1 [cs.CV])",
    "abstract": "Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the tra",
    "link": "http://arxiv.org/abs/2308.11473",
    "context": "Title: IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. (arXiv:2308.11473v1 [cs.CV])\nAbstract: Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the tra",
    "path": "papers/23/08/2308.11473.json",
    "total_tokens": 979,
    "translated_title": "IT3D: 利用显式视角合成改进的文本到3D生成技术",
    "translated_abstract": "近期的文本到3D技术的进展主要得益于从强大的大型文本到图像扩散模型（LDM）中提取知识。然而，现有的文本到3D方法常常面临过渡饱和、细节不足和不真实的输出等挑战。本研究提出了一种新的策略，利用显式合成的多视角图像来解决这些问题。我们的方法使用基于粗糙3D模型渲染的图像到图像流水线，借助LDM生成具有姿态和高质量的图像。尽管生成的图像大多数情况下缓解了前述问题，但由于大型扩散模型的生成性质，视角不一致和内容差异显著的挑战仍然存在，这在有效利用这些图像方面带来了巨大困难。为了克服这个障碍，我们提倡结合鉴别器和一种新颖的扩散生成对抗训练策略，来指导训练过程。",
    "tldr": "这项研究提出了一种利用显式视角合成多视角图像来改进文本到3D生成技术的策略，该方法通过结合图像到图像流水线和大型扩散模型生成高质量的图像，并通过鉴别器和扩散生成对抗训练来解决视角一致性和内容变化的挑战。",
    "en_tdlr": "This study presents a novel strategy that leverages explicitly synthesized multi-view images to improve Text-to-3D generation, addressing challenges such as over-saturation and unrealistic outputs. The proposed approach combines image-to-image pipelines and large diffusion models to generate high-quality images, and uses a discriminator and diffusion GAN training to overcome view inconsistency and content variance."
}