{
    "title": "GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training. (arXiv:2308.11331v1 [cs.CV])",
    "abstract": "Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at",
    "link": "http://arxiv.org/abs/2308.11331",
    "context": "Title: GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training. (arXiv:2308.11331v1 [cs.CV])\nAbstract: Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at",
    "path": "papers/23/08/2308.11331.json",
    "total_tokens": 906,
    "translated_title": "GrowCLIP: 数据感知的自动模型增长方法用于大规模对比语言-图像预训练",
    "translated_abstract": "跨模态预训练在各种下游任务中展现出令人印象深刻的性能，从互联网收集的大量图像-文本对受益匪浅。在实践中，在线数据不断增长，凸显了预训练模型能够从不断增长的数据中学习的重要性。现有的跨模态预训练工作主要集中在训练具有固定架构的网络上。然而，考虑到实际应用中预训练数据的不断增长的特性，限制模型容量是不切实际的。另一方面，利用当前模型中的知识来获得高效的训练和更好的性能是很重要的。为了解决上述问题，在本文中，我们提出了GrowCLIP，这是一种数据驱动的自动模型增长算法，用于基于连续图像-文本对的对比语言-图像预训练。特别地，我们采用了动态增长空间，并寻找最佳架构。",
    "tldr": "GrowCLIP是一种数据感知的自动模型增长算法，用于对比语言-图像预训练。它能够根据持续增长的图像-文本数据找到最佳架构，实现高效训练和更好的性能。"
}