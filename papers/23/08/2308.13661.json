{
    "title": "Go Beyond Imagination: Maximizing Episodic Reachability with World Models. (arXiv:2308.13661v1 [cs.LG])",
    "abstract": "Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on locomotion tasks from DeepMind Control Suite.",
    "link": "http://arxiv.org/abs/2308.13661",
    "context": "Title: Go Beyond Imagination: Maximizing Episodic Reachability with World Models. (arXiv:2308.13661v1 [cs.LG])\nAbstract: Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on locomotion tasks from DeepMind Control Suite.",
    "path": "papers/23/08/2308.13661.json",
    "total_tokens": 933,
    "translated_title": "超越想象：通过世界模型最大化情节可达性",
    "translated_abstract": "在强化学习中，高效探索是一个具有挑战性的问题，尤其是对于稀疏奖励任务。为了应对奖励稀疏性，人们通常会应用内在奖励来激励智能体有效地探索状态空间。在本文中，我们引入了一种新的内在奖励设计，称为GoBI - 超越想象，它将传统的终身新颖性动机与一个情节内在奖励相结合，旨在最大化逐步可达性扩展。具体而言，我们应用学习的世界模型来生成具有随机动作的预测未来状态。那些具有更多独特预测且不在情节记忆中的状态将被分配高内在奖励。我们的方法在12个最具挑战性的迷你网格导航任务中大大超越了先前最先进的方法，并提高了DeepMind Control Suite中运动任务的样本效率。",
    "tldr": "通过引入GoBI（超越想象）内在奖励设计，结合传统的终身新颖性动机和情节内在奖励，利用学习的世界模型最大化了逐步可达扩展，有效解决了强化学习中的奖励稀疏问题，并在迷你网格导航任务和DeepMind控制套件中取得了优越的性能。"
}