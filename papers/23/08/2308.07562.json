{
    "title": "Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels. (arXiv:2308.07562v1 [cs.LG])",
    "abstract": "Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subs",
    "link": "http://arxiv.org/abs/2308.07562",
    "context": "Title: Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels. (arXiv:2308.07562v1 [cs.LG])\nAbstract: Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subs",
    "path": "papers/23/08/2308.07562.json",
    "total_tokens": 919,
    "translated_title": "非随机缺失标签的多重插补半监督学习",
    "translated_abstract": "半监督学习是在训练算法时同时使用有标签和无标签数据的技术。这是机器学习的一个常见应用，因为获得完全标记的数据集是不现实的。研究人员已经解决了三个主要问题：随机缺失、完全随机缺失和非随机缺失。非随机缺失问题是三者中最具挑战性的，因为不能安全地假设所有类别分布相等。现有的方法，包括类感知插补和类似概率，大多忽视了无标签数据中的非随机性。本文提出了两种新的多重插补模型组合方法，以实现更高的准确性和更小的偏差。1）我们使用多重插补模型，创建置信区间，并应用阈值来忽略置信度较低的伪标签。2）我们的新方法，去偏插补半监督学习（SSL-DI），旨在通过过滤不准确的数据并找到一种子数据集，来减小偏差。",
    "tldr": "这篇论文提出了两种新的多重插补模型组合方法，实现了半监督学习中非随机缺失标签问题的更高准确性和更小偏差。",
    "en_tdlr": "This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias in semi-supervised learning with non-random missing labels."
}