{
    "title": "MOLE: MOdular Learning FramEwork via Mutual Information Maximization. (arXiv:2308.07772v1 [cs.LG])",
    "abstract": "This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.",
    "link": "http://arxiv.org/abs/2308.07772",
    "context": "Title: MOLE: MOdular Learning FramEwork via Mutual Information Maximization. (arXiv:2308.07772v1 [cs.LG])\nAbstract: This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.",
    "path": "papers/23/08/2308.07772.json",
    "total_tokens": 820,
    "translated_title": "MOLE: MOdular Learning FramEwork通过最大化互信息引入了一种异步和本地化的神经网络学习框架",
    "translated_abstract": "本文介绍了一种名为MOLE（MOdular Learning Framework）的异步和本地化学习框架，该框架通过层次化的方式对神经网络进行模块化，并通过最大化互信息的方式定义了每个模块的训练目标，并按顺序通过最大化互信息的方法对每个模块进行训练。MOLE使得训练变得具有局部优化和模块之间梯度隔离的特性，这种方案在生物学上更具可行性，比反向传播更加合理。我们在向量、网格和图形类型的数据上进行了实验。特别地，这个框架能够解决图形类型数据的图形级和节点级任务。因此，通过实验证明MOLE可以广泛适用于不同类型的数据。",
    "tldr": "MOLE是一种异步和本地化的神经网络学习框架，它通过层次化的模块化方式和最大化互信息的方法来进行训练，具有局部优化和梯度隔离的特性，并且在不同类型的数据上都有良好的应用效果。",
    "en_tdlr": "MOLE is an asynchronous and local learning framework for neural networks, which modularizes neural networks by layers and trains each module by maximizing mutual information. It has the characteristics of local optimization and gradient isolation, and has been proven to be applicable to different types of data."
}