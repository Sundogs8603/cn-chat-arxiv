{
    "title": "Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])",
    "abstract": "Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.",
    "link": "http://arxiv.org/abs/2308.06590",
    "context": "Title: Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])\nAbstract: Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.",
    "path": "papers/23/08/2308.06590.json",
    "total_tokens": 878,
    "translated_title": "基于价值分布模型的强化学习",
    "translated_abstract": "在解决顺序决策任务中，量化政策长期绩效的不确定性是很重要的。我们从基于模型的贝叶斯强化学习的角度研究这个问题，目标是学习由马尔科夫决策过程的参数（认知）不确定性引发的值函数的后验分布。以往的研究将分析限制在少数分布值上，或者约束分布形状，例如，高斯分布。受到分布式强化学习的启发，我们引入一个Bellman算子，其固定点是值分布函数。基于我们的理论，我们提出了Epistemic Quantile-Regression（EQR），这是一种基于模型的算法，可以学习一个值分布函数用于策略优化。在几个连续控制任务上的评估结果显示相对于已有的基于模型和基于模型的算法，EQR具有性能优势。",
    "tldr": "该论文介绍了一种基于价值分布模型的强化学习方法，该方法通过学习后验分布来解决决策任务中的政策不确定性问题。所提出的算法能够有效地优化策略，在连续控制任务中表现出性能优势。"
}