{
    "title": "A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])",
    "abstract": "In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.",
    "link": "http://arxiv.org/abs/2308.12568",
    "context": "Title: A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])\nAbstract: In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.",
    "path": "papers/23/08/2308.12568.json",
    "total_tokens": 845,
    "translated_title": "用于中文医学标点修复的小型快速BERT模型",
    "translated_abstract": "在临床听写中，没有明确标点符号的自动语音识别（ASR）导致了对听写报告的误解。为了使用ASR提供精确和易懂的临床报告，需要进行自动标点修复。考虑到实际情况，我们提出了一种基于“预训练和微调”范式的快速轻量级预训练模型，用于中文医学标点修复。在这项工作中，我们通过结合监督对比学习和一种新颖的辅助预训练任务（标点符号预测）来提炼预训练模型，使其适用于标点修复。我们在各种提炼模型上的实验表明，相对于最先进的中文RoBERTa模型，我们的模型可以在10%的模型大小的情况下实现95%的性能。",
    "tldr": "该论文提出了一种用于中文医学标点修复的快速小型BERT模型。通过结合监督对比学习和辅助预训练任务，该模型在具有较小模型大小的情况下，能够实现与最先进的中文RoBERTa模型相当的95%性能。",
    "en_tdlr": "This paper proposes a small and fast BERT model for Chinese medical punctuation restoration. By incorporating supervised contrastive learning and an auxiliary pre-training task, the model achieves 95% performance relative to the state-of-the-art Chinese RoBERTa, while having only 10% model size."
}