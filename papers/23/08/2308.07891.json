{
    "title": "Link-Context Learning for Multimodal LLMs. (arXiv:2308.07891v1 [cs.CV])",
    "abstract": "The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to reco",
    "link": "http://arxiv.org/abs/2308.07891",
    "context": "Title: Link-Context Learning for Multimodal LLMs. (arXiv:2308.07891v1 [cs.CV])\nAbstract: The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to reco",
    "path": "papers/23/08/2308.07891.json",
    "total_tokens": 922,
    "translated_title": "多模态LLM的链接上下文学习",
    "translated_abstract": "在人类对话中，从上下文中学习新概念并提供适当的回应是至关重要的。尽管目前的多模态大型语言模型(MLLMs)和大型语言模型(LLMs)在大规模数据集上进行了训练，但在无需训练的情况下识别未见过的图像或理解新概念仍然是一个挑战。上下文学习(ICL)探索了无需训练的少样本学习，鼓励模型从有限的任务中“学会学习”并泛化到未见过的任务。在这项工作中，我们提出了链接上下文学习(LCL)，强调“从因果关系中推理”以增强MLLMs的学习能力。LCL超越传统的ICL，通过明确加强支持集和查询集之间的因果关系，提供具有因果链接的演示，引导模型不仅辨别类比关系，还揭示数据点之间的潜在因果关联，提升了MLLMs实现相应推荐的能力。",
    "tldr": "链接上下文学习(LCL)通过从因果关系中推理来增强多模态大型语言模型(MLLMs)的学习能力，引导模型辨别类比关系并揭示潜在的因果关联，提升了对未见过数据的识别和理解。"
}