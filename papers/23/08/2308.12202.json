{
    "title": "Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])",
    "abstract": "Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose",
    "link": "http://arxiv.org/abs/2308.12202",
    "context": "Title: Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])\nAbstract: Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose",
    "path": "papers/23/08/2308.12202.json",
    "total_tokens": 921,
    "translated_title": "使用Adam进行课程学习：魔鬼在于错误的细节",
    "translated_abstract": "课程学习（CL）认为，与人类类似，机器学习模型可能更有效地从与其当前学习进展相匹配的数据中学习。然而，CL方法仍然被很少了解，在自然语言处理（NLP）领域尤其如此，其取得的成果也有限。本文探讨了其中的原因。我们尝试复现和扩展一些最近的课程方法，但发现当应用于NLP时，这些方法的结果出奇地脆弱。对某些情况下课程效果的深入研究向我们展示了原因：当将课程与广受欢迎的Adam优化算法结合使用时，它们往往会学习适应此算法的次优化参数。我们提供了几个不同的案例研究，涉及常见的手工制作和自动化CL方法，以说明这种现象，并发现其中没有一个能够胜过仅使用精心选择的Adam进行优化",
    "tldr": "本文研究了为何课程学习在自然语言处理领域取得有限的成功。研究发现，当将课程与Adam优化算法结合使用时，它们学习适应了次优化参数，导致其脆弱性增加",
    "en_tdlr": "This paper explores the limited success of curriculum learning in natural language processing (NLP), with a focus on the combination of curriculum methods with the Adam optimization algorithm. The study reveals that when the curricula are combined with Adam, the models learn to adapt to suboptimal optimization parameters, resulting in increased brittleness."
}