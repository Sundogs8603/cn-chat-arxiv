{
    "title": "Learning to generate and corr- uh I mean repair language in real-time. (arXiv:2308.11683v1 [cs.CL])",
    "abstract": "In conversation, speakers produce language incrementally, word by word, while continuously monitoring the appropriateness of their own contribution in the dynamically unfolding context of the conversation; and this often leads them to repair their own utterance on the fly. This real-time language processing capacity is furthermore crucial to the development of fluent and natural conversational AI. In this paper, we use a previously learned Dynamic Syntax grammar and the CHILDES corpus to develop, train and evaluate a probabilistic model for incremental generation where input to the model is a purely semantic generation goal concept in Type Theory with Records (TTR). We show that the model's output exactly matches the gold candidate in 78% of cases with a ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of the same model to generate self-repairs when the generation goal changes mid-utterance. Automatic evaluation shows that the model can generate self-repairs c",
    "link": "http://arxiv.org/abs/2308.11683",
    "context": "Title: Learning to generate and corr- uh I mean repair language in real-time. (arXiv:2308.11683v1 [cs.CL])\nAbstract: In conversation, speakers produce language incrementally, word by word, while continuously monitoring the appropriateness of their own contribution in the dynamically unfolding context of the conversation; and this often leads them to repair their own utterance on the fly. This real-time language processing capacity is furthermore crucial to the development of fluent and natural conversational AI. In this paper, we use a previously learned Dynamic Syntax grammar and the CHILDES corpus to develop, train and evaluate a probabilistic model for incremental generation where input to the model is a purely semantic generation goal concept in Type Theory with Records (TTR). We show that the model's output exactly matches the gold candidate in 78% of cases with a ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of the same model to generate self-repairs when the generation goal changes mid-utterance. Automatic evaluation shows that the model can generate self-repairs c",
    "path": "papers/23/08/2308.11683.json",
    "total_tokens": 877,
    "translated_title": "实时学习生成和修复语言",
    "translated_abstract": "在对话中，发言者逐字逐句地产生语言，并不断监控自己的贡献是否适当，同时动态地适应对话的环境，这经常导致他们在说话过程中即时修复自己的话语。这种实时语言处理能力对于流利和自然的对话人工智能的发展至关重要。在本文中，我们使用先前学习的动态语法和CHILDES语料库，开发、训练和评估了一个基于概率的逐步生成模型，模型的输入是一个纯粹的语义生成目标概念，使用类型理论与记录（TTR）。我们展示了模型在78%的情况下与最佳候选输出完全匹配，ROUGE-l得分为0.86。我们进一步对同一模型生成在话语过程中生成目标发生变化时的自我修复能力进行了零样本评估。自动评估表明，模型可以生成自我修复。",
    "tldr": "本文针对实时语言处理能力的发展，使用动态语法和CHILDES语料库开发了一个基于概率的逐步生成模型，在78%的情况下能完全匹配最佳候选输出，且具备自我修复能力。",
    "en_tdlr": "This paper develops a probabilistic incremental generation model using dynamic grammar and CHILDES corpus, which can match the gold candidate in 78% of cases and has the ability to generate self-repairs in real-time language processing."
}