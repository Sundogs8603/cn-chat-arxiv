{
    "title": "Foundation Model is Efficient Multimodal Multitask Model Selector. (arXiv:2308.06262v1 [cs.LG])",
    "abstract": "This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression",
    "link": "http://arxiv.org/abs/2308.06262",
    "context": "Title: Foundation Model is Efficient Multimodal Multitask Model Selector. (arXiv:2308.06262v1 [cs.LG])\nAbstract: This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression",
    "path": "papers/23/08/2308.06262.json",
    "total_tokens": 922,
    "translated_title": "Foundation Model是一个高效的多模态多任务模型选择器",
    "translated_abstract": "本文研究了一个不常见但非常重要的问题：在给定一组预训练的神经网络的情况下，如何在不对它们进行微调的情况下预测它们在每个多模态任务上的性能，比如图像识别、指代、字幕生成、视觉问答和文字问答。一种蛮力的方法是对所有模型在所有目标数据集上进行微调，这会带来高计算成本。虽然最近的一些先进方法采用轻量级指标来衡量模型的可迁移性，但它们往往严重依赖于单个任务的先验知识，使得它们在多模态多任务的场景中不适用。为了解决这个问题，我们提出了一种高效的多任务模型选择器（EMMS），它使用大规模的基于基础模型，将不同下游任务的各种标签格式，如类别、文本和边界框转换为统一的噪声标签嵌入。EMMS可以通过简单的加权线性回归来估计模型的可迁移性。",
    "tldr": "本文提出了一种高效的多模态多任务模型选择器（EMMS），它使用大规模的基础模型将不同任务的标签格式转换为统一的噪声标签嵌入，通过简单的加权线性回归来估计模型的可迁移性。",
    "en_tdlr": "This paper proposes an efficient multi-modal multitask model selector (EMMS), which uses large-scale foundation models to transform diverse label formats of different tasks into a unified noisy label embedding and estimates the transferability of models through a simple weighted linear regression."
}