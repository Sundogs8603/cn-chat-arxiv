{
    "title": "Faster Training of Neural ODEs Using Gau{\\ss}-Legendre Quadrature. (arXiv:2308.10644v1 [cs.LG])",
    "abstract": "Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gau{\\ss}-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models.",
    "link": "http://arxiv.org/abs/2308.10644",
    "context": "Title: Faster Training of Neural ODEs Using Gau{\\ss}-Legendre Quadrature. (arXiv:2308.10644v1 [cs.LG])\nAbstract: Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gau{\\ss}-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models.",
    "path": "papers/23/08/2308.10644.json",
    "total_tokens": 949,
    "translated_title": "使用Gauß-Legendre积分加速神经ODE的训练",
    "translated_abstract": "神经ODE在生成模型和时间序列建模中表现出强大性能。然而，通过伴随方法训练它们比离散模型慢，因为需要数值解ODE。为了加速神经ODE，常用的方法是对解进行正则化。然而，这种方法可能会影响模型的表达能力；当轨迹本身很重要时，这一点尤为重要。本文提出了一种加速神经ODE训练的另一种方法。关键思想是通过使用Gauß-Legendre积分比ODE方法更快地求解积分，同时保持内存效率，加速伴随方法。我们还通过训练相应的ODE并转移参数，将这种想法扩展到使用Wong-Zakai定理训练SDE。我们的方法导致了神经ODE的快速训练，特别是对于大型模型。它还提出了一种训练基于SDE的模型的新方法。",
    "tldr": "本文提出了一种使用Gauß-Legendre积分加速神经ODE训练的方法，并通过训练相应的ODE和转移参数的方式扩展到SDE训练，加快了神经ODE的训练速度，特别适用于大型模型。该方法也提供了一种训练基于SDE的模型的新方式。"
}