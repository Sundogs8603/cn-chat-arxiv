{
    "title": "Studying Large Language Model Generalization with Influence Functions. (arXiv:2308.03296v1 [cs.LG])",
    "abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of ",
    "link": "http://arxiv.org/abs/2308.03296",
    "context": "Title: Studying Large Language Model Generalization with Influence Functions. (arXiv:2308.03296v1 [cs.LG])\nAbstract: When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of ",
    "path": "papers/23/08/2308.03296.json",
    "total_tokens": 977,
    "translated_title": "使用影响函数研究大型语言模型的泛化能力",
    "translated_abstract": "在努力提高对机器学习模型的可见性以理解和减轻相关风险时，一个潜在有价值的证据来源是：哪些训练样本最大程度地影响了给定的行为？影响函数旨在回答一个反事实问题：如果将给定的序列添加到训练集中，模型的参数（因此也是模型的输出）将如何改变？尽管影响函数对小型模型产生了洞见，但由于计算逆Hessian-向量乘积（IHVP）的困难，它们很难扩展到大型语言模型（LLMs）。我们使用特征值修正的Kronecker-Factored Approximate Curvature（EK-FAC）近似方法，将影响函数扩展到具有520亿参数的LLMs。在我们的实验中，EK-FAC在IHVP计算速度快了数个数量级的情况下，实现了与传统影响函数估计器相似的准确性。我们研究了两种算法技术来减少成本",
    "tldr": "通过使用特征值修正的Kronecker-Factored Approximate Curvature (EK-FAC) 近似方法，这篇论文研究了如何使用影响函数来研究大型语言模型的泛化能力。实验证明，EK-FAC能够在计算速度更快的情况下，实现与传统影响函数估计器相似的准确性。"
}