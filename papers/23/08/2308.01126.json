{
    "title": "Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model. (arXiv:2308.01126v1 [cs.CV])",
    "abstract": "Current captioning approaches tend to generate correct but \"generic\" descriptions that lack real-world knowledge, e.g., named entities and contextual information. Considering that Vision-Language Pre-Training (VLP) models master massive such knowledge from large-scale web-harvested data, it is promising to utilize the generalizability of VLP models to incorporate knowledge into image descriptions. However, using VLP models faces challenges: zero-shot inference suffers from knowledge hallucination that leads to low-quality descriptions, but the generic bias in downstream task fine-tuning hinders the VLP model from expressing knowledge. To address these concerns, we propose a simple yet effective method called Knowledge-guided Replay (K-Replay), which enables the retention of pre-training knowledge during fine-tuning. Our approach consists of two parts: (1) a knowledge prediction task on automatically collected replay exemplars to continuously awaken the VLP model's memory about knowledg",
    "link": "http://arxiv.org/abs/2308.01126",
    "context": "Title: Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model. (arXiv:2308.01126v1 [cs.CV])\nAbstract: Current captioning approaches tend to generate correct but \"generic\" descriptions that lack real-world knowledge, e.g., named entities and contextual information. Considering that Vision-Language Pre-Training (VLP) models master massive such knowledge from large-scale web-harvested data, it is promising to utilize the generalizability of VLP models to incorporate knowledge into image descriptions. However, using VLP models faces challenges: zero-shot inference suffers from knowledge hallucination that leads to low-quality descriptions, but the generic bias in downstream task fine-tuning hinders the VLP model from expressing knowledge. To address these concerns, we propose a simple yet effective method called Knowledge-guided Replay (K-Replay), which enables the retention of pre-training knowledge during fine-tuning. Our approach consists of two parts: (1) a knowledge prediction task on automatically collected replay exemplars to continuously awaken the VLP model's memory about knowledg",
    "path": "papers/23/08/2308.01126.json",
    "total_tokens": 933,
    "translated_title": "超越通用：在视觉语言预训练模型中利用真实世界知识增强图像描述",
    "translated_abstract": "当前的图像描述方法往往生成正确但“通用”的描述，缺乏真实世界知识，例如命名实体和上下文信息。考虑到视觉语言预训练模型(VLP)可以从大规模的网络数据中掌握大量这样的知识，利用VLP模型的普适性将知识融入图像描述是有希望的。然而，使用VLP模型面临挑战：零样本推理会导致知识幻觉，从而产生低质量的描述，而下游任务微调中的通用偏差阻碍了VLP模型表达知识。为了解决这些问题，我们提出了一种简单而有效的方法，称为知识引导的回放(K-Replay)，在微调期间保持预训练知识。我们的方法包括两个部分：(1)在自动收集的回放示例上进行知识预测任务，连续唤醒VLP模型对知识的记忆。",
    "tldr": "本研究提出了一种称为知识引导的回放(K-Replay)的方法，通过在微调期间保持预训练知识，将真实世界知识融入图像描述，以解决当前图像描述方法的通用性和真实世界知识缺失的问题。"
}