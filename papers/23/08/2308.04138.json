{
    "title": "Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])",
    "abstract": "Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.",
    "link": "http://arxiv.org/abs/2308.04138",
    "context": "Title: Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])\nAbstract: Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.",
    "path": "papers/23/08/2308.04138.json",
    "total_tokens": 859,
    "translated_title": "大型语言模型中的提示链对于长篇法律文件分类的应用",
    "translated_abstract": "提示用于引导或指导语言模型生成与所需结果一致的适当回应。链接是一种将复杂任务分解为较小、可管理组件的策略。在本研究中，我们利用提示链对复杂的法律文件分类任务进行了广泛应用，这些任务由于其复杂的领域特定语言和 considerable length 的长度而具有困难。我们的方法从创建原始文件的简洁摘要开始，然后从训练语料库中进行语义搜索，寻找相关的示例文本及其相应的注释。最后，我们根据任务提示一个标签，通过利用少量提示中的上下文学习来指定标签。我们证明了通过提示链，我们不仅可以提高零样本任务的性能，还可以超过较大模型（如ChatGPT零样本）使用较小模型所达到的微F1得分。",
    "tldr": "本研究利用提示链的方法在处理复杂的法律文件分类任务时取得了成功，通过创建简洁摘要、语义搜索相关示例文本和利用上下文学习进行提示，可以提高零样本任务的性能并超过较大模型的得分。",
    "en_tdlr": "This study demonstrates the successful application of prompt chaining in tackling complex legal document classification tasks. By creating concise summaries, performing semantic searches for related exemplar texts, and utilizing in-context learning from prompts, the performance of zero-shot tasks can be improved, surpassing that achieved by larger models."
}