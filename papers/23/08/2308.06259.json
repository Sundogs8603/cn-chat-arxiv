{
    "title": "Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])",
    "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
    "link": "http://arxiv.org/abs/2308.06259",
    "context": "Title: Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])\nAbstract: We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
    "path": "papers/23/08/2308.06259.json",
    "total_tokens": 835,
    "translated_title": "使用指令反向翻译的自动对齐方法",
    "translated_abstract": "我们提出了一种可扩展的方法，通过自动为人工编写的文本添加相应的指令标签来构建高质量的指令跟踪语言模型。我们的方法名为指令反向翻译，它从在少量种子数据和给定的网络语料库上微调的语言模型开始。种子模型用于通过为网络文档生成指令提示（自我增强）来构建训练示例，然后从这些候选示例中选择高质量的示例（自我筛选）。然后使用这些数据来微调更强的模型。通过使用我们方法的两次迭代来微调LLaMa，我们得到的模型在Alpaca排行榜上击败了所有其他基于LLaMa的模型，而无需依赖蒸馏数据，展示了非常有效的自动对齐能力。",
    "tldr": "本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。"
}