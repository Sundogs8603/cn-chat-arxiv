{
    "title": "Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience. (arXiv:2308.03712v2 [cs.CV] UPDATED)",
    "abstract": "This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. T",
    "link": "http://arxiv.org/abs/2308.03712",
    "context": "Title: Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience. (arXiv:2308.03712v2 [cs.CV] UPDATED)\nAbstract: This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. T",
    "path": "papers/23/08/2308.03712.json",
    "total_tokens": 887,
    "translated_title": "通过缩放可能可以实现与人类相似的视觉经验和人类级别的物体识别能力",
    "translated_abstract": "本文研究了当前自监督学习方法是否能够通过足够的缩放来达到与人类学习相同类型和数量的视觉经验的人类级别视觉物体识别能力。以往的研究只考虑了数据规模的缩放。在本研究中，我们考虑数据规模、模型规模和图像分辨率的同时缩放。我们进行了一次缩放实验，使用了最多633M参数规模（ViT-H/14）的视觉转换器，以长达5K小时的类人视频数据（长时间连续、主要为自我中心的视频）进行训练，图像分辨率高达476x476像素。作为一种自监督学习算法，掩蔽自编码器（MAEs）的高效性使得可以在普通的学术预算下进行这种缩放实验。我们发现，通过同时缩放这些因素，即使在亚人类尺度的模型规模、数据规模和图像尺寸下，实现人类级别的物体识别能力是可行的。",
    "tldr": "本文研究了通过同时缩放数据规模、模型规模和图像分辨率，利用自监督学习算法能够以非人类尺度的条件实现人类级别的物体识别能力。"
}