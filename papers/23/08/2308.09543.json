{
    "title": "Latent State Models of Training Dynamics. (arXiv:2308.09543v3 [cs.LG] UPDATED)",
    "abstract": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image clas",
    "link": "http://arxiv.org/abs/2308.09543",
    "context": "Title: Latent State Models of Training Dynamics. (arXiv:2308.09543v3 [cs.LG] UPDATED)\nAbstract: The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image clas",
    "path": "papers/23/08/2308.09543.json",
    "total_tokens": 851,
    "translated_title": "潜在状态模型的训练动态",
    "translated_abstract": "随机性对模型训练的影响尚未得到很好的理解。数据顺序和初始化的差异如何实际体现在模型中，以至于一些训练运行表现出色或收敛更快？此外，我们如何解释产生的训练动态以及表征不同轨迹的相变？为了理解随机性对神经网络训练的动态和结果的影响，我们使用不同的随机种子多次训练模型，并在训练过程中计算各种指标，如神经网络权重的$L_2$范数、均值和方差。然后，我们在这些指标的结果序列上拟合了一个隐马尔可夫模型(HMM)。HMM表示训练过程是一个在潜在状态之间转换的随机过程，提供了对训练过程中显著变化的直观概述。使用我们的方法，我们对grokking任务、图像分类等训练动态进行了低维、离散表示。",
    "tldr": "这项研究使用隐马尔可夫模型解释了神经网络训练过程中随机性的影响，并提供了对训练动态的直观概述。",
    "en_tdlr": "This study utilizes hidden Markov models to explain the impact of randomness in neural network training and provides an intuitive overview of the training dynamics."
}