{
    "title": "Bridging Distribution Learning and Image Clustering in High-dimensional Space. (arXiv:2308.15667v1 [cs.LG])",
    "abstract": "Distribution learning focuses on learning the probability density function from a set of data samples. In contrast, clustering aims to group similar objects together in an unsupervised manner. Usually, these two tasks are considered unrelated. However, the relationship between the two may be indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge. In this paper, we focus on exploring the correlation between distribution learning and clustering, with the motivation to fill the gap between these two fields, utilizing an autoencoder (AE) to encode images into a high-dimensional latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler (KL) divergence loss are used to fit the Gaussian components of the GMM and learn the data distribution. Finally, image clustering is achieved through each Gaussian component of GMM. Yet, the \"curse of dimensionality\" poses severe challenges for most clustering algorithms. Compared with the classic Expectation-Maximiz",
    "link": "http://arxiv.org/abs/2308.15667",
    "context": "Title: Bridging Distribution Learning and Image Clustering in High-dimensional Space. (arXiv:2308.15667v1 [cs.LG])\nAbstract: Distribution learning focuses on learning the probability density function from a set of data samples. In contrast, clustering aims to group similar objects together in an unsupervised manner. Usually, these two tasks are considered unrelated. However, the relationship between the two may be indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge. In this paper, we focus on exploring the correlation between distribution learning and clustering, with the motivation to fill the gap between these two fields, utilizing an autoencoder (AE) to encode images into a high-dimensional latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler (KL) divergence loss are used to fit the Gaussian components of the GMM and learn the data distribution. Finally, image clustering is achieved through each Gaussian component of GMM. Yet, the \"curse of dimensionality\" poses severe challenges for most clustering algorithms. Compared with the classic Expectation-Maximiz",
    "path": "papers/23/08/2308.15667.json",
    "total_tokens": 944,
    "translated_title": "在高维空间中将分布学习与图像聚类相结合",
    "translated_abstract": "分布学习关注于从一组数据样本中学习概率密度函数，而聚类旨在以无监督的方式将相似的对象分组在一起。通常情况下，这两个任务被认为是无关的。然而，两者之间的关系可能存在间接的相关性，其中高斯混合模型（GMM）起到了桥梁作用。本文致力于探索分布学习与聚类之间的相关性，并利用自编码器（AE）将图像编码成高维潜空间，以填补这两个领域之间的差距。然后，利用蒙特卡洛边缘化（MCMarg）和Kullback-Leibler（KL）散度损失来拟合GMM的高斯分量和学习数据分布。最后，通过GMM的每个高斯分量实现图像聚类。然而，“维数灾难”给大多数聚类算法带来了严重的挑战。与经典的期望最大化相比，我们的方法在高维空间中能更好地处理图像聚类问题。",
    "tldr": "本文通过利用自编码器编码图像到高维潜空间并使用高斯混合模型进行分布学习，实现了图像聚类。然而，高维空间给聚类算法带来了挑战。",
    "en_tdlr": "This paper explores the correlation between distribution learning and clustering, bridging the gap between these two fields by utilizing an autoencoder to encode images into a high-dimensional space. Image clustering is achieved through the Gaussian components of a Gaussian Mixture Model. However, the curse of dimensionality poses challenges for clustering algorithms."
}