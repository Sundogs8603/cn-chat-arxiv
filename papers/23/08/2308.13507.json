{
    "title": "Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])",
    "abstract": "Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf",
    "link": "http://arxiv.org/abs/2308.13507",
    "context": "Title: Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])\nAbstract: Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf",
    "path": "papers/23/08/2308.13507.json",
    "total_tokens": 832,
    "translated_title": "提问澄清问题是否增加了生成代码的信心？关于大型语言模型的沟通能力的研究",
    "translated_abstract": "大型语言模型(LLMs)显著提高了代码生成任务的能力。然而，LLMs在成为顶级软件工程师方面仍存在差距。基于观察到顶级软件工程师通常会提出澄清问题以减少需求和编码解决方案的不确定性，我们认为在代码生成任务中，LLMs也应该采用同样的方法。通过在生成最终代码之前提出深入的问题，可以减轻使用LLMs进行编程所面临的挑战，如意图规范不明确、计算思维不足和代码质量不理想。这反过来增加了对生成代码的自信。在这项工作中，我们探讨如何利用更好的沟通技巧来增加对生成代码的信心。我们提出了一个以沟通为中心的过程，利用LLM生成的沟通器来识别高度不确定或信心低的问题。",
    "tldr": "通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。",
    "en_tdlr": "Asking clarifying questions before code generation can improve the code generation capability of large language models and increase confidence in the generated code."
}