{
    "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length. (arXiv:2308.05585v1 [cs.AI])",
    "abstract": "The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in shaping the impact of large language models (LLMs), contributing significantly to controlling output toxicity and selecting output styles, particularly as LLMs often harbor misleading content, highlighting the urgency to align them with human values for secure AI systems. The RLHF, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple task designed to employ Gloden as a reward model that validates the effectiveness of PPO and inspires it, primarily explaining the task of utilizing PPO to manipulate the tokenizer length of the output generated by the model. Experiments confirm that PPO is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhib",
    "link": "http://arxiv.org/abs/2308.05585",
    "context": "Title: Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length. (arXiv:2308.05585v1 [cs.AI])\nAbstract: The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in shaping the impact of large language models (LLMs), contributing significantly to controlling output toxicity and selecting output styles, particularly as LLMs often harbor misleading content, highlighting the urgency to align them with human values for secure AI systems. The RLHF, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple task designed to employ Gloden as a reward model that validates the effectiveness of PPO and inspires it, primarily explaining the task of utilizing PPO to manipulate the tokenizer length of the output generated by the model. Experiments confirm that PPO is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhib",
    "path": "papers/23/08/2308.05585.json",
    "total_tokens": 894,
    "translated_title": "接近策略优化实战：操作输出标记器长度",
    "translated_abstract": "从人类反馈中进行强化学习在塑造大型语言模型的影响方面起着关键作用，对于控制输出的毒性和选择输出风格做出了重要贡献，尤其是由于大型语言模型经常包含误导性内容，迫切需要将它们与人类价值观相一致以确保安全的人工智能系统。强化学习从人类反馈中主要存在复杂性、不稳定性和对超参数敏感性等特点，这使得对于复杂任务中奖励模型的评估变得困难，进而进一步增加了使用接近策略优化的复杂度。本文介绍了一项简单任务，旨在利用Gloden作为奖励模型，验证接近策略优化的有效性，并从中获得启示，主要解释了利用接近策略优化来操纵模型生成的输出的标记器长度的任务。实验证实，在这类任务中，接近策略优化不仅在一定程度上能够有效地操纵输出的标记器长度，还展示了其他的特性。",
    "tldr": "本文介绍了一种使用接近策略优化来操作模型输出标记器长度的方法，在复杂任务中验证了其有效性，并且发现了其他的特性。",
    "en_tdlr": "This paper introduces a method using Proximal Policy Optimization to manipulate the tokenizer length of the model's output and validates its effectiveness in complex tasks, uncovering additional features."
}