{
    "title": "ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts. (arXiv:2308.11236v1 [cs.RO])",
    "abstract": "In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to",
    "link": "http://arxiv.org/abs/2308.11236",
    "context": "Title: ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts. (arXiv:2308.11236v1 [cs.RO])\nAbstract: In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to",
    "path": "papers/23/08/2308.11236.json",
    "total_tokens": 928,
    "translated_title": "ROSGPT_Vision: 仅使用语言模型提示来控制机器人",
    "translated_abstract": "本文认为，下一代机器人可以仅通过语言模型的提示来进行命令。每个提示通过其模态语言模型（MLM）单独查询特定的机器人模态。中央任务模态通过大型语言模型（LLM）调节整个通信以执行机器人任务。本文将这种新的机器人设计模式命名为：Prompting Robotic Modalities（PRM）。此外，本文将这个PRM设计模式应用于构建一个名为ROSGPT_Vision的新的机器人框架。ROSGPT_Vision只需要两个提示即可执行机器人任务：一个是视觉提示，一个是LLM提示。视觉提示以自然语言提取与所考虑任务相关的视觉语义特征（视觉机器人模态）。同时，LLM提示调节机器人对视觉描述的反应（任务模态）。该框架自动化了这两个提示背后的所有机制。该框架使机器人能够...",
    "tldr": "本文提出了一种新的机器人设计模式，名为Prompting Robotic Modalities（PRM），通过仅使用语言模型的提示来控制机器人。并且在构建了一个名为ROSGPT_Vision的机器人框架上应用了这种设计模式。这个框架能够通过视觉提示和LLM提示执行机器人任务。",
    "en_tdlr": "This paper introduces a new robotic design pattern called Prompting Robotic Modalities (PRM) which enables commanding robots using only language models' prompts. It applies this design pattern in building a robotic framework named ROSGPT_Vision, allowing the execution of robotic tasks with visual and LLM prompts."
}