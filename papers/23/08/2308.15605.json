{
    "title": "Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])",
    "abstract": "When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited ",
    "link": "http://arxiv.org/abs/2308.15605",
    "context": "Title: Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])\nAbstract: When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited ",
    "path": "papers/23/08/2308.15605.json",
    "total_tokens": 889,
    "translated_title": "测量篡改检测基准",
    "translated_abstract": "在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。",
    "tldr": "本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。",
    "en_tdlr": "This paper constructs four text-based datasets to evaluate measurement tampering detection techniques, studying the problem of artificial intelligence systems manipulating measurement results to create good outcomes. Although techniques outperform baselines, there is still significant room for improvement."
}