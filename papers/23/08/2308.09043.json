{
    "title": "Kernel-Based Tests for Likelihood-Free Hypothesis Testing. (arXiv:2308.09043v1 [stat.ML])",
    "abstract": "Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\\infty$) the problem is solved optimally by the likelihood-ratio test; when $m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax ",
    "link": "http://arxiv.org/abs/2308.09043",
    "context": "Title: Kernel-Based Tests for Likelihood-Free Hypothesis Testing. (arXiv:2308.09043v1 [stat.ML])\nAbstract: Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\\infty$) the problem is solved optimally by the likelihood-ratio test; when $m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax ",
    "path": "papers/23/08/2308.09043.json",
    "total_tokens": 966,
    "translated_title": "基于核的无似然假设检验方法",
    "translated_abstract": "从两个平衡类别的n个观测中，考虑对额外m个已知属于其中一个类别的输入进行分类的任务。该问题的特殊情况已经被广泛研究：当完全了解类别分布时（n=∞），最优解是使用似然比检验；当m=1时，对应二分类问题；当m≈n时，等同于两样本检验。中间的情况出现在无似然推断领域，其中标记样本通过运行正向模拟获得，而未标记样本通过实验收集。最近的研究发现，m和n之间存在根本性的权衡：增加数据样本m会减少所需的训练/模拟数据量n。在本研究中，我们（a）引入了一个常常遇到的情况，即未标记样本来自两个类别的混合物；（b）研究了最小化风险的方法，其中风险定义为误分类概率的上界。",
    "tldr": "本文介绍了一种基于核的无似然假设检验方法，解决了对已知属于两个类别的输入进行分类的问题，在无似然推断领域，通过将标记样本通过正向模拟获得，未标记样本通过实验收集，给出了一个权衡m和n的方法。",
    "en_tdlr": "This paper introduces a kernel-based method for likelihood-free hypothesis testing, addressing the problem of classifying inputs known to belong to two classes. In the field of likelihood-free inference, where labeled samples are obtained through forward simulations and unlabeled samples are collected experimentally, the paper presents a trade-off approach between the number of labeled samples (m) and the amount of training/simulation data needed (n)."
}