{
    "title": "\"Beware of deception\": Detecting Half-Truth and Debunking it through Controlled Claim Editing. (arXiv:2308.07973v1 [cs.CL])",
    "abstract": "The prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. To help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. Our approach utilizes the T5 model for controlled claim editing; \"controlled\" here means precise adjustments to select parts of a claim. Our methodology achieves an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. Significantly, our T5-based approach outperforms other Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the half-truth detection model, setting a new benchmark in the field. While previous attempts have been made at half-truth detection, our approach is, to the b",
    "link": "http://arxiv.org/abs/2308.07973",
    "context": "Title: \"Beware of deception\": Detecting Half-Truth and Debunking it through Controlled Claim Editing. (arXiv:2308.07973v1 [cs.CL])\nAbstract: The prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. To help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. Our approach utilizes the T5 model for controlled claim editing; \"controlled\" here means precise adjustments to select parts of a claim. Our methodology achieves an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. Significantly, our T5-based approach outperforms other Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the half-truth detection model, setting a new benchmark in the field. While previous attempts have been made at half-truth detection, our approach is, to the b",
    "path": "papers/23/08/2308.07973.json",
    "total_tokens": 1057,
    "translated_title": "\"谨防欺骗\": 通过受控声明编辑检测半真相并揭穿",
    "translated_abstract": "随着互联网的广泛使用，半真相即包含一些真实信息但最终具有欺骗性的陈述越来越多。为了应对这个问题，我们创建了一个包括半真相检测模型和声明编辑模型的全面流程。我们的方法利用了T5模型进行受控声明编辑；这里的“受控”意味着对声明的选定部分进行精确调整。我们的方法在编辑后的声明上取得了平均BLEU分数0.88（在0-1的范围内）和85%的虚假信息揭穿得分。值得注意的是，我们基于T5的方法在虚假信息揭穿得分方面优于其他语言模型，如GPT2，RoBERTa，PEGASUS和Tailor，平均改进分别为82%，57%，42%和23%。通过扩展LIAR PLUS数据集，我们在半真相检测模型方面实现了82%的F1分数，创造了该领域的新基准。尽管之前已经有人尝试过半真相检测，但我们的方法在准确性和性能上是前所未有的。",
    "tldr": "本研究针对互联网上半真相的广泛存在问题，提出了一个包括半真相检测模型和声明编辑模型的全面流程。通过利用T5模型进行受控声明编辑，我们的方法在虚假信息揭穿得分方面优于其他语言模型，并在半真相检测模型上创造了新的性能基准。",
    "en_tdlr": "This study addresses the prevalence of half-truths on the internet by proposing a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. By utilizing the T5 model for controlled claim editing, our approach outperforms other language models in debunking misinformation and sets a new performance benchmark in half-truth detection."
}