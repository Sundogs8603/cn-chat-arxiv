{
    "title": "Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles. (arXiv:2308.14521v2 [cs.AI] UPDATED)",
    "abstract": "Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using",
    "link": "http://arxiv.org/abs/2308.14521",
    "context": "Title: Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles. (arXiv:2308.14521v2 [cs.AI] UPDATED)\nAbstract: Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using",
    "path": "papers/23/08/2308.14521.json",
    "total_tokens": 868,
    "translated_title": "通过马尔可夫决策过程实体嵌入和代理集合上下文感知地组合代理策略",
    "translated_abstract": "计算代理在生活的许多领域中支持人类，并因此存在异构环境。这意味着它们在快速变化的环境中运作，并且可能面临巨大的状态和动作空间。为了以目标导向的方式执行服务和活动，代理需要先前的知识，因此必须制定和追求依赖于上下文的策略。然而，预先规定策略在动态变化的环境中存在限制和不灵活性。此外，代理的上下文决定了它的动作选择。由于环境可能具有随机性，并且在状态和可行动作的数量上复杂，因此通常通过马尔可夫决策过程以简化的方式建模活动，以便使用强化学习的代理能够学习策略，从而帮助捕捉上下文并根据最优方式执行活动。",
    "tldr": "该论文提出了一种通过马尔可夫决策过程实体嵌入和代理集合的方法，以上下文感知地组合代理策略，以在复杂且动态变化的环境中优化执行活动。",
    "en_tdlr": "This paper proposes a method for context-aware composition of agent policies by using Markov Decision Process entity embeddings and agent ensembles, aiming to optimize the execution of activities in complex and dynamically changing environments."
}