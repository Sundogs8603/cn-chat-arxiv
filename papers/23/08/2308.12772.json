{
    "title": "Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward. (arXiv:2308.12772v1 [cs.RO])",
    "abstract": "Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In ",
    "link": "http://arxiv.org/abs/2308.12772",
    "context": "Title: Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward. (arXiv:2308.12772v1 [cs.RO])\nAbstract: Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In ",
    "path": "papers/23/08/2308.12772.json",
    "total_tokens": 867,
    "translated_title": "通过故意低估终止状态的值函数来解决时间差异学习中的错误奖励问题",
    "translated_abstract": "使用强化学习的机器人控制已经变得流行，但是由于安全和节约时间的原因，学习过程通常在一集的中途终止。本研究解决了在这种终止情况下时间差异（TD）学习执行的最常见异常处理问题。也就是说，在终止后通过强制假设值为零，会意外地引起低估或高估，这取决于正常状态下的奖励设计。当任务失败导致一集终止时，由于非预期的高估，失败可能被高度评价，并且可能获取错误的策略。虽然通过注意奖励设计可以避免此问题，但在实际使用中审查终止时的异常处理是TD学习的要点。因此，本文提出了一种方法来故意低估终止后的值，以避免由于非预期的高估而导致的学习失败。",
    "tldr": "本文提出了一种方法来解决在时间差异学习中终止状态下存在的奖励设计问题，通过故意低估终止后的值来避免学习错误的策略。",
    "en_tdlr": "This paper proposes a method to address the reward design issue in temporal-difference learning at terminal states by intentionally underestimating the value after termination, avoiding the acquisition of erroneous policies caused by unintentional overestimation."
}