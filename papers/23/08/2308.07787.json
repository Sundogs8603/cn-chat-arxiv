{
    "title": "DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding. (arXiv:2308.07787v1 [cs.SD])",
    "abstract": "Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations",
    "link": "http://arxiv.org/abs/2308.07787",
    "context": "Title: DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding. (arXiv:2308.07787v1 [cs.SD])\nAbstract: Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations",
    "path": "papers/23/08/2308.07787.json",
    "total_tokens": 859,
    "translated_title": "DiffV2S: 基于扩散的视频转语音合成与视觉引导的说话人嵌入",
    "translated_abstract": "最近的研究在视频转语音合成方面取得了令人印象深刻的结果，即仅通过视觉输入重建语音。然而，由于缺乏足够的指导来正确推断出适当声音的内容，以前的工作在合成语音方面仍存在困难。为了解决这个问题，他们采用额外的说话人嵌入作为来自参考听觉信息的说话风格指导。然而，在推断过程中，往往无法从相应的视频输入中获得音频信息。本文提出了一种新颖的使用自我监督预训练模型和提示调整技术的视觉引导说话人嵌入提取器。通过这样做，可以仅通过输入的视觉信息产生丰富的说话人嵌入信息，在推断过程中不需要额外的音频信息。",
    "tldr": "本文提出了一种基于扩散的视频转语音合成方法，采用视觉引导说话人嵌入的方式，可以在推断过程中不需要额外的音频信息，仅通过输入的视觉信息即可产生丰富的说话人嵌入信息。",
    "en_tdlr": "This paper proposes a diffusion-based method for video-to-speech synthesis with vision-guided speaker embedding. The method eliminates the need for extra audio information during inference by extracting rich speaker embedding solely from input visual information."
}