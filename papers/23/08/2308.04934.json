{
    "title": "JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition. (arXiv:2308.04934v1 [cs.CV])",
    "abstract": "We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate",
    "link": "http://arxiv.org/abs/2308.04934",
    "context": "Title: JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition. (arXiv:2308.04934v1 [cs.CV])\nAbstract: We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate",
    "path": "papers/23/08/2308.04934.json",
    "total_tokens": 849,
    "translated_title": "JEDI：一种用于视频动作识别的半监督多数据集学生-教师联合专家蒸馏方法",
    "translated_abstract": "我们提出了JEDI，一种多数据集半监督学习方法，能够有效地结合来自不同数据集上训练的多个专家的知识，以训练和提高单个数据集上的学生模型的性能。我们的方法解决了当前机器学习研究中的两个重要问题：跨数据集的泛化和有限标签数据导致的监督训练限制。我们从任意数量的专家开始，他们在各自特定数据集上进行预训练，形成初始的学生模型集合。教师立即通过连接学生倒数第二层的特征表示派生出来。然后，我们在学生-教师半监督学习场景中同时训练所有模型，直至收敛。我们的高效方法表明，学生和教师在训练过程中都提高了泛化能力。我们对该方法进行了验证。",
    "tldr": "JEDI是一种利用多个数据集的专家知识进行训练的半监督学习方法，能够解决跨数据集泛化和标签数据稀缺等问题。",
    "en_tdlr": "JEDI is a semi-supervised learning method that efficiently combines knowledge from multiple datasets to address generalization across datasets and limitations of supervised training due to scarcity of labeled data."
}