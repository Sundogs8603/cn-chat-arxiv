{
    "title": "Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])",
    "abstract": "This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-la",
    "link": "http://arxiv.org/abs/2308.01727",
    "context": "Title: Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])\nAbstract: This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-la",
    "path": "papers/23/08/2308.01727.json",
    "total_tokens": 897,
    "translated_title": "针对复杂结构化医学任务的本地大型语言模型",
    "translated_abstract": "本文介绍了一种将大型语言模型 (LLMs) 的语言推理能力与本地训练的优势相结合，以应对复杂的领域特定任务的方法。具体而言，作者通过从病理报告中提取结构化疾病代码来展示他们的方法。提出的方法利用本地LLMs，可以进行针对特定生成指令的微调，并提供结构化输出。作者收集了一个包含超过15万个未编辑的外科病理报告的数据集，其中包含了外观描述、最终诊断和疾病代码。他们训练了不同的模型架构，包括LLaMA、BERT和LongFormer，并评估了它们的性能。结果显示，基于LLaMA的模型在所有评估指标上明显优于BERT风格的模型，即使精确性大幅降低。LLaMA模型在大型数据集上表现特别出色，展示了它们处理复杂的多任务问题的能力。",
    "tldr": "本文介绍了一种利用本地大型语言模型 (LLMs) 与本地训练相结合的方法，用于处理复杂的结构化医学任务。作者提出的方法通过从病理报告中提取疾病代码来展示其效果，结果表明利用LLaMA模型相较于BERT风格的模型，在各种评估指标上具有明显的优势。而且，LLaMA模型在处理大型数据集方面表现特别出色。"
}