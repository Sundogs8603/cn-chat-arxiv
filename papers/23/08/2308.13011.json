{
    "title": "Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory. (arXiv:2308.13011v1 [cs.LG])",
    "abstract": "Risk-sensitive reinforcement learning (RL) has garnered significant attention in recent years due to the growing interest in deploying RL agents in real-world scenarios. A critical aspect of risk awareness involves modeling highly rare risk events (rewards) that could potentially lead to catastrophic outcomes. These infrequent occurrences present a formidable challenge for data-driven methods aiming to capture such risky events accurately. While risk-aware RL techniques do exist, their level of risk aversion heavily relies on the precision of the state-action value function estimation when modeling these rare occurrences. Our work proposes to enhance the resilience of RL agents when faced with very rare and risky events by focusing on refining the predictions of the extreme values predicted by the state-action value function distribution. To achieve this, we formulate the extreme values of the state-action value function distribution as parameterized distributions, drawing inspiration ",
    "link": "http://arxiv.org/abs/2308.13011",
    "context": "Title: Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory. (arXiv:2308.13011v1 [cs.LG])\nAbstract: Risk-sensitive reinforcement learning (RL) has garnered significant attention in recent years due to the growing interest in deploying RL agents in real-world scenarios. A critical aspect of risk awareness involves modeling highly rare risk events (rewards) that could potentially lead to catastrophic outcomes. These infrequent occurrences present a formidable challenge for data-driven methods aiming to capture such risky events accurately. While risk-aware RL techniques do exist, their level of risk aversion heavily relies on the precision of the state-action value function estimation when modeling these rare occurrences. Our work proposes to enhance the resilience of RL agents when faced with very rare and risky events by focusing on refining the predictions of the extreme values predicted by the state-action value function distribution. To achieve this, we formulate the extreme values of the state-action value function distribution as parameterized distributions, drawing inspiration ",
    "path": "papers/23/08/2308.13011.json",
    "total_tokens": 902,
    "translated_title": "使用极值理论在强化学习中的极端风险缓解",
    "translated_abstract": "近年来，由于在现实世界中部署强化学习智能体的兴趣增加，风险敏感的强化学习 (RL) 引起了广泛关注。风险意识的一个关键方面是对可能导致灾难性结果的高度罕见的风险事件 (奖励) 进行建模。这些罕见事件对于旨在准确捕捉此类风险事件的数据驱动方法来说是一个巨大的挑战。尽管存在风险感知的 RL 技术，当建模这些罕见事件时，它们的风险规避水平严重依赖于状态-动作值函数估计的精度。我们的工作提出通过专注于改进状态-动作值函数分布预测的极值来增强 RL 智能体在面对非常罕见和危险事件时的韧性。为了实现这一目标，我们将状态-动作值函数分布的极值表示为参数化分布，并从中获得灵感",
    "tldr": "本研究提出了一种使用极值理论在强化学习中缓解极端风险的方法，通过改进状态-动作值函数分布预测的极值来增强RL智能体在面对非常罕见和危险事件时的韧性。",
    "en_tdlr": "This paper proposes a method for mitigating extreme risks in reinforcement learning using extreme value theory, by enhancing the resilience of RL agents when facing very rare and risky events through refining the predictions of the extreme values of the state-action value function distribution."
}