{
    "title": "The Poison of Alignment. (arXiv:2308.13449v1 [cs.CL])",
    "abstract": "From the perspective of content safety issues, alignment has shown to limit large language models' (LLMs) harmful content generation. This intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. To be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP), performing worse than the counterpart tuned without alignment by 4-33%.",
    "link": "http://arxiv.org/abs/2308.13449",
    "context": "Title: The Poison of Alignment. (arXiv:2308.13449v1 [cs.CL])\nAbstract: From the perspective of content safety issues, alignment has shown to limit large language models' (LLMs) harmful content generation. This intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. To be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP), performing worse than the counterpart tuned without alignment by 4-33%.",
    "path": "papers/23/08/2308.13449.json",
    "total_tokens": 752,
    "translated_title": "对齐的毒性",
    "translated_abstract": "从内容安全问题的角度来看，对齐已经显示出限制大型语言模型（LLM）生成有害内容的能力。这种有意的方法是为了不让模型对某些用户输入作出响应，在许多现代开源指令调整数据集（如OpenAssistant或Guanaco）中都存在。我们引入了一个新颖的观点，指出对齐对指令调整模型的性能产生了影响。具体而言，我们注意到对齐的作用就像是对指令数据集进行了污染。通过实验证明，对齐的答案显著地降低了最终微调模型在各种推理基准测试（如Big Bench（BBH）、大规模多任务语言理解（MMLU）、人工评估和段落离散推理（DROP））上的性能，相比没有对齐的微调模型下降了4-33%。",
    "tldr": "对齐行为会污染指令数据集，降低微调模型的性能。",
    "en_tdlr": "Alignment poisons the instruction dataset and deteriorates the performance of fine-tuned models."
}