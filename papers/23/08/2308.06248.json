{
    "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods. (arXiv:2308.06248v1 [cs.CV])",
    "abstract": "The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different e",
    "link": "http://arxiv.org/abs/2308.06248",
    "context": "Title: FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods. (arXiv:2308.06248v1 [cs.CV])\nAbstract: The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different e",
    "path": "papers/23/08/2308.06248.json",
    "total_tokens": 869,
    "translated_title": "FunnyBirds: 一种用于可解释AI方法的基于部分分析的合成视觉数据集",
    "translated_abstract": "可解释人工智能（XAI）领域旨在揭示复杂深度神经模型的内部工作原理。尽管在安全关键领域至关重要，但XAI固有地缺乏真实解释，使其自动评估成为一个未解决的问题。我们通过提出一种新的合成视觉数据集FunnyBirds及其伴随的自动评估协议来应对这一挑战。我们的数据集允许进行语义上有意义的图像干预，例如，移除单个物体部分，这有三个重要的含义。首先，它使得能够对部分级别的解释进行分析，比现有方法在像素级别进行评估更接近于人类理解。其次，通过比较去除部分的输入的模型输出，我们可以估计应该反映在解释中的真实部分重要性。第三，通过将单个解释映射到共同的部分重要性空间，我们可以分析不同的解释。",
    "tldr": "FunnyBirds是一种合成视觉数据集，用于分析可解释AI方法。它允许对图像进行语义上有意义的干预，并可以在部分级别上对解释进行评估和分析。",
    "en_tdlr": "FunnyBirds is a synthetic vision dataset for analyzing explainable AI methods. It allows semantically meaningful interventions on images and enables evaluation and analysis of explanations at the part level."
}