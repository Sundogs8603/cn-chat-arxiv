{
    "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])",
    "abstract": "Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This surv",
    "link": "http://arxiv.org/abs/2308.04268",
    "context": "Title: Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])\nAbstract: Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This surv",
    "path": "papers/23/08/2308.04268.json",
    "total_tokens": 893,
    "translated_title": "知识蒸馏的师生架构：综述",
    "translated_abstract": "尽管深度神经网络（DNN）在许多领域的大规模问题中展现出强大的解决能力，但由于参数庞大，这种DNN很难应用于现实世界的系统中。为解决这个问题，提出了师生架构，其中简单的学生网络只有少量参数，却能达到与拥有许多参数的深度师傅网络相当的性能。最近，师生架构已被广泛应用于知识蒸馏的多个目标领域，包括知识压缩、知识扩展、知识适应和知识增强。借助师生架构，目前的研究能够通过轻量化和通用化的学生网络实现多个蒸馏目标。与现有的主要关注知识压缩的蒸馏调查不同，本调查首次探讨了跨多个蒸馏目标的师生架构。",
    "tldr": "本文综述了知识蒸馏的师生架构，在多个蒸馏目标领域中取得了有效并广泛的应用，包括知识压缩、知识扩展、知识适应和知识增强，通过轻量化和通用化的学生网络实现多个蒸馏目标。"
}