{
    "title": "To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks. (arXiv:2308.09955v1 [cs.LG])",
    "abstract": "Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.",
    "link": "http://arxiv.org/abs/2308.09955",
    "context": "Title: To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks. (arXiv:2308.09955v1 [cs.LG])\nAbstract: Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.",
    "path": "papers/23/08/2308.09955.json",
    "total_tokens": 834,
    "translated_title": "是修剪还是不修剪：一种基于混沌因果性的稠密神经网络有原则修剪方法",
    "translated_abstract": "在资源受限设备上通过去除权重而不影响性能来减小神经网络的大小（修剪），是一个重要的问题。过去，修剪通常是通过基于大小等标准对权重进行排序或惩罚，并在重新训练剩余权重之前去除低排名权重来实现的。修剪策略也可以涉及从网络中删除神经元，以实现所需的网络尺寸减小。我们将修剪问题定式为优化问题，目标是通过选择特定权重来最小化错分类。为实现这一目标，我们引入了学习中的混沌概念（李亚普诺夫指数），通过权重更新并利用因果关系来识别造成错分类的因果性权重。这样修剪后的网络保持了原始性能并保留了特征的可解释性。",
    "tldr": "这个论文提出了一种基于混沌因果性的稠密神经网络有原则修剪方法，通过选择特定权重来最小化错分类，修剪后的网络保持了原始性能和特征的可解释性。",
    "en_tdlr": "This paper presents a principled pruning method for dense neural networks based on chaos-causality approach, aiming to minimize misclassifications by selecting specific weights. The pruned network maintains the original performance and retains feature explainability."
}