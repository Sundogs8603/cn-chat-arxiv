{
    "title": "Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge. (arXiv:2308.15700v1 [cs.HC])",
    "abstract": "A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate \"appropriate reliance\" by comparing each decision against a ground truth label that cleanly maps to both the AI's predictive target and the human decision-maker's goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human's ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providi",
    "link": "http://arxiv.org/abs/2308.15700",
    "context": "Title: Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge. (arXiv:2308.15700v1 [cs.HC])\nAbstract: A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate \"appropriate reliance\" by comparing each decision against a ground truth label that cleanly maps to both the AI's predictive target and the human decision-maker's goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human's ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providi",
    "path": "papers/23/08/2308.15700.json",
    "total_tokens": 1024,
    "translated_title": "训练朝向批判使用：学习将人工智能预测置于人类知识之中",
    "translated_abstract": "越来越多的研究探讨如何支持人们更好地利用基于人工智能的决策支持，包括通过培训和入门指导。现有的研究集中在决策任务上，这些任务可以通过比较每个决策与现实标签的一致性来评估“适当的依赖”，这些标签清晰地映射到人工智能的预测目标和人类决策者的目标。然而，在许多现实世界的环境中，这种假设不成立，因为人工智能工具被部署在社会工作、刑事司法和医疗保健等领域。在本文中，我们引入了一种过程导向的适当依赖概念，称为批判使用，其将人类能够将人工智能预测置于他们独特的知识之中，而这些知识对于人工智能模型来说是不可获得的。为了探索训练如何支持批判使用，我们在一个复杂的社会决策场景中进行了一项随机在线实验：儿童虐待筛查。我们发现，通过提供...",
    "tldr": "本文介绍了一种过程导向的适当依赖概念，称为批判使用，旨在帮助人们更好地利用基于人工智能的决策支持。研究通过在儿童虐待筛查领域进行在线实验，发现通过提供特定培训可以支持人们的批判使用能力。",
    "en_tdlr": "This paper introduces a process-oriented notion of appropriate reliance called critical use, aiming to help humans make better use of AI-based decision support. The study conducted an online experiment in the field of child maltreatment screening and found that providing specific training can support people's critical use ability."
}