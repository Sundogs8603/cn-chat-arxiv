{
    "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])",
    "abstract": "Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel",
    "link": "http://arxiv.org/abs/2308.12711",
    "context": "Title: Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])\nAbstract: Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel",
    "path": "papers/23/08/2308.12711.json",
    "total_tokens": 885,
    "translated_title": "发挥大卫对战歌利亚的力量：探索不使用闭源模型的指令数据生成",
    "translated_abstract": "指令调整对于使大规模语言模型（LLMs）能够按照用户的指令完成各种开放领域任务至关重要。指令调整的成功依赖于高质量的指令数据的可用性。由于人工注释的高昂成本和质量不佳，最近的研究一直在深入探索利用强大的闭源模型自动生成指令数据。然而，这些方法存在潜在的风险，因为强大的闭源模型的使用要求严禁利用它们的输出来开发机器学习模型。为了解决这个问题，在这项工作中，我们探索了不依赖闭源模型的替代方法来生成高质量的指令数据。我们的探索包括对各种现有的指令生成方法的研究，最终将效率最高的变体与两种新颖的方法进行整合。",
    "tldr": "本文研究了不依赖闭源模型的方法生成高质量的指令数据，以应对使用闭源模型带来的潜在风险。在探索中，我们将现有的指令生成方法进行了调查，并将效率最高的方法与两种新颖的方法相结合。",
    "en_tdlr": "This paper explores alternative approaches to generate high-quality instruction data that do not rely on closed-source models, addressing potential risks associated with their usage. The investigation includes examining existing instruction generation methods and integrating the most efficient variant with two novel approaches."
}