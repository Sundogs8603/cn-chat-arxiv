{
    "title": "Ensemble Learning with Residual Transformer for Brain Tumor Segmentation. (arXiv:2308.00128v1 [eess.IV])",
    "abstract": "Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize",
    "link": "http://arxiv.org/abs/2308.00128",
    "context": "Title: Ensemble Learning with Residual Transformer for Brain Tumor Segmentation. (arXiv:2308.00128v1 [eess.IV])\nAbstract: Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize",
    "path": "papers/23/08/2308.00128.json",
    "total_tokens": 918,
    "translated_title": "使用残差变压器进行脑肿瘤分割的集成学习",
    "translated_abstract": "脑肿瘤分割是一个活跃的研究领域，由于高度复杂形状和纹理的肿瘤的划分困难以及常用的 U-Net 架构的失败。最近，在主流研究中，不同神经架构的组合成为一个重要研究方向，特别是 U-Net 和注意力机制结合的 Transformer。与之前的研究不同，本文提出了一种将 Transformer 整合到自适应 U-Net 中的新型网络架构，以合理的计算成本提取出 3D 体积信息。为了防止信息流失，我们进一步添加了残差连接并探索集成方法，因为评估的模型对不同案例和子区域有边缘。在 BraTS 2021 数据集上，我们的模型达到了87.6%的平均 Dice 分数，并超过了最先进的方法，展示了结合多个架构进行优化的潜力。",
    "tldr": "本文提出了一种将 Transformer 整合到 U-Net 中的新型网络架构，用于脑肿瘤分割。模型结合了注意力机制和像素级标签，通过残差连接和集成方法进一步提高了分割效果，在 BraTS 2021 数据集上取得了优于最先进方法的结果。"
}