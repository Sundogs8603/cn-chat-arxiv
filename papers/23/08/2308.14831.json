{
    "title": "Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])",
    "abstract": "Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag",
    "link": "http://arxiv.org/abs/2308.14831",
    "context": "Title: Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])\nAbstract: Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag",
    "path": "papers/23/08/2308.14831.json",
    "total_tokens": 897,
    "translated_title": "使用动态稀疏训练的持续学习：探索有效模型更新的算法",
    "translated_abstract": "持续学习是指智能系统从连续的数据流中以尽可能少的计算开销顺序获取和保留知识的能力。为了实现这一目标，已在文献中引入了正则化、重放、架构和参数隔离等方法。使用稀疏网络进行参数隔离，可以将神经网络的不同部分分配给不同的任务，并允许在任务相似时共享参数。动态稀疏训练(DST)是发现这些稀疏网络并为每个任务进行隔离的一种重要方法。本文是首个对CL范式下不同DST组件效果进行实证研究的研究，旨在填补关键研究空白并为CL下的DST的最佳配置提供指导。因此，我们在著名的CIFAR100和miniImage数据集上进行了综合研究，探究了各种DST组件以找到每个任务的最佳拓扑结构。",
    "tldr": "本文通过实证研究探究了持续学习中不同动态稀疏训练（DST）组件的影响，旨在填补关键研究空白并为持续学习下的DST提供最佳配置指导。"
}