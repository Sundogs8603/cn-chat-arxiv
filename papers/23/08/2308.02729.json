{
    "title": "Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks. (arXiv:2308.02729v1 [cs.LG])",
    "abstract": "Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical result",
    "link": "http://arxiv.org/abs/2308.02729",
    "context": "Title: Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks. (arXiv:2308.02729v1 [cs.LG])\nAbstract: Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical result",
    "path": "papers/23/08/2308.02729.json",
    "total_tokens": 921,
    "translated_title": "使用Actor-Critic算法和ReLU网络综合编程策略",
    "translated_abstract": "程序化可解释性强化学习(PIRL)通过人类可读的计算机程序来编码策略。最近引入了一些新算法来处理在程序化策略空间中缺乏梯度信号的问题。大多数PIRL算法首先训练一个神经策略，然后将其用作导引在程序化空间中搜索的“神谕”。本文中，我们展示了在编码程序化策略的语言上，这样的PIRL特定算法是不必要的。这是因为可以使用Actor-Critic算法直接获得程序化策略。我们利用ReLU神经网络和斜率决策树之间的联系，将使用Actor-Critic算法学习到的策略转化为程序化策略。这种从ReLU网络转化使我们能够综合编码了if-then-else结构、输入值的线性变换和PID操作的策略。",
    "tldr": "本文通过使用Actor-Critic算法和ReLU网络，展示了在编码程序化策略的语言上，不需要特定的PIRL算法。使用ReLU网络可以将使用Actor-Critic算法学习到的策略转化为具有if-then-else结构、线性变换和PID操作等的程序化策略。"
}