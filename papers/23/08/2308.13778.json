{
    "title": "Large-scale gradient-based training of Mixtures of Factor Analyzers. (arXiv:2308.13778v1 [cs.LG])",
    "abstract": "Gaussian Mixture Models (GMMs) are a standard tool in data analysis. However, they face problems when applied to high-dimensional data (e.g., images) due to the size of the required full covariance matrices (CMs), whereas the use of diagonal or spherical CMs often imposes restrictions that are too severe. The Mixture of Factor analyzers (MFA) model is an important extension of GMMs, which allows to smoothly interpolate between diagonal and full CMs based on the number of \\textit{factor loadings} $l$. MFA has successfully been applied for modeling high-dimensional image data. This article contributes both a theoretical analysis as well as a new method for efficient high-dimensional MFA training by stochastic gradient descent, starting from random centroid initializations. This greatly simplifies the training and initialization process, and avoids problems of batch-type algorithms such Expectation-Maximization (EM) when training with huge amounts of data. In addition, by exploiting the p",
    "link": "http://arxiv.org/abs/2308.13778",
    "context": "Title: Large-scale gradient-based training of Mixtures of Factor Analyzers. (arXiv:2308.13778v1 [cs.LG])\nAbstract: Gaussian Mixture Models (GMMs) are a standard tool in data analysis. However, they face problems when applied to high-dimensional data (e.g., images) due to the size of the required full covariance matrices (CMs), whereas the use of diagonal or spherical CMs often imposes restrictions that are too severe. The Mixture of Factor analyzers (MFA) model is an important extension of GMMs, which allows to smoothly interpolate between diagonal and full CMs based on the number of \\textit{factor loadings} $l$. MFA has successfully been applied for modeling high-dimensional image data. This article contributes both a theoretical analysis as well as a new method for efficient high-dimensional MFA training by stochastic gradient descent, starting from random centroid initializations. This greatly simplifies the training and initialization process, and avoids problems of batch-type algorithms such Expectation-Maximization (EM) when training with huge amounts of data. In addition, by exploiting the p",
    "path": "papers/23/08/2308.13778.json",
    "total_tokens": 940,
    "translated_title": "大规模基于梯度的混合因子分析器训练",
    "translated_abstract": "高斯混合模型是数据分析中的常用工具。然而，当应用于高维数据（如图像）时，由于所需的完整协方差矩阵（CM）的大小，它们面临问题，而对角线或球形CM的使用往往会带来过于严格的限制。混合因子分析器（MFA）模型是GMM的一个重要扩展，它允许根据因子加载数平滑地插值对角线和完整CM之间的差异。MFA成功应用于建模高维图像数据。这篇文章提供了理论分析以及一种新的高维MFA训练方法，采用随机梯度下降，从随机的质心初始化开始。这极大地简化了训练和初始化过程，并避免了批处理型算法（如期望-最大化算法）在处理大量数据时的问题。此外，通过利用p",
    "tldr": "这篇论文提出了一种大规模基于梯度的混合因子分析器（MFA）训练方法，通过随机梯度下降和随机质心初始化，在处理高维数据时能够简化训练和初始化过程，并避免使用传统方法（如期望-最大化算法）处理大量数据时可能出现的问题。",
    "en_tdlr": "This paper presents a large-scale gradient-based training method for Mixtures of Factor Analyzers (MFA), which simplifies the training and initialization process for high-dimensional data using stochastic gradient descent and random centroid initializations, while avoiding issues with traditional methods like Expectation-Maximization (EM) when dealing with massive amounts of data."
}