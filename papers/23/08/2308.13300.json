{
    "title": "Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])",
    "abstract": "Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.",
    "link": "http://arxiv.org/abs/2308.13300",
    "context": "Title: Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])\nAbstract: Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.",
    "path": "papers/23/08/2308.13300.json",
    "total_tokens": 802,
    "translated_title": "使用深度超参数化多任务学习来学习紧凑的神经网络",
    "translated_abstract": "紧凑的神经网络在实际应用中具有许多优点。然而，用小参数大小和低计算成本来训练紧凑的神经网络以达到与更复杂、更强大的体系结构相同或更好的模型性能通常是具有挑战性的。这在多任务学习中尤其如此，因为不同的任务竞争资源。我们提出了一种简单、高效、有效的多任务学习超参数化神经网络设计，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。在两个具有挑战性的多任务数据集（NYUv2和COCO）上的实验证明了所提方法在各种卷积网络和参数大小上的有效性。",
    "tldr": "本文提出了一种使用深度超参数化多任务学习来学习紧凑的神经网络的方法，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。",
    "en_tdlr": "This paper presents a method for learning compact neural networks using deep overparameterised multitask learning, achieving better optimization and generalization by overparameterising the model architecture in training and effectively sharing the overparameterised model parameters across different tasks."
}