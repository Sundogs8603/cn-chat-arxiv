{
    "title": "DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (arXiv:2308.07387v1 [cs.LG])",
    "abstract": "Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \\textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clien",
    "link": "http://arxiv.org/abs/2308.07387",
    "context": "Title: DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (arXiv:2308.07387v1 [cs.LG])\nAbstract: Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \\textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clien",
    "path": "papers/23/08/2308.07387.json",
    "total_tokens": 886,
    "translated_title": "DISBELIEVE：客户模型之间的距离对于有效的本地模型污染攻击非常重要",
    "translated_abstract": "联合学习是解决分享患者敏感数据相关隐私问题的一种有前景的方向。在医学图像分析领域中，联合系统通常假设参与的本地客户端是诚实的。然而，一些研究报告了一些恶意客户端可以通过的机制，这些机制可以污染联合设置，破坏全局模型的性能。为了克服这个问题，已经提出了一些坚固的聚合方法，可以防御这些攻击。我们观察到，大多数最先进的坚固聚合方法非常依赖于恶意客户端和良性客户端的参数或梯度之间的距离，这使得它们容易受到本地模型污染攻击的影响，当恶意和良性客户端的参数或梯度很接近时。基于这一点，我们介绍了一种名为DISBELIEVE的本地模型污染攻击，它创建恶意参数或梯度，使得它们与良性客户端之间的距离很小。",
    "tldr": "DISBELIEVE 是一种本地模型污染攻击，通过在参数或梯度之间创建恶意距离与良性客户端的距离较小的恶意参数或梯度，破坏联合系统的性能。"
}