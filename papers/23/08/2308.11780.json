{
    "title": "Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])",
    "abstract": "Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi",
    "link": "http://arxiv.org/abs/2308.11780",
    "context": "Title: Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])\nAbstract: Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi",
    "path": "papers/23/08/2308.11780.json",
    "total_tokens": 877,
    "translated_title": "在文本中使用离群学习进行少样本异常检测",
    "translated_abstract": "目前大多数文本异常检测方法都集中在构建仅依赖无标签数据的模型上。这些模型基于没有可用的标记异常示例的假设运行，在许多实际应用中，这些异常通常以小数量存在，这阻碍了它们利用先前已知的异常知识。此外，这些模型更注重学习特征嵌入而不是直接优化异常得分，这可能会导致次优的异常得分和学习过程中数据的低效利用。在本文中，我们介绍了FATE，一种基于深度少样本学习的框架，它利用有限的异常示例，并使用离群学习的端到端方法明确地学习异常得分。在这种方法中，将正常示例的异常得分调整为与先前分布获得的参考得分相似。相反，异常样本被迫具有明显偏离的异常得分。",
    "tldr": "本论文介绍了一种基于深度少样本学习的框架FATE，它通过离群学习明确地学习文本中的异常得分，并利用先前已知的少量异常示例，从而克服了传统方法中对无标签数据的依赖，并优化了异常得分的精确度和数据利用效率。"
}