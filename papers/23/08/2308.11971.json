{
    "title": "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])",
    "abstract": "Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t",
    "link": "http://arxiv.org/abs/2308.11971",
    "context": "Title: EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])\nAbstract: Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t",
    "path": "papers/23/08/2308.11971.json",
    "total_tokens": 781,
    "translated_title": "EVE: 使用遮蔽预测和模态感知的高效视觉-语言预训练",
    "translated_abstract": "在本文中，我们介绍了一种名为EVE的高效视觉-语言基础模型，它是由一种统一的Transformer进行预训练的统一多模态模型。EVE通过在图像-文本对上进行遮蔽信号建模来统一视觉和语言的预训练任务，以重建可见信号，即图像像素和文本标记。通过集成模态感知的稀疏专家混合模块，EVE在一个共享的Transformer网络中编码了视觉和语言，并通过选择性地切换到不同的专家来捕捉模态特定信息。",
    "tldr": "本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。",
    "en_tdlr": "This paper introduces EVE, an efficient vision-language pre-training model that combines masked signal modeling and modality-aware techniques to achieve a unified multimodal Transformer network, accelerating training and achieving good results."
}