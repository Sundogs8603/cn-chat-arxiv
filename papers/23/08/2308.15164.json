{
    "title": "ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters. (arXiv:2308.15164v1 [cs.LG])",
    "abstract": "As the size of models and datasets grows, it has become increasingly common to train models in parallel. However, existing distributed stochastic gradient descent (SGD) algorithms suffer from insufficient utilization of computational resources and poor convergence in heterogeneous clusters. In this paper, we propose a delayed synchronous SGD algorithm with adaptive batch size (ABS-SGD) for heterogeneous GPU clusters. In ABS-SGD, workers perform global synchronization to accumulate delayed gradients and use the accumulated delayed gradients to update parameters. While workers are performing global synchronization for delayed gradients, they perform the computation of the next batch without specifying batch size in advance, which lasts until the next global synchronization starts, realizing the full utilization of computational resources. Since the gradient delay is only one iteration, the stale gradient problem can be alleviated. We theoretically prove the convergence of ABS-SGD in hete",
    "link": "http://arxiv.org/abs/2308.15164",
    "context": "Title: ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters. (arXiv:2308.15164v1 [cs.LG])\nAbstract: As the size of models and datasets grows, it has become increasingly common to train models in parallel. However, existing distributed stochastic gradient descent (SGD) algorithms suffer from insufficient utilization of computational resources and poor convergence in heterogeneous clusters. In this paper, we propose a delayed synchronous SGD algorithm with adaptive batch size (ABS-SGD) for heterogeneous GPU clusters. In ABS-SGD, workers perform global synchronization to accumulate delayed gradients and use the accumulated delayed gradients to update parameters. While workers are performing global synchronization for delayed gradients, they perform the computation of the next batch without specifying batch size in advance, which lasts until the next global synchronization starts, realizing the full utilization of computational resources. Since the gradient delay is only one iteration, the stale gradient problem can be alleviated. We theoretically prove the convergence of ABS-SGD in hete",
    "path": "papers/23/08/2308.15164.json",
    "total_tokens": 862,
    "translated_title": "ABS-SGD: 一种用于异构GPU集群的延迟同步随机梯度下降算法和自适应批量大小",
    "translated_abstract": "随着模型和数据集的增长，以并行的方式训练模型变得越来越常见。然而，现有的分布式随机梯度下降（SGD）算法在异构集群中存在着计算资源利用不充分和收敛性差的问题。本文提出了一种用于异构GPU集群的延迟同步SGD算法和自适应批量大小（ABS-SGD）。在ABS-SGD中，工作节点进行全局同步以累积延迟梯度，并使用累积的延迟梯度来更新参数。在工作节点进行延迟梯度的全局同步时，它们在提前指定批量大小之前进行下一批次的计算，这样可以实现计算资源的充分利用。由于梯度延迟仅为一个迭代，可以缓解过时梯度问题。我们理论上证明了ABS-SGD在异构集群中的收敛性。",
    "tldr": "ABS-SGD 是一种用于异构GPU集群的延迟同步随机梯度下降算法，实现了计算资源的充分利用并缓解了过时梯度问题。",
    "en_tdlr": "ABS-SGD is a delayed synchronous stochastic gradient descent algorithm for heterogeneous GPU clusters, which achieves full utilization of computational resources and alleviates the problem of stale gradients."
}