{
    "title": "Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v2 [cs.LG] UPDATED)",
    "abstract": "Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can ac",
    "link": "http://arxiv.org/abs/2308.10544",
    "context": "Title: Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v2 [cs.LG] UPDATED)\nAbstract: Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can ac",
    "path": "papers/23/08/2308.10544.json",
    "total_tokens": 910,
    "translated_title": "较快的模型训练之路: 基于贝叶斯数据选择的方法",
    "translated_abstract": "实际场景中的错误标记、重复或有偏数据可能导致训练时间延长甚至阻碍模型收敛。传统方法优先考虑简单或困难样本，缺乏同时处理多样情况的灵活性。最近的研究提出了一种更合理的数据选择原则，通过检查数据对模型的泛化损失的影响。然而，其实际应用依赖于不太可靠的近似方法和额外的holdout数据。本文通过利用轻量级的贝叶斯方法，并结合基于大规模预训练模型构建的零样本预测器来解决这些问题。所得到的算法高效且易于实现。我们在具有大量数据噪声和不平衡性的在线批量选择场景下进行了广泛的实证研究，并观察到与竞争基线相比更高的训练效率。值得注意的是，在具有挑战性的WebVision基准测试上，我们的方法能够达到较快的训练速度。",
    "tldr": "通过对数据的贝叶斯选择和零样本预测器的利用，提出了一种更合理、高效且易于实现的模型训练方法，动态选取地在线批量训练样本，减少噪声和不平衡带来的影响。"
}