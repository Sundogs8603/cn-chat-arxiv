{
    "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])",
    "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music qu",
    "link": "http://arxiv.org/abs/2308.11276",
    "context": "Title: Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])\nAbstract: Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music qu",
    "path": "papers/23/08/2308.11276.json",
    "total_tokens": 968,
    "translated_title": "音乐理解LLaMA：应用问答和字幕推进文本到音乐生成",
    "translated_abstract": "由于缺乏具有自然语言字幕的大规模公开音乐数据集，文本到音乐生成（T2M-Gen）面临重大障碍。为了解决这个问题，我们提出了音乐理解LLaMA（MU-LLaMA），能够回答与音乐相关的问题并为音乐文件生成字幕。我们的模型利用预训练的MERT模型从音频中提取音乐特征。然而，获取适用于训练MU-LLaMA模型的合适数据集仍然具有挑战性，因为现有的公开可访问的音频问答数据集缺乏开放式音乐问答所需的深度。为了填补这一空白，我们提出了一种从现有音频字幕数据集生成问答对的方法，并介绍了设计用于回答开放式音乐相关问题的MusicQA数据集。实验证明，经过我们设计的MusicQA数据集训练的MU-LLaMA模型在音乐问答方面取得了优秀的性能。",
    "tldr": "本研究提出了一个名为音乐理解LLaMA（MU-LLaMA）的模型，通过应用问答和字幕生成的方法，解决了文本到音乐生成面临的数据稀缺问题。我们设计了一个新的MusicQA数据集，用于训练MU-LLaMA模型，并在音乐问答方面取得了出色的性能。",
    "en_tdlr": "This study proposes a model called Music Understanding LLaMA (MU-LLaMA) that addresses the scarcity of data in text-to-music generation by using question answering and captioning methods. A new dataset called MusicQA is introduced, and the MU-LLaMA model trained on this dataset achieves outstanding performance in music question answering."
}