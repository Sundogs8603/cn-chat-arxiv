{
    "title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations. (arXiv:2308.13007v1 [cs.SD])",
    "abstract": "While most research into speech synthesis has focused on synthesizing high-quality speech for in-dataset speakers, an equally essential yet unsolved problem is synthesizing speech for unseen speakers who are out-of-dataset with limited reference data, i.e., speaker adaptive speech synthesis. Many studies have proposed zero-shot speaker adaptive text-to-speech and voice conversion approaches aimed at this task. However, most current approaches suffer from the degradation of naturalness and speaker similarity when synthesizing speech for unseen speakers (i.e., speakers not in the training dataset) due to the poor generalizability of the model in out-of-distribution data. To address this problem, we propose GZS-TV, a generalizable zero-shot speaker adaptive text-to-speech and voice conversion model. GZS-TV introduces disentangled representation learning for both speaker embedding extraction and timbre transformation to improve model generalization and leverages the representation learning",
    "link": "http://arxiv.org/abs/2308.13007",
    "context": "Title: Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations. (arXiv:2308.13007v1 [cs.SD])\nAbstract: While most research into speech synthesis has focused on synthesizing high-quality speech for in-dataset speakers, an equally essential yet unsolved problem is synthesizing speech for unseen speakers who are out-of-dataset with limited reference data, i.e., speaker adaptive speech synthesis. Many studies have proposed zero-shot speaker adaptive text-to-speech and voice conversion approaches aimed at this task. However, most current approaches suffer from the degradation of naturalness and speaker similarity when synthesizing speech for unseen speakers (i.e., speakers not in the training dataset) due to the poor generalizability of the model in out-of-distribution data. To address this problem, we propose GZS-TV, a generalizable zero-shot speaker adaptive text-to-speech and voice conversion model. GZS-TV introduces disentangled representation learning for both speaker embedding extraction and timbre transformation to improve model generalization and leverages the representation learning",
    "path": "papers/23/08/2308.13007.json",
    "total_tokens": 835,
    "translated_title": "具有解耦表示的通用零样本说话者自适应语音合成",
    "translated_abstract": "虽然大多数关于语音合成的研究都集中在为数据集内的说话者合成高质量的语音上，但同样重要且未解决的问题是为没有参考数据的数据集外的未知说话者合成语音，即说话者自适应语音合成。许多研究提出了针对此任务的零样本说话者自适应文本到语音和声音转换方法。然而，由于模型在分布外数据中的泛化能力差，当前大多数方法在为未知说话者（即不在训练数据集中的说话者）合成语音时，会导致自然度和说话者相似性的降低。为了解决这个问题，我们提出了GZS-TV，一种通用的零样本说话者自适应文本到语音和声音转换模型。GZS-TV引入了解耦表示学习，以提高模型的泛化能力，并利用表示学习技术进行说话者嵌入提取和音色转换。",
    "tldr": "本研究提出了一种通用的零样本说话者自适应语音合成模型，通过解耦表示学习来改善模型在未知说话者方面的泛化能力。"
}