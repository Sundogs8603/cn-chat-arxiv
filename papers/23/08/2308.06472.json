{
    "title": "Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding. (arXiv:2308.06472v1 [cs.SD])",
    "abstract": "Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard datas",
    "link": "http://arxiv.org/abs/2308.06472",
    "context": "Title: Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding. (arXiv:2308.06472v1 [cs.SD])\nAbstract: Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard datas",
    "path": "papers/23/08/2308.06472.json",
    "total_tokens": 943,
    "translated_title": "基于同质音频-文本嵌入的灵活关键词检测",
    "translated_abstract": "在频繁使用文本来表示用户自定义/灵活关键词的场景中，通常使用昂贵的文本编码器与音频编码器在嵌入空间进行联合分析，但这可能导致模态表示不一致（即较大的不匹配）和增加复杂性。本文提出了一种新颖的架构，通过基于符合音频的文本编码器来高效检测任意关键词，这个编码器在音频嵌入方面具有同质的表示，且比兼容的文本编码器小得多。我们的文本编码器使用字素到音素（G2P）模型将文本转化为音素，然后使用从已配对的音频编码器提取的代表性音素向量将其转化为嵌入。此外，我们还通过产生混淆关键词来进一步增强我们的方法，从而开发出具有强大辨别能力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase难度数据集上优于最先进的结果。",
    "tldr": "本文提出了一种基于同质音频-文本嵌入的灵活关键词检测方法，通过使用小型的音频-compliant文本编码器，将文本转化为音素表示并与音频进行联合分析。实验结果表明，该方法在关键词检测上表现出色，优于当前最先进的方法。",
    "en_tdlr": "This paper proposes a flexible keyword spotting method based on homogeneous audio-text embedding. By using a smaller audio-compliant text encoder, the text is converted into phoneme representation and jointly analyzed with audio. Experimental results demonstrate that the proposed method outperforms the state-of-the-art approaches in keyword detection."
}