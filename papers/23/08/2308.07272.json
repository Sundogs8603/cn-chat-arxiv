{
    "title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)",
    "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework ba",
    "link": "http://arxiv.org/abs/2308.07272",
    "context": "Title: Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)\nAbstract: Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework ba",
    "path": "papers/23/08/2308.07272.json",
    "total_tokens": 900,
    "translated_title": "基于策略梯度的离散提示生成用于少样本学习的对话式方法",
    "translated_abstract": "在少样本自然语言处理任务中，基于提示的预训练语言模型(PLMs)范式取得了显著的成功。然而，现有的离散提示优化方法需要专业知识来设计基本提示集并识别高质量的提示，这既费时又低效，而且主观性较强。同时，现有的连续提示优化方法通过学习PLMs的梯度信息来改进性能，但计算成本高、可读性和通用性低常常是问题。为了填补研究空白，本文提出了一种基于对话的策略梯度离散提示优化方法($DP_2O$)。首先，我们设计了一种基于GPT-4的多轮对话对齐策略，用于生成可读性提示集。此外，我们提出了一种高效的提示筛选度量，以线性复杂度识别高质量的提示。最后，我们构建了一个强化学习(RL)框架。",
    "tldr": "本文介绍了一种基于对话的策略梯度离散提示优化方法，通过设计多轮对话对齐策略和高效的提示筛选度量，实现了在少样本学习任务中生成高质量提示集的目标。",
    "en_tdlr": "This paper presents a dialogue-based policy-gradient-based discrete prompt optimization method, which achieves the goal of generating high-quality prompt sets in few-shot learning tasks by designing multi-round dialogue alignment strategy and efficient prompt screening metric."
}