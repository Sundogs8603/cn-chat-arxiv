{
    "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of tru",
    "link": "http://arxiv.org/abs/2308.08090",
    "context": "Title: Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])\nAbstract: Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of tru",
    "path": "papers/23/08/2308.08090.json",
    "total_tokens": 836,
    "translated_title": "把高下分清楚：通过参数高效模块操作进行模型残缺性去学习",
    "translated_abstract": "大规模语言模型（LLMs）在各种应用中得到广泛应用，但存在与不真实和有毒性有关的问题。虽然参数高效模块（PEMs）已经证明了其在为模型赋予新技能方面的有效性，但利用PEMs进行残缺性去学习仍未充分探索。在这项工作中，我们提出了一种PEMs操作方法，即“提取-减去”（Ext-Sub），通过整合“专家”PEMs和“反专家”PEMs来增强LLMs的真实性和去毒性。值得注意的是，即使是反专家PEMs也具有宝贵的能力，因为它们擅长生成虚构内容，这需要语言建模和逻辑叙述能力。与仅仅否定参数不同，我们的方法涉及提取和消除反专家PEMs中的残缺能力，同时保留一般能力。为了评估我们的方法在真实性方面的有效性",
    "tldr": "通过提取和消除反专家PEMs中的残缺能力来提升大规模语言模型的真实性和去毒性。",
    "en_tdlr": "Enhance the truthfulness and detoxification of large language models by extracting and eliminating deficiency capabilities within anti-expert PEMs."
}