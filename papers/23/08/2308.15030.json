{
    "title": "Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])",
    "abstract": "Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request ",
    "link": "http://arxiv.org/abs/2308.15030",
    "context": "Title: Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])\nAbstract: Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request ",
    "path": "papers/23/08/2308.15030.json",
    "total_tokens": 942,
    "translated_title": "在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务",
    "translated_abstract": "混合专家(MoE)是深度学习中一种流行的技术，通过有条件激活的并行神经网络模块(专家)提升了模型的容量。然而，在资源受限的延迟关键的边缘场景中提供MoE模型的服务是一项具有挑战性的任务，因为模型的大小和复杂性显著增加。本文首先分析了MoE模型在连续推理场景中的行为模式，得出了关于专家激活的三个关键观察结果，包括时间局部性、可交换性和可跳过计算。基于这些观察，我们引入了PC-MoE，一种适用于资源受限的连续MoE模型服务的推理框架。PC-MoE的核心是一种新的数据结构，称为“参数委员会”，它智能地维护一部分重要的正在使用的专家，以减少资源消耗。通过基于性能分析的委员会规划器，在线找到参数委员会的最佳配置，并进行专家交换和请求处理。",
    "tldr": "本文提出了一种在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务的推理框架，该框架通过分析MoE模型的行为模式，引入了新的数据结构来减少资源消耗，并通过性能分析优化参数配置。",
    "en_tdlr": "This paper introduces an inference framework for serving MoE models on resource-constrained edge devices via dynamic expert swapping. The framework analyzes the behavior pattern of MoE models and introduces a new data structure to reduce resource consumption, optimizing the parameter configuration through profiling."
}