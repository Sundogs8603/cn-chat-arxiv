{
    "title": "Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])",
    "abstract": "Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning ",
    "link": "http://arxiv.org/abs/2308.09733",
    "context": "Title: Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])\nAbstract: Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning ",
    "path": "papers/23/08/2308.09733.json",
    "total_tokens": 871,
    "translated_title": "多目标马尔可夫决策过程中的内在动机层次策略学习",
    "translated_abstract": "多目标马尔可夫决策过程是涉及多个相互冲突的奖励函数的顺序决策问题，这些函数无法在没有妥协的情况下同时进行优化。这种问题无法像传统情况下那样通过单个最优策略来解决。相反，多目标强化学习方法发展了一个可以满足解决问题中所有可能偏好的最优策略覆盖集。然而，许多这些方法无法将其覆盖集推广到在非稳态环境中工作。在这些环境中，状态转移和奖励分布的参数随时间变化。这限制导致了进化策略集的性能严重下降。为了克服这个限制，需要学习一组通用的技能，可以在环境动态变化时引导策略覆盖集的演变，从而促进持续学习",
    "tldr": "该论文提出了一种解决多目标马尔可夫决策过程中的问题的方法，通过学习通用的技能集，使得策略覆盖集能够在非稳态环境中持续演化，从而提高性能。",
    "en_tdlr": "This paper proposes a method to solve the problems in multi-objective Markov decision processes by learning a generic skill set, enabling the evolution of policy coverage set in non-stationary environments, leading to improved performance."
}