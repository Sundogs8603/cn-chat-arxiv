{
    "title": "The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning. (arXiv:2308.03215v1 [stat.ML])",
    "abstract": "In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of \"feature selection\" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with f",
    "link": "http://arxiv.org/abs/2308.03215",
    "context": "Title: The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning. (arXiv:2308.03215v1 [stat.ML])\nAbstract: In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of \"feature selection\" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with f",
    "path": "papers/23/08/2308.03215.json",
    "total_tokens": 1025,
    "translated_title": "SGD批量大小对自编码器学习的影响：稀疏性、锐度和特征学习",
    "translated_abstract": "本研究探讨了随机梯度下降（SGD）在训练具有线性或ReLU激活的单神经元自编码器时的动态特性，使用正交数据进行实验。我们发现，在这个非凸问题中，对于任何批量大小选择，随机初始化的SGD使用恒定步长能够成功找到全局最小值。然而，找到的特定全局最小值取决于批量大小。在全批量设置下，我们发现解决方案是密集的（即非稀疏的），并且与其初始化方向高度一致，表明相对较少的特征学习发生。另一方面，对于任何小于样本数的批量大小，SGD能够找到一个稀疏且几乎与初始化正交的全局最小值，这表明随机梯度的随机性在这种情况下引发了一种不同类型的“特征选择”。此外，如果通过海森矩阵的迹来衡量最小值的锐度，那么使用更小的批量大小找到的最小值较锐利。",
    "tldr": "研究发现在非凸问题中，SGD使用不同的批量大小能找到不同类型的全局最小值，批量大小较大时解决方案较密集且与初始化方向高度一致，批量大小较小时解决方案较稀疏且几乎与初始化正交。此外，使用较小批量大小找到的最小值较锐利。"
}