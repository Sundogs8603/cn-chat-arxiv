{
    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability. (arXiv:2308.15419v1 [cs.CL])",
    "abstract": "How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \"forgotten\" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and les",
    "link": "http://arxiv.org/abs/2308.15419",
    "context": "Title: Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability. (arXiv:2308.15419v1 [cs.CL])\nAbstract: How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \"forgotten\" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and les",
    "path": "papers/23/08/2308.15419.json",
    "total_tokens": 1128,
    "translated_title": "语言模型预训练期间学习曲线的特征化：学习、遗忘和稳定性",
    "translated_abstract": "在本文中，我们从五个自回归英语语言模型预训练运行中提取学习曲线，用于上下文中的100万个标记。我们观察到，在学习生成更长、更连贯文本之前，语言模型会生成短而重复的短语。我们定量描述了单个上下文中标记的学习曲线的最终surprisal、运行内变异性、获得年龄、遗忘度和跨运行变异性。更频繁的标记达到较低的最终surprisal，其内部和预训练运行间变异性较小，学习得更早，并且在预训练过程中很少被“遗忘”。更高的n-gram概率进一步加强了这些效果。与目标标记无关，较短、更频繁的上下文与较稳定和快速获得的预测略有相关。词类的影响也较小，尽管名词倾向于较晚获得且遗忘率较低。",
    "tldr": "本研究通过从五个英语语言模型预训练运行中提取学习曲线，揭示了语言模型在预训练期间的学习过程。结果表明，语言模型在学习生成更长、更连贯的文本之前，会生成短而重复的短语。同时，频繁出现的标记在预训练过程中更早学习，具有更小的变异性，并且很少被遗忘。较短、更频繁的上下文与稳定和快速获得的预测有关。词类的影响较小，但名词倾向于较晚获得且遗忘率较低。",
    "en_tdlr": "This study characterizes the learning process of language models during pre-training and finds that they generate short repetitive phrases before learning to generate longer and more coherent text. More frequent tokens have lower surprisals, less variability, earlier age of acquisition, and lower forgettability during pre-training. Shorter and more frequent contexts are associated with more stable and quickly acquired predictions. The effects of part-of-speech are small, but nouns tend to be acquired later and have lower forgettability."
}