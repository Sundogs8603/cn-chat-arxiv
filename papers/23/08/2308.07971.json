{
    "title": "MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])",
    "abstract": "Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding ",
    "link": "http://arxiv.org/abs/2308.07971",
    "context": "Title: MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])\nAbstract: Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding ",
    "path": "papers/23/08/2308.07971.json",
    "total_tokens": 1001,
    "translated_title": "MultiSChuBERT: 高效的学术文档质量预测的多模态融合方法",
    "translated_abstract": "学术文档质量的自动评估是一项具有高潜力影响的困难任务。多模态学习，特别是将视觉信息与文本相结合，已经显示出在学术文档质量预测任务上提高性能的效果。我们提出了一种多模态的预测模型MultiSChuBERT。它结合了基于文本的模型（SChuBERT）和基于图像的模型（Inception V3）。我们的工作在学术文档质量预测方面有三个方面的贡献。首先，我们展示了结合视觉和文本嵌入的方法可以在结果上产生显著影响。其次，我们证明了逐渐解冻视觉子模型的权重可以减少过拟合数据的趋势，从而提高结果。第三，我们展示了采用最新的文本嵌入替换标准BERT$_{\\textrm{BASE}}$嵌入时多模态的优势。",
    "tldr": "MultiSChuBERT是一个多模态预测模型，通过结合文本和图像信息，在学术文档质量预测任务上取得了显著改进。我们的工作在结合视觉和文本嵌入、逐渐解冻视觉子模型权重以及采用最新文本嵌入替换标准BERT$_{\\textrm{BASE}}$嵌入方面做出了重要贡献。"
}