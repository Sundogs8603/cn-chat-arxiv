{
    "title": "ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])",
    "abstract": "Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin",
    "link": "http://arxiv.org/abs/2308.02537",
    "context": "Title: ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])\nAbstract: Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin",
    "path": "papers/23/08/2308.02537.json",
    "total_tokens": 876,
    "translated_title": "ALE: 用于NLP查询策略参数驱动比较的基于仿真的主动学习评估框架",
    "translated_abstract": "监督机器学习和深度学习需要大量标记数据，数据科学家通过手动和耗时的注释过程获取。为了减轻这个挑战，主动学习（AL）提出将有希望的数据点推荐给注释员，以便他们注释接下来的数据，而不是随机或连续的样本。这种方法旨在节省注释工作量，同时保持模型的性能。然而，实践者面临着许多用于不同任务的AL策略，并且需要一个实证基础来选择它们之间的比较。调研将AL策略分类为没有性能指标的分类法。新型AL策略的介绍性演示与少量策略的性能比较。我们的贡献通过引入一种可重现的主动学习评估（ALE）框架来解决这个问题，用于比较评估NLP中的AL策略。该框架可以以较低的成本实现AL策略，并通过数据驱动的比较进行公正的比较。",
    "tldr": "ALE是一个用于对比NLP中AL策略的仿真主动学习评估框架，提供实证基础和公正的比较。",
    "en_tdlr": "ALE is a simulation-based active learning evaluation framework for comparing AL strategies in NLP, providing an empirical basis and fair comparison."
}