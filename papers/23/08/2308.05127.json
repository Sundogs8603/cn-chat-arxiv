{
    "title": "Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])",
    "abstract": "A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. W",
    "link": "http://arxiv.org/abs/2308.05127",
    "context": "Title: Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])\nAbstract: A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. W",
    "path": "papers/23/08/2308.05127.json",
    "total_tokens": 887,
    "translated_title": "在目标检测的背景下，无数据模型提取攻击",
    "translated_abstract": "很多机器学习模型都容易受到模型提取攻击的威胁，这些攻击是通过使用针对目标模型的特殊查询来窃取模型的。这项任务通常使用训练数据的一部分或代理数据集来在白盒环境中训练一个模仿目标模型的新模型来完成。然而，在实际情况下，目标模型是在对手无法访问的私有数据集上训练的。无数据模型提取技术解决了这个问题，它使用类似生成对抗网络中使用的生成器来人工生成查询。我们首次提出了一个对回归问题的黑盒攻击，用于预测物体检测中的边界框坐标。在我们的研究中，我们发现定义损失函数和使用新颖的生成器设置是提取目标模型的关键因素之一。",
    "tldr": "该论文提出了一种在目标检测中进行无数据模型提取的攻击方法，通过使用生成器生成特殊查询，成功实现了对私有数据集上训练的目标模型的窃取。通过定义损失函数和使用新颖的生成器设置，实现了对边界框坐标的预测问题的黑盒攻击。",
    "en_tdlr": "This paper presents an attack method for data-free model extraction in object detection, successfully stealing the target model trained on private datasets by generating curated queries using a generator. By defining a loss function and using a novel generator setup, a black-box attack on predicting bounding box coordinates is achieved."
}