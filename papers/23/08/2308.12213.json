{
    "title": "CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v1 [cs.CV])",
    "abstract": "Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying \"no\" (\\textbf{CLIPN}), which empowers the logic of saying \"no\" within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable \"no\" prompt and a \"no\" text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to assoc",
    "link": "http://arxiv.org/abs/2308.12213",
    "context": "Title: CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v1 [cs.CV])\nAbstract: Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying \"no\" (\\textbf{CLIPN}), which empowers the logic of saying \"no\" within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable \"no\" prompt and a \"no\" text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to assoc",
    "path": "papers/23/08/2308.12213.json",
    "total_tokens": 880,
    "translated_title": "CLIPN用于零样本OOD检测：教CLIP说“不”。",
    "translated_abstract": "OOD检测是指在内部分布（ID）数据集上训练模型，以分类输入图像来自未知类别。设计基于卷积神经网络或Transformer的各种OOD检测方法已经付出了相当多的努力。然而，仅需要ID的类名的CLIP驱动的零样本OOD检测方法却受到较少关注。本文提出了一种新方法，即CLIP说“不”（CLIPN），它赋予了CLIP在逻辑上说“不”的能力。我们的主要动机是通过积极的语义提示和否定的语义提示，为CLIP提供区分OOD样本和ID样本的能力。具体而言，我们设计了一种新的可学习的“不”提示符和一个“不”文本编码器，以捕获图像中的否定语义。随后，我们引入了两种损失函数：图像-文本二元相反损失和文本语义相反损失，用于教授CLIPN关联OOD和ID样本。",
    "tldr": "本文提出了一种名为CLIPN的方法，通过积极的语义提示和否定的语义提示，为CLIP赋予了区分OOD和ID样本的能力。"
}