{
    "title": "Laying foundations to quantify the \"Effort of Reproducibility\". (arXiv:2308.12580v1 [cs.DL])",
    "abstract": "Why are some research studies easy to reproduce while others are difficult? Casting doubt on the accuracy of scientific work is not fruitful, especially when an individual researcher cannot reproduce the claims made in the paper. There could be many subjective reasons behind the inability to reproduce a scientific paper. The field of Machine Learning (ML) faces a reproducibility crisis, and surveying a portion of published articles has resulted in a group realization that although sharing code repositories would be appreciable, code bases are not the end all be all for determining the reproducibility of an article. Various parties involved in the publication process have come forward to address the reproducibility crisis and solutions such as badging articles as reproducible, reproducibility checklists at conferences (\\textit{NeurIPS, ICML, ICLR, etc.}), and sharing artifacts on \\textit{OpenReview} come across as promising solutions to the core problem. The breadth of literature on rep",
    "link": "http://arxiv.org/abs/2308.12580",
    "context": "Title: Laying foundations to quantify the \"Effort of Reproducibility\". (arXiv:2308.12580v1 [cs.DL])\nAbstract: Why are some research studies easy to reproduce while others are difficult? Casting doubt on the accuracy of scientific work is not fruitful, especially when an individual researcher cannot reproduce the claims made in the paper. There could be many subjective reasons behind the inability to reproduce a scientific paper. The field of Machine Learning (ML) faces a reproducibility crisis, and surveying a portion of published articles has resulted in a group realization that although sharing code repositories would be appreciable, code bases are not the end all be all for determining the reproducibility of an article. Various parties involved in the publication process have come forward to address the reproducibility crisis and solutions such as badging articles as reproducible, reproducibility checklists at conferences (\\textit{NeurIPS, ICML, ICLR, etc.}), and sharing artifacts on \\textit{OpenReview} come across as promising solutions to the core problem. The breadth of literature on rep",
    "path": "papers/23/08/2308.12580.json",
    "total_tokens": 846,
    "translated_title": "建立量化\"可重复性努力\"的基础",
    "translated_abstract": "为什么有些研究容易重现，而其他研究则难以重现？对科学工作的准确性产生怀疑并不是有益的，特别是当个体研究者无法重现论文中的主张时。无法重现科研论文可能有许多主观原因。机器学习领域面临着可重复性危机，对已发表文章的调查导致人们意识到虽然共享代码存储库可取，但代码并不能决定一篇文章的可重复性。参与出版过程的各方都在积极解决可重复性危机，诸如对具有可重复性的文章进行徽章标记、会议上的可重复性检查清单（如NeurIPS、ICML、ICLR等）以及在OpenReview上共享成果等解决方案都显得很有前景。",
    "tldr": "为了解决科学论文的可重复性危机，该研究提出了一些解决方案，包括标记可重复性文章、会议上的可重复性检查清单和在OpenReview上共享成果。",
    "en_tdlr": "This study proposes solutions to address the reproducibility crisis in scientific papers, including labeling reproducible articles, reproducibility checklists at conferences, and sharing artifacts on OpenReview."
}