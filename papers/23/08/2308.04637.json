{
    "title": "Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])",
    "abstract": "Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attentio",
    "link": "http://arxiv.org/abs/2308.04637",
    "context": "Title: Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])\nAbstract: Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attentio",
    "path": "papers/23/08/2308.04637.json",
    "total_tokens": 898,
    "translated_title": "稀疏二值变压器用于多元时间序列建模",
    "translated_abstract": "压缩神经网络有可能实现在新应用和较小的计算环境中进行深度学习。然而，目前对于这种模型在哪些学习任务中能够成功的了解不多。在这项工作中，我们将稀疏和二值权重的变压器应用于多元时间序列问题，结果显示这种轻量级模型的准确度与相同结构的密集浮点变压器相当。我们的模型在三个时间序列学习任务中取得了好的结果：分类、异常检测和单步预测。此外，为了降低注意机制的计算复杂性，我们进行了两个修改，这两个修改在模型性能上几乎没有下降：1) 在分类任务中，我们对查询、键和值激活应用了一个固定的掩码；2) 对于预测和异常检测，这些任务都依赖于对单个时间点的输出进行预测，我们提出了一种注意力机制的变体。",
    "tldr": "本研究将稀疏二值变压器应用于多元时间序列问题，并展示了该轻量级模型在分类、异常检测和单步预测任务中取得了与密集浮点变压器相当的准确度。同时，通过两个修改降低了注意机制的计算复杂性。",
    "en_tdlr": "This study applies sparse binary transformers to multivariate time series problems and demonstrates that the lightweight models achieve comparable accuracy to dense floating-point transformers in classification, anomaly detection, and single-step forecasting tasks. Additionally, computational complexity is reduced by introducing two modifications to the attention mechanism."
}