{
    "title": "How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])",
    "abstract": "Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.",
    "link": "http://arxiv.org/abs/2308.12247",
    "context": "Title: How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])\nAbstract: Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.",
    "path": "papers/23/08/2308.12247.json",
    "total_tokens": 763,
    "translated_title": "如何在优化大型语言模型时保护版权数据？",
    "translated_abstract": "大型语言模型（LLM）和生成式人工智能在计算机研究和应用中发挥了变革性的作用。关于这些模型是否输出受版权保护的数据引发了争议，这可能发生在模型训练的数据本身受版权保护的情况下。LLM是建立在Transformer神经网络架构上的，而Transformer依赖一种称为Attention的数学计算，其中使用了softmax函数。在本文中，我们展示了大型语言模型的训练和优化可以被看作是一个softmax回归问题。然后，我们建立了一种高效进行softmax回归的方法，以防止回归函数生成版权数据。这为在训练大型语言模型时避免生成版权数据建立了一个理论方法。",
    "tldr": "本文提出了一种方法来在优化大型语言模型时避免生成版权数据，通过将大型语言模型训练和优化视为softmax回归问题，并建立一种高效进行softmax回归的方法。"
}