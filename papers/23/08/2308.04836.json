{
    "title": "Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])",
    "abstract": "We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.",
    "link": "http://arxiv.org/abs/2308.04836",
    "context": "Title: Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])\nAbstract: We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.",
    "path": "papers/23/08/2308.04836.json",
    "total_tokens": 846,
    "translated_title": "通过意外记忆实现内在动机",
    "translated_abstract": "我们提出了一种新的计算模型，用于强化学习中的内在奖励，解决了现有基于意外的探索的局限性。奖励是意外的新颖性，而不是意外的规范。我们通过一个记忆网络中的检索错误来估计意外的新颖性，记忆存储和重建意外。我们的意外记忆（SM）增加了基于意外的内在动力学的能力，保持了对激动人心的探索的兴趣，同时减少了对不可预测或噪声观察的不必要的吸引力。我们的实验表明，结合各种意外预测器的SM展示了高效的探索行为，并显著提升了稀疏奖励环境中的最终性能，包括噪声电视、导航和具有挑战性的Atari游戏。",
    "tldr": "该论文提出了一种新的计算模型，通过意外记忆作为内在奖励的基础，在强化学习中解决了现有方法的局限性。实验结果表明，通过结合意外预测器的意外记忆在稀疏奖励环境中表现出高效的探索行为，并显著提升了最终性能。"
}