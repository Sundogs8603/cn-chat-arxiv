{
    "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech. (arXiv:2308.14909v1 [cs.SD])",
    "abstract": "For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity.",
    "link": "http://arxiv.org/abs/2308.14909",
    "context": "Title: Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech. (arXiv:2308.14909v1 [cs.SD])\nAbstract: For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity.",
    "path": "papers/23/08/2308.14909.json",
    "total_tokens": 934,
    "translated_title": "剪枝自注意力实现零唤醒多说话者文本到语音转换",
    "translated_abstract": "为了进行个性化语音生成，必须在限定的目标说话者数据中成功实现神经文本到语音（TTS）模型。为了达到这个目标，基线TTS模型需要广泛适用于领域外的数据（即目标说话者的语音）。然而，解决TTS中这个领域外泛化的问题的方法尚未被深入研究。在这项工作中，我们提出了一种有效的剪枝方法，即稀疏注意力，以提高TTS模型的泛化能力。特别地，我们剪枝自注意力层中的多余连接，其注意力权重低于阈值。为了灵活地确定剪枝强度以搜索最佳的泛化程度，我们还提出了一种新的可微的剪枝方法，使得模型可以自动学习阈值。对零唤醒多说话者TTS的评估验证了我们方法在声音质量和说话者相似性方面的有效性。",
    "tldr": "本文提出了一种剪枝方法来提高文本到语音转换模型的泛化能力，通过剪枝自注意力层中的多余连接，并采用可微的剪枝方法以自动学习剪枝阈值。实验证实该方法在零唤醒多说话者TTS中具有有效性。"
}