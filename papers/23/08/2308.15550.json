{
    "title": "Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning. (arXiv:2308.15550v1 [cs.LG])",
    "abstract": "This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find a robust policy that generalizes to unseen environments. We evaluate our approach on Procgen and Distracting Control Suite for generalization and sample efficiency. Empirically, ARPO shows improved performance compared to a few baseline algorithms, including data augmen",
    "link": "http://arxiv.org/abs/2308.15550",
    "context": "Title: Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning. (arXiv:2308.15550v1 [cs.LG])\nAbstract: This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find a robust policy that generalizes to unseen environments. We evaluate our approach on Procgen and Distracting Control Suite for generalization and sample efficiency. Empirically, ARPO shows improved performance compared to a few baseline algorithms, including data augmen",
    "path": "papers/23/08/2308.15550.json",
    "total_tokens": 993,
    "translated_title": "对抗式风格转移在深度强化学习中的鲁棒策略优化",
    "translated_abstract": "本文提出了一种算法，旨在通过消除对混淆特征过拟合来提高强化学习智能体的泛化能力。我们的方法包括了一个最大-最小博弈理论的目标。生成器在强化学习过程中转移观察样式。生成器的另一个目标是扰动观察，以最大化智能体采取不同行动的概率。相反，策略网络更新其参数以最小化这种扰动的影响，从而在最大化未来预期奖励的同时保持鲁棒性。基于这一设置，我们提出了一个实用的深度强化学习算法，Adversarial Robust Policy Optimization (ARPO)，以找到一个可以泛化到未见环境的鲁棒策略。我们在Procgen和Distracting Control Suite上评估了我们的方法的泛化和样本效率。实验证明，与几种基准算法（包括数据增广）相比，ARPO表现出了改进的性能。",
    "tldr": "本文提出了一种对抗式风格转移的算法，通过消除对混淆特征的过拟合来提高深度强化学习智能体的泛化能力。这种算法使用了生成器和策略网络，并通过最大-最小博弈的方式进行优化，以找到一个可以泛化到未见环境的鲁棒策略。实验证明，这种算法相比于其他基准算法有更好的性能。"
}