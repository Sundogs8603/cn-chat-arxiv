{
    "title": "Majorization-Minimization for sparse SVMs. (arXiv:2308.16858v1 [cs.LG])",
    "abstract": "Several decades ago, Support Vector Machines (SVMs) were introduced for performing binary classification tasks, under a supervised framework. Nowadays, they often outperform other supervised methods and remain one of the most popular approaches in the machine learning arena. In this work, we investigate the training of SVMs through a smooth sparse-promoting-regularized squared hinge loss minimization. This choice paves the way to the application of quick training methods built on majorization-minimization approaches, benefiting from the Lipschitz differentiabililty of the loss function. Moreover, the proposed approach allows us to handle sparsity-preserving regularizers promoting the selection of the most significant features, so enhancing the performance. Numerical tests and comparisons conducted on three different datasets demonstrate the good performance of the proposed methodology in terms of qualitative metrics (accuracy, precision, recall, and F 1 score) as well as computational ",
    "link": "http://arxiv.org/abs/2308.16858",
    "context": "Title: Majorization-Minimization for sparse SVMs. (arXiv:2308.16858v1 [cs.LG])\nAbstract: Several decades ago, Support Vector Machines (SVMs) were introduced for performing binary classification tasks, under a supervised framework. Nowadays, they often outperform other supervised methods and remain one of the most popular approaches in the machine learning arena. In this work, we investigate the training of SVMs through a smooth sparse-promoting-regularized squared hinge loss minimization. This choice paves the way to the application of quick training methods built on majorization-minimization approaches, benefiting from the Lipschitz differentiabililty of the loss function. Moreover, the proposed approach allows us to handle sparsity-preserving regularizers promoting the selection of the most significant features, so enhancing the performance. Numerical tests and comparisons conducted on three different datasets demonstrate the good performance of the proposed methodology in terms of qualitative metrics (accuracy, precision, recall, and F 1 score) as well as computational ",
    "path": "papers/23/08/2308.16858.json",
    "total_tokens": 906,
    "translated_title": "稀疏支持向量机的主要化-最小化方法",
    "translated_abstract": "几十年前，支持向量机（SVM）在监督框架下被引入用于执行二进制分类任务。如今，它们通常优于其他有监督方法，并且仍然是机器学习领域最流行的方法之一。在这项工作中，我们通过平滑的稀疏促进正则化的二次铰链损失最小化来研究SVM的训练。这个选择为基于主要化-最小化方法的快速训练方法的应用铺平了道路，从而受益于损失函数的Lipschitz可微性。此外，所提出的方法允许我们处理促进选择最重要特征的稀疏保留正则化器，从而提高性能。在三个不同的数据集上进行的数值测试和比较表明，所提出的方法在定性度量（准确率、精确率、召回率和F 1 得分）以及计算方面表现良好。",
    "tldr": "该论文提出了一种通过平滑的稀疏促进正则化的方法来训练支持向量机（SVM），并通过主要化-最小化方法实现快速训练。该方法可处理稀疏保留正则化器，提高性能，并通过在多个数据集上的测试验证其优越性能。",
    "en_tdlr": "This paper proposes a method to train Support Vector Machines (SVMs) through smooth sparse-promoting regularization, utilizing majorization-minimization techniques for fast training. The approach effectively handles sparsity-preserving regularizers to enhance performance, which is validated through numerical tests on multiple datasets."
}