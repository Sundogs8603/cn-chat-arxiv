{
    "title": "MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning. (arXiv:2308.13218v1 [cs.CV])",
    "abstract": "Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the",
    "link": "http://arxiv.org/abs/2308.13218",
    "context": "Title: MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning. (arXiv:2308.13218v1 [cs.CV])\nAbstract: Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the",
    "path": "papers/23/08/2308.13218.json",
    "total_tokens": 921,
    "translated_title": "MultiCapCLIP: 零样本多语言视觉字幕的自编码提示",
    "translated_abstract": "监督式视觉字幕模型通常需要大量的图像或视频与特定语言的描述进行配对（即视觉-字幕对）进行训练。然而，对于许多场景和语言来说，收集和标注大规模数据集是耗时且昂贵的。因此，通常无法提供足够的标记对。为了解决标签不足的问题，我们提出了一种简单而有效的零样本方法MultiCapCLIP，它可以在没有下游数据集的任何标记的情况下为不同场景和语言生成视觉字幕。在训练阶段，MultiCapCLIP仅需要文本数据作为输入。然后它进行两个主要步骤：1）检索保留新场景相关领域知识的概念提示；2）自编码提示以学习输出所需语言的字幕的写作风格。在测试阶段，MultiCapCLIP直接将视觉数据作为输入来检索...",
    "tldr": "MultiCapCLIP是一种零样本的方法，可以生成不同场景和语言的视觉字幕，无需下游数据集中的任何标记。在训练阶段，它通过检索概念提示并自编码学习写作风格来生成字幕。在测试阶段，它直接利用视觉数据进行检索。",
    "en_tdlr": "MultiCapCLIP is a zero-shot method that generates visual captions for different scenarios and languages without any labeled data from downstream datasets. In the training stage, it retrieves concept prompts and learns writing styles through auto-encoding to generate captions. In the testing stage, it directly uses visual data for retrieval."
}