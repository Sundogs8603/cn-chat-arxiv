{
    "title": "Learning (With) Distributed Optimization. (arXiv:2308.05548v1 [math.OC])",
    "abstract": "This paper provides an overview of the historical progression of distributed optimization techniques, tracing their development from early duality-based methods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergence of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm. The initial focus on Lagrangian relaxation for convex problems and decomposition strategies led to the refinement of methods like the Alternating Direction Method of Multipliers (ADMM). The resurgence of interest in distributed optimization in the late 2000s, particularly in machine learning and imaging, demonstrated ADMM's practical efficacy and its unifying potential. This overview also highlights the emergence of the proximal center method and its applications in diverse domains. Furthermore, the paper underscores the distinctive features of ALADIN, which offers convergence guarantees for non-convex scenarios without introducing auxiliary variables, differentiating it fro",
    "link": "http://arxiv.org/abs/2308.05548",
    "context": "Title: Learning (With) Distributed Optimization. (arXiv:2308.05548v1 [math.OC])\nAbstract: This paper provides an overview of the historical progression of distributed optimization techniques, tracing their development from early duality-based methods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergence of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm. The initial focus on Lagrangian relaxation for convex problems and decomposition strategies led to the refinement of methods like the Alternating Direction Method of Multipliers (ADMM). The resurgence of interest in distributed optimization in the late 2000s, particularly in machine learning and imaging, demonstrated ADMM's practical efficacy and its unifying potential. This overview also highlights the emergence of the proximal center method and its applications in diverse domains. Furthermore, the paper underscores the distinctive features of ALADIN, which offers convergence guarantees for non-convex scenarios without introducing auxiliary variables, differentiating it fro",
    "path": "papers/23/08/2308.05548.json",
    "total_tokens": 900,
    "translated_title": "学习（与）分布式优化",
    "translated_abstract": "本文概述了分布式优化技术的历史发展，追溯到20世纪60年代丹齐格、沃尔夫和本德斯开创的基于对偶性的方法，直到增广Lagrange交替方向非精确牛顿(ALADIN)算法的出现。最初的重点是对凸问题的拉格朗日松弛和分解策略，导致了Alternating Direction Method of Multipliers (ADMM)等方法的改进。2000年代末分布式优化再度受到关注，特别是在机器学习和成像领域，证明了ADMM的实际有效性和统一潜力。本文还突出了近端中心方法的出现及其在不同领域的应用。此外，本文强调了ALADIN的独特特点，它为非凸情况提供了收敛保证，而无需引入辅助变量，与其他方法有所区分。",
    "tldr": "本文回顾了分布式优化技术的历史发展，介绍了增广Lagrange交替方向非精确牛顿(ALADIN)算法以及Alternating Direction Method of Multipliers (ADMM)等方法的改进。同时突出了近端中心方法的应用和ALADIN的独特特点。",
    "en_tdlr": "This paper provides an overview of the historical development of distributed optimization techniques, introducing the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm and improvements of methods like the Alternating Direction Method of Multipliers (ADMM). The paper also highlights the applications of the proximal center method and the distinctive features of ALADIN."
}