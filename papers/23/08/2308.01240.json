{
    "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation. (arXiv:2308.01240v1 [cs.CL])",
    "abstract": "In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstrea",
    "link": "http://arxiv.org/abs/2308.01240",
    "context": "Title: Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation. (arXiv:2308.01240v1 [cs.CL])\nAbstract: In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstrea",
    "path": "papers/23/08/2308.01240.json",
    "total_tokens": 983,
    "translated_title": "在代码理解和生成方面评估指导调优的大型语言模型",
    "translated_abstract": "在这项工作中，我们评估了10种开源指导调优的大型语言模型在四个代表性的代码理解和生成任务上的表现。我们得到了以下主要发现。首先，在零知识迁移的情况下，指导调优的大型语言模型在代码理解和生成任务上非常有竞争力，有时甚至优于专门针对每个下游任务进行微调的小型最先进模型。我们还发现，更大的指导调优的大型语言模型并不总是在与代码相关的任务上更好。其次，在少训练数据的情况下，我们发现添加示范样例可以大大帮助指导调优的大型语言模型在大多数代码理解和生成任务上表现更好；然而，这些示范样例有时会导致不稳定甚至更差的性能。此外，我们发现广泛使用的基于BM25的样本选择策略在生成问题上明显优于基础的随机选择或固定选择。第三，对于微调设置，我们发现微调可以进一步提高模型在下游任务上的性能。",
    "tldr": "本研究在四个代表性的代码理解和生成任务上评估了10种开源指导调优的大型语言模型，发现在零知识迁移的情况下，指导调优的大型语言模型表现出很高的竞争力，在少训练数据的情况下，添加示范样例可以提升模型性能，在微调设置下，微调可以进一步提升模型性能。"
}