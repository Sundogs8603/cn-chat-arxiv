{
    "title": "Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])",
    "abstract": "As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.",
    "link": "http://arxiv.org/abs/2308.01776",
    "context": "Title: Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])\nAbstract: As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.",
    "path": "papers/23/08/2308.01776.json",
    "total_tokens": 858,
    "translated_title": "大型语言模型是否仍然存在纠错问题？",
    "translated_abstract": "随着GPT等大型语言模型不断提升自然语言处理（NLP）的能力，一个问题出现了：纠错问题是否仍然存在？本文通过进行两个实验，探讨了在大型语言模型的背景下纠错的作用。第一个实验将纠错作为独立的任务，使用GPT类模型和few-shot learning技术进行错误纠正。第二个实验则探讨了纠错作为其他NLP任务的准备任务的概念，检验大型语言模型在包含一定程度噪声或错误的文本上是否能够容忍并表现得足够好。通过解决这些实验，我们旨在揭示在大型语言模型时代纠错的重要性，以及对各种NLP应用的影响。",
    "tldr": "本文通过两个实验探讨了在大型语言模型背景下纠错问题的作用，第一个实验是将纠错作为独立任务进行研究，第二个实验则是探讨纠错作为其他NLP任务的准备任务的概念。研究结果将揭示纠错在大型语言模型时代的重要性及其对各种NLP应用的影响。"
}