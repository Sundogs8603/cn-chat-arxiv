{
    "title": "Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach. (arXiv:2308.13849v1 [cs.LG])",
    "abstract": "As a promising paradigm federated Learning (FL) is widely used in privacy-preserving machine learning, which allows distributed devices to collaboratively train a model while avoiding data transmission among clients. Despite its immense potential, the FL suffers from bottlenecks in training speed due to client heterogeneity, leading to escalated training latency and straggling server aggregation. To deal with this challenge, a novel split federated learning (SFL) framework that pairs clients with different computational resources is proposed, where clients are paired based on computing resources and communication rates among clients, meanwhile the neural network model is split into two parts at the logical level, and each client only computes the part assigned to it by using the SL to achieve forward inference and backward training. Moreover, to effectively deal with the client pairing problem, a heuristic greedy algorithm is proposed by reconstructing the optimization of training late",
    "link": "http://arxiv.org/abs/2308.13849",
    "context": "Title: Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach. (arXiv:2308.13849v1 [cs.LG])\nAbstract: As a promising paradigm federated Learning (FL) is widely used in privacy-preserving machine learning, which allows distributed devices to collaboratively train a model while avoiding data transmission among clients. Despite its immense potential, the FL suffers from bottlenecks in training speed due to client heterogeneity, leading to escalated training latency and straggling server aggregation. To deal with this challenge, a novel split federated learning (SFL) framework that pairs clients with different computational resources is proposed, where clients are paired based on computing resources and communication rates among clients, meanwhile the neural network model is split into two parts at the logical level, and each client only computes the part assigned to it by using the SL to achieve forward inference and backward training. Moreover, to effectively deal with the client pairing problem, a heuristic greedy algorithm is proposed by reconstructing the optimization of training late",
    "path": "papers/23/08/2308.13849.json",
    "total_tokens": 853,
    "translated_title": "有效的异构联邦学习：基于配对和分裂学习的方法",
    "translated_abstract": "作为一种有前景的隐私保护机器学习范式，联邦学习在分布式设备协作训练模型时避免了客户端之间的数据传输。然而，由于客户端的异构性，联邦学习存在训练速度的瓶颈问题，导致训练延迟和服务器聚合的延迟。为了解决这个挑战，提出了一种新颖的分裂联邦学习（SFL）框架，它基于计算资源和客户端之间的通信速率将客户端进行配对，并在逻辑层将神经网络模型分成两部分，每个客户端只计算其分配的部分，通过使用分裂学习实现前向推理和后向训练。此外，为了有效解决客户端配对问题，提出了一种启发式贪婪算法，通过重构训练延迟优化来解决该问题。",
    "tldr": "该论文提出了一种有效的异构联邦学习方法，通过配对和分裂学习，解决了异构性带来的训练速度问题，提高了联邦学习的效率。",
    "en_tdlr": "This paper proposes an effective heterogeneous federated learning approach that addresses the training speed issue caused by heterogeneity through pairing and split learning, improving the efficiency of federated learning."
}