{
    "title": "Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient. (arXiv:2308.09082v2 [cs.LG] UPDATED)",
    "abstract": "Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positiv",
    "link": "http://arxiv.org/abs/2308.09082",
    "context": "Title: Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient. (arXiv:2308.09082v2 [cs.LG] UPDATED)\nAbstract: Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positiv",
    "path": "papers/23/08/2308.09082.json",
    "total_tokens": 838,
    "translated_title": "基于无线计算的辅助联邦学习与归一化梯度聚合",
    "translated_abstract": "无线计算是联邦学习中一种通信高效的解决方案。在这样的系统中，进行迭代过程：每个移动设备更新、放大并传输私有损失函数的局部梯度；服务器接收一次性聚合的梯度，生成并广播更新的模型参数给每个移动设备。在放大因子选择方面，大多数相关工作假设局部梯度的最大范数始终发生，尽管实际上它在迭代中会波动，这可能降低收敛性能。为了解决这个问题，我们提出在放大之前将局部梯度归一化。在我们提出的方法下，当损失函数平滑时，我们证明了我们的方法可以以次线性速率收敛到稳定点。在平滑且强凸损失函数的情况下，我们证明了我们的方法可以以线性速率实现最小训练损失，且该速率可以任意小正数。",
    "tldr": "通过归一化梯度，我们在无线计算的辅助下改进了联邦学习，提高了收敛性能。",
    "en_tdlr": "By normalizing the gradients, we improve the convergence performance of federated learning with the assistance of over-the-air computation."
}