{
    "title": "WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)",
    "abstract": "Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate loc",
    "link": "http://arxiv.org/abs/2308.10195",
    "context": "Title: WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)\nAbstract: Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate loc",
    "path": "papers/23/08/2308.10195.json",
    "total_tokens": 912,
    "translated_title": "WMFormer++: 通过隐式联合学习的嵌套Transformer实现可见水印的去除",
    "translated_abstract": "水印是一种广泛采用的保护媒体版权的方法。与此同时，研究重点已经扩展到水印去除技术，提供了一种对抗性的手段来增强水印的稳健性，并推动水印技术的发展。现有的水印去除方法主要依赖于UNet，并具有专门的解码分支——一个用于水印定位，另一个用于背景图像恢复。然而，水印定位和背景恢复不是独立的任务；精确的水印定位本质上意味着需要恢复的区域，而背景恢复过程有助于更准确的水印定位。为了从两个分支综合地整合信息，我们引入了隐式联合学习范式。这使得网络能够通过门机制自主地导航隐式分支之间的信息流动。此外，我们还采用交叉通道注意力来促进定位和恢复的过程。",
    "tldr": "本文提出了一种名为WMFormer++的嵌套Transformer模型，通过隐式联合学习实现可见水印的去除。通过整合水印定位和背景恢复任务的信息，该模型能够自主地导航信息流动，并采用交叉通道注意力来促进定位和恢复的过程。",
    "en_tdlr": "WMFormer++ is a nested Transformer model that removes visible watermarks through implicit joint learning. By integrating information from watermark localization and background restoration tasks, the model autonomously navigates information flow and employs cross-channel attention to facilitate the localization and restoration process."
}