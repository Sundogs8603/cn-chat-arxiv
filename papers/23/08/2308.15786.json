{
    "title": "FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features. (arXiv:2308.15786v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client ",
    "link": "http://arxiv.org/abs/2308.15786",
    "context": "Title: FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features. (arXiv:2308.15786v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client ",
    "path": "papers/23/08/2308.15786.json",
    "total_tokens": 888,
    "translated_title": "FedCiR: 客户端不变表示学习用于联邦非独立同分布特征",
    "translated_abstract": "联邦学习（FL）是一种分布式学习范式，它在不共享原始数据的情况下最大化了边缘设备上数据驱动模型的潜力。然而，设备的数据往往是非独立同分布（non-IID）的，意味着它们的本地数据分布可能会有很大的差异。设备之间输入数据分布的异质性，通常称为特征偏移问题，可能会对全局模型的训练收敛性和准确性产生不利影响。为了分析特征偏移问题的内在原因，我们在FL中开发了一个广义误差边界，这激励我们提出了FedCiR，一种客户端不变表示学习框架，使客户能够提取信息且与客户无关的特征。具体而言，我们改进了表示和标签之间的互信息项，以鼓励表示携带基本的分类知识，并减小了客户端和表示之间的互信息项。",
    "tldr": "FedCiR是一种客户端不变表示学习框架，用于解决联邦学习中的特征偏移问题，通过改进表示和标签之间的互信息项来提取信息且与客户无关的特征。",
    "en_tdlr": "FedCiR is a client-invariant representation learning framework for addressing the feature shift problem in federated learning. It improves the mutual information term between representations and labels to extract informative and client-invariant features."
}