{
    "title": "Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])",
    "abstract": "This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera",
    "link": "http://arxiv.org/abs/2308.15394",
    "context": "Title: Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])\nAbstract: This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera",
    "path": "papers/23/08/2308.15394.json",
    "total_tokens": 962,
    "translated_title": "分布式能量存储系统的分散式多智能体强化学习电荷平衡策略",
    "translated_abstract": "本文提出了一种分散式多智能体强化学习（Dec-MARL）方法，用于解决分布式能量存储系统中的电荷平衡问题。首先，将电荷平衡问题转化为一个有限马尔可夫决策过程，并根据需求平衡得到的动作约束，采用Dec-MARL来解决该问题。具体而言，利用一阶平均一致性算法以完全分散的方式扩展分布式能量存储系统状态的观测，并根据这些观测决定初始动作（即输出功率）。为了得到允许范围内的最终动作，提出了一个反事实需求平衡算法来平衡总需求和初始动作。然后，智能体执行最终动作并从环境中获得局部奖励，使系统进入下一个状态。最后，通过一阶平均一致性算法，智能体获得平均奖励，并通过迭代更新策略来不断优化电荷平衡策略。",
    "tldr": "本文提出了一种基于分散式多智能体强化学习的电荷平衡策略，通过分布式方法解决了分布式能量存储系统中的电荷平衡问题，并通过平均一致性算法来进行优化。",
    "en_tdlr": "This paper presents a decentralized multi-agent reinforcement learning approach to address the state-of-charge balancing problem in distributed energy storage systems. By utilizing a fully-decentralized method and the first-order average consensus algorithm, the proposed strategy effectively solves the problem and achieves optimized charge balancing."
}