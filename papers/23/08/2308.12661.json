{
    "title": "Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])",
    "abstract": "Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into",
    "link": "http://arxiv.org/abs/2308.12661",
    "context": "Title: Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])\nAbstract: Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into",
    "path": "papers/23/08/2308.12661.json",
    "total_tokens": 876,
    "translated_title": "不要望向太阳：对图像分类器的对抗性曝光攻击",
    "translated_abstract": "评估深度神经网络对于超出分布范围的输入的鲁棒性是至关重要的，特别是在像自动驾驶这样的安全关键领域，但也适用于恶意攻击者可以数字地修改输入以绕过安全保护的安全系统中。然而，设计有效的超出分布测试，在保持准确的标签信息的同时涵盖所有可能的场景，是一项具有挑战性的任务。现有的方法往往需要在攻击的多样性和限制水平之间做出妥协，有时甚至两者兼而有之。作为对图像分类模型更全面的鲁棒性评估的第一步，我们引入了一种基于图像曝光的攻击方法，该方法在概念上简单明了，但又能避免局部范围内的自然图像的全局结构受到损害。通过对多个ImageNet模型进行全面评估，我们证明该攻击能够显著降低准确性，前提是它没有集成到",
    "tldr": "通过对图像进行曝光攻击，我们引入了一种简单而有效的方法来评估图像分类器的鲁棒性，并证明该攻击能够显著降低准确性。",
    "en_tdlr": "By performing solarization attacks on images, we introduce a simple yet effective methodology to assess the robustness of image classifiers, and demonstrate its significant potential to degrade accuracy."
}