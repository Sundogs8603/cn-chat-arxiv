{
    "title": "Input margins can predict generalization too. (arXiv:2308.15466v1 [cs.LG])",
    "abstract": "Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and ou",
    "link": "http://arxiv.org/abs/2308.15466",
    "context": "Title: Input margins can predict generalization too. (arXiv:2308.15466v1 [cs.LG])\nAbstract: Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and ou",
    "path": "papers/23/08/2308.15466.json",
    "total_tokens": 813,
    "translated_title": "输入界限也可以预测泛化性。",
    "translated_abstract": "深度神经网络中的泛化性如何理解是一个积极研究的领域。一种有前景的探索方法是界限测量：给定样本或其在网络内的表示到决策边界的最短距离。虽然已经显示了在隐藏表示（隐藏界限）中测量时界限与模型的泛化能力相关，但尚未建立起输入界限与泛化之间的联系。我们表明，虽然输入界限通常不能预测泛化，但如果适当地约束搜索空间，它们可以起到预测作用。我们基于输入界限开发了这样一种度量，称之为“受限界限”。我们展示了这种新度量的预测能力，与隐藏表示界限进行了对比，并在“深度学习中预测泛化性”（PGDL）数据集上进行了验证。我们发现，受限界限取得了极具竞争力的分数和优势。",
    "tldr": "本研究发现当搜索空间适当约束时，输入界限可预测泛化性能，且与隐藏表示界限相比取得了极具竞争力的结果。",
    "en_tdlr": "This study shows that input margins can predict generalization when the search space is appropriately constrained, achieving competitive results compared to hidden representation margins."
}