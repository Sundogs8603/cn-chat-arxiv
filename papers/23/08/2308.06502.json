{
    "title": "Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])",
    "abstract": "This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.",
    "link": "http://arxiv.org/abs/2308.06502",
    "context": "Title: Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])\nAbstract: This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.",
    "path": "papers/23/08/2308.06502.json",
    "total_tokens": 817,
    "translated_title": "使用大型语言模型评估聊天的三种方法",
    "translated_abstract": "本文描述了团队6提交的ChatEval系统，这是DSTC 11 Track 4竞赛的一部分。我们提出了三种不同的方法来预测基于大型语言模型（LLMs）的聊天机器人回复的转向级别质量。我们通过从向量存储中使用动态的少样本示例作为ChatGPT的提示，报告了相对于基准线的改进。我们还分析了另外两种方法的性能，并报告了未来工作的改进方向。我们仅用两周时间开发了这三个系统，展示了LLMs在此任务中的潜力。在挑战截止日期后进行的消融研究表明，新的Llama 2模型正在缩小ChatGPT和开源LLMs之间的性能差距。然而，我们发现Llama 2模型无法像ChatGPT那样从少样本示例中受益。",
    "tldr": "本文提出了三种使用大型语言模型（LLMs）评估聊天的方法，并报告了相对于基准线的改进。还分析了另外两种方法的性能，并提出了未来工作的改进方向。",
    "en_tdlr": "This paper proposes three approaches to evaluate chat using large language models (LLMs) and reports improvements over the baseline. The performance of the other two approaches is analyzed and suggestions for future work are provided."
}