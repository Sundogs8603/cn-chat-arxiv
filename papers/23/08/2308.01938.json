{
    "title": "Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])",
    "abstract": "This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind sp",
    "link": "http://arxiv.org/abs/2308.01938",
    "context": "Title: Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])\nAbstract: This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind sp",
    "path": "papers/23/08/2308.01938.json",
    "total_tokens": 981,
    "translated_title": "在线多任务学习中基于递归最小二乘和递归核方法的应用",
    "translated_abstract": "本文提出了两种新颖的在线多任务学习（MTL）回归问题的方法。我们采用高性能基于图的MTL公式，基于加权递归最小二乘（WRLS）和在线稀疏最小二乘支持向量回归（OSLSSVR）开发其递归版本。采用任务堆叠转换，我们展示了存在一个单矩阵，它融合了多任务之间的关系，并为MT-WRLS方法的初始化过程和MT-OSLSSVR的多任务核函数提供结构信息。与现有大部分基于在线梯度下降（OGD）或不精确立方逼近方法的文献相比，我们实现了精确和近似递归，其每个实例的代价在输入空间的维度（MT-WRLS）或实例字典的大小上是二次的。我们将我们的在线MTL方法与其他竞争者在实际风短期预测挑战上进行了比较。",
    "tldr": "本文提出了两种新的在线多任务学习方法，分别基于递归最小二乘和递归核方法。与基于梯度下降或不精确逼近的方法不同，我们的方法在每个实例的代价上具有二次复杂度。我们将这些方法应用于风力短期预测挑战，并与其他竞争者进行了比较。",
    "en_tdlr": "This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. The methods are based on Recursive Least Squares and Recursive Kernel methods, and achieve quadratic per-instance cost, outperforming existing methods based on Online Gradient Descent or inexact approaches. The proposed methods are applied to wind short-term prediction challenge and compared with other contenders."
}