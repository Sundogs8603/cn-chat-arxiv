{
    "title": "Attention Calibration for Transformer-based Sequential Recommendation",
    "abstract": "arXiv:2308.09419v2 Announce Type: replace  Abstract: Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights: sub-optimal position encoding and noisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple ",
    "link": "https://arxiv.org/abs/2308.09419",
    "context": "Title: Attention Calibration for Transformer-based Sequential Recommendation\nAbstract: arXiv:2308.09419v2 Announce Type: replace  Abstract: Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights: sub-optimal position encoding and noisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple ",
    "path": "papers/23/08/2308.09419.json",
    "total_tokens": 800,
    "translated_title": "基于Transformer的顺序推荐中的注意力校准",
    "translated_abstract": "近年来，基于Transformer的顺序推荐（SR）蓬勃发展，其中自注意力机制是其关键组成部分。自注意力被普遍认为能够通过为这些项目学习更大的关注权重，有效地选择来自互动项目序列的信息丰富且相关项目，以用于下一个项目预测。然而，在现实中，这并不总是正确的。我们对一些代表性的基于Transformer的SR模型进行的经验分析显示，很常见的现象是将较大的注意力权重分配给不够相关的项目，这可能导致不准确的推荐。通过进一步深入分析，我们发现两个因素可能导致这种不准确的注意力权重分配：次优的位置编码和嘈杂的输入。因此，在本文中，我们旨在解决现有研究中存在的这一重要且具有挑战性的差距。明确地说，我们提出了一种简单的",
    "tldr": "本文针对基于Transformer的顺序推荐中存在的注意力权重分配不准确问题，提出了一种解决方案。",
    "en_tdlr": "This paper addresses the issue of inaccurate attention weight assignment in Transformer-based sequential recommendation, proposing a solution."
}