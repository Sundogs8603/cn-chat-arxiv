{
    "title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)",
    "abstract": "Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.",
    "link": "http://arxiv.org/abs/2308.06911",
    "context": "Title: GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)\nAbstract: Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.",
    "path": "papers/23/08/2308.06911.json",
    "total_tokens": 967,
    "translated_title": "GIT-Mol：一种多模态大型语言模型用于分子科学中的图像，图形和文本",
    "translated_abstract": "大型语言模型在自然语言处理方面取得了重要进展，通过处理分子的文本表示，为分子科学中的创新应用提供了可能。然而，大多数现有的语言模型无法捕捉具有复杂分子结构或图像的丰富信息。在本文中，我们引入了GIT-Mol，一种集成了图形、图像和文本信息的多模态大型语言模型。为了促进多模态分子数据的集成，我们提出了GIT-Former，一种新颖的架构，能够将所有模态对齐到统一的潜在空间中。与基线相比，我们在性质预测方面实现了5%-10%的准确性提高，并在分子生成有效性方面提高了20.2%。通过任意到语言的分子翻译策略，我们的模型有潜力进行更多的下游任务，例如化合物名称识别和化学反应预测。",
    "tldr": "GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。",
    "en_tdlr": "GIT-Mol is a multi-modal large language model that processes graph, image, and text information in molecular science. With the novel GIT-Former architecture, it aligns multi-modal molecular data into a unified latent space. It significantly improves properties prediction and molecule generation validity compared to baselines. Additionally, it can be used for downstream tasks such as compound name recognition and chemical reaction prediction."
}