{
    "title": "AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients. (arXiv:2308.00258v1 [cs.LG])",
    "abstract": "The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the e",
    "link": "http://arxiv.org/abs/2308.00258",
    "context": "Title: AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients. (arXiv:2308.00258v1 [cs.LG])\nAbstract: The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the e",
    "path": "papers/23/08/2308.00258.json",
    "total_tokens": 834,
    "translated_title": "AQUILA: 自适应量化懒汇聚梯度的通信高效联邦学习",
    "translated_abstract": "联邦学习的广泛应用受到高通信开销的挑战，主要来自大规模模型的传输。现有的自适应量化方法在每一轮训练中都假设设备参与均匀，在实践中不可行。此外，这些方法在选取量化级别时存在局限性，并经常忽视本地设备数据的偏差，从而影响全局模型的鲁棒性。为了解决这些问题，本文引入了一种名为AQUILA（自适应量化懒汇聚梯度）的新型自适应框架，以增强联邦学习的效率和鲁棒性。AQUILA整合了一种复杂的设备选择方法，优先考虑设备更新的质量和实用性。",
    "tldr": "AQUILA是一个自适应量化梯度的通信高效联邦学习框架，解决了传输大规模模型时的通信开销和局部数据偏差导致的全局模型鲁棒性问题。",
    "en_tdlr": "AQUILA is an adaptive quantization-based communication efficient federated learning framework that addresses the challenges of high communication overheads and inherent biases in local device data, enhancing the efficiency and robustness of the global model."
}