{
    "title": "FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])",
    "abstract": "With recent empirical observations, it has been argued that the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size. Following this argument, we opted to fine tune a one billion parameter size trained general purpose causal language model with a dataset curated on team statistics of the Italian football league first ten game weeks, using low rank adaptation. The limited training dataset was compiled based on a framework where a powerful commercial large language model provides distilled paragraphs and question answer pairs as intended. The training duration was kept relatively short to provide a basis for our minimal setting exploration. We share our key observations on the process related to developing a specific purpose language model which is intended to interpret soccer data with constrained resources in this article.",
    "link": "http://arxiv.org/abs/2308.08610",
    "context": "Title: FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])\nAbstract: With recent empirical observations, it has been argued that the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size. Following this argument, we opted to fine tune a one billion parameter size trained general purpose causal language model with a dataset curated on team statistics of the Italian football league first ten game weeks, using low rank adaptation. The limited training dataset was compiled based on a framework where a powerful commercial large language model provides distilled paragraphs and question answer pairs as intended. The training duration was kept relatively short to provide a basis for our minimal setting exploration. We share our key observations on the process related to developing a specific purpose language model which is intended to interpret soccer data with constrained resources in this article.",
    "path": "papers/23/08/2308.08610.json",
    "total_tokens": 900,
    "translated_title": "FootGPT：在最小设置上进行的大规模语言模型开发实验",
    "translated_abstract": "最近的实证观察表明，相比于神经参数数量、训练时长或数据集大小，开发准确的语言模型的最重要方面可能是适当的数据集内容和训练策略。基于这个观点，我们选择使用低秩适应性对一个10亿参数规模的通用因果语言模型进行微调，数据集由意大利足球联赛前十轮的球队统计信息构建，并使用强大的商业语言模型提供的精简段落和问答对。我们将训练时长保持相对较短，以提供我们最小设置探索的基础。在本文中，我们分享了与使用有限资源解释足球数据的特定目的语言模型开发相关的关键观察结果。",
    "tldr": "本文介绍了一个在最小设置上进行的大规模语言模型开发实验，研究发现准确的语言模型的关键在于适当的数据集内容和训练策略，而不是神经参数数量、训练时长或数据集大小。通过对一个10亿参数规模的通用因果语言模型进行微调，并使用商业语言模型提供的精简段落和问答对构建数据集，可以有效解释足球数据。",
    "en_tdlr": "This paper presents a large-scale language model development experiment conducted on a minimal setting. The study highlights the importance of dataset content and training strategy in developing accurate language models, rather than the number of neural parameters, training duration, or dataset size. By fine-tuning a one billion parameter size general purpose causal language model and utilizing distilled paragraphs and question-answer pairs from a commercial language model, the experiment successfully interprets soccer data with limited resources."
}