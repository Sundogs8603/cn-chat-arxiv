{
    "title": "Group Membership Bias. (arXiv:2308.02887v1 [cs.IR])",
    "abstract": "When learning to rank from user interactions, search and recommendation systems must address biases in user behavior to provide a high-quality ranking. One type of bias that has recently been studied in the ranking literature is when sensitive attributes, such as gender, have an impact on a user's judgment about an item's utility. For example, in a search for an expertise area, some users may be biased towards clicking on male candidates over female candidates. We call this type of bias group membership bias or group bias for short. Increasingly, we seek rankings that not only have high utility but are also fair to individuals and sensitive groups. Merit-based fairness measures rely on the estimated merit or utility of the items. With group bias, the utility of the sensitive groups is under-estimated, hence, without correcting for this bias, a supposedly fair ranking is not truly fair. In this paper, first, we analyze the impact of group bias on ranking quality as well as two well-know",
    "link": "http://arxiv.org/abs/2308.02887",
    "context": "Title: Group Membership Bias. (arXiv:2308.02887v1 [cs.IR])\nAbstract: When learning to rank from user interactions, search and recommendation systems must address biases in user behavior to provide a high-quality ranking. One type of bias that has recently been studied in the ranking literature is when sensitive attributes, such as gender, have an impact on a user's judgment about an item's utility. For example, in a search for an expertise area, some users may be biased towards clicking on male candidates over female candidates. We call this type of bias group membership bias or group bias for short. Increasingly, we seek rankings that not only have high utility but are also fair to individuals and sensitive groups. Merit-based fairness measures rely on the estimated merit or utility of the items. With group bias, the utility of the sensitive groups is under-estimated, hence, without correcting for this bias, a supposedly fair ranking is not truly fair. In this paper, first, we analyze the impact of group bias on ranking quality as well as two well-know",
    "path": "papers/23/08/2308.02887.json",
    "total_tokens": 869,
    "translated_title": "群组成员偏见",
    "translated_abstract": "当从用户交互中学习排名时，搜索和推荐系统必须解决用户行为中的偏见问题，以提供高质量的排名。在排名文献中最近研究的一种偏见类型是敏感属性（如性别）对用户对项目效用的判断产生的影响。例如，在寻找某个专业领域时，一些用户可能对男性候选人比女性候选人更有偏见。我们将这种偏见称为群组成员偏见或群组偏见。越来越多的人希望获得不仅具有高效用性而且对个人和敏感群体也公平的排名。基于价值的公平度量依赖于项目的估计价值或效用。在群组偏见的情况下，敏感群体的效用被低估，因此，在不纠正这种偏见的情况下，所谓的公平排名并不真正公平。首先，本文分析了群组偏见对排名质量以及两个众所周知的情况的影响",
    "tldr": "研究分析了群组偏见对排名质量的影响，指出在不纠正群组偏见的情况下，所谓的公平排名不真正公平。"
}