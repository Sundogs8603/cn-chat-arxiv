{
    "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)",
    "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose ",
    "link": "http://arxiv.org/abs/2308.09300",
    "context": "Title: V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)\nAbstract: Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose ",
    "path": "papers/23/08/2308.09300.json",
    "total_tokens": 895,
    "translated_title": "V2A-Mapper：通过连接基础模型实现轻量级的视听生成解决方案",
    "translated_abstract": "在人工智能研究中，基于一组基础模型（FMs）构建人工智能系统正在成为一种新的范式。这些模型通过大量数据学习得到的代表性和生成能力可以轻松地适应和迁移至各种下游任务，而无需额外的从头训练。然而，在涉及音频模态的跨模态生成中，利用FMs仍然是一个未充分研究的领域。另一方面，从视觉输入中自动生成语义相关的声音是跨模态生成研究中的一个重要问题。为了解决这个视听生成（V2A）问题，现有方法倾向于使用规模适中的数据集从头设计和构建复杂的系统。在本文中，我们提出了一个轻量级的解决方案，通过利用基础模型，具体来说是CLIP、CLAP和AudioLDM。我们首先研究了视觉CLIP模型和听觉CLAP模型的潜在空间之间的领域差距。然后我们提出了...",
    "tldr": "本研究提出了一种轻量级的解决方案，通过连接基础模型，特别是CLIP、CLAP和AudioLDM模型，解决了视听生成的问题。",
    "en_tdlr": "This study proposes a lightweight solution to vision-to-audio generation by connecting foundation models, specifically CLIP, CLAP, and AudioLDM."
}