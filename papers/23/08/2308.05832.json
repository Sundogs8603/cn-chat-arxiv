{
    "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks. (arXiv:2308.05832v1 [cs.CR])",
    "abstract": "Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield f",
    "link": "http://arxiv.org/abs/2308.05832",
    "context": "Title: FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks. (arXiv:2308.05832v1 [cs.CR])\nAbstract: Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield f",
    "path": "papers/23/08/2308.05832.json",
    "total_tokens": 932,
    "translated_title": "FLShield：一种基于验证的联邦学习框架，用于防御投毒攻击",
    "translated_abstract": "联邦学习（FL）正在改变我们从数据中学习的方式。随着它的日益流行，它现在被应用于许多安全关键的领域，如自动驾驶和医疗保健。由于成千上万的参与者可以在这种协作环境中贡献数据，确保这种系统的安全性和可靠性变得具有挑战性。这突显了设计安全和鲁棒的FL系统的需求，以抵御恶意参与者的行为，同时确保高效的效用、本地数据的隐私和效率。在本文中，我们提出了一种新颖的FL框架，称为FLShield，它利用FL参与者的良性数据在将其考虑进全局模型生成之前对本地模型进行验证。这与现有的依赖于服务器对清洁数据集的访问的防御方法形成鲜明对比，这种假设在现实场景中经常是不切实际的，并且与FL的基本原理相冲突。我们进行了大量实验证明了我们的FLShield的有效性。",
    "tldr": "本文提出了一种名为FLShield的新型联邦学习框架，利用FL参与者的良性数据对本地模型进行验证，以防御恶意参与者的投毒攻击，并确保FL系统的安全性和效用。",
    "en_tdlr": "This paper proposes a novel federated learning framework called FLShield, which utilizes benign data from FL participants to validate the local models and defend against poisoning attacks by malicious participants, ensuring the security and utility of the FL system."
}