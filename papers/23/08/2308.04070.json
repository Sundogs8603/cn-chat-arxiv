{
    "title": "ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data. (arXiv:2308.04070v1 [cs.CV])",
    "abstract": "Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose \"ConDistFL\", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability c",
    "link": "http://arxiv.org/abs/2308.04070",
    "context": "Title: ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data. (arXiv:2308.04070v1 [cs.CV])\nAbstract: Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose \"ConDistFL\", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability c",
    "path": "papers/23/08/2308.04070.json",
    "total_tokens": 893,
    "translated_title": "ConDistFL：针对部分标注数据的联邦学习条件蒸馏",
    "translated_abstract": "开发一个能够同时描绘多个器官和疾病的广义分割模型具有很高的价值。联邦学习（FL）是一种无需交换训练数据的协作开发模型的关键技术。然而，有限的全标注训练数据的获取权限给训练可泛化模型带来了重大挑战。我们提出了“ConDistFL”框架，通过将FL与知识蒸馏相结合来解决这个问题。本地模型可以通过充分设计的条件概率表示从全局模型的部分标注数据中提取无标注器官和肿瘤的知识。我们在来自MSD和KiTS19挑战赛的四个不同部分标注腹部CT数据集上验证了我们的框架。实验结果表明，所提出的框架明显优于FedAvg和FedOpt基线。此外，对外部测试数据集的性能表现显示了更好的泛化能力。",
    "tldr": "ConDistFL框架将联邦学习与知识蒸馏相结合，通过充分设计条件概率表示从部分标注数据中提取无标注信息，解决了有限全标注训练数据的挑战，并在多个腹部CT数据集上展现了显著的性能优势。"
}