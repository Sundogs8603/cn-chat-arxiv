{
    "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. (arXiv:2308.13504v1 [cs.LG])",
    "abstract": "We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quantized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q introduces a unique formulation inspired by weight normalization that constrains the L1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guarantee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while maintaining model accuracy competitive with a floating-point baseline. In our evaluations, we consider the impact of A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully exploit custom accumulator bit widths. Our experimentation sho",
    "link": "http://arxiv.org/abs/2308.13504",
    "context": "Title: A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. (arXiv:2308.13504v1 [cs.LG])\nAbstract: We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quantized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q introduces a unique formulation inspired by weight normalization that constrains the L1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guarantee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while maintaining model accuracy competitive with a floating-point baseline. In our evaluations, we consider the impact of A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully exploit custom accumulator bit widths. Our experimentation sho",
    "path": "papers/23/08/2308.13504.json",
    "total_tokens": 996,
    "translated_title": "A2Q:带有保证避免溢出功能的累加器感知量化",
    "translated_abstract": "我们提出了一种累加器感知量化（A2Q）方法，用于训练量化神经网络（QNNs），以避免在推理过程中使用低精度累加器时发生溢出。A2Q引入了一种受权值规范化启发的独特公式，根据我们导出的累加器位宽边界约束模型权重的L1范数。因此，在训练低精度累加的QNNs时，A2Q还自动促进非结构化权重稀疏性，以确保避免溢出。我们将该方法应用于基于深度学习的计算机视觉任务，展示了A2Q可以训练低精度累加器的QNNs，并保持模型准确性与浮点数基准相竞争。在我们的评估中，我们考虑了A2Q对通用平台和可编程硬件的影响。但是，我们主要针对在FPGAs上部署模型，因为它们可以被编程以充分利用自定义累加器位宽。我们的实验证明了A2Q在使用低精度累加器时可以有效地训练QNNs，并保持模型的准确性。",
    "tldr": "A2Q是一种用于训练量化神经网络的累加器感知量化方法，通过引入一种独特的公式，并根据累加器位宽边界约束模型权重的L1范数，以避免在低精度累加器上发生溢出。该方法在计算机视觉任务中得到验证，能够有效地训练QNNs并保持模型准确性，特别适用于部署在可编程硬件如FPGAs上。"
}