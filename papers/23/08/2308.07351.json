{
    "title": "IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse. (arXiv:2308.07351v1 [cs.LG])",
    "abstract": "Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source poli",
    "link": "http://arxiv.org/abs/2308.07351",
    "context": "Title: IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse. (arXiv:2308.07351v1 [cs.LG])\nAbstract: Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source poli",
    "path": "papers/23/08/2308.07351.json",
    "total_tokens": 887,
    "translated_title": "IOB：集成优化传递和行为传递用于多策略复用",
    "translated_abstract": "人类有能力重复利用之前学习的策略来快速解决新任务，而强化学习（RL）代理也可以通过从源策略向相关目标任务传递知识来做到同样的事情。传递RL方法可以通过改变策略优化目标（优化传递）或者影响行为策略（行为传递）来利用源策略。然而，在有限的样本下选择适当的源策略来引导目标策略学习一直是一个挑战。之前的方法引入了额外的组件，比如层次策略或者源策略的值函数估计，这可能导致非平稳的策略优化或者大量的采样成本，从而降低了传递的有效性。为了解决这个挑战，我们提出了一种新颖的传递RL方法，选择源策略而无需训练额外的组件。我们的方法利用了actor-critic框架中的Q函数来指导策略选择，选择源策略。",
    "tldr": "该论文提出了一种新颖的传递强化学习方法，通过在actor-critic框架中使用Q函数来选择源策略，解决了在有限样本下选择适当的源策略的挑战。"
}