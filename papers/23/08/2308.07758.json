{
    "title": "Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])",
    "abstract": "Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \\citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \\textit{a simple template}, i.e., ``\\textit{\\textbf{If we know the answer of the above question is \\{a candidate answer\\}, what is the value of unknown variable ${\\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three",
    "link": "http://arxiv.org/abs/2308.07758",
    "context": "Title: Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])\nAbstract: Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \\citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \\textit{a simple template}, i.e., ``\\textit{\\textbf{If we know the answer of the above question is \\{a candidate answer\\}, what is the value of unknown variable ${\\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three",
    "path": "papers/23/08/2308.07758.json",
    "total_tokens": 967,
    "translated_title": "在大型语言模型中使用反向推理进行验证",
    "translated_abstract": "链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。",
    "tldr": "本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。",
    "en_tdlr": "This paper investigates the use of backward reasoning in large language models for verification. The authors propose a novel technique that masks a token in the question and asks the language model to predict the masked token to verify candidate answers. They also propose a method that combines forward and backward reasoning to estimate the probability of candidate answers."
}