{
    "title": "Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])",
    "abstract": "We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.",
    "link": "http://arxiv.org/abs/2308.04024",
    "context": "Title: Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])\nAbstract: We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.",
    "path": "papers/23/08/2308.04024.json",
    "total_tokens": 764,
    "translated_title": "不平衡分类和RL探索的范围损失",
    "translated_abstract": "我们展示了强化学习问题和监督分类问题之间的等价性。我们因此将强化学习中的探索利用权衡等同于监督分类中的数据集不平衡问题，并找出了它们在解决方式上的相似之处。通过对上述问题的分析，我们提出了一种新的适用于强化学习和监督分类的损失函数- 范围损失。范围损失可以调整梯度，以防止过度利用和数据集不平衡引起的性能损失，而无需进行任何调整。我们在一系列基准强化学习任务和一个偏斜的分类数据集上测试了范围损失，结果显示范围损失优于其他损失函数。",
    "tldr": "本文介绍了范围损失，这是一种新的适用于强化学习和监督分类的损失函数，它能够解决探索利用权衡和数据集不平衡问题，并在实验中表现出优于其他损失函数的性能。",
    "en_tdlr": "This paper introduces Scope Loss, a novel loss function for reinforcement learning and supervised classification, which addresses the exploration exploitation trade-off and dataset imbalance problem, and demonstrates superior performance compared to other loss functions in experimental evaluations."
}