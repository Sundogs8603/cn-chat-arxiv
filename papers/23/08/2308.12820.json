{
    "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])",
    "abstract": "Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces",
    "link": "http://arxiv.org/abs/2308.12820",
    "context": "Title: Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])\nAbstract: Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces",
    "path": "papers/23/08/2308.12820.json",
    "total_tokens": 1065,
    "translated_title": "不排除预测：基于可达集的后续验证方法",
    "translated_abstract": "机器学习模型常被用于决定谁有资格得到贷款、面试或公共福利。标准技术用于构建这些模型时，会使用关于人的特征，但忽视他们的可操作性。因此，模型可能会分配固定的预测，这意味着被拒绝贷款、面试或福利的消费者可能永久被排除在获得信贷、就业或援助的机会之外。在这项工作中，我们引入了一种正式的测试程序来检测分配固定预测的模型，我们称之为后续验证。我们开发了一套机制可靠地确定给定模型是否能提供对决策对象的后续手段，这些手段由用户指定的可操作性约束确定。我们演示了我们的工具如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并利用它们研究了在真实世界的贷款数据集中实现后续措施的不可行性。我们的结果凸显了模型如何无意中分配固定预测，从而永久禁止使用者获得相关权益。",
    "tldr": "这项研究引入了一种称为后续验证的正式测试程序，用于检测模型分配固定预测的情况。通过开发可靠的机制，可以确定给定模型是否能为决策对象提供后续措施，从而解决了模型分配固定预测可能带来的问题。该研究还展示了如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并探讨了在贷款数据集中实现后续措施的不可行性。",
    "en_tdlr": "This research introduces a formal testing procedure called recourse verification to detect models that assign fixed predictions. By developing reliable mechanisms, it can determine if a given model can provide recourse to decision subjects, addressing the issue of assigning fixed predictions. The study also demonstrates how to ensure recourse and adversarial robustness in real-world datasets and explores the infeasibility of implementing recourse in lending datasets."
}