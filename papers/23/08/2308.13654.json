{
    "title": "Pretty darn good control: when are approximate solutions better than approximate models. (arXiv:2308.13654v1 [cs.LG])",
    "abstract": "Existing methods for optimal control struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and data heterogeneity. Instead of tackling these system complexities directly, researchers have typically sought to simplify models to fit optimal control methods. But when is the optimal solution to an approximate, stylized model better than an approximate solution to a more accurate model? While this question has largely gone unanswered owing to the difficulty of finding even approximate solutions for complex models, recent algorithmic and computational advances in deep reinforcement learning (DRL) might finally allow us to address these questions. DRL methods have to date been applied primarily in the context of games or robotic mechanics, which operate under precisely known rules. Here, we demonstrate the ability for DRL algorithms using deep neural networks to successfully approximate solutions (the \"policy funct",
    "link": "http://arxiv.org/abs/2308.13654",
    "context": "Title: Pretty darn good control: when are approximate solutions better than approximate models. (arXiv:2308.13654v1 [cs.LG])\nAbstract: Existing methods for optimal control struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and data heterogeneity. Instead of tackling these system complexities directly, researchers have typically sought to simplify models to fit optimal control methods. But when is the optimal solution to an approximate, stylized model better than an approximate solution to a more accurate model? While this question has largely gone unanswered owing to the difficulty of finding even approximate solutions for complex models, recent algorithmic and computational advances in deep reinforcement learning (DRL) might finally allow us to address these questions. DRL methods have to date been applied primarily in the context of games or robotic mechanics, which operate under precisely known rules. Here, we demonstrate the ability for DRL algorithms using deep neural networks to successfully approximate solutions (the \"policy funct",
    "path": "papers/23/08/2308.13654.json",
    "total_tokens": 886,
    "translated_title": "相对准确模型中近似解何时优于近似模型——一个创新性的研究",
    "translated_abstract": "现有的优化控制方法往往难以处理实际系统中遇到的复杂性，包括维度、过程误差、模型偏差和数据异质性。研究人员通常倾向于简化模型来适应优化控制方法，而不是直接解决系统复杂性。然而，当近似、简化的模型的最优解比更准确模型的近似解更好时，我们又该如何选择呢？虽然这个问题一直没有得到解决，因为找到复杂模型的近似解非常困难，但是最近深度强化学习（DRL）在算法和计算方面的进展或许可以帮助我们回答这些问题。目前，DRL方法主要应用于游戏或机器人力学等精确已知规则的领域。在本文中，我们展示了使用深度神经网络的DRL算法成功逼近解决问题的能力。",
    "tldr": "本研究探讨了在实际系统中，当近似、简化的模型的最优解比更准确模型的近似解更好时的情况，并通过深度强化学习算法使用深度神经网络成功逼近解决了这一问题。",
    "en_tdlr": "This study investigates when the optimal solution to an approximate, simplified model is better than an approximate solution to a more accurate model in real-world systems, and successfully addresses this question using deep reinforcement learning algorithms with deep neural networks."
}