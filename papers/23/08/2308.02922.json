{
    "title": "Structured Low-Rank Tensors for Generalized Linear Models. (arXiv:2308.02922v1 [stat.ML])",
    "abstract": "Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vector",
    "link": "http://arxiv.org/abs/2308.02922",
    "context": "Title: Structured Low-Rank Tensors for Generalized Linear Models. (arXiv:2308.02922v1 [stat.ML])\nAbstract: Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vector",
    "path": "papers/23/08/2308.02922.json",
    "total_tokens": 831,
    "translated_title": "结构化低秩张量用于广义线性模型",
    "translated_abstract": "最近的研究表明，在回归问题中强加张量结构在参数估计和样本复杂性方面比基于向量的方法更可靠。本研究在广义线性模型问题中研究一种新的低秩张量模型，称为低分离秩（LSR）模型。LSR模型推广了著名的Tucker和CANDECOMP / PARAFAC（CP）模型，并且是块张量分解（BTD）模型的一个特例。本研究在GLM模型中对参数张量施加LSR结构，并提出了一种块坐标下降算法用于LSR结构张量GLM的参数估计。最重要的是，它推导出了在LSR张量GLM问题中估计系数张量的误差阈值的极小化下界。这个极小化下界与LSR张量GLM问题中的固有自由度成正比，表明其样本复杂性可能明显低于向量方法。",
    "tldr": "结构化低秩张量在广义线性模型中的应用能够更可靠地估计参数和降低样本复杂性。",
    "en_tdlr": "Structured low-rank tensors improve parameter estimation and reduce sample complexity in generalized linear models."
}