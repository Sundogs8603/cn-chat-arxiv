{
    "title": "Convergence guarantee for consistency models. (arXiv:2308.11449v1 [math.NA])",
    "abstract": "We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of \"Consistency Models, Yang Song ",
    "link": "http://arxiv.org/abs/2308.11449",
    "context": "Title: Convergence guarantee for consistency models. (arXiv:2308.11449v1 [math.NA])\nAbstract: We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of \"Consistency Models, Yang Song ",
    "path": "papers/23/08/2308.11449.json",
    "total_tokens": 1068,
    "translated_title": "一致性模型的收敛保证",
    "translated_abstract": "我们首次为一致性模型（CMs）提供了收敛保证，它是一种新兴的一步生成模型，能够生成与扩散模型生成的样本相媲美的样本。我们的主要结果是，在基本的分数匹配误差、一致性误差和数据分布的平滑性的假设下，CMs能够以小的$W_2$误差在一步内有效地从任何真实数据分布中进行采样。我们的结果：（1）适用于$L^2$精确的分数和一致性假设（而非$L^\\infty$精确）；（2）不需要对数据分布做出如log-Sobelev不等式的强假设；（3）所有参数的尺度多项式地增长；（4）与基于分数的生成模型（SGMs）的最新收敛保证相匹配。我们还提供了多步一致性采样过程可以进一步减小误差的结果，支撑了原始论述中的\"一致性模型\"，杨松",
    "tldr": "本文提供了一致性模型（CMs）的收敛保证，该模型可以在一步内从任何真实数据分布中有效地进行采样，且具有较小的$W_2$误差。这一结果对于$L^2$精确的分数和一致性假设成立，并且不需要强假设，同时在所有参数上呈多项式尺度增长，与基于分数的生成模型（SGMs）的最新收敛保证相匹配。另外，本文还提供了多步一致性采样过程可以进一步减小误差的结果。",
    "en_tdlr": "This paper provides convergence guarantees for Consistency Models (CMs), which can efficiently sample from any realistic data distribution in one step with small $W_2$ error. The guarantees hold for $L^2$-accurate score and consistency assumption, do not require strong assumptions on the data distribution, scale polynomially in all parameters, and match the state-of-the-art convergence guarantee for score-based generative models (SGMs). The paper also shows that the Multistep Consistency Sampling procedure can further reduce the error."
}