{
    "title": "Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures. (arXiv:2308.04539v1 [cs.LG])",
    "abstract": "The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.  Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the ",
    "link": "http://arxiv.org/abs/2308.04539",
    "context": "Title: Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures. (arXiv:2308.04539v1 [cs.LG])\nAbstract: The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.  Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the ",
    "path": "papers/23/08/2308.04539.json",
    "total_tokens": 874,
    "translated_title": "利用生物启发的架构提高连续学习任务的性能",
    "translated_abstract": "在设计智能系统中，从连续的数据流中无间断地学习而不会发生灾难性遗忘的能力至关重要。许多连续学习方法依赖于随机梯度下降及其变体，这些方法采用全局误差更新，因此需要采取策略，如内存缓冲区或回放，以规避其稳定性、贪婪和短期记忆的限制。为了解决这个问题，我们开发了一种受生物启发的轻量级神经网络架构，它包括突触可塑性机制和神经调节，并通过本地误差信号进行学习，以实现在线连续学习而无需随机梯度下降。我们的方法在Split-MNIST、Split-CIFAR-10和Split-CIFAR-100数据集上比其他内存受限的学习方法表现出更好的在线连续学习性能，并且与最先进的内存密集型回放方法相匹配。",
    "tldr": "本研究提出了一种受生物启发的轻量级神经网络架构，通过本地误差信号实现在线连续学习，克服了传统方法的局限性，在多个数据集上展现出优秀的表现。",
    "en_tdlr": "This paper proposes a biologically inspired lightweight neural network architecture that achieves online continual learning by learning through local error signals, overcoming the limitations of traditional methods and demonstrating superior performance on multiple datasets."
}