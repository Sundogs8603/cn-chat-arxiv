{
    "title": "Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration. (arXiv:2308.02749v1 [cs.AR])",
    "abstract": "Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared ",
    "link": "http://arxiv.org/abs/2308.02749",
    "context": "Title: Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration. (arXiv:2308.02749v1 [cs.AR])\nAbstract: Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared ",
    "path": "papers/23/08/2308.02749.json",
    "total_tokens": 913,
    "translated_title": "利用Versal体系结构的片上异构性加速GNN推理",
    "translated_abstract": "图神经网络（GNN）在社交网络分析、生物信息学等许多机器学习（ML）应用中都取得了革命性的突破。通过利用输入图中的数据稀疏性、顶点特征和GNN计算中的中间数据，可以加速GNN推理。为了实现动态稀疏性利用，我们利用AMD Versal ACAP架构的异构计算能力来加速GNN推理。我们开发了一个自定义硬件模块，该模块在可编程逻辑（PL）上执行计算核心的稀疏原语，并使用AI引擎（AIE）高效计算密集型原语。为了在推理过程中利用数据稀疏性，我们设计了一种运行时内核映射策略，根据数据稀疏性动态分配计算任务给PL和AIE。在VCK5000 ACAP平台上的实现相比于CPU、GPU、ACAP和其他自定义GNN加速器的最新实现具有更优异的性能。",
    "tldr": "该论文利用AMD Versal ACAP架构的异构计算能力，通过开发自定义硬件模块和设计运行时内核映射策略，实现了对GNN推理的加速。该方法在VCK5000 ACAP平台上的实现具有优越性能。"
}