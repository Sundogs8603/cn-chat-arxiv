{
    "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. (arXiv:2308.04026v1 [cs.AI])",
    "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .",
    "link": "http://arxiv.org/abs/2308.04026",
    "context": "Title: AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. (arXiv:2308.04026v1 [cs.AI])\nAbstract: With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .",
    "path": "papers/23/08/2308.04026.json",
    "total_tokens": 896,
    "translated_title": "AgentSims: 一个用于大型语言模型评估的开源沙盒平台",
    "translated_abstract": "在 ChatGPT 等大型语言模型占据主导地位的情况下，如何评估这些模型的能力成为一个开放问题。现有的评估方法存在以下问题：（1）有限的评估能力，（2）脆弱的评估基准，（3）不客观的评估指标。我们提出了一种基于任务的评估方法，即在模拟环境中让语言模型代理完成任务，这是解决上述问题的一个综合性解决方案。我们推出了 AgentSims，这是一个易于使用的基础设施，供各学科的研究人员测试他们感兴趣的特定能力。研究人员可以通过交互式图形界面添加代理和建筑物来构建评估任务，也可以通过几行代码部署和测试新的支持机制，例如记忆、规划和工具使用系统。我们的演示版本可以在 https://agentsims.com上获得。",
    "tldr": "AgentSims是一个开放源码的沙盒平台，用于评估大型语言模型的能力。它通过任务驱动的评估方法，解决了现有评估方法的局限性，并提供了易于使用的界面，供研究人员测试特定能力。演示版本可在https://agentsims.com获得。"
}