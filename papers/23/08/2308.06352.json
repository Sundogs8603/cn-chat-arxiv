{
    "title": "Learning Distributions via Monte-Carlo Marginalization. (arXiv:2308.06352v1 [cs.LG])",
    "abstract": "We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned",
    "link": "http://arxiv.org/abs/2308.06352",
    "context": "Title: Learning Distributions via Monte-Carlo Marginalization. (arXiv:2308.06352v1 [cs.LG])\nAbstract: We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned",
    "path": "papers/23/08/2308.06352.json",
    "total_tokens": 889,
    "translated_title": "通过蒙特卡洛边缘化学习分布",
    "translated_abstract": "我们提出了一种新的方法来从样本中学习难以计算的分布。主要思想是使用参数化分布模型，如高斯混合模型（GMM），通过最小化KL散度来近似难以计算的分布。基于这个思想，有两个需要解决的挑战。首先，当分布的维度增加时，KL散度的计算复杂度是不可接受的。我们提出了蒙特卡洛边缘化（MCMarg）来解决这个问题。第二个挑战是优化过程的可微性，因为目标分布是难以计算的。我们通过使用核密度估计（KDE）来处理这个问题。提出的方法是学习复杂分布的有力工具，整个过程是可微的。因此，它可以更好地替代变分自动编码器（VAE）中的变分推断。我们的方法受益的一个强有力的证据是学习的分布",
    "tldr": "通过蒙特卡洛边缘化学习难以计算的分布，使用参数化分布模型近似，解决了KL散度计算复杂度和优化过程可微性的问题。"
}