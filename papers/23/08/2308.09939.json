{
    "title": "Understanding Self-attention Mechanism via Dynamical System Perspective. (arXiv:2308.09939v1 [cs.CV])",
    "abstract": "The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, current explanations of this mechanism are mainly based on intuitions and experiences, while there still lacks direct modeling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspective of the residual neural network, we first show that the intrinsic stiffness phenomenon (SP) in the high-precision solution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model's representational ability t",
    "link": "http://arxiv.org/abs/2308.09939",
    "context": "Title: Understanding Self-attention Mechanism via Dynamical System Perspective. (arXiv:2308.09939v1 [cs.CV])\nAbstract: The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, current explanations of this mechanism are mainly based on intuitions and experiences, while there still lacks direct modeling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspective of the residual neural network, we first show that the intrinsic stiffness phenomenon (SP) in the high-precision solution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model's representational ability t",
    "path": "papers/23/08/2308.09939.json",
    "total_tokens": 823,
    "translated_title": "通过动力系统角度理解自注意机制",
    "translated_abstract": "自注意机制（SAM）广泛应用于人工智能的各个领域，并成功提升了不同模型的性能。然而，目前关于该机制的解释主要基于直觉和经验，而缺乏对SAM如何帮助性能的直接建模。为了解决这个问题，本文基于残差神经网络的动力系统视角，首先展示了在普通微分方程的高精度解中普遍存在的内在僵硬现象（SP）也存在于高性能神经网络（NN）中。因此，NN在特征层面上测量SP的能力对于获得高性能是必要的，并且是训练NN困难的重要因素。类似于解决僵硬ODE的自适应步长方法，我们展示了SAM也是一种能够增强模型表征能力的僵硬感知步长适配器。",
    "tldr": "本文通过动力系统的视角，展示了自注意机制在神经网络性能提升中的重要作用，并且将其比喻为一种僵硬感知步长适配器。",
    "en_tdlr": "This paper provides a dynamical system perspective on the self-attention mechanism and shows its important role in enhancing neural network performance, comparing it to a stiffness-aware step size adaptor."
}