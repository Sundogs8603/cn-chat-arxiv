{
    "title": "Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])",
    "abstract": "As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarizati",
    "link": "http://arxiv.org/abs/2308.12157",
    "context": "Title: Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])\nAbstract: As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarizati",
    "path": "papers/23/08/2308.12157.json",
    "total_tokens": 858,
    "translated_title": "用最长支持子序列评估忠实性",
    "translated_abstract": "随着日益复杂的语言模型的出现，它们的可信度成为一个关键问题，特别是在总结和问答等任务中。由于语言多样性和可能的答案种类繁多，确保它们的响应在语境中有根据、忠实可信是具有挑战性的。在本文中，我们引入了一种新方法来评估机器生成文本的忠实性，通过计算由语境支持的主张中最长的非连续子字符串，我们称之为最长支持子序列（LSS）。使用一个新的人工注释数据集，我们微调了一个模型以生成LSS。我们提出了一种新的评估方法，并证明当使用LSS时，这些度量与人类评分更相关，相对于不使用LSS时。我们的提出的度量在我们的数据集上对忠实性的最先进度量的改进达到了18%。我们的度量在总结任务上一直优于其他度量。",
    "tldr": "本文引入了一种新方法来评估机器生成文本的忠实性，通过计算由语境支持的主张中最长的非连续子字符串，称之为最长支持子序列（LSS）。证明了当使用LSS时，这种度量与人类评分更相关，并且在忠实性评估上相较于先前最先进的度量有18%的改进。"
}