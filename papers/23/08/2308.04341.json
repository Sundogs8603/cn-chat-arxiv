{
    "title": "Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage. (arXiv:2308.04341v1 [cs.LG])",
    "abstract": "Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.",
    "link": "http://arxiv.org/abs/2308.04341",
    "context": "Title: Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage. (arXiv:2308.04341v1 [cs.LG])\nAbstract: Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.",
    "path": "papers/23/08/2308.04341.json",
    "total_tokens": 964,
    "translated_title": "准确、可解释和私密的模型：在最小化训练数据泄漏的同时提供追索权",
    "translated_abstract": "机器学习模型在影响深远的领域越来越多地被用于预测个体结果。因此，许多模型为收到负面结果的个体提供了算法追索权。然而，追索权可以被攻击者利用来披露私人信息。本文首次尝试缓解此类攻击。我们提出了两种新颖的方法来生成差分隐私追索权：差分隐私模型（DPM）和拉普拉斯追索权（LR）。使用逻辑回归分类器和真实世界和合成数据集，我们发现DPM和LR在减少攻击者可以推断的信息方面表现良好，特别是在低FPR下。当训练数据集的大小足够大时，我们发现我们的新颖LR方法在保持模型和追索准确性的同时成功地防止了隐私泄漏。",
    "tldr": "本研究提出了两种新颖的方法来生成差分隐私追索权：差分隐私模型（DPM）和拉普拉斯追索权（LR）。在逻辑回归分类器和真实世界合成数据集的实验中发现，DPM和LR在减少攻击者可以推断的信息方面表现良好，特别是在低误报率下。当训练数据集足够大时，我们的新颖LR方法成功地防止了隐私泄漏。",
    "en_tdlr": "This study proposes two novel methods for generating differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Experimental results using logistic regression classifiers and real world and synthetic datasets demonstrate that DPM and LR perform well in reducing what an adversary can infer, especially at low false positive rates. Our novel LR method successfully prevents privacy leakage while maintaining model and recourse accuracy when the training dataset is sufficiently large."
}