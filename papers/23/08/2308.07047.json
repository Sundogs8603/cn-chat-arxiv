{
    "title": "No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning. (arXiv:2308.07047v1 [cs.LG])",
    "abstract": "Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we ",
    "link": "http://arxiv.org/abs/2308.07047",
    "context": "Title: No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning. (arXiv:2308.07047v1 [cs.LG])\nAbstract: Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we ",
    "path": "papers/23/08/2308.07047.json",
    "total_tokens": 935,
    "translated_title": "无需正则化：一种用于不完整标签分布学习的高效且有效模型",
    "translated_abstract": "标签分布学习（LDL）为样本分配软标签，即度量。现实中，获得完整度量通常是费力的，从而引发了不完整LDL（InLDL）。然而，InLDL经常遭受性能退化。为了解决这个问题，现有方法需要一个或多个显式正则化，导致繁琐的参数调整和额外的计算。我们认为，标签分布本身可能提供有用的先验信息，当适当使用时，可以在不需要任何显式正则化的情况下解决InLDL问题。在本文中，我们提供了一种合理的替代方法来利用这样的先验信息。我们的观点是大度量很可能更受关注，小度量很容易被忽视，而缺失的度量在InLDL中完全被忽略。为了学习准确的标签分布，重要的是不忽视观察到的小度量，而是给它们适当的大权重，同时逐渐增加缺失度量的权重。",
    "tldr": "本文提出了一种无需正则化的高效且有效的模型，用于解决不完整标签分布学习问题。通过正确处理观察到的小度量并逐渐增加缺失度量的权重，可以学习到准确的标签分布。",
    "en_tdlr": "This paper proposes an efficient and effective model for Incomplete Label Distribution Learning (InLDL) that does not require regularization. By properly handling small observed degrees and gradually increasing the weights of missing degrees, accurate label distributions can be learned."
}