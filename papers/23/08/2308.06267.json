{
    "title": "DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning. (arXiv:2308.06267v1 [cs.DC])",
    "abstract": "Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \\ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the",
    "link": "http://arxiv.org/abs/2308.06267",
    "context": "Title: DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning. (arXiv:2308.06267v1 [cs.DC])\nAbstract: Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \\ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the",
    "path": "papers/23/08/2308.06267.json",
    "total_tokens": 830,
    "translated_title": "DynamicFL：平衡通信动态和客户端操作用于联邦学习",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习（ML）范例，旨在利用数百万边缘设备上的分散数据来训练全局模型。与集中式学习相比，FL通过避免明确下载客户端数据来保护客户隐私。然而，在现实中，由于地理分布的边缘设备（如移动设备、汽车、火车或地铁）的网络高度动态，从参与设备聚合所有模型更新将导致不可避免的长尾延迟。这将严重降低训练过程的效率。为了解决FL场景中高系统异质性的时间敏感性问题，我们提出了一种新的FL框架DynamicFL，通过考虑边缘设备上的通信动态和数据质量，并采用特殊设计的客户端操作策略，\\ours基于动态网络条件和先前的网络预测选择客户端进行模型更新。",
    "tldr": "DynamicFL是一个解决在联邦学习中通信和时效性问题的新框架，通过考虑边缘设备的通信动态和数据质量，以及采用特殊设计的客户端操作策略，实现了高效的模型更新。"
}