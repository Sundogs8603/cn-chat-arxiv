{
    "title": "Evaluation of really good grammatical error correction. (arXiv:2308.08982v1 [cs.CL])",
    "abstract": "Although rarely stated, in practice, Grammatical Error Correction (GEC) encompasses various models with distinct objectives, ranging from grammatical error detection to improving fluency. Traditional evaluation methods fail to fully capture the full range of system capabilities and objectives. Reference-based evaluations suffer from limitations in capturing the wide variety of possible correction and the biases introduced during reference creation and is prone to favor fixing local errors over overall text improvement. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outpe",
    "link": "http://arxiv.org/abs/2308.08982",
    "context": "Title: Evaluation of really good grammatical error correction. (arXiv:2308.08982v1 [cs.CL])\nAbstract: Although rarely stated, in practice, Grammatical Error Correction (GEC) encompasses various models with distinct objectives, ranging from grammatical error detection to improving fluency. Traditional evaluation methods fail to fully capture the full range of system capabilities and objectives. Reference-based evaluations suffer from limitations in capturing the wide variety of possible correction and the biases introduced during reference creation and is prone to favor fixing local errors over overall text improvement. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outpe",
    "path": "papers/23/08/2308.08982.json",
    "total_tokens": 845,
    "translated_title": "对优质语法错误修正的评估",
    "translated_abstract": "尽管很少提及，在实践中，语法错误修正（GEC）涵盖了各种具有不同目标的模型，范围从语法错误检测到改善流畅度。传统的评估方法无法完全捕捉系统的全部能力和目标。基于参考的评估受限于捕捉可能修正的各种多样化以及参考创建过程中引入的偏见，易于偏好修复局部错误而忽视整体文本改进。大型语言模型（LLMs）的出现进一步凸显了这些评估策略的缺点，强调了评估方法的范式转变的必要性。在当前研究中，我们使用最近发布的瑞典学习者文本数据集对各种GEC系统进行全面评估。评估使用已建立的评估指标和人工评委进行。我们发现，在几次训练的情况下，GPT-3在性能上遥遥领先。",
    "tldr": "这项研究在最近发布的瑞典学习者文本数据集上对各种GEC系统进行了综合评估，并发现在几次训练的情况下，GPT-3具有明显的优势。",
    "en_tdlr": "This study comprehensively evaluated various GEC systems using a recently published dataset of Swedish learner texts and found that GPT-3 has a significant advantage in a few-shot setting."
}