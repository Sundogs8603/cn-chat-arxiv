{
    "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])",
    "abstract": "Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \\textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new inst",
    "link": "http://arxiv.org/abs/2308.05696",
    "context": "Title: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])\nAbstract: Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \\textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new inst",
    "path": "papers/23/08/2308.05696.json",
    "total_tokens": 837,
    "translated_title": "复杂性与对齐之间固有关系的初步研究",
    "translated_abstract": "使用开放域指令数据培训大型语言模型(LLMs)在对齐到最终任务和用户偏好方面取得了显著成功。大量研究表明，提高指令数据的质量和多样性始终能够改善性能。然而，作为一个关键指标，数据复杂性的影响尚未被充分探索，主要包括三个方面：(1)扩展规律，性能改进在复杂性增加时的可持续性尚不确定，(2)额外的标记，复杂性带来的改进是否来自引入更多训练标记，以及(3)课程设置，将从简单到困难的指令纳入是否具有潜在优势也尚未完全理解。在本文中，我们提出了\"tree-instruct\"，以可控的方式系统地增强指令数据的复杂性。这种方法将指定数量的节点添加到指令语义树中，从而产生新的指令数据。",
    "tldr": "本研究初步探讨复杂性与对齐之间的固有关系。通过使用\"tree-instruct\"方法来增强指令数据的复杂性，可以在对齐到任务和用户偏好方面提供更好的性能。"
}