{
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "abstract": "arXiv:2308.09124v2 Announce Type: replace  Abstract: Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
    "link": "https://arxiv.org/abs/2308.09124",
    "context": "Title: Linearity of Relation Decoding in Transformer Language Models\nAbstract: arXiv:2308.09124v2 Announce Type: replace  Abstract: Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
    "path": "papers/23/08/2308.09124.json",
    "total_tokens": 747,
    "translated_title": "Transformer语言模型中关系解码的线性性",
    "translated_abstract": "Transformer语言模型中编码的许多知识可以用关系的形式表达：词语及其同义词之间的关系，实体及其属性之间的关系等。我们展示，对于某些关系子集，这种计算可以很好地近似为对主题表示进行单一线性转换。线性关系表示可以通过从单个提示构建对LM的一阶近似来获得，并且可应用于各种事实，常识和语言关系。然而，我们还发现许多情况，LM的预测虽然准确地捕捉了关系知识，但这种知识并没有线性地编码在它们的表示中。因此，我们的结果揭示了Transformer语言模型中一种简单、可解释但异质部署的知识表示策略。",
    "tldr": "在Transformer语言模型中，部分关系的计算可以通过对主题表示进行单一线性转换来很好地近似，但并非所有关系都能通过线性编码。"
}