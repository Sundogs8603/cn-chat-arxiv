{
    "title": "SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])",
    "abstract": "RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF ta",
    "link": "http://arxiv.org/abs/2308.11200",
    "context": "Title: SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])\nAbstract: RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF ta",
    "path": "papers/23/08/2308.11200.json",
    "total_tokens": 1000,
    "translated_title": "SegRNN: 长期时间序列预测的分段循环神经网络",
    "translated_abstract": "在长期时间序列预测（LTSF）领域中，基于循环神经网络（RNN）的方法在处理过长的回溯窗口和预测范围时面临挑战。因此，该领域的主导地位已经转向Transformer、MLP和CNN方法。RNN在LTSF中存在限制的根本原因是循环迭代的数量相当多。为了解决这些问题，我们提出了两种减少RNN在LTSF任务中迭代次数的新策略：分段迭代和并行多步预测（PMF）。将这些策略结合起来的SegRNN显著减少了LTSF所需的循环迭代次数，从而显著提高了预测准确性和推理速度。大量实验表明，SegRNN不仅优于现有的基于Transformer模型的方法，还将运行时间和内存使用量减少了超过78%。这些成果为RNN在LTSF任务中的出色表现提供了强有力的证据。",
    "tldr": "SegRNN是一种针对长期时间序列预测任务的分段循环神经网络，通过两种新策略（分段迭代和并行多步预测）显著减少了循环迭代次数，提高了预测准确性和推理速度。与现有的基于Transformer模型的方法相比，SegRNN不仅表现更好，还大幅减少了运行时间和内存使用量。",
    "en_tdlr": "SegRNN is a segment recurrent neural network designed for long-term time series forecasting. By incorporating two novel strategies (segment-wise iterations and parallel multi-step forecasting), SegRNN significantly reduces the number of recurrent iterations, leading to improved forecast accuracy and inference speed. Compared to state-of-the-art Transformer-based models, SegRNN outperforms them while reducing runtime and memory usage by over 78%."
}