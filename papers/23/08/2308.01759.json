{
    "title": "Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])",
    "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributi",
    "link": "http://arxiv.org/abs/2308.01759",
    "context": "Title: Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])\nAbstract: Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributi",
    "path": "papers/23/08/2308.01759.json",
    "total_tokens": 926,
    "translated_title": "Bag of Policies for Distributional Deep Exploration（分布式深度探索中的一种策略组合方法）",
    "translated_abstract": "在强化学习中，高效地探索复杂环境仍然是一个重大挑战。与以往的启发式机制相比，我们关注的是在分布式强化学习中进行深度探索。我们提出了一种通用的方法，袋子策略（Bag of Policies，BoP），它可以在任何返回分布估计器的基础上构建，通过维护一个副本的种群。BoP由多个独立更新的头部组成。在训练过程中，每个episode由一个头部控制，收集到的状态-动作对被用来离线更新所有头部，为每个头部提供不同的学习信号，从而多样化学习和行为。我们通过使用贝叶斯分布进行分布式的actor-critic群体实现了BoP方法，以测试乐观集成方法是否能够像Bootstrapped DQN在标量强化学习中一样提高分布式强化学习的效果。",
    "tldr": "本论文提出了一种名为\"袋子策略\"的通用方法，用于分布式强化学习中的深度探索。该方法通过维护一个副本的种群，利用独立更新的头部和离线更新策略，为每个头部提供不同的学习信号，从而实现了多样化的学习和行为。"
}