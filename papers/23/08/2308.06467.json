{
    "title": "Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (arXiv:2308.06467v1 [cs.LG])",
    "abstract": "Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the imp",
    "link": "http://arxiv.org/abs/2308.06467",
    "context": "Title: Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (arXiv:2308.06467v1 [cs.LG])\nAbstract: Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the imp",
    "path": "papers/23/08/2308.06467.json",
    "total_tokens": 973,
    "translated_title": "并不那么强大：评估深度神经网络对未知对抗攻击的鲁棒性",
    "translated_abstract": "深度神经网络(DNNs)在各种应用中取得了显著的成就，如分类、识别和预测，这引起了对其属性的增加关注。传统DNN的一个基本属性是它们对输入数据的修改的脆弱性，这导致了对对抗攻击的调查。这些攻击通过操纵数据来误导DNN。本研究旨在挑战针对对抗攻击的当代防御机制的有效性和泛化性。具体而言，我们探讨了Ilyas等人提出的假设，即DNN的图像特征可以是鲁棒的或非鲁棒的，而对抗性攻击针对的是后者。该假设认为，仅在由鲁棒特征组成的数据集上训练DNN应该产生对抗攻击具有抵抗力的模型。然而，我们的实验表明这并不普遍成立。为了进一步了解我们的发现，我们对我们的实验结果进行了分析。",
    "tldr": "这项研究旨在评估深度神经网络对未知对抗攻击的鲁棒性，并挑战针对这些攻击的现有防御机制的有效性。实验结果表明，在仅使用鲁棒特征的数据集上训练的DNN模型并不一定能抵抗对抗性攻击。"
}