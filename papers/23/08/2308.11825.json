{
    "title": "Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks. (arXiv:2308.11825v1 [cs.AR])",
    "abstract": "Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.  Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memor",
    "link": "http://arxiv.org/abs/2308.11825",
    "context": "Title: Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks. (arXiv:2308.11825v1 [cs.AR])\nAbstract: Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.  Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memor",
    "path": "papers/23/08/2308.11825.json",
    "total_tokens": 908,
    "translated_title": "Accel-GCN: 高性能图卷积网络的GPU加速器设计",
    "translated_abstract": "图卷积网络（GCNs）在从各个领域的图数据中提取潜在信息方面起到了关键作用，但它们在主流GPU上的加速受到了工作负载不平衡和内存访问不规则性的挑战。为了解决这些挑战，我们提出了Accel-GCN，一种针对GCNs的GPU加速器架构。Accel-GCN的设计包括：（i）一个轻量级的度排序阶段，以组合具有相似度的节点；（ii）一个块级分区策略，动态调整warp工作负载大小，增强共享内存本地性和工作负载平衡，并减少与GNNAdvisor等设计相比的元数据开销；（iii）一种综合warp策略，提高在稠密矩阵的列维度上的内存合并和计算并行性。利用这些原则，我们针对GCNs在稀疏矩阵乘法（SpMM）中的核心进行了设计，采用块级分区和综合warp策略。这种方法提高了性能和多层记忆。",
    "tldr": "Accel-GCN使用轻量级度排序、块级分区和综合warp策略的GPU加速器架构，提高了图卷积网络（GCNs）在主流GPU上的性能。",
    "en_tdlr": "Accel-GCN is a GPU accelerator architecture for Graph Convolutional Networks (GCNs), which improves their performance on mainstream GPUs by using lightweight degree sorting, block-level partitioning, and combined warp strategy."
}