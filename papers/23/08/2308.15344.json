{
    "title": "Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary. (arXiv:2308.15344v1 [cs.LG])",
    "abstract": "Although Deep Neural Networks (DNNs), such as the convolutional neural networks (CNN) and Vision Transformers (ViTs), have been successfully applied in the field of computer vision, they are demonstrated to be vulnerable to well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The research in AEs has been active, and many adversarial attacks and explanations have been proposed since they were discovered in 2014. The mystery of the AE's existence is still an open question, and many studies suggest that DNN training algorithms have blind spots. The salient objects usually do not overlap with boundaries; hence, the boundaries are not the DNN model's attention. Nevertheless, recent studies show that the boundaries can dominate the behavior of the DNN models. Hence, this study aims to look at the AEs from a different perspective and proposes an imperceptible adversarial attack that systemically attacks the input image boundary for finding the AEs. The experimental results ha",
    "link": "http://arxiv.org/abs/2308.15344",
    "context": "Title: Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary. (arXiv:2308.15344v1 [cs.LG])\nAbstract: Although Deep Neural Networks (DNNs), such as the convolutional neural networks (CNN) and Vision Transformers (ViTs), have been successfully applied in the field of computer vision, they are demonstrated to be vulnerable to well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The research in AEs has been active, and many adversarial attacks and explanations have been proposed since they were discovered in 2014. The mystery of the AE's existence is still an open question, and many studies suggest that DNN training algorithms have blind spots. The salient objects usually do not overlap with boundaries; hence, the boundaries are not the DNN model's attention. Nevertheless, recent studies show that the boundaries can dominate the behavior of the DNN models. Hence, this study aims to look at the AEs from a different perspective and proposes an imperceptible adversarial attack that systemically attacks the input image boundary for finding the AEs. The experimental results ha",
    "path": "papers/23/08/2308.15344.json",
    "total_tokens": 915,
    "translated_title": "从图像边界进行的对深度神经网络的不可察觉对抗攻击",
    "translated_abstract": "虽然深度神经网络（DNNs），例如卷积神经网络（CNN）和视觉转换器（ViTs），已成功应用于计算机视觉领域，但它们被证明容易被伪装成对抗性样本（AEs）所欺骗。对AE的研究一直很活跃，自从2014年发现以来已经提出了许多对抗性攻击和解释。AE存在的奥秘仍然是一个未解之谜，许多研究表明DNN训练算法存在盲区。显著对象通常不与边界重叠，因此边界不是DNN模型的关注点。然而，最近的研究表明，边界可以主导DNN模型的行为。因此，本研究旨在从不同的角度观察AE，并提出一种对输入图像边界进行系统性攻击以寻找AE的不可察觉的对抗攻击。实验结果表明，",
    "tldr": "本文提出了一种对深度神经网络进行的不可察觉对抗攻击，通过系统性地攻击输入图像边界来查找对抗性样本。实验结果表明，边界可以主导DNN模型的行为。",
    "en_tdlr": "This paper proposes an imperceptible adversarial attack on deep neural networks by systematically attacking the input image boundary to find adversarial examples. The experimental results show that the boundaries can dominate the behavior of the DNN models."
}