{
    "title": "Distilled GPT for Source Code Summarization",
    "abstract": "A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as \"changes all visible polygons to the color blue\" can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation. Our model is small enough (350m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT-3.",
    "link": "https://arxiv.org/abs/2308.14731",
    "context": "Title: Distilled GPT for Source Code Summarization\nAbstract: A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as \"changes all visible polygons to the color blue\" can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation. Our model is small enough (350m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT-3.",
    "path": "papers/23/08/2308.14731.json",
    "total_tokens": 879,
    "translated_title": "用于源代码摘要的蒸馏GPT",
    "translated_abstract": "代码摘要是源代码的简短自然语言描述。摘要通常只有一句话，但却构成了开发者文档的核心。例如，“将所有可见的多边形变为蓝色”这样的简短描述可以让程序员对代码的功能有一个高层次的了解，而无需阅读代码本身。最近，基于大型语言模型（如ChatGPT）的产品已经展示了自动编写这些描述的强大能力。然而，为了使用这些工具，程序员必须将他们的代码发送给不可信的第三方进行处理（例如，通过API调用）。这种失去控制权的情况对许多组织来说是不可接受的。在本文中，我们提出了一种替代方案：我们使用由GPT-3.5生成的样本输出来训练一个开源模型，这个过程与知识蒸馏相关。我们的模型足够小（350m参数）以在单个16GB GPU上运行，但我们在评估中展示了它足够大以模仿GPT-3。",
    "tldr": "这篇论文提出了一种用于源代码摘要的蒸馏GPT方法，通过训练一个开源模型，避免了将代码发送给不可信的第三方进行处理的安全问题。",
    "en_tdlr": "This paper presents a distilled GPT approach for source code summarization, which trains an open source model to avoid the security concern of sending code to untrusted third parties for processing."
}