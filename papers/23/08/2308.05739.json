{
    "title": "Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])",
    "abstract": "Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a \"surrogate\" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competit",
    "link": "http://arxiv.org/abs/2308.05739",
    "context": "Title: Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics. (arXiv:2308.05739v1 [cs.CV])\nAbstract: Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a \"surrogate\" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competit",
    "path": "papers/23/08/2308.05739.json",
    "total_tokens": 940,
    "translated_title": "永远不给任何零梯度：学习非可微图形的局部替代损失",
    "translated_abstract": "基于梯度的优化在图形领域变得普遍，但不幸的是无法应用于具有未定义或零梯度的问题。为了解决这个问题，可以通过手动替换损失函数来使用类似极小值但可微的“替代损失”。我们提出的ZeroGrads框架通过学习目标函数的神经逼近，即替代损失，来自动化这个过程，从而可以通过任意黑盒图形流程进行微分。我们训练替代损失在目标函数的主动平滑版本上，并鼓励局部性，使替代损失的容量集中在当前训练阶段的关键内容上。拟合是在线执行的，与参数优化同时进行，自监督进行，无需预先计算数据或预训练模型。由于目标的采样是昂贵的（需要完整的渲染或模拟运行），我们设计了一个高效的采样方案，以实现可行的运行时间和竞争力。",
    "tldr": "本论文提出了ZeroGrads框架，通过学习非可微图形的局部替代损失函数来解决无梯度问题，并通过主动平滑和局部性约束优化替代损失的拟合，同时设计了高效的采样方案，实现了可行的运行时间和竞争力。"
}