{
    "title": "LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models. (arXiv:2308.13207v1 [cs.CL])",
    "abstract": "The advent of Large Language Models (LLM) has revolutionized the field of natural language processing, enabling significant progress in various applications. One key area of interest is the construction of Knowledge Bases (KB) using these powerful models. Knowledge bases serve as repositories of structured information, facilitating information retrieval and inference tasks. Our paper proposes LLM2KB, a system for constructing knowledge bases using large language models, with a focus on the Llama 2 architecture and the Wikipedia dataset. We perform parameter efficient instruction tuning for Llama-2-13b-chat and StableBeluga-13B by training small injection models that have only 0.05 % of the parameters of the base models using the Low Rank Adaptation (LoRA) technique. These injection models have been trained with prompts that are engineered to utilize Wikipedia page contexts of subject entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer relevant object entities fo",
    "link": "http://arxiv.org/abs/2308.13207",
    "context": "Title: LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models. (arXiv:2308.13207v1 [cs.CL])\nAbstract: The advent of Large Language Models (LLM) has revolutionized the field of natural language processing, enabling significant progress in various applications. One key area of interest is the construction of Knowledge Bases (KB) using these powerful models. Knowledge bases serve as repositories of structured information, facilitating information retrieval and inference tasks. Our paper proposes LLM2KB, a system for constructing knowledge bases using large language models, with a focus on the Llama 2 architecture and the Wikipedia dataset. We perform parameter efficient instruction tuning for Llama-2-13b-chat and StableBeluga-13B by training small injection models that have only 0.05 % of the parameters of the base models using the Low Rank Adaptation (LoRA) technique. These injection models have been trained with prompts that are engineered to utilize Wikipedia page contexts of subject entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer relevant object entities fo",
    "path": "papers/23/08/2308.13207.json",
    "total_tokens": 912,
    "translated_title": "LLM2KB: 使用经过指令调整的上下文感知大语言模型构建知识库",
    "translated_abstract": "大语言模型（LLM）的出现彻底改变了自然语言处理领域，使得在各种应用中取得了重大进展。其中一个关键领域是利用这些强大的模型构建知识库（KB）。知识库作为结构化信息的存储库，能够促进信息检索和推理任务。我们的论文提出了LLM2KB，这是一个使用大语言模型构建知识库的系统，重点关注于Llama 2架构和维基百科数据集。我们通过训练小的注入模型来进行参数高效的指令调整，这些注入模型仅具有基础模型参数的0.05%，使用了低秩适应（LoRA）技术。这些注入模型通过使用密集通道检索（DPR）算法提取的主体实体的维基百科页面上下文相对应的提示进行训练，以回答相关的客体实体。",
    "tldr": "本文提出了LLM2KB，一种使用大语言模型构建知识库的系统，通过对Llama 2架构和维基百科数据集进行参数高效的指令调整，利用上下文感知和低秩适应技术，实现了有效构建知识库的目标。",
    "en_tdlr": "This paper proposes LLM2KB, a system for constructing knowledge bases using large language models. By performing parameter efficient instruction tuning and leveraging context awareness and low rank adaptation techniques, LLM2KB achieves effective knowledge base construction based on the Llama 2 architecture and the Wikipedia dataset."
}