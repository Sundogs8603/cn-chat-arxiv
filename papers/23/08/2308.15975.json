{
    "title": "RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v1 [cs.RO])",
    "abstract": "For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.",
    "link": "http://arxiv.org/abs/2308.15975",
    "context": "Title: RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v1 [cs.RO])\nAbstract: For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.",
    "path": "papers/23/08/2308.15975.json",
    "total_tokens": 867,
    "translated_title": "RoboTAP: 追踪任意点进行少样本视觉模仿",
    "translated_abstract": "为了使机器人在实验室和专门的工厂之外也能发挥作用，我们需要一种快速教授它们新的有用行为的方法。当前的方法要么缺乏普适性以进行新任务的上线，而不需要特定任务的工程化，要么缺乏数据效率，无法在实践中使用的时间范围内完成。在这项工作中，我们探索了稠密追踪作为一种表示工具，以实现更快速、更普适的示教学习。我们的方法利用Track-Any-Point (TAP)模型，将演示中的相关运动隔离出来，并参数化一个低级控制器，在场景配置发生变化时重现该运动。我们展示了这种方法可以生成稳健的机器人策略，可以解决复杂的物体排列任务，如形状匹配、叠放，甚至可以完成完整的路径跟踪任务，如施胶和粘合物体，所有这些任务的演示可以在几分钟内收集到。",
    "tldr": "RoboTAP通过利用稠密追踪技术实现了快速、普适的学习，可以从短时间内收集的演示中解决复杂的物体排列任务。",
    "en_tdlr": "RoboTAP enables fast and general learning by using dense tracking, and can solve complex object-arrangement tasks from demonstrations collected in a short amount of time."
}