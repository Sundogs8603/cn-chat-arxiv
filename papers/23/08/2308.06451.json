{
    "title": "Semantic Equivariant Mixup. (arXiv:2308.06451v1 [cs.CV])",
    "abstract": "Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mix",
    "link": "http://arxiv.org/abs/2308.06451",
    "context": "Title: Semantic Equivariant Mixup. (arXiv:2308.06451v1 [cs.CV])\nAbstract: Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mix",
    "path": "papers/23/08/2308.06451.json",
    "total_tokens": 744,
    "translated_title": "语义等变 Mixup",
    "translated_abstract": "Mixup是一种广为使用的数据增强技术，通过在标签等变的基础上创建混合样本，扩展训练分布并规范神经网络。然而，先前的Mixup变体可能未能充分利用训练过程中混合样本中的独立于标签的信息，其中通常包含更丰富的语义信息。为了进一步发挥Mixup的作用，我们首先改进了先前的标签等变假设为语义等变假设，即输入数据的比例混合应导致相应表示的比例混合。然后，我们提出了一种在表示级别上的通用Mixup正则化方法，可以通过混合样本中的语义信息进一步规范模型。",
    "tldr": "这篇论文提出了一种基于语义等变的Mixup方法，通过在表示级别上进行混合来充分利用混合样本中的语义信息，进一步规范神经网络。",
    "en_tdlr": "This paper presents a semantic-equivariant Mixup method that utilizes semantic information in mixed samples by conducting mixing at the representation level, further regularizing neural networks."
}