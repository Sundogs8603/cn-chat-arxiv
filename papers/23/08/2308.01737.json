{
    "title": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])",
    "abstract": "With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain",
    "link": "http://arxiv.org/abs/2308.01737",
    "context": "Title: MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])\nAbstract: With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain",
    "path": "papers/23/08/2308.01737.json",
    "total_tokens": 868,
    "translated_title": "MAP: 一个模型无关的预训练框架用于点击率预测",
    "translated_abstract": "随着个性化在线服务的广泛应用，点击率（CTR）预测越来越受到关注和研究。CTR预测的最突出特点是其多字段分类数据格式和庞大而日益增长的数据量。神经模型的大容量有助于在监督学习范式下消化如此大量的数据，但是它们未能充分利用大量数据的潜力，因为1比特的点击信号不足以指导模型学习功能强大的特征和实例表示。自我监督学习范式提供了更有前景的预训练-微调解决方案，以更好地利用大量用户点击日志并学习更广义和有效的表示。然而，对于CTR预测的自我监督学习仍然是一个开放的问题，因为当前在这方面的工作仅仅是初步和基础的。为此，我们提出了一个模型无关的预训练框架。",
    "tldr": "提出了一个模型无关的预训练框架，用于点击率预测，可以更好地利用多字段分类数据和大量用户点击日志，学习更广义和有效的特征和实例表示。"
}