{
    "title": "INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing. (arXiv:2308.05930v1 [cs.AR])",
    "abstract": "An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kern",
    "link": "http://arxiv.org/abs/2308.05930",
    "context": "Title: INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing. (arXiv:2308.05930v1 [cs.AR])\nAbstract: An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kern",
    "path": "papers/23/08/2308.05930.json",
    "total_tokens": 946,
    "translated_title": "INR-Arch：一种用于隐式神经表征处理中任意阶梯度计算的数据流架构和编译器",
    "translated_abstract": "越来越多的研究人员发现了对各种应用的n阶梯度计算的用途，包括图形、元学习（MAML）、科学计算以及最近的隐式神经表征（INR）。最近的工作表明，INR的梯度可以直接用于编辑其所代表的数据，而无需将其转换回离散表示。然而，对于表示为计算图的函数，传统架构在高阶梯度的高需求和数据移动的高复杂度下面临挑战，这使得它成为FPGA加速的有希望的目标。在这项工作中，我们引入了INR-Arch，这是一个将n阶梯度的计算图转化为硬件优化的数据流架构的框架。我们通过两个阶段来解决这个问题。首先，我们设计了一个使用FIFO流和优化的计算内核的数据流架构。",
    "tldr": "本论文提出了INR-Arch，这是一种用于隐式神经表征处理中任意阶梯度计算的数据流架构和编译器。该工作通过将计算图转化为硬件优化的数据流架构来解决了传统架构在高阶梯度计算上的挑战，为FPGA加速提供了有希望的目标。"
}