{
    "title": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])",
    "abstract": "Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the \"unseen regions\" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the \"indescribable\" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an",
    "link": "http://arxiv.org/abs/2308.08545",
    "context": "Title: TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])\nAbstract: Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the \"unseen regions\" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the \"indescribable\" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an",
    "path": "papers/23/08/2308.08545.json",
    "total_tokens": 932,
    "translated_title": "TeCH: 文本引导的逼真服饰人物重建",
    "translated_abstract": "尽管最近在从单张图像中重建着装人物方面取得了研究进展，但准确恢复“未曾看见的区域”并添加高级细节仍然是一个未解决的挑战，缺乏关注。现有的方法往往会生成过于平滑的背面表面和模糊的纹理。本文提出了TeCH模型，通过利用1）通过服装解析模型和视觉问答（VQA）自动生成的描述性文本提示（例如，服饰、颜色、发型）；2）经过个性化微调的文本到图像扩散模型（T2I）来学习“无法描述”的外观，从而对3D人类形象进行重建。为了以更低的成本表示高分辨率的3D服饰人物，我们提出了基于DMTet的混合3D表示，它由明确的身体形状网格和一个……（摘要较长，省略部分内容）",
    "tldr": "该论文提出了TeCH模型，通过文本引导的方法来重建逼真的服饰人物。模型可以准确恢复“未曾看见的区域”并添加高级细节，采用了基于DMTet的混合3D表示以达到更低的成本。",
    "en_tdlr": "The paper introduces the TeCH model for text-guided reconstruction of lifelike clothed humans. The model accurately restores \"unseen regions\" with high-level details by leveraging descriptive text prompts and a personalized Text-to-Image diffusion model. It proposes a hybrid 3D representation based on DMTet for affordable high-resolution reconstruction."
}