{
    "title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads",
    "abstract": "arXiv:2308.14456v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference cos",
    "link": "https://arxiv.org/abs/2308.14456",
    "context": "Title: Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads\nAbstract: arXiv:2308.14456v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference cos",
    "path": "papers/23/08/2308.14456.json",
    "total_tokens": 842,
    "translated_title": "语音自监督表示基准测试：更大的探测头的案例",
    "translated_abstract": "自监督学习（SSL）利用大量未标记的语音数据集，在减少注释数据量的情况下达到了出色的性能。大量提出的方法促使出现了全面的基准测试，评估它们在一组探索语音信号各个方面的下游任务上的性能。然而，尽管考虑到的任务数量不断增加，大多数提议仍依赖于一个将冻结的SSL表示映射到任务标签的下游架构。本研究研究了如何改变探测头架构会影响基准测试结果。有趣的是，我们发现改变下游架构结构会导致评估模型的性能排名出现显著波动。与语音SSL基准测试中的常见做法相对立，我们评估了更大容量的探测头，展示了它们对性能和推断成本的影响。",
    "tldr": "本研究探讨了如何改变探测头架构会对基准测试结果产生影响，在语音自监督学习中评估了更大容量的探测头，展示了其对性能和推断成本的影响。",
    "en_tdlr": "This study examines how changes in the probing head architecture affect benchmarking results, evaluates larger-capacity probing heads in speech self-supervised learning, and demonstrates their impact on performance and inference cost."
}