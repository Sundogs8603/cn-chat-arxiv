{
    "title": "Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level. (arXiv:2308.13506v1 [cs.CL])",
    "abstract": "As research on machine translation moves to translating text beyond the sentence level, it remains unclear how effective automatic evaluation metrics are at scoring longer translations. In this work, we first propose a method for creating paragraph-level data for training and meta-evaluating metrics from existing sentence-level data. Then, we use these new datasets to benchmark existing sentence-level metrics as well as train learned metrics at the paragraph level. Interestingly, our experimental results demonstrate that using sentence-level metrics to score entire paragraphs is equally as effective as using a metric designed to work at the paragraph level. We speculate this result can be attributed to properties of the task of reference-based evaluation as well as limitations of our datasets with respect to capturing all types of phenomena that occur in paragraph-level translations.",
    "link": "http://arxiv.org/abs/2308.13506",
    "context": "Title: Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level. (arXiv:2308.13506v1 [cs.CL])\nAbstract: As research on machine translation moves to translating text beyond the sentence level, it remains unclear how effective automatic evaluation metrics are at scoring longer translations. In this work, we first propose a method for creating paragraph-level data for training and meta-evaluating metrics from existing sentence-level data. Then, we use these new datasets to benchmark existing sentence-level metrics as well as train learned metrics at the paragraph level. Interestingly, our experimental results demonstrate that using sentence-level metrics to score entire paragraphs is equally as effective as using a metric designed to work at the paragraph level. We speculate this result can be attributed to properties of the task of reference-based evaluation as well as limitations of our datasets with respect to capturing all types of phenomena that occur in paragraph-level translations.",
    "path": "papers/23/08/2308.13506.json",
    "total_tokens": 829,
    "translated_title": "在段落级别上训练和元评估机器翻译评估指标",
    "translated_abstract": "随着机器翻译研究的发展，将文本翻译到句子以上的级别，自动评估指标在评分更长的翻译上的有效性仍不清楚。在本研究中，我们首先提出了一种方法，通过使用现有的句子级别数据创建段落级别的数据，用于训练和元评估指标。然后，我们使用这些新数据集来评估现有的句子级别指标，并在段落级别上训练学习指标。有趣的是，我们的实验结果表明，使用句子级别的指标来评分整个段落与使用专门设计用于段落级别工作的指标一样有效。我们推测这个结果可能归因于参考评估任务的特性以及我们的数据集在捕捉段落级别翻译中出现的所有类型现象方面的局限性。",
    "tldr": "本论文研究了在机器翻译中，如何在段落级别评估翻译质量。实验结果表明，使用句子级别的评估指标来评分整个段落与使用段落级别的指标一样有效。",
    "en_tdlr": "This paper investigates the evaluation of translation quality at the paragraph level in machine translation. Experimental results show that using sentence-level evaluation metrics to score entire paragraphs is equally effective as using paragraph-level metrics."
}