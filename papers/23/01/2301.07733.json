{
    "title": "Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)",
    "abstract": "D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \\url{https://github.com/facebookresearch/dadaptation}.",
    "link": "http://arxiv.org/abs/2301.07733",
    "context": "Title: Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)\nAbstract: D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \\url{https://github.com/facebookresearch/dadaptation}.",
    "path": "papers/23/01/2301.07733.json",
    "total_tokens": 790,
    "translated_title": "通过D适应实现学习率自由学习",
    "translated_abstract": "D适应是一种自动设置学习率的方法，可以渐近地实现最优收敛速率，用于最小化凸性Lipschitz函数，无需回溯或线性搜索，并且每步无需进行额外的函数值或梯度评估。我们的方法是这一类问题的第一个无超参数且收敛速率无需额外对数因子改进的方法。我们针对SGD和Adam变体展示了广泛的实验，其中该方法自动匹配手动调整的学习率，在十多个不同的机器学习问题中应用，包括大规模的视觉和语言问题。开源实现在 \\url{https://github.com/facebookresearch/dadaptation}.",
    "tldr": "D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。",
    "en_tdlr": "D-Adaptation is a method that can automatically set the learning rate for achieving the optimal convergence rate for minimizing convex Lipschitz functions, without needing hyperparameters or additional logarithmic factors. It can match hand-tuned learning rates for diverse machine learning problems, including large-scale vision and language problems. The open-source implementation is available at \\url{https://github.com/facebookresearch/dadaptation}."
}