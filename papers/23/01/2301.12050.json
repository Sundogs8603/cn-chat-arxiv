{
    "title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to",
    "link": "http://arxiv.org/abs/2301.12050",
    "context": "Title: Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to",
    "path": "papers/23/01/2301.12050.json",
    "total_tokens": 972,
    "translated_title": "基于语言引导的世界模型的具身决策制定",
    "translated_abstract": "强化学习代理通常在没有先前的世界知识的情况下进行学习。然而，如果初始化高层子目标和子目标之间的转换知识，强化学习代理可以利用此抽象世界模型（AWM）进行规划和探索。我们提出使用少量样本的大型语言模型（LLM）来假设AWM，通过世界经验进行验证，以提高强化学习代理的样本效率。我们的DECKARD代理将LLM引导的探索应用于Minecraft中的物品制作，分为两个阶段：（1）梦想阶段，代理使用LLM将任务分解为一系列子目标，即假设的AWM；（2）唤醒阶段，代理为每个子目标学习模块化策略并验证或纠正假设的AWM。我们通过LLMs假设AWM，然后根据代理经验验证AWM的方法不仅可以将样本效率提高一个数量级，而且还具有稳健性。",
    "tldr": "本论文研究使用少量的大型语言模型来提高强化学习代理的样本效率。通过假设抽象世界模型并通过代理的世界经验进行验证，可以进行规划和探索。这种方法不仅可提高样本效率一个数量级，而且还具有稳健性。",
    "en_tdlr": "This paper proposes using few-shot large language models to improve the sample efficiency of RL agents by hypothesizing an abstract world model (AWM) and verifying it based on agent experience. This method not only increases sample efficiency by an order of magnitude but also exhibits robustness."
}