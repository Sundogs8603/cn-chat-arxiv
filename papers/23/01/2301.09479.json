{
    "title": "Modality-Agnostic Variational Compression of Implicit Neural Representations. (arXiv:2301.09479v3 [stat.ML] UPDATED)",
    "abstract": "We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate dat",
    "link": "http://arxiv.org/abs/2301.09479",
    "context": "Title: Modality-Agnostic Variational Compression of Implicit Neural Representations. (arXiv:2301.09479v3 [stat.ML] UPDATED)\nAbstract: We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate dat",
    "path": "papers/23/01/2301.09479.json",
    "total_tokens": 883,
    "translated_title": "无模态偏见的隐式神经表示变分压缩算法",
    "translated_abstract": "我们提出了一种基于数据的函数视图，并用隐式神经表示（INR）参数化的无模态神经压缩算法。我们通过软门控机制将非线性映射到紧凑的潜在表示中，从而弥合了潜在编码和稀疏性之间的差距。这允许每个数据项通过子网络选择来定制共享的INR网络的专业化。在获取这种潜在表示的数据集后，我们在无模态空间中直接优化速率/失真的折衷方案，使用神经压缩。隐式神经表示的变分压缩（VC-INR）在具有相同表示容量的量化之前显示出改进的性能，同时优于其他INR技术所使用的先前量化方案。我们的实验结果显示，使用相同的算法而不需要任何特定于模态的归纳偏差，可以在各种不同的模态上取得卓越的结果。我们展示了在图像、气候数据、文本和音频数据上的结果。",
    "tldr": "提出了一种无模态偏见的隐式神经表示变分压缩算法，能够在不同的数据模态上表现出卓越的压缩性能和效果。",
    "en_tdlr": "A modality-agnostic neural compression algorithm based on Implicit Neural Representation (INR) is proposed, achieving superior compression performance and results across diverse data modalities without modality-specific biases."
}