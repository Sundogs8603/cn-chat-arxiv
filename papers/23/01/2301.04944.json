{
    "title": "ViTs for SITS: Vision Transformers for Satellite Image Time Series. (arXiv:2301.04944v3 [cs.CV] UPDATED)",
    "abstract": "In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classific",
    "link": "http://arxiv.org/abs/2301.04944",
    "context": "Title: ViTs for SITS: Vision Transformers for Satellite Image Time Series. (arXiv:2301.04944v3 [cs.CV] UPDATED)\nAbstract: In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classific",
    "path": "papers/23/01/2301.04944.json",
    "total_tokens": 951,
    "translated_title": "ViTs for SITS：基于Vision Transformer的卫星图像时序处理模型",
    "translated_abstract": "本文介绍了一种全自注意力模型——Temporo-Spatial Vision Transformer（TSViT），用于处理通用的卫星图像时序(SITS)，并将其基于Vision Transformer（ViT）进行了改进。TSViT将SITS记录在时空上划分成非重叠的块，这些块被记号化后由分解的时空编码器处理。我们认为，相比自然图像，时域优先空间次之的分解对于SITS处理更加直观，并提供了实验证据。此外，我们通过引入两种新的机制，即获取时间特定的编码和多个可学习类标记，增强了模型的判别能力。所有新设计的效果通过广泛的消融研究进行了评估。我们提出的架构在三个公开的SITS语义分割和分类中取得了显著的优势，达到了最先进水平。",
    "tldr": "本文介绍了一种全自注意力模型TSViT，用于处理通用的卫星图像时序(SITS)，通过分解的时空编码器处理非重叠的块，提出了两种新的机制以增强模型的判别能力，最终在SITS语义分割和分类中取得了显著的优势，达到了最先进水平。",
    "en_tdlr": "This paper proposes the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model based on the Vision Transformer (ViT) for processing general Satellite Image Time Series (SITS). The proposed architecture splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. The paper also introduces two novel mechanisms to enhance the model's discriminative power, and achieves state-of-the-art performance in SITS semantic segmentation and classification."
}