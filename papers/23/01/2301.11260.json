{
    "title": "Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming. (arXiv:2301.11260v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either (i) optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called \\textit{maximum optimality margin} which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data",
    "link": "http://arxiv.org/abs/2301.11260",
    "context": "Title: Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming. (arXiv:2301.11260v2 [cs.LG] UPDATED)\nAbstract: In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either (i) optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called \\textit{maximum optimality margin} which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data",
    "path": "papers/23/01/2301.11260.json",
    "total_tokens": 918,
    "translated_title": "最大最优性边际：上下文线性规划和逆线性规划的统一方法",
    "translated_abstract": "本文研究了预测-优化问题，其中机器学习预测任务的输出用作某个下游优化问题（例如线性规划的目标系数向量）的输入。该问题也被称为预测分析或上下文线性规划。现有方法在很大程度上要么受到（i）优化不可解性（非凸目标函数）/统计效率低下（子优一般化界限）的困扰，要么要求强条件（例如没有约束或损失校准）。我们开发了一种名为“最大最优性边际”的新方法，通过下游优化的最优性条件设计机器学习损失函数。最大边际公式既具有计算效率，又具有好的学习程序的理论性质。更重要的是，我们的新方法只需要训练数据中最优解的观测值。",
    "tldr": "本论文提出了一种名为“最大最优性边际”的新方法来解决预测-优化问题，通过下游优化的最优性条件设计机器学习损失函数，兼具计算效率和较好的理论性质，而且只需要训练数据中最优解的观测值。"
}