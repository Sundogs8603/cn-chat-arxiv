{
    "title": "Conformal inference is (almost) free for neural networks trained with early stopping. (arXiv:2301.11556v2 [stat.ML] UPDATED)",
    "abstract": "Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.",
    "link": "http://arxiv.org/abs/2301.11556",
    "context": "Title: Conformal inference is (almost) free for neural networks trained with early stopping. (arXiv:2301.11556v2 [stat.ML] UPDATED)\nAbstract: Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.",
    "path": "papers/23/01/2301.11556.json",
    "total_tokens": 808,
    "translated_title": "对于使用早停的神经网络，conformal推理是几乎免费的。",
    "translated_abstract": "基于保留数据的早停是一种流行的正则化技术，用于减轻过拟合并提高神经网络的预测准确性。使用早停训练的模型通常提供相对准确的预测，但除非进一步使用独立的保留数据进行校准，否则它们通常仍然缺乏精确的统计保证。本文通过将早停与conformal校准相结合，同时高效地重复使用相同的保留数据，解决了上述限制。这导致了既准确又能够提供精确预测推断的模型，而无需多次数据拆分或过于保守的调整。为不同的学习任务（异常值检测，多类分类，回归）开发了实际实现，并在真实数据上展示了它们的竞争性能。",
    "tldr": "本文介绍了一种将早停与conformal校准相结合的新方法，以解决使用早停训练的神经网络在缺乏独立校准数据时无法提供准确统计保证的问题。",
    "en_tdlr": "This paper introduces a novel method that combines early stopping with conformal calibration to address the issue of neural networks trained with early stopping lacking precise statistical guarantees without independent calibration data."
}