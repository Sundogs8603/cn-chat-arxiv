{
    "title": "SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)",
    "abstract": "In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa",
    "link": "http://arxiv.org/abs/2301.12357",
    "context": "Title: SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)\nAbstract: In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa",
    "path": "papers/23/01/2301.12357.json",
    "total_tokens": 904,
    "translated_title": "SPEED: 线性异方差 Bandit 策略评估的实验设计",
    "translated_abstract": "本文研究了线性 Bandit 下策略评估的最优数据收集问题。在策略评估中，我们需要估计多臂赌博机环境中执行目标策略将获得的期望收益。本文是首个专注于解决线性 Bandit 环境下包含异方差奖励噪声的策略评估的最优数据收集策略的工作。我们首先在线性 Bandit 环境下制定了加权最小二乘估计的最优设计，以减少目标策略价值的均方误差。接着，我们使用该设计来推导出数据收集期间每个动作的最优样本分配。然后，我们引入了一种名为 SPEED（Structured Policy Evaluation Experimental Design）的新算法，该算法跟踪最优设计，并计算其与最优设计的遗憾。最后，我们通过实验证明 SPEED 可以实现带有均方误差比较小的策略评估。",
    "tldr": "本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。",
    "en_tdlr": "This paper proposes a new algorithm SPEED for optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting, which achieves policy evaluation with mean squared error comparably small."
}