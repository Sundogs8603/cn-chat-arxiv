{
    "title": "Expected Gradients of Maxout Networks and Consequences to Parameter Initialization. (arXiv:2301.06956v2 [stat.ML] UPDATED)",
    "abstract": "We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks. Experiments with deep fully-connected and convolutional networks show that this strategy improves SGD and Adam training of deep maxout networks. In addition, we obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK.",
    "link": "http://arxiv.org/abs/2301.06956",
    "context": "Title: Expected Gradients of Maxout Networks and Consequences to Parameter Initialization. (arXiv:2301.06956v2 [stat.ML] UPDATED)\nAbstract: We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks. Experiments with deep fully-connected and convolutional networks show that this strategy improves SGD and Adam training of deep maxout networks. In addition, we obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK.",
    "path": "papers/23/01/2301.06956.json",
    "total_tokens": 757,
    "translated_title": "Maxout网络的期望梯度及其对参数初始化的影响",
    "translated_abstract": "本文研究了Maxout网络相对于输入和参数的梯度，并根据网络结构和参数分布得出梯度的矩上界。我们观察到，输入输出Jacobian的分布取决于输入，这使得稳定的参数初始化变得复杂。基于梯度矩，我们提出了避免在宽网络中梯度消失和爆炸的参数初始化策略。在深度全连接和卷积神经网络上的实验表明，这种策略改善了Maxout网络的SGD和Adam训练。此外，我们还得到了关于期望线性区域数量、期望曲线长度失真和NTK的精细界限结果。",
    "tldr": "本文研究了Maxout网络关于输入和参数的梯度，提出了避免梯度消失和爆炸的参数初始化策略，并在实验中证明了其有效性。",
    "en_tdlr": "This paper studies the gradients of a Maxout network with respect to inputs and parameters, proposes a parameter initialization strategy to avoid vanishing and exploding gradients, and demonstrates its effectiveness through experiments on deep neural networks. Refining bounds on the expected number of linear regions, expected curve length distortion, and NTK are obtained as well."
}