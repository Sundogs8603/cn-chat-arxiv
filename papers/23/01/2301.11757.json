{
    "title": "Mo\\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)",
    "abstract": "Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another \"language\" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source",
    "link": "http://arxiv.org/abs/2301.11757",
    "context": "Title: Mo\\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)\nAbstract: Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another \"language\" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source",
    "path": "papers/23/01/2301.11757.json",
    "total_tokens": 916,
    "translated_title": "Mo^usai: 使用长上下文潜在扩散进行文本到音乐生成",
    "translated_abstract": "近年来，大规模生成模型在文本领域取得了快速发展；然而，对于文本与另一种“语言”——音乐的关联关系，研究相对较少。音乐与文本一样，可以传达情感、故事和思想，具有自己独特的结构和语法。我们的工作通过一种高效、表现力强且能够处理长期结构的文本到音乐生成模型，将文本与音乐联系起来。具体而言，我们开发了Mo^usai，这是一个级联的两阶段潜在扩散模型，可以根据文本描述生成多分钟的高质量48kHz立体声音乐。此外，我们的模型具有高效性，可以在单个消费级GPU上进行实时推断，并具有合理的速度。通过实验证明了我们模型在多种标准下相对于现有音乐生成模型的优势。最后，为了推动开源文化，我们提供了一套开源的工具集。",
    "tldr": "本研究开发了一种高效、表现力强且能处理长期结构的文本到音乐生成模型Mo^usai，可以根据文本描述生成多分钟高质量音乐。通过实验证明了该模型在多个标准上的优势。",
    "en_tdlr": "This study develops an efficient and expressive text-to-music generation model called Mo^usai, which can generate high-quality music for multiple minutes based on textual descriptions. The model demonstrates its superiority over existing music generation models in various criteria."
}