{
    "title": "Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)",
    "abstract": "A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical \"risk\" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully ",
    "link": "http://arxiv.org/abs/2301.12767",
    "context": "Title: Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)\nAbstract: A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical \"risk\" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully ",
    "path": "papers/23/01/2301.12767.json",
    "total_tokens": 861,
    "translated_title": "压缩、泛化和学习",
    "translated_abstract": "压缩函数是一种将观测集缩小为尺寸减小的子集的映射，同时保留其信息内容。在多个应用中，新观测使压缩集发生变化的条件被解释为新观测带来了额外的信息，在学习理论中，这对应于错误分类或错误预测。本文建立了一个新理论的基础，允许在压缩的改变概率上保持控制（与学习应用中的统计“风险”相对应）。在适当的条件下，压缩集的基数被证明是压缩的改变概率的一致估计量（不对压缩集的尺寸设置上限）；此外，在普遍适用的偏好条件下获得了前所未有的紧密的有限样本边界来评估压缩的改变概率。所有结果都可以在完全应用中使用。",
    "tldr": "本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。",
    "en_tdlr": "This paper introduces a new theory that allows control over the probability of change in compression and provides tight finite-sample bounds to evaluate this probability. It is particularly relevant for applications in learning where it relates to classification errors and mispredictions."
}