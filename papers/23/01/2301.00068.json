{
    "title": "On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)",
    "abstract": "Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se",
    "link": "http://arxiv.org/abs/2301.00068",
    "context": "Title: On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)\nAbstract: Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se",
    "path": "papers/23/01/2301.00068.json",
    "total_tokens": 924,
    "translated_title": "关于遮蔽语言模型学习条件句的不一致性",
    "translated_abstract": "已经证明了在序列中学习预测遮蔽标记是一个对大型语言模型来说很有力的预训练目标。训练后，这些遮蔽语言模型可以提供基于双向上下文的标记分布。本论文展示了与常见假设相反，这种双向条件句经常表现出相当大的不一致性，即在考虑在一起时不能从一个连贯的联合分布导出它们。我们在遮蔽语言模型的两种常见风格（T5风格和BERT风格）的简单双字母词比较场景中通过实证量化了这种不一致性。例如，我们发现T5模型经常混淆自己对两个相似双字母词的偏好。我们还展示了不一致性在不同尺寸和配置的遮蔽语言模型中普遍存在，从RoBERTa-base到GLM-130B。作为解决这个问题的初始尝试，我们提出了条件句集合，在推断阶段处理这个问题。",
    "tldr": "本论文研究发现，遮蔽语言模型学习的条件句往往存在着不一致性，无法从一个连贯的联合分布中推导出来。我们通过实证发现这种不一致性普遍存在于不同尺寸和配置的遮蔽语言模型中。为了解决这个问题，我们提出了条件句集合方法来在推断阶段处理不一致性。"
}