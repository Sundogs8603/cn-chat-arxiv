{
    "title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (arXiv:2301.11913v2 [cs.DC] UPDATED)",
    "abstract": "Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap \"preemptible\" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model",
    "link": "http://arxiv.org/abs/2301.11913",
    "context": "Title: SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (arXiv:2301.11913v2 [cs.DC] UPDATED)\nAbstract: Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap \"preemptible\" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model",
    "path": "papers/23/01/2301.11913.json",
    "total_tokens": 980,
    "translated_title": "SWARM并行性: 训练大模型可以在通信效率上有惊人的效果",
    "translated_abstract": "许多深度学习应用受益于使用包含数十亿个参数的大模型。由于需要专用的HPC集群，训练这些模型通常非常昂贵。在这项工作中，我们考虑了训练大模型的替代方法：使用廉价的“可抢占”实例或从多个区域汇集现有资源。我们分析了这些条件下现有模型并行算法的性能，并找到了训练更大模型时通信密集度较低的配置。基于这些发现，我们提出了SWARM并行性，这是一种针对连接差、异构和不可靠设备的模型并行训练算法。SWARM在节点之间创建临时的随机化管道，并在出现故障时进行重新平衡。我们通过实验证实了我们的发现，并将SWARM并行性与现有的大规模训练方法进行了比较。最后，我们将我们的见解与压缩策略相结合，训练了一个大型的Transformer语言模型。",
    "tldr": "我们提出了SWARM并行性，一种用于训练大模型的模型并行算法，适用于连接差、异构和不可靠设备。通过在节点之间创建临时的随机化管道并进行重新平衡，SWARM可以实现更少的通信密集度。与现有的大规模训练方法相比，我们的方法具有更好的性能，并与压缩策略结合使用来训练大型Transformer语言模型。",
    "en_tdlr": "We propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous, and unreliable devices. By creating temporary randomized pipelines between nodes and re-balancing them in case of failure, SWARM achieves lower communication-intensive training for larger models. Our approach outperforms existing large-scale training methods and combines with compression strategies to train a large Transformer language model."
}