{
    "title": "Test of Time: Instilling Video-Language Models with a Sense of Time. (arXiv:2301.02074v2 [cs.CV] UPDATED)",
    "abstract": "Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIP, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when ",
    "link": "http://arxiv.org/abs/2301.02074",
    "context": "Title: Test of Time: Instilling Video-Language Models with a Sense of Time. (arXiv:2301.02074v2 [cs.CV] UPDATED)\nAbstract: Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIP, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when ",
    "path": "papers/23/01/2301.02074.json",
    "total_tokens": 987,
    "translated_title": "时光考验：为视频-语言模型注入时间感知能力",
    "translated_abstract": "在当今的视频理解模型中，建模和理解时间仍然是一个挑战。随着语言成为强大推理的关键驱动因素，对于基础视频-语言模型具备时间感知能力是必要的。在本文中，我们考虑了时间理解的一个特定方面：以before/after关系引出的时间顺序的一致性。我们发现，即使是这样简单的时间关系，七个现有的视频-语言模型都很难理解。然后，我们质疑在不从头开始训练的情况下，是否有可能为这些基础模型装备时间感知能力。为此，我们提出了一种基于视频-文本数据的后预训练的视频CLIP模型的时间适应配方。我们对适应后的模型进行了零-shot评估，其中包括需要不同程度的时间感知的三个下游任务的六个数据集。我们观察到，在需要更显式时间理解的任务（例如视频字幕生成）上，性能提升是令人鼓舞的。总体而言，我们的工作突显了为视频-语言模型配备时间感知能力的紧迫需求，并提供了一个可行的解决方案。",
    "tldr": "本文针对视频-语言模型中时间理解的问题，提出了一种基于后预训练的时间适应配方，使模型能够在需要时间理解的任务中获得性能提升。",
    "en_tdlr": "This paper proposes a temporal adaptation recipe based on post-pretraining on a small amount of video-text data for video-language models to understand temporal relations. The adapted models show improvement in performance for tasks requiring explicit time understanding, indicating the pressing need to equip video-language models with a sense of time."
}