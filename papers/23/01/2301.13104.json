{
    "title": "Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models. (arXiv:2301.13104v2 [cs.CV] UPDATED)",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) limits the amount of private information deep learning models can memorize during training. This is achieved by clipping and adding noise to the model's gradients, and thus networks with more parameters require proportionally stronger perturbation. As a result, large models have difficulties learning useful information, rendering training with DP-SGD exceedingly difficult on more challenging training tasks. Recent research has focused on combating this challenge through training adaptations such as heavy data augmentation and large batch sizes. However, these techniques further increase the computational overhead of DP-SGD and reduce its practical applicability. In this work, we propose using the principle of sparse model design to solve precisely such complex tasks with fewer parameters, higher accuracy, and in less time, thus serving as a promising direction for DP-SGD. We achieve such sparsity by design by introducing equiv",
    "link": "http://arxiv.org/abs/2301.13104",
    "context": "Title: Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models. (arXiv:2301.13104v2 [cs.CV] UPDATED)\nAbstract: Differentially Private Stochastic Gradient Descent (DP-SGD) limits the amount of private information deep learning models can memorize during training. This is achieved by clipping and adding noise to the model's gradients, and thus networks with more parameters require proportionally stronger perturbation. As a result, large models have difficulties learning useful information, rendering training with DP-SGD exceedingly difficult on more challenging training tasks. Recent research has focused on combating this challenge through training adaptations such as heavy data augmentation and large batch sizes. However, these techniques further increase the computational overhead of DP-SGD and reduce its practical applicability. In this work, we propose using the principle of sparse model design to solve precisely such complex tasks with fewer parameters, higher accuracy, and in less time, thus serving as a promising direction for DP-SGD. We achieve such sparsity by design by introducing equiv",
    "path": "papers/23/01/2301.13104.json",
    "total_tokens": 988,
    "translated_title": "等变差分隐私深度学习：DP-SGD为什么需要更稀疏的模型",
    "translated_abstract": "差分隐私随机梯度下降（DP-SGD）通过修剪和添加噪声来限制深度学习模型在训练过程中可以记忆的私有信息量。因此，具有更多参数的网络需要相应更强的扰动，导致大型模型难以学习有用信息，使得在更具挑战性的训练任务上使用DP-SGD变得非常困难。最近的研究集中在通过训练适应性技术，如大规模数据增强和大批量大小，来克服这一挑战。但是，这些技术进一步增加了DP-SGD的计算开销，并降低了其实际适用性。在本文中，我们提出了使用稀疏模型设计原则来解决这些复杂任务，具有更少的参数、更高的准确性和更短的时间，从而成为DP-SGD的一种有前途的方向。我们通过引入等变差分隐私深度学习来实现这种稀疏性设计，利用数据中的群体对称性来获得更稀疏的模型同时保持隐私保证。",
    "tldr": "本文提出等变差分隐私深度学习，利用数据中的群体对称性来实现更稀疏的模型，并保持隐私保证，从而解决了DP-SGD在更具挑战性的任务上所面临网络规模困难的问题。",
    "en_tdlr": "This paper proposes equivariant differentially private deep learning, which leverages group symmetries in data to obtain sparser models while maintaining privacy guarantees, thus solving the challenge of network size in DP-SGD on more challenging tasks."
}