{
    "title": "Single-round Self-supervised Distributed Learning using Vision Transformer. (arXiv:2301.02064v3 [cs.CV] UPDATED)",
    "abstract": "Despite the recent success of deep learning in the field of medicine, the issue of data scarcity is exacerbated by concerns about privacy and data ownership. Distributed learning approaches, including federated learning, have been investigated to address these issues. However, they are hindered by the need for cumbersome communication overheads and weaknesses in privacy protection. To tackle these challenges, we propose a self-supervised masked sampling distillation method for the vision transformer. This method can be implemented without continuous communication and can enhance privacy by utilizing a vision transformer-specific encryption technique. We conducted extensive experiments on two different tasks, which demonstrated the effectiveness of our method. We achieved superior performance compared to the existing distributed learning strategy as well as the fine-tuning only baseline. Furthermore, since the self-supervised model created using our proposed method can achieve a general",
    "link": "http://arxiv.org/abs/2301.02064",
    "context": "Title: Single-round Self-supervised Distributed Learning using Vision Transformer. (arXiv:2301.02064v3 [cs.CV] UPDATED)\nAbstract: Despite the recent success of deep learning in the field of medicine, the issue of data scarcity is exacerbated by concerns about privacy and data ownership. Distributed learning approaches, including federated learning, have been investigated to address these issues. However, they are hindered by the need for cumbersome communication overheads and weaknesses in privacy protection. To tackle these challenges, we propose a self-supervised masked sampling distillation method for the vision transformer. This method can be implemented without continuous communication and can enhance privacy by utilizing a vision transformer-specific encryption technique. We conducted extensive experiments on two different tasks, which demonstrated the effectiveness of our method. We achieved superior performance compared to the existing distributed learning strategy as well as the fine-tuning only baseline. Furthermore, since the self-supervised model created using our proposed method can achieve a general",
    "path": "papers/23/01/2301.02064.json",
    "total_tokens": 924,
    "translated_title": "使用Vision Transformer的单轮自监督分布式学习",
    "translated_abstract": "尽管深度学习在医学领域取得了一系列的成功，但由于对隐私和数据所有权的担忧，数据稀缺问题更加严重。已经研究过分布式学习方法，包括联合学习，以解决这些问题。然而，由于需要繁琐的通信开销和隐私保护存在的弱点，这些方法受到了阻碍。为了解决这些挑战，我们提出了一种利用Vision Transformer特定加密技术，采用自监督遮蔽采样蒸馏方法的方法。这种方法可以在不连续通信的情况下实现，并可以通过利用Vision Transformer特定的加密技术来增强隐私保护。我们在两个不同的任务上进行了大量实验，证明了我们方法的有效性。与现有的分布式学习策略以及仅基于微调的基线相比，我们达到了更为优秀的性能。此外，由于我们提出的方法创建的自监督模型可以实现一般化。",
    "tldr": "本文提出了一种自监督蒸馏方法，可以在分布式学习中实现不需要连续通信，并通过利用Vision Transformer特定的加密技术来增强隐私保护。实验结果表明，该方法在两个不同任务上均优于现有的分布式学习策略和微调基线。",
    "en_tdlr": "This paper proposes a self-supervised distillation method that can be implemented without continuous communication in distributed learning and enhance privacy by utilizing a vision transformer-specific encryption technique. The experimental results demonstrate the effectiveness of the method, which outperforms existing distributed learning strategies and fine-tuning baselines on two different tasks."
}