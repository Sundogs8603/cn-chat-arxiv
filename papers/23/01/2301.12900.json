{
    "title": "DepGraph: Towards Any Structural Pruning. (arXiv:2301.12900v2 [cs.AI] UPDATED)",
    "abstract": "Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and {fully automatic} method, \\emph{Dependency Graph} (DepGraph), to explicitly model the dependency between layers and comprehens",
    "link": "http://arxiv.org/abs/2301.12900",
    "context": "Title: DepGraph: Towards Any Structural Pruning. (arXiv:2301.12900v2 [cs.AI] UPDATED)\nAbstract: Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and {fully automatic} method, \\emph{Dependency Graph} (DepGraph), to explicitly model the dependency between layers and comprehens",
    "path": "papers/23/01/2301.12900.json",
    "total_tokens": 868,
    "translated_title": "DepGraph: 实现任何结构剪枝",
    "translated_abstract": "结构剪枝通过从神经网络中删除结构分组参数来加速模型，然而，在不同模型中，参数分组模式差异很大，依赖于手动设计的分组方案的特定结构剪枝器无法推广到新的架构。在这项工作中，我们研究了一项极具挑战性但鲜有探索的任务，任何结构剪枝，以应对任意架构（如CNN、RNN、GNN和Transformer）的通用结构剪枝。实现这一目标的最大障碍在于结构耦合，它不仅强制同时剪枝不同层，而且还期望所有已删除参数都一致不重要，从而避免结构问题和剪枝后的性能显著下降。为解决这个问题，我们提出了一种通用和完全自动化的方法，“依赖图”（DepGraph），来明确模型层之间的依赖关系。",
    "tldr": "这篇论文提出了一种通用且完全自动化的方法，称为“Dependency Graph”（DepGraph），用于解决任何结构剪枝，它明确模型层之间的依赖关系，以避免出现结构问题和显著的性能下降。",
    "en_tdlr": "This paper proposes a general and fully automatic method called DepGraph for any structural pruning, which explicitly models the dependency between layers to prevent structural issues and significant performance degradation after pruning."
}