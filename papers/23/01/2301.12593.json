{
    "title": "Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)",
    "abstract": "Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e",
    "link": "http://arxiv.org/abs/2301.12593",
    "context": "Title: Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)\nAbstract: Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e",
    "path": "papers/23/01/2301.12593.json",
    "total_tokens": 915,
    "translated_title": "分布鲁棒安全强化学习的风险厌恶模型不确定性",
    "translated_abstract": "许多现实领域需要在不确定的环境中进行安全决策。本研究引入了一个深度强化学习框架来解决这个重要问题。我们考虑过渡模型的分布，并通过使用相干失真风险度量来对模型不确定性采取风险厌恶的观点。我们通过展示它等价于一类特定的分布鲁棒安全强化学习问题，为这个框架提供了鲁棒性保证。然而，与现有的深度强化学习鲁棒性方法不同，我们的表达不涉及极小化最大优化。这导致我们的方法可以高效、不依赖模型地在单个训练环境中仅需要标准数据收集来实施。在具有安全约束的连续控制任务的实验中，我们证明了我们的框架在部署时能够产生稳健的性能和安全性。",
    "tldr": "该论文提出了一种分布鲁棒安全强化学习框架，通过使用失真风险度量来处理模型不确定性。该方法不需要极小极大优化，具有高效且不依赖模型的特点。实验证明该框架能够在具有安全约束的控制任务中实现稳健的性能和安全性。",
    "en_tdlr": "This paper introduces a distributionally robust safe reinforcement learning framework that addresses model uncertainty using distortion risk measures. The approach does not involve minimax optimization and is efficient and model-free. Experimental results demonstrate robust performance and safety in tasks with safety constraints."
}