{
    "title": "On the Connection Between MPNN and Graph Transformer. (arXiv:2301.11956v3 [cs.LG] UPDATED)",
    "abstract": "Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1)",
    "link": "http://arxiv.org/abs/2301.11956",
    "context": "Title: On the Connection Between MPNN and Graph Transformer. (arXiv:2301.11956v3 [cs.LG] UPDATED)\nAbstract: Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1)",
    "path": "papers/23/01/2301.11956.json",
    "total_tokens": 1008,
    "translated_title": "MPNN与图转换器之间的连接",
    "translated_abstract": "最近出现了一种新的图学习算法范例——图转换器（GT），在多个基准测试中表现优于之前流行的消息传递神经网络（MPNN）。以前的工作表明，通过适当的位置嵌入，GT可以任意逼近MPNN，这意味着GT至少与MPNN一样强大。本文研究了反向连接，并展示了带有虚拟节点（VN）的MPNN足够强大，可以任意逼近GT的自注意力层。特别地，我们首先展示了如果考虑一种线性变换器——所谓的表现者/线性变换器，则具有O（1）深度和O（1）宽度的MPNN + VN可以逼近表现者/线性变换器的自我注意层。接下来，通过MPNN + VN与DeepSets之间的联系，我们证明了具有O(n^d)宽度和O(1)深度的MPNN + VN可以逼近GT的任何层，其中n是图中的节点数，d是图的直径。我们的工作阐明了两种图学习范式之间的关系和能力平衡。",
    "tldr": "本文研究了MPNN与图转换器之间的连接，并证明了带有虚拟节点的MPNN可以任意逼近GT的自注意力层。我们的工作阐明了两种图学习范式之间的关系和能力平衡。",
    "en_tdlr": "This paper studies the connection between MPNN and Graph Transformer, and proves that MPNN with virtual node can approximate the self-attention layer of GT. The work sheds light on the relation and power balance between the two graph learning paradigms."
}