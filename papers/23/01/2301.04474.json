{
    "title": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model. (arXiv:2301.04474v3 [cs.CV] UPDATED)",
    "abstract": "Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.",
    "link": "http://arxiv.org/abs/2301.04474",
    "context": "Title: Speech Driven Video Editing via an Audio-Conditioned Diffusion Model. (arXiv:2301.04474v3 [cs.CV] UPDATED)\nAbstract: Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.",
    "path": "papers/23/01/2301.04474.json",
    "total_tokens": 813,
    "translated_title": "基于音频条件扩散模型的语音驱动视频编辑",
    "translated_abstract": "本文受到最近扩散模型在视觉生成任务中的应用启发，提出了一种使用去噪扩散模型实现端到端语音驱动视频编辑的方法。给定一个说话者的视频和单独的音频录音，通过将音频梅尔频谱特征作为条件，重新同步唇部和下巴运动，而不依赖于中间的结构表示，如面部标志或3D面部模型。我们在CREMA-D音频视觉数据集上展示了单说话人和多说话人视频编辑的结果，提供了一个基准模型。据我们所知，这是第一个证明和验证应用端到端去噪扩散模型到语音驱动视频编辑任务中的可行性的工作。",
    "tldr": "本文提出了一种使用音频条件下的扩散模型进行端到端语音驱动视频编辑的方法，通过条件生成器实现对唇部和下巴运动的同步，避免了对中间结构表示的依赖。",
    "en_tdlr": "The paper proposes a method for end-to-end speech-driven video editing using a denoising diffusion model, which synchronizes lip and jaw motions without relying on intermediate structural representations. It is the first work to validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing."
}