{
    "title": "Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)",
    "abstract": "In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&L). Our study demo",
    "link": "http://arxiv.org/abs/2301.08360",
    "context": "Title: Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)\nAbstract: In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&L). Our study demo",
    "path": "papers/23/01/2301.08360.json",
    "total_tokens": 962,
    "translated_title": "领域自适应学习和模仿：用于电力套利的深度强化学习",
    "translated_abstract": "本文讨论荷兰电力市场，由日前市场和即时平衡市场组成，并且运作方式类似拍卖。由于供需波动，通常存在不平衡导致两个市场价格不同，从而提供套利机会。为了解决这个问题，我们对问题进行了重构，并提出了一种协作式双代理深度强化学习方法，用于欧洲电力套利交易的双层仿真与优化。我们还引入了两种新的实现方式，通过模仿电力交易员的交易行为来融入领域特定知识。通过利用奖励工程来模仿领域专业知识，我们能够重新设计强化学习代理的奖励系统，改善训练中的收敛性并提高整体性能。此外，订单分段增加了竞标成功率，显著提高了利润和损失。我们的研究演示了通过领域自适应学习和模仿的深度强化学习方法在电力套利中的应用。",
    "tldr": "本文提出了一种领域自适应学习和模仿的深度强化学习方法，用于电力套利交易。通过利用奖励工程和订单分段的方式，该方法能够提高训练的收敛性，增加竞标成功率，并显著提高利润和损失。"
}