{
    "title": "Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels. (arXiv:2301.11624v2 [cs.LG] UPDATED)",
    "abstract": "Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks (NNs). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy. Here we provide analytic formulas for Wasserstein schemes starting at a Dirac measure and s",
    "link": "http://arxiv.org/abs/2301.11624",
    "context": "Title: Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels. (arXiv:2301.11624v2 [cs.LG] UPDATED)\nAbstract: Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks (NNs). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy. Here we provide analytic formulas for Wasserstein schemes starting at a Dirac measure and s",
    "path": "papers/23/01/2301.11624.json",
    "total_tokens": 996,
    "translated_title": "用神经Wasserstein梯度流求解带有Riesz核的最大均值差异",
    "translated_abstract": "非光滑的Riesz核最大均值差异函数的Wasserstein梯度流显示出丰富的结构，奇异测度可以变成绝对连续的测度，反之亦然。本文旨在贡献于对这种流的理解。我们提出用神经网络（NN）逼近Jordan、Kinderlehrer和Otto的反向方案，以计算这种Wasserstein梯度流，同时提出了一种前向方案，用于所谓的Wasserstein最陡下降流。因为我们不能把自己限制在绝对连续的量度上，所以我们必须处理传输计划和速度计划，而不是通常的传输映射和速度场。的确，我们用生成的NN近似了两个计划的分解，这些计划是根据适当的损失函数学习的。为了评估两个神经系统的质量，我们将其基准化为相互作用能。在这里，我们提供了由Dirac测度开始的Wasserstein方案的解析公式。",
    "tldr": "本文提出用神经网络逼近Jordan、Kinderlehrer和Otto的反向方案以及一种前向方案，用于计算非光滑Riesz核最大均值差异函数的Wasserstein梯度流。我们通过学习适当的损失函数来近似处理计划的分解，并在交互能上基准测量神经网络的质量。"
}