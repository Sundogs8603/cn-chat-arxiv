{
    "title": "On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems. (arXiv:2301.10587v2 [cs.SD] UPDATED)",
    "abstract": "The performance of neural network-based speech enhancement systems is primarily influenced by the model architecture, whereas training times and computational resource utilization are primarily affected by training parameters such as the batch size. Since noisy and reverberant speech mixtures can have different duration, a batching strategy is required to handle variable size inputs during training, in particular for state-of-the-art end-to-end systems. Such strategies usually strive for a compromise between zero-padding and data randomization, and can be combined with a dynamic batch size for a more consistent amount of data in each batch. However, the effect of these strategies on resource utilization and more importantly network performance is not well documented. This paper systematically investigates the effect of different batching strategies and batch sizes on the training statistics and speech enhancement performance of a Conv-TasNet, evaluated in both matched and mismatched co",
    "link": "http://arxiv.org/abs/2301.10587",
    "context": "Title: On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems. (arXiv:2301.10587v2 [cs.SD] UPDATED)\nAbstract: The performance of neural network-based speech enhancement systems is primarily influenced by the model architecture, whereas training times and computational resource utilization are primarily affected by training parameters such as the batch size. Since noisy and reverberant speech mixtures can have different duration, a batching strategy is required to handle variable size inputs during training, in particular for state-of-the-art end-to-end systems. Such strategies usually strive for a compromise between zero-padding and data randomization, and can be combined with a dynamic batch size for a more consistent amount of data in each batch. However, the effect of these strategies on resource utilization and more importantly network performance is not well documented. This paper systematically investigates the effect of different batching strategies and batch sizes on the training statistics and speech enhancement performance of a Conv-TasNet, evaluated in both matched and mismatched co",
    "path": "papers/23/01/2301.10587.json",
    "total_tokens": 856,
    "translated_title": "批量处理变长语音增强系统的训练输入",
    "translated_abstract": "基于神经网络的语音增强系统的性能主要受到模型架构的影响，而训练时间和计算资源的利用则主要受到诸如批量大小之类的训练参数的影响。由于不同的噪声和混响语音组合可以具有不同的持续时间，在培训期间需要一种批处理策略来处理变长输入，特别是对于最先进的端到端系统。这些策略通常力求在零填充和数据随机化之间取得一种折衷，并且可以与动态批量大小相结合，以获得每个批次中一致的数据量。然而，这些策略对资源利用和更重要的网络性能的影响并没有得到很好的记录。本文系统地研究了不同批处理策略和批量大小对Conv-TasNet的训练统计数据和语音增强性能的影响，评估了配对和不匹配的情况下。",
    "tldr": "本文系统地研究了不同批处理策略和批量大小对语音增强系统的训练效果的影响，以及如何在如何在实现高网络性能的同时最大化资源利用。",
    "en_tdlr": "This paper systematically investigates the impact of different batching strategies and batch sizes on the training effectiveness of speech enhancement system and how to maximize resource utilization while achieving high network performance."
}