{
    "title": "Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining. (arXiv:2301.12596v3 [eess.AS] UPDATED)",
    "abstract": "While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error",
    "link": "http://arxiv.org/abs/2301.12596",
    "context": "Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining. (arXiv:2301.12596v3 [eess.AS] UPDATED)\nAbstract: While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error",
    "path": "papers/23/01/2301.12596.json",
    "total_tokens": 910,
    "translated_title": "从文本学会语言：使用无监督文本预训练的零样本多语言文本转语音",
    "translated_abstract": "尽管神经文本到语音合成（TTS）已经实现了类人的自然合成语音，但多语言TTS系统由于需要配对的文本和工作室质量的音频数据，仅限于资源丰富的语言。本文提出了一种使用目标语言的仅文本数据进行零样本多语言TTS的方法。使用仅文本数据允许开发仅存在文本资源的低资源语言的TTS系统，使TTS可以支持数千种语言。受多语言语言模型强大的跨语言可转移性的启发，我们的框架首先对多语言纯文本数据执行掩蔽语言模型预训练。然后我们用成对数据超模式训练该模型，在冻结语言感知嵌入层的同时。这允许对未包含在配对数据中但出现在仅文本数据中的语言进行推理。评估结果表明具有高度可理解的零样本TTS，其中字符错误率低于0.3％。",
    "tldr": "本研究利用仅文本数据进行零样本多语言TTS，允许开发低资源语言的TTS系统，评估结果表明具有高度可理解的零样本TTS。",
    "en_tdlr": "This study proposes a method for zero-shot multilingual TTS using text-only data, allowing the development of TTS systems for low-resource languages and demonstrating highly intelligible zero-shot TTS with a character error rate lower than 0.3%."
}