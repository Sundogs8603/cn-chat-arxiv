{
    "title": "Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies. (arXiv:2301.13019v2 [cs.RO] UPDATED)",
    "abstract": "This paper presents our solution for the Real Robot Challenge (RRC) III, a competition featured in the NeurIPS 2022 Competition Track, aimed at addressing dexterous robotic manipulation tasks through learning from pre-collected offline data. Participants were provided with two types of datasets for each task: expert and mixed datasets with varying skill levels. While the simplest offline policy learning algorithm, Behavioral Cloning (BC), performed remarkably well when trained on expert datasets, it outperformed even the most advanced offline reinforcement learning (RL) algorithms. However, BC's performance deteriorated when applied to mixed datasets, and the performance of offline RL algorithms was also unsatisfactory. Upon examining the mixed datasets, we observed that they contained a significant amount of expert data, although this data was unlabeled. To address this issue, we proposed a semi-supervised learning-based classifier to identify the underlying expert behavior within mix",
    "link": "http://arxiv.org/abs/2301.13019",
    "context": "Title: Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies. (arXiv:2301.13019v2 [cs.RO] UPDATED)\nAbstract: This paper presents our solution for the Real Robot Challenge (RRC) III, a competition featured in the NeurIPS 2022 Competition Track, aimed at addressing dexterous robotic manipulation tasks through learning from pre-collected offline data. Participants were provided with two types of datasets for each task: expert and mixed datasets with varying skill levels. While the simplest offline policy learning algorithm, Behavioral Cloning (BC), performed remarkably well when trained on expert datasets, it outperformed even the most advanced offline reinforcement learning (RL) algorithms. However, BC's performance deteriorated when applied to mixed datasets, and the performance of offline RL algorithms was also unsatisfactory. Upon examining the mixed datasets, we observed that they contained a significant amount of expert data, although this data was unlabeled. To address this issue, we proposed a semi-supervised learning-based classifier to identify the underlying expert behavior within mix",
    "path": "papers/23/01/2301.13019.json",
    "total_tokens": 1122,
    "translated_title": "在离线训练数据集中识别专家行为改善了机器人操作策略的行为克隆",
    "translated_abstract": "本文介绍了我们在NeurIPS 2022竞赛中提出的解决方案，该竞赛旨在通过从预先收集的离线数据中进行学习来解决机器人灵巧操作任务。我们为每个任务提供了两种类型的数据集：专家数据集和不同技能水平的混合数据集。虽然在专家数据集上训练的最简单的离线策略学习算法（行为克隆）表现出色，甚至超过了最先进的离线强化学习算法，但它在应用于混合数据集时性能下降，离线强化学习算法的表现也令人不满意。经过检查混合数据集，我们发现其中包含大量未标记的专家数据。为了解决这个问题，我们提出了一种基于半监督学习的分类器来识别混合数据中的专家行为。",
    "tldr": "本文提出了一个解决方案，通过在离线数据集中识别专家行为，改善了机器人操作策略的行为克隆。在NeurIPS 2022竞赛中，我们发现最简单的离线学习算法，行为克隆，在专家数据集上表现出色，甚至超过了最先进的离线强化学习算法。然而，当应用于混合数据集时，行为克隆的性能下降，离线强化学习算法的表现也不理想。通过对混合数据集进行分析，我们发现其中包含大量未标记的专家数据。为了解决这个问题，我们提出了一种半监督学习的分类器，用于识别混合数据中的专家行为。",
    "en_tdlr": "This paper proposes a solution to improve the behavioral cloning of robotic manipulation policies by identifying expert behavior in offline training datasets. It is found that the simplest offline learning algorithm, behavioral cloning, outperforms even the most advanced offline reinforcement learning algorithms when trained on expert datasets. However, its performance deteriorates when applied to mixed datasets, which also affects the performance of offline reinforcement learning algorithms. By examining the mixed datasets, it is discovered that they contain a significant amount of unlabeled expert data. To address this issue, a semi-supervised learning-based classifier is proposed to identify the underlying expert behavior in the mixed datasets."
}