{
    "title": "Learning Decorrelated Representations Efficiently Using Fast Fourier Transform. (arXiv:2301.01569v2 [cs.LG] UPDATED)",
    "abstract": "Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.",
    "link": "http://arxiv.org/abs/2301.01569",
    "context": "Title: Learning Decorrelated Representations Efficiently Using Fast Fourier Transform. (arXiv:2301.01569v2 [cs.LG] UPDATED)\nAbstract: Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.",
    "path": "papers/23/01/2301.01569.json",
    "total_tokens": 854,
    "translated_title": "使用快速傅里叶变换高效地学习不相关表示",
    "translated_abstract": "Barlow Twins和VICReg是自监督表示学习模型，它们使用正则化器来去除特征之间的相关性。虽然这些模型与传统的表示学习模型一样有效，但如果投影嵌入的维度d很高，则它们的训练可能需要大量计算资源。由于正则化器是基于交叉-correlation或covariance矩阵的单个元素来定义的，计算n个样本的损失需要O(n d^2)的时间。在本文中，我们提出了一种放松的去相关正则化器，可以通过快速傅里叶变换在O(n d log d)的时间内计算。我们还提出了一种廉价的技术来缓解放松时出现的不良局部最小值。所提出的正则化器在下游任务中展现出与现有正则化器相当的准确性，而且对于大的d，其训练所需的内存更少，速度更快。源代码可用。",
    "tldr": "本文提出了一种使用快速傅里叶变换计算去相关正则化器的方法，相较于传统方法训练更快且内存需求更小，在下游任务中显示出同等的准确性。"
}