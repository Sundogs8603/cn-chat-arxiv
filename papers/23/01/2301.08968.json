{
    "title": "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation. (arXiv:2301.08968v2 [cs.LG] UPDATED)",
    "abstract": "Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on ",
    "link": "http://arxiv.org/abs/2301.08968",
    "context": "Title: The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation. (arXiv:2301.08968v2 [cs.LG] UPDATED)\nAbstract: Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on ",
    "path": "papers/23/01/2301.08968.json",
    "total_tokens": 1001,
    "translated_title": "最佳结合：通过无数据超知识蒸馏实现准确的全局和个性化模型的联邦学习",
    "translated_abstract": "数据的异构性限制了通过联邦学习训练的全局模型的性能，尤其是在具有局部数据高度不平衡的情况下。个性化联邦学习（PFL）作为解决异构数据挑战的潜在解决方案已经出现。但是现有的PFL方法通常以牺牲全局模型准确性为代价来提高本地模型的性能。我们提出了FedHKD（联邦超知识蒸馏），这是一种新颖的FL算法，其中客户端依靠知识蒸馏（KD）来训练本地模型。我们在几个基准数据集上的实验表明，FedHKD可以在全局和个性化准确性方面实现最先进的性能，超越现有的FL方法，甚至超过以集中方式训练的所有数据模型。",
    "tldr": "该论文介绍了一种名为FedHKD的联邦学习算法，利用知识蒸馏实现无数据超知识蒸馏，解决了异构数据导致的全局和个性化模型准确性问题，在多个基准数据集上实验表明优于现有FL方法和中心化训练方式下的模型。",
    "en_tdlr": "This paper proposes a FL algorithm called FedHKD, which uses knowledge distillation for data-free hyper-knowledge distillation to solve the problem of global and personalized model accuracy caused by heterogeneous data. The experiments on several benchmark datasets show that it outperforms existing FL methods and even models trained with all data in a centralized manner."
}