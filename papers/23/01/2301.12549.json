{
    "title": "Scaling in Depth: Unlocking Robustness Certification on ImageNet. (arXiv:2301.12549v2 [cs.LG] UPDATED)",
    "abstract": "Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \\emph{Linear ResNet} (LiResNet) architecture. We then introduce \\emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \\emph{all} classes. Together, these contributions yield new \\emph{state-of-the-art} robust accuracy ",
    "link": "http://arxiv.org/abs/2301.12549",
    "context": "Title: Scaling in Depth: Unlocking Robustness Certification on ImageNet. (arXiv:2301.12549v2 [cs.LG] UPDATED)\nAbstract: Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \\emph{Linear ResNet} (LiResNet) architecture. We then introduce \\emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \\emph{all} classes. Together, these contributions yield new \\emph{state-of-the-art} robust accuracy ",
    "path": "papers/23/01/2301.12549.json",
    "total_tokens": 965,
    "translated_title": "深度尺度：在ImageNet上实现稳健性认证",
    "translated_abstract": "尽管基于Lipschitz方法在确定性保证下能够实现稳健深度学习的承诺，但目前最先进的结果仅限于对低维数据，例如CIFAR-10的前馈卷积网络（ConvNet）。本文研究了将可证明的稳健训练扩展到更大、更深模型的策略。证明深度网络的一个关键挑战是计算ResNet和ViT体系结构中的残差块的Lipschitz界的高效方法。我们展示了用于常规ResNet的Lipschitz常数边界的快速方法往往不准确，并展示了如何通过设计新的残差块来解决这个问题，从而实现了\\emph{Linear ResNet} (LiResNet)架构。然后，我们介绍了\\emph{Efficient Margin MAximization} (EMMA)损失函数，通过同时惩罚来自\\emph{所有}类别的最坏情况对抗性示例稳定稳健训练。这些贡献共同产生了新的\\emph{最先进}的稳健准确性。",
    "tldr": "本文提出了一些新策略和方法解决了证明深度网络稳健性的难点，引入了Linear ResNet架构和Efficient Margin MAximization损失函数，最终实现了新的最先进稳健准确性。",
    "en_tdlr": "This paper proposes new strategies and methods to address the challenge of proving the robustness of deep networks, introduces the Linear ResNet architecture and the Efficient Margin MAximization loss function, and ultimately achieves new state-of-the-art robust accuracy on ImageNet."
}