{
    "title": "Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\\H{o}s-R\\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret",
    "link": "http://arxiv.org/abs/2301.13060",
    "context": "Title: Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\\H{o}s-R\\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret",
    "path": "papers/23/01/2301.13060.json",
    "total_tokens": 933,
    "translated_title": "图神经网络的零一定律",
    "translated_abstract": "图神经网络(GNN)是用于对图进行机器学习的深度学习标准体系结构。这导致了大量的工作分析这些模型的能力和限制，特别是他们的表示和外推能力。我们提供了一个新的理论视角，回答了一个问题：当图节点的数量变得非常大时，GNNs的行为如何？在温和的假设下，我们证明，当我们从Erdős-Rényi模型中绘制不断增大的图时，这些图映射到GNN分类器的特定输出的概率趋于零或一。这个类包括流行的图卷积网络体系结构。这个结果建立了这些GNN的零一定律，并且类比于其他收敛定律，带来了它们在理论上的局限性。我们通过实验证实了我们的结果，观察到理论与实践相符。",
    "tldr": "本文提出了一个新的理论研究视角，回答了当图节点数量变得非常大时GNN的行为如何的问题。通过证明不断增大的图映射到GNN分类器的特定输出的概率趋于零或一，建立了这些GNN的零一定律，限制了它们的能力。实验证实了理论结论。",
    "en_tdlr": "This paper offers a novel theoretical perspective on the behavior of graph neural networks (GNNs) as the number of graph nodes become very large. By proving that graphs of increasing size from the Erdős-Rényi model tend to be mapped to a particular output by a class of GNN classifiers with either probability zero or one, the authors establish 'zero-one laws' for these GNNs, limiting their capacity. Empirical evidence supports their findings."
}