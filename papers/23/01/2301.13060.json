{
    "title": "Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\\H{o}s-R\\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret",
    "link": "http://arxiv.org/abs/2301.13060",
    "raw_ret": "{\n    \"translated_title\": \"图神经网络的零一定律\",\n    \"translated_abstract\": \"图神经网络（GNNs）是机器学习中图形深度学习架构的标准。我们通过回答问题来提供一个新的理论视角，即：当图形节点数量变得非常大时，GNNs如何行为？ 在温和的假设下，我们表明，当我们从Erd\\H{o}s-R\\'enyi模型中不断增加大小的图形时，类GNN分类器将这样的图形映射到特定输出的概率趋近于零或一。这个类包括流行的图卷积网络架构。结果为这些GNNs建立了“零一定律”，类似于其他收敛定律，限制了它们的容量。我们通过实验证实了我们的结果，观察到理论...\",\n    \"tldr\": \"这篇论文探讨了图神经网络在节点数量非常大时的行为，并且通过实验验证了他们的理论。研究表明，GNNs被映射到特定输出的概率趋近于0或1，这被称为“零一定律”，并对它们的容量施加了限制。\"\n}",
    "total_tokens": 809
}