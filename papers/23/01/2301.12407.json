{
    "title": "FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)",
    "abstract": "Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.",
    "link": "http://arxiv.org/abs/2301.12407",
    "context": "Title: FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)\nAbstract: Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.",
    "path": "papers/23/01/2301.12407.json",
    "total_tokens": 815,
    "translated_title": "FedEBA+：基于熵的模型实现公平和有效联邦学习",
    "translated_abstract": "确保公平性是联邦学习中至关重要的方面，它使模型在所有客户端上保持一致表现。然而，设计一种可以同时提高全局模型性能和促进公平的联邦学习算法仍然是一个艰巨的挑战，因为实现后者通常需要与前者的权衡。为了解决这一问题，我们提出了一种新的联邦学习算法FedEBA+，它在同时提高全局模型性能的同时提高公平性，该算法采用公平聚合方案和对齐更新方法。此外，我们提供了理论收敛分析，证明了FedEBA+的公平性。大量实验表明FedEBA+在公平性和全局模型性能方面均优于其他SOTA的公平联邦学习方法。",
    "tldr": "FedEBA+是一种新的联邦学习算法，它采用公平聚合方案和对齐更新方法，在同时提高全局模型性能的同时提高公平性。实验证明FedEBA+优于其他公平性联邦学习方法。",
    "en_tdlr": "FedEBA+ is a novel federated learning algorithm that enhances fairness while simultaneously improving global model performance by incorporating a fair aggregation scheme and an alignment update method. Experimental results show that FedEBA+ outperforms other fairness FL methods."
}