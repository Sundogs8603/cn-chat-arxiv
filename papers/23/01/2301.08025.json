{
    "title": "Generalization through Diversity: Improving Unsupervised Environment Design. (arXiv:2301.08025v2 [cs.AI] UPDATED)",
    "abstract": "Agent decision making using Reinforcement Learning (RL) heavily relies on either a model or simulator of the environment (e.g., moving in an 8x8 maze with three rooms, playing Chess on an 8x8 board). Due to this dependence, small changes in the environment (e.g., positions of obstacles in the maze, size of the board) can severely affect the effectiveness of the policy learned by the agent. To that end, existing work has proposed training RL agents on an adaptive curriculum of environments (generated automatically) to improve performance on out-of-distribution (OOD) test scenarios. Specifically, existing research has employed the potential for the agent to learn in an environment (captured using Generalized Advantage Estimation, GAE) as the key factor to select the next environment(s) to train the agent. However, such a mechanism can select similar environments (with a high potential to learn) thereby making agent training redundant on all but one of those environments. To that end, we ",
    "link": "http://arxiv.org/abs/2301.08025",
    "context": "Title: Generalization through Diversity: Improving Unsupervised Environment Design. (arXiv:2301.08025v2 [cs.AI] UPDATED)\nAbstract: Agent decision making using Reinforcement Learning (RL) heavily relies on either a model or simulator of the environment (e.g., moving in an 8x8 maze with three rooms, playing Chess on an 8x8 board). Due to this dependence, small changes in the environment (e.g., positions of obstacles in the maze, size of the board) can severely affect the effectiveness of the policy learned by the agent. To that end, existing work has proposed training RL agents on an adaptive curriculum of environments (generated automatically) to improve performance on out-of-distribution (OOD) test scenarios. Specifically, existing research has employed the potential for the agent to learn in an environment (captured using Generalized Advantage Estimation, GAE) as the key factor to select the next environment(s) to train the agent. However, such a mechanism can select similar environments (with a high potential to learn) thereby making agent training redundant on all but one of those environments. To that end, we ",
    "path": "papers/23/01/2301.08025.json",
    "total_tokens": 892,
    "translated_title": "多样性推广：改善无监督环境设计",
    "translated_abstract": "使用强化学习（RL）进行智能决策主要依赖于环境的模型或模拟器（例如，在一个8x8大小的迷宫中移动，或者在一个8x8大小的棋盘上下棋）。由于这种依赖性，环境的微小变化（例如，迷宫中障碍物的位置，棋盘的大小）可能严重影响智能体学习到的策略的有效性。为此，已有研究提出在自适应课程环境（自动生成）上训练RL智能体，以改善在分布外（OOD）的测试场景上的性能。具体而言，现有研究将智能体在环境中学习的潜力（通过广义优势估计，GAE）作为选择下一个环境进行训练的关键因素。然而，这样的机制可能选择相似的环境（具有高学习潜力），从而使智能体在除其中一个环境外的所有环境上的训练变得多余。为此，我们...",
    "tldr": "这项研究改进了无监督环境设计，通过训练RL智能体适应各种环境变化，提高了在分布外测试场景上的性能。",
    "en_tdlr": "This research improves unsupervised environment design by training RL agents to adapt to various environmental changes, thus enhancing performance on out-of-distribution test scenarios."
}