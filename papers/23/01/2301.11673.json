{
    "title": "Bayesian Self-Supervised Contrastive Learning. (arXiv:2301.11673v3 [cs.LG] UPDATED)",
    "abstract": "Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.",
    "link": "http://arxiv.org/abs/2301.11673",
    "context": "Title: Bayesian Self-Supervised Contrastive Learning. (arXiv:2301.11673v3 [cs.LG] UPDATED)\nAbstract: Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.",
    "path": "papers/23/01/2301.11673.json",
    "total_tokens": 891,
    "translated_title": "贝叶斯自学习对比学习",
    "translated_abstract": "近年来，对比学习在多个领域表现出了出色的应用，然而其自监督版本仍存在许多激动人心的挑战。由于负样本从未标记的数据集中选择，因此随机选择的样本可能实际上是一个伪负样本，导致编码器训练不正确。本文提出了一种新的自监督对比损失——BCL损失，它仍然使用未标记数据的随机样本，同时通过重要性权重修正导致的偏差。关键思想是在贝叶斯框架下设计所需的采样分布，从而采样难以得到的真实负样本。突出优点在于所需的采样分布是一个参数结构，其中具有位置参数以纠正伪负样本以及具有浓度参数以采矿难负样本。实验证明BCL损失的有效性和优越性。",
    "tldr": "本文提出了一种新的自监督对比损失——BCL损失，通过重要性权重修正导致的偏差，设计所需的采样分布来采样难以得到的真实负样本，修正伪负样本，采矿难负样本以提高编码器训练的准确性。",
    "en_tdlr": "This paper proposes a new self-supervised contrastive loss called the BCL loss, which corrects the resulting bias with importance weights, designs the desired sampling distribution for sampling hard true negative samples under the Bayesian framework, thus improving the accuracy of encoder training by correcting false negatives, and mining hard negatives."
}