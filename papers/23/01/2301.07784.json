{
    "title": "Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)",
    "abstract": "Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\\epsilon$-o",
    "link": "http://arxiv.org/abs/2301.07784",
    "context": "Title: Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)\nAbstract: Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\\epsilon$-o",
    "path": "papers/23/01/2301.07784.json",
    "total_tokens": 988,
    "translated_title": "通过广义策略优化优先级实现高效多目标学习",
    "translated_abstract": "多目标强化学习 (MORL) 算法解决了代理在可能冲突的奖励函数上有不同偏好的顺序决策问题。这些算法通常学习一组策略（每个策略都是为不同代理偏好而优化的），这组策略可以后来用于解决具有新偏好的问题。我们介绍了一种新算法，该算法使用广义策略改进 (GPI) 定义了原则上得出的组数，从而提高了固定样本数的学习效率。通过该算法，代理可以实现主动学习策略，并可以在每一时刻确定最有前途的偏好或目标，以更快地解决给定的MORL问题。同时，该算法也可以通过一种新的Dyna风格的MORL方法，识别出学习特定代理偏好的策略时最相关的以往经验。我们证明了我们的算法保证始终在有限的步数内收敛到最优解，或收敛到距离最优解 $\\epsilon$-o。",
    "tldr": "该论文提出了一种使用广义策略改进优先级来实现高效多目标学习的算法，从而通过主动学习策略，可以识别出每一时刻最有前途的偏好或目标，以更快地解决MORL问题，同时也可以识别出学习特定代理偏好的策略时最相关的历史经验。",
    "en_tdlr": "This paper proposes an algorithm that uses generalized policy improvement prioritization to achieve sample-efficient multi-objective learning, which can identify the most promising preferences/objectives to train on at each moment to more rapidly solve a given MORL problem, and can also identify which previous experiences are most relevant when learning a policy for a particular agent preference."
}