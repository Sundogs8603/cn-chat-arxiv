{
    "title": "A Policy Optimization Method Towards Optimal-time Stability. (arXiv:2301.00521v2 [cs.RO] UPDATED)",
    "abstract": "In current model-free reinforcement learning (RL) algorithms, stability criteria based on sampling methods are commonly utilized to guide policy optimization. However, these criteria only guarantee the infinite-time convergence of the system's state to an equilibrium point, which leads to sub-optimality of the policy. In this paper, we propose a policy optimization technique incorporating sampling-based Lyapunov stability. Our approach enables the system's state to reach an equilibrium point within an optimal time and maintain stability thereafter, referred to as \"optimal-time stability\". To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations conducted on ten robotic tasks, our approach outperforms previous studies significantly, effectively guiding the system to generate stable patterns.",
    "link": "http://arxiv.org/abs/2301.00521",
    "context": "Title: A Policy Optimization Method Towards Optimal-time Stability. (arXiv:2301.00521v2 [cs.RO] UPDATED)\nAbstract: In current model-free reinforcement learning (RL) algorithms, stability criteria based on sampling methods are commonly utilized to guide policy optimization. However, these criteria only guarantee the infinite-time convergence of the system's state to an equilibrium point, which leads to sub-optimality of the policy. In this paper, we propose a policy optimization technique incorporating sampling-based Lyapunov stability. Our approach enables the system's state to reach an equilibrium point within an optimal time and maintain stability thereafter, referred to as \"optimal-time stability\". To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations conducted on ten robotic tasks, our approach outperforms previous studies significantly, effectively guiding the system to generate stable patterns.",
    "path": "papers/23/01/2301.00521.json",
    "total_tokens": 909,
    "translated_title": "一种面向最优时间稳定性的策略优化方法",
    "translated_abstract": "在当前的无模型强化学习算法中，基于采样方法的稳定性标准常用于指导策略优化。然而，这些标准仅保证系统状态无限时间收敛到一个平衡点，从而导致策略的次优性。在本文中，我们提出了一种将基于采样的李雅普诺夫稳定性纳入策略优化的技术。我们的方法使系统状态能够在最优时间内达到平衡点，并保持稳定性，即\"最优时间稳定性\"。为实现这一目标，我们将优化方法整合到Actor-Critic框架中，从而开发出自适应李雅普诺夫Actor-Critic（ALAC）算法。通过对十个机器人任务进行评估，我们的方法明显优于先前的研究，在引导系统生成稳定模式方面表现出很好的效果。",
    "tldr": "本文提出了一种面向最优时间稳定性的策略优化方法，将基于采样的李雅普诺夫稳定性与Actor-Critic框架相结合，发展出了自适应李雅普诺夫Actor-Critic（ALAC）算法。通过对十个机器人任务的评估，该方法在引导系统生成稳定模式方面表现优异。",
    "en_tdlr": "This paper proposes a policy optimization method towards optimal-time stability, integrating sampling-based Lyapunov stability with the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations on ten robotic tasks, the approach outperforms previous studies significantly in guiding the system to generate stable patterns."
}