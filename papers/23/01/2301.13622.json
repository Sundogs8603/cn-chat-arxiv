{
    "title": "Learning Data Representations with Joint Diffusion Models. (arXiv:2301.13622v2 [cs.LG] UPDATED)",
    "abstract": "Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations.",
    "link": "http://arxiv.org/abs/2301.13622",
    "context": "Title: Learning Data Representations with Joint Diffusion Models. (arXiv:2301.13622v2 [cs.LG] UPDATED)\nAbstract: Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations.",
    "path": "papers/23/01/2301.13622.json",
    "total_tokens": 720,
    "translated_title": "使用联合扩散模型进行数据表示学习",
    "translated_abstract": "联合机器学习模型通常允许合成和分类数据，但常常在这些任务之间表现不平衡，或者不稳定。本文提出了扩展香草扩散模型以使用分类器进行稳定的联合端到端训练的方法。该模型在所有评估基准测试中在分类和生成质量方面均优于最新的混合方法。在我们的联合训练方法上，本文还介绍了如何通过引入一种方法进行直接益处共享生成和鉴别表示，以提供视觉反事实解释。",
    "tldr": "本文提出了一种扩展香草扩散模型的方法，使用分类器进行联合端到端训练，以提高分类和生成数据的质量。",
    "en_tdlr": "This paper proposes a method to extend the vanilla diffusion model by using a classifier for stable joint end-to-end training, which outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality. The paper also introduces a method for visual counterfactual explanations based on shared generative and discriminative representations."
}