{
    "title": "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v2 [cs.CL] UPDATED)",
    "abstract": "When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children's linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children's linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than",
    "link": "http://arxiv.org/abs/2301.11462",
    "context": "Title: How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v2 [cs.CL] UPDATED)\nAbstract: When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children's linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children's linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than",
    "path": "papers/23/01/2301.11462.json",
    "total_tokens": 1249,
    "translated_title": "论语言输入贫乏程度对于神经网络层级概括能力的影响的评估",
    "translated_abstract": "儿童对于学习语法时通常会选择层次性规则，人们想知道这种偏好是由于学习层级结构的偏差还是其他一些情况。通过训练两种不带层级偏见的神经网络，我们探讨了这些可能性。结果表明，尽管两种模型表现良好，但它们概括的能力更符合错误线性规则而不是正确的层级规则。因此，我们得出结论，仅仅从语料库中进行类人概括需要比现有模型更强的偏见。",
    "tldr": "该研究探讨了在类似于儿童语言输入数据的语料库上训练的两种神经网络的层级概括能力。结果表明这些模型概括的能力与正确的层级规则不符，从结论可以看出，仅从语料库中进行类人概括需要比现有模型更强的偏见。",
    "en_tdlr": "This study explores the hierarchical generalization ability of two neural networks trained on data similar to children's linguistic input. The results suggest that the models' generalization does not follow the correct hierarchical rule, indicating a need for stronger biases in human-like generalization from text alone."
}