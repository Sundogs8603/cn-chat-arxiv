{
    "title": "Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)",
    "abstract": "Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.",
    "link": "http://arxiv.org/abs/2301.13748",
    "context": "Title: Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)\nAbstract: Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.",
    "path": "papers/23/01/2301.13748.json",
    "total_tokens": 839,
    "translated_title": "原型分析++：重新思考初始化策略",
    "translated_abstract": "原型分析是一种带有凸性约束的矩阵分解方法。由于局部最小值的存在，好的初始化非常重要，但是经常使用的初始化方法要么产生次优的起始点，要么容易陷入不良的局部最小值。在本文中，我们提出了原型分析++（AA ++），这是一种针对原型分析的概率初始化策略，它根据点对目标的影响顺序地进行采样，类似于$k$-means++。实际上，我们认为$k$-means++已近逼近了所提出的初始化方法。此外，我们建议将$k$-means++的高效蒙特卡罗近似方法应用于AA++。在对13个不同大小和维度的实际数据集进行广泛的实证评估并考虑两个预处理策略的情况下，我们表明AA++几乎总是优于所有的基线方法，包括最常用的方法。",
    "tldr": "本文提出了一种针对原型分析的概率初始化策略 AA ++，能够在13个不同大小和维度的实际数据集上表现优异。"
}