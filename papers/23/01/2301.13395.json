{
    "title": "Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)",
    "abstract": "In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.",
    "link": "http://arxiv.org/abs/2301.13395",
    "context": "Title: Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)\nAbstract: In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.",
    "path": "papers/23/01/2301.13395.json",
    "total_tokens": 868,
    "translated_title": "使用Davis-Yin分裂实现更快的预测与优化",
    "translated_abstract": "在许多应用中，需要反复解决具有相似但不同参数的组合问题。然而，参数$w$并非直接观察到的；只有与$w$相关的上下文数据$d$可用。我们很容易就会想到使用神经网络来根据$d$预测$w$，但是训练这样的模型需要将组合优化的离散性与用于训练神经网络的梯度优化框架相结合。当所讨论的问题是整数线性规划（ILP）时，克服这个问题的一种方法是考虑组合问题的连续放松。虽然现有方法使用这种方法在小型问题（10-100个变量）上显示出了高度的效果，但在大型问题上扩展能力不足。在本研究中，我们借鉴了现代凸优化的思想，设计了一个网络和训练方案，可以轻松地扩展到具有数千个变量的问题。",
    "tldr": "本文介绍了一种使用Davis-Yin分裂方法实现更快的预测与优化的方法，该方法借鉴了现代凸优化的思想，能够在具有数千个变量的问题上轻松扩展。",
    "en_tdlr": "This paper introduces a method for faster predict-and-optimize using Davis-Yin splitting, which draws on ideas from modern convex optimization and scales effortlessly to problems with thousands of variables."
}