{
    "title": "Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)",
    "abstract": "Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm",
    "link": "http://arxiv.org/abs/2301.08491",
    "context": "Title: Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)\nAbstract: Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm",
    "path": "papers/23/01/2301.08491.json",
    "total_tokens": 1055,
    "translated_title": "用多智能体强化学习模拟社会困境中的道德选择",
    "translated_abstract": "在实际应用中，人工智能（AI）在智能代理中纳入道德选择的重要性不断展现。同时也强调，按照任何一种道德观定义顶层的AI伦理约束非常具有挑战，并且会带来风险。从底层学习的角度出发，或许更适合研究和开发AI代理的道德行为。我们认为，分析根据预定义的道德奖励在社会困境中实行行动的强化学习代理的新兴行为是一个有趣和富有洞察力的起点。在这项工作中，我们对强化学习代理根据道德理论的奖励进行的选择进行了系统分析。我们旨在设计简化但代表一组关键伦理系统的奖励结构。因此，我们首先定义了区分后果和规范伦理的道德奖励函数，并将它们混合以创建新的奖励方案。然后，我们通过训练在社会困境下进行内在动机驱动的强化学习代理来评估这些奖励函数。结果表明，我们的方法能够复制并扩展有关道德选择的文献研究中的许多发现，并能够出现以前未曾报道的新行为。",
    "tldr": "本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。",
    "en_tdlr": "This paper uses multi-agent reinforcement learning to simulate moral choices in social dilemmas, design a series of moral reward structures, and aim to analyze and develop ethical behavior in AI agents."
}