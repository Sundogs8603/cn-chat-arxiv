{
    "title": "Logical Message Passing Networks with One-hop Inference on Atomic Formulas. (arXiv:2301.08859v3 [cs.LG] UPDATED)",
    "abstract": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct o",
    "link": "http://arxiv.org/abs/2301.08859",
    "context": "Title: Logical Message Passing Networks with One-hop Inference on Atomic Formulas. (arXiv:2301.08859v3 [cs.LG] UPDATED)\nAbstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct o",
    "path": "papers/23/01/2301.08859.json",
    "total_tokens": 927,
    "translated_title": "带有原子公式一跳推理的逻辑消息传递网络",
    "translated_abstract": "知识图谱中的复杂查询回答已经引起了广泛关注。考虑到知识图谱通常是不完整的，因此提出了使用神经模型通过将集合操作参数化为复杂神经网络来回答逻辑查询的方法。然而，这种方法通常使用大量的实体和关系嵌入来训练神经集合操作符，而嵌入或神经集合操作符如何对性能做出贡献仍不清楚。本文提出了一个简单的复杂查询回答框架，从神经集合操作符中分解出知识图谱嵌入。我们将复杂查询表示为查询图，并在查询图的基础上，提出了逻辑消息传递神经网络 (LMPNN)，将原子公式的局部单跳推理连接到复杂查询回答的全局逻辑推理中。我们利用现有的有效知识图谱嵌入进行推理。",
    "tldr": "本文提出了一个逻辑消息传递神经网络（LMPNN）框架，通过将集合操作分解成神经集合操作符来进行复杂查询回答，并将将原子公式的局部单跳推理连接到全局逻辑推理中。",
    "en_tdlr": "This paper proposes the Logical Message Passing Neural Network (LMPNN) framework for complex query answering over knowledge graphs. The framework decomposes the knowledge graph embeddings from neural set operators, represents complex queries as query graphs, and connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. Furthermore, the framework leverages existing effective knowledge graph embeddings to conduct inference."
}