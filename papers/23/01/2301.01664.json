{
    "title": "Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)",
    "abstract": "Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.",
    "link": "http://arxiv.org/abs/2301.01664",
    "context": "Title: Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)\nAbstract: Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.",
    "path": "papers/23/01/2301.01664.json",
    "total_tokens": 906,
    "translated_title": "基于句子Transformer进行多方面的可解释感知关系预测",
    "translated_abstract": "最近的知识图谱（KGs）研究表明，基于预训练语言模型的基于路径的方法在提供感知和可解释关系预测方面表现良好。本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，以提高模型性能。此外，我们提出了“知识推理句子Transformer”（KRST）来预测KG中的感知关系。 KRST被设计为在KG中编码提取出的可靠路径，使我们能够正确地聚类路径并提供多方面的解释。我们在三个真实数据集上进行了广泛的实验。实验结果表明，与SOTA模型相比，KRST在大多数感知和感知测试用例（6个中的4个）以及12个few-shot测试用例中的11个上取得了最佳性能。",
    "tldr": "本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，提高基于句子Transformer的知识推理句子Transformer的性能，从而在KG中实现了多方面的可解释感知关系预测。",
    "en_tdlr": "This paper proposes the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training, and introduces Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST encodes the extracted reliable paths in KGs to provide multi-aspect explanations and achieves the best performance compared to SOTA models in most transductive and inductive test cases and 11 of 12 few-shot test cases."
}