{
    "title": "ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning. (arXiv:2301.12618v2 [cs.LG] UPDATED)",
    "abstract": "Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a ser",
    "link": "http://arxiv.org/abs/2301.12618",
    "context": "Title: ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning. (arXiv:2301.12618v2 [cs.LG] UPDATED)\nAbstract: Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a ser",
    "path": "papers/23/01/2301.12618.json",
    "total_tokens": 919,
    "translated_title": "ForkMerge: 缓解辅助任务学习中的负迁移",
    "translated_abstract": "辅助任务学习（ATL）旨在通过利用与目标任务相关的知识来提高目标任务的性能。然而，有时同时学习多个任务会导致比仅学习目标任务的准确率更低，这被称为负迁移。这个问题通常归因于任务之间的梯度冲突，并且在以前的工作中常常通过协调任务梯度来解决。然而，这些基于优化的方法在很大程度上忽略了辅助目标泛化能力。为了更好地理解负迁移的根本原因，我们从优化和泛化角度进行了实验研究。基于我们的研究发现，我们引入了ForkMerge，一种新方法，它会定期将模型分为多个分支，通过最小化目标验证错误自动搜索不同的任务权重，并动态地合并所有分支来过滤有害的任务参数更新。在一系列基准任务中，ForkMerge优于现有的ATL方法，并减轻了负迁移，证明其在提高多任务学习方面的有效性。",
    "tldr": "ForkMerge是一种新方法，它帮助缓解了辅助任务学习中的负迁移问题，并在多任务学习中表现出良好的性能优于现有的ATL方法。",
    "en_tdlr": "ForkMerge is a novel approach that mitigates negative transfer in auxiliary-task learning by periodically forking the model, searching varying task weights, and dynamically merges all branches to filter out detrimental task-parameter updates, outperforming existing methods on benchmark tasks."
}