{
    "title": "oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation",
    "abstract": "arXiv:2301.01333v3 Announce Type: replace  Abstract: With the rapid development of deep learning models and hardware support for dense computing, the deep learning workload characteristics changed significantly from a few hot spots on compute-intensive operations to a broad range of operations scattered across the models. Accelerating a few compute-intensive operations using the expert-tuned implementation of primitives does not fully exploit the performance potential of AI hardware. Various efforts have been made to compile a full deep neural network (DNN) graph. One of the biggest challenges is to achieve high-performance tensor compilation by generating expert level performance code for the dense compute-intensive operations and applying compilation optimization at the scope of DNN computation graph across multiple compute-intensive operations.   We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid approach of using techniques from both compiler optimization and",
    "link": "https://arxiv.org/abs/2301.01333",
    "context": "Title: oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation\nAbstract: arXiv:2301.01333v3 Announce Type: replace  Abstract: With the rapid development of deep learning models and hardware support for dense computing, the deep learning workload characteristics changed significantly from a few hot spots on compute-intensive operations to a broad range of operations scattered across the models. Accelerating a few compute-intensive operations using the expert-tuned implementation of primitives does not fully exploit the performance potential of AI hardware. Various efforts have been made to compile a full deep neural network (DNN) graph. One of the biggest challenges is to achieve high-performance tensor compilation by generating expert level performance code for the dense compute-intensive operations and applying compilation optimization at the scope of DNN computation graph across multiple compute-intensive operations.   We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid approach of using techniques from both compiler optimization and",
    "path": "papers/23/01/2301.01333.json",
    "total_tokens": 775,
    "translated_title": "oneDNN图编译器：用于高性能深度学习编译的混合方法",
    "translated_abstract": "随着深度学习模型的快速发展和硬件对密集计算的支持，深度学习工作负载特征发生了显著变化，从计算密集型操作上的几个热点到分布在模型中的广泛操作。利用专家调优的基元实现加速几个计算密集型操作并不能充分利用AI硬件的性能潜力。已经做出了各种努力来编译完整的深度神经网络（DNN）图。最大的挑战之一是通过生成专家级性能代码来实现高性能张量编译，对DNN计算图的范围跨多个计算密集操作应用编译优化。",
    "tldr": "提出了oneDNN图编译器，采用了编译优化和混合方法，旨在实现高性能张量编译，生成专家级性能代码并在DNN计算图范围内应用编译优化",
    "en_tdlr": "Introducing oneDNN Graph Compiler, a tensor compiler that utilizes a hybrid approach to achieve high-performance tensor compilation, generating expert-level performance code and applying compilation optimization within the scope of DNN computation graph."
}