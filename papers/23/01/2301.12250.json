{
    "title": "Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions. (arXiv:2301.12250v2 [cs.LG] UPDATED)",
    "abstract": "We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\\mu$ and covariance $\\Sigma$, our $(\\varepsilon,\\delta)$-differentially private estimator produces $\\tilde{\\mu}$ such that $\\|\\mu - \\tilde{\\mu}\\|_{\\Sigma} \\leq \\alpha$ as long as $n \\gtrsim \\tfrac d {\\alpha^2} + \\tfrac{d \\sqrt{\\log 1/\\delta}}{\\alpha \\varepsilon}+\\frac{d\\log 1/\\delta}{\\varepsilon}$. The Mahalanobis error metric $\\|\\mu - \\hat{\\mu}\\|_{\\Sigma}$ measures the distance between $\\hat \\mu$ and $\\mu$ relative to $\\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\\tilde{O}(nd^{\\omega - 1} + nd/\\varepsilon)$, where $\\omega < 2.38$ is the matrix multiplication exponent.  We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthi",
    "link": "http://arxiv.org/abs/2301.12250",
    "context": "Title: Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions. (arXiv:2301.12250v2 [cs.LG] UPDATED)\nAbstract: We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\\mu$ and covariance $\\Sigma$, our $(\\varepsilon,\\delta)$-differentially private estimator produces $\\tilde{\\mu}$ such that $\\|\\mu - \\tilde{\\mu}\\|_{\\Sigma} \\leq \\alpha$ as long as $n \\gtrsim \\tfrac d {\\alpha^2} + \\tfrac{d \\sqrt{\\log 1/\\delta}}{\\alpha \\varepsilon}+\\frac{d\\log 1/\\delta}{\\varepsilon}$. The Mahalanobis error metric $\\|\\mu - \\hat{\\mu}\\|_{\\Sigma}$ measures the distance between $\\hat \\mu$ and $\\mu$ relative to $\\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\\tilde{O}(nd^{\\omega - 1} + nd/\\varepsilon)$, where $\\omega < 2.38$ is the matrix multiplication exponent.  We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthi",
    "path": "papers/23/01/2301.12250.json",
    "total_tokens": 1242,
    "translated_title": "高维次高斯分布的快速，样本有效，仿射不变私有均值和协方差估计",
    "translated_abstract": "我们提出了一种快速的差分私有算法，用于具有几乎最优样本复杂度的高维协方差感知均值估计。以前已知只有指数时间估计器才能实现此保证。给定从具有未知均值 $μ$ 和协方差 $Σ$ 的（亚）高斯分布中抽取的$n$个样本，我们的 $(\\varepsilon,\\delta)$-差分式私有估计器生成$\\tilde{\\mu}$，使得只要 $n \\gtrsim \\tfrac d {\\alpha^2} + \\tfrac{d \\sqrt{\\log 1/\\delta}}{\\alpha \\varepsilon}+\\frac{d\\log 1/\\delta}{\\varepsilon}$，就满足 $\\|\\mu - \\tilde{\\mu}\\|_{\\Sigma} \\leq \\alpha$。Mahalanobis误差度量 $\\|\\mu - \\hat{\\mu}\\|_{\\Sigma}$ 衡量了$\\hat \\mu$与$\\mu$在$\\Sigma$相对距离; 它表征了样本平均值的误差。我们的算法运行时间为$\\tilde{O}(nd^{\\omega - 1} + nd/\\varepsilon)$，其中$\\omega < 2.38$是矩阵乘法指数。我们使用 Brown、Gaboardi、Smith、Ullman 和 Zakynthiadaki[BGSUZ18] 的指数时间方法来计算问题的最优估计的足够统计量，并将其用于通过随机线性代数构造线性时间估计器。",
    "tldr": "本论文提出了一种快速的差分私有算法，用于具有几乎最优样本复杂度的高维协方差感知均值估计。在Mahalanobis误差度量中，也就是相对于协方差的平均误差中，我们的算法使得$\\hat \\mu$更接近$\\mu$。",
    "en_tdlr": "This paper proposes a fast differentially private algorithm with nearly optimal sample complexity for high-dimensional covariance-aware mean estimation. The algorithm achieves an $\\alpha$ error bound and a Mahalanobis error metric, which measures the distance between the estimated mean and true mean relative to the covariance matrix. The algorithm uses randomized linear algebra and builds upon the exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthiadaki."
}