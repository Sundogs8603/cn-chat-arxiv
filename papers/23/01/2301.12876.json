{
    "title": "Guiding Online Reinforcement Learning with Action-Free Offline Pretraining. (arXiv:2301.12876v2 [cs.LG] UPDATED)",
    "abstract": "Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and pe",
    "link": "http://arxiv.org/abs/2301.12876",
    "context": "Title: Guiding Online Reinforcement Learning with Action-Free Offline Pretraining. (arXiv:2301.12876v2 [cs.LG] UPDATED)\nAbstract: Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and pe",
    "path": "papers/23/01/2301.12876.json",
    "total_tokens": 946,
    "translated_title": "使用无动作离线数据预训练指导在线强化学习",
    "translated_abstract": "离线RL方法通过训练代理使用离线收集的数据来减少环境交互的需求。然而，这些方法通常需要在数据收集期间记录动作信息，这在某些实际情况下可能是困难甚至不可能的。在本文中，我们研究了使用无动作离线数据集来改善在线强化学习的潜力，并将此问题命名为具有无动作离线预训练的强化学习（AFP-RL）。我们介绍了AF-Guide，一种通过从无动作离线数据集中提取知识来指导在线训练的方法。AF-Guide包括实施Upside-Down强化学习变体的无动作决策Transformer（AFDT），它学习从离线数据集中规划下一个状态，以及一个通过AFDT指导在线学习的Guided Soft Actor-Critic（Guided SAC）。实验结果表明，在离线数据集中不存在动作信息的环境中，AF-Guide可以提高在线RL的样本效率和性能。",
    "tldr": "本文研究了使用无动作离线数据集来提高在线强化学习的效率和性能的方法，提出了AF-Guide，实现变体的Upside-Down强化学习和指导在线学习的Guided SAC，实验结果表明该方法可以成功改善在离线数据集中不存在动作信息的情况下的性能。",
    "en_tdlr": "This paper investigates improving online reinforcement learning using action-free offline datasets, introducing AF-Guide which consists of an Action-Free Decision Transformer and a Guided Soft Actor-Critic. Experimental results show that AF-Guide improves performance in environments where action information is unavailable."
}