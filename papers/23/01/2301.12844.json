{
    "title": "Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?. (arXiv:2301.12844v2 [cs.LG] UPDATED)",
    "abstract": "Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation (BO) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. Those results motivate the development of the random decomposition upper-confidence bound algorithm (RDUCB) that is straightforward to implement - (almost) plug-and-play -",
    "link": "http://arxiv.org/abs/2301.12844",
    "context": "Title: Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?. (arXiv:2301.12844v2 [cs.LG] UPDATED)\nAbstract: Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation (BO) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. Those results motivate the development of the random decomposition upper-confidence bound algorithm (RDUCB) that is straightforward to implement - (almost) plug-and-play -",
    "path": "papers/23/01/2301.12844.json",
    "total_tokens": 815,
    "translated_title": "在高维贝叶斯优化中，随机分解是否足够？",
    "translated_abstract": "学习昂贵的黑盒函数分解有望将贝叶斯优化扩展到高维问题。然而，这些技术的成功取决于找到准确表示黑盒函数的适当分解。我们研究本文中关于数据独立分解采样规则的方法。我们发现，基于数据学习分解可以很容易地被误导到局部分解上，而这些分解在整个搜索空间中并不准确。然后，我们正式证明了基于随机树的分解采样器展现了有利的理论保证，可以有效权衡最大信息增益和实际黑盒函数及其分解之间的函数失配。这些结果促进了随机分解上置信度算法（RDUCB）的发展，该算法易于实现，几乎是即插即用的。",
    "tldr": "本文研究了数据独立分解采样规则，证明了随机树分解采样器有利的理论保证，促进了随机分解上置信度算法（RDUCB）的发展。"
}