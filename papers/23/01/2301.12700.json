{
    "title": "CSDR-BERT: a pre-trained scientific dataset match model for Chinese Scientific Dataset Retrieval. (arXiv:2301.12700v3 [cs.IR] UPDATED)",
    "abstract": "As the number of open and shared scientific datasets on the Internet increases under the open science movement, efficiently retrieving these datasets is a crucial task in information retrieval (IR) research. In recent years, the development of large models, particularly the pre-training and fine-tuning paradigm, which involves pre-training on large models and fine-tuning on downstream tasks, has provided new solutions for IR match tasks. In this study, we use the original BERT token in the embedding layer, improve the Sentence-BERT model structure in the model layer by introducing the SimCSE and K-Nearest Neighbors method, and use the cosent loss function in the optimization phase to optimize the target output. Our experimental results show that our model outperforms other competing models on both public and self-built datasets through comparative experiments and ablation implementations. This study explores and validates the feasibility and efficiency of pre-training techniques for se",
    "link": "http://arxiv.org/abs/2301.12700",
    "context": "Title: CSDR-BERT: a pre-trained scientific dataset match model for Chinese Scientific Dataset Retrieval. (arXiv:2301.12700v3 [cs.IR] UPDATED)\nAbstract: As the number of open and shared scientific datasets on the Internet increases under the open science movement, efficiently retrieving these datasets is a crucial task in information retrieval (IR) research. In recent years, the development of large models, particularly the pre-training and fine-tuning paradigm, which involves pre-training on large models and fine-tuning on downstream tasks, has provided new solutions for IR match tasks. In this study, we use the original BERT token in the embedding layer, improve the Sentence-BERT model structure in the model layer by introducing the SimCSE and K-Nearest Neighbors method, and use the cosent loss function in the optimization phase to optimize the target output. Our experimental results show that our model outperforms other competing models on both public and self-built datasets through comparative experiments and ablation implementations. This study explores and validates the feasibility and efficiency of pre-training techniques for se",
    "path": "papers/23/01/2301.12700.json",
    "total_tokens": 919,
    "translated_title": "CSDR-BERT：一种用于汉语科学数据检索的预训练科学数据集匹配模型",
    "translated_abstract": "随着开放科学运动下开放和共享科学数据集的数量的增加，有效地检索这些数据集是信息检索(IR)研究中的一个关键任务。近年来，大模型的发展，特别是预训练和微调范式，即在大模型上进行预训练并在下游任务上进行微调的范式，为IR匹配任务提供了新的解决方案。在本研究中，我们使用嵌入层中的原始BERT令牌，在模型层中引入SimCSE和K-最近邻方法改进了Sentence-BERT模型结构，使用余弦损失函数在优化阶段优化目标输出。通过比较实验和消融实现，我们的实验结果表明，我们的模型在公共数据集和自建数据集上均优于其他竞争模型。本研究探讨和验证了预训练技术对科学数据集匹配领域的可行性和有效性。",
    "tldr": "本文介绍了CSDR-BERT，一种用于汉语科学数据检索的预训练科学数据集匹配模型，采用了预训练和微调范式以及改进的模型结构和优化方法，在公共和自建数据集上均表现出更好的性能。"
}