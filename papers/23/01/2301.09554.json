{
    "title": "Deep Learning Meets Sparse Regularization: A Signal Processing Perspective. (arXiv:2301.09554v3 [stat.ML] UPDATED)",
    "abstract": "Deep learning has been wildly successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networ",
    "link": "http://arxiv.org/abs/2301.09554",
    "context": "Title: Deep Learning Meets Sparse Regularization: A Signal Processing Perspective. (arXiv:2301.09554v3 [stat.ML] UPDATED)\nAbstract: Deep learning has been wildly successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networ",
    "path": "papers/23/01/2301.09554.json",
    "total_tokens": 992,
    "translated_title": "深度学习与稀疏正则化：信号处理的观点",
    "translated_abstract": "深度学习在实践中取得了巨大成功，大多数最先进的机器学习方法都基于神经网络。然而，缺乏一个严格的数学理论，能够充分解释深度神经网络的惊人性能。本文介绍了一个相对较新的数学框架，提供了对深度学习的更深入理解的开端。这个框架恰当地表征了被训练对数据拟合的神经网络的功能属性。支持这个框架的关键数学工具包括变换域稀疏正则化、计算机断层摄影的Radon变换和逼近理论，这些技术都深深根植于信号处理。这个框架解释了加权衰减正则化在神经网络训练中的作用，网络体系结构中跳跃连接和低秩权重矩阵的使用，稀疏在神经网络中的作用，并解释了为什么神经网络可作为优秀的特征提取器。",
    "tldr": "本文介绍了一个新的数学框架，恰当地表征了被训练对数据拟合的神经网络的功能属性。这个框架解释了深度神经网络的一些优秀性能、网络体系结构中跳跃连接和低秩权重矩阵的使用、稀疏在神经网络中的作用以及为什么神经网络可作为优秀的特征提取器。",
    "en_tdlr": "This article introduces a new mathematical framework that characterizes the functional properties of neural networks trained to fit data. The framework explains the excellent performance of deep neural networks, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and why neural networks behave as good feature extractors."
}