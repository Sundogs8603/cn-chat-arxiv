{
    "title": "Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations. (arXiv:2301.02184v2 [cs.CV] UPDATED)",
    "abstract": "Can conversational videos captured from multiple egocentric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously unseen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multiple people (\"egos\") move in a scene and talk among themselves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and reduce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to efficiently chart out the space. We evaluate the approach using a state-of-the-art audi",
    "link": "http://arxiv.org/abs/2301.02184",
    "context": "Title: Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations. (arXiv:2301.02184v2 [cs.CV] UPDATED)\nAbstract: Can conversational videos captured from multiple egocentric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously unseen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multiple people (\"egos\") move in a scene and talk among themselves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and reduce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to efficiently chart out the space. We evaluate the approach using a state-of-the-art audi",
    "path": "papers/23/01/2301.02184.json",
    "total_tokens": 949,
    "translated_title": "Chat2Map：从多重自我对话中高效构建场景地图",
    "translated_abstract": "本文旨在回答一个问题：从多个自我视角捕捉的对话视频中能否以成本有效的方式揭示场景地图？我们提出了一个新问题：通过利用自然对话参与者的自我视听观察中的共享信息，高效构建先前未见的3D环境地图。我们的假设是，多个人（“自我”）在场景中移动并相互交谈时，接收到的丰富视听线索可以帮助揭示场景中未知的地区。鉴于持续处理自我中心视觉流的高成本，我们进一步探索如何主动协调视觉信息的采样，以最小化冗余并减少功耗。为此，我们提出了一种音视频深度强化学习方法，与我们的共享场景对应相配合，选择性地打开相机以高效地绘制空间图。我们使用最先进的音视频自我场景共享数据集评估了该方法。",
    "tldr": "本文提出了一个新的问题：利用多人自我视听观察中的共享信息，高效构建先前未见的3D环境地图。为了高效绘制空间图，作者提出了一种音视频深度强化学习方法，与共享场景对应相配合，选择性地打开相机以最小化冗余并减少功耗。",
    "en_tdlr": "This paper proposes a new problem of efficiently building the map of a previously unseen 3D environment by exploiting shared information in egocentric audio-visual observations of participants in a natural conversation. An audio-visual deep reinforcement learning approach is presented to selectively turn on the camera to efficiently chart out the space, which is evaluated using a state-of-the-art audio-visual ego-scene sharing dataset."
}