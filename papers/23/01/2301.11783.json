{
    "title": "Certified Invertibility in Neural Networks via Mixed-Integer Programming. (arXiv:2301.11783v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks are known to be vulnerable to adversarial attacks, which are small, imperceptible perturbations that can significantly alter the network's output. Conversely, there may exist large, meaningful perturbations that do not affect the network's decision (excessive invariance). In our research, we investigate this latter phenomenon in two contexts: (a) discrete-time dynamical system identification, and (b) the calibration of a neural network's output to that of another network. We examine noninvertibility through the lens of mathematical optimization, where the global solution measures the ``safety\" of the network predictions by their distance from the non-invertibility boundary. We formulate mixed-integer programs (MIPs) for ReLU networks and $L_p$ norms ($p=1,2,\\infty$) that apply to neural network approximators of dynamical systems. We also discuss how our findings can be useful for invertibility certification in transformations between neural networks, e.g. between differ",
    "link": "http://arxiv.org/abs/2301.11783",
    "context": "Title: Certified Invertibility in Neural Networks via Mixed-Integer Programming. (arXiv:2301.11783v2 [cs.LG] UPDATED)\nAbstract: Neural networks are known to be vulnerable to adversarial attacks, which are small, imperceptible perturbations that can significantly alter the network's output. Conversely, there may exist large, meaningful perturbations that do not affect the network's decision (excessive invariance). In our research, we investigate this latter phenomenon in two contexts: (a) discrete-time dynamical system identification, and (b) the calibration of a neural network's output to that of another network. We examine noninvertibility through the lens of mathematical optimization, where the global solution measures the ``safety\" of the network predictions by their distance from the non-invertibility boundary. We formulate mixed-integer programs (MIPs) for ReLU networks and $L_p$ norms ($p=1,2,\\infty$) that apply to neural network approximators of dynamical systems. We also discuss how our findings can be useful for invertibility certification in transformations between neural networks, e.g. between differ",
    "path": "papers/23/01/2301.11783.json",
    "total_tokens": 919,
    "translated_title": "混合整数规划在神经网络中的可逆性认证",
    "translated_abstract": "众所周知，神经网络容易遭受对抗攻击，即微小但影响显著的扰动可以改变网络的输出。相反，可能存在大的、有意义的扰动，但不影响网络的判断（过于不变性）。本研究探究这后一种现象在两个情境下的表现：（a）离散时间动态系统识别，以及（b）将神经网络的输出校准到另一个网络的输出。我们通过数学优化的方式来研究非可逆性，其中全局解通过与非可逆性边界的距离来度量网络预测的“安全性”。我们针对ReLU网络和$L_p$范数（$p = 1,2,\\infty$）构建了混合整数规划（MIP），这些规划适用于神经网络近似动态系统的情况。我们还讨论了如何将我们的发现用于神经网络之间的可逆性认证，例如不同网络之间的转换。",
    "tldr": "本研究利用混合整数规划来认证神经网络的可逆性，并探究了过于不变性的现象在两个情境下的表现，同时讨论了如何用这些发现进行神经网络之间的可逆性认证。",
    "en_tdlr": "This study employs mixed-integer programming to certify the invertibility of neural networks, investigates the phenomenon of excessive invariance in two contexts, and discusses how these findings can be applied to invertibility certification in transformations between neural networks."
}