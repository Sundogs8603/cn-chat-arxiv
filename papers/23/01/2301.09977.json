{
    "title": "The Backpropagation algorithm for a math student. (arXiv:2301.09977v3 [cs.LG] UPDATED)",
    "abstract": "A Deep Neural Network (DNN) is a composite function of vector-valued functions, and in order to train a DNN, it is necessary to calculate the gradient of the loss function with respect to all parameters. This calculation can be a non-trivial task because the loss function of a DNN is a composition of several nonlinear functions, each with numerous parameters. The Backpropagation (BP) algorithm leverages the composite structure of the DNN to efficiently compute the gradient. As a result, the number of layers in the network does not significantly impact the complexity of the calculation. The objective of this paper is to express the gradient of the loss function in terms of a matrix multiplication using the Jacobian operator. This can be achieved by considering the total derivative of each layer with respect to its parameters and expressing it as a Jacobian matrix. The gradient can then be represented as the matrix product of these Jacobian matrices. This approach is valid because the ch",
    "link": "http://arxiv.org/abs/2301.09977",
    "context": "Title: The Backpropagation algorithm for a math student. (arXiv:2301.09977v3 [cs.LG] UPDATED)\nAbstract: A Deep Neural Network (DNN) is a composite function of vector-valued functions, and in order to train a DNN, it is necessary to calculate the gradient of the loss function with respect to all parameters. This calculation can be a non-trivial task because the loss function of a DNN is a composition of several nonlinear functions, each with numerous parameters. The Backpropagation (BP) algorithm leverages the composite structure of the DNN to efficiently compute the gradient. As a result, the number of layers in the network does not significantly impact the complexity of the calculation. The objective of this paper is to express the gradient of the loss function in terms of a matrix multiplication using the Jacobian operator. This can be achieved by considering the total derivative of each layer with respect to its parameters and expressing it as a Jacobian matrix. The gradient can then be represented as the matrix product of these Jacobian matrices. This approach is valid because the ch",
    "path": "papers/23/01/2301.09977.json",
    "total_tokens": 891,
    "translated_title": "面向数学生的反向传播算法",
    "translated_abstract": "深度神经网络是向量值函数的复合函数，为了训练深度神经网络，需要计算相对于所有参数的损失函数梯度。这个计算是一个非常棘手的任务，因为深度神经网络的损失函数是由许多非线性函数组成的，并且每个函数都有许多参数。反向传播算法利用了深度神经网络的组合结构来高效地计算梯度。因此，网络中层数的数量不会显著影响计算的复杂性。本文的目的是使用Jacobian算子将损失函数的梯度表示为矩阵乘积，并通过考虑每层对其参数的全导数并将其表示为Jacobian矩阵来实现这一目标。可以将梯度表示为这些Jacobian矩阵的矩阵乘积。这种方法是有效的，因为计算链规则所需的乘积规则等于Jacobian矩阵的乘积。",
    "tldr": "本文介绍了针对深度神经网络的反向传播算法。通过使用Jacobian算子将损失函数的梯度表示为矩阵乘积，可以高效地计算梯度，而且该方法适用于不同数量的层数。"
}