{
    "title": "Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)",
    "abstract": "The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficie",
    "link": "http://arxiv.org/abs/2301.12579",
    "context": "Title: Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)\nAbstract: The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficie",
    "path": "papers/23/01/2301.12579.json",
    "total_tokens": 943,
    "translated_title": "通过本地规划实现样本有效的深度强化学习",
    "translated_abstract": "本文的重点是在模拟器上进行样本有效的深度强化学习。模拟器的一个有用特性是可以将环境重置到先前观察到的状态。我们提出了一种名为“不确定性优先本地规划”（UFLP）的算法框架，利用了这个特性。具体而言，在每个数据收集迭代中，我们的元算法以一定的概率将环境重置为具有高不确定性的观察状态，而不是根据初始状态分布进行采样。然后，代理-环境交互就像在标准在线强化学习设置中一样进行。我们证明了这个简单的过程可以显著改善几个基线强化学习算法在困难的探索任务上的采样成本。值得注意的是，利用我们的框架，我们可以使用简单（分布式）双重DQN在臭名昭著的难度很高的Atari游戏“蒙特祖玛之复仇”上实现超人类的表现。我们的工作可以看作是一种有效的方法。",
    "tldr": "提出了一种名为UFLP的算法框架，通过重置环境到高不确定性状态来提高深度强化学习的样本效率，实验证明这个简单的过程可以显著改善采样成本，并在困难的探索任务上取得超人类的表现。",
    "en_tdlr": "Proposed an algorithmic framework called UFLP to improve the sample efficiency of deep reinforcement learning by resetting the environment to high uncertainty states, demonstrated that this simple procedure can significantly improve sample cost and achieve super-human performance on difficult exploration tasks."
}