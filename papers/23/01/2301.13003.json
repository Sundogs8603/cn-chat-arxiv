{
    "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation. (arXiv:2301.13003v2 [cs.CL] UPDATED)",
    "abstract": "Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.",
    "link": "http://arxiv.org/abs/2301.13003",
    "context": "Title: Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation. (arXiv:2301.13003v2 [cs.CL] UPDATED)\nAbstract: Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.",
    "path": "papers/23/01/2301.13003.json",
    "total_tokens": 947,
    "translated_title": "通过分层蒸馏将预训练语言模型的知识转移到基于CIF的语音识别器",
    "translated_abstract": "大规模的预训练语言模型（PLMs）在自然语言处理任务中展现出了巨大的潜力。利用PLMs来增强自动语音识别（ASR）系统也成为了一个有前途的研究方向。然而，先前的研究受到PLMs结构不灵活和PLMs利用不充分等问题限制。为了缓解这些问题，我们提出了在连续积分和火灾（CIF）基础上用层次化知识蒸馏（HKD）。为了将PLMs的知识转移至ASR模型，HKD使用交叉模态知识蒸馏和声学级别对比损失以及语言级别的知识蒸馏和回归损失。与原始的CIF模型相比，我们的方法在AISHELL-1和LibriSpeech数据集上分别实现了15％和9％的相对误差率降低。",
    "tldr": "本文提出了一种分层蒸馏技术，在声学和语言级别上将预训练语言模型（PLMs）的知识转移到基于CIF的自动语音识别（ASR）模型，相较原始模型，在AISHELL-1和LibriSpeech数据集上分别实现了15%和9%的相对误差率降低。",
    "en_tdlr": "This paper proposes a hierarchical knowledge distillation technique to transfer knowledge from pre-trained language models (PLMs) to CIF-based automatic speech recognition (ASR) models at both acoustic and linguistic levels, achieving a relative error rate reduction of 15% and 9% on the AISHELL-1 and LibriSpeech datasets, respectively, compared to the original CIF model."
}