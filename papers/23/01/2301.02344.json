{
    "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. (arXiv:2301.02344v2 [cs.CR] UPDATED)",
    "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes ",
    "link": "http://arxiv.org/abs/2301.02344",
    "context": "Title: TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. (arXiv:2301.02344v2 [cs.CR] UPDATED)\nAbstract: With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes ",
    "path": "papers/23/01/2301.02344.json",
    "total_tokens": 949,
    "translated_title": "TrojanPuzzle: 隐秘地污染代码建议模型",
    "translated_abstract": "使用类似GitHub Copilot的工具，自动代码建议在软件工程中已经不再是一个梦想。这些基于大规模语言模型的工具通常在未经审核的公共代码来源中进行训练。因此，这些模型容易受到数据污染攻击，即对抗手通过注入恶意数据来操纵模型的训练。污染攻击可以被设计成在运行时影响模型对于特定上下文的建议，例如诱导模型建议不安全的代码负载。为了实现这一目标，先前的攻击会将不安全的代码负载明确地注入训练数据中，这使得恶意数据可以被静态分析工具检测并从训练集中删除。在这项工作中，我们展示了两种新颖的攻击，COVERT和TROJANPUZZLE，它们可以通过将恶意污染数据植入到上下文之外的区域（如文档字符串）来绕过静态分析。我们最新颖的攻击，TROJANPUZZLE，使得建议模型无法通过静态分析检测到恶意数据。",
    "tldr": "本研究展示了两种新型攻击，COVERT和TROJANPUZZLE，可以通过在文档字符串等上下文之外的区域植入恶意数据，绕过静态分析，对代码建议模型进行隐秘污染。"
}