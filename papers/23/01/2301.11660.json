{
    "title": "Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)",
    "abstract": "As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the tremendous cost of fine-tuning. Despite the impressive results achieved by large pre-trained language models (PLMs) and various parameter-efficient transfer learning (PETL) methods on sundry benchmarks, it remains unclear if they can handle inputs that have been distributionally shifted effectively. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, on three different intention classification tasks, each utilizing various language models with different scales.",
    "link": "http://arxiv.org/abs/2301.11660",
    "context": "Title: Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)\nAbstract: As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the tremendous cost of fine-tuning. Despite the impressive results achieved by large pre-trained language models (PLMs) and various parameter-efficient transfer learning (PETL) methods on sundry benchmarks, it remains unclear if they can handle inputs that have been distributionally shifted effectively. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, on three different intention classification tasks, each utilizing various language models with different scales.",
    "path": "papers/23/01/2301.11660.json",
    "total_tokens": 853,
    "translated_title": "使用参数高效的迁移学习评估语言模型的超分布鲁棒性",
    "translated_abstract": "随着预训练语言模型（PLM）的规模不断增加，最近提出了许多参数高效的迁移学习方法，以弥补微调成本的巨大代价。尽管大型预训练语言模型（PLMs）和各种参数高效的迁移学习（PETL）方法在各种基准测试中取得了令人印象深刻的结果，但它们是否能有效地处理已分布改变的输入仍不清楚。在本研究中，我们系统地探讨了随着PLM大小增长或改变传输方法，检测超分布（OOD）的能力如何改变。具体而言，我们评估了各种PETL技术，包括微调、Adapter、LoRA和前缀调整，在三个不同的意图分类任务上进行了评估，每个任务都使用不同规模的语言模型。",
    "tldr": "本文评估了各种参数高效的迁移学习方法对不同规模的语言模型在三个不同的意图分类任务中检测超分布（OOD）的能力，旨在为语言模型的超分布鲁棒性提供参考。",
    "en_tdlr": "This paper evaluates various parameter-efficient transfer learning methods on different language models with various scales for three different intention classification tasks to investigate the ability to detect out-of-distribution changes, aiming to provide a reference for the out-of-distribution robustness of language models."
}