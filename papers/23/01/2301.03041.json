{
    "title": "Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning. (arXiv:2301.03041v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised learning enables networks to learn discriminative features from massive data itself. Most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. By utilizing the consistency of two augmentations, the burden of manual annotations can be freed. Contrastive learning exploits instance-level information to learn robust features. However, the learned information is probably confined to different views of the same instance. In this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. In contrast to instance-level information, the similarity between two distinct images may provide more useful information. Besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. These two losses are essential for most deep learning methods. However, the relation between these two losses is not clear. Similarity loss helps o",
    "link": "http://arxiv.org/abs/2301.03041",
    "context": "Title: Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning. (arXiv:2301.03041v2 [cs.CV] UPDATED)\nAbstract: Self-supervised learning enables networks to learn discriminative features from massive data itself. Most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. By utilizing the consistency of two augmentations, the burden of manual annotations can be freed. Contrastive learning exploits instance-level information to learn robust features. However, the learned information is probably confined to different views of the same instance. In this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. In contrast to instance-level information, the similarity between two distinct images may provide more useful information. Besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. These two losses are essential for most deep learning methods. However, the relation between these two losses is not clear. Similarity loss helps o",
    "path": "papers/23/01/2301.03041.json",
    "total_tokens": 1165,
    "translated_title": "自监督学习中相似性损失和聚类损失之间关系的研究",
    "translated_abstract": "自监督学习通过大量数据使网络学习具有辨别力的特征。大多数最先进的方法基于对比学习，通过最大化图像两个扩增版本之间的相似性来实现。通过利用两个扩增版本的一致性，可以解放手动注释的负担。然而，对比学习只是利用了实例级别的信息来学习鲁棒的特征，而学到的信息可能仅局限于同一实例的不同视图。本文试图利用两个不同图像之间的相似性来提高自监督学习中的表示。我们分析了相似性损失和特征级交叉熵损失之间的关系。这两个损失对大多数深度学习方法至关重要，但它们之间的关系并不清楚。相似性损失有助于在自监督学习中获得具有辨别力和鲁棒性的特征。聚类损失强调在聚类中分组相似数据并分离不同数据。我们提出了一个新的自监督学习框架，该框架由相似性损失和聚类损失组成。我们的框架利用了两个不同图像之间的相似性，并利用聚类损失进一步增强表示。实验结果表明，我们的框架在ImageNet数据集和下游任务中优于最先进的自监督方法。",
    "tldr": "本文提出了一种新的自监督学习框架，该框架由相似性损失和聚类损失组成。该框架利用了两个不同图像之间的相似性，并利用聚类损失进一步增强表示。实验结果表明，该框架在ImageNet数据集和下游任务中优于最先进的自监督方法。",
    "en_tdlr": "This paper proposes a novel self-supervised learning framework consisting of similarity loss and clustering loss, which utilizes the similarity between two distinct images and further enhances representation through clustering. The framework outperforms the state-of-the-art self-supervised methods on ImageNet and downstream tasks."
}