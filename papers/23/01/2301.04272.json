{
    "title": "Data Distillation: A Survey. (arXiv:2301.04272v2 [cs.LG] UPDATED)",
    "abstract": "The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.",
    "link": "http://arxiv.org/abs/2301.04272",
    "context": "Title: Data Distillation: A Survey. (arXiv:2301.04272v2 [cs.LG] UPDATED)\nAbstract: The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.",
    "path": "papers/23/01/2301.04272.json",
    "total_tokens": 879,
    "translated_title": "数据精炼综述",
    "translated_abstract": "深度学习的流行导致了大量各种各样的数据集的整理。尽管在个别任务上表现接近人类水平，但在大型数据集上训练参数庞大的模型面临多方面的问题，如（a）高模型训练时间；（b）慢的研究迭代；和（c）差的生态可持续性。作为替代方案，数据精炼方法旨在合成简洁的数据摘要，这些摘要可以作为原始数据集的有效替代品，用于模型训练、推理、架构搜索等场景。在本综述中，我们提出了数据精炼的一个形式框架，并提供了现有方法的详细分类。此外，我们还涵盖了针对不同数据类型的数据精炼方法，包括图像、图形和用户-项目交互（推荐系统），同时确定了当前的挑战和未来的研究方向。",
    "tldr": "这篇综述介绍了数据精炼的概念和方法，以及针对不同数据类型的应用。数据精炼方法可以用于模型训练、推理和架构搜索等场景，以解决使用大型数据集训练模型所带来的问题。",
    "en_tdlr": "This survey provides an overview of data distillation, including its concepts, methods, and applications for different data types. Data distillation approaches offer solutions to the challenges of training models on large datasets, and can be applied in scenarios such as model training, inference, and architecture search."
}