{
    "title": "Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization. (arXiv:2301.13303v2 [stat.ML] UPDATED)",
    "abstract": "To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky (SIC) factors. We combine this variational approximation of the posterior with a similar and efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular SIC ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.",
    "link": "http://arxiv.org/abs/2301.13303",
    "context": "Title: Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization. (arXiv:2301.13303v2 [stat.ML] UPDATED)\nAbstract: To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky (SIC) factors. We combine this variational approximation of the posterior with a similar and efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular SIC ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.",
    "path": "papers/23/01/2301.13303.json",
    "total_tokens": 982,
    "translated_title": "双Kullback-Leibler最小化的变分稀疏逆Cholesky近似用于潜在高斯过程",
    "translated_abstract": "为了实现可扩展和准确的潜在高斯过程推断，我们提出了一种基于一族具有稀疏逆Cholesky（SIC）因子的高斯分布的变分逼近。我们将该变分逼近的后验与类似的高效SIC约束的Kullback-Leibler最优先验逼近相结合。然后，我们重点研究了特定的SIC排序和基于最近邻的稀疏模式，从而产生了高度准确的先验和后验逼近。对于这种设置，我们的变分逼近可以通过每次迭代的对数多项式时间的随机梯度下降来计算。我们提供了数字比较，表明所提出的双Kullback-Leibler最优高斯过程逼近（DKLGP）有时可以比诸如诱导点和均值场逼近等在类似计算复杂度下更准确地预测平稳核函数。",
    "tldr": "本文提出了一种基于稀疏逆Cholesky因子的高斯分布的变分逼近方法，结合同样高效的SIC约束的Kullback-Leibler最优先验逼近，并在特定SIC排序和稀疏模式下，实现对潜在高斯过程的高度准确先验和后验逼近。与其他方法相比，该方法可以在类似计算复杂度下更准确地预测平稳核函数。",
    "en_tdlr": "This paper proposes a variational approximation method based on Gaussian distributions with sparse inverse Cholesky factors, combined with efficient SIC-constrained Kullback-Leibler-optimal prior approximation. It achieves highly accurate prior and posterior approximations for latent Gaussian processes with specific SIC ordering and sparse patterns, and outperforms alternative approaches at similar computational complexity."
}