{
    "title": "Train Hard, Fight Easy: Robust Meta Reinforcement Learning. (arXiv:2301.11147v2 [cs.LG] UPDATED)",
    "abstract": "A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both *biased gradients* and *data inefficiency*. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple na",
    "link": "http://arxiv.org/abs/2301.11147",
    "context": "Title: Train Hard, Fight Easy: Robust Meta Reinforcement Learning. (arXiv:2301.11147v2 [cs.LG] UPDATED)\nAbstract: A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both *biased gradients* and *data inefficiency*. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple na",
    "path": "papers/23/01/2301.11147.json",
    "total_tokens": 906,
    "translated_title": "刻苦训练，轻松战斗：强健的元强化学习",
    "translated_abstract": "强化学习在实际应用中面临着环境、任务或客户之间的差异，元强化学习(MRL)通过学习适应新任务的元策略来解决这个问题。标准的MRL方法优化任务的平均回报，但在高风险或高难度的任务中往往结果不佳。由于事先不知道测试任务，这限制了系统的可靠性。在这项工作中，我们定义了一个具有可控鲁棒性水平的强健MRL目标。我们证明了在我们提出的MRL框架中，梯度偏差消失了。通过新颖的Robust Meta RL算法（RoML），我们解决了数据效率的问题。RoML是一个元算法，通过在训练过程中识别和过采样更难的任务来生成任何给定MRL算法的强健版本。我们证明了RoML在多个任务上实现了强健回报。",
    "tldr": "本文提出了一个强健的元强化学习算法，通过学习适应新任务的元策略来解决强化学习中环境和任务变化的挑战，通过识别和过采样更难的任务来提高系统的可靠性和效率。",
    "en_tdlr": "This paper proposes a robust meta reinforcement learning algorithm that adapts to new tasks to address the challenge of variation in environments and tasks in reinforcement learning, improving system reliability and efficiency by identifying and oversampling harder tasks."
}