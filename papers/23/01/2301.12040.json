{
    "title": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts. (arXiv:2301.12040v2 [q-bio.BM] UPDATED)",
    "abstract": "Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original represen",
    "link": "http://arxiv.org/abs/2301.12040",
    "context": "Title: ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts. (arXiv:2301.12040v2 [q-bio.BM] UPDATED)\nAbstract: Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original represen",
    "path": "papers/23/01/2301.12040.json",
    "total_tokens": 954,
    "translated_title": "ProtST: 蛋白质序列与生物医学文本的多模态学习",
    "translated_abstract": "当前的蛋白质语言模型（PLMs）主要基于蛋白质的序列学习蛋白质的表征，从而很好地捕捉到共进化信息，但它们无法明确获得蛋白质的功能，这是蛋白质表征学习的最终目标。幸运的是，对于许多蛋白质来说，它们的文本属性描述是可用的，其中也描述了它们的各种功能。受到这个事实的启发，我们首先构建了ProtDescribe数据集，通过蛋白质序列的文本描述来增强它们的功能和其他重要属性。基于这个数据集，我们提出了ProtST框架，通过生物医学文本来增强蛋白质序列的预训练和理解。在预训练过程中，我们设计了三种类型的任务，即单模态掩码预测、多模态表示对齐和多模态掩码预测，以增强PLM具有不同粒度的蛋白质属性信息，并同时保持PLM的原始表征。",
    "tldr": "ProtST是一个多模态学习框架，利用蛋白质的文本描述来增强蛋白质序列的预训练和理解，通过设计多种类型的任务，该框架能够获得不同粒度的蛋白质属性信息，并保持原始表征。",
    "en_tdlr": "ProtST is a multi-modality learning framework that enhances protein sequence pre-training and understanding by utilizing textual descriptions. By designing various tasks, the framework acquires protein property information at different granularities while preserving the original representation."
}