{
    "title": "On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)",
    "abstract": "Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.",
    "link": "http://arxiv.org/abs/2301.01828",
    "context": "Title: On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)\nAbstract: Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.",
    "path": "papers/23/01/2301.01828.json",
    "total_tokens": 943,
    "translated_title": "关于连续学习的顺序贝叶斯推断",
    "translated_abstract": "顺序贝叶斯推断可用于连续学习，以防止过去任务的灾难性遗忘，并在学习新任务时提供信息丰富的先验。我们重新审视顺序贝叶斯推断，并测试是否有访问真实后验的保证可以防止贝叶斯神经网络中的灾难性遗忘。为了做到这一点，我们使用哈密顿蒙特卡洛执行顺序贝叶斯推断。我们通过对哈密顿蒙特卡洛样本拟合密度估计器，将后验传播为新任务的先验。我们发现这种方法无法防止灾难性遗忘，证明了在神经网络中执行顺序贝叶斯推断的困难。然后，我们研究了顺序贝叶斯推断和连续学习的简单分析示例，并强调了模型错误说明问题，即使进行了准确的推断，也可能导致次优的连续学习性能。此外，我们讨论了任务数据不平衡可能导致遗忘的问题。",
    "tldr": "这篇论文研究了顺序贝叶斯推断在连续学习中的应用。研究表明，尽管使用了真实后验，这种方法仍无法防止神经网络中的灾难性遗忘。同时，模型误差和任务数据不平衡也会导致连续学习性能的下降。",
    "en_tdlr": "This paper investigates the use of sequential Bayesian inference in continual learning. The study shows that this approach fails to prevent catastrophic forgetting in neural networks, even with access to the true posterior. Additionally, model misspecification and task data imbalances can also lead to sub-optimal continual learning performance."
}