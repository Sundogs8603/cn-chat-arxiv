{
    "title": "On Enhancing Expressive Power via Compositions of Single Fixed-Size ReLU Network. (arXiv:2301.12353v2 [cs.LG] UPDATED)",
    "abstract": "This paper explores the expressive power of deep neural networks through the framework of function compositions. We demonstrate that the repeated compositions of a single fixed-size ReLU network exhibit surprising expressive power, despite the limited expressive capabilities of the individual network itself. Specifically, we prove by construction that $\\mathcal{L}_2\\circ \\boldsymbol{g}^{\\circ r}\\circ \\boldsymbol{\\mathcal{L}}_1$ can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\\mathcal{O}(r^{-1/d})$, where $\\boldsymbol{g}$ is realized by a fixed-size ReLU network, $\\boldsymbol{\\mathcal{L}}_1$ and $\\mathcal{L}_2$ are two affine linear maps matching the dimensions, and $\\boldsymbol{g}^{\\circ r}$ denotes the $r$-times composition of $\\boldsymbol{g}$. Furthermore, we extend such a result to generic continuous functions on $[0,1]^d$ with the approximation error characterized by the modulus of continuity. Our results reveal that a continuous-depth network generat",
    "link": "http://arxiv.org/abs/2301.12353",
    "context": "Title: On Enhancing Expressive Power via Compositions of Single Fixed-Size ReLU Network. (arXiv:2301.12353v2 [cs.LG] UPDATED)\nAbstract: This paper explores the expressive power of deep neural networks through the framework of function compositions. We demonstrate that the repeated compositions of a single fixed-size ReLU network exhibit surprising expressive power, despite the limited expressive capabilities of the individual network itself. Specifically, we prove by construction that $\\mathcal{L}_2\\circ \\boldsymbol{g}^{\\circ r}\\circ \\boldsymbol{\\mathcal{L}}_1$ can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\\mathcal{O}(r^{-1/d})$, where $\\boldsymbol{g}$ is realized by a fixed-size ReLU network, $\\boldsymbol{\\mathcal{L}}_1$ and $\\mathcal{L}_2$ are two affine linear maps matching the dimensions, and $\\boldsymbol{g}^{\\circ r}$ denotes the $r$-times composition of $\\boldsymbol{g}$. Furthermore, we extend such a result to generic continuous functions on $[0,1]^d$ with the approximation error characterized by the modulus of continuity. Our results reveal that a continuous-depth network generat",
    "path": "papers/23/01/2301.12353.json",
    "total_tokens": 852,
    "translated_title": "通过单个固定大小RELU网络的组合来增强表达能力",
    "translated_abstract": "本文探讨了深度神经网络的表达能力，通过函数组合的框架，我们证明了重复组合单个固定大小RELU网络的惊人表达能力，尽管单个网络的表达能力有限。我们进一步将此结果扩展到了$[0,1]^d$上的一般连续函数，其逼近误差由连续性模量表征。",
    "tldr": "本文探讨了深度神经网络的表达能力，通过组合单个固定大小RELU网络，证明了其具有惊人的表达能力，尤其是在逼近具有$1-$Lipschitz连续性和一般连续性的函数时。"
}