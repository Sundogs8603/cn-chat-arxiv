{
    "title": "Continuous Spatiotemporal Transformers. (arXiv:2301.13338v2 [cs.LG] UPDATED)",
    "abstract": "Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.",
    "link": "http://arxiv.org/abs/2301.13338",
    "context": "Title: Continuous Spatiotemporal Transformers. (arXiv:2301.13338v2 [cs.LG] UPDATED)\nAbstract: Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.",
    "path": "papers/23/01/2301.13338.json",
    "total_tokens": 794,
    "translated_title": "连续时空转换器",
    "translated_abstract": "在机器学习中，建模时空动态系统是一个重要的挑战。Transformer模型在自然语言处理和计算机视觉领域取得了很大的成功，能够提供可解释的数据表示。然而，在建模连续动态系统时，传统的Transformer模型存在一个限制，即它们是基于离散时间和空间的模型，因此无法保证连续采样。为了解决这个挑战，我们提出了一种新的连续时空转换器（CST）架构，专门用于建模连续系统。这种新的框架通过在Sobolev空间中进行优化，保证了连续平滑的输出。我们将CST与传统的Transformer模型以及其他时空动态建模方法进行了对比实验，结果表明在多个合成和真实系统的任务中，包括从钙成像数据中学习脑动力学，CST取得了卓越的性能。",
    "tldr": "连续时空转换器（CST）是一种新的转换器架构，通过在Sobolev空间中进行优化，能够建模连续系统并保证连续平滑的输出。在多个任务上，包括学习脑动力学，CST表现出卓越的性能。"
}