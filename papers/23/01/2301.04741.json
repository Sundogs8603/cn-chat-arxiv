{
    "title": "Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models",
    "abstract": "Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual's preferences without requiring a hand-crafted reward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical eviden",
    "link": "https://arxiv.org/abs/2301.04741",
    "context": "Title: Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models\nAbstract: Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual's preferences without requiring a hand-crafted reward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical eviden",
    "path": "papers/23/01/2301.04741.json",
    "total_tokens": 1011,
    "translated_title": "使用学习到的动力学模型的高效基于偏好的强化学习",
    "translated_abstract": "基于偏好的强化学习（PbRL）可以使机器人在不需要手工制作的奖励函数的情况下，根据个体的偏好学习执行任务。然而，现有的方法要么假设可以访问高保真度的模拟器或解析模型，要么采用无模型的方法，需要大量、可能不安全的在线环境交互。在本文中，我们研究了在执行PbRL时使用学习到的动力学模型的好处和挑战。特别地，我们提供了以下证据，表明在执行PbRL时，学习到的动力学模型提供了以下好处：（1）相对于基于模型的无模型PbRL，偏好引导和策略优化需要更少的环境交互，（2）多样化的偏好查询可以安全高效地作为标准的基于模型的RL的副产品合成，（3）基于次优示范的奖励预训练可以在没有任何环境交互的情况下进行。我们的论文提供实证证据",
    "tldr": "本文研究了在基于偏好的强化学习（PbRL）中使用学习到的动力学模型的好处和挑战，并提供了实证证据：（1）相对于模型无关的PbRL，学习到的动力学模型可以大大减少环境交互次数，（2）学习到的动力学模型可以安全高效地合成多样化的偏好查询，（3）基于次优示范的奖励预训练可以在没有环境交互的情况下进行。",
    "en_tdlr": "This paper investigates the benefits and challenges of using a learned dynamics model in preference-based reinforcement learning (PbRL), and provides empirical evidence: (1) The learned dynamics model can significantly reduce the number of environment interactions compared to model-free PbRL, (2) The learned dynamics model can safely and efficiently synthesize diverse preference queries, (3) Reward pre-training based on suboptimal demonstrations can be conducted without any environmental interaction."
}