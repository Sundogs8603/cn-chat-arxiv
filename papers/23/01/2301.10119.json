{
    "title": "Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)",
    "abstract": "Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call \"minimal value-equivalent partial models\". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds",
    "link": "http://arxiv.org/abs/2301.10119",
    "context": "Title: Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)\nAbstract: Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call \"minimal value-equivalent partial models\". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds",
    "path": "papers/23/01/2301.10119.json",
    "total_tokens": 866,
    "translated_title": "基于最小价值等价部分模型的生涯强化学习的可扩展和鲁棒规划",
    "translated_abstract": "从纯交互中学习环境模型通常被认为是构建生涯强化学习智能体的至关组成部分。然而，在基于模型的强化学习中，通常的做法是学习模型来对智能体的环境的每个方面进行建模，无论这些方面是否在提出最优决策方面重要。本文认为这种模型并不适合在生涯强化学习场景中执行可扩展和鲁棒的规划，因此提出了只模拟环境中相关方面的新型模型，称为“最小价值等价部分模型”。本文提供了这些模型的正式定义，并提供了理论结果来证明使用这些模型进行规划的可扩展性优势，然后进行实验以从实证角度说明我们的理论结果，最后提出一些有用的启发式方法来学习这些模型。",
    "tldr": "本文提出了一种仅模拟环境中相关方面的“最小价值等价部分模型”，并证明了这些模型用于规划在生涯强化学习场景中具有可扩展性优势。",
    "en_tdlr": "This paper proposes \"minimal value-equivalent partial models\" that only model the relevant aspects of the environment for scalable and robust planning in lifelong reinforcement learning scenarios, and proves the scalability advantages of using these models for planning."
}