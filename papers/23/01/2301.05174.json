{
    "title": "Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study. (arXiv:2301.05174v2 [cs.IR] UPDATED)",
    "abstract": "Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, ",
    "link": "http://arxiv.org/abs/2301.05174",
    "context": "Title: Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study. (arXiv:2301.05174v2 [cs.IR] UPDATED)\nAbstract: Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, ",
    "path": "papers/23/01/2301.05174.json",
    "total_tokens": 928,
    "translated_title": "基于场景和对象的图像-文本跨模态检索：一项可复现性研究",
    "translated_abstract": "大多数跨模态检索（CMR）方法要么聚焦于以对象为中心的数据集，即每个文档描绘或描述一个单一对象，要么聚焦于以场景为中心的数据集，即每个图像描绘或描述相互关联的多个对象和关系的复杂场景。我们认为一个强大的CMR模型应该在两种数据集类型上都具有良好的泛化能力。尽管CMR取得了一些进展，但结果的可复现性及其在不同数据集类型上的泛化性尚未被研究过。我们填补了这个空白，并关注当在以对象为中心和以场景为中心的数据集上评估时，最先进的CMR结果的可复现性。我们选择了两种具有不同体系结构的最先进CMR模型：（i）CLIP；以及（ii）X-VLM。此外，我们选择了两个以场景为中心的数据集和三个以对象为中心的数据集，并确定了所选模型在这些数据集上的相对性能。",
    "tldr": "这项研究关注基于场景和对象的图像-文本跨模态检索的可复现性，通过选择不同体系结构的最先进模型并在不同类型的数据集上进行评估，探讨了其在不同数据集类型上的泛化能力。"
}