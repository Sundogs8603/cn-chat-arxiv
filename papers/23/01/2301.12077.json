{
    "title": "ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning. (arXiv:2301.12077v2 [cs.CV] UPDATED)",
    "abstract": "Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical guarantees, called ``Adjusting Label Importance Mechanism (ALIM)''. It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on benchmark datasets demonstrate that our method can achieve state-of-",
    "link": "http://arxiv.org/abs/2301.12077",
    "context": "Title: ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning. (arXiv:2301.12077v2 [cs.CV] UPDATED)\nAbstract: Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical guarantees, called ``Adjusting Label Importance Mechanism (ALIM)''. It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on benchmark datasets demonstrate that our method can achieve state-of-",
    "path": "papers/23/01/2301.12077.json",
    "total_tokens": 945,
    "translated_title": "ALIM: 为噪声局部标签学习调整标签重要性机制",
    "translated_abstract": "噪声局部标签学习（noisy PLL）是弱监督学习的一个重要分支。与要求地面真实标签藏在候选标签集中的PLL不同，噪声PLL放松了这个限制，并允许地面真实标签可能不在候选标签集中。为了解决这个具有挑战性的问题，大部分现有的工作试图检测噪声样本并为每个噪声样本估计地面真实标签。然而，检测误差是不可避免的。这些错误可以在训练期间积累，并持续影响模型优化。为此，我们提出了一种新的具有理论保证的噪声PLL框架，称为“调整标签重要性机制（ALIM）”。它旨在通过权衡初始候选集和模型输出来减少检测错误的负面影响。ALIM是一种可与现有PLL方法集成的插件策略。在基准数据集上的实验结果表明，我们的方法可以在噪声PLL任务中达到最先进的性能。",
    "tldr": "提出一种噪声局部标签学习的框架ALIM，通过权衡候选集和模型输出来减少检测误差的负面影响，可以与现有PLL方法集成，并在基准数据集上达到最先进性能。",
    "en_tdlr": "A novel framework called ALIM is proposed for noisy partial label learning, which aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. It can be integrated with existing PLL approaches and achieves state-of-the-art performance on benchmark datasets."
}