{
    "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training",
    "abstract": "arXiv:2301.07482v3 Announce Type: replace  Abstract: A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the currently-available shortcuts involved. To address these limitations, we instead propose FreshGNN, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is de",
    "link": "https://arxiv.org/abs/2301.07482",
    "context": "Title: FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training\nAbstract: arXiv:2301.07482v3 Announce Type: replace  Abstract: A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the currently-available shortcuts involved. To address these limitations, we instead propose FreshGNN, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is de",
    "path": "papers/23/01/2301.07482.json",
    "total_tokens": 914,
    "translated_title": "FreshGNN: 通过稳定的历史嵌入减少图神经网络训练中的内存访问",
    "translated_abstract": "训练大规模真实世界图上的图神经网络（GNN）模型时的一个关键性能瓶颈是将节点特征加载到GPU上。由于GPU内存有限，需要进行昂贵的数据移动，以便在速度较慢的备用设备上存储这些特征（例如CPU内存）。此外，图结构的不规则性导致数据局部性差，进一步加剧了问题。因此，目前能够高效训练大型GNN模型的现有框架通常由于涉及当前可用的快捷方式而导致显著的准确性降低。 为解决这些限制，我们提出了FreshGNN，这是一个通用的GNN小批量训练框架，它利用一个历史缓存来存储和重复使用GNN节点嵌入，而不是通过在每次迭代中提取原始特征来重新计算它们。 其成功的关键在于相应的缓存策略。",
    "tldr": "FreshGNN是一种通用的GNN小批量训练框架，通过稳定的历史缓存存储和重复使用GNN节点嵌入，在训练大型GNN模型时减少内存访问，解决了当前框架存在的准确性降低问题。",
    "en_tdlr": "FreshGNN is a general-purpose GNN mini-batch training framework that reduces memory access during training large GNN models by storing and reusing GNN node embeddings through stable historical cache, addressing the accuracy degradation issue in existing frameworks."
}