{
    "title": "MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)",
    "abstract": "Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in",
    "link": "http://arxiv.org/abs/2301.13287",
    "context": "Title: MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)\nAbstract: Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in",
    "path": "papers/23/01/2301.13287.json",
    "total_tokens": 868,
    "translated_title": "MILO: 模型无关子集选择框架，用于高效模型训练和调优。",
    "translated_abstract": "训练深度网络和调优大型数据集的超参数是计算密集型的。减少训练成本的主要研究方向之一是通过选择很好的训练数据子集来实现。与简单的自适应随机子集选择基准相比，现有的智能子集选择方法由于耗时的子集选择步骤而不具竞争力，该步骤涉及计算依赖于模型的梯度和特征嵌入，并应用子模块目标的贪心最大化。我们的关键洞察是消除对下游模型参数的依赖，将子集选择作为预处理步骤，并使其能够在不增加成本的情况下训练多个模型。在这个工作中，我们提出了 MILO，一个模型无关的子集选择框架，它将子集选择与模型训练分离，同时通过使用一个易到难的课程实现了卓越的模型收敛和性能。通过实验结果验证了我们的方法。",
    "tldr": "提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。"
}