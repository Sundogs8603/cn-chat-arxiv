{
    "title": "Alternating Updates for Efficient Transformers. (arXiv:2301.13310v2 [cs.LG] UPDATED)",
    "abstract": "It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language ",
    "link": "http://arxiv.org/abs/2301.13310",
    "context": "Title: Alternating Updates for Efficient Transformers. (arXiv:2301.13310v2 [cs.LG] UPDATED)\nAbstract: It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language ",
    "path": "papers/23/01/2301.13310.json",
    "total_tokens": 909,
    "translated_title": "高效Transformer的交替更新方法",
    "translated_abstract": "众所周知，增加深度Transformer网络的规模可以提高模型的质量和性能。然而，这种规模的增加往往会导致计算成本和推理延迟的大幅增加。我们引入了交替更新（AltUp）方法，这是一种简单易实现的方法，可以增加模型的容量而不增加计算负担。AltUp通过在每一层中对扩展表示的子块进行操作，并使用预测和修正机制来更新未激活的块，从而实现了仅在延迟上微不足道的情况下扩大了学习表示，即标记嵌入。我们还介绍了AltUp的扩展，例如其在序列维度上的适用性，并展示了如何将AltUp与现有方法（如稀疏专家混合模型）结合起来，以获得具有更高容量的高效模型。我们在基准Transformer模型和语言任务上进行了实验证明了AltUp方法的有效性。",
    "tldr": "本文介绍了一种交替更新（AltUp）的方法，可以在不增加计算负担的情况下增加模型的容量，通过对扩展表示的子块进行操作并使用预测和修正机制来更新未激活的块。实验证明AltUp方法在提高Transformer模型的容量和效率方面是有效的。",
    "en_tdlr": "This paper introduces a method called Alternating Updates (AltUp) that increases a model's capacity without increasing computational burden. AltUp works on subblocks of the widened representation using predict-and-correct mechanism to update inactivated blocks. Experimental results demonstrate the effectiveness of AltUp in increasing the capacity and efficiency of Transformer models."
}