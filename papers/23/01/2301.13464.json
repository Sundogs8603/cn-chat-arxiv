{
    "title": "Training with Mixed-Precision Floating-Point Assignments. (arXiv:2301.13464v2 [cs.LG] UPDATED)",
    "abstract": "When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment -- a mapping from all tensors (arising in training) to precision levels (high or low) -- that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classification tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2x memory reduction over a bas",
    "link": "http://arxiv.org/abs/2301.13464",
    "context": "Title: Training with Mixed-Precision Floating-Point Assignments. (arXiv:2301.13464v2 [cs.LG] UPDATED)\nAbstract: When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment -- a mapping from all tensors (arising in training) to precision levels (high or low) -- that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classification tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2x memory reduction over a bas",
    "path": "papers/23/01/2301.13464.json",
    "total_tokens": 800,
    "translated_title": "混合精度浮点数运算的训练",
    "translated_abstract": "在深度神经网络训练中，将所有张量保持在高精度（例如32位甚至16位浮点数）通常是浪费的。但是，将所有张量都保持在低精度（例如8位浮点数）会导致不可接受的准确性损失。因此，使用一个精度分配 - 即将所有张量（在训练中出现的）映射到精度级别（高或低） - 非常重要，它将大多数张量保持在低精度并导致足够准确的模型。",
    "tldr": "通过运用混合精度浮点数运算的训练方法，我们为卷积神经网络生成精度分配，相比低精度浮点数训练中考虑的精度分配，其使用更少的内存，同时也导致更准确的卷积网络。"
}