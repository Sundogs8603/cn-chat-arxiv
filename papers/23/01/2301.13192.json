{
    "title": "Robust empirical risk minimization via Newton's method. (arXiv:2301.13192v2 [stat.ML] UPDATED)",
    "abstract": "A new variant of Newton's method for empirical risk minimization is studied, where at each iteration of the optimization algorithm, the gradient and Hessian of the objective function are replaced by robust estimators taken from existing literature on robust mean estimation for multivariate data. After proving a general theorem about the convergence of successive iterates to a small ball around the population-level minimizer, consequences of the theory in generalized linear models are studied when data are generated from Huber's epsilon-contamination model and/or heavytailed distributions. An algorithm for obtaining robust Newton directions based on the conjugate gradient method is also proposed, which may be more appropriate for high-dimensional settings, and conjectures about the convergence of the resulting algorithm are offered. Compared to robust gradient descent, the proposed algorithm enjoys the faster rates of convergence for successive iterates often achieved by second-order al",
    "link": "http://arxiv.org/abs/2301.13192",
    "context": "Title: Robust empirical risk minimization via Newton's method. (arXiv:2301.13192v2 [stat.ML] UPDATED)\nAbstract: A new variant of Newton's method for empirical risk minimization is studied, where at each iteration of the optimization algorithm, the gradient and Hessian of the objective function are replaced by robust estimators taken from existing literature on robust mean estimation for multivariate data. After proving a general theorem about the convergence of successive iterates to a small ball around the population-level minimizer, consequences of the theory in generalized linear models are studied when data are generated from Huber's epsilon-contamination model and/or heavytailed distributions. An algorithm for obtaining robust Newton directions based on the conjugate gradient method is also proposed, which may be more appropriate for high-dimensional settings, and conjectures about the convergence of the resulting algorithm are offered. Compared to robust gradient descent, the proposed algorithm enjoys the faster rates of convergence for successive iterates often achieved by second-order al",
    "path": "papers/23/01/2301.13192.json",
    "total_tokens": 967,
    "translated_title": "通过牛顿方法实现鲁棒经验风险最小化研究",
    "translated_abstract": "本文研究了一种新的牛顿方法变种，用于经验风险最小化。在优化算法的每次迭代中，目标函数的梯度和海森矩阵被替换为现有文献中针对多变量数据的鲁棒估计方法。在证明了连续迭代收敛到种群水平最小化器周围小球的一般定理之后，研究了当数据来自Huber的epsilon污染模型和/或重尾分布时，该理论在广义线性模型中的后果。还提出了一种基于共轭梯度方法获取鲁棒牛顿方向的算法，这可能更适用于高维情况，并提出了关于结果算法收敛性的猜想。与鲁棒梯度下降相比，所提出的算法能够实现更快的收敛速度。",
    "tldr": "本研究提出了一种鲁棒经验风险最小化的新的牛顿方法变种，并通过使用鲁棒估计方法来替换梯度和海森矩阵，证明了连续迭代收敛到种群水平最小化器周围小球。该方法在广义线性模型中的应用具有潜在的优势，并提出了一种基于共轭梯度方法的算法来获取鲁棒牛顿方向。"
}