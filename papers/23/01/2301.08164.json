{
    "title": "DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v2 [cs.LG] UPDATED)",
    "abstract": "We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mu",
    "link": "http://arxiv.org/abs/2301.08164",
    "context": "Title: DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v2 [cs.LG] UPDATED)\nAbstract: We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mu",
    "path": "papers/23/01/2301.08164.json",
    "total_tokens": 889,
    "translated_title": "DiME：通过熵矩阵的差异最大化互信息",
    "translated_abstract": "我们引入了一种信息理论量，具有与相互信息类似的性质，并可从数据中估计，而不需要对潜在分布进行明确假设。该数量基于最近提出的基于矩阵的熵，该熵利用规范化 Gram 矩阵的特征值来计算再生核 Hilbert 空间中未集中协方差运算符的特征值的估计值。我们展示了一种差异矩阵熵（DiME）对于涉及随机变量之间相互信息最大化的问题非常适用。虽然许多此类任务的方法可能会导致平凡解，但 DiME 自然会对这样的结果进行惩罚。我们将 DiME 与多个相互信息基准估计器在一个玩具高斯数据集上进行比较。我们提供了 DiME 的用例示例，例如潜在因子分解和多视图表示学习问题，其中 DiME 被用于学习视图之间的共享表示，该表示具有高互信息量。",
    "tldr": "本文提出了一种称为DiME的信息理论量，可以估计随机变量之间的互信息最大化，避免了平凡解，适用于多个实际应用。",
    "en_tdlr": "This paper proposes an information-theoretic quantity called DiME, which can estimate the maximization of mutual information between random variables without trivial solutions and is applicable to multiple practical applications such as latent factor decomposition and multi-view representation learning."
}