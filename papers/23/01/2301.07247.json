{
    "title": "Tailor: Altering Skip Connections for Resource-Efficient Inference. (arXiv:2301.07247v2 [cs.CV] UPDATED)",
    "abstract": "Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this paper, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network's skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware efficient implementation with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network's skip connections to lower their hardware cost. Tailor improves resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a 2D processing element array arc",
    "link": "http://arxiv.org/abs/2301.07247",
    "context": "Title: Tailor: Altering Skip Connections for Resource-Efficient Inference. (arXiv:2301.07247v2 [cs.CV] UPDATED)\nAbstract: Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this paper, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network's skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware efficient implementation with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network's skip connections to lower their hardware cost. Tailor improves resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a 2D processing element array arc",
    "path": "papers/23/01/2301.07247.json",
    "total_tokens": 952,
    "translated_title": "Tailor：为资源效率推断改变跳跃连接",
    "translated_abstract": "深度神经网络使用跳跃连接来提高训练收敛性。然而，这些跳跃连接在硬件上很昂贵，需要额外的缓冲区，并增加了片上和片外存储器的利用和带宽需求。本文展示了通过硬件-软件协同设计方法来优化硬件上的跳跃连接。作者认为，尽管网络的跳跃连接对网络的学习是必要的，但可以在不损失精度的情况下去除或缩短跳跃连接，以提供更高效的硬件实现。作者引入了Tailor，这是一个代码设计工具，其硬件感知的训练算法逐渐去除或缩短一个完全训练好的网络的跳跃连接，从而降低它们的硬件代价。Tailor在片上数据流式体系结构中提高了34％的BRAM利用率，13％的FF利用率和16％的LUT利用率。Tailor提高了30％的性能，并降低了45％的内存带宽。",
    "tldr": "本文介绍了一种硬件-软件协同设计方法，通过逐渐去除或缩短神经网络中的跳跃连接来实现更高效的硬件推断。实验结果显示，该方法可以显著提高硬件资源利用率和性能，同时减少内存带宽的需求。",
    "en_tdlr": "This paper presents a hardware-software co-design approach to achieve more efficient hardware inference by gradually removing or shortening skip connections in neural networks. Experimental results show that this method can significantly improve hardware resource utilization and performance, while reducing memory bandwidth requirements."
}