{
    "title": "A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence",
    "abstract": "Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous be",
    "link": "https://arxiv.org/abs/2301.13139",
    "context": "Title: A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence\nAbstract: Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous be",
    "path": "papers/23/01/2301.13139.json",
    "total_tokens": 835,
    "translated_title": "一种具有通用参数化和线性收敛的政策镜面下降新框架",
    "translated_abstract": "强化学习中现代政策优化方法（如TRPO和PPO）的成功归功于参数化政策的使用。然而，尽管已经为这类算法在标签设置中建立了理论保证，但对于通用参数化方案的使用仍然没有得到充分证明。在这项工作中，我们介绍了一种基于镜面下降的政策优化新框架，可以自然地适应通用参数化。我们方案所产生的政策类可以恢复已知的类，如softmax，并根据镜面映射的选择生成新类。使用我们的框架，我们获得了关于涉及通用参数化的基于政策梯度的方法的线性收敛的首个结果。为了展示我们的框架适应通用参数化方案的能力，我们提供了使用浅层神经网络时的样本复杂性，并展示它相对于先前方法的改进。",
    "tldr": "我们提出了一种新的政策优化框架，通过镜面下降自然地适应通用参数化，并获得了应用于通用参数化的基于政策梯度的方法的线性收敛保证。",
    "en_tdlr": "We propose a novel framework for policy optimization that naturally accommodates general parameterization using mirror descent, and obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization."
}