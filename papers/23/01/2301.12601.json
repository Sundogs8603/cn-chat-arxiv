{
    "title": "Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents. (arXiv:2301.12601v2 [cs.LG] UPDATED)",
    "abstract": "The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.",
    "link": "http://arxiv.org/abs/2301.12601",
    "context": "Title: Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents. (arXiv:2301.12601v2 [cs.LG] UPDATED)\nAbstract: The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.",
    "path": "papers/23/01/2301.12601.json",
    "total_tokens": 847,
    "translated_title": "基于递归优化等价性的马尔可夫决策过程的遗憾边界研究",
    "translated_abstract": "优化等价性（OCE）是一类风险测量，涵盖了重要的实例，如熵风险，条件风险价值和均值方差模型。在本文中，我们提出了一个新的基于表格马尔可夫决策过程和递归OCE的情节化风险敏感强化学习公式。我们设计了一个基于值迭代和上置信界的有效学习算法。我们推导了所提出算法的遗憾上界，同时建立了一个极小-最大下界。我们的界限表明，所提出的算法实现的遗憾率对于情歌数和动作数具有最优依赖性。",
    "tldr": "本文提出了基于递归优化等价性的马尔可夫决策过程的风险敏感强化学习公式，设计了一种有效学习算法，并且推导了算法的遗憾上界和极小-最大下界，表明该算法实现的遗憾率对于情歌数和动作数具有最优依赖性。",
    "en_tdlr": "This paper proposes a risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive optimized certainty equivalents (OCE), and designs an efficient learning algorithm based on value iteration and upper confidence bound. The upper and minimax lower bounds on the regret of the proposed algorithm are derived, showing that the regret rate achieved by this algorithm has optimal dependence on the number of episodes and the number of actions."
}