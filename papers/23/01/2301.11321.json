{
    "title": "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning. (arXiv:2301.11321v2 [cs.LG] UPDATED)",
    "abstract": "Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditi",
    "link": "http://arxiv.org/abs/2301.11321",
    "context": "Title: Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning. (arXiv:2301.11321v2 [cs.LG] UPDATED)\nAbstract: Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditi",
    "path": "papers/23/01/2301.11321.json",
    "total_tokens": 925,
    "translated_title": "轨迹感知的资格追踪在离线强化学习中的应用",
    "translated_abstract": "离线多步返回的非政策学习对于节约样本的强化学习至关重要，但抵消偏差的同时不加剧方差是具有挑战性的。一般来说，非政策偏差是通过资格追踪的方法来进行修正的，资格追踪通过通吃因子(Impotance Sampling)比例对过去的时间差分误差进行重新加权以纠正。许多离线算法都依赖这种机制，不同的是针对IS的统计估计方法所采用的“裁剪IS比例”协议的不同。不幸的是，一旦资格追踪被完全裁剪，其影响就无法逆转。这已经导致了将多个过去经历同时考虑在内的信用分配策略的出现。这些轨迹感知的方法尚未得到广泛的分析，它们的理论依据仍然不确定。本文提出了一种多步运算符，可以同时表达每个决策和轨迹感知的方法，并证明它们的收敛条件。",
    "tldr": "提出一种轨迹感知的资格追踪多步运算符，可以同时表达每个决策和轨迹感知的方法，并解决了被完全裁剪的资格追踪无法逆转的问题。",
    "en_tdlr": "A trajectory-aware eligibility traces multistep operator is proposed to solve the problem that the effect of eligibility traces cannot be reversed once fully cut, and it can express both per-decision and trajectory-aware methods."
}