{
    "title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)",
    "abstract": "Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack",
    "link": "http://arxiv.org/abs/2301.07836",
    "context": "Title: Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)\nAbstract: Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack",
    "path": "papers/23/01/2301.07836.json",
    "total_tokens": 928,
    "translated_title": "掩模自编码在大规模自然语言监督中没有帮助",
    "translated_abstract": "自我监督和自然语言监督已成为训练通用图像编码器的两种有效方法。最近的研究表明，这些方法可以有效地结合使用，但这些结果主要使用了小的预训练数据集（<50M样本），并且没有有效地反映出常用于这些方法的大规模数据集（>100M样本）的情况。本研究调查了类似的方法在使用更大量数据进行训练时是否有效。我们发现，在对11.3M的图像-文本对进行训练时，掩模自编码器和对比语言图像预训练的组合可以比只使用对比语言图像预训练更好，但在对1.4B个图像进行训练时，它与仅使用对比语言图像预训练相比，几乎没有任何优势（在一套常见的视觉任务中评估）。本研究为这些方法的有效性提供了一些需要的清晰度。",
    "tldr": "本研究旨在探讨大规模训练下掩模自编码和对比语言图像预训练的有效性。研究结果表明，在小规模训练中这两种方法可以有效地结合使用，但在大规模训练中没有明显的优势。",
    "en_tdlr": "This study aims to explore the effectiveness of combining masked auto-encoders and contrastive language image pre-training in large-scale training. The results show that these approaches can be effectively combined in small-scale training, but have little to no benefit in large-scale training."
}