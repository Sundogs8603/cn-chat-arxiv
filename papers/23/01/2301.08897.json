{
    "title": "ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)",
    "abstract": "Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).",
    "link": "http://arxiv.org/abs/2301.08897",
    "context": "Title: ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)\nAbstract: Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).",
    "path": "papers/23/01/2301.08897.json",
    "total_tokens": 819,
    "translated_title": "ScaDLES: 边缘端流式数据上的可扩展深度学习",
    "translated_abstract": "分布式深度学习（DDL）训练系统设计用于云和数据中心环境，假设具有均匀计算资源、高网络带宽、足够的内存和存储，以及在所有节点上独立和同分布的数据。然而，这些假设在边缘端不一定适用，特别是在在线方式下训练神经网络时。边缘计算面临系统和统计异质性的挑战。系统异质性归因于每个设备的计算资源和带宽的差异，而统计异质性则来自于边缘端的不平衡和偏斜数据。在处理流式数据时，设备之间的不同流速也可以成为异质性的另一个来源。如果流速低于训练批量大小，设备需要等待足够的样本流入后才能执行一次随机梯度下降（SGD）迭代。",
    "tldr": "ScaDLES是一种用于边缘端流式数据上的可扩展深度学习方法，解决了系统和统计异质性的挑战。",
    "en_tdlr": "ScaDLES is a scalable deep learning approach for streaming data at the edge, addressing challenges posed by system and statistical heterogeneity."
}