{
    "title": "Multimodal Lyrics-Rhythm Matching. (arXiv:2301.02732v2 [cs.SD] UPDATED)",
    "abstract": "Despite the recent increase in research on artificial intelligence for music, prominent correlations between key components of lyrics and rhythm such as keywords, stressed syllables, and strong beats are not frequently studied. This is likely due to challenges such as audio misalignment, inaccuracies in syllabic identification, and most importantly, the need for cross-disciplinary knowledge. To address this lack of research, we propose a novel multimodal lyrics-rhythm matching approach in this paper that specifically matches key components of lyrics and music with each other without any language limitations. We use audio instead of sheet music with readily available metadata, which creates more challenges yet increases the application flexibility of our method. Furthermore, our approach creatively generates several patterns involving various multimodalities, including music strong beats, lyrical syllables, auditory changes in a singer's pronunciation, and especially lyrical keywords, w",
    "link": "http://arxiv.org/abs/2301.02732",
    "context": "Title: Multimodal Lyrics-Rhythm Matching. (arXiv:2301.02732v2 [cs.SD] UPDATED)\nAbstract: Despite the recent increase in research on artificial intelligence for music, prominent correlations between key components of lyrics and rhythm such as keywords, stressed syllables, and strong beats are not frequently studied. This is likely due to challenges such as audio misalignment, inaccuracies in syllabic identification, and most importantly, the need for cross-disciplinary knowledge. To address this lack of research, we propose a novel multimodal lyrics-rhythm matching approach in this paper that specifically matches key components of lyrics and music with each other without any language limitations. We use audio instead of sheet music with readily available metadata, which creates more challenges yet increases the application flexibility of our method. Furthermore, our approach creatively generates several patterns involving various multimodalities, including music strong beats, lyrical syllables, auditory changes in a singer's pronunciation, and especially lyrical keywords, w",
    "path": "papers/23/01/2301.02732.json",
    "total_tokens": 1043,
    "translated_title": "多模式歌词-节奏匹配",
    "translated_abstract": "尽管近年来人工智能在音乐领域的研究增加了，但关于歌词和节奏这两个关键部分的主要相关性，如关键词、重音音节和强节拍等，不经常被研究。这可能是由于音频错位、音节识别的不准确性以及跨学科知识需求等挑战。为了解决这一不足，我们在本文中提出了一种新颖的多模式歌词-节奏匹配方法，其特别是在没有任何语言限制的情况下，将歌词和音乐的关键部分相互匹配。我们使用音频而不是有可用元数据的谱面，这增加了我们的方法的应用灵活性，但也带来了更多的挑战。此外，我们的方法创造性地生成了几种包含各种多模态的模式，包括音乐的强节拍、歌词音节、歌手发音的听觉变化以及尤其是歌词关键词，它们通常指示一首歌的中心信息。我们的实验表明，我们提出的方法在一个包含100首歌的数据集上，它们有歌词注释和MIDI文件，表现出优于现有方法的效果。",
    "tldr": "本文提出了一种多模式歌词-节奏匹配方法，采用音频而不是有可用元数据的谱面，可将歌词和音乐的关键部分相互匹配，包括音乐的强节拍、歌词音节和歌词关键词，并在数据集实验中表现出优越性能。",
    "en_tdlr": "This paper proposes a novel multimodal lyrics-rhythm matching approach using audio instead of sheet music, which matches key components of lyrics and music including music strong beats, lyrical syllables, and lyrical keywords, and outperforms existing methods on a dataset of 100 songs with lyric annotations and MIDI files."
}