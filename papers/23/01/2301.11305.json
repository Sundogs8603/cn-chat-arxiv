{
    "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v2 [cs.CL] UPDATED)",
    "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model samp",
    "link": "http://arxiv.org/abs/2301.11305",
    "context": "Title: DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v2 [cs.CL] UPDATED)\nAbstract: The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model samp",
    "path": "papers/23/01/2301.11305.json",
    "total_tokens": 922,
    "translated_title": "DetectGPT：使用概率曲率进行零样本机器生成文本检测",
    "translated_abstract": "大型语言模型（LLM）的流畅度和广泛使用突显了希望有相应的工具来帮助检测LLM生成的文本的需求。在本文中，我们发现了LLM概率函数结构的一个有用属性，对于这种检测非常有用。具体而言，我们证明从LLM中采样的文本倾向于占据模型的对数概率函数的负曲率区域。基于这一观察，我们定义了一种新的基于曲率的准则，用于判断一个段落是否是由给定的LLM生成的。这种方法被称为DetectGPT，不需要训练单独的分类器，收集真实或生成段落的数据集，也不需要明确地给生成的文本加水印。它只使用所关注模型计算的对数概率和来自另一个通用预训练语言模型（例如T5）的段落的随机扰动。我们发现DetectGPT比现有的零样本方法更具有区分能力，用于模型采样。",
    "tldr": "本论文提出了一种名为DetectGPT的方法，使用概率曲率来判断文本是否由一个给定的大型语言模型生成。该方法不需要训练分类器、收集数据集或明确加水印，只使用模型计算的对数概率和另一个预训练语言模型的随机扰动。实验证明，DetectGPT在模型采样方面比现有的零样本方法更具有区分能力。"
}