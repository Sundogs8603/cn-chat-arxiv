{
    "title": "A Stochastic Proximal Polyak Step Size. (arXiv:2301.04935v2 [math.OC] UPDATED)",
    "abstract": "Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.",
    "link": "http://arxiv.org/abs/2301.04935",
    "context": "Title: A Stochastic Proximal Polyak Step Size. (arXiv:2301.04935v2 [math.OC] UPDATED)\nAbstract: Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.",
    "path": "papers/23/01/2301.04935.json",
    "total_tokens": 929,
    "translated_title": "一种随机Proximal Polyak步长方法",
    "translated_abstract": "最近，随机Polyak步长（SPS）已成为随机梯度下降的竞争性自适应步长方案。在本文中，我们开发了ProxSPS，这是SPS的proximal变体，可以处理正则化项。开发SPS的proximal变体特别重要，因为SPS需要目标函数的下界才能发挥良好的作用。当目标函数是损失和正则化项的总和时，可用的总和下界估计可能不准确。相比之下，ProxSPS只需要对损失进行下界估计，而这通常很容易得到。因此，我们展示了ProxSPS更易于调整，在正则化的情况下更稳定。此外，对于图像分类任务，ProxSPS表现与AdamW一样好，几乎不需要调整，并且导致具有更小权重参数的网络。我们还为ProxSPS提供了广泛的收敛性分析，包括非光滑、光滑、弱凸和强凸设置。",
    "tldr": "本文开发了一种正则化的随机梯度下降ProxSPS算法，相比随机Polyak步长（SPS）更稳定易调整，同时在图像分类任务中表现良好，可导致网络具有更小的权重参数。",
    "en_tdlr": "This paper proposes a ProxSPS algorithm for stochastic gradient descent with regularization, which is more stable and easier to tune than the stochastic Polyak step size (SPS). It performs well in image classification tasks and leads to a network with smaller weight parameters. The paper also provides extensive convergence analysis for ProxSPS."
}