{
    "title": "Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)",
    "abstract": "While pre-trained language models can generate individually fluent sentences for automatic story generation, they struggle to generate stories that are coherent, sensible and interesting. Current state-of-the-art (SOTA) story generation models explore using higher-level features such as plots or commonsense knowledge to improve the quality of generated stories. Prompt-based learning using very large pre-trained language models (VLPLMs) such as GPT3 has demonstrated impressive performance even across various NLP tasks. In this paper, we present an extensive study using automatic and human evaluation to compare the story generation capability of VLPLMs to those SOTA models in three different datasets where stories differ in style, register and length. Our results show that VLPLMs generate much higher quality stories than other story generation models, and to a certain extent rival human authors, although preliminary investigation also reveals that they tend to ``plagiarise'' real stories",
    "link": "http://arxiv.org/abs/2301.09790",
    "context": "Title: Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)\nAbstract: While pre-trained language models can generate individually fluent sentences for automatic story generation, they struggle to generate stories that are coherent, sensible and interesting. Current state-of-the-art (SOTA) story generation models explore using higher-level features such as plots or commonsense knowledge to improve the quality of generated stories. Prompt-based learning using very large pre-trained language models (VLPLMs) such as GPT3 has demonstrated impressive performance even across various NLP tasks. In this paper, we present an extensive study using automatic and human evaluation to compare the story generation capability of VLPLMs to those SOTA models in three different datasets where stories differ in style, register and length. Our results show that VLPLMs generate much higher quality stories than other story generation models, and to a certain extent rival human authors, although preliminary investigation also reveals that they tend to ``plagiarise'' real stories",
    "path": "papers/23/01/2301.09790.json",
    "total_tokens": 978,
    "translated_title": "极大预训练语言模型是否能够在很少的样例下学习故事创作？",
    "translated_abstract": "尽管预训练语言模型可以生成语法通顺的句子用于自动生成故事，但是它们难以生成连贯、有意义和有趣的故事。当前最先进的故事生成模型通过探索更高级的特征，例如情节或常识知识以提高生成故事的质量。使用极大预训练语言模型（VLPLMs）如GPT3的提示式学习已经已经在各种自然语言处理任务中表现出惊人的性能。本文提出了一项广泛的研究，使用自动和人类评估来比较VLPLMs与那些在风格、语言和长度等方面不同的SOTA模型在三个不同数据集上的故事生成能力。我们的研究结果表明VLPLMs生成的故事质量远远高于其他故事生成模型，并在一定程度上可以与人类作者相抗衡，尽管初步调查也揭示了它们倾向于“抄袭”真实的故事。",
    "tldr": "本文探讨了使用极大预训练语言模型（VLPLMs）创作故事的可能性，并通过与SOTA模型在不同数据集上的比较，证明VLPLMs生成的故事质量更高，并展示一定程度上可与人类作者相抗衡，尽管初步调查揭示了它们倾向于“抄袭”真实的故事。",
    "en_tdlr": "This paper explores the possibility of using very large pretrained language models (VLPLMs) for storytelling, and through comparison with state-of-the-art (SOTA) models on different datasets, demonstrates that VLPLMs generate higher quality stories and can rival human authors to some extent, although preliminary investigation reveals their tendency to \"plagiarize\" real stories."
}