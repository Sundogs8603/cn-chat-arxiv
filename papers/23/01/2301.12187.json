{
    "title": "Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming. (arXiv:2301.12187v2 [cs.LG] UPDATED)",
    "abstract": "Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evalu",
    "link": "http://arxiv.org/abs/2301.12187",
    "context": "Title: Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming. (arXiv:2301.12187v2 [cs.LG] UPDATED)\nAbstract: Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evalu",
    "path": "papers/23/01/2301.12187.json",
    "total_tokens": 906,
    "translated_title": "基于两阶段动态规划的高效卷积神经网络深度压缩算法",
    "translated_abstract": "近期针对神经网络修剪的最新研究表明，通过减少网络深度而非通道剪枝来减少运行时内存使用和加速推理延迟更加有效。其中，一些最近的工作提出了合并卷积层的深度压缩算法。然而，现有的算法有一个狭窄的搜索空间，并依赖于人工设计的启发式策略。本文提出了一种针对通用卷积运算的新型深度压缩算法。我们提出了一种子集选择问题，用恒等函数替换低效激活层，并将连续的卷积操作优化地合并成浅层等效卷积操作，实现端到端的高效推理延迟。由于所提出的子集选择问题是NP-hard问题，我们提出了一个代理优化问题，可以通过两阶段动态规划在几秒钟内精确求解。",
    "tldr": "本文提出了基于两阶段动态规划的深度压缩算法，该算法能够将神经网络的深度合并成等效的浅层卷积操作，从而实现高效的推理延迟和内存占用，同时不会影响模型精度。",
    "en_tdlr": "This paper proposes a depth compression algorithm based on two-stage dynamic programming, which can merge the depth of convolutional neural networks into equivalent shallow operations and achieve efficient inference latency and memory usage without sacrificing model accuracy."
}