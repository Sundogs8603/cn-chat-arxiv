{
    "title": "Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State. (arXiv:2301.06889v2 [cs.LG] UPDATED)",
    "abstract": "Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\\mathcal{O}(e)$ where $e=\\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|} +\\sqrt{|\\mathcal{U}|}\\right]$. The size of the agent population is denoted by the term $N$, and $|\\mathcal{X}|, |\\mathcal",
    "link": "http://arxiv.org/abs/2301.06889",
    "context": "Title: Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State. (arXiv:2301.06889v2 [cs.LG] UPDATED)\nAbstract: Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\\mathcal{O}(e)$ where $e=\\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|} +\\sqrt{|\\mathcal{U}|}\\right]$. The size of the agent population is denoted by the term $N$, and $|\\mathcal{X}|, |\\mathcal",
    "path": "papers/23/01/2301.06889.json",
    "total_tokens": 1017,
    "translated_title": "基于均场控制的非可分共享全局状态下多智能体强化学习的近似方法",
    "translated_abstract": "均场控制是解决大规模多智能体强化学习问题的一种强大的近似工具。然而，均场控制的成功取决于一个假设，即在给定所有智能体的局部状态和行为的情况下，智能体的下一个（局部）状态会互相独立地演变。本文证明了即使在智能体共享一个全局状态的MARL场景中，MFC仍然可以作为一个好的近似工具，前提是智能体的局部状态仍具有条件独立性。我们假设全局状态是非可分的，即不能将它表示为智能体的局部状态的集合。我们将近似误差计算为$\\mathcal{O}(e)$，其中$e=\\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|} +\\sqrt{|\\mathcal{U}|}\\right]$，代表智能体数量的术语为$N$，$|\\mathcal{X}|, |\\mathcal{U}|$ 表示状态和动作空间的大小。",
    "tldr": "本文提出了一种基于均场控制的多智能体强化学习的近似方法，即使智能体共享一个非可分全局状态，也能具有较好的适用性和近似效果。",
    "en_tdlr": "This paper proposes a mean-field control based approximation method for multi-agent reinforcement learning, which can still be effective in the presence of a non-decomposable shared global state, as long as the local states of agents are conditionally independent. The approximation error is calculated as $\\mathcal{O}(e)$, where $e=\\frac{1}{\\sqrt{N}}\\left[\\sqrt{|\\mathcal{X}|} +\\sqrt{|\\mathcal{U}|}\\right]$."
}