{
    "title": "Graph Neural Tangent Kernel: Convergence on Large Graphs. (arXiv:2301.10808v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- , and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK. We further prove that the spectrum of the GNTK, which is related to the directions of fastest learning which becomes relevant during early stopping, converges to the spectrum of the graphon NTK. This implies that in the large-graph limit, the GNTK fitted on a graph of moderate size can be used to s",
    "link": "http://arxiv.org/abs/2301.10808",
    "context": "Title: Graph Neural Tangent Kernel: Convergence on Large Graphs. (arXiv:2301.10808v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- , and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK. We further prove that the spectrum of the GNTK, which is related to the directions of fastest learning which becomes relevant during early stopping, converges to the spectrum of the graphon NTK. This implies that in the large-graph limit, the GNTK fitted on a graph of moderate size can be used to s",
    "path": "papers/23/01/2301.10808.json",
    "total_tokens": 1000,
    "translated_title": "图神经切比雪夫核：大规模图上的收敛分析",
    "translated_abstract": "图神经网络在图机器学习任务中表现出色，但在大规模图数据上训练时往往比较困难，因为它们的学习动态难以理解。本文利用图神经切比雪夫核（GNTK）和图核函数探究大规模图神经网络的训练动态。在宽度趋于无穷大时，过参数化神经网络的优化等价于在NTK上进行核回归。我们研究了GNTK在另一个独立的维度（图大小）变化时的演化情况，并使用图核函数来定义极限对象——GNN的图核函数和GNTK的图核函数，并证明，在一系列图上，GNTK收敛于图核函数。进一步证明了GNTK的谱依赖于最快学习的方向，这在早期停止训练时变得特别重要，谱也收敛于图核函数的谱。这意味着在大规模图的限制下，对中等大小的图进行拟合的GNTK可以用于在图上进行拟合。",
    "tldr": "本文利用GNTK和图核函数探究大规模图神经网络的训练动态，证明了GNTK在收敛于图核函数时的确切性质。这意味着在大规模图的情况下，可以在中等大小的图上进行拟合并在整个图上使用。"
}