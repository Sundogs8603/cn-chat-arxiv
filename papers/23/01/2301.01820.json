{
    "title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. (arXiv:2301.01820v4 [cs.IR] UPDATED)",
    "abstract": "Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu",
    "link": "http://arxiv.org/abs/2301.01820",
    "context": "Title: InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. (arXiv:2301.01820v4 [cs.IR] UPDATED)\nAbstract: Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu",
    "path": "papers/23/01/2301.01820.json",
    "total_tokens": 912,
    "translated_title": "InPars-v2: 利用大型语言模型作为信息检索高效数据集生成器",
    "translated_abstract": "近来，InPars 提出了一种利用大型语言模型（LLMs）在信息检索任务中高效生成相关查询的方法：通过少量样本，诱导 LLM 生成与文档相关的查询，在此基础上生成合成的查询-文档对，用于训练检索器。然而，InPars 和 Promptagator 等方法依赖于 GPT-3 和 FLAN 等专有 LLMs 生成这些数据集。本文提出了 InPars-v2，该数据集生成器使用开放源代码的 LLM 和现有的强大再排序器来选择用于训练的合成查询-文档对。一个简单的 BM25 检索管道，在经过由 InPars-v2 数据微调的 monoT5 再排序器之后，便可在 BEIR 基准测试中达到最新的最好结果。为了让研究人员进一步提高我们的方法，我们开源了代码、数据和微调模型：https://github.com/zetaalphavector/inPars/tree/master/tpu。",
    "tldr": "本文提出 InPars-v2，使用开源 LLMs 和强大再排序器生成用于信息检索中训练的合成查询-文档对，可在 BEIR 基准测试中达到最新的最好结果。",
    "en_tdlr": "In this work, we propose InPars-v2, a dataset generator that uses open-source LLMs and powerful rerankers to generate synthetic query-document pairs for information retrieval training. Our method achieves state-of-the-art results on the BEIR benchmark and is open-sourced for further research."
}