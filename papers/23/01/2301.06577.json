{
    "title": "Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health. (arXiv:2301.06577v2 [cs.SE] UPDATED)",
    "abstract": "When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g. the number of closed pull requests in twelve months time). The training data for this task may be very small (e.g. five years of data, collected every month means just 60 rows of training data). The models generated from such tiny data sets can make many prediction errors.  Those errors can be tamed by a {\\em landscape analysis} that selects better learner control parameters. Our niSNEAK tool (a)~clusters the data to find the general landscape of the hyperparameters; then (b)~explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g. FLASH, HYPEROPT, OPTUNA).  The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as $C$= number of commits; $I$=number of clos",
    "link": "http://arxiv.org/abs/2301.06577",
    "context": "Title: Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health. (arXiv:2301.06577v2 [cs.SE] UPDATED)\nAbstract: When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g. the number of closed pull requests in twelve months time). The training data for this task may be very small (e.g. five years of data, collected every month means just 60 rows of training data). The models generated from such tiny data sets can make many prediction errors.  Those errors can be tamed by a {\\em landscape analysis} that selects better learner control parameters. Our niSNEAK tool (a)~clusters the data to find the general landscape of the hyperparameters; then (b)~explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g. FLASH, HYPEROPT, OPTUNA).  The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as $C$= number of commits; $I$=number of clos",
    "path": "papers/23/01/2301.06577.json",
    "total_tokens": 991,
    "translated_title": "学习少量数据：关于使用景观分析来预测软件项目健康状况的价值",
    "translated_abstract": "当数据稀缺时，软件分析可能会出现许多错误。例如，考虑学习开源项目健康状况的预测器（例如，在十二个月后的已关闭拉取请求的数量）。这个任务的训练数据可能非常少（例如，五年的数据，每个月收集一次，只有60行的训练数据）。从如此小的数据集生成的模型可能会产生许多预测错误。这些错误可以通过选择更好的学习控制参数进行“景观分析”来解决。我们的niSNEAK工具(a)~对数据进行聚类，以找到超参数的一般景观；然后(b)~从每个部分中探索几个代表性样本。niSNEAK比之前的最新超参数优化算法（例如FLASH、HYPEROPT、OPTUNA）更快且更有效。niSNEAK找到的配置错误远远少于其他方法。例如，对于项目健康指标（如$C$=提交次数；$I$=已关闭的拉取请求次数），niSNEAK的错误要少得多。",
    "tldr": "该论文探讨了在数据稀缺的情况下，利用景观分析来预测软件项目健康状况的价值。通过对数据进行聚类并选择更好的学习控制参数，通过niSNEAK工具能够比之前的方法更快且更有效地找到更准确的配置，减少了预测错误。",
    "en_tdlr": "This paper discusses the value of landscape analysis for predicting software project health when data is scarce. By clustering the data and selecting better learner control parameters, the niSNEAK tool is able to find more accurate configurations faster and more effectively than previous methods, reducing prediction errors."
}