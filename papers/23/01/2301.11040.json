{
    "title": "Random Grid Neural Processes for Parametric Partial Differential Equations. (arXiv:2301.11040v2 [cs.LG] UPDATED)",
    "abstract": "We introduce a new class of spatially stochastic physics and data informed deep latent models for parametric partial differential equations (PDEs) which operate through scalable variational neural processes. We achieve this by assigning probability measures to the spatial domain, which allows us to treat collocation grids probabilistically as random variables to be marginalised out. Adapting this spatial statistics view, we solve forward and inverse problems for parametric PDEs in a way that leads to the construction of Gaussian process models of solution fields. The implementation of these random grids poses a unique set of challenges for inverse physics informed deep learning frameworks and we propose a new architecture called Grid Invariant Convolutional Networks (GICNets) to overcome these challenges. We further show how to incorporate noisy data in a principled manner into our physics informed model to improve predictions for problems where data may be available but whose measurem",
    "link": "http://arxiv.org/abs/2301.11040",
    "context": "Title: Random Grid Neural Processes for Parametric Partial Differential Equations. (arXiv:2301.11040v2 [cs.LG] UPDATED)\nAbstract: We introduce a new class of spatially stochastic physics and data informed deep latent models for parametric partial differential equations (PDEs) which operate through scalable variational neural processes. We achieve this by assigning probability measures to the spatial domain, which allows us to treat collocation grids probabilistically as random variables to be marginalised out. Adapting this spatial statistics view, we solve forward and inverse problems for parametric PDEs in a way that leads to the construction of Gaussian process models of solution fields. The implementation of these random grids poses a unique set of challenges for inverse physics informed deep learning frameworks and we propose a new architecture called Grid Invariant Convolutional Networks (GICNets) to overcome these challenges. We further show how to incorporate noisy data in a principled manner into our physics informed model to improve predictions for problems where data may be available but whose measurem",
    "path": "papers/23/01/2301.11040.json",
    "total_tokens": 923,
    "translated_title": "面向参数化偏微分方程的随机网格神经过程",
    "translated_abstract": "我们引入了一类新的基于可扩展变分神经过程的具有空间随机物理学和数据信息的深度潜在模型，用于处理参数化偏微分方程(PDE)。我们通过将概率测度赋予空间域来实现这一目的，这使我们能够将配点网格视为随机变量来边际化。通过适应这种空间统计视图，我们以产生解场的高斯过程模型作为结果，解决参数PDE的正向和反向问题。这些随机网格的实现为反向物理信息深度学习框架提出了一系列独特的挑战，我们提出了一种名为Grid Invariant Convolutional Networks(GICNets)的新架构来克服这些挑战。我们进一步展示了如何以原则性的方式将有噪声的数据纳入我们的物理知识模型中，以改进对一些可能有可用数据但测量结果被噪声污染的问题的预测能力。",
    "tldr": "本论文引入了一种新的随机网格神经算法，用于处理参数化偏微分方程，创新地将概率测度赋予空间域，形成高斯过程模型，提供了一种解决数据受噪声干扰问题的方法。",
    "en_tdlr": "The paper proposes a novel random grid neural algorithm for handling parametric partial differential equations, which assigns probability measures to the spatial domain and leads to the construction of Gaussian process models. The paper also presents a method for incorporating noisy data in a principled manner to improve predictions."
}