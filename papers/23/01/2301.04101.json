{
    "title": "Neural Radiance Field Codebooks. (arXiv:2301.04101v2 [cs.CV] UPDATED)",
    "abstract": "Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% ",
    "link": "http://arxiv.org/abs/2301.04101",
    "context": "Title: Neural Radiance Field Codebooks. (arXiv:2301.04101v2 [cs.CV] UPDATED)\nAbstract: Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% ",
    "path": "papers/23/01/2301.04101.json",
    "total_tokens": 825,
    "translated_title": "神经辐射场代码本。",
    "translated_abstract": "世界的组合表示是实现高级场景理解和有效转移至下游任务的有希望的一步。为了实现这个目标，我们通过新颖的视图重构方法引入了神经辐射场代码本(NRC)，这是一种学习对象为中心的表示的可扩展方法。NRC学习使用对象编码字典通过体积渲染器解码以从新视图重建场景。这使得能够发现在场景中反复出现的视觉和几何模式，并且可转移到下游任务。我们展示了NRC表示在THOR中的对象导航中的良好转移能力，成功率比2D和3D表示学习方法高出3.1%。我们证明了我们的方法能够比之前的方法更好地执行更复杂的合成（THOR）和真实场景（NYU Depth）的无监督分割(29%)。",
    "tldr": "某项研究提出了一种新的方法 NRC，能够学习复杂场景、实现对象为中心的表示、并能够自动分割，在THOR中的对象导航中表现出良好的转移能力。",
    "en_tdlr": "The Neural Radiance Field Codebooks is a scalable method for learning object-centric representations and transferable patterns across scenes, achieving better performance in object navigation and unsupervised segmentation than previous methods."
}