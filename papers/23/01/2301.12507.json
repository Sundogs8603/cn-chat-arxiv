{
    "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents. (arXiv:2301.12507v2 [cs.AI] UPDATED)",
    "abstract": "Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquir",
    "link": "http://arxiv.org/abs/2301.12507",
    "context": "Title: Distilling Internet-Scale Vision-Language Models into Embodied Agents. (arXiv:2301.12507v2 [cs.AI] UPDATED)\nAbstract: Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquir",
    "path": "papers/23/01/2301.12507.json",
    "total_tokens": 952,
    "translated_title": "将互联网规模的视觉语言模型转化为具体代理",
    "translated_abstract": "指令跟踪代理必须将语言接地到其观察和操作空间中。学习接地语言具有挑战性，通常需要特定领域的工程或大量人类交互数据。为解决这个问题，我们建议使用预训练的视觉语言模型 (VLMs) 来监督具体代理。我们结合了模型蒸馏和事后经验回放 (HER) 的想法，使用 VLM 来追溯性地生成描述代理行为的语言。简单的提示允许我们控制监督信号，教代理根据其名称 (例如，飞机) 或其特征 (例如，颜色) 与新型物体交互在3D渲染环境中。Fewshot提示使我们可以教授抽象的类别成员资格，包括现有类别（食品 vs 玩具）和自发类别（对对象的任意偏好）。我们的工作概述了一种新的有效方法，使用互联网规模的VLMs，重新利用它们从大量文本数据中学习的通用语言接地技能，将语言接地到具体代理的行动中。",
    "tldr": "该研究提出了使用预训练的视觉语言模型(VLMs)来监督具体代理，将互联网规模的VLMs的通用语言接地技能重新利用到具体代理的行动中。",
    "en_tdlr": "This research proposes using pre-trained vision-language models to supervise embodied agents, repurposing the generic language grounding skills learned from massive amounts of text data to the specific task of grounding language into action for embodied agents."
}