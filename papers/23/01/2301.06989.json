{
    "title": "Negative Flux Aggregation to Estimate Feature Attributions. (arXiv:2301.06989v2 [cs.LG] UPDATED)",
    "abstract": "There are increasing demands for understanding deep neural networks' (DNNs) behavior spurred by growing security and/or transparency concerns. Due to multi-layer nonlinearity of the deep neural network architectures, explaining DNN predictions still remains as an open problem, preventing us from gaining a deeper understanding of the mechanisms. To enhance the explainability of DNNs, we estimate the input feature's attributions to the prediction task using divergence and flux. Inspired by the divergence theorem in vector analysis, we develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient approximation algorithm to estimate attribution map. Unlike the previous techniques, ours doesn't rely on fitting a surrogate model nor need any path integration of gradients. Both qualitative and quantitative experiments demonstrate a superior performance of NeFLAG in generating more faithful attribution maps than the competing methods. Our code is available at \\url{https://git",
    "link": "http://arxiv.org/abs/2301.06989",
    "context": "Title: Negative Flux Aggregation to Estimate Feature Attributions. (arXiv:2301.06989v2 [cs.LG] UPDATED)\nAbstract: There are increasing demands for understanding deep neural networks' (DNNs) behavior spurred by growing security and/or transparency concerns. Due to multi-layer nonlinearity of the deep neural network architectures, explaining DNN predictions still remains as an open problem, preventing us from gaining a deeper understanding of the mechanisms. To enhance the explainability of DNNs, we estimate the input feature's attributions to the prediction task using divergence and flux. Inspired by the divergence theorem in vector analysis, we develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient approximation algorithm to estimate attribution map. Unlike the previous techniques, ours doesn't rely on fitting a surrogate model nor need any path integration of gradients. Both qualitative and quantitative experiments demonstrate a superior performance of NeFLAG in generating more faithful attribution maps than the competing methods. Our code is available at \\url{https://git",
    "path": "papers/23/01/2301.06989.json",
    "total_tokens": 862,
    "translated_title": "用负通量聚合估计特征归因",
    "translated_abstract": "由于增长中的安全和/或透明度问题，对于理解深度神经网络(DNN)行为的需求不断增加。 由于深度神经网络架构的多层非线性，解释DNN预测仍然是一个未解决的问题，这阻碍了我们深入了解机制。为了增强DNN的可解释性，我们使用离散和通量来估计预测任务的输入特征的归因。受矢量分析中的散度定理启发，我们开发了一种新的负通量聚合（NeFLAG）公式和有效的近似算法来估计归因图。与先前的技术不同，我们的方法不依赖于拟合替代模型，也不需要梯度的路径积分。定性和定量实验都证明了NeFLAG在生成更准确的归因图方面优于竞争方法。我们的代码可在 \\url{https://git",
    "tldr": "该论文提出了一种新的算法，名为负通量聚合（NeFLAG），可用于估计深度神经网络中输入特征对预测的影响，该方法不需要拟合替代模型或路径积分梯度。",
    "en_tdlr": "The paper proposes a novel algorithm, named Negative Flux Aggregation (NeFLAG), for estimating the impact of input features on predictions in deep neural networks. Unlike previous methods, this approach does not require fitting surrogate models or path integration of gradients."
}