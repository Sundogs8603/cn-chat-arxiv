{
    "title": "Optimal Decision Tree Policies for Markov Decision Processes",
    "abstract": "arXiv:2301.13185v2 Announce Type: replace Abstract: Interpretability of reinforcement learning policies is essential for many real-world tasks but learning such interpretable policies is a hard problem. Particularly rule-based policies such as decision trees and rules lists are difficult to optimize due to their non-differentiability. While existing techniques can learn verifiable decision tree policies there is no guarantee that the learners generate a decision that performs optimally. In this work, we study the optimization of size-limited decision trees for Markov Decision Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a user-defined size limit and MDP formulation OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimall",
    "link": "https://arxiv.org/abs/2301.13185",
    "context": "Title: Optimal Decision Tree Policies for Markov Decision Processes\nAbstract: arXiv:2301.13185v2 Announce Type: replace Abstract: Interpretability of reinforcement learning policies is essential for many real-world tasks but learning such interpretable policies is a hard problem. Particularly rule-based policies such as decision trees and rules lists are difficult to optimize due to their non-differentiability. While existing techniques can learn verifiable decision tree policies there is no guarantee that the learners generate a decision that performs optimally. In this work, we study the optimization of size-limited decision trees for Markov Decision Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a user-defined size limit and MDP formulation OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimall",
    "path": "papers/23/01/2301.13185.json",
    "total_tokens": 970,
    "translated_title": "Markov决策过程的最优决策树策略",
    "translated_abstract": "强化学习策略的解释性对于许多实际任务至关重要，但学习这种可解释的策略是一个困难的问题。特别是像决策树和规则列表这样的基于规则的策略，由于其不可微性，很难进行优化。尽管现有技术可以学习可验证的决策树策略，但不能保证学习者生成的决策是最优的。在这项工作中，我们研究了有限大小决策树在Markov决策过程（MDPs）中的优化，并提出了OMDTs：最优MDP决策树。给定用户定义的大小限制和MDP形式，OMDT通过混合整数线性规划直接最大化决策树的期望折扣回报。通过为不同的MDP训练最优决策树策略，我们在实证上研究了现有模仿学习技术的最优性差距，并发现它们表现为次优。",
    "tldr": "本研究研究了有限大小决策树在Markov决策过程（MDPs）中的优化，并提出了OMDTs：最优MDP决策树。通过混合整数线性规划直接最大化决策树的期望折扣回报。研究发现现有模仿学习技术的最优性差距，并发现它们表现为次优。",
    "en_tdlr": "This study investigates the optimization of size-limited decision trees for Markov Decision Processes (MDPs) and proposes OMDTs: Optimal MDP Decision Trees. OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. The research finds the optimality gap for existing imitation learning techniques and reveals their sub-optimality."
}