{
    "title": "Replug: Retrieval-augmented black-box language models. (arXiv:2301.12652v3 [cs.CL] UPDATED)",
    "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",
    "link": "http://arxiv.org/abs/2301.12652",
    "context": "Title: Replug: Retrieval-augmented black-box language models. (arXiv:2301.12652v3 [cs.CL] UPDATED)\nAbstract: We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",
    "path": "papers/23/01/2301.12652.json",
    "total_tokens": 827,
    "translated_title": "Replug：检索增强的黑盒语言模型",
    "translated_abstract": "我们介绍了 REPLUG，一种检索增强的语言建模框架，将语言模型（LM）视为黑盒，并用可调整的检索模型增强它。与以前通过特殊的交叉关注机制对语言模型进行训练以对检索文本进行编码的检索增强型LM不同，REPLUG仅将检索到的文档前置到冻结的黑盒LM输入中。这种简单的设计可以轻松地应用于任何现有的检索和语言模型。此外，我们显示LM可以用于监督检索模型，该检索模型可以找到帮助LM进行更好预测的文档。我们的实验证明，在调整了检索器的情况下，REPLUG可以使GPT-3（175B）的语言建模性能提高6.3％，同时使Codex在五次测试MMLU时性能提高5.1％。",
    "tldr": "REPLUG是一种检索增强的语言建模框架，它通过在黑盒语言模型输入前添加检索到的文档来增强模型，同时可以通过LM监督检索模型来提高预测性能。",
    "en_tdlr": "REPLUG is a retrieval-augmented language modeling framework that enhances the black-box language model by prepending retrieved documents to its input, and can improve prediction performance by supervising retrieval model with LM."
}