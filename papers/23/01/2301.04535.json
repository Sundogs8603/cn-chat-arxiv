{
    "title": "Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings. (arXiv:2301.04535v2 [cs.CL] UPDATED)",
    "abstract": "Despite the increasing popularity of the stance detection task, existing approaches are predominantly limited to using the textual content of social media posts for the classification, overlooking the social nature of the task. The stance detection task becomes particularly challenging in cross-target classification scenarios, where even in few-shot training settings the model needs to predict the stance towards new targets for which the model has only seen few relevant samples during training. To address the cross-target stance detection in social media by leveraging the social nature of the task, we introduce CT-TN, a novel model that aggregates multimodal embeddings derived from both textual and network features of the data. We conduct experiments in a few-shot cross-target scenario on six different combinations of source-destination target pairs. By comparing CT-TN with state-of-the-art cross-target stance detection models, we demonstrate the effectiveness of our model by achieving",
    "link": "http://arxiv.org/abs/2301.04535",
    "context": "Title: Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings. (arXiv:2301.04535v2 [cs.CL] UPDATED)\nAbstract: Despite the increasing popularity of the stance detection task, existing approaches are predominantly limited to using the textual content of social media posts for the classification, overlooking the social nature of the task. The stance detection task becomes particularly challenging in cross-target classification scenarios, where even in few-shot training settings the model needs to predict the stance towards new targets for which the model has only seen few relevant samples during training. To address the cross-target stance detection in social media by leveraging the social nature of the task, we introduce CT-TN, a novel model that aggregates multimodal embeddings derived from both textual and network features of the data. We conduct experiments in a few-shot cross-target scenario on six different combinations of source-destination target pairs. By comparing CT-TN with state-of-the-art cross-target stance detection models, we demonstrate the effectiveness of our model by achieving",
    "path": "papers/23/01/2301.04535.json",
    "total_tokens": 924,
    "translated_title": "融合多模态嵌入的跨目标立场检测的小样本学习",
    "translated_abstract": "尽管立场检测任务越来越受欢迎，但现有方法主要限于使用社交媒体帖子的文本内容进行分类，忽略了任务的社交性质。在跨目标分类场景中，立场检测任务变得尤其具有挑战性，即使在少样本训练设置中，模型也需要预测对于其在训练期间仅看到少量相关示例的新目标的立场。为了利用任务的社交性质解决社交媒体中的跨目标立场检测问题，我们引入了CT-TN，这是一种新的模型，它聚合了数据的文本和网络特征派生的多模态嵌入。我们在六种不同的源-目标目标对的少样本跨目标场景下进行实验。通过将CT-TN与最先进的跨目标立场检测模型进行比较，我们证明了CT-TN模型的有效性。",
    "tldr": "本文提出了一种称为CT-TN的模型，在社交媒体中进行跨目标立场检测，利用了任务的社交性质，通过聚合多模态嵌入来解决少样本情境下的问题。实验表明，该模型在六个不同的源-目标目标对上比最先进的跨目标立场检测模型表现更好。",
    "en_tdlr": "This paper proposes a novel few-shot learning model named CT-TN for cross-target stance detection in social media, by leveraging the social nature of the task and aggregating multimodal embeddings. The model shows effectiveness in six different source-destination target pairs and outperforms state-of-the-art models."
}