{
    "title": "Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)",
    "abstract": "To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym",
    "link": "http://arxiv.org/abs/2301.04746",
    "context": "Title: Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)\nAbstract: To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym",
    "path": "papers/23/01/2301.04746.json",
    "total_tokens": 1014,
    "translated_title": "可切换轻量级反对称处理（SLAP）在 Gomoku 强化学习中使用 CNN 比数据增强更快",
    "translated_abstract": "本文提出了一种名为 SLAP 的方法，用于加强经验以加速机器学习并减少样本大小，以代替数据增强。SLAP是一种模型无关的协议/函数，可以产生相同的输出，但给予不同的变换变量。在Gomoku游戏状态的实验中，SLAP提高了卷积神经网络学习的收敛速度达83％，样本大小只有数据增强的1/8。在Gomoku强化学习中，使用AlphaGo Zero / AlphaZero算法和数据增强作为基线，SLAP将训练样本数量减少了8倍，并在与相同评估器对比时实现了类似的获胜率，但尚不能证明它可以加速强化学习。这些益处至少应适用于对称或特定变换不变的领域。作为未来的工作，SLAP可能有助于更可解释的学习以及不适用于对称的领域的转移学习。",
    "tldr": "本论文提出了一种名为SLAP的方法，它可以替代数据增强，加强经验以加速机器学习并减少样本大小。 在Gomoku游戏和强化学习领域的实验中，证明了SLAP的有效性，可以提高模型的收敛速度，同时减少了样本数量。这种策略至少适用于对称或特定变换不变的领域。",
    "en_tdlr": "This paper proposes a method called SLAP to replace data augmentation, which intensifies experience to speed up machine learning and reduce the sample size. The experiments in the Gomoku game and reinforcement learning domains prove the effectiveness of SLAP by improving the convergence speed of the model and reducing the sample size. This strategy is at least applicable to domains that are invariant to symmetry or certain transformations."
}