{
    "title": "Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. (arXiv:2301.00051v2 [cs.LG] UPDATED)",
    "abstract": "Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires effective exploration during an online reinforcement learning phase. In this work, we show that the standard, naive approach to exploration can manifest as a suboptimal local maximum if a policy learned with AIL sufficiently matches the expert distribution without fully learning the desired task. This can be particularly catastrophic for manipulation tasks, where the difference between an expert and a non-expert state-action pair is often subtle. We present Learning from Guided Play (LfGP), a framework in which we leverage expert demonstrations of multiple exploratory, auxiliary tasks in addition to a main task. The addition of these auxiliary tasks forces the agent to explore states and actions that standard AIL may learn to ignore. Additionally, this particular formulation allows for the reusability of",
    "link": "http://arxiv.org/abs/2301.00051",
    "context": "Title: Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. (arXiv:2301.00051v2 [cs.LG] UPDATED)\nAbstract: Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires effective exploration during an online reinforcement learning phase. In this work, we show that the standard, naive approach to exploration can manifest as a suboptimal local maximum if a policy learned with AIL sufficiently matches the expert distribution without fully learning the desired task. This can be particularly catastrophic for manipulation tasks, where the difference between an expert and a non-expert state-action pair is often subtle. We present Learning from Guided Play (LfGP), a framework in which we leverage expert demonstrations of multiple exploratory, auxiliary tasks in addition to a main task. The addition of these auxiliary tasks forces the agent to explore states and actions that standard AIL may learn to ignore. Additionally, this particular formulation allows for the reusability of",
    "path": "papers/23/01/2301.00051.json",
    "total_tokens": 937,
    "translated_title": "从引导式游戏学习中提升对抗性模仿学习中的探索能力",
    "translated_abstract": "对抗性模仿学习（AIL）已成为减少监督模仿学习的分布偏移的流行替代方法。然而，在在线强化学习阶段，AIL需要有效的探索。在这项工作中，我们展示了标准的、天真的探索方法，如果使用AIL学习的策略与专家分布足够匹配但没有完全学会所需的任务，可能会表现为一个次优的局部最大值。这对于操作任务来说可能尤为灾难，因为专家和非专家的状态-动作对之间的差别通常是微妙的。我们提出了从引导游戏中学习（LfGP）的框架，其中我们利用了多个探索性辅助任务的专家演示，除了一个主任务。这些辅助任务的添加强制代理人探索标准AIL可能学会忽视的状态和动作。此外，这种特定的公式允许辅助任务的可重复使用性",
    "tldr": "本研究提出了从引导式游戏学习（LfGP）的框架，通过引入多个辅助任务和主任务的专家演示，提升对抗性模仿学习（AIL）中的探索能力，并解决了传统AIL在学习操作任务时可能陷入次优解的问题。",
    "en_tdlr": "This paper introduces the Learning from Guided Play (LfGP) framework, which improves exploration for Adversarial Imitation Learning (AIL) by utilizing expert demonstrations of multiple auxiliary tasks alongside a main task, addressing the issue of suboptimal solutions that traditional AIL may encounter in learning manipulation tasks."
}