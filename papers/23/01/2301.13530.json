{
    "title": "Domain-Generalizable Multiple-Domain Clustering",
    "abstract": "This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.",
    "link": "https://arxiv.org/abs/2301.13530",
    "context": "Title: Domain-Generalizable Multiple-Domain Clustering\nAbstract: This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.",
    "path": "papers/23/01/2301.13530.json",
    "total_tokens": 867,
    "translated_title": "通用多领域聚类问题的解决方法",
    "translated_abstract": "本研究将无监督领域通用化问题推广到无标签样本的情况（完全无监督）。我们获得了来自多个源领域的未标记样本，并旨在学习一个共享的预测器，将示例分配到语义相关的聚类中。通过在以前未见的领域中预测聚类分配来进行评估。为实现这一目标，我们提出了一个两阶段的训练框架：（1）自助预训练用于提取领域不变的语义特征。（2）使用伪标签的多头聚类预测，该伪标签依赖于特征空间和聚类头预测，进一步利用了一种新颖的基于预测的标签平滑方案。我们的实验证明，与需要使用目标领域样本进行微调或某种程度的监督的基线模型相比，我们的模型更准确。我们的代码可在https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering找到。",
    "tldr": "本研究解决了无监督领域通用化问题，并提出了一个两阶段的训练框架，该框架使用自助预训练和伪标签的多头聚类预测来提高准确性。",
    "en_tdlr": "This work addresses the problem of unsupervised domain generalization and introduces a two-stage training framework that improves accuracy through self-supervised pre-training and multi-head cluster prediction with pseudo labels."
}