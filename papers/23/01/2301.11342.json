{
    "title": "A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks. (arXiv:2301.11342v2 [cs.LG] UPDATED)",
    "abstract": "Counterexample-guided repair aims at creating neural networks with mathematical safety guarantees, facilitating the application of neural networks in safety-critical domains. However, whether counterexample-guided repair is guaranteed to terminate remains an open question. We approach this question by showing that counterexample-guided repair can be viewed as a robust optimisation algorithm. While termination guarantees for neural network repair itself remain beyond our reach, we prove termination for more restrained machine learning models and disprove termination in a general setting. We empirically study the practical implications of our theoretical results, demonstrating the suitability of common verifiers and falsifiers for repair despite a disadvantageous theoretical result. Additionally, we use our theoretical insights to devise a novel algorithm for repairing linear regression models based on quadratic programming, surpassing existing approaches.",
    "link": "http://arxiv.org/abs/2301.11342",
    "context": "Title: A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks. (arXiv:2301.11342v2 [cs.LG] UPDATED)\nAbstract: Counterexample-guided repair aims at creating neural networks with mathematical safety guarantees, facilitating the application of neural networks in safety-critical domains. However, whether counterexample-guided repair is guaranteed to terminate remains an open question. We approach this question by showing that counterexample-guided repair can be viewed as a robust optimisation algorithm. While termination guarantees for neural network repair itself remain beyond our reach, we prove termination for more restrained machine learning models and disprove termination in a general setting. We empirically study the practical implications of our theoretical results, demonstrating the suitability of common verifiers and falsifiers for repair despite a disadvantageous theoretical result. Additionally, we use our theoretical insights to devise a novel algorithm for repairing linear regression models based on quadratic programming, surpassing existing approaches.",
    "path": "papers/23/01/2301.11342.json",
    "total_tokens": 822,
    "translated_title": "对神经网络反例引导修复的鲁棒优化视角",
    "translated_abstract": "反例引导修复旨在创建具有数学安全性保证的神经网络，以便在安全关键领域应用神经网络。然而，反例引导修复是否保证终止仍然是一个未解之谜。我们通过表明反例引导修复可以被视为鲁棒优化算法来解决这个问题。虽然神经网络修复本身的终止保证仍然超出了我们的能力范围，但是我们证明了更受限制的机器学习模型的终止性，并证明了一般情况下的不终止性。我们通过实验证明了我们理论结果的实际影响，证明了常见验证器和违背者对修复的适用性，尽管理论结果具有不利因素。此外，我们使用我们的理论洞见设计了一种基于二次规划的线性回归模型修复新算法，超越了现有方法。",
    "tldr": "本研究从鲁棒优化角度解决了神经网络反例引导修复是否保证终止的问题，并提出一种基于二次规划的线性回归模型修复新算法。",
    "en_tdlr": "This study provides a robust optimization perspective on the termination problem of counterexample-guided repair for neural networks, and proposes a novel algorithm based on quadratic programming for repairing linear regression models."
}