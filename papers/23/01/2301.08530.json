{
    "title": "Self-Organization Towards $1/f$ Noise in Deep Neural Networks",
    "abstract": "arXiv:2301.08530v2 Announce Type: replace-cross  Abstract: The presence of $1/f$ noise, also known as pink noise, is a well-established phenomenon in biological neural networks, and is thought to play an important role in information processing in the brain. In this study, we find that such $1/f$ noise is also found in deep neural networks trained on natural language, resembling that of their biological counterparts. Specifically, we trained Long Short-Term Memory (LSTM) networks on the `IMDb' AI benchmark dataset, then measured the neuron activations. The detrended fluctuation analysis (DFA) on the time series of the different neurons demonstrate clear $1/f$ patterns, which is absent in the time series of the inputs to the LSTM. Interestingly, when the neural network is at overcapacity, having more than enough neurons to achieve the learning task, the activation patterns deviate from $1/f$ noise and shifts towards white noise. This is because many of the neurons are not effectively us",
    "link": "https://arxiv.org/abs/2301.08530",
    "context": "Title: Self-Organization Towards $1/f$ Noise in Deep Neural Networks\nAbstract: arXiv:2301.08530v2 Announce Type: replace-cross  Abstract: The presence of $1/f$ noise, also known as pink noise, is a well-established phenomenon in biological neural networks, and is thought to play an important role in information processing in the brain. In this study, we find that such $1/f$ noise is also found in deep neural networks trained on natural language, resembling that of their biological counterparts. Specifically, we trained Long Short-Term Memory (LSTM) networks on the `IMDb' AI benchmark dataset, then measured the neuron activations. The detrended fluctuation analysis (DFA) on the time series of the different neurons demonstrate clear $1/f$ patterns, which is absent in the time series of the inputs to the LSTM. Interestingly, when the neural network is at overcapacity, having more than enough neurons to achieve the learning task, the activation patterns deviate from $1/f$ noise and shifts towards white noise. This is because many of the neurons are not effectively us",
    "path": "papers/23/01/2301.08530.json",
    "total_tokens": 887,
    "translated_title": "深度神经网络中自组织产生1/f噪声",
    "translated_abstract": "$1/f$噪声，也称为粉红噪声，是生物神经网络中一个被广泛认可的现象，被认为在大脑信息处理中起着重要作用。本研究发现，在训练自然语言的深度神经网络中，也存在类似生物网络的1/f噪声。具体来说，我们在'IMDb' AI基准数据集上训练了长短时记忆（LSTM）网络，然后测量了神经元的激活。对不同神经元时间序列进行去趋势波动分析（DFA）表明明显的1/f模式，而在LSTM输入的时间序列中却不存在这种模式。有趣的是，当神经网络处于过度容量状态，拥有足够多的神经元来完成学习任务时，激活模式将偏离1/f噪声并转向白噪声。这是因为许多神经元未能有效地利用。",
    "tldr": "本研究发现在深度神经网络中训练的模型会产生类似生物神经网络的1/f噪声，但当神经网络过度容量时，激活模式会转向白噪声。",
    "en_tdlr": "This study reveals the generation of $1/f$ noise resembling that of biological neural networks in deep neural networks trained on natural language, with a shift towards white noise in overcapacity neural networks."
}