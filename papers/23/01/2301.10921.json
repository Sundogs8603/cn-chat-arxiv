{
    "title": "SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. (arXiv:2301.10921v2 [cs.LG] UPDATED)",
    "abstract": "The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and im",
    "link": "http://arxiv.org/abs/2301.10921",
    "context": "Title: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. (arXiv:2301.10921v2 [cs.LG] UPDATED)\nAbstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and im",
    "path": "papers/23/01/2301.10921.json",
    "total_tokens": 914,
    "translated_title": "SoftMatch：解决半监督学习中的数量-质量权衡问题",
    "translated_abstract": "半监督学习的关键挑战是如何有效利用有限的标注数据和大量的未标注数据提高模型的泛化性能。本文首先通过统一的样本加权公式重新审视了流行的伪标签方法，并演示了伪标签阈值法固有的数量-质量权衡问题，可能会阻碍学习。为此，我们提出了SoftMatch，通过在训练期间保持伪标签的高数量和高质量来克服这种权衡，有效地利用未标注的数据。我们推导出一个截断的高斯函数来根据样本的置信度对样本进行加权，这可以看作是置信度阈值的软版本。我们进一步通过提出统一的对齐方法来增强对弱学习类的利用。在实验中，SoftMatch在包括图像、文本和影片等各种基准测试中显示出了实质性的改进。",
    "tldr": "本文提出了SoftMatch，通过在训练中保持高数量和高质量的伪标签来克服半监督学习中数量-质量权衡问题，有效地利用未标注的数据。 在实验中，SoftMatch在图像、文本和影片等多个基准测试中都显示了实质性的改进。",
    "en_tdlr": "This paper proposes SoftMatch to address the quantity-quality trade-off problem in semi-supervised learning by maintaining high quantity and high quality of pseudo-labels during training using a truncated Gaussian function for sample weighting and a uniform alignment approach to enhance the utilization of weakly-learned classes. SoftMatch shows significant improvements across various benchmarks in experiments."
}