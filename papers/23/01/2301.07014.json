{
    "title": "Dataset Distillation: A Comprehensive Review. (arXiv:2301.07014v3 [cs.LG] UPDATED)",
    "abstract": "Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks.Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training \\emph{per se} yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation~(DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework foll",
    "link": "http://arxiv.org/abs/2301.07014",
    "context": "Title: Dataset Distillation: A Comprehensive Review. (arXiv:2301.07014v3 [cs.LG] UPDATED)\nAbstract: Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks.Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training \\emph{per se} yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation~(DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework foll",
    "path": "papers/23/01/2301.07014.json",
    "total_tokens": 828,
    "translated_title": "数据集精简：综述",
    "translated_abstract": "深度学习的成功主要归功于大量用于训练深度神经网络的数据。然而，庞大的数据量增加了存储和传输的负担，并进一步导致了繁琐的模型训练过程。另外，依靠原始数据进行训练存在隐私和版权方面的问题。为了解决这些问题，数据集精简（DD）被引入，并在学术界引起了广泛关注。DD旨在从原始数据集中提取出包含合成样本的较小数据集，基于该数据集训练的模型性能可与在原始数据集上训练的模型相媲美。本文对DD及其应用的最新进展进行了全面的回顾和总结。",
    "tldr": "数据集精简（DD）是一种从原始数据集中提取出包含合成样本的较小数据集的方法，在减轻数据存储和传输负担的同时保证训练出的模型性能与在原始数据集上训练的模型相媲美。本文综述了DD及其应用的最新进展。",
    "en_tdlr": "Dataset distillation (DD) is a method that extracts a smaller dataset containing synthetic samples from the original dataset, while ensuring that the trained models have performance comparable to those trained on the original dataset. This paper provides a comprehensive review of recent advances in DD and its applications."
}