{
    "title": "RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)",
    "abstract": "Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c",
    "link": "http://arxiv.org/abs/2301.10222",
    "context": "Title: RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)\nAbstract: Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c",
    "path": "papers/23/01/2301.10222.json",
    "total_tokens": 968,
    "translated_title": "RangeViT：面向自动驾驶中的三维语义分割的视觉Transformer",
    "translated_abstract": "将室外LiDAR点云的语义分割视为二维问题（例如通过距离投影），这是一种有效和流行的方法。这些基于投影的方法通常受益于快速计算，并且与使用其他点云表示的技术相结合，可以实现最先进的结果。目前，投影方法利用2D CNNs，但计算机视觉的最新进展表明，视觉Transformer（ViTs）在许多基于图像的基准测试中已经取得了最先进的成果。在这项工作中，我们质疑是否可以通过ViTs的最新改进来改进三维语义分割的投影方法。我们回答是肯定的，但只有在结合了三个关键因素之后才能实现：（a）ViTs难以训练，并且需要大量的训练数据来学习强大的表示。通过保留与RGB图像相同的骨干结构，我们可以利用对大图像集合的长时间训练的知识，这些集合比相应的点云数据集要小得多。",
    "tldr": "本文旨在探究视觉Transformer是否可以应用于自动驾驶中的3D语义分割中，通过保留与RGB图像相同的骨干结构，这项工作证明了ViTs在结合投影方法，大数据训练和具有噪声鲁棒性的新损失函数后可以取得最先进的结果。",
    "en_tdlr": "This paper investigates whether vision transformers can be applied to 3D semantic segmentation in autonomous driving. By preserving the same backbone architecture as for RGB images, this work demonstrates ViTs can achieve state-of-the-art results when combined with projection-based methods, large-scale training, and a new loss function with noise robustness."
}