{
    "title": "Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)",
    "abstract": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Furth",
    "link": "http://arxiv.org/abs/2301.08721",
    "context": "Title: Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)\nAbstract: Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Furth",
    "path": "papers/23/01/2301.08721.json",
    "total_tokens": 1039,
    "translated_title": "批量提示：使用大型语言模型API进行高效推断",
    "translated_abstract": "在工业和实际应用中，使用大型语言模型（LLM）进行大量样本的推断可能会在计算和财务上代价高昂。我们提出了批量提示的简单而有效的提示方法，使LLM能够批量进行推断，而不是逐个样本。我们的方法减少了令牌和时间成本，同时保持了下游性能。我们从理论上证明，在少样本上下文学习的情况下，随着每批样本数量的增加，推断成本几乎以倒数线性关系降低。我们在常识问答、算术推理和NLI/NLU等十个数据集上广泛验证了批量提示的有效性：批量提示显著（每批六个样本时最高可减少5倍）降低了LLM（Codex）的推断令牌和时间成本，同时实现了更好或可比较的性能。对于最先进的基于聊天的LLM，例如GPT-3.5和GPT-4，我们还展示了批量提示的好处。",
    "tldr": "批量提示是一种简单但有效的方法，可以降低使用大型语言模型进行推断的计算和财务成本，同时保持下游性能。理论上证明，在少样本情况下，批量样本数量的增加几乎以倒数线性关系降低了推断成本。在多个数据集上的验证实验证明了批量提示的有效性，并且对于最先进的Chat-based LLMs，如GPT-3.5和GPT-4，批量提示也具有好处。",
    "en_tdlr": "Batch prompting is a simple yet effective method that reduces the computational and financial costs of using large language models for inference while maintaining performance. Theoretical analysis shows that increasing the number of samples in each batch decreases inference costs almost inversely linearly under a few-shot learning setting. Extensive validation on multiple datasets demonstrates the effectiveness of batch prompting, and the benefits also apply to state-of-the-art Chat-based LLMs like GPT-3.5 and GPT-4."
}