{
    "title": "Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v3 [cs.CL] UPDATED)",
    "abstract": "Word embedding has become ubiquitous and is widely used in various text mining and natural language processing (NLP) tasks, such as information retrieval, semantic analysis, and machine translation, among many others. Unfortunately, it is prohibitively expensive to train the word embedding in a relatively large corpus. We propose a graph-based word embedding algorithm, called Word-Graph2vec, which converts the large corpus into a word co-occurrence graph, then takes the word sequence samples from this graph by randomly traveling and trains the word embedding on this sampling corpus in the end. We posit that because of the stable vocabulary, relative idioms, and fixed expressions in English, the size and density of the word co-occurrence graph change slightly with the increase in the training corpus. So that Word-Graph2vec has stable runtime on the large scale data set, and its performance advantage becomes more and more obvious with the growth of the training corpus. Extensive experime",
    "link": "http://arxiv.org/abs/2301.04312",
    "raw_ret": "import requests\nimport json\n\n# 输入信息\ntitle = \"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v3 [cs.CL] UPDATED)\"\nabstract = \"Word embedding has become ubiquitous and is widely used in various text mining and natural language processing (NLP) tasks, such as information retrieval, semantic analysis, and machine translation, among many others. Unfortunately, it is prohibitively expensive to train the word embedding in a relatively large corpus. We propose a graph-based word embedding algorithm, called Word-Graph2vec, which converts the large corpus into a word co-occurrence graph, then takes the word sequence samples from this graph by randomly traveling and trains the word embedding on this sampling corpus in the end. We posit that because of the stable vocabulary, relative idioms, and fixed expressions in English, the size and density of the word co-occurrence graph change slightly with the increase in the training corpus. So that Word-Graph2vec has stable runtime on the large scale data set, and its performance advantage becomes more and more obvious with the growth of the training corpus. Extensive experime\"\n\n# 使用有道翻译API将英文翻译为中文\ndef translate(text):\n    url = \"http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n    data = {\n        \"i\": text,\n        \"from\": \"AUTO\",\n        \"to\": \"AUTO\",\n        \"smartresult\": \"dict\",\n        \"client\": \"fanyideskweb\",\n        \"doctype\": \"json\",\n        \"version\": \"2.1\",\n        \"keyfrom\": \"fanyi.web\",\n        \"action\": \"FY_BY_REALTIME\",\n        \"typoResult\": \"false\"\n    }\n    response = requests.post(url, headers=headers, data=data)\n    result = json.loads(response.text)\n    return result[\"translateResult\"][0][0][\"tgt\"]\n\n# 使用上面的函数翻译标题和摘要\ntranslated_title = translate(title)\ntranslated_abstract = translate(abstract)\n\n# 总结出论文的主要创新和贡献\ntldr = \"提出了一种基于图的词嵌入算法Word-Graph2vec，该算法将大规模语料库转化为词共现图，并通过随机游走取样训练词嵌入，解决了大规模数据集中训练词嵌入的性能问题，并得到了验证。\"\n\n# 输出JSON格式的答案\nprint(json.dumps({\"translated_title\": translated_title, \"translated_abstract\": translated_abstract, \"tldr\": tldr}))<|im_sep|>",
    "total_tokens": 1127
}