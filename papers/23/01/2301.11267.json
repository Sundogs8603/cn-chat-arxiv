{
    "title": "Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)",
    "abstract": "This paper studies online convex optimization with stochastic constraints. We propose a variant of the drift-plus-penalty algorithm that guarantees $O(\\sqrt{T})$ expected regret and zero constraint violation, after a fixed number of iterations, which improves the vanilla drift-plus-penalty method with $O(\\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of the time horizon $T$, in contrast to the vanilla drift-plus-penalty method. This is based on our novel drift lemma that provides time-varying bounds on the virtual queue drift and, as a result, leads to time-varying bounds on the expected virtual queue length. Moreover, we extend our framework to stochastic-constrained online convex optimization under two-point bandit feedback. We show that by adapting our algorithmic framework to the bandit feedback setting, we may still achieve $O(\\sqrt{T})$ expected regret and zero constraint violation, improving upon the previous work for the case of identical constraint f",
    "link": "http://arxiv.org/abs/2301.11267",
    "context": "Title: Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)\nAbstract: This paper studies online convex optimization with stochastic constraints. We propose a variant of the drift-plus-penalty algorithm that guarantees $O(\\sqrt{T})$ expected regret and zero constraint violation, after a fixed number of iterations, which improves the vanilla drift-plus-penalty method with $O(\\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of the time horizon $T$, in contrast to the vanilla drift-plus-penalty method. This is based on our novel drift lemma that provides time-varying bounds on the virtual queue drift and, as a result, leads to time-varying bounds on the expected virtual queue length. Moreover, we extend our framework to stochastic-constrained online convex optimization under two-point bandit feedback. We show that by adapting our algorithmic framework to the bandit feedback setting, we may still achieve $O(\\sqrt{T})$ expected regret and zero constraint violation, improving upon the previous work for the case of identical constraint f",
    "path": "papers/23/01/2301.11267.json",
    "total_tokens": 1052,
    "translated_title": "在具有随机约束的在线凸优化中零违约和赌博反馈",
    "translated_abstract": "本论文研究了具有随机约束的在线凸优化。我们提出了一种漂移加惩罚算法的变体，在固定的迭代次数后，它能够保证$O(\\sqrt{T})$的期望遗憾和零约束违规率，这改进了普通的漂移加惩罚方法，它的约束违规率为$O(\\sqrt{T})$。与普通的漂移加惩罚方法不同，我们的算法对于时间范围$T$的长度没有任何要求。这是基于我们的新型漂移引理，它提供了时间变化的虚拟队列漂移界限，从而导致了对期望虚拟队列长度的时间变化界限。此外，我们将我们的框架扩展到了具有两点赌博反馈的随机约束在线凸优化。我们表明，通过将我们的算法框架适应于赌博反馈场景，我们仍然可以实现$O(\\sqrt{T})$的期望遗憾和零约束违规率，改进了先前工作中关于相同约束情况的方法。",
    "tldr": "本论文提出了一种针对具有随机约束的在线凸优化的漂移加惩罚算法变体，其保证在固定迭代次数后达到了$O(\\sqrt{T})$的期望遗憾和零约束违规率。同时，将算法框架扩展到了具有赌博反馈的情况下，仍然能够实现$O(\\sqrt{T})$的期望遗憾和零约束违规率。"
}