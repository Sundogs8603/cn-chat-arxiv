{
    "title": "Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)",
    "abstract": "We consider the optimization problem of the form $\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}_{\\xi} [F(x; \\xi)]$, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2} \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$.",
    "link": "http://arxiv.org/abs/2301.06428",
    "context": "Title: Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)\nAbstract: We consider the optimization problem of the form $\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}_{\\xi} [F(x; \\xi)]$, where the component $F(x;\\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2} \\delta^{-1} \\epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\\delta,\\epsilon)$-Goldstein stationary point of objective function, where $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$.",
    "path": "papers/23/01/2301.06428.json",
    "total_tokens": 1033,
    "translated_title": "无平滑非凸随机优化问题的更快无梯度算法",
    "translated_abstract": "本文考虑形如 $\\min_{x \\in \\mathbb{R}^d} f(x) \\triangleq \\mathbb{E}_{\\xi} [F(x; \\xi)]$ 的优化问题，其中分量 $F(x;\\xi)$ 是 $L$ 平均均方偏差的 Lipschitz 但可能是非凸非光滑函数。最近提出的无梯度方法最多需要 $\\mathcal{O}( L^4 d^{3/2} \\epsilon^{-4} + \\Delta L^3 d^{3/2} \\delta^{-1} \\epsilon^{-4})$ 的随机零阶预处理器复杂度来找到目标函数的 $(\\delta,\\epsilon)$-Goldstein 静止点，其中 $\\Delta = f(x_0) - \\inf_{x \\in \\mathbb{R}^d} f(x)$，$x_0$ 是算法的初始点。本文提出了一种更高效的算法，使用随机递归梯度估计器，将复杂度改进为 $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$。",
    "tldr": "本文提出了一种使用随机递归梯度估计器的更高效算法，来解决无平滑非凸随机优化问题，其复杂度为 $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$。",
    "en_tdlr": "This paper proposes a more efficient algorithm using stochastic recursive gradient estimators to solve nonsmooth nonconvex stochastic optimization problem, with a complexity of $\\mathcal{O}(L^3 d^{3/2} \\epsilon^{-3}+ \\Delta L^2 d^{3/2} \\delta^{-1} \\epsilon^{-3})$."
}