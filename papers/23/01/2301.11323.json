{
    "title": "Joint Training of Deep Ensembles Fails Due to Learner Collusion. (arXiv:2301.11323v2 [cs.LG] UPDATED)",
    "abstract": "Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base",
    "link": "http://arxiv.org/abs/2301.11323",
    "context": "Title: Joint Training of Deep Ensembles Fails Due to Learner Collusion. (arXiv:2301.11323v2 [cs.LG] UPDATED)\nAbstract: Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base",
    "path": "papers/23/01/2301.11323.json",
    "total_tokens": 844,
    "translated_title": "深度集成的联合训练因学习者勾结而失败",
    "translated_abstract": "机器学习模型的集成已被广泛认可为一种提高性能的强大方法。传统上，集成算法独立或顺序地训练它们的基础学习者，目的是优化它们的联合性能。然而，在深度神经网络的集成中，我们有机会直接优化真正的目标：整个集成的联合性能。然而令人惊讶的是，在实践中很少直接最小化集成损失。相反，大多数先前的研究是独立地训练个别模型，并在训练后进行集成。在这项工作中，我们展示了这样做的原因是很好的 - 集成损失的联合优化会导致退化行为。我们通过将集成目标分解为基础学习者的强度和它们之间的差异来解决这个问题。我们发现联合优化导致了一种基础学习者勾结的现象。",
    "tldr": "深度神经网络的深度集成联合训练通常会导致基础学习者勾结，因此直接优化整个集成的损失很少被应用。",
    "en_tdlr": "Joint training of deep ensembles often leads to learner collusion, thus directly optimizing the loss of the whole ensemble is rarely applied."
}