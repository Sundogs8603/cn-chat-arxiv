{
    "title": "ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)",
    "abstract": "Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks. In this work, we show that neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to augment retrieval datasets with explanations and train a sequence-to-sequence ranking model to output a relevance label and an explanation for a given query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand examples with synthetic explanations performs on par with models finetuned on 3x more examples without explanations. Furthermore, the ExaRanker model incurs no additional computational cost during ranking and allows explanations to be requested on demand.",
    "link": "http://arxiv.org/abs/2301.10521",
    "context": "Title: ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)\nAbstract: Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks. In this work, we show that neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to augment retrieval datasets with explanations and train a sequence-to-sequence ranking model to output a relevance label and an explanation for a given query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand examples with synthetic explanations performs on par with models finetuned on 3x more examples without explanations. Furthermore, the ExaRanker model incurs no additional computational cost during ranking and allows explanations to be requested on demand.",
    "path": "papers/23/01/2301.10521.json",
    "total_tokens": 746,
    "translated_title": "ExaRanker: 解释增强型神经排序模型",
    "translated_abstract": "最近的研究表明，让大型语言模型在输出答案前生成解释是提高各种推理任务性能的有效策略。本文提出神经排序模型也受益于解释。我们使用GPT-3.5等语言模型来增强具有解释的检索数据集，并训练一个序列到序列的排序模型，以输出给定查询-文档对的相关度标签和解释。我们的模型被称为ExaRanker，在使用合成解释的几千个样本进行微调后，性能与无解释的3倍样本微调的模型相当。此外，ExaRanker模型在排序过程中没有额外的计算成本，并允许根据需要请求解释。",
    "tldr": "本文提出了一个名为ExaRanker的解释增强型神经排序模型，使用大型语言模型增强检索数据集，可在输出相关度标签与解释时提高性能。",
    "en_tdlr": "This paper proposes an explanation-augmented neural ranker model called ExaRanker, which uses large language models to enhance retrieval datasets and improve performance when outputting relevance labels and explanations."
}