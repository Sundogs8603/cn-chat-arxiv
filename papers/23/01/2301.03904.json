{
    "title": "RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration. (arXiv:2301.03904v2 [cs.AR] UPDATED)",
    "abstract": "The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V core",
    "link": "http://arxiv.org/abs/2301.03904",
    "context": "Title: RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration. (arXiv:2301.03904v2 [cs.AR] UPDATED)\nAbstract: The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V core",
    "path": "papers/23/01/2301.03904.json",
    "total_tokens": 984,
    "translated_title": "RedMule: 用于芯片线性代数和TinyML训练加速的混合精度矩阵计算引擎",
    "translated_abstract": "近期对于功耗只有几十毫瓦的近传感器机器学习（TinyML）的兴趣逐渐增长，而当前TinyML训练算法基于各种形式的误差和梯度反向传播，需要浮点矩阵运算来满足精度和动态范围要求。然而，迄今为止这些运算的能量和功耗成本被认为太高，无法适应TinyML场景。本文旨在解决近传感器训练在少量毫瓦功耗预算下的开放性挑战，并提出RedMulE-简化精度矩阵乘法引擎（Reduced-Precision Matrix Multiplication Engine），这是一种专为多精度浮点通用矩阵乘法（GEMM-Ops）加速而设计的低功耗加速器，支持FP16和混合FP8格式，采用符号、指数、尾数为（{1,4,3}，{1,5,2}）。我们将RedMule集成到一个包含八个节能RISC-V核心的并行超低功耗（PULP）集群中。",
    "tldr": "该论文介绍了一种名为RedMule的低功耗混合精度矩阵计算引擎，它支持多种精度和格式，用于解决近传感器训练的能耗问题。",
    "en_tdlr": "This paper introduces RedMule, a low-power mixed-precision matrix operation engine that supports multiple precisions and formats to address the energy consumption issue during near-sensor training."
}