{
    "title": "SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)",
    "abstract": "Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov",
    "link": "http://arxiv.org/abs/2301.12247",
    "context": "Title: SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)\nAbstract: Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov",
    "path": "papers/23/01/2301.12247.json",
    "total_tokens": 979,
    "translated_title": "SEGA：使用语义引导指导文本到图像模型的指导",
    "translated_abstract": "最近，文本到图像扩散模型因其令人惊讶的能力可以仅通过文本生成高保真度的图像而受到了广泛的关注。然而，实现与用户意图对齐的单次生成几乎是不可能的，而输入提示的微小改变往往会导致非常不同的图像。这使得用户在语义控制方面有限。为了使用户实现控制，我们展示了如何通过与扩散过程互动来灵活地引导语义方向。这种语义引导(SEGA)可以推广到任何使用无分类器引导的生成架构。更重要的是，它允许进行细微和广泛的编辑，组成和风格的变化，以及优化整体艺术构思。我们使用各种任务展示了SEGA在潜在和像素级扩散模型（如Stable Diffusion，Paella和DeepFloyd-IF）上的有效性，从而为其多功能性，灵活性和改进提供了有力的证据。",
    "tldr": "本论文介绍了一种称为SEGA的语义引导方法，用于指导文本到图像模型的生成过程。通过与扩散过程的互动，SEGA可以灵活地在语义方向上引导模型的生成，实现细微和广泛的编辑以及优化整体艺术构思。实验证明SEGA在多种任务和生成架构上都表现出了出色的多功能性、灵活性和改进。",
    "en_tdlr": "This paper presents a semantic guidance method called SEGA for instructing the generation process of text-to-image models. By interacting with the diffusion process, SEGA can flexibly steer the model's generation along semantic directions to achieve subtle and extensive edits, as well as optimizing the overall artistic conception. Experimental results demonstrate the outstanding versatility, flexibility, and improvements of SEGA in various tasks and generative architectures."
}