{
    "title": "Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)",
    "abstract": "Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame",
    "link": "http://arxiv.org/abs/2301.01286",
    "context": "Title: Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)\nAbstract: Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame",
    "path": "papers/23/01/2301.01286.json",
    "total_tokens": 922,
    "translated_title": "DARTS搜索空间的伪反转瓶颈卷积",
    "translated_abstract": "可微架构搜索（DARTS）作为一种基于梯度的神经结构搜索方法引起了广泛关注。然而，自引入DARTS以来，很少有工作基于最先进的卷积神经网络结构设计原则逐步扩展DARTS搜索空间。本文旨在通过增加ConvNeXt的微小设计变化来逐步扩充DARTS搜索空间，并研究准确性、评估层数和计算成本之间的权衡。我们引入了伪反转瓶颈卷积（PIBConv）块，旨在减少ConvNeXt中提出的反转瓶颈块的计算占用。我们提出的架构对于评估层数的敏感度要小得多，并且在层数仅为2时显着优于一个具有类似规模的DARTS网络。此外，使用更少的层数，它不仅在较低的GMACs和参数数量下实现更高的准确性 ，计算占用也更小。",
    "tldr": "本文增加了ConvNeXt的微小设计变化来扩充DARTS搜索空间，提出了PIBConv块来减少计算占用，我们的架构在层数仅为2时优于一个具有类似规模的DARTS网络。",
    "en_tdlr": "This work incrementally augments the DARTS search space with micro-design changes inspired by ConvNeXt, introduces the PIBConv block intending to reduce computational footprint, and outperforms a DARTS network significantly with similar size, at layer counts as small as 2."
}