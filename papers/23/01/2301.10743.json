{
    "title": "Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)",
    "abstract": "Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.",
    "link": "http://arxiv.org/abs/2301.10743",
    "context": "Title: Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)\nAbstract: Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.",
    "path": "papers/23/01/2301.10743.json",
    "total_tokens": 792,
    "translated_title": "对Transformer编码器表达能力的更紧密界定",
    "translated_abstract": "用更易理解的形式系统表征神经网络有潜力揭示这些网络的能力和局限性，然而对于Transformer来说这仍是一个活跃的研究领域。Bhattamishra等人已经表明，Transformer编码器至少与一种特定的计数机同等表达能力，而Merrill和Sabharwal则表明固定精度的Transformer编码器只能识别统一的$TC^0$语言。我们通过确认具有计数量词的一阶逻辑的变体，既是固定精度Transformer编码器的上界，也是Transformer编码器的下界，从而将这些结果联系起来并加以加强。这使我们比以前更接近准确刻画Transformer编码器可识别的语言的目标。",
    "tldr": "本文旨在更紧密地界定Transformer编码器的表达能力，提供了一个同时是下限和上限的一阶逻辑变体的策略，使我们更加接近准确刻画Transformer编码器可识别语言的目标。",
    "en_tdlr": "This paper aims to provide tighter bounds on the expressivity of Transformer encoders by identifying a variant of first-order logic with counting quantifiers that serves as both a lower bound and an upper bound. The results bring us closer to an exact characterization of the languages that Transformer encoders can recognize."
}