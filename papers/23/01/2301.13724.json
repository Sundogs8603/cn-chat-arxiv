{
    "title": "Towards fully covariant machine learning. (arXiv:2301.13724v2 [stat.ML] UPDATED)",
    "abstract": "Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physic",
    "link": "http://arxiv.org/abs/2301.13724",
    "context": "Title: Towards fully covariant machine learning. (arXiv:2301.13724v2 [stat.ML] UPDATED)\nAbstract: Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physic",
    "path": "papers/23/01/2301.13724.json",
    "total_tokens": 924,
    "translated_title": "迈向完全协变的机器学习",
    "translated_abstract": "任何对数据的表示都涉及到任意的研究者选择。由于这些选择是外部于数据生成过程的，每个选择都导致了一个确切的对称性，对应于将一个可能的表示转化为另一个表示的变换群。这些被动对称性包括坐标自由度、规范对称性和单位协变性，在物理学中都产生了重要的结果。在机器学习中，最明显的被动对称性是图的重新标记或置换对称性。我们的目标是理解这些被动对称性对机器学习的影响。如果要尊重被动对称性，我们讨论了机器学习实践中的应该和不应该。我们还讨论了与因果建模的联系，并认为在学习问题的目标是样本外推广时，实现被动对称性特别有价值。这篇论文是概念性的：它在物理学的语言之间进行翻译。",
    "tldr": "本文探讨了机器学习中多个被动对称性的影响，并提出了关于机器学习实践中尊重被动对称性的 dos and don'ts。此外，还讨论了被动对称性与因果建模的关系，并指出在学习问题的目标是样本外推广时，实现被动对称性尤其有价值。",
    "en_tdlr": "This paper discusses the implications of multiple passive symmetries in machine learning and provides dos and don'ts for respecting these symmetries in machine learning practice. It also explores the relationship between passive symmetries and causal modeling, highlighting the value of implementing passive symmetries when the goal is generalization out of sample in the learning problem."
}