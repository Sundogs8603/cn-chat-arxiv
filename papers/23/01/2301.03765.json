{
    "title": "Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding",
    "abstract": "arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr",
    "link": "https://arxiv.org/abs/2301.03765",
    "context": "Title: Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding\nAbstract: arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr",
    "path": "papers/23/01/2301.03765.json",
    "total_tokens": 861,
    "translated_title": "通过交叉模型比较损失增强语言理解中神经元效用",
    "translated_abstract": "当前自然语言理解（NLU）模型在模型规模和输入背景方面不断扩大，引入了更多隐藏神经元和输入神经元，大体上提高了性能。然而，额外的神经元并不能为所有实例带来一致的改进，因为一些隐藏神经元是冗余的，混入输入神经元的噪声往往会分散模型的注意力。之前的工作主要侧重于通过附加的后处理或预处理，如网络修剪和上下文选择，从外部降低低效神经元的数量，以避免这个问题。除此之外，我们是否可以通过增强每个神经元的效用来使模型减少冗余参数并抑制输入噪声？如果一个模型能够有效地利用神经元，那么不管哪些神经元被剥离（禁用），剥离后的子模型的性能都不应该优于原始完整模型。根据这样的比较",
    "tldr": "本论文提出通过交叉模型比较损失的方法来增强语言理解模型中神经元的效用，实现减少冗余参数和抑制输入噪声的目标。",
    "en_tdlr": "This paper proposes a method of enhancing the utility of neurons in language understanding models by cross-model comparative loss, aiming to reduce redundant parameters and suppress input noise."
}