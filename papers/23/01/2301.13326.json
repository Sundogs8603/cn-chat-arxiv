{
    "title": "A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)",
    "abstract": "We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\\alpha$-regret methods that only require bandit feedback, achieving $\\mathcal{O}\\left(T^\\frac{2}{3}\\log(T)^\\frac{1}{3}\\right)$ expected cumulative $\\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed",
    "link": "http://arxiv.org/abs/2301.13326",
    "context": "Title: A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)\nAbstract: We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\\alpha$-regret methods that only require bandit feedback, achieving $\\mathcal{O}\\left(T^\\frac{2}{3}\\log(T)^\\frac{1}{3}\\right)$ expected cumulative $\\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed",
    "path": "papers/23/01/2301.13326.json",
    "total_tokens": 994,
    "translated_title": "适应离线算法解决带有赌徒反馈的组合多臂赌博问题的框架",
    "translated_abstract": "我们研究了随机的组合多臂赌博问题，其中学习者只能访问赌徒反馈，并且奖励函数可以是非线性的。我们提供了一个通用的框架，将离线离散逼近算法调整为只需要赌徒反馈的亚线性α-后悔方法，实现了对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。该框架只需要离线算法对函数评估中的小误差具有鲁棒性。适应过程甚至不需要显式地知道离线逼近算法--离线算法可以被用作黑盒子子程序。为了证明所提出的框架的实用性，将该框架应用于子模最大化的不同应用中。在具有背包约束的子模最大化中，新的CMAB算法优于开发的完全赌徒方法。",
    "tldr": "这个论文介绍了一个框架，用于将离线算法调整为只需要赌徒反馈的亚线性α-后悔方法来解决组合多臂赌博问题，该框架可以实现对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。",
    "en_tdlr": "This paper presents a framework to adapt offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback, achieving sublinear $\\alpha$-regret dependence on the horizon T. The framework does not require explicit knowledge of the offline approximation algorithm and can be applied to submodular maximization with knapsack constraints."
}