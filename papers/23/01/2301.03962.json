{
    "title": "A Unified Theory of Diversity in Ensemble Learning",
    "abstract": "We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity",
    "link": "https://arxiv.org/abs/2301.03962",
    "context": "Title: A Unified Theory of Diversity in Ensemble Learning\nAbstract: We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity",
    "path": "papers/23/01/2301.03962.json",
    "total_tokens": 956,
    "translated_title": "集成学习多样性的统一理论",
    "translated_abstract": "我们提出了一个集成多样性的理论，解释了在各种监督学习场景中多样性的本质。这个挑战被称为集成学习的圣杯，是一个开放的研究问题已经有30多年了。我们的框架揭示了多样性实际上是集成损失的偏差-方差分解中的一个隐藏维度。我们证明了一族精确的偏差-方差-多样性分解，适用于回归和分类的各种损失函数，例如平方损失、交叉熵损失和泊松损失。对于没有可加性偏差-方差分解的损失函数（例如0/1损失），我们提出了一种替代方法：量化多样性的效果，结果依赖于标签分布。总体而言，我们认为多样性是模型拟合度的度量，与偏差和方差具有相同的意义，但考虑了集成成员之间的统计依赖关系。因此，我们不应该最大化多样性。",
    "tldr": "这篇论文提出了一个统一的集成学习多样性理论，解释了多样性在各种监督学习场景中的本质。它揭示了多样性在集成损失的偏差-方差分解中的作用，同时提出了一种量化多样性效果的方法。这个理论对于提高集成模型的性能具有重要意义。",
    "en_tdlr": "This paper presents a unified theory of diversity in ensemble learning, explaining the nature of diversity in various supervised learning scenarios. It reveals the role of diversity in the bias-variance decomposition of ensemble loss and introduces a method for quantifying the effects of diversity. This theory is significant for improving the performance of ensemble models."
}