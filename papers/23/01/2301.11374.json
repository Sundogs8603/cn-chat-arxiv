{
    "title": "Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation. (arXiv:2301.11374v2 [cs.LG] UPDATED)",
    "abstract": "We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial atta",
    "link": "http://arxiv.org/abs/2301.11374",
    "context": "Title: Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation. (arXiv:2301.11374v2 [cs.LG] UPDATED)\nAbstract: We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial atta",
    "path": "papers/23/01/2301.11374.json",
    "total_tokens": 874,
    "translated_title": "可证明鲁棒性强化学习：基于模型的抽象解释方法",
    "translated_abstract": "我们提出了一个强化学习框架 CAROL，其学习出的策略带有可证明的对抗鲁棒性证书。我们的方法学习了环境的模型，并在每个学习迭代中使用该模型和外部抽象解释器构建可微分的证明鲁棒性信号以引导学习，直到收敛时，该抽象解释就直接导致了可靠性证书。我们给出了一个理论分析，界定了 CAROL 的最坏情况下累积奖励。我们还在四个连续状态和动作空间的 MuJoCo 环境上对 CAROL 进行了实验评估。在这些任务中，CAROL 学习出的策略与现有强化学习算法相比，展示出显著增强的认证性能下限和可比较的对抗性能。",
    "tldr": "CAROL是一个强化学习框架，它基于模型和抽象解释方法，学习出的策略具有机器可证明的对抗鲁棒性证书，在实验上表现出更好的认证性能和可比较的对抗性能。",
    "en_tdlr": "CAROL is a reinforcement learning framework that uses model-based abstract interpretation to learn policies that come with machine-checkable certificates of adversarial robustness, and shows better certified performance and comparable adversarial performance in experiments."
}