{
    "title": "CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence",
    "abstract": "arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)\", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\\mathcal{O}(n{C^3}/(1-\\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\\mathcal{O}(n^3{C^6}/(1-\\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\\lambda_2)$ denotes the spectr",
    "link": "https://arxiv.org/abs/2301.05872",
    "context": "Title: CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence\nAbstract: arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)\", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\\mathcal{O}(n{C^3}/(1-\\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\\mathcal{O}(n^3{C^6}/(1-\\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\\lambda_2)$ denotes the spectr",
    "path": "papers/23/01/2301.05872.json",
    "total_tokens": 945,
    "translated_title": "CEDAS：一种具有改进收敛性的压缩分布式随机梯度法",
    "translated_abstract": "在本文中，我们考虑在通信受限环境下解决多代理网络上的分布式优化问题。我们研究了一种称为“具有自适应步长的压缩精确扩散（CEDAS）”的压缩分布式随机梯度方法，并证明该方法在无偏压缩运算符下渐近地实现了与集中式随机梯度下降（SGD）相当的收敛速度，适用于光滑强凸目标函数和光滑非凸目标函数。特别地，据我们所知，CEDAS迄今为止以其最短的瞬态时间（关于图的特性）实现了与集中式SGD相同的收敛速度，其在光滑强凸目标函数下表现为$\\mathcal{O}(n{C^3}/(1-\\lambda_2)^{2})$，在光滑非凸目标函数下表现为$\\mathcal{O}(n^3{C^6}/(1-\\lambda_2)^4)$，其中$(1-\\lambda_2)$表示谱...",
    "tldr": "CEDAS提出了一种压缩分布式随机梯度方法，在无偏压缩运算符下具有与集中式随机梯度下降相当的收敛速度，实现了最短的瞬态时间，对光滑强凸和非凸目标函数都适用。",
    "en_tdlr": "CEDAS proposes a compressed decentralized stochastic gradient method with comparable convergence rate to centralized SGD under unbiased compression operators, achieving the shortest transient time and applicability to both smooth strongly convex and nonconvex objective functions."
}