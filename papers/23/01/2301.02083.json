{
    "title": "Self-Motivated Multi-Agent Exploration. (arXiv:2301.02083v2 [cs.LG] UPDATED)",
    "abstract": "In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (",
    "link": "http://arxiv.org/abs/2301.02083",
    "context": "Title: Self-Motivated Multi-Agent Exploration. (arXiv:2301.02083v2 [cs.LG] UPDATED)\nAbstract: In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (",
    "path": "papers/23/01/2301.02083.json",
    "total_tokens": 897,
    "translated_title": "自我激励的多智能体探索",
    "translated_abstract": "在合作多智能体强化学习中，智能体在自我探索和团队合作之间实现平衡至关重要。然而，智能体在没有协调的情况下几乎无法完成团队任务，并且容易陷入只进行简单合作的局部最优解中，无法进行足够的个体探索。最近的研究主要集中在智能体的协调探索，导致状态空间的探索指数级增加。为了解决这个问题，我们提出了自我激励的多智能体探索（SMMAE），旨在通过自适应地找到自我探索和团队合作之间的权衡来取得团队任务的成功。在SMMAE中，我们针对每个智能体训练一个独立的探索策略，以最大化它们自己所访问的状态空间。每个智能体根据联合团队策略的稳定性学习可调节的探索概率。在StarCraft II微管理基准测试中进行了实验，验证了SMMAE的有效性。",
    "tldr": "这项研究提出了自我激励的多智能体探索方法，在合作多智能体强化学习中找到自我探索和团队合作的平衡，以实现团队任务的成功。",
    "en_tdlr": "This research proposes a self-motivated multi-agent exploration method to achieve a balance between self-exploration and team cooperation in cooperative multi-agent reinforcement learning, resulting in successful team tasks."
}