{
    "title": "FedPH: Privacy-enhanced Heterogeneous Federated Learning. (arXiv:2301.11705v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning is a distributed machine-learning environment that allows clients to learn collaboratively without sharing private data. This is accomplished by exchanging parameters. However, the differences in data distributions and computing resources among clients make related studies difficult. To address these heterogeneous problems, we propose a novel Federated Learning method. Our method utilizes a pre-trained model as the backbone of the local model, with fully connected layers comprising the head. The backbone extracts features for the head, and the embedding vector of classes is shared between clients to improve the head and enhance the performance of the local model. By sharing the embedding vector of classes instead of gradient-based parameters, clients can better adapt to private data, and communication between the server and clients is more effective. To protect privacy, we propose a privacy-preserving hybrid method that adds noise to the embedding vector of classes. ",
    "link": "http://arxiv.org/abs/2301.11705",
    "context": "Title: FedPH: Privacy-enhanced Heterogeneous Federated Learning. (arXiv:2301.11705v2 [cs.LG] UPDATED)\nAbstract: Federated Learning is a distributed machine-learning environment that allows clients to learn collaboratively without sharing private data. This is accomplished by exchanging parameters. However, the differences in data distributions and computing resources among clients make related studies difficult. To address these heterogeneous problems, we propose a novel Federated Learning method. Our method utilizes a pre-trained model as the backbone of the local model, with fully connected layers comprising the head. The backbone extracts features for the head, and the embedding vector of classes is shared between clients to improve the head and enhance the performance of the local model. By sharing the embedding vector of classes instead of gradient-based parameters, clients can better adapt to private data, and communication between the server and clients is more effective. To protect privacy, we propose a privacy-preserving hybrid method that adds noise to the embedding vector of classes. ",
    "path": "papers/23/01/2301.11705.json",
    "total_tokens": 874,
    "translated_title": "FedPH: 隐私增强型异构联邦学习",
    "translated_abstract": "联邦学习是一种分布式机器学习环境，允许客户端在不共享私有数据的情况下进行协作学习，通过交换参数来实现。然而，客户端之间的数据分布和计算资源差异使得相关研究变得困难。为了解决这些异构问题，我们提出了一种新的联邦学习方法。我们的方法利用预训练模型作为本地模型的骨干，完全连接的层构成头部。骨干提取头部特征，类的嵌入向量在客户端之间共享，以改善头部并增强本地模型的性能。通过共享类的嵌入向量而不是梯度参数，客户端可以更好地适应私有数据，服务器和客户端之间的通信更加有效。为了保护隐私，我们提出了一种隐私保护的混合方法，向类的嵌入向量添加噪声。",
    "tldr": "该论文提出了一种利用预训练模型作为本地模型的骨干，共享嵌入类向量来增强本地模型性能的异构联邦学习方法，并采用隐私保护的混合方法来保护隐私。",
    "en_tdlr": "The paper proposes a novel Federated Learning method that uses a pre-trained model as the backbone of the local model and shares embedding class vectors to enhance performance, while also using a privacy-preserving hybrid method to protect privacy."
}