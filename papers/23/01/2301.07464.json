{
    "title": "CLIPTER: Looking at the Bigger Picture in Scene Text Recognition. (arXiv:2301.07464v2 [cs.CV] UPDATED)",
    "abstract": "Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved ",
    "link": "http://arxiv.org/abs/2301.07464",
    "context": "Title: CLIPTER: Looking at the Bigger Picture in Scene Text Recognition. (arXiv:2301.07464v2 [cs.CV] UPDATED)\nAbstract: Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved ",
    "path": "papers/23/01/2301.07464.json",
    "total_tokens": 940,
    "translated_title": "CLIPTER: 在场景文本识别中关注更大的背景信息",
    "translated_abstract": "在现实世界的场景中阅读文本通常需要理解其周围的上下文，尤其是在处理质量较差的文本时。然而，当前的场景文本识别器在处理裁剪的文本图像时并不了解更大的背景信息。在这项研究中，我们利用现代视觉语言模型（如CLIP）的代表性能力，将场景级别的信息提供给基于裁剪的识别器。我们通过一个门控交叉注意机制，将整个图像的丰富表示（来自视觉语言模型）与识别器的单词级特征融合。该组件逐渐转向增强上下文的表示，从而实现对预训练识别器的稳定微调。我们在领先的文本识别架构上展示了我们的模型无关框架CLIPTER（CLIP文本识别）的有效性，并在多个基准测试中取得了最先进的结果。此外，我们的分析突出了改进了的...",
    "tldr": "本研究提出了CLIPTER（CLIP文本识别）框架，利用现代视觉语言模型提供整个图像的信息，通过门控交叉注意机制融合到裁剪的文本图像识别器中。结果表明，这种方法在多个基准测试上取得了最先进的效果。",
    "en_tdlr": "This study introduces the CLIPTER framework, which leverages modern vision-language models to provide scene-level information to crop-based text recognizers. The rich representation of the entire image is fused with the recognizer word-level features through a gated cross-attention mechanism. The results demonstrate the state-of-the-art performance of this approach across multiple benchmarks."
}