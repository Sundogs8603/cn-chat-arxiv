{
    "title": "Knowledge Distillation in Federated Edge Learning: A Survey",
    "abstract": "arXiv:2301.05849v3 Announce Type: replace  Abstract: The increasing demand for intelligent services and privacy protection of mobile and Internet of Things (IoT) devices motivates the wide application of Federated Edge Learning (FEL), in which devices collaboratively train on-device Machine Learning (ML) models without sharing their private data. Limited by device hardware, diverse user behaviors and network infrastructure, the algorithm design of FEL faces challenges related to resources, personalization and network environments. Fortunately, Knowledge Distillation (KD) has been leveraged as an important technique to tackle the above challenges in FEL. In this paper, we investigate the works that KD applies to FEL, discuss the limitations and open problems of existing KD-based FEL approaches, and provide guidance for their real deployment.",
    "link": "https://arxiv.org/abs/2301.05849",
    "context": "Title: Knowledge Distillation in Federated Edge Learning: A Survey\nAbstract: arXiv:2301.05849v3 Announce Type: replace  Abstract: The increasing demand for intelligent services and privacy protection of mobile and Internet of Things (IoT) devices motivates the wide application of Federated Edge Learning (FEL), in which devices collaboratively train on-device Machine Learning (ML) models without sharing their private data. Limited by device hardware, diverse user behaviors and network infrastructure, the algorithm design of FEL faces challenges related to resources, personalization and network environments. Fortunately, Knowledge Distillation (KD) has been leveraged as an important technique to tackle the above challenges in FEL. In this paper, we investigate the works that KD applies to FEL, discuss the limitations and open problems of existing KD-based FEL approaches, and provide guidance for their real deployment.",
    "path": "papers/23/01/2301.05849.json",
    "total_tokens": 781,
    "translated_title": "知识蒸馏在联邦边缘学习中的应用：一项调查",
    "translated_abstract": "随着对智能服务和移动设备以及物联网设备隐私保护要求的增加，促使了联邦边缘学习（FEL）的广泛应用，其中设备在不共享私人数据的情况下协同训练设备上的机器学习（ML）模型。由于设备硬件、用户行为和网络基础设施的多样性，FEL的算法设计面临着与资源、个性化和网络环境相关的挑战。幸运的是，知识蒸馏（KD）已被利用作为解决FEL中上述挑战的重要技术。本文调查了KD应用于FEL的研究成果，讨论了现有基于KD的FEL方法的局限性和未解决的问题，并为它们的实际部署提供指导。",
    "tldr": "研究调查了知识蒸馏在联邦边缘学习中的应用，讨论了现有方法的局限性和问题，并提供了实际部署的指导。",
    "en_tdlr": "The study surveyed the application of knowledge distillation in federated edge learning, discussed the limitations and issues of existing methods, and provided guidance for practical deployment."
}