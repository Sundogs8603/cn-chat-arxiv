{
    "title": "Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)",
    "abstract": "Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p",
    "link": "http://arxiv.org/abs/2301.12950",
    "context": "Title: Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)\nAbstract: Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p",
    "path": "papers/23/01/2301.12950.json",
    "total_tokens": 721,
    "translated_title": "通过学习程序组合的方法实现分层编程强化学习",
    "translated_abstract": "Trivedi等人(2021)提出了一种方法(LEAPS)，旨在产生具有人类可解释性并且可以更好地推广到新场景的加强学习（RL）策略。方法先学习一个程序嵌入空间，以连续参数化来自预生成的程序数据集的多样化程序，然后在给定任务时在学习的程序嵌入空间中搜索解决任务的程序。",
    "tldr": "本论文提出了一种名为HPRL的分层编程强化学习框架，通过学习程序组合的方法实现，能够产生具有人类可解释性并且在评估候选方案时可以准确奖励和惩罚的策略。",
    "en_tdlr": "The paper proposes a hierarchical programmatic reinforcement learning framework called HPRL, which learns to compose programs and can produce human-interpretable policies that can accurately reward and penalize candidate solutions during evaluation."
}