{
    "title": "Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties. (arXiv:2301.11592v2 [cs.LG] UPDATED)",
    "abstract": "Constrained Reinforcement Learning has been employed to enforce safety constraints on policy through the use of expected cost constraints. The key challenge is in handling expected cost accumulated using the policy and not just in a single step. Existing methods have developed innovative ways of converting this cost constraint over entire policy to constraints over local decisions (at each time step). While such approaches have provided good solutions with regards to objective, they can either be overly aggressive or conservative with respect to costs. This is owing to use of estimates for \"future\" or \"backward\" costs in local cost constraints.  To that end, we provide an equivalent unconstrained formulation to constrained RL that has an augmented state space and reward penalties. This intuitive formulation is general and has interesting theoretical properties. More importantly, this provides a new paradigm for solving constrained RL problems effectively. As we show in our experimental",
    "link": "http://arxiv.org/abs/2301.11592",
    "context": "Title: Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties. (arXiv:2301.11592v2 [cs.LG] UPDATED)\nAbstract: Constrained Reinforcement Learning has been employed to enforce safety constraints on policy through the use of expected cost constraints. The key challenge is in handling expected cost accumulated using the policy and not just in a single step. Existing methods have developed innovative ways of converting this cost constraint over entire policy to constraints over local decisions (at each time step). While such approaches have provided good solutions with regards to objective, they can either be overly aggressive or conservative with respect to costs. This is owing to use of estimates for \"future\" or \"backward\" costs in local cost constraints.  To that end, we provide an equivalent unconstrained formulation to constrained RL that has an augmented state space and reward penalties. This intuitive formulation is general and has interesting theoretical properties. More importantly, this provides a new paradigm for solving constrained RL problems effectively. As we show in our experimental",
    "path": "papers/23/01/2301.11592.json",
    "total_tokens": 907,
    "translated_title": "通过状态增强和奖励惩罚解决复杂约束强化学习问题",
    "translated_abstract": "约束强化学习通过使用预期成本约束来执行策略的安全约束。其主要挑战在于处理使用策略积累的预期成本，而不仅仅是单个步骤中的成本。现有方法已经开发出了将整个策略的成本约束转换为本地决策（每个时间步骤）约束的创新方法。虽然这些方法在客观上提供了好的解决方案，但是它们可能在成本方面过于激进或过于保守。这是由于在本地成本约束中使用了“未来”或“后向”成本的估计。为此，我们提出了一个等效的无约束RL公式，其中包括增强状态空间和奖励惩罚。这种直观的公式具有广泛的适用性和有趣的理论属性。更重要的是，这为有效解决约束RL问题提供了新的范例。正如我们在实验结果中展示的那样，我们提出的方法在安全性和目标性能方面优于现有技术。",
    "tldr": "本文提出了一种基于状态增强和奖励惩罚的约束强化学习新方法，相较于现有技术，在保证安全性和目标性能方面表现更好。",
    "en_tdlr": "This paper proposes a new constrained reinforcement learning method based on state augmentation and reward penalties, which outperforms existing techniques in terms of safety and objective performance."
}