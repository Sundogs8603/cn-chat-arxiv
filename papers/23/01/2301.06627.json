{
    "title": "Dissociating language and thought in large language models",
    "abstract": "arXiv:2301.06627v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence - knowledge of linguistic rules and patterns - and functional linguistic competence - understanding and using language in the world. We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of mechanisms specialized for formal linguistic co",
    "link": "https://arxiv.org/abs/2301.06627",
    "context": "Title: Dissociating language and thought in large language models\nAbstract: arXiv:2301.06627v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence - knowledge of linguistic rules and patterns - and functional linguistic competence - understanding and using language in the world. We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of mechanisms specialized for formal linguistic co",
    "path": "papers/23/01/2301.06627.json",
    "total_tokens": 800,
    "translated_title": "在大语言模型中区分语言和思维",
    "translated_abstract": "大型语言模型（LLMs）迄今为止在掌握人类语言方面做得最好，然而人们对它们的语言和认知能力仍存在分歧。本文使用形式语言能力（对语言规则和模式的了解）与功能语言能力（理解和使用语言在世界中的方式）的区别来评估LLMs。我们通过人类神经科学来确立这一区别，人类神经科学显示形式和功能能力依赖于不同的神经机制。尽管LLMs在形式能力方面表现出人们的惊人水平，但它们在功能能力任务上的表现仍然不稳定，并且通常需要专门的精细调整和/或与外部模块的耦合。我们认为，那些以类似人类方式使用语言的模型将需要掌握这两种能力类型，而这反过来可能需要为形式语言能力专门化的机制的出现。",
    "tldr": "大型语言模型在形式语言能力方面表现出色，但在功能语言能力任务上表现不稳定，可能需要专门的调整和外部模块的支持。",
    "en_tdlr": "Large Language Models excel in formal linguistic competence but struggle in functional linguistic competence tasks, potentially requiring specialized adjustments and external module support."
}