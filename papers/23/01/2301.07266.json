{
    "title": "ACQ: Improving Generative Data-free Quantization Via Attention Correction. (arXiv:2301.07266v2 [cs.CV] UPDATED)",
    "abstract": "Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited modes of attention. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the batch-normalization statistics matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. An attention center position-condition generator is established regarding the homogenization of intra-class attent",
    "link": "http://arxiv.org/abs/2301.07266",
    "context": "Title: ACQ: Improving Generative Data-free Quantization Via Attention Correction. (arXiv:2301.07266v2 [cs.CV] UPDATED)\nAbstract: Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited modes of attention. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the batch-normalization statistics matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. An attention center position-condition generator is established regarding the homogenization of intra-class attent",
    "path": "papers/23/01/2301.07266.json",
    "total_tokens": 737,
    "translated_title": "ACQ: 借助注意力修正改进生成式无数据量化方法",
    "translated_abstract": "无数据量化旨在实现模型量化而不访问任何真实样本。在涉及数据隐私的应用场景中具有重要意义。通过生成器将噪声向量转化为合成样本是一种常用的无数据量化方法，称为生成式无数据量化。然而，合成样本和真实样本之间存在注意力差异，这一点经常被忽视，限制了量化性能。ACQ是本文提出的修正合成样本注意力的方法。该方法建立了一个基于注意力中心位置的条件生成器，旨在实现类内关注度的均一化。",
    "tldr": "本论文提出了ACQ方法，通过修正合成样本的注意力，改进了生成式无数据量化方法。通过建立注意力中心位置的条件生成器，实现了类内关注度的均一化。",
    "en_tdlr": "This paper proposes ACQ, which improves generative data-free quantization by fixing the attention of synthetic samples. By establishing a conditional generator based on the attention center position, it achieves intra-class attention normalization."
}