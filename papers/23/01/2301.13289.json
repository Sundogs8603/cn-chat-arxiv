{
    "title": "On the Statistical Benefits of Temporal Difference Learning",
    "abstract": "arXiv:2301.13289v3 Announce Type: replace Abstract: Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent. Next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: TD's errors are bounded in terms of a novel measure - the problem's trajectory crossing time - which can be much smaller than the pr",
    "link": "https://arxiv.org/abs/2301.13289",
    "context": "Title: On the Statistical Benefits of Temporal Difference Learning\nAbstract: arXiv:2301.13289v3 Announce Type: replace Abstract: Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent. Next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: TD's errors are bounded in terms of a novel measure - the problem's trajectory crossing time - which can be much smaller than the pr",
    "path": "papers/23/01/2301.13289.json",
    "total_tokens": 861,
    "translated_title": "关于时序差异学习的统计优势",
    "translated_abstract": "给定一个关于动作和长期奖励的数据集，直接估计方法通过将值函数与训练数据的预测误差最小化来拟合。而时序差异学习(TD)方法则通过最小化在连续时间步骤中进行的估计之间的时序不一致程度来拟合值函数。针对有限状态Markov链，我们提供了关于这种方法的统计优势的清晰渐进理论。首先，我们证明了一个直观的逆轨迹汇集系数完全刻画了值估计均方误差的百分比减少。根据问题结构的不同，这种减少可以是巨大的或不存在的。接下来，我们证明了两个状态的值差估计可以有巨大的改进：TD的误差受到问题轨迹交叉时间的界限，而这个界限可能远小于概率。",
    "tldr": "时序差异学习方法通过最小化连续时间步骤中的估计时序不一致度来拟合值函数，具有统计优势，可以显著减少值估计的均方误差，并且可以在两个状态的值差估计中获得显著改进。"
}