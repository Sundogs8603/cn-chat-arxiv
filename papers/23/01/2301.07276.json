{
    "title": "Data thinning for convolution-closed distributions. (arXiv:2301.07276v2 [stat.ME] UPDATED)",
    "abstract": "We propose data thinning, an approach for splitting an observation into two or more independent parts that sum to the original observation, and that follow the same distribution as the original observation, up to a (known) scaling of a parameter. This very general proposal is applicable to any convolution-closed distribution, a class that includes the Gaussian, Poisson, negative binomial, gamma, and binomial distributions, among others. Data thinning has a number of applications to model selection, evaluation, and inference. For instance, cross-validation via data thinning provides an attractive alternative to the usual approach of cross-validation via sample splitting, especially in unsupervised settings in which the latter is not applicable. In simulations and in an application to single-cell RNA-sequencing data, we show that data thinning can be used to validate the results of unsupervised learning approaches, such as k-means clustering and principal components analysis.",
    "link": "http://arxiv.org/abs/2301.07276",
    "context": "Title: Data thinning for convolution-closed distributions. (arXiv:2301.07276v2 [stat.ME] UPDATED)\nAbstract: We propose data thinning, an approach for splitting an observation into two or more independent parts that sum to the original observation, and that follow the same distribution as the original observation, up to a (known) scaling of a parameter. This very general proposal is applicable to any convolution-closed distribution, a class that includes the Gaussian, Poisson, negative binomial, gamma, and binomial distributions, among others. Data thinning has a number of applications to model selection, evaluation, and inference. For instance, cross-validation via data thinning provides an attractive alternative to the usual approach of cross-validation via sample splitting, especially in unsupervised settings in which the latter is not applicable. In simulations and in an application to single-cell RNA-sequencing data, we show that data thinning can be used to validate the results of unsupervised learning approaches, such as k-means clustering and principal components analysis.",
    "path": "papers/23/01/2301.07276.json",
    "total_tokens": 962,
    "translated_title": "数据稀疏化技术用于卷积封闭分布",
    "translated_abstract": "我们提出了一种称为数据稀疏化的方法，将一个观测值分成两个或更多个互相独立的部分，这些部分都加起来等于原始数据，并且与原始观测值相同的分布，只是经过一个已知参数调整。这个非常普适的方法适用于任何卷积封闭分布，包括高斯分布、泊松分布、负二项分布、伽玛分布和二项分布等。数据稀疏化在模型选择、评价和推理方面有多种应用。例如，通过数据稀疏化的交叉验证提供了一种吸引人的替代方法来进行交叉验证，特别是在无监督的情况下，传统方法的样本划分不适用。我们在模拟和应用于单细胞RNA测序数据的实验中展示了数据稀疏化的普遍性，可以用于验证无监督学习方法的结果，如k-means聚类和主成分分析。",
    "tldr": "本文提出了数据稀疏化方法，适用于很多分布类型，包括高斯分布、泊松分布、负二项分布、伽玛分布和二项分布等。该方法具有广泛的应用，如在交叉验证方面提供了一种新的方法，能有效验证无监督学习算法的可靠性。",
    "en_tdlr": "This paper proposes a data thinning approach applicable to convolution-closed distributions like Gaussian, Poisson, negative binomial, gamma, and binomial distributions. It has broad applications, such as providing an alternative to cross-validation via sample splitting, especially in unsupervised settings. It can be used to validate the results of unsupervised learning approaches like k-means clustering and principal components analysis."
}