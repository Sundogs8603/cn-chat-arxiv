{
    "title": "Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient",
    "abstract": "arXiv:2301.04431v4 Announce Type: replace-cross  Abstract: Backtracking linesearch is the de facto approach for minimizing continuously differentiable functions with locally Lipschitz gradient. In recent years, it has been shown that in the convex setting it is possible to avoid linesearch altogether, and to allow the stepsize to adapt based on a local smoothness estimate without any backtracks or evaluations of the function value. In this work we propose an adaptive proximal gradient method, adaPG, that uses novel estimates of the local smoothness modulus which leads to less conservative stepsize updates and that can additionally cope with nonsmooth terms. This idea is extended to the primal-dual setting where an adaptive three-term primal-dual algorithm, adaPD, is proposed which can be viewed as an extension of the PDHG method. Moreover, in this setting the \"essentially\" fully adaptive variant adaPD$^+$ is proposed that avoids evaluating the linear operator norm by invoking a backtra",
    "link": "https://arxiv.org/abs/2301.04431",
    "context": "Title: Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient\nAbstract: arXiv:2301.04431v4 Announce Type: replace-cross  Abstract: Backtracking linesearch is the de facto approach for minimizing continuously differentiable functions with locally Lipschitz gradient. In recent years, it has been shown that in the convex setting it is possible to avoid linesearch altogether, and to allow the stepsize to adapt based on a local smoothness estimate without any backtracks or evaluations of the function value. In this work we propose an adaptive proximal gradient method, adaPG, that uses novel estimates of the local smoothness modulus which leads to less conservative stepsize updates and that can additionally cope with nonsmooth terms. This idea is extended to the primal-dual setting where an adaptive three-term primal-dual algorithm, adaPD, is proposed which can be viewed as an extension of the PDHG method. Moreover, in this setting the \"essentially\" fully adaptive variant adaPD$^+$ is proposed that avoids evaluating the linear operator norm by invoking a backtra",
    "path": "papers/23/01/2301.04431.json",
    "total_tokens": 895,
    "translated_title": "适应性远端算法用于凸优化在梯度的局部Lipschitz连续性下",
    "translated_abstract": "回溯线搜索是最常用的方法，用于最小化具有局部Lipschitz梯度的连续可微函数。近年来，已经证明在凸设置中完全可以避免线搜索，并且允许步长根据局部平滑度估计进行自适应调整，而无需任何回溯或评估函数值。在这项工作中，我们提出了一种自适应贴近梯度方法，adaPG，它使用局部平滑度模量的新估计，导致更少保守的步长更新，还可以处理非光滑项。这个想法被扩展到原始-对偶设置，其中提出了一种自适应三项原始-对偶算法，adaPD，它可以被看作是PDHG方法的扩展。此外，在这个设置中，提出了“本质上”完全自适应的变体adaPD$^+$，通过调用一个回溯方法，避免了评估线性算子范数。",
    "tldr": "在此工作中，我们提出了一种自适应贴近梯度方法，通过使用新的局部平滑度模量的估计，可以在凸优化中避免使用回溯线搜索，并根据局部平滑度估计自适应调整步长。",
    "en_tdlr": "In this work, we propose an adaptive proximal gradient method that avoids backtracking line search in convex optimization by utilizing novel estimates of the local smoothness modulus to adaptively adjust the step size."
}