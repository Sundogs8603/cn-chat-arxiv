{
    "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. (arXiv:2301.12005v2 [cs.LG] UPDATED)",
    "abstract": "Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without i",
    "link": "http://arxiv.org/abs/2301.12005",
    "context": "Title: EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. (arXiv:2301.12005v2 [cs.LG] UPDATED)\nAbstract: Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without i",
    "path": "papers/23/01/2301.12005.json",
    "total_tokens": 880,
    "translated_title": "EmbedDistill: 一种用于信息检索的几何知识蒸馏模型",
    "translated_abstract": "大型神经模型（如Transformers）在信息检索（IR）方面取得了最先进的性能。本文旨在改进蒸馏方法，以便在实践中实现这些模型的资源高效部署。受到我们对IR模型的师生泛化差距的理论分析的启发，我们提出了一种新的蒸馏方法，利用大型教师模型学到的查询和文档之间的相对几何关系。与现有的基于教师分数的蒸馏方法不同，我们的方法采用嵌入匹配任务，以提供更强的信号来对齐教师和学生模型的表示。此外，它还利用查询生成来探索数据流形，以减少训练数据稀缺情况下学生和教师之间的差异。此外，我们的分析还推动了学生模型的新型非对称架构，实现了更好的嵌入对齐。",
    "tldr": "这项研究提出了一种新的几何知识蒸馏方法，通过利用查询和文档的相对几何关系以及嵌入匹配任务来对齐大型教师模型和学生模型的表示，从而提高信息检索的效果。",
    "en_tdlr": "This research proposes a novel geometric knowledge distillation method that improves information retrieval by aligning the representations of large teacher models and student models through leveraging the relative geometry among queries and documents, as well as embedding matching tasks."
}