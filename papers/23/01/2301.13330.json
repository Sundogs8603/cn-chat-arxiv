{
    "title": "Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)",
    "abstract": "For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and",
    "link": "http://arxiv.org/abs/2301.13330",
    "context": "Title: Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)\nAbstract: For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and",
    "path": "papers/23/01/2301.13330.json",
    "total_tokens": 865,
    "translated_title": "混合精度神经网络量化的高效有效方法，用于更快、更节能的推断。",
    "translated_abstract": "为了实现高效的神经网络推断，希望以最简单的网络、最少的计算、存储和功耗达到最先进的准确性。将网络量化为较低的精度是简化网络的强大技术。由于网络的每一层对量化的敏感程度可能不同，混合精度量化方法选择性地调整各个层的精度，以实现任务性能（如准确性）的最小下降。为了估计层精度选择对任务性能的影响，引入了两种方法：i) 基于熵近似的层选择（EAGL）快速地使用权重分布的熵，ii) 基于准确性感知的层精度选择（ALPS）简单地依赖层精度降低后的单次迭代微调。使用EAGL和ALPS进行层精度选择，在ResNet-50、ResNet-101和...",
    "tldr": "通过混合精度量化方法，选择性调整神经网络的各个层的精度，以实现在任务性能下降最小的情况下，神经网络的快速、节能推断。",
    "en_tdlr": "By selectively adjusting the precision of individual layers through mixed precision quantization methods, efficient and energy-efficient inference can be achieved with minimal drop in task performance."
}