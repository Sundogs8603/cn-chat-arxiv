{
    "title": "Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification. (arXiv:2301.01597v2 [quant-ph] UPDATED)",
    "abstract": "Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that ",
    "link": "http://arxiv.org/abs/2301.01597",
    "context": "Title: Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification. (arXiv:2301.01597v2 [quant-ph] UPDATED)\nAbstract: Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that ",
    "path": "papers/23/01/2301.01597.json",
    "total_tokens": 1015,
    "translated_title": "揭示基于问题的量子神经网络在多类分类上的功效",
    "translated_abstract": "量子神经网络（QNNs）已成为理解物理世界的重要工具，但它们的优势和局限性尚未完全理解。一些使用特定编码方法的QNNs可以通过经典代理有效地模拟，而具有量子记忆的其他QNNs可能比经典分类器表现更好。在这里，我们系统地调查了量子神经分类器（QCs）在多类分类任务上的问题相关能力。通过对期望风险的分析，该指标综合考虑了分类器的训练损失和泛化误差，我们发现了两个关键发现：首先，训练损失主导着功效，而不是泛化能力；第二，QCs经历U形风险曲线，与深度神经分类器的双峰风险曲线相反。我们还揭示了最优QCs与Helstrom边界和等角紧框之间的内在联系。基于这些发现，我们提出了一种方法，其中我们通过量子待测试样本与最优QCs之间的最小角度，实现了基于问题的量子神经分类器的优化。",
    "tldr": "本研究揭示了基于问题的量子神经网络在多类分类任务上的功效，发现训练损失主导着其性能，与深度神经分类器的双峰风险曲线相反。此外，发现最优量子神经分类器与Helstrom边界和等角紧框之间存在内在联系，并提出了一种基于最小角度的优化方法。"
}