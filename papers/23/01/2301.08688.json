{
    "title": "Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets. (arXiv:2301.08688v2 [q-fin.TR] UPDATED)",
    "abstract": "We employ deep reinforcement learning (RL) to train an agent to successfully translate a high-frequency trading signal into a trading strategy that places individual limit orders. Based on the ABIDES limit order book simulator, we build a reinforcement learning OpenAI gym environment and utilise it to simulate a realistic trading environment for NASDAQ equities based on historic order book messages. To train a trading agent that learns to maximise its trading return in this environment, we use Deep Duelling Double Q-learning with the APEX (asynchronous prioritised experience replay) architecture. The agent observes the current limit order book state, its recent history, and a short-term directional forecast. To investigate the performance of RL for adaptive trading independently from a concrete forecasting algorithm, we study the performance of our approach utilising synthetic alpha signals obtained by perturbing forward-looking returns with varying levels of noise. Here, we find that ",
    "link": "http://arxiv.org/abs/2301.08688",
    "context": "Title: Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets. (arXiv:2301.08688v2 [q-fin.TR] UPDATED)\nAbstract: We employ deep reinforcement learning (RL) to train an agent to successfully translate a high-frequency trading signal into a trading strategy that places individual limit orders. Based on the ABIDES limit order book simulator, we build a reinforcement learning OpenAI gym environment and utilise it to simulate a realistic trading environment for NASDAQ equities based on historic order book messages. To train a trading agent that learns to maximise its trading return in this environment, we use Deep Duelling Double Q-learning with the APEX (asynchronous prioritised experience replay) architecture. The agent observes the current limit order book state, its recent history, and a short-term directional forecast. To investigate the performance of RL for adaptive trading independently from a concrete forecasting algorithm, we study the performance of our approach utilising synthetic alpha signals obtained by perturbing forward-looking returns with varying levels of noise. Here, we find that ",
    "path": "papers/23/01/2301.08688.json",
    "total_tokens": 922,
    "translated_title": "异步深度双对决Q学习在限价交易市场中的交易信号执行",
    "translated_abstract": "我们利用深度强化学习（RL）训练一个代理，将高频交易信号成功转化为能够下达独立限价订单的交易策略。基于ABIDES限价订单簿模拟器，我们构建了一个强化学习OpenAI gym环境，并利用它在基于历史订单簿消息的NASDAQ股票交易环境中进行了模拟。为了训练一个能够在此环境中最大化交易回报的交易代理，我们使用了Deep Duelling Double Q-learning与APEX（异步优先经验回放）架构。代理观察当前限价订单簿状态、其最近历史和短期方向预测。为了独立地研究适应性交易的RL性能而不涉及具体的预测算法，我们使用通过扰动前瞻收益获得的合成alpha信号来研究我们方法的性能，这些信号具有不同级别的噪声。",
    "tldr": "该论文利用深度强化学习训练了一个代理，将高频交易信号转化为交易策略，并使用异步双对决Q学习进行交易信号执行。该方法独立研究了适应性交易的性能以及与具体的预测算法无关的影响。",
    "en_tdlr": "This paper utilizes deep reinforcement learning to train an agent that translates high-frequency trading signals into trading strategies, and employs asynchronous double duelling Q-learning for signal execution. It independently investigates the performance of adaptive trading and its impact, regardless of specific forecasting algorithms."
}