{
    "title": "Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. (arXiv:2301.12714v2 [cs.LG] UPDATED)",
    "abstract": "We propose A-Crab (Actor-Critic Regularized by Average Bellman error), a new practical algorithm for offline reinforcement learning (RL) in complex environments with insufficient data coverage. Our algorithm combines the marginalized importance sampling framework with the actor-critic paradigm, where the critic returns evaluations of the actor (policy) that are pessimistic relative to the offline data and have a small average (importance-weighted) Bellman error. Compared to existing methods, our algorithm simultaneously offers a number of advantages: (1) It achieves the optimal statistical rate of $1/\\sqrt{N}$ -- where $N$ is the size of offline dataset -- in converging to the best policy covered in the offline dataset, even when combined with general function approximators. (2) It relies on a weaker average notion of policy coverage (compared to the $\\ell_\\infty$ single-policy concentrability) that exploits the structure of policy visitations. (3) It outperforms the data-collection be",
    "link": "http://arxiv.org/abs/2301.12714",
    "context": "Title: Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. (arXiv:2301.12714v2 [cs.LG] UPDATED)\nAbstract: We propose A-Crab (Actor-Critic Regularized by Average Bellman error), a new practical algorithm for offline reinforcement learning (RL) in complex environments with insufficient data coverage. Our algorithm combines the marginalized importance sampling framework with the actor-critic paradigm, where the critic returns evaluations of the actor (policy) that are pessimistic relative to the offline data and have a small average (importance-weighted) Bellman error. Compared to existing methods, our algorithm simultaneously offers a number of advantages: (1) It achieves the optimal statistical rate of $1/\\sqrt{N}$ -- where $N$ is the size of offline dataset -- in converging to the best policy covered in the offline dataset, even when combined with general function approximators. (2) It relies on a weaker average notion of policy coverage (compared to the $\\ell_\\infty$ single-policy concentrability) that exploits the structure of policy visitations. (3) It outperforms the data-collection be",
    "path": "papers/23/01/2301.12714.json",
    "total_tokens": 986,
    "translated_title": "权重重要性加权的演员-评论家算法用于优化保守型离线强化学习",
    "translated_abstract": "我们提出了A-Crab（平均贝尔曼误差正则化的演员-评论家算法），这是一种针对数据覆盖不足的复杂环境中离线强化学习（RL）的实用新算法。我们的算法将边际化重要性采样框架与演员-评论家范式相结合，其中评论家返回相对于离线数据悲观的演员（策略）评估，并具有小的平均（权重重要性加权的）贝尔曼误差。与现有方法相比，我们的算法同时具有以下优势：（1）在收敛到离线数据集中最佳策略时，即使与通用函数逼近器结合使用，也能达到最佳统计速率$1/\\sqrt{N}$，其中$N$是离线数据集的大小。（2）它依赖于较弱的策略覆盖平均概念（与$l_\\infty$单策略集中性相比），利用策略访问结构。（3）它优于数据收集",
    "tldr": "这里是中文总结出的一句话要点 这篇论文提出了一种权重重要性加权的演员-评论家算法，用于解决数据覆盖不足的复杂环境下的离线强化学习问题，相较于现有方法具有更好的收敛速率和策略覆盖概念。",
    "en_tdlr": "这里是英文总结出的一句话要点 This paper proposes an importance weighted actor-critic algorithm for addressing the problem of insufficient data coverage in complex environments for offline reinforcement learning, which achieves better convergence rate and policy coverage compared to existing methods."
}