{
    "title": "Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)",
    "abstract": "This paper explores deep latent variable models for semi-supervised paraphrase generation, where the missing target pair for unlabelled data is modelled as a latent paraphrase sequence. We present a novel unsupervised model named variational sequence auto-encoding reconstruction (VSAR), which performs latent sequence inference given an observed text. To leverage information from text pairs, we additionally introduce a novel supervised model we call dual directional learning (DDL), which is designed to integrate with our proposed VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning. Still, the combined model suffers from a cold-start problem. To further combat this issue, we propose an improved weight initialisation solution, leading to a novel two-stage training scheme we call knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the combined model yields competitive performance against the state-of-the-art supervised basel",
    "link": "http://arxiv.org/abs/2301.02275",
    "context": "Title: Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)\nAbstract: This paper explores deep latent variable models for semi-supervised paraphrase generation, where the missing target pair for unlabelled data is modelled as a latent paraphrase sequence. We present a novel unsupervised model named variational sequence auto-encoding reconstruction (VSAR), which performs latent sequence inference given an observed text. To leverage information from text pairs, we additionally introduce a novel supervised model we call dual directional learning (DDL), which is designed to integrate with our proposed VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning. Still, the combined model suffers from a cold-start problem. To further combat this issue, we propose an improved weight initialisation solution, leading to a novel two-stage training scheme we call knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the combined model yields competitive performance against the state-of-the-art supervised basel",
    "path": "papers/23/01/2301.02275.json",
    "total_tokens": 976,
    "translated_title": "语言作为潜在序列：用于半监督释义生成的深度潜变量模型",
    "translated_abstract": "本文探讨了用于半监督释义生成的深度潜变量模型，其中未标记数据的缺失目标对被建模为潜在释义序列。我们提出了一种名为变分序列自编码重构（VSAR）的新型无监督模型，该模型可在给定观察文本的情况下进行潜在序列推断。为了利用文本对的信息，我们还引入了一种名为双向学习（DDL）的新型监督模型，该模型旨在与我们提出的VSAR模型结合使用。将VSAR与DDL（DDL+VSAR）结合起来使我们能够进行半监督学习。然而，组合模型存在冷启动问题。为了进一步解决这个问题，我们提出了一种改进的权重初始化解决方案，从而导致一个名为知识增强学习（KRL）的新型两阶段训练方案。我们的实证评估表明，组合模型在性能上与最先进的有监督基线模型竞争力持平。",
    "tldr": "本文提出了用于半监督释义生成的深度潜变量模型，通过将未标记数据的缺失目标对建模为潜在释义序列，并结合双向学习和改进的权重初始化方案进行训练，实验结果表明这个模型在性能上与最先进的有监督基线模型有竞争力。",
    "en_tdlr": "This paper proposes deep latent variable models for semi-supervised paraphrase generation, modeling the missing target pair for unlabelled data as a latent paraphrase sequence. By combining dual directional learning and an improved weight initialization scheme, the proposed model achieves competitive performance against state-of-the-art supervised baselines."
}