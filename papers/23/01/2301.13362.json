{
    "title": "Optimizing DDPM Sampling with Shortcut Fine-Tuning. (arXiv:2301.13362v3 [cs.LG] UPDATED)",
    "abstract": "In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting i",
    "link": "http://arxiv.org/abs/2301.13362",
    "context": "Title: Optimizing DDPM Sampling with Shortcut Fine-Tuning. (arXiv:2301.13362v3 [cs.LG] UPDATED)\nAbstract: In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting i",
    "path": "papers/23/01/2301.13362.json",
    "total_tokens": 916,
    "translated_title": "使用Shortcut Fine-Tuning优化DDPM采样",
    "translated_abstract": "本研究提出了Shortcut Fine-Tuning（SFT），这是一种新的方法，用于解决预训练去噪扩散概率模型（DDPMs）的快速采样挑战。SFT提倡通过直接最小化积分概率度量（IPM）来对DDPM采样器进行微调，而不是学习向后扩散过程。这使采样器能够发现一条替代的更高效的采样捷径，偏离向后扩散过程。通过控制角度的启示，我们提出了一种新算法SFT-PG：使用Policy Gradient进行的Shortcut Fine-Tuning，并证明在某些假设下，扩散模型相对于IPM的梯度下降等价于执行Policy Gradient。据我们所知，这是首次尝试利用强化学习（RL）方法来训练扩散模型。通过实证评估，我们证明了我们的微调方法可以进一步增强现有的快速DDPM采样器，从而导致显着的加速和质量提高。",
    "tldr": "本文提出了Shortcut Fine-Tuning（SFT）算法，利用直接最小化积分概率度量（IPM）来对DDPM采样器进行微调，从而有效提高DDPM采样效率和质量。",
    "en_tdlr": "This paper proposes the Shortcut Fine-Tuning (SFT) algorithm, which fine-tunes DDPM samplers by directly minimizing Integral Probability Metrics (IPM), leading to significant improvements in both efficiency and quality. This is the first attempt to use reinforcement learning methods to train diffusion models."
}