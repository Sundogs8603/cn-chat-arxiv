{
    "title": "Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)",
    "abstract": "Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S",
    "link": "http://arxiv.org/abs/2301.09164",
    "context": "Title: Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)\nAbstract: Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S",
    "path": "papers/23/01/2301.09164.json",
    "total_tokens": 966,
    "translated_title": "自监督学习和动态计算之间的统一协同作用",
    "translated_abstract": "在资源受限的工业环境中，计算昂贵的训练策略使得自监督学习（SSL）变得不切实际。为了获取轻量级模型，通常会使用知识蒸馏（KD）、动态计算（DC）和剪枝等技术，这通常涉及到对大型预训练模型进行多次微调（或蒸馏步骤），使得计算更具挑战性。在这项工作中，我们提出了自监督学习和动态计算范式之间相互作用的新视角。特别地，我们展示了在自监督学习的设置中，同时从零开始学习密集和门控子网络是可行的，而不需要额外的微调或剪枝步骤。预训练期间密集和门控编码器的共同演化提供了良好的准确性与效率之间的平衡，从而为应用特定的工业环境提供了通用且多功能的架构。我们在包括CIFAR-10/100等多个图像分类基准上进行了大量实验。",
    "tldr": "本文提出了自监督学习和动态计算之间相互作用的新视角。通过在自监督学习的设置中同时学习密集和门控子网络，无需额外的微调或剪枝步骤，可以获得一个通用且高效的架构，适用于资源受限的工业环境。",
    "en_tdlr": "This paper presents a new perspective on the interplay between self-supervised learning and dynamic computation. By simultaneously learning dense and gated sub-networks in a self-supervised learning setting without the need for additional fine-tuning or pruning steps, a generic and efficient architecture for resource constrained industrial settings is obtained."
}