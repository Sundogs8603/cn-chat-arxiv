{
    "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)",
    "abstract": "Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to $3\\times$ for t",
    "link": "http://arxiv.org/abs/2301.12017",
    "context": "Title: Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)\nAbstract: Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to $3\\times$ for t",
    "path": "papers/23/01/2301.12017.json",
    "total_tokens": 996,
    "translated_title": "理解Transformer模型的INT4量化：延迟速度提升、可组合性和故障案例",
    "translated_abstract": "鉴于Transformer基于语言模型的高计算和内存成本，提高其部署效率一直是一个挑战。尽管最近已经证明了INT8量化在减少内存成本和延迟方面的有效性，同时还保持了模型的准确性，但我们是否可以利用INT4（可以使硬件峰值吞吐量增加一倍）来实现进一步的延迟改进还不清楚。在这项研究中，我们探讨了在语言模型中采用INT4权重和激活（W4A4）量化的可行性。我们的发现表明，对于仅编码器和编码器-解码器模型，W4A4量化引入的准确性降低可以忽略不计，但对于仅解码器模型而言，会导致显著的准确性下降。为了实现使用W4A4的性能增益，我们开发了一个高度优化的端到端W4A4编码器推断管道，支持不同的量化策略。我们的INT4管道在面向延迟的场景下的速度可以提高8.5倍，在其他场景下最多可以提高3倍。",
    "tldr": "本文研究了在语言模型中采用INT4权重和激活量化的可行性，并开发了高度优化的W4A4编码器推断管道，支持不同的量化策略。使用W4A4可以实现模型在延迟方面的显著提高。"
}