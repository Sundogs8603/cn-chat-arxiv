{
    "title": "Overcoming Simplicity Bias in Deep Networks using a Feature Sieve. (arXiv:2301.13293v3 [cs.LG] UPDATED)",
    "abstract": "Simplicity bias is the concerning tendency of deep networks to over-depend on simple, weakly predictive features, to the exclusion of stronger, more complex features. This is exacerbated in real-world applications by limited training data and spurious feature-label correlations, leading to biased, incorrect predictions. We propose a direct, interventional method for addressing simplicity bias in DNNs, which we call the feature sieve. We aim to automatically identify and suppress easily-computable spurious features in lower layers of the network, thereby allowing the higher network levels to extract and utilize richer, more meaningful representations. We provide concrete evidence of this differential suppression & enhancement of relevant features on both controlled datasets and real-world images, and report substantial gains on many real-world debiasing benchmarks (11.4% relative gain on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior knowledge of spurious attributes",
    "link": "http://arxiv.org/abs/2301.13293",
    "context": "Title: Overcoming Simplicity Bias in Deep Networks using a Feature Sieve. (arXiv:2301.13293v3 [cs.LG] UPDATED)\nAbstract: Simplicity bias is the concerning tendency of deep networks to over-depend on simple, weakly predictive features, to the exclusion of stronger, more complex features. This is exacerbated in real-world applications by limited training data and spurious feature-label correlations, leading to biased, incorrect predictions. We propose a direct, interventional method for addressing simplicity bias in DNNs, which we call the feature sieve. We aim to automatically identify and suppress easily-computable spurious features in lower layers of the network, thereby allowing the higher network levels to extract and utilize richer, more meaningful representations. We provide concrete evidence of this differential suppression & enhancement of relevant features on both controlled datasets and real-world images, and report substantial gains on many real-world debiasing benchmarks (11.4% relative gain on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior knowledge of spurious attributes",
    "path": "papers/23/01/2301.13293.json",
    "total_tokens": 963,
    "translated_title": "使用特征筛选克服深度神经网络中的简单偏差",
    "translated_abstract": "简单偏差是深度神经网络倾向于依赖简单且预测性较弱特征的令人担忧的趋势，从而排除更强、更复杂的特征。在真实世界应用中，由于训练数据有限和虚假特征标签相关性，导致了偏向性、不正确的预测。我们提出了一种直接、干预深度神经网络中简单偏差的方法，称为特征筛选。我们的目标是自动识别和抑制网络较低层的易计算虚假特征，从而让更高层的网络提取和利用更丰富、更有意义的表示。我们提供了控制数据集和真实世界图像上有关有效特征不同压制和增强的具体证据，并在许多真实世界去偏差基准测试中报告了显著性提高（Imagenet-A相对增益11.4％；BAR 3.2％等）。关键是，我们不依赖虚假属性的先验知识。",
    "tldr": "提出了一种特征筛选方法，通过抑制网络较低层易计算虚假特征，使得更高层的网络提取和利用更丰富、更有意义的特征表示，从而克服了深度神经网络中的简单偏差。"
}