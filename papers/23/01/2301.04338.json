{
    "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks. (arXiv:2301.04338v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks under the absence of original training data, previous work has proposed a data-free knowledge distillation method where synthetic data are generated using a generator model trained adversarially against the student model. These synthetic data and their labels predicted by the teacher model are then used to train the student model. In this study, we investigate the behavior of various synthetic data generation methods and propose a new synthetic data generation strategy that directly optimizes for a large",
    "link": "http://arxiv.org/abs/2301.04338",
    "context": "Title: Synthetic data generation method for data-free knowledge distillation in regression neural networks. (arXiv:2301.04338v2 [cs.LG] UPDATED)\nAbstract: Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks under the absence of original training data, previous work has proposed a data-free knowledge distillation method where synthetic data are generated using a generator model trained adversarially against the student model. These synthetic data and their labels predicted by the teacher model are then used to train the student model. In this study, we investigate the behavior of various synthetic data generation methods and propose a new synthetic data generation strategy that directly optimizes for a large",
    "path": "papers/23/01/2301.04338.json",
    "total_tokens": 846,
    "tldr": "本文提出了一个针对回归任务的无数据知识蒸馏方法，利用生成对抗网络生成合成数据，通过原师生网络的标签训练学生网络。研究了不同的生成方法，提出了新的生成策略，可以直接优化学生网络的性能。",
    "translated_title": "一种用于回归神经网络中数据无关知识蒸馏的合成数据生成方法",
    "translated_abstract": "知识蒸馏是一种将大小不同的神经网络压缩至效果相近的技术。现有的知识蒸馏方法主要适用于分类任务，并需要使用用于训练原模型的数据。为了解决回归任务下无原始数据的知识蒸馏问题，先前的研究提出了一种数据无关知识蒸馏方法，其中使用生成对抗网络生成合成数据，并用原模型预测的标签来训练学生模型。在本研究中，我们研究了各种合成数据生成方法的行为，并提出了一种新的合成数据生成策略，该策略直接优化了学生模型的性能。",
    "en_tdlr": "This paper proposes a synthetic data generation method for data-free knowledge distillation in regression neural networks, where synthetic data are generated through generative adversarial networks and used to train the student model with the teacher model's predicted labels. The study investigates various synthetic data generation methods and proposes a new strategy that directly optimizes the performance of the student model."
}