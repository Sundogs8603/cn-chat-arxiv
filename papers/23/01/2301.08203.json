{
    "title": "An SDE for Modeling SAM: Theory and Insights. (arXiv:2301.08203v3 [cs.LG] UPDATED)",
    "abstract": "We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments.",
    "link": "http://arxiv.org/abs/2301.08203",
    "context": "Title: An SDE for Modeling SAM: Theory and Insights. (arXiv:2301.08203v3 [cs.LG] UPDATED)\nAbstract: We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments.",
    "path": "papers/23/01/2301.08203.json",
    "total_tokens": 843,
    "translated_title": "用于建模SAM的随机微分方程：理论与洞见",
    "translated_abstract": "我们研究了SAM（Sharpness-Aware Minimization）优化器，由于其在比更传统的随机梯度下降变体上表现出的更高性能，最近吸引了很多人的关注。我们的主要贡献是推导出连续时间模型（以SDEs的形式）来处理SAM及其两个变体，包括全批量和小批量设置。我们证明了这些SDE是离散时间算法的严格逼近（以弱意义，与学习速率线性缩放）。利用这些模型，我们解释了为什么SAM更喜欢平坦的极小值而非尖峰值-—通过展示SAM在最小化具有Hessian相关噪声结构的隐式正则化损失函数。最后，我们证明了SAM在某些现实条件下会被吸引到鞍点。我们的理论结果获得了详细实验证明。",
    "tldr": "我们推导了适用于SAM及其变体的连续时间模型，并解释了为什么SAM更喜欢平坦的极小值而非尖峰值，同时证明了SAM在某些现实条件下会被吸引到鞍点。",
    "en_tdlr": "We derive continuous-time models for the SAM optimizer and provide insights on why it prefers flat minima, also proving that it can be attracted to saddle points under certain conditions."
}