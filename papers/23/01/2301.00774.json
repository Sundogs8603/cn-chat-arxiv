{
    "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. (arXiv:2301.00774v3 [cs.LG] UPDATED)",
    "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
    "link": "http://arxiv.org/abs/2301.00774",
    "context": "Title: SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. (arXiv:2301.00774v3 [cs.LG] UPDATED)\nAbstract: We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
    "path": "papers/23/01/2301.00774.json",
    "total_tokens": 786,
    "translated_title": "SparseGPT：无需重新训练即可将大规模语言模型精确剪枝至至少50%稀疏度的方法",
    "translated_abstract": "我们首次展示了一种名为SparseGPT的新的剪枝方法，能够高效准确地应用于大规模GPT家族模型，而无需重新训练，将这些模型至少剪枝50%的稀疏度，且准确度下降很小。我们能够在不到4.5小时内即可在最大的开源模型OPT-175B和BLOOM-176B上执行SparseGPT，并在几乎不增加困惑度的情况下实现60%的无结构稀疏度：这些模型的超过1000亿个参数可以在推理时忽略。",
    "tldr": "SparseGPT通过一种高效、精确的方法，能使大规模语言模型在不重新训练的情况下至少剪枝50%的稀疏度，而准确度下降很小。",
    "en_tdlr": "SparseGPT provides an efficient and accurate method to prune large-scale language models to at least 50% sparsity without retraining, with minimal loss of accuracy."
}