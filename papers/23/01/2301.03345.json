{
    "title": "Latent Spectral Regularization for Continual Learning. (arXiv:2301.03345v3 [cs.LG] UPDATED)",
    "abstract": "While biological intelligence grows organically as new knowledge is gathered throughout life, Artificial Neural Networks forget catastrophically whenever they face a changing training data distribution. Rehearsal-based Continual Learning (CL) approaches have been established as a versatile and reliable solution to overcome this limitation; however, sudden input disruptions and memory constraints are known to alter the consistency of their predictions. We study this phenomenon by investigating the geometric characteristics of the learner's latent space and find that replayed data points of different classes increasingly mix up, interfering with classification. Hence, we propose a geometric regularizer that enforces weak requirements on the Laplacian spectrum of the latent space, promoting a partitioning behavior. We show that our proposal, called Continual Spectral Regularizer (CaSpeR), can be easily combined with any rehearsal-based CL approach and improves the performance of SOTA meth",
    "link": "http://arxiv.org/abs/2301.03345",
    "context": "Title: Latent Spectral Regularization for Continual Learning. (arXiv:2301.03345v3 [cs.LG] UPDATED)\nAbstract: While biological intelligence grows organically as new knowledge is gathered throughout life, Artificial Neural Networks forget catastrophically whenever they face a changing training data distribution. Rehearsal-based Continual Learning (CL) approaches have been established as a versatile and reliable solution to overcome this limitation; however, sudden input disruptions and memory constraints are known to alter the consistency of their predictions. We study this phenomenon by investigating the geometric characteristics of the learner's latent space and find that replayed data points of different classes increasingly mix up, interfering with classification. Hence, we propose a geometric regularizer that enforces weak requirements on the Laplacian spectrum of the latent space, promoting a partitioning behavior. We show that our proposal, called Continual Spectral Regularizer (CaSpeR), can be easily combined with any rehearsal-based CL approach and improves the performance of SOTA meth",
    "path": "papers/23/01/2301.03345.json",
    "total_tokens": 908,
    "translated_title": "潜在谱规范化用于持续学习的研究",
    "translated_abstract": "尽管生物智能在一生中随着获取新知识的积累而有机地增长，但人工神经网络在面对不断变化的训练数据分布时容易遭受灾难性的遗忘。基于重演的持续学习（CL）方法已被证明是克服这个限制的一种多功能可靠解决方案；然而，突然的输入中断和内存限制会改变它们预测的一致性。我们通过研究学习者潜在空间的几何特征来研究这一现象，并发现重新演示的不同类别的数据点越来越混合，干扰了分类。因此，我们提出了一种几何规范化方法，它对潜在空间的拉普拉斯谱施加了较弱的要求，促进了分区行为。我们证明了我们的方法，称为持续谱规范化器（CaSpeR），可以与任何基于重演的CL方法轻松组合，并提高了SOTA方法的性能。",
    "tldr": "本研究提出了一种持续谱规范化器（CaSpeR），通过对潜在空间的几何特征进行规范化，在面对不断变化的训练数据分布时改善了基于重演的持续学习方法的性能。",
    "en_tdlr": "This study proposes a Continual Spectral Regularizer (CaSpeR) that improves the performance of rehearsal-based Continual Learning approaches by regularizing the geometric characteristics of the latent space when facing changing training data distributions."
}