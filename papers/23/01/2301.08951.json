{
    "title": "Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v3 [cs.CV] UPDATED)",
    "abstract": "When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when an object is completely occluded from certain viewpoints. Meanwhile, humans are able to imagine novel views after observing multiple viewpoints. Recent remarkable advances in multi-view object-centric learning still leaves some unresolved problems: 1) The shapes of partially or completely occluded objects can not be well reconstructed. 2) The novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit rules in view representations. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of an object accurately, we enhance the disentanglement between the latent representations of objects and views, where the latent representations of time-conditioned views are jointly inferred with a Transformer and then are input to a sequential extension of Slot Attention",
    "link": "http://arxiv.org/abs/2301.08951",
    "context": "Title: Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v3 [cs.CV] UPDATED)\nAbstract: When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when an object is completely occluded from certain viewpoints. Meanwhile, humans are able to imagine novel views after observing multiple viewpoints. Recent remarkable advances in multi-view object-centric learning still leaves some unresolved problems: 1) The shapes of partially or completely occluded objects can not be well reconstructed. 2) The novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit rules in view representations. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of an object accurately, we enhance the disentanglement between the latent representations of objects and views, where the latent representations of time-conditioned views are jointly inferred with a Transformer and then are input to a sequential extension of Slot Attention",
    "path": "papers/23/01/2301.08951.json",
    "total_tokens": 1092,
    "translated_title": "基于对象中心表征的视频分解和预测的时间条件生成建模",
    "translated_abstract": "人类在从多个视角感知世界时，即使某个对象被完全遮挡，也能以组合方式理解完整物体。与此同时，人类能够在观察多个视角之后想象新视角。最近多视图对象中心学习的显着进步仍存在一些未解决的问题：1）部分或完全遮挡对象的形状无法被准确重建。2）新视角预测依赖于昂贵的视角注释，而不是视图表征中的隐式规则。本文提出了一种用于视频的时间条件生成模型。为了准确重建对象的完整形状，我们增强了对象和视图的潜在表征之间的解耦，其中时间条件的视图的潜在表征与Transformer一起联合推断，然后输入到Slot Attention Networks (SANs)的顺序扩展中。我们进一步提出了从预训练模型中学习隐式视角规则的方法，从而消除了显式视角注释的需要。实验结果表明，我们的方法在部分和完全遮挡对象完成和新视角预测任务中优于现有方法。",
    "tldr": "本文提出了一种基于时间条件生成模型的视频分解和预测方法，采用提升的对象和视图的潜在表征之间的解耦技术以及从预训练模型中学习隐式视角规则的方法解决了现有方法存在的部分或完全遮挡对象的形状无法被准确重建和新视角预测依赖于昂贵的视角注释的问题。",
    "en_tdlr": "This paper proposes a time-conditioned generative model for video decomposition and prediction, which uses enhanced disentanglement between object and view latent representations, and learns implicit viewpoint rules from a pre-trained model to address the issues of incomplete object reconstruction in occluded scenes and expensive viewpoint annotations for novel viewpoint prediction found in existing methods. Experiment results show that the proposed method outperforms state-of-the-art methods in occluded object completion and novel viewpoint prediction."
}