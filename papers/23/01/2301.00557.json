{
    "title": "Learning to Maximize Mutual Information for Dynamic Feature Selection. (arXiv:2301.00557v2 [cs.LG] UPDATED)",
    "abstract": "Feature selection helps reduce data acquisition costs in ML, but the standard approach is to train models with static feature subsets. Here, we consider the dynamic feature selection (DFS) problem where a model sequentially queries features based on the presently available information. DFS is often addressed with reinforcement learning, but we explore a simpler approach of greedily selecting features based on their conditional mutual information. This method is theoretically appealing but requires oracle access to the data distribution, so we develop a learning approach based on amortized optimization. The proposed method is shown to recover the greedy policy when trained to optimality, and it outperforms numerous existing feature selection methods in our experiments, thus validating it as a simple but powerful approach for this problem.",
    "link": "http://arxiv.org/abs/2301.00557",
    "context": "Title: Learning to Maximize Mutual Information for Dynamic Feature Selection. (arXiv:2301.00557v2 [cs.LG] UPDATED)\nAbstract: Feature selection helps reduce data acquisition costs in ML, but the standard approach is to train models with static feature subsets. Here, we consider the dynamic feature selection (DFS) problem where a model sequentially queries features based on the presently available information. DFS is often addressed with reinforcement learning, but we explore a simpler approach of greedily selecting features based on their conditional mutual information. This method is theoretically appealing but requires oracle access to the data distribution, so we develop a learning approach based on amortized optimization. The proposed method is shown to recover the greedy policy when trained to optimality, and it outperforms numerous existing feature selection methods in our experiments, thus validating it as a simple but powerful approach for this problem.",
    "path": "papers/23/01/2301.00557.json",
    "total_tokens": 760,
    "translated_title": "学习最大化互信息用于动态特征选择",
    "translated_abstract": "特征选择有助于减少机器学习中的数据采集成本，但标准方法是使用静态特征子集来训练模型。本文考虑了动态特征选择（DFS）问题，即模型根据现有信息顺序查询特征。DFS通常采用强化学习方法处理，但本文探索了一种更简单的方法，即基于条件互信息贪婪地选择特征。虽然这种方法在理论上十分有吸引力，但需要对数据分布进行完全访问。因此，我们提出了一种基于摊销优化的学习方法。实验证明，该方法在训练到最优状态后可以恢复贪心策略，并且在我们的实验中优于许多现有的特征选择方法，从而验证其作为这个问题的简单而强大的解决方法。",
    "tldr": "本文提出一种基于互信息的动态特征选择方法，使用摊销优化进行学习。在实验中取得了比现有方法更好效果。",
    "en_tdlr": "This paper proposes a dynamic feature selection method based on mutual information, and trains it using amortized optimization. The approach outperforms existing methods in experiments."
}