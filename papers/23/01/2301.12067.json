{
    "title": "Learning Optimal Features via Partial Invariance. (arXiv:2301.12067v2 [cs.LG] UPDATED)",
    "abstract": "Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via $\\textit{partial invariance}$. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions.",
    "link": "http://arxiv.org/abs/2301.12067",
    "context": "Title: Learning Optimal Features via Partial Invariance. (arXiv:2301.12067v2 [cs.LG] UPDATED)\nAbstract: Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via $\\textit{partial invariance}$. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions.",
    "path": "papers/23/01/2301.12067.json",
    "total_tokens": 832,
    "translated_title": "通过部分不变性学习最优特征",
    "translated_abstract": "学习对于分布变化具有鲁棒性的模型是其在实际应用中的一个重点关注点。不变风险最小化（IRM）是一种流行的框架，旨在从多个环境中学习鲁棒模型。IRM的成功需要一个重要的假设：潜在的因果机制/特征在多个环境中保持不变。当该假设不成立时，我们证明IRM可能会导致预测器过度约束，并提出了通过 $\\textit{部分不变性}$ 来缓解此问题。本文理论上突出了IRM的次优性，并展示了如何从训练域的一个分区中学习以提高不变模型。我们进行了多个实验，包括在线性设置和深度神经网络上，涉及语言和图像数据集，验证了我们的结论。",
    "tldr": "本文证明了不变风险最小化（IRM）的次优性，并提出了通过部分不变性来缓解此问题的方法，同时展示了从训练域的一个分区中学习以提高不变性模型的有效性。",
    "en_tdlr": "This paper proves the sub-optimality of Invariant Risk Minimization (IRM) and proposes a solution through partial invariance to improve invariant models. It also demonstrates the effectiveness of learning from a partition of training domains to enhance invariant models, with experiments conducted on linear and deep neural network settings and language and image datasets."
}