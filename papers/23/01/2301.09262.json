{
    "title": "AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)",
    "abstract": "Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct",
    "link": "http://arxiv.org/abs/2301.09262",
    "context": "Title: AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)\nAbstract: Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct",
    "path": "papers/23/01/2301.09262.json",
    "total_tokens": 916,
    "translated_title": "AttMEMO: 在大内存系统上利用记忆化加速Transformers",
    "translated_abstract": "Transformer模型因其优越的推理准确性和推理吞吐量而受到欢迎。然而，由于Transformer是计算密集型的，导致推理时间较长。现有的Transformer推理加速工作存在限制，这些限制要么是由于修改Transformer架构，要么是需要专门的硬件。本文提出了使用记忆化加速Transformer模型的机会，而不涉及以上限制。基于这样的独特观察，即在推理序列内Attention计算中存在丰富的相似性，我们构建了一个利用新兴的大内存系统的记忆化数据库。我们引入了一种新的嵌入技术来查找语义上相似的输入，以识别计算相似性。我们还引入了一系列技术，如内存映射和选择性记忆化，以避免内存复制和不必要的开销。我们使得推理延迟降低了22%，而且内存需求适中。我们的方法可以很容易地适用于各种Transformer模型，而且无需进行重要修改。",
    "tldr": "本文提出一种利用记忆技术加速自注意力机制的Transformer模型的推理过程的方法，该方法可以在不需要修改模型架构或使用特殊硬件的情况下进行，并可以使推理延迟降低22%。",
    "en_tdlr": "This paper proposes an approach to accelerate the inference process of transformer models using memoization techniques on self-attention mechanism, without the need for modifying the model architecture or using specialized hardware, resulting in a 22% reduction in inference latency."
}