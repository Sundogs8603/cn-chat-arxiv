{
    "title": "Towards Models that Can See and Read. (arXiv:2301.07389v2 [cs.CV] UPDATED)",
    "abstract": "Visual Question Answering (VQA) and Image Captioning (CAP), which are among the most popular vision-language tasks, have analogous scene-text versions that require reasoning from the text in the image. Despite their obvious resemblance, the two are treated independently and, as we show, yield task-specific methods that can either see or read, but not both. In this work, we conduct an in-depth analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text approach, which grants existing multimodal architectures scene-text understanding capabilities. Specifically, we treat scene-text information as an additional modality, fusing it with any pretrained encoder-decoder-based architecture via designated modules. Thorough experiments reveal that UniTNT leads to the first single model that successfully handles both task types. Moreover, we show that scene-text understanding capabilities can boost vision-language models' performance on general VQA and CAP by up to 2.69% and 0.6 CIDEr,",
    "link": "http://arxiv.org/abs/2301.07389",
    "context": "Title: Towards Models that Can See and Read. (arXiv:2301.07389v2 [cs.CV] UPDATED)\nAbstract: Visual Question Answering (VQA) and Image Captioning (CAP), which are among the most popular vision-language tasks, have analogous scene-text versions that require reasoning from the text in the image. Despite their obvious resemblance, the two are treated independently and, as we show, yield task-specific methods that can either see or read, but not both. In this work, we conduct an in-depth analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text approach, which grants existing multimodal architectures scene-text understanding capabilities. Specifically, we treat scene-text information as an additional modality, fusing it with any pretrained encoder-decoder-based architecture via designated modules. Thorough experiments reveal that UniTNT leads to the first single model that successfully handles both task types. Moreover, we show that scene-text understanding capabilities can boost vision-language models' performance on general VQA and CAP by up to 2.69% and 0.6 CIDEr,",
    "path": "papers/23/01/2301.07389.json",
    "total_tokens": 887,
    "translated_title": "走向既能看又能读的模型",
    "translated_abstract": "视觉问答（VQA）和图像字幕生成（CAP）是最受欢迎的视觉语言任务之一，它们有着类似的场景文本版本，需要从图像中的文本进行推理。尽管它们明显相似，但两者独立处理，产生可以看或读但不能两者兼备的专门方法。在这项工作中，我们对这种现象进行了深入分析，并建议UniTNT，一种统一的文本-非文本方法，为现有的多模态架构提供场景文本理解能力。具体而言，我们将场景文本信息视为一种额外的模态，并通过指定的模块将其与任何预训练的编码器-解码器架构融合在一起。彻底的实验表明，UniTNT是第一个成功处理两种任务类型的单一模型。此外，我们证明场景文本的理解能力可以将视觉语言模型在一般VQA和CAP上的性能提高高达2.69％和0.6 CIDEr。",
    "tldr": "本文提出了UniTNT模型，该模型能兼顾场景文本和图像的理解，通过与先前的模态融合提高了图像问题回答和图像字幕生成任务的性能。",
    "en_tdlr": "This paper proposes a UniTNT model that can handle both scene-text understanding and image understanding tasks by fusing text information as an additional modality, resulting in improved performance in Visual Question Answering and Image Captioning."
}