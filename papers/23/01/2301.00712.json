{
    "title": "Bilevel Optimization without Lower-level Strong Convexity from the Hyper-Objective Perspective. (arXiv:2301.00712v2 [math.OC] UPDATED)",
    "abstract": "Bilevel optimization reveals the inner structure of otherwise oblique optimization problems, such as hyperparameter tuning and meta-learning. A common goal in bilevel optimization is to find stationary points of the hyper-objective function. Although this hyper-objective approach is widely used, its theoretical properties have not been thoroughly investigated in cases where the lower-level functions lack strong convexity. In this work, we take a step forward and study the hyper-objective approach without the typical lower-level strong convexity assumption. Our hardness results show that the hyper-objective of general convex lower-level functions can be intractable either to evaluate or to optimize. To tackle this challenge, we introduce the gradient dominant condition, which strictly relaxes the strong convexity assumption by allowing the lower-level solution set to be non-singleton. Under the gradient dominant condition, we propose the Inexact Gradient-Free Method (IGFM), which uses t",
    "link": "http://arxiv.org/abs/2301.00712",
    "context": "Title: Bilevel Optimization without Lower-level Strong Convexity from the Hyper-Objective Perspective. (arXiv:2301.00712v2 [math.OC] UPDATED)\nAbstract: Bilevel optimization reveals the inner structure of otherwise oblique optimization problems, such as hyperparameter tuning and meta-learning. A common goal in bilevel optimization is to find stationary points of the hyper-objective function. Although this hyper-objective approach is widely used, its theoretical properties have not been thoroughly investigated in cases where the lower-level functions lack strong convexity. In this work, we take a step forward and study the hyper-objective approach without the typical lower-level strong convexity assumption. Our hardness results show that the hyper-objective of general convex lower-level functions can be intractable either to evaluate or to optimize. To tackle this challenge, we introduce the gradient dominant condition, which strictly relaxes the strong convexity assumption by allowing the lower-level solution set to be non-singleton. Under the gradient dominant condition, we propose the Inexact Gradient-Free Method (IGFM), which uses t",
    "path": "papers/23/01/2301.00712.json",
    "total_tokens": 911,
    "translated_abstract": "双层优化揭示出原本晦涩的优化问题内部结构，比如超参数调整和元学习。双层优化的一个常见目标是找到超目标函数的驻点。尽管这种超目标方法被广泛使用，但在下层函数缺乏强凸性的情况下，它的理论性质尚未得到深入研究。在这项工作中，我们向前迈出一步，并研究了不需要典型下层强凸性假设的超目标方法。我们的难度结果表明，普通凸下层函数的超目标可能难以评估或优化。为了解决这个挑战，我们引入了梯度支配条件，它通过允许下层解集为空集严格放松了强凸性假设。在梯度支配条件下，我们提出了不精确无梯度法（IGFM），该方法使用t",
    "tldr": "研究了不需要典型下层强凸性假设的超目标方法，并引入梯度支配条件，在此基础上提出了不精确无梯度法（IGFM）来解决普通凸下层函数的超目标可能难以评估或优化的问题。"
}