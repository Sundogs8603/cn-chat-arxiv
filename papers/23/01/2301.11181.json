{
    "title": "Deep Laplacian-based Options for Temporally-Extended Exploration. (arXiv:2301.11181v2 [cs.LG] UPDATED)",
    "abstract": "Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up o",
    "link": "http://arxiv.org/abs/2301.11181",
    "context": "Title: Deep Laplacian-based Options for Temporally-Extended Exploration. (arXiv:2301.11181v2 [cs.LG] UPDATED)\nAbstract: Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up o",
    "path": "papers/23/01/2301.11181.json",
    "total_tokens": 896,
    "translated_title": "基于拉普拉斯算子的深度选项方法用于时间扩展探索",
    "translated_abstract": "在强化学习中，选择具有丰富经验流的探索性动作以进行更好学习是一项基本挑战。解决此问题的一种方法是根据特定的策略在一段时间内选择动作，也称为选项。最近一种用于获取此类探索选项的工作基于图拉普拉斯算子的本征函数。重要的是，直到现在，这些方法在图表领域中大多被限制在以下方面：(1)图拉普拉斯矩阵已给出或可以完全估计，(2)在该矩阵上进行特征分解是可计算的，(3)值函数可以被完全学习。此外，这些方法需要一个单独的选项发现阶段。这些假设在根本上是不可扩展的。在本文中，我们解决了这些限制，并展示了如何利用最近直接逼近拉普拉斯算子本征函数的结果，真正实现选项方法的可伸缩性。",
    "tldr": "本文提出了一种基于深度学习的选项方法，以处理强化学习中的探索性动作选择问题，并利用最近的结果来拓展至可伸缩性更好的领域。",
    "en_tdlr": "This paper proposes a deep learning-based option method to address the challenge of selecting exploratory actions in reinforcement learning. The method uses recent results for better scalability and extends beyond the limitations of previous approaches in the tabular domain."
}