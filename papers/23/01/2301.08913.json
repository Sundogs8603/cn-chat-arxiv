{
    "title": "Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v2 [cs.CL] UPDATED)",
    "abstract": "Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four d",
    "link": "http://arxiv.org/abs/2301.08913",
    "context": "Title: Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v2 [cs.CL] UPDATED)\nAbstract: Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four d",
    "path": "papers/23/01/2301.08913.json",
    "total_tokens": 773,
    "translated_title": "统一结构推理和语言模型预训练以进行复杂推理",
    "translated_abstract": "最近，配备基础推理技能的预训练语言模型（PLMs）在下游复杂任务上表现出了显著的性能。然而，显著的结构推理技能很少被研究，这涉及对文本中的隐含结构信息进行建模，并对其进行明确的逻辑推理以推导出结论。本文提出了一个统一的学习框架，将明确的结构推理和语言预训练相结合，赋予PLMs结构推理能力。它首先识别上下文中的几个基本结构，构建结构化查询，并沿着查询逐步推理以确定答案实体。通过使用PLMs学习的上下文表示来初始化结构的表示空间，并在这个语义表示空间上进行逐步推理，实现了文本语义和结构推理的融合。在四个数据集上的实验证明了该方法的有效性。",
    "tldr": "本文提出了一个统一的学习框架，将明确的结构推理和语言预训练相结合，以赋予预训练语言模型结构推理能力。"
}