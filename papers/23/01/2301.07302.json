{
    "title": "PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav. (arXiv:2301.07302v2 [cs.LG] UPDATED)",
    "abstract": "We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of $65.0\\%$ on ObjectNav ($+5.0\\%$ absolute over previous state-of-the-art). Using this BC$\\rightarrow$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can b",
    "link": "http://arxiv.org/abs/2301.07302",
    "context": "Title: PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav. (arXiv:2301.07302v2 [cs.LG] UPDATED)\nAbstract: We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of $65.0\\%$ on ObjectNav ($+5.0\\%$ absolute over previous state-of-the-art). Using this BC$\\rightarrow$RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can b",
    "path": "papers/23/01/2301.07302.json",
    "total_tokens": 972,
    "translated_title": "PIRLNav: 对于ObjectNav进行模仿学习和强化学习微调的预训练研究",
    "translated_abstract": "本研究探讨了ObjectGoal Navigation——在新环境中要求虚拟机器人导航到一个物体。以前的研究表明，使用人类演示的数据集进行行为克隆（BC）的模仿学习（IL）取得了有希望的结果。然而，这种方法也存在局限性——1）由于训练只模仿动作而不是后果，因此BC策略对新状态的泛化能力较差，2）收集演示数据成本高昂。另一方面，强化学习（RL）容易扩展，但需要精心设计奖励来实现理想的行为。我们提出PIRLNav，一个两阶段的学习方案，用于对人类演示进行BC预训练，然后进行RL微调。这导致该策略在ObjectNav上实现了65.0％的成功率（绝对值比以前的最新技术高5.0％）。使用这种BC $ \\rightarrow $ RL训练方法，我们对设计选择进行了严格的实证分析。首先，我们调查了使用人类演示是否可行。",
    "tldr": "本文提出了PIRLNav，通过人类演示的BC预训练和RL微调两个阶段的学习方案，成功率达到ObjectNav的65.0％，比以前的最新技术高5.0％。",
    "en_tdlr": "This paper proposes PIRLNav, a two-stage learning scheme for ObjectNav, which achieves a success rate of 65.0% by pretraining with human demonstrations using behavior cloning and finetuning with reinforcement learning. This outperforms the previous state-of-the-art by 5.0%."
}