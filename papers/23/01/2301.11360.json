{
    "title": "The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)",
    "abstract": "Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \\emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall ",
    "link": "http://arxiv.org/abs/2301.11360",
    "context": "Title: The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)\nAbstract: Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \\emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall ",
    "path": "papers/23/01/2301.11360.json",
    "total_tokens": 882,
    "translated_title": "线性组合的威力：随机卷积学习",
    "translated_abstract": "现代卷积神经网络通过增加模型深度、宽度和卷积核大小来保持与更先进的模型（如基于变换器的模型）的竞争力，导致有大量的可训练模型参数需要在训练过程中进行处理。本文质疑卷积神经网络中学习到的卷积核的重要性。实验证明，很多当代的卷积神经网络结构，甚至在不更新初始化的随机卷积核的情况下就可以达到高的测试准确率。实际上，简单的线性组合可以有效地将随机卷积核组合成表达能力强的网络运算符，其通过高效的 $1 \\times 1$ 卷积来实现。此外，这些随机卷积核的组合可以隐式地正则化结果运算符，减轻过拟合，提高整体性能。",
    "tldr": "本研究质疑了卷积神经网络中学习到的卷积核的重要性，提出了简单的线性组合方法，从随机卷积核中创建出表达能力强的网络运算符，通过隐含的正则化技术，可以提高整体性能。",
    "en_tdlr": "This study questions the significance of learned convolution filters in convolutional neural networks and proposes a simple linear combination method to effectively create expressive network operators from random convolution filters. The resulting operations are implicitly regularized, mitigating overfitting and enhancing overall performance."
}