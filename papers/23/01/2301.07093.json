{
    "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.",
    "link": "http://arxiv.org/abs/2301.07093",
    "context": "Title: GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)\nAbstract: Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.",
    "path": "papers/23/01/2301.07093.json",
    "total_tokens": 888,
    "translated_title": "GLIGEN：开放式基于文本的图像生成方法",
    "translated_abstract": "大规模的文本到图像扩散模型取得了令人惊叹的进展。然而，现状是仅使用文本输入，这可能会限制可控性。在这项工作中，我们提出了GLIGEN，基于语言关联的图像生成方法，这是一种新颖的方法，它建立在现有预训练的文本到图像扩散模型的基础上，并使其能够依赖于语言关联性输入。为了保留预训练模型的广泛概念知识，我们冻结所有权重，并通过门控机制将连结信息注入到新的可训练层中。我们的模型实现了开放式基于关键字和边界框的文本到图像生成，而且连结能力在新的空间配置和概念上具有良好的普适性。GLIGEN在COCO和LVIS的零样本表现优于现有的监督布局到图像的基线。",
    "tldr": "GLIGEN是一种开放式基于语言关联性和预训练的文本到图像生成方法，通过门控机制注入连结信息，能够实现零样本的基于关键字和边界框的文本到图像生成，性能优于现有的监督布局到图像的基线。",
    "en_tdlr": "GLIGEN is an open-set grounded text-to-image generation method that builds upon existing pre-trained models by injecting grounding inputs via a gated mechanism. It achieves zero-shot performance on caption and bounding box inputs and outperforms existing supervised layout-to-image baselines."
}