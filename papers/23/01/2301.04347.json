{
    "title": "Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v2 [cs.CL] UPDATED)",
    "abstract": "Language models have demonstrated strong performance on various natural language understanding tasks. Similar to humans, language models could also have their own bias that is learned from the training data. As more and more downstream tasks integrate language models as part of the pipeline, it is necessary to understand the internal stereotypical representation and the methods to mitigate the negative effects. In this paper, we proposed a simple method to test the internal stereotypical representation in pre-trained language models using counterexamples. We mainly focused on gender bias, but the method can be extended to other types of bias. We evaluated models on 9 different cloze-style prompts consisting of knowledge and base prompts. Our results indicate that pre-trained language models show a certain amount of robustness when using unrelated knowledge, and prefer shallow linguistic cues, such as word position and syntactic structure, to alter the internal stereotypical representat",
    "link": "http://arxiv.org/abs/2301.04347",
    "context": "Title: Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v2 [cs.CL] UPDATED)\nAbstract: Language models have demonstrated strong performance on various natural language understanding tasks. Similar to humans, language models could also have their own bias that is learned from the training data. As more and more downstream tasks integrate language models as part of the pipeline, it is necessary to understand the internal stereotypical representation and the methods to mitigate the negative effects. In this paper, we proposed a simple method to test the internal stereotypical representation in pre-trained language models using counterexamples. We mainly focused on gender bias, but the method can be extended to other types of bias. We evaluated models on 9 different cloze-style prompts consisting of knowledge and base prompts. Our results indicate that pre-trained language models show a certain amount of robustness when using unrelated knowledge, and prefer shallow linguistic cues, such as word position and syntactic structure, to alter the internal stereotypical representat",
    "path": "papers/23/01/2301.04347.json",
    "total_tokens": 910,
    "translated_title": "Counteracts：在预训练语言模型中测试刻板印象的对抗性。",
    "translated_abstract": "语言模型在各种自然语言理解任务中表现出了强大的性能。与人类一样，语言模型也可能从训练数据中学习到自己的偏见。随着越来越多的下游任务将语言模型作为管道的一部分集成，有必要了解内部刻板印象和减轻负面影响的方法。在本文中，我们提出了一种简单的方法，使用反例来测试预训练语言模型中的内部刻板印象。我们主要关注性别偏见，但该方法可以扩展到其他类型的偏见。我们评估了9个不同的填空式提示的模型，包括知识和基础提示。结果表明，在使用不相关的知识时，预训练语言模型表现出一定的鲁棒性，并且更倾向于使用浅层的语言线索，如单词位置和句法结构来改变内部刻板印象。",
    "tldr": "本文提出了一种使用反例测试预训练语言模型中内部刻板印象的方法，重点是性别偏见，结果表明模型在使用不相关的知识时表现出一定的鲁棒性，更倾向于使用浅层的语言线索来改变内部刻板印象。",
    "en_tdlr": "This paper proposes a method to test the internal stereotypical representation in pre-trained language models using counterexamples, with a focus on gender bias. The results indicate that the model has a certain degree of robustness when using unrelated knowledge, and prefers to use shallow linguistic cues to alter the internal stereotypical representation."
}