{
    "title": "ZiCo: Zero-shot NAS via Inverse Coefficient of Variation on Gradients. (arXiv:2301.11300v3 [cs.LG] UPDATED)",
    "abstract": "Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS,",
    "link": "http://arxiv.org/abs/2301.11300",
    "context": "Title: ZiCo: Zero-shot NAS via Inverse Coefficient of Variation on Gradients. (arXiv:2301.11300v3 [cs.LG] UPDATED)\nAbstract: Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS,",
    "path": "papers/23/01/2301.11300.json",
    "total_tokens": 1013,
    "translated_title": "基于梯度系数变异的零样本NAS：ZiCo",
    "translated_abstract": "神经结构搜索(NAS)被广泛用于自动获得在大量候选结构中性能最佳的神经网络。为了缩短搜索时间，零样本NAS旨在设计可以预测给定结构的测试性能的无需训练的代理。然而，最近的研究表明，迄今为止提出的所有零样本代理都不能比朴素代理 (#Params) 一致地获得更好的效果。为了改善这种现状，本文首先阐述了一些特定的梯度属性如何影响神经网络的收敛速度和泛化能力。在此基础上，我们提出了一种新的零样本代理ZiCo，它是第一个比#Params一致更好的代理。我们证明了ZiCo在几个流行的NAS基准测试(NASBench101、NATSBench-SSS/TSS和NASBench201)上比最先进的代理效果更好，并且可以显著降低寻找最佳神经结构的搜索成本。",
    "tldr": "本文提出了一种基于梯度的零样本代理ZiCo，通过研究特定的梯度属性如何影响神经网络的收敛速度和泛化能力，ZiCo成为了第一个在多个NAS基准测试上一致优于朴素代理(#Params)的代理，可以显著降低寻找最佳神经结构的搜索成本。",
    "en_tdlr": "This paper proposes a new gradient-based zero-shot proxy, named ZiCo, which is the first proxy to consistently outperform the naive proxy #Params on multiple NAS benchmarks. The theoretical analysis on how specific gradient properties affect the convergence rate and generalization capacity of neural networks underpins the development of ZiCo, which can significantly reduce the search cost for finding the best neural architecture."
}