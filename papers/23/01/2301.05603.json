{
    "title": "A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)",
    "abstract": "Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi",
    "link": "http://arxiv.org/abs/2301.05603",
    "context": "Title: A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)\nAbstract: Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi",
    "path": "papers/23/01/2301.05603.json",
    "total_tokens": 802,
    "translated_title": "数据集压缩的综合调查",
    "translated_abstract": "在过去的十年中，深度学习技术得到了前所未有的发展，并在许多应用领域成为首选。这一进展主要归功于快速发展的计算资源与先进算法的系统协同，用以处理海量数据。然而，处理有限计算能力下无限增长的数据逐渐变得具有挑战性。为此，提出了各种方法来提高数据处理效率。数据集压缩方法是一种数据集缩减方法，通过从大规模数据中综合出一个小型典型数据集来解决这个问题，并引起了深度学习界的广泛关注。现有的数据集压缩方法可以根据是否明确模仿目标数据的性能将其分类为元学习和数据匹配框架。尽管数据集压缩在压缩数据集方面表现出惊人的性能，但仍存在一些限制。",
    "tldr": "数据集压缩是处理海量数据的一种方法，通过综合一个小型典型数据集来提高数据处理效率。这一方法在压缩数据集方面取得了惊人的性能，但仍存在一些限制。",
    "en_tdlr": "Dataset distillation is an approach to handle massive data by synthesizing a small typical dataset, which has shown surprising performance in compressing datasets but still has limitations."
}