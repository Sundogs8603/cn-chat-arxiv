{
    "title": "FedICT: Federated Multi-task Distillation for Multi-access Edge Computing. (arXiv:2301.00389v2 [cs.LG] UPDATED)",
    "abstract": "The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Federated MultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between ",
    "link": "http://arxiv.org/abs/2301.00389",
    "context": "Title: FedICT: Federated Multi-task Distillation for Multi-access Edge Computing. (arXiv:2301.00389v2 [cs.LG] UPDATED)\nAbstract: The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Federated MultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between ",
    "path": "papers/23/01/2301.00389.json",
    "total_tokens": 835,
    "translated_title": "FedICT:用于多接入边缘计算的联邦多任务蒸馏",
    "translated_abstract": "对于移动设备智能服务和隐私保护的日益关注，联邦学习在多接入边缘计算（MEC）中得到了广泛应用。多样的用户行为要求在不同设备上使用个性化服务和异构机器学习（ML）模型。提出了联邦多任务学习（FMTL）来为不同设备训练相关但个性化的ML模型，然而之前的工作在训练过程中存在过多的通信开销，并忽视了MEC中设备之间的模型异构性。将知识蒸馏引入FMTL可以同时实现高效的通信和客户端之间的模型异构性，而现有方法依赖于公共数据集，这在实际中是不切实际的。为了解决这个困境，提出了用于多接入边缘计算的联邦多任务蒸馏（FedICT）。",
    "tldr": "FedICT是一种用于多接入边缘计算的联邦多任务蒸馏方法，可以在个性化服务和异构机器学习模型的同时实现高效通信和模型异构性。"
}