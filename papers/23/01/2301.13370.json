{
    "title": "On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters. (arXiv:2301.13370v2 [cs.LG] UPDATED)",
    "abstract": "Recent work has shown that forward- and reverse- mode automatic differentiation (AD) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. W",
    "link": "http://arxiv.org/abs/2301.13370",
    "context": "Title: On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters. (arXiv:2301.13370v2 [cs.LG] UPDATED)\nAbstract: Recent work has shown that forward- and reverse- mode automatic differentiation (AD) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. W",
    "path": "papers/23/01/2301.13370.json",
    "total_tokens": 913,
    "translated_title": "关于具有机器可表示参数的神经网络自动微分正确性的研究",
    "translated_abstract": "最近的研究表明，实数域上的前向和反向模式自动微分几乎始终在数学上是准确的。然而，实际编程使用的是机器可表示的数字（例如浮点数），而不是实数。本文研究了当神经网络的参数空间仅由机器可表示的数字组成时，自动微分的正确性。我们分析了两组可能导致自动微分不正确的参数：一组是网络可微但自动微分无法计算其导数的参数组，另一组是网络不可微的参数组。对于带有偏置参数的神经网络，我们首先证明了第一组参数组始终为空。然后我们给出了一个线性上限来限制第二组参数组中不可微性在激活函数中的数目，并给出了一个简单的必要和充分条件来判断一个参数是否在这个参数组中。",
    "tldr": "本论文研究了神经网络参数为机器可表示数字时自动微分的正确性问题，证明了神经网络带偏置参数时自动微分始终正确，给出了限制不可微性在激活函数中数目的界，并提供了判断参数是否在不可微参数组中的条件。",
    "en_tdlr": "This paper studies the correctness of automatic differentiation for neural networks with machine-representable parameters. It proves that automatic differentiation is always correct for neural networks with bias parameters and provides a bound on the size of the non-differentiable set and a necessary and sufficient condition for a parameter to be in this set."
}