{
    "title": "Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)",
    "abstract": "Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.",
    "link": "http://arxiv.org/abs/2301.13616",
    "context": "Title: Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)\nAbstract: Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.",
    "path": "papers/23/01/2301.13616.json",
    "total_tokens": 826,
    "translated_title": "随机网络碾压对防止探索的影响",
    "translated_abstract": "尽管随机网络碾压 (RND) 在各种领域都取得了成功，但在用作离线强化学习中惩罚越界操作的不确定性估计器时，它被证明不具有足够的区分度。在本文中，我们重新审视了这些结果，并表明，通过对 RND 先验进行朴素的调节选择，演员有效地最小化反探索奖励变得不可行，并且区分度不再是问题。我们展示了可以通过基于特征线性调制 (FiLM) 的调节来避免这种局限性，从而得到一个简单而高效的基于软行为者-评论家算法的无集成算法。我们在 D4RL 基准测试上进行了评估，结果表明，它能够实现与基于集成的方法相当的性能，并显著优于无集成方法。",
    "tldr": "本文介绍了基于特征线性调制的随机网络碾压算法，可以有效防止探索，避免了区分度的问题，在 D4RL 基准测试中取得了可与集成方法相媲美的性能。",
    "en_tdlr": "This paper introduces a random network distillation algorithm based on feature-wise linear modulation, which effectively prevents exploration and avoids the problem of discriminativity. It achieves comparable performance to ensemble-based methods in the D4RL benchmark."
}