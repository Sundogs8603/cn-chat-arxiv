{
    "title": "Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments. (arXiv:2301.13446v2 [cs.LG] UPDATED)",
    "abstract": "We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new analysis techniques show to this algorithm enjoys variance-dependent bounds with respect to our proposed norms. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind. We further initiate the study on model-free algorithms with variance-dependent regret bounds by designing a reference-function-based algorithm with a novel capped-doubling reference update schedule. Lastly, we also provid",
    "link": "http://arxiv.org/abs/2301.13446",
    "context": "Title: Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments. (arXiv:2301.13446v2 [cs.LG] UPDATED)\nAbstract: We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new analysis techniques show to this algorithm enjoys variance-dependent bounds with respect to our proposed norms. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind. We further initiate the study on model-free algorithms with variance-dependent regret bounds by designing a reference-function-based algorithm with a novel capped-doubling reference update schedule. Lastly, we also provid",
    "path": "papers/23/01/2301.13446.json",
    "total_tokens": 995,
    "translated_title": "强化学习中的尖锐方差相关界限：随机和确定性环境的最佳结合",
    "translated_abstract": "本文研究马尔可夫决策过程（MDPs）的方差相关遗憾界限。具有方差相关遗憾保证的算法可以自动利用具有低方差（例如，在确定性MDP上享有常量遗憾）的环境。现有算法要么独立于方差要么次优。我们首先提出两个新的环境规范来表征环境的细粒度方差属性。对于基于模型的方法，我们设计了MVP算法(Zhang等，2021a)的变种，并使用新的分析技术展示了该算法相对于我们提出的规范享有方差相关的界限。特别地，这一界限对于随机和确定性MDP同时是极小极大最优的，这是其种类中的第一个结果。我们进一步通过设计一种参考函数的算法以及一个新的带有上限加倍参考更新进度表的策略启动了关于具有方差相关遗憾界限的无模型算法的研究。最后，我们还提供了一些启示。",
    "tldr": "本研究将马尔可夫决策过程的方差相关遗憾界限应用到强化学习中，提出了两个新的环境规范来表征环境的方差属性，并设计出基于模型和无模型的算法，对于随机和确定性环境同时极小极大最优的界限是第一次被证明出来的。",
    "en_tdlr": "This study applies variance-dependent regret bounds to reinforcement learning for Markov decision processes and proposes two new environment norms to characterize the fine-grained variance properties. The research designs model-based and model-free algorithms that simultaneously achieve minimax optimal bounds for both stochastic and deterministic environments, which is the first result of its kind."
}