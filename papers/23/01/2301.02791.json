{
    "title": "Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment. (arXiv:2301.02791v2 [cs.LG] UPDATED)",
    "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. %These identified sub-structures can provide interpretations of GNN's behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanation",
    "link": "http://arxiv.org/abs/2301.02791",
    "context": "Title: Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment. (arXiv:2301.02791v2 [cs.LG] UPDATED)\nAbstract: Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. %These identified sub-structures can provide interpretations of GNN's behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanation",
    "path": "papers/23/01/2301.02791.json",
    "total_tokens": 854,
    "translated_title": "忠实且一致的图神经网络解释与原理对齐",
    "translated_abstract": "近年来，揭示图神经网络（GNN）预测背后的原理引起了越来越多的关注。实例级GNN解释旨在发现目标GNN依赖于进行预测的关键输入元素，如节点或边缘。这些识别出的子结构可以解释GNN的行为。尽管提出了各种算法，但其中大多数通过搜索能够保留原始预测的最小子图来形式化这个任务。然而，这个框架根深蒂固地具有归纳偏见：几个子图可能会产生与原始图相同或相似的输出。因此，它们可能会提供虚假的解释，并且无法提供一致的解释。将它们应用于解释表现较差的GNN会进一步放大这些问题。为了解决这个问题，我们从因果关系的角度在理论上检查GNN的预测。伪解释的两个典型原因",
    "tldr": "本文研究了图神经网络的预测，提出了解释不一致的问题，并从因果关系的角度对其进行了理论分析",
    "en_tdlr": "This paper investigates the predictions made by graph neural networks (GNNs), and highlights the problem of inconsistent explanations. It provides a theoretical analysis of GNN predictions from a causality perspective."
}