{
    "title": "On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)",
    "abstract": "Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.",
    "link": "http://arxiv.org/abs/2301.12309",
    "context": "Title: On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)\nAbstract: Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.",
    "path": "papers/23/01/2301.12309.json",
    "total_tokens": 925,
    "translated_title": "关于深度网络和双重下降的利普希茨常数",
    "translated_abstract": "目前关于深度网络泛化误差的界限都是基于输入变量的平滑或有界依赖性，没有研究探究实践中控制这些因素的机制。本文对经历双重衰减的深度网络的实验利普希茨常数进行了广泛的实验研究，并强调了非单调的趋势，与测试误差密切相关。通过建立随机梯度下降的参数空间和输入空间梯度之间的联系，我们分离出两个重要因素，即损失函数曲率和距离初始化参数的距离，分别控制关键点周围的优化动态，并限制模型函数的复杂度，即使在训练数据之外。我们的研究揭示了超参数化的隐式正则化和实践中网络的有效模型复杂度的新见解。",
    "tldr": "本文通过实验研究发现，深度网络的利普希茨常数趋势与测试误差密切相关，通过建立参数空间和输入空间梯度之间的联系，确定了损失函数曲率和距离初始化参数的距离对于深度网络的优化和模型函数复杂度限制是关键因素，该研究对隐式正则化和网络的有效模型复杂度提供了新的见解。",
    "en_tdlr": "This article presents an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and isolates two important factors, namely loss landscape curvature and distance of parameters from initialization, respectively controlling optimization dynamics around a critical point and bounding model function complexity beyond the training data. The research provides new insights into implicit regularization and effective model complexity for networks trained in practice."
}