{
    "title": "Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)",
    "abstract": "Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin",
    "link": "http://arxiv.org/abs/2301.13734",
    "context": "Title: Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)\nAbstract: Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin",
    "path": "papers/23/01/2301.13734.json",
    "total_tokens": 1093,
    "translated_title": "通过离线数据提升蒙特卡罗评估方法",
    "translated_abstract": "蒙特卡罗 (MC) 方法是估计策略表现最广泛使用的方法。给定一个感兴趣的策略，MC 方法通过重复运行该策略以收集样本并取出结果平均值来给出估计值。在此过程中收集的样本称为在线样本。为了获得准确的估计值，MC 方法需要消耗大量在线样本。当在线样本昂贵时，例如在线推荐和库存管理，我们希望在实现相同的估计准确度的同时减少在线样本数量。为此，我们使用离线 MC 方法，通过运行不同的策略（称为行为策略）评估感兴趣的策略。我们设计了一个定制的行为策略，使离线 MC 估计器的方差明显小于普通 MC 估计器。重要的是，该定制行为策略可以从现有的离线数据，即先前记录的数据中高效学习，这比在线样本要便宜得多。我们的实验表明，与现有的最先进方法相比，我们的方法只需使用小部分在线样本就能实现相同的估计精度。",
    "tldr": "本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。",
    "en_tdlr": "This paper presents a method to improve Monte Carlo evaluation by using offline data to reduce the number of online samples required while achieving the same estimate accuracy. A tailored behavior policy is designed to produce smaller variance of the off-policy MC estimator compared to the ordinary MC estimator. The behavior policy can be efficiently learned from existing offline data, and experiments show that this method requires only a small fraction of online samples compared to the state-of-the-art methods."
}