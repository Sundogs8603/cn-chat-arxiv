{
    "title": "Domain Expansion of Image Generators. (arXiv:2301.05225v2 [cs.CV] UPDATED)",
    "abstract": "Can one inject new concepts into an already trained generative model, while respecting its existing structure and knowledge? We propose a new task - domain expansion - to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new domains? Interestingly, we find that the latent space offers unused, \"dormant\" directions, which do not affect the output. This provides an opportunity: By \"repurposing\" these directions, we can represent new domains without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several - even hundreds - of new domains! Using our expansion method, one \"expanded\" model can supersede numerous domain-specific models, without expanding",
    "link": "http://arxiv.org/abs/2301.05225",
    "context": "Title: Domain Expansion of Image Generators. (arXiv:2301.05225v2 [cs.CV] UPDATED)\nAbstract: Can one inject new concepts into an already trained generative model, while respecting its existing structure and knowledge? We propose a new task - domain expansion - to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new domains? Interestingly, we find that the latent space offers unused, \"dormant\" directions, which do not affect the output. This provides an opportunity: By \"repurposing\" these directions, we can represent new domains without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several - even hundreds - of new domains! Using our expansion method, one \"expanded\" model can supersede numerous domain-specific models, without expanding",
    "path": "papers/23/01/2301.05225.json",
    "total_tokens": 918,
    "translated_title": "图像生成器的领域扩展",
    "translated_abstract": "一个已经训练好的生成模型是否能够注入新的概念，同时保持其现有的结构和知识？我们提出了一个新的任务——领域扩展，来解决这个问题。给定一个预训练的生成器和新领域，我们扩展生成器以共同模拟所有领域，新旧领域和谐地结合。我们发现生成器包含有意义的，并经过了预训练的潜在空间。能否在最小程度上扰动这个辛苦获得的表示，同时最大程度地表示新领域？有趣的是，我们发现潜在空间提供了未使用的“休眠”方向，不影响输出。这提供了一个机会：通过“重定向”这些方向，我们可以表示新的领域而不会扰乱原始表示。事实上，我们发现预训练的生成器有能力添加几个——甚至数百个——新领域！使用我们的扩展方法，一个“扩展”模型可以取代许多特定领域的模型，而不需要扩展。",
    "tldr": "提出一种新任务——领域扩展，来注入新的概念到一个已经训练好的生成模型，同时保持其现有的结构和知识。发现预训练的生成器具有添加多个新领域的能力，而不需要扩展。",
    "en_tdlr": "Introduce a new task of domain expansion to inject new concepts into a pretrained generative model while respecting its existing structure and knowledge. Discover that pretrained generators have the capacity to add multiple new domains without expansion."
}