{
    "title": "Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)",
    "abstract": "Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\\times$ speedu",
    "link": "http://arxiv.org/abs/2301.00330",
    "context": "Title: Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)\nAbstract: Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\\times$ speedu",
    "path": "papers/23/01/2301.00330.json",
    "total_tokens": 868,
    "translated_title": "梯度过滤技术实现高效的设备端训练",
    "translated_abstract": "尽管在联邦学习、连续学习和其他许多应用中很重要，但设备端训练仍然是EdgeAI的一个难题。本文提出了一种新的梯度过滤技术，使得设备端的卷积神经网络模型训练成为可能。该方法通过创建具有较少唯一元素的特殊结构，从而显著减少了训练期间反向传播的计算复杂度和内存消耗。在多个CNN模型（例如MobileNet、DeepLabV3、UPerNet）和设备（例如Raspberry Pi和Jetson Nano）上进行的图像分类和语义分割的大量实验表明了我们方法的有效性和广泛适用性。例如，与SOTA相比，我们实现了高达19倍的训练加速度。",
    "tldr": "本文提出一种新的梯度过滤方法，通过创建具有较少唯一元素的特殊结构来实现设备端卷积神经网络模型的训练，从而大大减少了计算复杂度和内存消耗，实现最高19倍的训练加速度。",
    "en_tdlr": "This paper proposes a novel gradient filtering approach to enable on-device CNN model training by creating a special structure with fewer unique elements in the gradient map, significantly reducing the computational complexity and memory consumption during back propagation. Extensive experiments on multiple CNN models and devices show up to 19x speedup compared to state-of-the-art."
}