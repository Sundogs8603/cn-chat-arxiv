{
    "title": "Equivariant Architectures for Learning in Deep Weight Spaces. (arXiv:2301.12780v2 [cs.LG] UPDATED)",
    "abstract": "Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show ",
    "link": "http://arxiv.org/abs/2301.12780",
    "context": "Title: Equivariant Architectures for Learning in Deep Weight Spaces. (arXiv:2301.12780v2 [cs.LG] UPDATED)\nAbstract: Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show ",
    "path": "papers/23/01/2301.12780.json",
    "total_tokens": 860,
    "translated_title": "深层权重空间中学习的等变架构",
    "translated_abstract": "设计用于以原始权重矩阵形式处理神经网络的机器学习架构是一条新引入的研究方向。不幸的是，深层权重空间的独特对称结构使得这种设计非常具有挑战性。如果成功，这样的架构将能够执行广泛的有趣任务，从将预训练的网络适应到新的领域，到编辑作为函数表示的对象（INRs或NeRFs）。作为实现这一目标的第一步，我们在这里提出了一种新的深层权重空间中学习的网络架构。它以预训练MLP的权重和偏置的串联作为输入，并使用一组对MLP权重的自然置换对称等变的层组成来处理它：改变MLP中间层中神经元的顺序不会影响它所表示的函数。我们为这些对称结构提供了所有仿射等变和不变层的完整特征，并展示...",
    "tldr": "本论文提出了一种新的网络架构，用于在深层权重空间中学习，它对MLP权重的自然置换对称等变，可以处理广泛有趣的任务。",
    "en_tdlr": "This paper proposes a novel network architecture for learning in deep weight spaces, which is equivariant to the natural permutation symmetry of the MLP's weights and capable of processing a wide range of intriguing tasks."
}