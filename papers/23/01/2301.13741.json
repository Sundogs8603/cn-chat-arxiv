{
    "title": "UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)",
    "abstract": "Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \\textbf{U}nified and \\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit",
    "link": "http://arxiv.org/abs/2301.13741",
    "context": "Title: UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)\nAbstract: Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \\textbf{U}nified and \\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit",
    "path": "papers/23/01/2301.13741.json",
    "total_tokens": 842,
    "translated_title": "UPop：用于压缩视觉语言Transformer模型的统一和渐进式剪枝方法",
    "translated_abstract": "真实世界的数据包含大量的多模态信息，其中视觉和语言是最具代表性的两种模态。此外，越来越重的模型，例如Transformer，已经引起了研究人员对模型压缩的关注。然而，如何压缩多模态模型，特别是视觉语言Transformer，仍然未被充分探索。本文提出了一种名为UPop的通用视觉语言Transformer压缩框架，它包括1）在原始模型中在连续优化空间中统一搜索多模态子网，从而实现可压缩模态和结构之间自动分配剪枝比率；2）渐进式搜索和微调子网，从而保持搜索和微调之间的收敛，以实现更高的压缩比率。",
    "tldr": "UPop是一种通用的视觉语言Transformer压缩框架，采用统一和渐进式剪枝方法，可自动分配剪枝比率，实现更高的压缩比率。",
    "en_tdlr": "UPop is a universal compression framework for vision-language Transformers, which uses unified and progressive pruning methods to automatically allocate pruning ratios and achieve higher compression ratios."
}