{
    "title": "Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging. (arXiv:2301.10520v2 [eess.IV] UPDATED)",
    "abstract": "We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tra",
    "link": "http://arxiv.org/abs/2301.10520",
    "context": "Title: Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging. (arXiv:2301.10520v2 [eess.IV] UPDATED)\nAbstract: We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tra",
    "path": "papers/23/01/2301.10520.json",
    "total_tokens": 957,
    "translated_title": "超级NeRF：用于超声成像的神经辐射场",
    "translated_abstract": "我们提出了一种物理增强的隐式神经表示（INR），用于从重叠的超声扫描中学习组织属性的超声成像。我们提出的方法利用基于光线追踪的神经渲染进行新视图的超声合成。最近的研究表明，INR模型可以从一组二维超声框架中编码三维场景的表示。然而，这些模型未能考虑到超声成像固有的外观和几何视角相关变化。在我们的工作中，我们讨论了场景中方向相关的变化，并展示了物理启发式渲染如何提高超声图像合成的保真度。特别是，我们实验性地证明了我们提出的方法能够为具有模糊表示的区域生成几何精确的B模式图像，这是由于超声图像的视角相关差异所致。我们使用模拟肝脏B模式超声扫描和脊柱幻影超声扫描进行实验验证。",
    "tldr": "该论文提出了一种物理增强的隐式神经表示（INR）用于超声成像，该表示可以从重叠的超声扫描中学习组织属性。利用基于光线追踪的神经渲染进行新视图的超声合成，实现几何精确的B模式图像的生成。",
    "en_tdlr": "The paper proposes a physics-enhanced implicit neural representation (INR) for ultrasound imaging, which learns tissue properties from overlapping scans. The proposed method leverages ray-tracing-based neural rendering for synthesizing novel views. The approach shows direction-dependent changes in the scene, and physics-inspired rendering improves the fidelity of ultrasound image synthesis, generating geometrically accurate B-mode images for regions with ambiguous representation."
}