{
    "title": "Quasi-optimal Reinforcement Learning with Continuous Actions. (arXiv:2301.08940v2 [stat.ML] UPDATED)",
    "abstract": "Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel \\emph{quasi-optimal learning algorithm}, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated expe",
    "link": "http://arxiv.org/abs/2301.08940",
    "context": "Title: Quasi-optimal Reinforcement Learning with Continuous Actions. (arXiv:2301.08940v2 [stat.ML] UPDATED)\nAbstract: Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel \\emph{quasi-optimal learning algorithm}, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated expe",
    "path": "papers/23/01/2301.08940.json",
    "total_tokens": 885,
    "translated_title": "具有连续动作的准最优强化学习",
    "translated_abstract": "强化学习在许多现实应用中需要在连续动作环境中做出决策。在医疗治疗方案的开发中，确定最佳剂量水平起着至关重要的作用。然而，将现有的强化学习算法应用于医疗应用中的一个挑战是，流行的无穷支持随机策略（例如高斯策略）可能会分配过高的剂量，严重危害患者。因此，引导一个支持仅包含近似最优动作的策略类别，并缩小效果和可靠性的动作搜索区域是很重要的。为了实现这一点，我们开发了一种新的“准最优学习算法”，该算法在离线策略设置下可以轻松优化，并在一般函数逼近下保证收敛。在理论上，我们分析了所提出算法的一致性、样本复杂度、适应性和收敛性。我们通过全面的模拟实验评估了我们的算法。",
    "tldr": "本研究提出了一种准最优学习算法，用于解决强化学习中连续动作环境下的决策问题，特别适用于医疗应用中确定最佳剂量水平的问题。"
}