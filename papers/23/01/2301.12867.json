{
    "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. (arXiv:2301.12867v4 [cs.CL] UPDATED)",
    "abstract": "Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this pape",
    "link": "http://arxiv.org/abs/2301.12867",
    "context": "Title: Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. (arXiv:2301.12867v4 [cs.CL] UPDATED)\nAbstract: Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this pape",
    "path": "papers/23/01/2301.12867.json",
    "total_tokens": 1195,
    "translated_title": "通过越狱技术进行红队演练：ChatGPT 的偏见，鲁棒性，可靠性和毒性研究",
    "translated_abstract": "自然语言处理方面的突破，使得能够以开放的方式合成和理解连贯的文本，将理论算法转化为实际应用成为可能。大型语言模型（LLMs）已经显著影响了报告摘要软件和撰稿人等业务。但观察表明，LLMs可能表现出社会偏见和毒性，从而造成不负责任的道德和社会危险后果。因此，应该开发负责任的大规模基准来确保LLMs的问责。尽管有几项实证调查揭示了现代LLMs的一些伦理困难存在，但是对当前LLMs使用的风险和有害行为的系统性调查和用户研究仍然很少。为了更好地教育未来建设负责任的LLMs，我们通过越狱技术对OpenAI的ChatGPT进行了一种质性研究方法称为“红队演练”。我们的研究调查了ChatGPT的可靠性，鲁棒性，偏见和毒性。我们发现ChatGPT存在与性别，种族和宗教有关的偏见，并且容易受到对抗性攻击的影响。我们的研究还强调了开发和部署LLMs的透明度和问责制的重要性。",
    "tldr": "这篇论文通过对OpenAI的ChatGPT进行越狱技术实验的方法，研究了它的可靠性、鲁棒性、偏见和毒性等问题。研究发现，ChatGPT存在种族、性别和宗教等相关偏见，并容易受到对抗性攻击的影响，因此建议在开发和部署LLMs时应考虑透明度和问责制问题。",
    "en_tdlr": "This paper studies the reliability, robustness, bias, and toxicity of OpenAI's ChatGPT through the method of jailbreaking. ChatGPT was found to exhibit biases related to race, gender, and religion, and is vulnerable to adversarial attacks. The study highlights the importance of transparency and accountability in the development and deployment of LLMs."
}