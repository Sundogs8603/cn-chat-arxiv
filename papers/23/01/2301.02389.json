{
    "title": "Provable Reset-free Reinforcement Learning by No-Regret Reduction. (arXiv:2301.02389v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm.",
    "link": "http://arxiv.org/abs/2301.02389",
    "context": "Title: Provable Reset-free Reinforcement Learning by No-Regret Reduction. (arXiv:2301.02389v3 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm.",
    "path": "papers/23/01/2301.02389.json",
    "total_tokens": 923,
    "translated_title": "可证明的无重置强化学习——基于无懊悔规约的方法",
    "translated_abstract": "迄今为止，强化学习在现实世界中的应用非常有限。一个关键挑战是，典型的强化学习算法严重依赖于重置机制来采样合适的初始状态；在实践中，由于需要人工干预或者复杂的环境工程，这种重置机制的实现成本较高。为了使学习更加实用，我们提出了一个通用的无懊悔规约方法来系统地设计无重置强化学习算法。我们的规约将无重置强化学习问题转化为一个两人博弈问题。我们证明，在这个两人博弈中实现亚线性懊悔将意味着在原始的强化学习问题中学习到具有亚线性性能悔恨和亚线性重置总次数的策略。这意味着智能体最终学会了最优化行为并避免重置。为了展示这个方法的有效性，我们设计了一个线性马尔可夫决策过程的实例，这是第一个具有可证明正确性的无重置强化学习算法。",
    "tldr": "该论文提出了一个通用的无懊悔规约方法，用于解决强化学习中重置机制带来的实际应用问题，提出了第一个可证明正确性的无重置强化学习算法。",
    "en_tdlr": "The paper introduces a generic no-regret reduction method to address the practical challenges of using reset mechanisms in reinforcement learning, and presents the first provably correct reset-free RL algorithm."
}