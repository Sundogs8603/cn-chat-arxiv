{
    "title": "PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)",
    "abstract": "Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.",
    "link": "http://arxiv.org/abs/2301.12776",
    "context": "Title: PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)\nAbstract: Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.",
    "path": "papers/23/01/2301.12776.json",
    "total_tokens": 883,
    "translated_title": "PAC-Bayesian软演员-评论家学习",
    "translated_abstract": "演员-评论家算法通过两个分别作策略评估和改进的功能逼近器来解决增强学习(RL)的双重目标。此方法的实用性是以训练不稳定为代价的，主要原因是评论家逼近误差对演员的破坏性影响。我们通过首次采用一个现有的可能近似正确(PAC)Bayesian界限作为Soft Actor-Critic (SAC)算法的评论家训练目标来解决这个瓶颈。此外，我们进一步证明了当随机演员通过评论家引导的随机搜索探索多个未来时，在线学习性能显著提高。我们观察到我们得到的算法在多个经典控制和运动任务中，在样本效率和遗憾最小化方面与现有技术相比具有明显优势。",
    "tldr": "本文提出了一种使用PAC-Bayesian bound作为Soft Actor-Critic (SAC)算法评论家训练目标的方法，以解决训练不稳定的问题，并通过评论家引导的随机搜索探索多个未来来提高在线学习性能。在多个经典控制和运动任务中，该算法具有样本效率和遗憾最小化方面的明显优势。",
    "en_tdlr": "This paper proposes a method using PAC-Bayesian bound as the critic training objective of Soft Actor-Critic (SAC) algorithm to solve the problem of training instability and improve online learning performance by critic-guided random search. The algorithm shows significant advantages in sample efficiency and regret minimization in multiple classical control and locomotion tasks."
}