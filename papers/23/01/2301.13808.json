{
    "title": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v2 [cs.CL] UPDATED)",
    "abstract": "Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excludin",
    "link": "http://arxiv.org/abs/2301.13808",
    "context": "Title: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v2 [cs.CL] UPDATED)\nAbstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excludin",
    "path": "papers/23/01/2301.13808.json",
    "total_tokens": 1055,
    "translated_title": "大型语言模型是多才多艺的分解器：将证据和问题分解为表格推理",
    "translated_abstract": "基于表格的推理已经在结合深度模型和离散推理方面取得了显著的进展，它需要对自由形式的自然语言问题和结构化表格数据进行推理。然而，以往的基于表格的推理解决方案通常会在海量证据（表格）上遭遇显著的性能退化。此外，大多数现有方法在处理复杂问题时也面临困难，因为所需信息分散在不同的位置。为了缓解上述挑战，我们利用大型语言模型（LLMs）作为有效的基于表格推理的分解器，将（i）巨大的证据（一个巨大的表格）分解成子证据（一个小表格），以减轻无用信息对表格推理的干扰；和（ii）将复杂问题分解成更简单的子问题进行文本推理。具体而言，我们首先使用LLMs分解当前问题涉及的证据（表格），保留相关证据并排除不相关部分。然后，我们使用LLMs将复杂问题重新表述为更简单的子问题，以便更精确地检索每个子问题的相应证据。我们在几个基准数据集上评估了我们的方法，实验结果表明我们的方法显著优于现有的基于表格推理的方法。",
    "tldr": "这篇论文介绍了利用大型语言模型作为分解器，解决基于表格推理中的性能下降和复杂问题的问题，并在多个基准数据集上显著优于现有方法。",
    "en_tdlr": "This paper introduces the use of large language models as decomposers to address the performance degradation and complexity issues in table-based reasoning, and outperforms existing methods significantly on multiple benchmark datasets."
}