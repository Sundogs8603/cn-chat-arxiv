{
    "title": "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) repor",
    "link": "http://arxiv.org/abs/2301.04558",
    "context": "Title: Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v2 [cs.CV] UPDATED)\nAbstract: Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) repor",
    "path": "papers/23/01/2301.04558.json",
    "total_tokens": 928,
    "translated_title": "学习利用时间结构进行生物医学视觉语言处理",
    "translated_abstract": "视觉语言处理中的自监督学习利用了成像和文本模态之间的语义对齐。生物医学VLP的先前工作大多依赖于单个图像和报告对的对齐，即使临床记录通常会涉及以前的图像。这不仅引入了模态之间差劲的对齐，而且错过了利用数据中现有时间内容的丰富自监督的机会。在这项工作中，我们在训练和微调期间明确考虑了之前的图像和报告（如果有）。我们的方法名为BioViL-T，使用了CNN-Transformer混合多图像编码器，与文本模型一起联合训练。它被设计成适用于出现的挑战，如姿态变化和缺失的时间内输入图像。得到的模型在单个和多图像设置中均表现优异，在分类、短语定位和报告可视化三个downstream任务上均实现了最先进的性能。",
    "tldr": "该论文提出了一种自监督学习的方法，利用时间内容丰富了生物医学视觉语言处理。该方法考虑了之前的图像和报告，使用了CNN-Transformer混合多图像编码器，实现了在单个和多图像设置中的最先进性能。",
    "en_tdlr": "This paper proposes a self-supervised learning method that exploits temporal content for biomedical vision-language processing. The approach accounts for prior images and reports, and uses a CNN-Transformer hybrid multi-image encoder to achieve state-of-the-art performance on both single- and multi-image setups for downstream tasks including progression classification, phrase grounding, and report visualization."
}