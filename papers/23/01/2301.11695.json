{
    "title": "LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v2 [stat.ML] UPDATED)",
    "abstract": "Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \\emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \\emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\\mathbb{R}^{C-1}$ and the projected probability simplex $\\tilde{\\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\\sc LegendreTron} as a novel and practical method that jointly learns \\emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently ou",
    "link": "http://arxiv.org/abs/2301.11695",
    "context": "Title: LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v2 [stat.ML] UPDATED)\nAbstract: Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \\emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \\emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\\mathbb{R}^{C-1}$ and the projected probability simplex $\\tilde{\\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\\sc LegendreTron} as a novel and practical method that jointly learns \\emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently ou",
    "path": "papers/23/01/2301.11695.json",
    "total_tokens": 921,
    "translated_title": "LegendreTron：升级版多类别正确多项损失学习",
    "translated_abstract": "损失函数是监督学习的基础，通常在模型开发之前选择。为避免选择损失函数可能出现的特定选择，统计决策理论描述了损失的一种理想属性，称为“正确性”，它断言贝叶斯规则是最优的。最近的研究尝试联合学习损失和模型。现有方法通过拟合一个将$\\mathbb{R}$单调映射到$[0,1]$的反解标准链接函数来估计二元问题的概率。本文通过使用凸函数梯度的单调性将单调性扩展到$\\mathbb{R}^{C-1}$到概率的正投影$\\tilde{\\Delta}^{C-1}$的映射上。我们提出了一种新颖而实用的方法{\\sc LegendreTron}，用于联合学习多类别问题的正确标准损失和概率。在最多1,000种类别的领域基准测试中，我们的实验结果表明，我们的方法始终优于其他基准方法。",
    "tldr": "本文提出了一种新颖和实用的方法{\\sc LegendreTron}，用于联合学习多类别问题的正确标准损失和概率。这种方法在基准测试中经常优于其他方法。",
    "en_tdlr": "This paper proposes a novel and practical method, LegendreTron, for jointly learning proper canonical losses and probabilities for multiclass problems. The method consistently outperformed other benchmark methods in testing with up to 1,000 classes."
}