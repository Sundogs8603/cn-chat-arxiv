{
    "title": "Explaining the effects of non-convergent sampling in the training of Energy-Based Models. (arXiv:2301.09428v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we quantify the impact of using non-convergent Markov chains to train Energy-Based models (EBMs). In particular, we show analytically that EBMs trained with non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data, not at the level of the equilibrium measure, but through a precise dynamical process. Our results provide a first-principles explanation for the observations of recent works proposing the strategy of using short runs starting from random initial conditions as an efficient way to generate high-quality samples in EBMs, and lay the groundwork for using EBMs as diffusion models. After explaining this effect in generic EBMs, we analyze two solvable models in which the effect of the non-convergent sampling in the trained parameters can be described in detail. Finally, we test these predictions numerically on a ConvNet EBM and a Boltzmann machine.",
    "link": "http://arxiv.org/abs/2301.09428",
    "context": "Title: Explaining the effects of non-convergent sampling in the training of Energy-Based Models. (arXiv:2301.09428v2 [cs.LG] UPDATED)\nAbstract: In this paper, we quantify the impact of using non-convergent Markov chains to train Energy-Based models (EBMs). In particular, we show analytically that EBMs trained with non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data, not at the level of the equilibrium measure, but through a precise dynamical process. Our results provide a first-principles explanation for the observations of recent works proposing the strategy of using short runs starting from random initial conditions as an efficient way to generate high-quality samples in EBMs, and lay the groundwork for using EBMs as diffusion models. After explaining this effect in generic EBMs, we analyze two solvable models in which the effect of the non-convergent sampling in the trained parameters can be described in detail. Finally, we test these predictions numerically on a ConvNet EBM and a Boltzmann machine.",
    "path": "papers/23/01/2301.09428.json",
    "total_tokens": 881,
    "translated_title": "解释非收敛采样对基于能量模型(Energy-Based Models, EBMs)训练的影响",
    "translated_abstract": "本文量化了使用非收敛马尔可夫链(Markov chains)训练EBMs的影响，证明了使用短的非持续运行来估计梯度进行训练的EBMs能够通过精确的动态过程完美复制数据的一组统计信息，而不是通过平衡测度的水平。我们的结果为最近提出的使用起始于随机初态的短跑作为EBMs中生成高质量样本的有效策略提供了一个根据，为使用EBMs作为扩散模型奠定了基础。在解释了这种效果在通用EBMs中的情况后，我们分析了两个可解的模型，其中描述了训练参数中非收敛采样的效果。最后，我们在ConvNet EBM和Boltzmann机器上进行了数字预测的测试。",
    "tldr": "本文证明，使用非持续运行的EBM训练可以通过一个精确的动态过程完美地复制数据集的一组统计信息，而不是通过平衡测度的水平，这为使用EBM作为扩散模型奠定了基础。",
    "en_tdlr": "This paper shows that training Energy-Based Models (EBMs) using non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data through a precise dynamical process, rather than at the level of the equilibrium measure. This lays the groundwork for using EBMs as diffusion models."
}