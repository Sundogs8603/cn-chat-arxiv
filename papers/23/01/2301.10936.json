{
    "title": "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation. (arXiv:2301.10936v2 [cs.LG] UPDATED)",
    "abstract": "Dynamic sparsity, where the sparsity patterns are unknown until runtime, poses a significant challenge to deep learning. The state-of-the-art sparsity-aware deep learning solutions are restricted to pre-defined, static sparsity patterns due to significant overheads associated with preprocessing. Efficient execution of dynamic sparse computation often faces the misalignment between the GPU-friendly tile configuration for efficient execution and the sparsity-aware tile shape that minimizes coverage wastes (non-zero values in tensor).  In this paper, we propose PIT, a deep-learning compiler for dynamic sparsity. PIT proposes a novel tiling mechanism that leverages Permutation Invariant Transformation (PIT), a mathematically proven property, to transform multiple sparsely located micro-tiles into a GPU-efficient dense tile without changing the computation results, thus achieving both high GPU utilization and low coverage waste. Given a model, PIT first finds feasible PIT rules for all its ",
    "link": "http://arxiv.org/abs/2301.10936",
    "context": "Title: PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation. (arXiv:2301.10936v2 [cs.LG] UPDATED)\nAbstract: Dynamic sparsity, where the sparsity patterns are unknown until runtime, poses a significant challenge to deep learning. The state-of-the-art sparsity-aware deep learning solutions are restricted to pre-defined, static sparsity patterns due to significant overheads associated with preprocessing. Efficient execution of dynamic sparse computation often faces the misalignment between the GPU-friendly tile configuration for efficient execution and the sparsity-aware tile shape that minimizes coverage wastes (non-zero values in tensor).  In this paper, we propose PIT, a deep-learning compiler for dynamic sparsity. PIT proposes a novel tiling mechanism that leverages Permutation Invariant Transformation (PIT), a mathematically proven property, to transform multiple sparsely located micro-tiles into a GPU-efficient dense tile without changing the computation results, thus achieving both high GPU utilization and low coverage waste. Given a model, PIT first finds feasible PIT rules for all its ",
    "path": "papers/23/01/2301.10936.json",
    "total_tokens": 882,
    "translated_title": "PIT: 通过排列不变变换优化动态稀疏深度学习模型",
    "translated_abstract": "动态稀疏性，在运行时才确定稀疏模式，对深度学习构成了重大挑战。现有的稀疏感知深度学习解决方案由于与预处理相关的重大开销，仅限于预定义的静态稀疏模式。动态稀疏计算的高效执行常常面临GPU友好瓷砖配置与最小化覆盖浪费（张量中的非零值）的稀疏感知瓷砖形状之间的不匹配。在本文中，我们提出了PIT，一种针对动态稀疏性的深度学习编译器。PIT提出了一种新颖的平铺机制，利用数学证明的排列不变变换(PIT)将多个稀疏位置的微瓷砖转变为GPU高效的稠密瓷砖，而不改变计算结果，从而实现高GPU利用率和低覆盖浪费。给定一个模型，PIT首先找到其所有可行的PIT规则",
    "tldr": "通过排列不变变换，提出了一种针对动态稀疏深度学习模型的编译器，实现了高GPU利用率和低覆盖浪费。",
    "en_tdlr": "A compiler for dynamic sparse deep learning models is proposed, which achieves high GPU utilization and low coverage waste by leveraging the Permutation Invariant Transformation."
}