{
    "title": "GPU-based Private Information Retrieval for On-Device Machine Learning Inference. (arXiv:2301.10904v3 [cs.CR] UPDATED)",
    "abstract": "On-device machine learning (ML) inference can enable the use of private user data on user devices without revealing them to remote servers. However, a pure on-device solution to private ML inference is impractical for many applications that rely on embedding tables that are too large to be stored on-device. In particular, recommendation models typically use multiple embedding tables each on the order of 1-10 GBs of data, making them impractical to store on-device. To overcome this barrier, we propose the use of private information retrieval (PIR) to efficiently and privately retrieve embeddings from servers without sharing any private information. As off-the-shelf PIR algorithms are usually too computationally intensive to directly use for latency-sensitive inference tasks, we 1) propose novel GPU-based acceleration of PIR, and 2) co-design PIR with the downstream ML application to obtain further speedup. Our GPU acceleration strategy improves system throughput by more than $20 \\times$",
    "link": "http://arxiv.org/abs/2301.10904",
    "context": "Title: GPU-based Private Information Retrieval for On-Device Machine Learning Inference. (arXiv:2301.10904v3 [cs.CR] UPDATED)\nAbstract: On-device machine learning (ML) inference can enable the use of private user data on user devices without revealing them to remote servers. However, a pure on-device solution to private ML inference is impractical for many applications that rely on embedding tables that are too large to be stored on-device. In particular, recommendation models typically use multiple embedding tables each on the order of 1-10 GBs of data, making them impractical to store on-device. To overcome this barrier, we propose the use of private information retrieval (PIR) to efficiently and privately retrieve embeddings from servers without sharing any private information. As off-the-shelf PIR algorithms are usually too computationally intensive to directly use for latency-sensitive inference tasks, we 1) propose novel GPU-based acceleration of PIR, and 2) co-design PIR with the downstream ML application to obtain further speedup. Our GPU acceleration strategy improves system throughput by more than $20 \\times$",
    "path": "papers/23/01/2301.10904.json",
    "total_tokens": 951,
    "translated_title": "基于GPU的设备上机器学习推断的私密信息检索",
    "translated_abstract": "设备上的机器学习推断可以在不将用户数据透露给远程服务器的情况下利用私密用户数据。然而，纯设备上的私密机器学习推断解决方案对于许多依赖于嵌入式表的应用来说是不实际的，这些嵌入式表的大小太大无法存储在设备上。特别是，推荐模型通常使用多个嵌入式表，每个表大约1-10GB的数据，这使得将它们存储在设备上变得不实际。为了克服这个障碍，我们提出了使用私密信息检索（PIR）从服务器有效且私密地检索嵌入式表，而不共享任何私密信息。由于现成的PIR算法通常对于延迟敏感的推断任务来说计算量过大，我们1）提出了一种基于GPU的PIR加速策略，并且2）与下游的机器学习应用程序共同设计PIR，以获得进一步的加速。我们的GPU加速策略将系统吞吐量提高了超过20倍。",
    "tldr": "本论文提出了一种基于GPU的私密信息检索的方法，可以在设备上进行机器学习推断，而无需将私密用户数据透露给远程服务器。通过加速私密信息检索并与下游机器学习应用程序共同设计，我们的方法极大地提高了系统吞吐量。",
    "en_tdlr": "This paper proposes a GPU-based approach for private information retrieval, enabling on-device machine learning inference without revealing private user data to remote servers. By accelerating private information retrieval and co-designing with downstream machine learning applications, our method greatly improves system throughput."
}