{
    "title": "Constrained Policy Optimization with Explicit Behavior Density for Offline Reinforcement Learning",
    "abstract": "arXiv:2301.12130v2 Announce Type: replace  Abstract: Due to the inability to interact with the environment, offline reinforcement learning (RL) methods face the challenge of estimating the Out-of-Distribution (OOD) points. Existing methods for addressing this issue either control policy to exclude the OOD action or make the $Q$ function pessimistic. However, these methods can be overly conservative or fail to identify OOD areas accurately. To overcome this problem, we propose a Constrained Policy optimization with Explicit Behavior density (CPED) method that utilizes a flow-GAN model to explicitly estimate the density of behavior policy. By estimating the explicit density, CPED can accurately identify the safe region and enable optimization within the region, resulting in less conservative learning policies. We further provide theoretical results for both the flow-GAN estimator and performance guarantee for CPED by showing that CPED can find the optimal $Q$-function value. Empirically,",
    "link": "https://arxiv.org/abs/2301.12130",
    "context": "Title: Constrained Policy Optimization with Explicit Behavior Density for Offline Reinforcement Learning\nAbstract: arXiv:2301.12130v2 Announce Type: replace  Abstract: Due to the inability to interact with the environment, offline reinforcement learning (RL) methods face the challenge of estimating the Out-of-Distribution (OOD) points. Existing methods for addressing this issue either control policy to exclude the OOD action or make the $Q$ function pessimistic. However, these methods can be overly conservative or fail to identify OOD areas accurately. To overcome this problem, we propose a Constrained Policy optimization with Explicit Behavior density (CPED) method that utilizes a flow-GAN model to explicitly estimate the density of behavior policy. By estimating the explicit density, CPED can accurately identify the safe region and enable optimization within the region, resulting in less conservative learning policies. We further provide theoretical results for both the flow-GAN estimator and performance guarantee for CPED by showing that CPED can find the optimal $Q$-function value. Empirically,",
    "path": "papers/23/01/2301.12130.json",
    "total_tokens": 817,
    "translated_title": "具有显式行为密度的约束策略优化用于线下强化学习",
    "translated_abstract": "由于无法与环境交互，线下强化学习（RL）方法面临着估计分布之外点（OOD）的挑战。现有方法要么控制策略以排除OOD动作，要么使$Q$函数变得悲观。然而，这些方法可能过于保守或无法准确识别OOD区域。为了克服这个问题，我们提出了一种利用flow-GAN模型显式估计行为策略密度的约束策略优化（CPED）方法。通过估计显式密度，CPED可以准确识别安全区域并在该区域内进行优化，从而得到更少保守的学习策略。我们进一步为flow-GAN估计器和CPED的性能保证提供了理论结果，表明CPED可以找到最优的$Q$函数值。",
    "tldr": "提出了一种具有显式行为密度的约束策略优化方法，通过使用flow-GAN模型来准确识别安全区域，实现了更少保守的学习策略。",
    "en_tdlr": "Proposed a constrained policy optimization method with explicit behavior density, utilizing a flow-GAN model to accurately identify the safe region and achieve less conservative learning policies."
}