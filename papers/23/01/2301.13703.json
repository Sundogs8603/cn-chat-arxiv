{
    "title": "Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning. (arXiv:2301.13703v2 [cs.LG] UPDATED)",
    "abstract": "Understanding when the noise in stochastic gradient descent (SGD) affects generalization of deep neural networks remains a challenge, complicated by the fact that networks can operate in distinct training regimes. Here we study how the magnitude of this noise $T$ affects performance as the size of the training set $P$ and the scale of initialization $\\alpha$ are varied. For gradient descent, $\\alpha$ is a key parameter that controls if the network is `lazy'($\\alpha\\gg1$) or instead learns features ($\\alpha\\ll1$). For classification of MNIST and CIFAR10 images, our central results are: (i) obtaining phase diagrams for performance in the $(\\alpha,T)$ plane. They show that SGD noise can be detrimental or instead useful depending on the training regime. Moreover, although increasing $T$ or decreasing $\\alpha$ both allow the net to escape the lazy regime, these changes can have opposite effects on performance. (ii) Most importantly, we find that the characteristic temperature $T_c$ where th",
    "link": "http://arxiv.org/abs/2301.13703",
    "context": "Title: Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning. (arXiv:2301.13703v2 [cs.LG] UPDATED)\nAbstract: Understanding when the noise in stochastic gradient descent (SGD) affects generalization of deep neural networks remains a challenge, complicated by the fact that networks can operate in distinct training regimes. Here we study how the magnitude of this noise $T$ affects performance as the size of the training set $P$ and the scale of initialization $\\alpha$ are varied. For gradient descent, $\\alpha$ is a key parameter that controls if the network is `lazy'($\\alpha\\gg1$) or instead learns features ($\\alpha\\ll1$). For classification of MNIST and CIFAR10 images, our central results are: (i) obtaining phase diagrams for performance in the $(\\alpha,T)$ plane. They show that SGD noise can be detrimental or instead useful depending on the training regime. Moreover, although increasing $T$ or decreasing $\\alpha$ both allow the net to escape the lazy regime, these changes can have opposite effects on performance. (ii) Most importantly, we find that the characteristic temperature $T_c$ where th",
    "path": "papers/23/01/2301.13703.json",
    "total_tokens": 1010,
    "translated_title": "《揭示随机梯度下降噪声在深度学习不同模式下的作用》",
    "translated_abstract": "深度神经网络的随机梯度下降（SGD）噪声何时会影响到其泛化效果一直是一个挑战，因为网络可以在不同的训练模式下运行。本文研究了如何通过改变训练集大小P和初始化规模a的方法来控制SGD噪声大小T对性能的影响。对于MNIST和CIFAR10图像分类，我们的主要贡献有：（i）在（a，T）平面上得到性能的相图。它们表明，根据训练模式，SGD噪声可能有害或有益。此外，尽管增加T或减小α都能使网络跳出惰性模式，但这些变化对性能的影响可能是相反的。 （ii）最重要的是，我们发现特征温度Tc与训练集大小存在相似的尺度关系，在训练集为1000～10000之间（这是许多应用的实际训练集大小）时，Tc在深度网络中的性能达到最佳水平。",
    "tldr": "本文通过改变训练集大小P和初始化规模a的方法来控制SGD噪声大小T对于MNIST和CIFAR10图像分类的影响，得到SGD噪声可能有害或有益的相图，同时发现特征温度Tc存在相似的尺度关系，在训练集为1000～10000之间时达到最佳水平。",
    "en_tdlr": "This paper studies how the magnitude of SGD noise affects performance by varying the size of the training set and the scale of initialization for MNIST and CIFAR10 image classification, obtaining phase diagrams for performance and discovering a similarity relation between the characteristic temperature Tc and the size of training data."
}