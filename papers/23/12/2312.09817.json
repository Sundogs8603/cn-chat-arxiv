{
    "title": "Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space. (arXiv:2312.09817v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. In FL, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. The closest FL techniques to achieving such goals are the Bayesian FL methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. To improve scalability for larger models, one common Bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. In this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing $\\beta$-Predictive Bayes, a Bayesian FL algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter $\\beta$. This parameter is tuned to improve th",
    "link": "http://arxiv.org/abs/2312.09817",
    "context": "Title: Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space. (arXiv:2312.09817v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. In FL, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. The closest FL techniques to achieving such goals are the Bayesian FL methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. To improve scalability for larger models, one common Bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. In this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing $\\beta$-Predictive Bayes, a Bayesian FL algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter $\\beta$. This parameter is tuned to improve th",
    "path": "papers/23/12/2312.09817.json",
    "total_tokens": 929,
    "translated_title": "在预测空间中带有贝叶斯推断的校准一轮联邦学习",
    "translated_abstract": "联邦学习是指在分布在客户端中的数据集上训练模型，每个客户端的数据集是本地化且可能是异质的。在联邦学习中，小而噪声的数据集很常见，强调了需要能够表示预测的不确定性的良好校准模型。最接近实现这一目标的联邦学习技术是贝叶斯联邦学习方法，它从局部后验中收集参数样本，并将它们聚合以近似全局后验。为了提高更大模型的可扩展性，贝叶斯方法通常是通过将局部预测后验相乘来近似全局预测后验。本研究表明，这种方法会导致系统性的过于自信的预测结果。为了解决这个问题，我们提出了一种称为$\\beta$-Predictive Bayes的贝叶斯联邦学习算法，它在预测后验的混合和乘积之间进行插值，使用一个可调参数$\\beta$来实现。",
    "tldr": "本研究提出了一个名为$\\beta$-Predictive Bayes的贝叶斯联邦学习算法，在预测后验的混合和乘积之间进行插值，通过调整参数$\\beta$来解决现有方法中过于自信的预测问题。"
}