{
    "title": "LEFL: Low Entropy Client Sampling in Federated Learning",
    "abstract": "Federated learning (FL) is a machine learning paradigm where multiple clients collaborate to optimize a single global model using their private data. The global model is maintained by a central server that orchestrates the FL training process through a series of training rounds. In each round, the server samples clients from a client pool before sending them its latest global model parameters for further optimization. Naive sampling strategies implement random client sampling and fail to factor client data distributions for privacy reasons. Hence we propose LEFL, an alternative sampling strategy by performing a one-time clustering of clients based on their model's learned high-level features while respecting data privacy. This enables the server to perform stratified client sampling across clusters in every round. We show datasets of sampled clients selected with this approach yield a low relative entropy with respect to the global data distribution. Consequently, the FL training becom",
    "link": "https://arxiv.org/abs/2312.17430",
    "context": "Title: LEFL: Low Entropy Client Sampling in Federated Learning\nAbstract: Federated learning (FL) is a machine learning paradigm where multiple clients collaborate to optimize a single global model using their private data. The global model is maintained by a central server that orchestrates the FL training process through a series of training rounds. In each round, the server samples clients from a client pool before sending them its latest global model parameters for further optimization. Naive sampling strategies implement random client sampling and fail to factor client data distributions for privacy reasons. Hence we propose LEFL, an alternative sampling strategy by performing a one-time clustering of clients based on their model's learned high-level features while respecting data privacy. This enables the server to perform stratified client sampling across clusters in every round. We show datasets of sampled clients selected with this approach yield a low relative entropy with respect to the global data distribution. Consequently, the FL training becom",
    "path": "papers/23/12/2312.17430.json",
    "total_tokens": 849,
    "translated_title": "LEFL: 高却熵客户端采样在联邦学习中的应用",
    "translated_abstract": "联邦学习（FL）是一种机器学习范例，多个客户端协作使用私有数据优化单一全局模型。全局模型由中央服务器维护，通过一系列训练轮次来进行FL训练过程的编排。在每个轮次，服务器从客户端池中进行客户端采样，在发送最新的全局模型参数给客户端进行进一步优化之前。传统的采样策略使用随机客户端采样，但由于隐私原因未能考虑客户端数据分布。因此，我们提出了LEFL采样策略，通过根据模型学到的高级特征进行一次性客户端聚类，同时尊重数据隐私。这使得服务器能够在每一轮次中对簇进行分层客户端采样。我们展示通过这种方法选择的样本客户端数据集相对于全局数据分布的相对熵较低。因此，FL训练变得...",
    "tldr": "LEFL提出了一种以保护数据隐私为目标的联邦学习采样策略，通过对客户端数据进行聚类，实现在每一轮次中的分层采样，从而优化全局模型的训练效果。",
    "en_tdlr": "LEFL proposes a federated learning sampling strategy that aims to protect data privacy by clustering client data and performing stratified sampling in each round, improving the training effectiveness of the global model."
}