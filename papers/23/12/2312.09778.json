{
    "title": "Hypergraph-MLP: Learning on Hypergraphs without Message Passing",
    "abstract": "arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p",
    "link": "https://arxiv.org/abs/2312.09778",
    "context": "Title: Hypergraph-MLP: Learning on Hypergraphs without Message Passing\nAbstract: arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p",
    "path": "papers/23/12/2312.09778.json",
    "total_tokens": 866,
    "translated_title": "超图-MLP：在无需消息传递的超图上学习",
    "translated_abstract": "超图在建模包含两个以上实体的高阶关系数据中至关重要，在机器学习和信号处理中越来越受重视。许多超图神经网络利用在超图结构上的消息传递来增强节点表征学习，从而在超图节点分类等任务中取得了令人印象深刻的表现。然而，这些基于消息传递的模型面临着过度平滑以及在推理时对结构扰动的高延迟和敏感性等挑战。为了应对这些挑战，我们提出了一种另类方法，即将关于超图结构的信息集成到训练监督中，而无需明确的消息传递，从而在推理时也消除了对其的依赖。具体而言，我们引入了Hypergraph-MLP，一种新颖的用于超图结构数据的学习框架，其中学习模型是一个简单的多层感知机。",
    "tldr": "提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。",
    "en_tdlr": "Introduced a novel learning framework called Hypergraph-MLP for handling hypergraph-structured data, which integrates information about hypergraph structures into training supervision without message passing, thereby mitigating challenges of oversmoothing and structural perturbations during inference."
}