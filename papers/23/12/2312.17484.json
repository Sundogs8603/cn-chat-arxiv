{
    "title": "Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)",
    "abstract": "Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\\% to 74.5\\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent ",
    "link": "http://arxiv.org/abs/2312.17484",
    "context": "Title: Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)\nAbstract: Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\\% to 74.5\\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent ",
    "path": "papers/23/12/2312.17484.json",
    "total_tokens": 1030,
    "translated_title": "真实森林：通过干预而无需调整，实现大型语言模型的多尺度真实性",
    "translated_abstract": "尽管大型语言模型（LLM）在各种任务中取得了巨大成功，但它们存在生成幻觉的问题。我们引入了真实森林，一种通过使用多维正交探针揭示LLM中隐藏的真实表示来增强真实性的方法。具体而言，它通过将正交约束融入探针中来创建多个用于建模真实性的正交基。此外，我们引入了随机窥视，这是一种系统技术，考虑了序列中更广泛的位置范围，减小了LLM中辨别和生成真实特征之间的差距。通过采用这种方法，在TruthfulQA上将Llama-2-7B的真实性从40.8％提高到74.5％。类似地，在微调模型中也观察到了显著的改进。我们对探针使用了彻底的真实特征分析。我们的可视化结果显示，正交探针捕捉到互补的与真实相关的特征，形成了清晰定义的聚类，揭示了内在的真实性",
    "tldr": "该论文提出了一种名为真实森林的方法，通过使用多维正交探针，揭示隐藏的真实表示，从而增强大型语言模型中的真实性。作者将正交约束融入探针，创建不同的正交基，通过随机窥视技术，减小了模型生成和识别真实特征之间的差距。实验证明，该方法显著提高了模型的真实性。"
}