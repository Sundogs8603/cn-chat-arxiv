{
    "title": "Traces of Memorisation in Large Language Models for Code. (arXiv:2312.11658v2 [cs.CR] UPDATED)",
    "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentiall",
    "link": "http://arxiv.org/abs/2312.11658",
    "context": "Title: Traces of Memorisation in Large Language Models for Code. (arXiv:2312.11658v2 [cs.CR] UPDATED)\nAbstract: Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentiall",
    "path": "papers/23/12/2312.11658.json",
    "total_tokens": 824,
    "translated_title": "《代码中的大型语言模型中的记忆痕迹》",
    "translated_abstract": "大型语言模型因其生成类似人类文本的能力和在软件工程等领域的潜在应用而受到广泛关注。代码的大型语言模型通常是在从互联网上抓取的大规模未经过滤的源代码语料库上进行训练的。这些数据集的内容被记住，并且可以通过数据提取攻击被攻击者提取出来。在本文中，我们探讨了大型代码语言模型中的记忆痕迹，并将记忆痕迹率与针对自然语言的大型语言模型进行了比较。我们采用了一个现有的自然语言基准，并通过识别容易受到攻击的样本构建了一个代码基准。我们运行了两个基准对多种模型进行测试，并进行了数据提取攻击。我们发现，代码的大型语言模型像它们的自然语言对应物一样容易受到数据提取攻击。从被确定为潜在受攻击的训练数据中，我们发现...",
    "tldr": "大型语言模型在代码中有记忆痕迹，容易受到数据提取攻击。",
    "en_tdlr": "Large language models for code have traces of memorization, making them vulnerable to data extraction attacks."
}