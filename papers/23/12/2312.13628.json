{
    "title": "Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples. (arXiv:2312.13628v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted \\emph{adversarial examples}, which are generated through either well-conceived $\\mathcal{L}_p$-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer \\emph{where to attack}. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate \\textbf{C}ounterfactual \\textbf{AD}vers",
    "link": "http://arxiv.org/abs/2312.13628",
    "context": "Title: Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples. (arXiv:2312.13628v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted \\emph{adversarial examples}, which are generated through either well-conceived $\\mathcal{L}_p$-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer \\emph{where to attack}. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate \\textbf{C}ounterfactual \\textbf{AD}vers",
    "path": "papers/23/12/2312.13628.json",
    "total_tokens": 830,
    "translated_title": "如何发起攻击？一种灵感来源于因果关系的生成反事实对抗样本的方法",
    "translated_abstract": "深度神经网络（DNNs）已经被证明对精心设计的\"对抗样本\"易受攻击，这些攻击是通过受限或非受限的$\\mathcal{L}_p$范数生成的。然而，大多数方法假设对手可以任意修改任何特征，并忽视了数据的因果生成过程，这是不合理和不切实际的。通过考虑被低估的因果生成过程，我们首先通过因果关系的视角确定了DNNs的脆弱性的源头，然后给出了回答\"如何发起攻击\"的理论结果。其次，考虑到攻击干预对当前样本状态的影响，以生成更真实的对抗样本，我们提出了一个名为CADE的框架。",
    "tldr": "该论文通过考虑因果关系的视角，确定了深度神经网络（DNNs）脆弱性的源头，并提出了一种生成更真实的对抗样本的方法。"
}