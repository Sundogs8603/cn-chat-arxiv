{
    "title": "MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training",
    "abstract": "arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an \"after-thought\" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as",
    "link": "https://arxiv.org/abs/2312.08656",
    "context": "Title: MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training\nAbstract: arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an \"after-thought\" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as",
    "path": "papers/23/12/2312.08656.json",
    "total_tokens": 856,
    "translated_title": "MaxK-GNN: 探索加速图神经网络训练的理论速度极限",
    "translated_abstract": "在深度神经网络训练加速方面，GPU已经成为主流平台。 GPU在GNN上面临着诸多挑战，如工作负载不平衡和内存访问不规则，导致硬件利用不充分。现有解决方案例如PyG、DGL与cuSPARSE，以及GNNAdvisor框架部分解决了这些挑战，但内存流量仍然很显著。 我们认为，只有通过算法与系统创新的垂直优化才能实现显著的性能提升，而不是将加速优化视为“事后思考”（即（i）给定GNN算法，设计加速器，或（ii）给定硬件，主要优化GNN算法）。 本文介绍了MaxK-GNN，一种集成算法与系统创新的先进高性能GPU训练系统。 （i）我们引入了MaxK非线性并提供了MaxK非线性的理论分析，",
    "tldr": "MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。",
    "en_tdlr": "MaxK-GNN is an advanced high-performance GPU training system that achieves vertical optimization for graph neural network training through the MaxK nonlinearity and theoretical analysis."
}