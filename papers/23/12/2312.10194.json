{
    "title": "Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization",
    "abstract": "arXiv:2312.10194v3 Announce Type: replace  Abstract: A novel method, the Pareto Envelope Augmented with Reinforcement Learning (PEARL), has been developed to address the challenges posed by multi-objective problems, particularly in the field of engineering where the evaluation of candidate solutions can be time-consuming. PEARL distinguishes itself from traditional policy-based multi-objective Reinforcement Learning methods by learning a single policy, eliminating the need for multiple neural networks to independently solve simpler sub-problems. Several versions inspired from deep learning and evolutionary techniques have been crafted, catering to both unconstrained and constrained problem domains. Curriculum Learning is harnessed to effectively manage constraints in these versions. PEARL's performance is first evaluated on classical multi-objective benchmarks. Additionally, it is tested on two practical PWR core Loading Pattern optimization problems to showcase its real-world applicab",
    "link": "https://arxiv.org/abs/2312.10194",
    "context": "Title: Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization\nAbstract: arXiv:2312.10194v3 Announce Type: replace  Abstract: A novel method, the Pareto Envelope Augmented with Reinforcement Learning (PEARL), has been developed to address the challenges posed by multi-objective problems, particularly in the field of engineering where the evaluation of candidate solutions can be time-consuming. PEARL distinguishes itself from traditional policy-based multi-objective Reinforcement Learning methods by learning a single policy, eliminating the need for multiple neural networks to independently solve simpler sub-problems. Several versions inspired from deep learning and evolutionary techniques have been crafted, catering to both unconstrained and constrained problem domains. Curriculum Learning is harnessed to effectively manage constraints in these versions. PEARL's performance is first evaluated on classical multi-objective benchmarks. Additionally, it is tested on two practical PWR core Loading Pattern optimization problems to showcase its real-world applicab",
    "path": "papers/23/12/2312.10194.json",
    "total_tokens": 837,
    "translated_title": "基于多目标强化学习的压水堆优化方法",
    "translated_abstract": "一种新颖的方法，被称为带有强化学习的Pareto包络（PEARL），已被开发出来以解决多目标问题所带来的挑战，特别是在工程领域中，候选解的评估可能会耗费大量时间。PEARL通过学习单一策略来区别于传统基于策略的多目标强化学习方法，消除了需要多个神经网络独立解决简单子问题的需求。受深度学习和进化技术启发，已经开发了几个版本，以适应无约束和有约束的问题领域。课程学习被利用来有效管理这些版本中的约束。首先，PEARL的性能在经典多目标基准上进行了评估。此外，它还在两个实际的PWR堆芯装载方案优化问题上进行了测试，以展示其在实际应用中的可行性。",
    "tldr": "提出了一种新的多目标强化学习方法PEARL，通过学习单一策略解决多目标问题，避免了需要多个神经网络解决简单子问题的需求。",
    "en_tdlr": "Introduced a novel multi-objective reinforcement learning method PEARL which learns a single policy to solve multi-objective problems, eliminating the need for multiple neural networks to independently solve simpler sub-problems."
}