{
    "title": "Compressed Context Memory For Online Language Model Interaction",
    "abstract": "This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac",
    "link": "https://arxiv.org/abs/2312.03414",
    "context": "Title: Compressed Context Memory For Online Language Model Interaction\nAbstract: This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac",
    "path": "papers/23/12/2312.03414.json",
    "total_tokens": 836,
    "translated_title": "在线语言模型交互中的压缩上下文记忆",
    "translated_abstract": "本论文介绍了一种用于在线场景中Transformer语言模型的上下文键/值压缩方法，其中上下文不断扩展。随着上下文的增加，注意力过程需要更多的内存和计算，进而降低语言模型的吞吐量。为了解决这个挑战，我们提出了一个压缩上下文记忆系统，将累积的注意力键/值对不断压缩到紧凑的内存空间中，以便在计算环境的有限内存空间中进行语言模型推断。我们的压缩过程涉及将轻量级的条件LoRA整合到语言模型的前向传递中进行推断，而无需微调模型的所有权重。通过将递归压缩过程建模为单个并行化的前向计算，我们实现了高效的训练。通过对对话、个性化和多任务学习的评估，我们证明了我们的方法的有效性。",
    "tldr": "本论文提出了一种用于在线场景中Transformer语言模型的压缩上下文记忆系统，可以在有限的内存空间中实现语言模型推断，提高吞吐量，并通过个性化和多任务学习的评估证明了其有效性。",
    "en_tdlr": "This paper presents a compressed context memory system for Transformer language models in online scenarios, allowing for inference in a limited memory space, improving throughput, and demonstrating its effectiveness through evaluations on personalization and multi-task learning."
}