{
    "title": "Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)",
    "abstract": "Catastrophic Forgetting (CF) is a prominent issue in continual learning. Parameter isolation addresses this challenge by masking a sub-network for each task to mitigate interference with old tasks. However, these sub-networks are constructed relying on weight magnitude, which does not necessarily correspond to the importance of weights, resulting in maintaining unimportant weights and constructing redundant sub-networks. To overcome this limitation, inspired by information bottleneck, which removes redundancy between adjacent network layers, we propose \\textbf{\\underline{I}nformation \\underline{B}ottleneck \\underline{M}asked sub-network (IBM)} to eliminate redundancy within sub-networks. Specifically, IBM accumulates valuable information into essential weights to construct redundancy-free sub-networks, not only effectively mitigating CF by freezing the sub-networks but also facilitating new tasks training through the transfer of valuable knowledge. Additionally, IBM decomposes hidden r",
    "link": "http://arxiv.org/abs/2312.00840",
    "context": "Title: Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)\nAbstract: Catastrophic Forgetting (CF) is a prominent issue in continual learning. Parameter isolation addresses this challenge by masking a sub-network for each task to mitigate interference with old tasks. However, these sub-networks are constructed relying on weight magnitude, which does not necessarily correspond to the importance of weights, resulting in maintaining unimportant weights and constructing redundant sub-networks. To overcome this limitation, inspired by information bottleneck, which removes redundancy between adjacent network layers, we propose \\textbf{\\underline{I}nformation \\underline{B}ottleneck \\underline{M}asked sub-network (IBM)} to eliminate redundancy within sub-networks. Specifically, IBM accumulates valuable information into essential weights to construct redundancy-free sub-networks, not only effectively mitigating CF by freezing the sub-networks but also facilitating new tasks training through the transfer of valuable knowledge. Additionally, IBM decomposes hidden r",
    "path": "papers/23/12/2312.00840.json",
    "total_tokens": 950,
    "translated_title": "朝着无冗余子网络的不断学习之路",
    "translated_abstract": "在不断学习中，灾难性遗忘是一个突出的问题。参数隔离通过遮蔽每个任务的子网络来解决这个挑战，以减轻对旧任务的干扰。然而，这些子网络是基于权重大小构建的，而权重大小并不一定对应权重的重要性，导致保留不重要的权重并构建冗余的子网络。为了克服这个限制，我们受到信息瓶颈的启发，该方法消除了相邻网络层之间的冗余。我们提出了信息瓶颈遮蔽子网络 (IBM) 来消除子网络内的冗余。具体地，IBM将有价值的信息累积到重要的权重中，构建无冗余的子网络，不仅通过冻结子网络有效地减轻了灾难性遗忘，还通过有价值的知识传递促进了新任务的训练。此外，IBM分解了隐藏的r",
    "tldr": "本研究提出了一种名为信息瓶颈遮蔽子网络 (IBM) 的方法，通过消除子网络内部的冗余来解决不断学习中的灾难性遗忘问题。该方法通过累积有价值的信息到重要的权重中，构建了无冗余的子网络，有效地减轻了灾难性遗忘，并促进了新任务的训练。",
    "en_tdlr": "This study proposes a method called Information Bottleneck Masked sub-network (IBM) to address the issue of catastrophic forgetting in continual learning. The IBM method eliminates redundancy within sub-networks by accumulating valuable information into essential weights, resulting in the construction of redundancy-free sub-networks. This approach effectively mitigates catastrophic forgetting and facilitates training of new tasks."
}