{
    "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
    "abstract": "arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset",
    "link": "https://arxiv.org/abs/2312.02554",
    "context": "Title: ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference\nAbstract: arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset",
    "path": "papers/23/12/2312.02554.json",
    "total_tokens": 853,
    "translated_title": "ULMA：人类演示和逐点偏好统一语言模型对齐",
    "translated_abstract": "将语言模型与人类期望（例如，有益和无害）对齐已成为大型语言模型的迫切挑战。典型的对齐过程包括监督微调和偏好学习。大多数偏好学习方法（如RLHF和DPO）依赖于成对偏好数据，这并不充分地解决人类反馈是逐点的情况，导致潜在信息丢失和性能次优。为了解决这一问题，我们引入了逐点直接偏好优化，一种旨在有效利用逐点反馈的新颖偏好学习方法。我们的工作还揭示了监督微调和逐点偏好学习之间的新颖联系，最终形成了统一语言模型对齐，这是一种将对齐与人类演示和逐点偏好统一的单步方法。在逐点偏好数据集上进行了大量实验。",
    "tldr": "提出了一种逐点直接偏好优化方法，用于统一语言模型对齐，通过将人类演示和逐点偏好相结合，解决了偏好学习中存在的信息丢失和性能次优问题。",
    "en_tdlr": "Proposed a method of Point-wise Direct Preference Optimization for Unified Language Model Alignment, which combines human demonstrations and point-wise preferences to address the issues of information loss and suboptimal performance in preference learning."
}