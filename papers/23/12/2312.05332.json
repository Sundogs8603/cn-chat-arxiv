{
    "title": "Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)",
    "abstract": "In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a",
    "link": "http://arxiv.org/abs/2312.05332",
    "context": "Title: Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)\nAbstract: In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a",
    "path": "papers/23/12/2312.05332.json",
    "total_tokens": 927,
    "translated_title": "消除差距：基于模型预测控制的可验证模型无关二次规划控制器的学习",
    "translated_abstract": "本文介绍了一种新的参数化控制器类，受到模型预测控制（MPC）的启发。该控制器类似于线性MPC问题的二次规划（QP）求解器，但控制器的参数是通过深度强化学习（DRL）进行训练，而不是从系统模型中推导出来的。这种方法解决了常见控制器中使用MLP或其他通用神经网络架构的DRL的可验证性和性能保证的局限性，并且所学习的控制器具有与MPC类似的持续可行性和渐近稳定性等可验证属性。另一方面，数值实验表明，所提出的控制器在控制性能上与MPC和MLP控制器相匹配，并且对建模不确定性和噪声具有更优的鲁棒性。此外，所提出的控制器在计算效率上明显优于MPC。",
    "tldr": "本文提出了一种新的参数化控制器类，利用深度强化学习训练其控制器的参数，从而消除了常见控制器中的可验证性和性能保证的限制。该控制器类似于模型预测控制问题的二次规划求解器，具有可验证的属性，并且在控制性能和鲁棒性方面与其他控制器相媲美。同时，该控制器的计算效率显著优于传统的模型预测控制。"
}