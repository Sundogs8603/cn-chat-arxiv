{
    "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
    "abstract": "arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d",
    "link": "https://arxiv.org/abs/2312.02073",
    "context": "Title: A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia\nAbstract: arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d",
    "path": "papers/23/12/2312.02073.json",
    "total_tokens": 853,
    "translated_title": "一剂病毒？使用Fakepedia定位和检测语言模型的潜在问题",
    "translated_abstract": "大型语言模型（LLMs）具有从其上下文中提供的新颖信息中获得的出色能力。然而，尚不清楚在上下文信息与参数中存储的事实知识相矛盾的情况下，支撑这种上下文基础的机制，LLMs在回忆方面也表现出色。偏好上下文信息对于检索增强生成方法至关重要，这些方法通过将上下文与最新信息丰富，希望基础可以纠正过时或有噪声的存储知识。我们提出了一种使用Fakepedia的新颖方法来研究基础能力，这是一个用于与模型内部参数知识冲突的反事实文本数据集。我们使用Fakepedia对各种LLMs进行基准测试，然后我们进行因果中介分析，基于我们的遮蔽分组因果追踪（MGCT），对回答Fakepedia查询时的LLM组件进行分析。在这个分析中，我们鉴别出d",
    "tldr": "通过Fakepedia数据集研究语言模型的基础能力和进行因果中介分析，以解决上下文信息与存储知识相矛盾的问题。"
}