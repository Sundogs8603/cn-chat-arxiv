{
    "title": "Automating Continual Learning",
    "abstract": "arXiv:2312.00276v2 Announce Type: replace  Abstract: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves \"in-context catastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification da",
    "link": "https://arxiv.org/abs/2312.00276",
    "context": "Title: Automating Continual Learning\nAbstract: arXiv:2312.00276v2 Announce Type: replace  Abstract: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves \"in-context catastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification da",
    "path": "papers/23/12/2312.00276.json",
    "total_tokens": 819,
    "translated_title": "自动化持续学习",
    "translated_abstract": "一般用途的学习系统应在不断变化的环境中以开放式方式不断改进自身。然而，神经网络的传统学习算法常常遭受灾难性遗忘（CF）-当学习新任务时，先前获得的技能被遗忘。我们提出了自动化持续学习（ACL）来训练自引用神经网络，使其元学习自身的上下文持续(meta-)学习算法。ACL将所有期望表现良好于新旧任务的要求编码到其元学习目标中。我们的实验表明ACL有效地解决了“上下文灾难性遗忘”; 我们通过ACL学到的算法在无重放设置下优于手工制定的算法，例如在Split-MNIST基准测试中，并且可以持续学习由多个少量示例和标准图像分类数据组成的各种任务。",
    "tldr": "提出了自动化持续学习（ACL）来训练自引用神经网络，使其元学习自身的上下文持续学习算法，并在解决“上下文灾难性遗忘”方面取得成功。",
    "en_tdlr": "Introduced Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual learning algorithms, effectively solving \"in-context catastrophic forgetting\"."
}