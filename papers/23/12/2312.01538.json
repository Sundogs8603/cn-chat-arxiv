{
    "title": "Recurrent Distance Filtering for Graph Representation Learning",
    "abstract": "Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with ",
    "link": "https://arxiv.org/abs/2312.01538",
    "context": "Title: Recurrent Distance Filtering for Graph Representation Learning\nAbstract: Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with ",
    "path": "papers/23/12/2312.01538.json",
    "total_tokens": 905,
    "translated_title": "递归距离过滤器用于图表示学习",
    "translated_abstract": "基于迭代一跳信息传递的图神经网络在有效利用远距离节点的信息方面存在困难。相反，图变换器允许每个节点直接关注所有其他节点，但缺乏图的归纳偏差并且必须依赖于特定的位置编码。在本文中，我们提出了一种新的架构来解决这些挑战。我们的方法源自于在顺序数据上提供的深度状态空间模型在长距离建模方面的最新突破：对于给定的目标节点，我们的模型通过目标节点到其他节点的最短距离来聚合其他节点，并使用线性RNN对跳代表的序列进行编码。线性RNN以特定对角形式参数化，以实现稳定的长距离信号传播，并在理论上具有足够的表达能力来编码邻居层次结构。在不需要位置编码的情况下，我们经验证明，我们模型的性能与其他模型相比具有很高的竞争力。",
    "tldr": "本文提出了一种新的图网络架构，利用最短距离对节点进行聚合，并使用线性RNN对跳代表的序列进行编码，用以解决图神经网络在远距离节点信息利用上的困难，该模型在实验中表现出与其他模型相当的竞争力。",
    "en_tdlr": "This paper proposes a new architecture for graph representation learning that aggregates nodes based on shortest distances and uses a linear RNN to encode hop representations. It addresses the challenge of effectively utilizing information from distant nodes and achieves competitive performance compared to other models."
}