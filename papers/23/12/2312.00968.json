{
    "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
    "abstract": "arXiv:2312.00968v2 Announce Type: replace-cross  Abstract: Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach ",
    "link": "https://arxiv.org/abs/2312.00968",
    "context": "Title: Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts\nAbstract: arXiv:2312.00968v2 Announce Type: replace-cross  Abstract: Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach ",
    "path": "papers/23/12/2312.00968.json",
    "total_tokens": 860,
    "translated_title": "Omni-SMoLA: 用软低秩专家的混合提升通用多模态模型",
    "translated_abstract": "大型多模态模型（LMMs）在许多任务中表现出色。然而，通用LMMs在调优大量任务时往往会出现性能下降。最近的研究表明，专家混合（MoE）架构对指导调优很有用，但对于参数大小约为O(50-100B)的LMMs，复制和存储专家模型的成本限制了我们可以使用的专家数量。我们提出了Omni-SMoLA，这种架构使用软MoE方法（软地）混合许多多模态低秩专家，并避免引入与传统MoE模型相比显著数量的新参数。这里的核心直觉是，大型模型提供了基础支撑，而不同的轻量级专家在专门知识上残余学习，可以是单模态的或多模态的。大量实验证明SMoLA方法",
    "tldr": "Omni-SMoLA提出了使用软MoE方法混合多个多模态低秩专家的架构，避免引入大量新参数，以提升通用多模态模型的性能。",
    "en_tdlr": "Omni-SMoLA proposes an architecture that uses the Soft Mixture of Experts approach to mix many multimodal low-rank experts, avoiding introducing a significant number of new parameters compared to conventional MoE models, in order to boost the performance of generalist multimodal models."
}