{
    "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective",
    "abstract": "Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \\emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \\emph{offline} settings with pre-collected data, where states are obtained from a \\emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal",
    "link": "https://arxiv.org/abs/2312.00054",
    "context": "Title: Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective\nAbstract: Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \\emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \\emph{offline} settings with pre-collected data, where states are obtained from a \\emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal",
    "path": "papers/23/12/2312.00054.json",
    "total_tokens": 857,
    "translated_title": "逆向强化学习比标准强化学习更困难吗？一个理论的观点",
    "translated_abstract": "逆向强化学习（IRL）是从专家策略的示范中学习奖励函数的问题，在开发智能系统中起着关键作用。尽管在应用中广泛使用，但与标准强化学习相比，IRL的理论理解存在独特的挑战，且发展相对较少。本文首次提出了使用多项式样本和运行时间在标准离线和在线设置下进行高效IRL的结果线索。我们的算法和分析巧妙地采用了离线强化学习中常用的悲观原则，并在比现有工作中考虑的更强的度量标准下实现了IRL的保证。我们提供了下界，表明我们的样本复杂性几乎是最优的。",
    "tldr": "逆向强化学习是从专家策略示范中学习奖励函数的问题，本文提出了在标准离线和在线设置下用多项式样本和运行时间进行高效逆向强化学习的结果线索，并提供了几乎最优的样本复杂性的下界。"
}