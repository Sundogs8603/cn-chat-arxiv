{
    "title": "UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray Classification",
    "abstract": "arXiv:2312.11038v2 Announce Type: replace-cross  Abstract: Vision-Language Pre-training (VLP) that utilizes the multi-modal information to promote the training efficiency and effectiveness, has achieved great success in vision recognition of natural domains and shown promise in medical imaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly pay attention to the exploration on single dataset of CXRs, which locks the potential of this powerful paradigm on larger hybrid of multi-source CXRs datasets. We identify that although blending samples from the diverse sources offers the advantages to improve the model generalization, it is still challenging to maintain the consistent superiority for the task of each source due to the existing heterogeneity among sources. To handle this dilemma, we design a Conquer-and-Divide pre-training framework, termed as UniChest, aiming to make full use of the collaboration benefit of multiple sources of CXRs while reducing the negative i",
    "link": "https://arxiv.org/abs/2312.11038",
    "context": "Title: UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray Classification\nAbstract: arXiv:2312.11038v2 Announce Type: replace-cross  Abstract: Vision-Language Pre-training (VLP) that utilizes the multi-modal information to promote the training efficiency and effectiveness, has achieved great success in vision recognition of natural domains and shown promise in medical imaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly pay attention to the exploration on single dataset of CXRs, which locks the potential of this powerful paradigm on larger hybrid of multi-source CXRs datasets. We identify that although blending samples from the diverse sources offers the advantages to improve the model generalization, it is still challenging to maintain the consistent superiority for the task of each source due to the existing heterogeneity among sources. To handle this dilemma, we design a Conquer-and-Divide pre-training framework, termed as UniChest, aiming to make full use of the collaboration benefit of multiple sources of CXRs while reducing the negative i",
    "path": "papers/23/12/2312.11038.json",
    "total_tokens": 702,
    "translated_title": "UniChest：征服分割预训练用于多源胸部X射线分类",
    "translated_abstract": "Vision-Language Pre-training (VLP)利用多模态信息促进训练效率和有效性，在自然领域视觉识别取得巨大成功，并在胸部X射线（CXR）的医学影像诊断中显示出潜力。本文提出了一个名为UniChest的Conquer-and-Divide预训练框架，旨在充分利用多个CXR来源的合作效益，同时减少负面影响。",
    "tldr": "提出了UniChest框架，采用征服和分割的预训练方法，使得模型能够充分利用多个来源的胸部X射线数据，提高模型泛化能力。",
    "en_tdlr": "Introducing the UniChest framework with Conquer-and-Divide pre-training method to leverage multiple sources of chest X-ray data, enhancing model generalization."
}