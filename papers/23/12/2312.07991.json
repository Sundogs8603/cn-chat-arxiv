{
    "title": "Accelerating the Global Aggregation of Local Explanations. (arXiv:2312.07991v3 [cs.LG] UPDATED)",
    "abstract": "Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a na\\\"ive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. % We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-$k$ words with the highest global impact according to different a",
    "link": "http://arxiv.org/abs/2312.07991",
    "context": "Title: Accelerating the Global Aggregation of Local Explanations. (arXiv:2312.07991v3 [cs.LG] UPDATED)\nAbstract: Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a na\\\"ive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. % We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-$k$ words with the highest global impact according to different a",
    "path": "papers/23/12/2312.07991.json",
    "total_tokens": 757,
    "translated_title": "加速局部解释的全局汇总",
    "translated_abstract": "局部解释方法突出显示对文档分类结果有重要影响的输入标记。例如，Anchor算法对分类器对标记变化的敏感性进行了统计分析。对数据集进行局部解释的全局汇总提供了模型的全局解释。这种汇总旨在检测对模型影响最大的单词，从而提供宝贵的对模型的理解，例如模型在训练中学到了什么以及哪些对抗性示例暴露了模型的弱点。然而，标准的汇总方法计算成本高：简单的实现对每个文档的每个标记应用昂贵的算法，因此在短期分析会话的范围内对一个普通用户来说是不可行的。",
    "tldr": "我们提出了加速Anchor算法的全局汇总技术，旨在计算出对模型影响最大的前k个单词。"
}