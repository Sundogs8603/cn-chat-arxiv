{
    "title": "Policy Optimization in RLHF: The Impact of Out-of-preference Data",
    "abstract": "arXiv:2312.10584v2 Announce Type: replace  Abstract: Aligning intelligent agents with human preferences and values is important. This paper examines two popular alignment methods: Direct Preference Optimization (DPO) and Reward-Model-Based Policy Optimization (RMB-PO). A variant of RMB-PO, referred to as RMB-PO+ is also considered. These methods, either explicitly or implicitly, learn a reward model from preference data and differ in the data used for policy optimization to unlock the generalization ability of the reward model. In particular, compared with DPO, RMB-PO additionally uses policy-generated data, and RMB-PO+ further leverages new, preference-free data. We examine the impact of such out-of-preference data. Our study, conducted through controlled and synthetic experiments, demonstrates that DPO performs poorly, whereas RMB-PO+ performs the best. In particular, even when providing the policy model with a good feature representation, we find that policy optimization with adequa",
    "link": "https://arxiv.org/abs/2312.10584",
    "context": "Title: Policy Optimization in RLHF: The Impact of Out-of-preference Data\nAbstract: arXiv:2312.10584v2 Announce Type: replace  Abstract: Aligning intelligent agents with human preferences and values is important. This paper examines two popular alignment methods: Direct Preference Optimization (DPO) and Reward-Model-Based Policy Optimization (RMB-PO). A variant of RMB-PO, referred to as RMB-PO+ is also considered. These methods, either explicitly or implicitly, learn a reward model from preference data and differ in the data used for policy optimization to unlock the generalization ability of the reward model. In particular, compared with DPO, RMB-PO additionally uses policy-generated data, and RMB-PO+ further leverages new, preference-free data. We examine the impact of such out-of-preference data. Our study, conducted through controlled and synthetic experiments, demonstrates that DPO performs poorly, whereas RMB-PO+ performs the best. In particular, even when providing the policy model with a good feature representation, we find that policy optimization with adequa",
    "path": "papers/23/12/2312.10584.json",
    "total_tokens": 890,
    "translated_title": "RLHF中的政策优化：超出偏好数据的影响",
    "translated_abstract": "本文研究了两种流行的智能体与人类偏好和价值观对齐的方法：直接偏好优化 (DPO) 和基于奖励模型的政策优化 (RMB-PO)。还考虑了RMB-PO的变体，称为RMB-PO+。这些方法通过偏好数据明确或隐式地学习奖励模型，在用于政策优化的数据中有所不同，以解锁奖励模型的泛化能力。具体来说，与DPO相比，RMB-PO还使用策略生成的数据，而RMB-PO+进一步利用新的无偏好数据。我们研究了这种超出偏好数据的影响。通过受控和合成实验进行的研究表明，DPO表现不佳，而RMB-PO+表现最佳。特别是，即使为策略模型提供了良好的特征表示，我们发现用充分的政策优化进行时",
    "tldr": "本研究比较了直接偏好优化与基于奖励模型的政策优化方法，研究表明超出偏好数据对政策优化的影响，发现在实验证实中，RMB-PO+表现最佳。",
    "en_tdlr": "This study compares Direct Preference Optimization and Reward-Model-Based Policy Optimization methods, investigates the impact of out-of-preference data on policy optimization, and finds that in empirical experiments, RMB-PO+ performs the best."
}