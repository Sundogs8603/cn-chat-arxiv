{
    "title": "Best Arm Identification with Fixed Budget: A Large Deviation Perspective",
    "abstract": "arXiv:2312.12137v2 Announce Type: replace  Abstract: We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \\sr (Successive Re",
    "link": "https://arxiv.org/abs/2312.12137",
    "context": "Title: Best Arm Identification with Fixed Budget: A Large Deviation Perspective\nAbstract: arXiv:2312.12137v2 Announce Type: replace  Abstract: We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \\sr (Successive Re",
    "path": "papers/23/12/2312.12137.json",
    "total_tokens": 800,
    "translated_title": "带有固定预算的最佳臂识别：大偏差视角",
    "translated_abstract": "我们考虑使用固定抽样预算在随机多臂老虎机(MABs)中识别最佳臂的问题。表征该问题的最小特定实例误差概率构成MABs中一直存在的重要开放问题之一。当使用静态抽样策略选择臂时，错误概率随着样本数呈指数衰减，其速率可以通过大偏差技术明确推导。然而，分析具有自适应抽样策略的算法的性能要困难得多。在本文中，我们建立了通过经验比例满足的大偏差原理(LDP)和通过经验臂奖励满足的LDP之间的连接。这种连接适用于任何自适应算法，并被利用来( i ) 提高某些现有算法的错误概率上界，例如著名的\\sr (Successive Re",
    "tldr": "本文通过建立经验比例和经验臂奖励之间的连接，提高了一些现有算法的错误概率上界。",
    "en_tdlr": "The paper improves error probability upper bounds of some existing algorithms by establishing a connection between the empirical proportions and the empirical arm rewards."
}