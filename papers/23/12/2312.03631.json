{
    "title": "Mitigating Open-Vocabulary Caption Hallucinations",
    "abstract": "arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in",
    "link": "https://arxiv.org/abs/2312.03631",
    "context": "Title: Mitigating Open-Vocabulary Caption Hallucinations\nAbstract: arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in",
    "path": "papers/23/12/2312.03631.json",
    "total_tokens": 795,
    "translated_title": "缓解开放词汇描述幻觉",
    "translated_abstract": "近年来，图像条件的文本生成取得了快速进展，但图像字幕仍然存在幻觉的基本问题，即生成与给定图像无法推断的虚假细节。现有方法在图像字幕中大多使用封闭词汇对象列表来缓解或评估幻觉，忽略了实践中发生的大多数幻觉类型。为此，我们提出了一个框架，以应对开放词汇设置中图像字幕中的幻觉，包括量化它们的存在并优化以减轻这种幻觉。我们的OpenCHAIR基准利用生成基础模型来评估开放词汇描述幻觉，在多样性和准确性方面都超过了流行的CHAIR基准。为了在序列级别上缓解开放词汇的幻觉，我们提出了MOCHa，一种利用进展的方法",
    "tldr": "提出了在开放词汇设置中解决图像字幕幻觉问题的框架，并提出了一种新方法MOCHa来缓解幻觉",
    "en_tdlr": "Proposed a framework to address image caption hallucinations in open-vocabulary setting and introduced a new approach MOCHa to mitigate the hallucinations."
}