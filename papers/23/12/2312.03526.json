{
    "title": "On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm",
    "abstract": "arXiv:2312.03526v2 Announce Type: replace-cross  Abstract: Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprisin",
    "link": "https://arxiv.org/abs/2312.03526",
    "context": "Title: On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm\nAbstract: arXiv:2312.03526v2 Announce Type: replace-cross  Abstract: Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprisin",
    "path": "papers/23/12/2312.03526.json",
    "total_tokens": 834,
    "translated_title": "关于数据集的多样性和现实性：一种高效的数据集精炼范式",
    "translated_abstract": "当今的机器学习要求在庞大的数据集上训练大型神经网络，因此面临高计算需求的挑战。数据集精炼作为一种最近兴起的策略，旨在压缩真实世界的数据集以进行高效训练。然而，这一研究领域目前在处理大规模和高分辨率的数据集时存在困难，阻碍了其实用性和可行性。为此，我们重新审视现有的数据集精炼方法，并确定了用于大规模真实世界应用的三个所需属性，即现实性，多样性和效率。为了解决这个问题，我们提出了RDED，一种新颖的计算效率高且有效的数据精炼范式，以实现数据的多样性和现实性。对各种神经结构和数据集进行的大量实证结果表明了RDED的进展：我们可以将完整的ImageNet-1K精炼为一个小数据集",
    "tldr": "我们提出了一种新颖的计算效率高且有效的数据精炼范式RDED，以实现数据的多样性和现实性。"
}