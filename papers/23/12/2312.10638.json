{
    "title": "HyperPIE: Hyperparameter Information Extraction from Scientific Publications. (arXiv:2312.10638v2 [cs.CL] UPDATED)",
    "abstract": "Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves a",
    "link": "http://arxiv.org/abs/2312.10638",
    "context": "Title: HyperPIE: Hyperparameter Information Extraction from Scientific Publications. (arXiv:2312.10638v2 [cs.CL] UPDATED)\nAbstract: Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves a",
    "path": "papers/23/12/2312.10638.json",
    "total_tokens": 874,
    "translated_title": "HyperPIE: 从科学论文中提取超参数信息",
    "translated_abstract": "从论文中自动提取信息是实现科学知识机器可读化的关键。提取出的信息可以促进学术搜索、决策制定和知识图谱构建。现有方法没有涵盖的一类重要信息是超参数信息。在本文中，我们将超参数信息提取（HyperPIE）形式化为实体识别和关系提取任务，并创建了一个标记数据集来涵盖各种计算机科学学科的论文。利用这个数据集，我们训练和评估了基于BERT的微调模型以及五个大型语言模型：GPT-3.5、GALACTICA、Falcon、Vicuna和WizardLM。对于微调模型，我们提出了一种关系提取方法，相较于最先进的基准模型，F1值提升了29%。对于大型语言模型，我们提出了一种利用YAML输出进行结构化数据提取的方法，取得了",
    "tldr": "本文提出了 HyperPIE 方法，用于从科学论文中提取超参数信息。通过训练和评估多种模型，包括BERT微调模型和五个大型语言模型，我们实现了关系提取和结构化数据提取，并取得了显著的性能改进。"
}