{
    "title": "FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models",
    "abstract": "arXiv:2312.08459v2 Announce Type: replace-cross  Abstract: We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality m",
    "link": "https://arxiv.org/abs/2312.08459",
    "context": "Title: FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models\nAbstract: arXiv:2312.08459v2 Announce Type: replace-cross  Abstract: We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality m",
    "path": "papers/23/12/2312.08459.json",
    "total_tokens": 912,
    "translated_title": "FaceTalk: 面部特征驱动的神经参数头部模型的运动扩散",
    "translated_abstract": "我们引入了FaceTalk，这是一种新颖的生成方法，旨在从输入音频信号合成说话人头部的高保真3D运动序列。为了捕捉人头部的富有表现力和细节性质，包括头发、耳朵和更细微的眼睛运动，我们提议将语音信号与神经参数头部模型的潜在空间相结合，以创建高保真、时间连贯的运动序列。我们为这一任务提出了一种新的潜在扩散模型，它在神经参数头部模型的表达空间中运行，以合成受音频驱动的逼真头部序列。鉴于缺乏具有相应NPHM表达的数据集，我们优化这些对应关系，以生成适合音视频记录中说话人的时间优化NPHM表达的数据集。据我们所知，这是首个提出一种生成方法用于生成逼真且高质量运动序列的工作。",
    "tldr": "FaceTalk是一种生成方法，利用神经参数头部模型的潜在空间和潜在扩散模型，从输入音频信号中合成高保真3D运动序列，是首个提出这种方法用于合成逼真和高质量运动序列的工作。",
    "en_tdlr": "FaceTalk is a generative approach that synthesizes high-fidelity 3D motion sequences of talking human heads from input audio signals using the latent space of neural parametric head models and a latent diffusion model, representing the first proposal of such a method for synthesizing realistic and high-quality motion sequences."
}