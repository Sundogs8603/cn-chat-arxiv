{
    "title": "Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation transfers knowledge from large models into small models, and has recently made remarkable achievements. However, few studies has investigated the mechanism of knowledge distillation against distribution shift. Distribution shift refers to the data distribution drifts between training and testing phases. In this paper, we reconsider the paradigm of knowledge distillation by reformulating the objective function in shift situations. Under the real scenarios, we propose a unified and systematic framework to benchmark knowledge distillation against two general distributional shifts including diversity and correlation shift. The evaluation benchmark covers more than 30 methods from algorithmic, data-driven, and optimization perspectives for five benchmark datasets. Overall, we conduct extensive experiments on the student model. We reveal intriguing observations of poor teaching performance under distribution shifts; in particular, complex algorithms and data augmentati",
    "link": "http://arxiv.org/abs/2312.16242",
    "context": "Title: Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)\nAbstract: Knowledge distillation transfers knowledge from large models into small models, and has recently made remarkable achievements. However, few studies has investigated the mechanism of knowledge distillation against distribution shift. Distribution shift refers to the data distribution drifts between training and testing phases. In this paper, we reconsider the paradigm of knowledge distillation by reformulating the objective function in shift situations. Under the real scenarios, we propose a unified and systematic framework to benchmark knowledge distillation against two general distributional shifts including diversity and correlation shift. The evaluation benchmark covers more than 30 methods from algorithmic, data-driven, and optimization perspectives for five benchmark datasets. Overall, we conduct extensive experiments on the student model. We reveal intriguing observations of poor teaching performance under distribution shifts; in particular, complex algorithms and data augmentati",
    "path": "papers/23/12/2312.16242.json",
    "total_tokens": 921,
    "translated_title": "重新审视在分布偏移下的知识蒸馏",
    "translated_abstract": "知识蒸馏将大型模型的知识传递给小型模型，并在最近取得了显著成就。然而，很少有研究探讨了知识蒸馏在面对分布偏移时的机制。分布偏移是指在训练和测试阶段之间数据分布发生的漂移。在本文中，我们重新思考了知识蒸馏的范式，通过在偏移情况下重新制定目标函数。在真实场景下，我们提出了一个统一而系统的框架，用于对两种常见的分布偏移，即多样性偏移和相关性偏移，进行知识蒸馏的基准评估。该评估基准覆盖了来自算法、数据驱动和优化视角的30多种方法，针对五个基准数据集进行评估。总体上，我们对学生模型进行了大量实验。我们揭示了在分布偏移下教学性能较差的有趣观察结果；特别是，复杂的算法和数据增强方法可能对知识蒸馏效果产生负面影响。",
    "tldr": "本文通过重新制定目标函数，在分布偏移情况下重新审视了知识蒸馏的范式。使用统一的框架，对多样性偏移和相关性偏移进行了基准评估，并揭示了在分布偏移下教学性能较差的现象。",
    "en_tdlr": "This paper reexamines the paradigm of knowledge distillation under distribution shift by reformulating the objective function. It proposes a unified framework to benchmark knowledge distillation against diversity and correlation shifts, revealing poor teaching performance under distribution shifts."
}