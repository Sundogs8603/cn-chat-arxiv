{
    "title": "Social Learning: Towards Collaborative Learning with Large Language Models",
    "abstract": "We introduce the framework of \"social learning\" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.",
    "link": "https://arxiv.org/abs/2312.11441",
    "context": "Title: Social Learning: Towards Collaborative Learning with Large Language Models\nAbstract: We introduce the framework of \"social learning\" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.",
    "path": "papers/23/12/2312.11441.json",
    "total_tokens": 857,
    "translated_title": "社会学习：朝着与大型语言模型的协作学习",
    "translated_abstract": "我们在大型语言模型（LLMs）的背景下引入了“社会学习”的框架，在隐私保护的前提下，模型通过使用自然语言相互共享知识。我们提出并评估了两种在LLMs之间进行知识传递的方法。在第一种情况下，我们允许模型生成抽象提示以便教授任务。在第二种方法中，模型通过生成合成示例来传递知识。我们跨多个数据集评估了这些方法，并以记忆化作为隐私损失的代理进行量化。这些受到社会学习启发的技术取得了有希望的结果，原始数据的记忆化程度较低。特别是，我们展示了使用这些方法的性能与使用原始标签和提示的结果相媲美。我们的工作证明了社会学习在LLMs中的可行性，建立了基线方法，并突出了几个未开发的领域供未来研究使用。",
    "tldr": "本论文在大型语言模型的背景下引入了“社会学习”的框架，通过自然语言相互共享知识，提出了两种知识传递方法，并证明了这些方法的可行性和效果。",
    "en_tdlr": "This paper introduces the framework of \"social learning\" in the context of large language models, where models share knowledge with each other using natural language. It presents and evaluates two approaches for knowledge transfer between models, showing their feasibility and effectiveness."
}