{
    "title": "Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using Mirror Descent. (arXiv:2312.13486v2 [cs.LG] UPDATED)",
    "abstract": "Utilizing task-invariant prior knowledge extracted from related tasks, meta-learning is a principled framework that empowers learning a new task especially when data records are limited. A fundamental challenge in meta-learning is how to quickly \"adapt\" the extracted prior in order to train a task-specific model within a few optimization steps. Existing approaches deal with this challenge using a preconditioner that enhances convergence of the per-task training process. Though effective in representing locally a quadratic training loss, these simple linear preconditioners can hardly capture complex loss geometries. The present contribution addresses this limitation by learning a nonlinear mirror map, which induces a versatile distance metric to enable capturing and optimizing a wide range of loss geometries, hence facilitating the per-task training. Numerical tests on few-shot learning datasets demonstrate the superior expressiveness and convergence of the advocated approach.",
    "link": "http://arxiv.org/abs/2312.13486",
    "context": "Title: Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using Mirror Descent. (arXiv:2312.13486v2 [cs.LG] UPDATED)\nAbstract: Utilizing task-invariant prior knowledge extracted from related tasks, meta-learning is a principled framework that empowers learning a new task especially when data records are limited. A fundamental challenge in meta-learning is how to quickly \"adapt\" the extracted prior in order to train a task-specific model within a few optimization steps. Existing approaches deal with this challenge using a preconditioner that enhances convergence of the per-task training process. Though effective in representing locally a quadratic training loss, these simple linear preconditioners can hardly capture complex loss geometries. The present contribution addresses this limitation by learning a nonlinear mirror map, which induces a versatile distance metric to enable capturing and optimizing a wide range of loss geometries, hence facilitating the per-task training. Numerical tests on few-shot learning datasets demonstrate the superior expressiveness and convergence of the advocated approach.",
    "path": "papers/23/12/2312.13486.json",
    "total_tokens": 956,
    "translated_title": "使用镜像下降的多功能损失几何元进行快速自适应的元学习",
    "translated_abstract": "元学习是一种基于相关任务中提取的任务不变先验知识的原则性框架，它可以在数据记录有限的情况下训练新任务。元学习中的一个基本挑战是如何快速“调整”提取的先验知识，以便在几次优化步骤内训练特定任务的模型。现有方法使用预处理器来处理这个挑战，以增强每个任务训练过程的收敛性。尽管在表示二次训练损失时有效，但这些简单的线性预处理器很难捕捉复杂的损失几何元。本文通过学习非线性镜像映射来解决这个限制，该映射引入了一种多功能距离度量，可以捕捉和优化各种损失几何元，从而促进每个任务的训练。在几种少样本学习数据集上的数值测试表明，所提出的方法具有更强的表达能力和收敛性。",
    "tldr": "本论文提出了一种使用镜像下降的多功能损失几何元的元学习方法，通过学习非线性镜像映射来快速适应提取的先验知识，以便在少量优化步骤内训练特定任务的模型。在少样本学习数据集上的实验中表明，该方法具有更强的表达能力和收敛性。",
    "en_tdlr": "This paper proposes a meta-learning method using versatile loss geometries and mirror descent, which learns nonlinear mirror map to quickly adapt the extracted prior knowledge for training task-specific models within a few optimization steps. Experimental results on few-shot learning datasets demonstrate the superior expressiveness and convergence of the advocated approach."
}