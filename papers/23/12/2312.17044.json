{
    "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding",
    "abstract": "arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met",
    "link": "https://arxiv.org/abs/2312.17044",
    "context": "Title: Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding\nAbstract: arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met",
    "path": "papers/23/12/2312.17044.json",
    "total_tokens": 832,
    "translated_title": "Transformer长度外推：从位置编码的角度进行调查",
    "translated_abstract": "Transformer自诞生以来已经在自然语言处理（NLP）领域掀起了一股风暴。建立在其基础上的大型语言模型（LLMs）由于其出色的能力而受到全球关注。然而，包括这些强大的LLMs在内的所有基于Transformer的模型都受制于预设的长度限制，很难从短训练序列推广到更长的推断序列，即它们无法进行长度外推。因此，已经提出了大量方法来增强Transformer的长度外推能力，其中位置编码（PE）被认为是主要因素。 在这项调查中，我们从PE的角度以统一符号介绍了这些关于长度外推的进展。具体而言，我们首先介绍了可外推的PE，包括绝对和相对PE。然后，我们深入探讨了基于它们的外推方法，涵盖了位置插值和随机化位置方法。",
    "tldr": "本调查从位置编码的角度总结了用于增强Transformer长度外推能力的各种方法，包括绝对和相对位置编码以及基于它们的外推方法。",
    "en_tdlr": "This survey presents various methods to enhance the length extrapolation capability of Transformers from the perspective of position encoding, including absolute and relative position encodings, as well as extrapolation methods based on them."
}