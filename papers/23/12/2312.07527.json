{
    "title": "BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability",
    "abstract": "arXiv:2312.07527v2 Announce Type: replace-cross  Abstract: While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the \"content effect\"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning ac",
    "link": "https://arxiv.org/abs/2312.07527",
    "context": "Title: BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability\nAbstract: arXiv:2312.07527v2 Announce Type: replace-cross  Abstract: While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the \"content effect\"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning ac",
    "path": "papers/23/12/2312.07527.json",
    "total_tokens": 953,
    "translated_title": "BaRDa: 一个将事实准确性和推理能力分开的信念和推理数据集",
    "translated_abstract": "尽管存在许多基准来比较现代语言模型（LMs）的性能，但最终任务评估往往混淆了*事实准确性*（\"真相\"）和*推理能力*（\"合理性\"，或者根据正确报告信念含义来定义的\"诚实\"）。我们的目标是创建一个能够清晰区分这两个概念的数据集。我们的方法是利用和扩展一组人类注释的*蕴涵树*，用于表达良好和恶劣的推理链，并使用真实和虚假事实的混合，特别是包括反事实的例子，以避免信念偏见（也称为\"内容效应\"）。结果数据集名为BaRDa，包含3000个蕴涵（1787个有效，1213个无效），使用6681个真实和2319个虚假陈述。在四个GPT系列模型 GPT3(curie)/GPT3(davinici)/3.5/4 上进行测试，发现事实准确性（真相）得分为74.1/80.6/82.6/87.1以及推理能力",
    "tldr": "BaRDa数据集通过使用人类注释的蕴涵树，混合真实和虚假事实，并包括反事实例子，成功区分了事实准确性和推理能力。"
}