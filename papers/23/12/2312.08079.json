{
    "title": "Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v2 [cs.CL] UPDATED)",
    "abstract": "Target-speaker automatic speech recognition (ASR) aims to transcribe the desired speech of a target speaker from multi-talker overlapped utterances. Most of the existing target-speaker ASR (TS-ASR) methods involve either training from scratch or fully fine-tuning a pre-trained model, leading to significant training costs and becoming inapplicable to large foundation models. This work leverages prompt tuning, a parameter-efficient fine-tuning approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR. Variants of prompt tuning approaches along with their configurations are explored and optimized for TS-ASR.Experimental results show that prompt tuning can achieve performance comparable to state-of-the-art full training approaches while only requiring about 1\\% of task-specific model parameters. Notably, the original Whisper's features, such as inverse text normalization and timestamp tagging, are retained in target-speaker ASR, keeping the generated transcriptions natu",
    "link": "http://arxiv.org/abs/2312.08079",
    "context": "Title: Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v2 [cs.CL] UPDATED)\nAbstract: Target-speaker automatic speech recognition (ASR) aims to transcribe the desired speech of a target speaker from multi-talker overlapped utterances. Most of the existing target-speaker ASR (TS-ASR) methods involve either training from scratch or fully fine-tuning a pre-trained model, leading to significant training costs and becoming inapplicable to large foundation models. This work leverages prompt tuning, a parameter-efficient fine-tuning approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR. Variants of prompt tuning approaches along with their configurations are explored and optimized for TS-ASR.Experimental results show that prompt tuning can achieve performance comparable to state-of-the-art full training approaches while only requiring about 1\\% of task-specific model parameters. Notably, the original Whisper's features, such as inverse text normalization and timestamp tagging, are retained in target-speaker ASR, keeping the generated transcriptions natu",
    "path": "papers/23/12/2312.08079.json",
    "total_tokens": 910,
    "translated_title": "使用提示调整扩展Whisper以针对特定说话者的ASR",
    "translated_abstract": "目标说话者自动语音识别（ASR）旨在从多个说话者重叠的话语中转录特定说话者的所需语音。大多数现有的目标说话者ASR（TS-ASR）方法要么从头开始训练，要么完全微调预训练模型，导致训练成本高昂，并且不适用于大型基础模型。本研究利用提示调整（prompt tuning），一种参数高效的微调方法，将Whisper，一个大规模单说话者ASR模型，扩展到TS-ASR。对提示调整方法及其配置的变体进行探索和优化以适用于TS-ASR。实验结果表明，提示调整在只需大约1\\%的任务特定模型参数的情况下，可以达到与现有最先进的全训练方法相当的性能。值得注意的是，原来Whisper的特性（如逆文本归一化和时间戳标记）在目标说话者ASR中保留下来，保持了生成的转录的自然性。",
    "tldr": "本研究利用提示调整的方法扩展Whisper模型，实现了针对特定说话者的自动语音识别。在只使用约1%的模型参数的情况下，达到了与现有最先进的全训练方法相当的性能。",
    "en_tdlr": "This study extends the Whisper model using prompt tuning to achieve target-speaker automatic speech recognition (ASR), achieving comparable performance to state-of-the-art full training approaches while only using about 1% of the model parameters."
}