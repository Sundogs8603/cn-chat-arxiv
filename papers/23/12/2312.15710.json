{
    "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
    "abstract": "arXiv:2312.15710v2 Announce Type: replace-cross  Abstract: Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance th",
    "link": "https://arxiv.org/abs/2312.15710",
    "context": "Title: Alleviating Hallucinations of Large Language Models through Induced Hallucinations\nAbstract: arXiv:2312.15710v2 Announce Type: replace-cross  Abstract: Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance th",
    "path": "papers/23/12/2312.15710.json",
    "total_tokens": 862,
    "translated_title": "通过诱导性幻觉缓解大型语言模型的幻觉",
    "translated_abstract": "尽管大型语言模型(LLMs)具有令人印象深刻的能力，但已观察到它们生成的响应中包含不准确或虚假信息，这种现象通常称为“幻觉”。在这项工作中，我们提出了一个简单的“诱导-对比解码”(ICD)策略来减轻幻觉。我们首先通过从原始LLMs中诱导幻觉来构建一个事实上薄弱的LLM。然后，在解码过程中惩罚这些诱导的幻觉以增强生成内容的真实性。具体来说，我们通过对比解码来放大原模型的预测并贬低诱导的不真实预测来确定最终的下一个标记预测。对基于歧视和基于生成的幻觉评估基准，如TruthfulQA和FActScore等进行的实验结果表明，我们提出的ICD方法能够有效增强",
    "tldr": "通过诱导虚假信息来构建一个事实薄弱的语言模型，并在解码过程中通过对比解码来惩罚这些诱导的虚假信息，从而有效提升生成内容的真实性。",
    "en_tdlr": "Constructing a factually weak language model by inducing hallucinations and penalizing these induced hallucinations during decoding via contrastive decoding to effectively enhance the factuality of the generated content."
}