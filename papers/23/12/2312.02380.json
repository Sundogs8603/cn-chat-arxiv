{
    "title": "FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)",
    "abstract": "The growth of global consumption has motivated important applications of deep learning to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present pretraining and fine-tuning frameworks for identifying bearing faults based on transformer models. In particular, we investigate different tokenization and data augmentation strategies to improve performance and reach state of the art accuracies. Furthermore, we demonstrate masked self-supervised pretraining for vibration signals and its application to low-data regimes, task adaptation, and dataset adaptation. Pretraining is able to improve performance on 10-way bearing classification on scarce, unseen training samples. Transformer models also benefit from pretraining when fine-tuning on fault classes outside of the pretraining distribution. Lastly, pretrained transformers are shown",
    "link": "http://arxiv.org/abs/2312.02380",
    "context": "Title: FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)\nAbstract: The growth of global consumption has motivated important applications of deep learning to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present pretraining and fine-tuning frameworks for identifying bearing faults based on transformer models. In particular, we investigate different tokenization and data augmentation strategies to improve performance and reach state of the art accuracies. Furthermore, we demonstrate masked self-supervised pretraining for vibration signals and its application to low-data regimes, task adaptation, and dataset adaptation. Pretraining is able to improve performance on 10-way bearing classification on scarce, unseen training samples. Transformer models also benefit from pretraining when fine-tuning on fault classes outside of the pretraining distribution. Lastly, pretrained transformers are shown",
    "path": "papers/23/12/2312.02380.json",
    "total_tokens": 918,
    "translated_title": "FaultFormer: 预训练Transformer用于适应性轴承故障分类",
    "translated_abstract": "全球消费的增长推动了深度学习在智能制造和机器健康监测方面的重要应用。特别是，振动数据提供了丰富可靠的信息，能够对机器健康和预测性维护进行有意义的洞察。在本工作中，我们提出了基于Transformer模型的轴承故障识别的预训练和微调框架。我们研究了不同的标记分割和数据增强策略，以提高性能并达到最先进的准确率。此外，我们展示了针对振动信号的掩码自监督预训练及其在低数据环境、任务适应和数据集适应中的应用。预训练能够提升在稀缺未见训练样本上的10类轴承分类性能。当在预训练分布之外的故障类别上进行微调时，Transformer模型也受益于预训练。最后，我们展示了预训练的Transformer模型。",
    "tldr": "本论文提出了一个使用预训练Transformer模型进行适应性轴承故障分类的框架。通过研究不同的标记分割和数据增强策略，该方法在稀缺数据环境中能够达到最先进的准确率，并在微调时改善了性能。",
    "en_tdlr": "This paper presents a framework for adaptable bearing fault classification using pretrained Transformer models. By investigating different tokenization and data augmentation strategies, this approach achieves state-of-the-art accuracies in a scarce data environment and improves performance during fine-tuning."
}