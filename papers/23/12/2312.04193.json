{
    "title": "Language Model Knowledge Distillation for Efficient Question Answering in Spanish",
    "abstract": "arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st",
    "link": "https://arxiv.org/abs/2312.04193",
    "context": "Title: Language Model Knowledge Distillation for Efficient Question Answering in Spanish\nAbstract: arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st",
    "path": "papers/23/12/2312.04193.json",
    "total_tokens": 860,
    "translated_title": "西班牙语问答中的语言模型知识蒸馏",
    "translated_abstract": "最近发展的预训练西班牙语言模型的进展在许多自然语言处理(NLP)任务中取得了重大进展，如问答。然而，缺乏高效模型对这些模型在资源受限环境中的采用构成了一道障碍。因此，针对西班牙语的较小的蒸馏模型可能被证明是高度可扩展的，并促进它们在各种任务和场景中的进一步采用。在这项工作中，我们朝着这个方向迈出了一步，通过开发基于RoBERTa的西班牙语高效问答压缩语言模型SpanishTinyRoBERTa。为了实现这一目标，我们从一个大模型向一个更轻的模型进行知识蒸馏，这使得更广泛的实现成为可能，即使在计算资源有限的地区，也能实现可忽略的性能牺牲。我们的实验表明，这种密集的蒸馏模型能够实现限制的计算性能兼容。",
    "tldr": "通过知识蒸馏，我们开发了SpanishTinyRoBERTa，一个基于RoBERTa的西班牙语压缩语言模型，用于提高西班牙语问答的效率。"
}