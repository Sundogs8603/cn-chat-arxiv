{
    "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
    "abstract": "arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p",
    "link": "https://arxiv.org/abs/2312.03724",
    "context": "Title: DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\nAbstract: arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p",
    "path": "papers/23/12/2312.03724.json",
    "total_tokens": 875,
    "translated_title": "DP-OPT：使大型语言模型成为您的隐私保护提示工程师",
    "translated_abstract": "大型语言模型（LLMs）已成为各种任务的主要工具，尤其是通过提示调整针对特定目标时。然而，由于调整的提示依赖于敏感的私人信息，围绕数据隐私的担忧提出了障碍。一个实际的解决方案是托管一个本地的LLM，并使用数据私下优化一个软提示。然而，当模型所有权受到保护时，托管一个本地模型就变得有问题。将数据发送给模型提供程序进行培训等替代方法加剧了这些隐私问题，面对一个不受信任的提供者。在本文中，我们提出了一种名为差分私密离线提示调整（DP-OPT）的新颖解决方案以解决这一挑战。我们的方法涉及在客户端调整离散提示，然后将其应用于所需的云模型。我们演示了LLMs本身建议的提示可以在不暴露p的情况下进行传递",
    "tldr": "提出了差分私密离线提示调整（DP-OPT）解决大型语言模型调整提示时的隐私问题，通过在客户端调整提示并应用于云模型实现了隐私保护和数据传输",
    "en_tdlr": "Introduced DP-OPT, a novel solution to address privacy concerns when tuning prompts for large language models, achieving privacy protection and data transfer by adjusting prompts on the client side and applying them to cloud models."
}