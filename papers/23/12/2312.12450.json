{
    "title": "Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions",
    "abstract": "arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod",
    "link": "https://arxiv.org/abs/2312.12450",
    "context": "Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions\nAbstract: arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod",
    "path": "papers/23/12/2312.12450.json",
    "total_tokens": 841,
    "translated_title": "大型语言模型能否进行编辑？评估其遵循代码编辑指令的能力",
    "translated_abstract": "大量研究集中在开发和评估大型语言模型用于各种代码合成任务。这些任务包括从自然语言指令中合成代码，从代码中合成测试，以及从代码中合成解释。与此相反，使用LLMs进行指令式代码编辑的行为研究不足。这些任务要求模型按照提供的提示更新一块代码。编辑指令可能要求添加或删除功能，描述错误并要求修复，要求不同类型的解决方案，或者其他常见的代码编辑任务。我们引入了一个精心设计的代码编辑任务基准，并用它评估了几个最先进的LLMs。我们的评估展示了当前最先进的开放和封闭模型之间的显著差距。例如，即使是GPT-3.5-Turbo也比最好的开放模型在编辑代码方面好了8.8%。",
    "tldr": "该研究评估了大型语言模型遵循代码编辑指令的能力，在指令式代码编辑任务上发现了开放和封闭模型之间的显著差距。",
    "en_tdlr": "This study evaluates the ability of large language models to follow code editing instructions, revealing a significant gap between open and closed models in instructional code editing tasks."
}