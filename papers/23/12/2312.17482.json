{
    "title": "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)",
    "abstract": "Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pr",
    "link": "http://arxiv.org/abs/2312.17482",
    "context": "Title: MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)\nAbstract: Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pr",
    "path": "papers/23/12/2312.17482.json",
    "total_tokens": 871,
    "translated_title": "MosaicBERT：一种针对快速预训练进行优化的双向编码器",
    "translated_abstract": "尽管BERT风格的编码器模型在NLP研究中广泛使用，但由于训练成本高昂，许多研究人员不会从头开始预训练自己的BERT。在BERT首次崭露头角的过去半-decade，已经对其他变压器架构和训练配置进行了许多进展，但尚未系统地纳入BERT中。在这里，我们引入了MosaicBERT，一种经验优化用于快速预训练的BERT风格编码器架构和训练方法。这种高效的架构将FlashAttention、带有线性偏差的Attention (ALiBi)、门控线性单元 (GLU)、动态移除填充令牌的模块和低精度LayerNorm等引入了经典的变压器编码器块。训练方法还包括30%的掩码比率用于遮蔽语言建模 (MLM) 目标，bfloat16精度，以及针对GPU吞吐量进行优化的词汇大小，此外还采用了RoBERTa和其他编码器模型的最佳实践。",
    "tldr": "MosaicBERT是一种优化的BERT风格双向编码器，通过引入多项创新技术和优化方案，实现了快速预训练。"
}