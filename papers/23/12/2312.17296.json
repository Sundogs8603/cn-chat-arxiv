{
    "title": "Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)",
    "abstract": "Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \\method{} enhances model performance and can be used to t",
    "link": "http://arxiv.org/abs/2312.17296",
    "context": "Title: Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)\nAbstract: Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \\method{} enhances model performance and can be used to t",
    "path": "papers/23/12/2312.17296.json",
    "total_tokens": 972,
    "translated_title": "LLM训练中的结构化填充改进了长上下文利用",
    "translated_abstract": "长上下文大型语言模型（LCLM）的最新进展引起了广泛关注，特别是在查询科学研究论文等应用中。然而，它们的潜力往往受到上下文利用不足的限制。我们确定典型训练数据中缺乏长程语义依赖是主要障碍。为了解决这个问题，我们深入研究了频繁将相关文档纳入训练输入的好处。利用代码数据的固有目录结构作为训练示例的来源，我们证明了即使对于与编码无关的任务，囊括相关文档能够改进模型的困惑度。基于这些发现，并且更具广泛的关注，我们引入了一种名为Structured Packing for Long Context (SPLiCe)的创新方法。 SPLiCe是一种使用检索方法将最互相关文档汇集到单个训练上下文中的方法。我们的结果表明，\\method{}提高了模型的性能，并可用于t",
    "tldr": "本论文研究了长上下文大型语言模型（LLM）中上下文利用不足的问题，并通过将相关文档纳入训练示例中来改进模型的困惑度。通过引入Structured Packing for Long Context (SPLiCe)方法，使用检索方法将最互相关文档汇集到单个训练上下文中，进一步提高了模型的性能。",
    "en_tdlr": "This paper addresses the issue of inadequate context utilization in long-context Large Language Models (LLMs) and improves model perplexity by incorporating related documents into the training examples. The introduction of the Structured Packing for Long Context (SPLiCe) method, which collates the most mutually relevant documents into a single training context using a retrieval method, further enhances the model's performance."
}