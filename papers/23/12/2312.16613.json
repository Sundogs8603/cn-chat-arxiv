{
    "title": "Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v2 [cs.SD] UPDATED)",
    "abstract": "In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.",
    "link": "http://arxiv.org/abs/2312.16613",
    "context": "Title: Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v2 [cs.SD] UPDATED)\nAbstract: In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.",
    "path": "papers/23/12/2312.16613.json",
    "total_tokens": 905,
    "translated_title": "自我监督预训练用于恶劣条件下的个性化语音活动检测的鲁棒性提升",
    "translated_abstract": "本文提出了在大规模未标记数据集上使用自我监督预训练的方法，以改善恶劣条件下个性化语音活动检测模型的性能。我们使用自回归预测编码框架预训练了一个长短期记忆(LSTM)-编码器，并对个性化语音活动检测进行了微调。此外，我们还提出了一种降噪变体的自回归预测编码方法，旨在提高个性化语音活动检测的鲁棒性。我们对训练好的模型进行了系统评估，包括在干净语音和不同信噪比水平下受各种噪声污染的语音上的评估，并与纯监督模型进行了比较。实验证明，自我监督预训练不仅能在干净条件下提高性能，而且相比于纯监督学习，还能产生更具鲁棒性的模型。",
    "tldr": "本文提出了使用自我监督预训练方法来改善恶劣条件下个性化语音活动检测模型性能，并且实验证明该方法不仅可以提高在干净条件下的性能，而且能够得到更鲁棒的模型。",
    "en_tdlr": "This paper proposes the use of self-supervised pretraining to improve the performance of personalized voice activity detection (VAD) models in adverse conditions. The experiments show that this method not only improves performance in clean conditions, but also yields more robust models."
}