{
    "title": "Parameterized Projected Bellman Operator",
    "abstract": "Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec",
    "link": "https://arxiv.org/abs/2312.12869",
    "context": "Title: Parameterized Projected Bellman Operator\nAbstract: Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec",
    "path": "papers/23/12/2312.12869.json",
    "total_tokens": 850,
    "translated_title": "参数化投影贝尔曼算子",
    "translated_abstract": "近似值迭代（AVI）是一类用于强化学习（RL）的算法家族，旨在获得最优值函数的近似。通常，AVI算法采用迭代过程，每个步骤包括（i）贝尔曼算子的应用和（ii）投影步骤到考虑的函数空间中。众所周知，贝尔曼算子利用转移样本，这些样本强烈影响其行为，因为无信息的样本可能导致可忽略的更新或长时间的绕行，而计算密集的投影步骤进一步加剧了这些不利影响。为了解决这些问题，我们提出了一种新颖的替代方法，该方法采用学习的方式得到贝尔曼算子的近似版本，而不是像AVI方法那样通过样本进行估计。通过这种方式，我们能够（i）在转移样本之间进行泛化，（ii）避免计算密集的投影步骤。因此，我们称我们的新算子为\"projec\"算子。",
    "tldr": "本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。"
}