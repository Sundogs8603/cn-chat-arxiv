{
    "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
    "abstract": "arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si",
    "link": "https://arxiv.org/abs/2312.09979",
    "context": "Title: LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin\nAbstract: arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si",
    "path": "papers/23/12/2312.09979.json",
    "total_tokens": 737,
    "translated_title": "LoRAMoE: 通过MoE风格的插件缓解大型语言模型中的世界知识遗忘",
    "translated_abstract": "监督微调（SFT）是大型语言模型（LLMs）的关键步骤，使它们能够与人类指令对齐，并增强它们在下游任务中的能力。我们发现，指令数据的大规模增加可能会破坏LLMs先前存储的世界知识。为了解决这一挑战，我们提出了LoRAMoE，这是一个引入了多个低秩适配器（LoRA）并通过路由器网络集成它们的创新性框架，类似于专家混合（MoE）的插件版本。",
    "tldr": "LoRAMoE是一个新颖的框架，通过引入低秩适配器和路由器网络，类似于MoE的插件版本，来解决大型语言模型中世界知识遗忘的问题。"
}