{
    "title": "Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi",
    "link": "http://arxiv.org/abs/2312.15965",
    "context": "Title: Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)\nAbstract: Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi",
    "path": "papers/23/12/2312.15965.json",
    "total_tokens": 861,
    "translated_title": "有效的强化学习方法：探索与利用分离",
    "translated_abstract": "当前的离线强化学习技术过于保守，对现有数据集的处理限制了深度神经网络(DNN)的泛化能力。这种方法常常导致算法只适应某个特定数据集的次优解。同样，在在线强化学习中，之前的惩罚性悲观主义也剥夺了模型的探索潜力。我们的研究提出了一种新的框架，乐观与悲观演员强化学习（OPARL）。OPARL采用独特的双演员方法：乐观演员专注于探索，悲观演员专注于利用，从而有效区分探索和利用策略。这种组合在强化学习方法中促进了更平衡和高效的方法。它通过悲观的利用策略优化着重于产生高奖励动作的策略。",
    "tldr": "本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。",
    "en_tdlr": "This research proposes a novel framework for reinforcement learning that separates exploration and utilization strategies, and employs a dual-actor approach of optimistic and pessimistic actors to achieve a more balanced and efficient optimization strategy."
}