{
    "title": "Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting",
    "abstract": "arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro",
    "link": "https://arxiv.org/abs/2312.00817",
    "context": "Title: Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting\nAbstract: arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro",
    "path": "papers/23/12/2312.00817.json",
    "total_tokens": 861,
    "translated_title": "可推广的Transformer预训练用于超长时间序列预测",
    "translated_abstract": "大规模预训练模型（PTMs），如BERT和GPT，最近在自然语言处理和计算机视觉领域取得了巨大成功。然而，PTMs在时间序列数据上的发展滞后。这凸显了现有基于transformer的架构的局限性，特别是它们处理大规模数据和捕捉长期时间依赖性的可扩展性。本研究提出了即时生成预训练Transformer（TimelyGPT）。TimelyGPT采用可推广位置（xPos）嵌入将趋势和周期模式编码到时间序列表示中。它还集成了循环注意力和时间卷积模块，以有效地捕捉全局和局部的时间依赖关系。我们的实验表明，TimelyGPT在建模连续监测的生物信号和经常出现在纵向电磁波领域中不规则采样的时间序列数据方面表现出色。",
    "tldr": "提出了一种名为TimelyGPT的可推广的Transformer预训练模型，该模型通过可推广的位置嵌入和循环注意力以及时间卷积模块有效地捕捉超长时间序列数据中的全局和局部时间依赖关系。",
    "en_tdlr": "A novel extrapolatable Transformer pre-training model called TimelyGPT is proposed, which effectively captures global and local temporal dependencies in ultra long time-series data through extrapolatable position embedding, recurrent attention, and temporal convolution modules."
}