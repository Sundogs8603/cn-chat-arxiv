{
    "title": "Building Efficient Universal Classifiers with Natural Language Inference",
    "abstract": "arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets",
    "link": "https://arxiv.org/abs/2312.17543",
    "context": "Title: Building Efficient Universal Classifiers with Natural Language Inference\nAbstract: arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets",
    "path": "papers/23/12/2312.17543.json",
    "total_tokens": 860,
    "translated_title": "使用自然语言推理构建高效的通用分类器",
    "translated_abstract": "arXiv:2312.17543v2 公告类型：替换交叉。生成型大型语言模型(LLMs)已经成为零样本学习和零样本学习的主流选择，这要归功于文本生成的通用性。然而，许多用户在只想自动化一个分类任务时，并不需要生成型LLMs的广泛能力。较小的类似BERT的模型也可以学习通用任务，这使它们可以在不需要微调（零样本分类）的情况下执行任何文本分类任务，或者只用少量样本学习新任务（少样本），同时比生成型LLMs高效得多。本文(1) 解释了如何将自然语言推理（NLI）作为通用分类任务，其原理类似于生成型LLMs的指导微调，(2) 提供了用于构建通用分类器的可重用Jupyter笔记本的逐步指南，(3) 共享了经过训练的通用分类器，在33个数据集上训练",
    "tldr": "本文探讨了如何利用自然语言推理作为通用分类任务，提供了构建通用分类器的详细步骤，并分享了该通用分类器在33个数据集上的训练结果",
    "en_tdlr": "This paper discusses how to use Natural Language Inference as a universal classification task, provides a detailed guide for building a universal classifier, and shares the training results of the universal classifier on 33 datasets."
}