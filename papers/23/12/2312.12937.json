{
    "title": "Robust Loss Functions for Training Decision Trees with Noisy Labels. (arXiv:2312.12937v2 [cs.LG] UPDATED)",
    "abstract": "We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise se",
    "link": "http://arxiv.org/abs/2312.12937",
    "context": "Title: Robust Loss Functions for Training Decision Trees with Noisy Labels. (arXiv:2312.12937v2 [cs.LG] UPDATED)\nAbstract: We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise se",
    "path": "papers/23/12/2312.12937.json",
    "total_tokens": 1012,
    "translated_title": "训练带有噪声标签的决策树的鲁棒损失函数",
    "translated_abstract": "我们考虑使用有噪声标签的数据训练决策树，重点研究可以导致鲁棒学习算法的损失函数。我们的贡献有三个。首先，我们对决策树学习背景下许多现有损失函数的鲁棒性提供了新颖的理论洞察力。我们展示了一些损失属于我们所称的保守损失类别，并且保守损失在训练过程中会出现提前停止行为，而在测试过程中具有容忍噪声的预测能力。其次，我们引入了一个构建鲁棒损失函数的框架，称为分布损失。这些损失基于假设的边缘分布应用基于百分位的惩罚，它们通过鲁棒性参数自然地允许适应不同的噪声率。特别地，我们引入了一种称为负指数损失的新损失，它可以导致高效的贪婪减少不纯度的学习算法。最后，我们在多个数据集和噪声条件下进行了实验证明了我们的方法的有效性。",
    "tldr": "本文研究了在带有噪声标签的数据上训练决策树的鲁棒损失函数。我们的研究主要有三个贡献：提供了对现有损失函数鲁棒性的新洞察，引入了分布损失函数的框架，并介绍了一种高效的贪婪减少不纯度的学习算法。",
    "en_tdlr": "This paper investigates robust loss functions for training decision trees with noisy labels. The contributions include novel theoretical insights on the robustness of existing loss functions, a framework for constructing robust loss functions called distribution losses, and the introduction of a efficient greedy impurity-reduction learning algorithm."
}