{
    "title": "Split and Rephrase with Large Language Models",
    "abstract": "arXiv:2312.11075v3 Announce Type: replace  Abstract: The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned ",
    "link": "https://arxiv.org/abs/2312.11075",
    "context": "Title: Split and Rephrase with Large Language Models\nAbstract: arXiv:2312.11075v3 Announce Type: replace  Abstract: The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned ",
    "path": "papers/23/12/2312.11075.json",
    "total_tokens": 813,
    "translated_title": "利用大型语言模型进行分割与重述任务",
    "translated_abstract": "Split and Rephrase (SPRP)任务旨在将复杂句子分解为一系列更短的符合语法规则的句子，同时保持原始含义，有助于人类和机器处理复杂文本。这也是一个有价值的测试平台，可以评估自然语言处理模型，因为其需要对复杂的语法方面进行建模。在这项工作中，我们评估了大型语言模型在该任务上的表现，显示它们可以在主要指标上比现有技术有很大改进，尽管在拆分一致性方面仍有差距。来自两项人类评估的结果进一步支持自动度量结果得出的结论。我们提供了一项全面的研究，包括提示变体、领域转移、参数规模和训练数据量不同的微调预训练语言模型，同时与指导调整的零射和少射方法进行对比。",
    "tldr": "评估了大型语言模型在Split and Rephrase任务上的表现，表明在主要指标上有显著改进，但在分割一致性方面仍有待提高。",
    "en_tdlr": "Evaluated large language models on the Split and Rephrase task, showing significant improvements on main metrics but still lagging in terms of splitting compliance."
}