{
    "title": "\"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)",
    "abstract": "Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires \"effective\" rather than simply \"long\" data. Based on this insight, we propose using the \"original text paraphrasing\" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).",
    "link": "http://arxiv.org/abs/2312.11193",
    "context": "Title: \"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)\nAbstract: Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires \"effective\" rather than simply \"long\" data. Based on this insight, we propose using the \"original text paraphrasing\" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).",
    "path": "papers/23/12/2312.11193.json",
    "total_tokens": 923,
    "translated_title": "\"原文改写\"提高了高精度长文本问答的效果",
    "translated_abstract": "当面对长文本时，大多数开源生成式语言模型的上下文窗口限制在4k以内，这限制了它们的能力。即使是具有更长上下文窗口的模型也无法在长上下文问题上保证令人满意的准确性。为了解决这个问题，我们从训练数据的角度出发，从理论上证明了提高处理长上下文能力需要的是\"有效\"而不仅仅是\"长\"的数据。基于这个洞见，我们提出了使用\"原文改写\"任务，并通过一种低成本高效的方法，成功将现有模型的上下文窗口扩展到32k。我们的微调模型在具有相近规模的模型中在多文档问答方面达到了最先进的准确性。模型和训练数据已经在HuggingFace（https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k）和WiseModel（https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k）上提供。",
    "tldr": "本论文提出了一种名为\"原文改写\"的任务来处理长文本问答，通过低成本高效的方法成功扩展了现有模型的上下文窗口至32k，并在多文档问答中达到了最先进的准确性。"
}