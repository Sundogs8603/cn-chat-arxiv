{
    "title": "Parrot Captions Teach CLIP to Spot Text",
    "abstract": "Despite CLIP being the foundation model in numerous vision-language applications, the CLIP suffers from a severe text spotting bias. Such bias causes CLIP models to `Parrot' the visual text embedded within images while disregarding the authentic visual semantics. We uncover that in the most popular image-text dataset LAION-2B, the captions also densely parrot (spell) the text embedded in images. Our analysis shows that around 50% of images are embedded with visual text content, and around 30% of captions words are in these embedded visual content. Based on such observation, we thoroughly inspect the different released versions of CLIP models and verify that the visual text is the dominant factor in measuring the LAION-style image-text similarity for these models. To examine whether these parrot captions shape the text spotting bias, we train a series of CLIP models with LAION subsets curated by different parrot-caption-oriented criteria. We show that training with parrot captions easil",
    "link": "https://arxiv.org/abs/2312.14232",
    "context": "Title: Parrot Captions Teach CLIP to Spot Text\nAbstract: Despite CLIP being the foundation model in numerous vision-language applications, the CLIP suffers from a severe text spotting bias. Such bias causes CLIP models to `Parrot' the visual text embedded within images while disregarding the authentic visual semantics. We uncover that in the most popular image-text dataset LAION-2B, the captions also densely parrot (spell) the text embedded in images. Our analysis shows that around 50% of images are embedded with visual text content, and around 30% of captions words are in these embedded visual content. Based on such observation, we thoroughly inspect the different released versions of CLIP models and verify that the visual text is the dominant factor in measuring the LAION-style image-text similarity for these models. To examine whether these parrot captions shape the text spotting bias, we train a series of CLIP models with LAION subsets curated by different parrot-caption-oriented criteria. We show that training with parrot captions easil",
    "path": "papers/23/12/2312.14232.json",
    "total_tokens": 944,
    "translated_title": "Parrot Captions Teach CLIP to Spot Text",
    "translated_abstract": "尽管CLIP在很多视觉语言应用中是基础模型，但它存在严重的文本定位偏差。这种偏差导致CLIP模型“模仿”图像中嵌入的视觉文本，而忽视了真实的视觉语义。我们发现在最流行的图像-文本数据集LAION-2B中，标题也密集地“模仿”图像中嵌入的文本。我们的分析表明，大约50%的图像嵌入了视觉文本内容，而约30%的标题单词属于这些嵌入的视觉内容。基于这样的观察，我们彻底检查了不同发布版本的CLIP模型，并验证了视觉文本是衡量这些模型的LAION风格图像-文本相似性的主要因素。为了检查这些“模仿”的标题是否塑造了文本定位偏差，我们使用不同的“模仿标题”为标准，训练了一系列以LAION子集为基础的CLIP模型。我们展示了使用“模仿”标题进行训练可以很容易地解决文本定位偏差。",
    "tldr": "本研究发现在图像-文本数据集LAION-2B中，标题密集地“模仿”图像中嵌入的视觉文本，导致CLIP模型受到文本定位偏差的影响。通过使用以“模仿标题”为标准的训练集，可以有效解决这一问题。"
}