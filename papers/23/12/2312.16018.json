{
    "title": "RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. (arXiv:2312.16018v2 [cs.IR] UPDATED)",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations. Specificall",
    "link": "http://arxiv.org/abs/2312.16018",
    "context": "Title: RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. (arXiv:2312.16018v2 [cs.IR] UPDATED)\nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations. Specificall",
    "path": "papers/23/12/2312.16018.json",
    "total_tokens": 744,
    "translated_title": "RecRanker: 使用大型语言模型作为排名器进行前k项推荐的指令调优",
    "translated_abstract": "大型语言模型(LLMs)展示了卓越的能力并广泛应用于各个领域，包括推荐系统。许多研究采用专门的“提示”来利用LLMs的上下文学习能力。例如，LLMs被提示为零-shot排名器，用于对由检索模型生成的候选项进行列表排名，以用于推荐。最近的研究还使用指令调优技术，通过与人类偏好的对齐来提供更有前景的推荐。尽管具有潜力，但目前的研究忽视了整合多个排名任务以提高模型性能。此外，传统推荐模型的信号未与LLM整合，限制了当前系统的性能。",
    "tldr": "本文介绍了RecRanker，专门用于指令调优LLM以作为前k项推荐的排名器。"
}