{
    "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators. (arXiv:2312.15407v2 [cs.CL] UPDATED)",
    "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 ",
    "link": "http://arxiv.org/abs/2312.15407",
    "context": "Title: A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators. (arXiv:2312.15407v2 [cs.CL] UPDATED)\nAbstract: Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 ",
    "path": "papers/23/12/2312.15407.json",
    "total_tokens": 928,
    "translated_title": "大型语言模型作为自动对话评估器的有效性综合分析",
    "translated_abstract": "自动评估是对话系统研究中的重要方面。传统的基于参考的自然语言生成指标通常不适用于对话评估。因此，最近的研究提出了各种独特的基于神经网络的无参考指标，更符合人类评估的要求。其中特别值得注意的是大型语言模型（LLMs），尤其是调整指令的变体，如ChatGPT，被证明是人类评判的有希望的替代品。然而，目前关于利用LLMs进行自动对话评估的研究在元评估数据集的数量、评估方式、LLMs的覆盖范围等方面存在局限性。因此，LLMs的效果如何仍然未定。为此，我们对LLMs在自动对话评估中的应用进行了全面的研究。具体而言，我们分析了30种最近出现的LLMs在对话和转向层面上的多维评估能力，使用了一套全面的12个元评估数据集进行评估。",
    "tldr": "本研究对利用大型语言模型（LLMs）进行自动对话评估的应用进行了全面研究，发现LLMs在对话和转向层面上具有多维评估能力，并提出了一套全面的评估方法。"
}