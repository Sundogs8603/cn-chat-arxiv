{
    "title": "Simplicial Representation Learning with Neural $k$-Forms",
    "abstract": "arXiv:2312.08515v2 Announce Type: replace  Abstract: Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data, and over-smoothing. In this paper, we take a different approach, focusing on leveraging geometric information from simplicial complexes embedded in $\\mathbb{R}^n$ using node coordinates. We use differential k-forms in \\mathbb{R}^n to create representations of simplices, offering interpretability and geometric consistency without message passing. This approach also enables us to apply differential geometry tools and achieve universal approximation. Our method is efficient, versatile, and applicable to various input complexes, including graphs, simplicial complexes, and cell complexes. It outperforms existing message passing neural networks i",
    "link": "https://arxiv.org/abs/2312.08515",
    "context": "Title: Simplicial Representation Learning with Neural $k$-Forms\nAbstract: arXiv:2312.08515v2 Announce Type: replace  Abstract: Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data, and over-smoothing. In this paper, we take a different approach, focusing on leveraging geometric information from simplicial complexes embedded in $\\mathbb{R}^n$ using node coordinates. We use differential k-forms in \\mathbb{R}^n to create representations of simplices, offering interpretability and geometric consistency without message passing. This approach also enables us to apply differential geometry tools and achieve universal approximation. Our method is efficient, versatile, and applicable to various input complexes, including graphs, simplicial complexes, and cell complexes. It outperforms existing message passing neural networks i",
    "path": "papers/23/12/2312.08515.json",
    "total_tokens": 875,
    "translated_title": "用神经$k$-形式进行单纯表示学习",
    "translated_abstract": "Geometric deep learning扩展了深度学习，能够融合关于几何和拓扑数据的信息，特别是在复杂领域，如图形中。尽管消息传递在这一领域很受欢迎，但存在诸如需要重连图、数据解释模糊和过度平滑等限制。本文采用了一种不同的方法，专注于利用嵌入在$\\mathbb{R}^n$中的单纯复合物的几何信息，利用节点坐标。我们使用$\\mathbb{R}^n$中的微分k-形式来创建对单纯体的表示，提供了可解释性和几何一致性，无需消息传递。该方法还使我们能够应用微分几何工具，并实现了通用逼近。我们的方法高效、多功能，并适用于各种输入复合物，包括图形、单纯复合物和胞复合物。它胜过了现有的消息传递神经网络。",
    "tldr": "本文提出了一种使用神经$k$-形式进行单纯复合物表示学习的方法，无需消息传递即可获取几何信息，具有解释性和几何一致性，并能应用微分几何工具实现通用逼近。",
    "en_tdlr": "This paper introduces a method for learning representations of simplicial complexes using neural $k$-forms, which captures geometric information without message passing, offering interpretability and geometric consistency, and enabling universal approximation with the use of differential geometry tools."
}