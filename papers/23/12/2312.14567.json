{
    "title": "Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise",
    "abstract": "arXiv:2312.14567v2 Announce Type: replace  Abstract: Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for quadratic regression problems. Although it is widely conjectured that heavy-ball momentum method can provide accelerated convergence and should work well in large batch settings, there is no rigorous theoretical analysis. In this paper, we fill this theoretical gap by establishing a non-asymptotic convergence bound for stochastic heavy-ball methods with step decay scheduler on quadratic objectives, under the anisotropic gradient noise condition. As a direct implication, we show that heavy-ball momentum can provide $\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$ accelerated convergence of the bias term of SGD while still achieving near-optimal convergence rat",
    "link": "https://arxiv.org/abs/2312.14567",
    "context": "Title: Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise\nAbstract: arXiv:2312.14567v2 Announce Type: replace  Abstract: Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for quadratic regression problems. Although it is widely conjectured that heavy-ball momentum method can provide accelerated convergence and should work well in large batch settings, there is no rigorous theoretical analysis. In this paper, we fill this theoretical gap by establishing a non-asymptotic convergence bound for stochastic heavy-ball methods with step decay scheduler on quadratic objectives, under the anisotropic gradient noise condition. As a direct implication, we show that heavy-ball momentum can provide $\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$ accelerated convergence of the bias term of SGD while still achieving near-optimal convergence rat",
    "path": "papers/23/12/2312.14567.json",
    "total_tokens": 847,
    "translated_title": "非各向同性梯度噪声下随机重球方法的加速收敛性",
    "translated_abstract": "对于优化深度学习模型，带有递减学习率的重球动量经常与SGD一起使用。然而，与其在实践中的流行相比，对其理论性质的了解仍然非常有限，特别是在标准的二次回归问题的各向异性梯度噪声条件下。本文填补了这一理论空白，通过在二次目标下，建立了一种具有步长衰减调度器的随机重球方法的非渐近收敛界，而此条件下的梯度噪声是各向异性的。直接影响是，我们展示了重球动量可以提供$\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$的SGD偏差项加速收敛，同时实现接近最优的收敛率。",
    "tldr": "填补了理论空白，建立了一种非渐近收敛界，证明了在各向异性梯度噪声条件下，随机重球方法在二次目标上可以提供$\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$的加速收敛。",
    "en_tdlr": "Filling the theoretical gap, establishing a non-asymptotic convergence bound, it is shown that the stochastic heavy-ball method can provide $\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$ accelerated convergence on quadratic objectives under anisotropic gradient noise conditions."
}