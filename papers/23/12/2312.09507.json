{
    "title": "WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)",
    "abstract": "Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce $\\texttt{WAVER}$, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. $\\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets,",
    "link": "http://arxiv.org/abs/2312.09507",
    "context": "Title: WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)\nAbstract: Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce $\\texttt{WAVER}$, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. $\\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets,",
    "path": "papers/23/12/2312.09507.json",
    "total_tokens": 884,
    "translated_title": "WAVER:通过开放词汇知识通过视觉-语言模型提供对写作风格不受束缚的文本-视频检索",
    "translated_abstract": "文本-视频检索是多模态信息检索领域中一个重要的子领域，在近年来取得了显著的增长。然而，现有方法假设视频场景与无偏的描述一致。这些限制与真实世界的情况不符，因为描述可能受到注释者的偏见、不同的写作风格和不同的文本视角的影响。为了克服上述问题，我们介绍了WAVER，一种通过视觉-语言模型通过开放词汇知识进行跨域知识蒸馏的框架，旨在解决处理视频描述中不同写作风格的挑战。WAVER利用预训练的视觉-语言模型中的开放词汇属性，并采用隐式知识蒸馏方法，将基于文本的知识从教师模型传递给基于视觉的学生模型。在四个标准基准数据集上进行了实证研究。",
    "tldr": "WAVER是一种通过开放词汇知识进行跨域知识蒸馏的框架，用于解决视频描述中不同写作风格的问题。它利用预训练的视觉-语言模型，并采用隐式知识蒸馏方法，将文本知识从教师模型传递给学生模型。"
}