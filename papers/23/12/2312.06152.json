{
    "title": "Improving the performance of weak supervision searches using transfer and meta-learning",
    "abstract": "arXiv:2312.06152v2 Announce Type: replace-cross  Abstract: Weak supervision searches have in principle the advantages of both being able to train on experimental data and being able to learn distinctive signal properties. However, the practical applicability of such searches is limited by the fact that successfully training a neural network via weak supervision can require a large amount of signal. In this work, we seek to create neural networks that can learn from less experimental signal by using transfer and meta-learning. The general idea is to first train a neural network on simulations, thereby learning concepts that can be reused or becoming a more efficient learner. The neural network would then be trained on experimental data and should require less signal because of its previous training. We find that transfer and meta-learning can substantially improve the performance of weak supervision searches.",
    "link": "https://arxiv.org/abs/2312.06152",
    "context": "Title: Improving the performance of weak supervision searches using transfer and meta-learning\nAbstract: arXiv:2312.06152v2 Announce Type: replace-cross  Abstract: Weak supervision searches have in principle the advantages of both being able to train on experimental data and being able to learn distinctive signal properties. However, the practical applicability of such searches is limited by the fact that successfully training a neural network via weak supervision can require a large amount of signal. In this work, we seek to create neural networks that can learn from less experimental signal by using transfer and meta-learning. The general idea is to first train a neural network on simulations, thereby learning concepts that can be reused or becoming a more efficient learner. The neural network would then be trained on experimental data and should require less signal because of its previous training. We find that transfer and meta-learning can substantially improve the performance of weak supervision searches.",
    "path": "papers/23/12/2312.06152.json",
    "total_tokens": 774,
    "translated_title": "通过转移学习和元学习提高弱监督搜索的性能",
    "translated_abstract": "弱监督搜索原则上具有能够在实验数据上训练和学习独特信号特性的优势。然而，这种搜索的实际适用性受到成功通过弱监督训练神经网络可能需要大量信号的限制。在这项工作中，我们试图通过使用转移学习和元学习创建可以从较少实验信号中学习的神经网络。总的想法是首先在模拟数据上训练神经网络，从而学习可以重复使用的概念或成为更有效的学习者。然后，神经网络将在实验数据上进行训练，并且由于先前的训练而需要较少的信号。我们发现转移学习和元学习可以显著提高弱监督搜索的性能。",
    "tldr": "通过转移学习和元学习，本研究通过在模拟数据上训练神经网络来提高弱监督搜索的性能，从而减少了在实验数据上需要的信号量。",
    "en_tdlr": "This study improves the performance of weak supervision searches by using transfer and meta-learning to train neural networks on simulated data, reducing the amount of signal required on experimental data."
}