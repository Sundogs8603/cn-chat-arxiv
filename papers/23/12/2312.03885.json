{
    "title": "Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives",
    "abstract": "We consider a gradient-based optimization method applied to a function $\\mathcal{L}$ of a vector of variables $\\boldsymbol{\\theta}$, in the case where $\\boldsymbol{\\theta}$ is represented as a tuple of tensors $(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\\mathcal{L}$, especially about the interactions between the tensors $\\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\\boldsymbol{\\theta}$ into tensors $(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\\mathcal{",
    "link": "https://arxiv.org/abs/2312.03885",
    "context": "Title: Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives\nAbstract: We consider a gradient-based optimization method applied to a function $\\mathcal{L}$ of a vector of variables $\\boldsymbol{\\theta}$, in the case where $\\boldsymbol{\\theta}$ is represented as a tuple of tensors $(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\\mathcal{L}$, especially about the interactions between the tensors $\\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\\boldsymbol{\\theta}$ into tensors $(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\\mathcal{",
    "path": "papers/23/12/2312.03885.json",
    "total_tokens": 890,
    "translated_title": "通过高阶导数总结，将牛顿法应用于神经网络的改进",
    "translated_abstract": "我们考虑了一种应用于向量变量$\\boldsymbol{\\theta}$上的函数$\\mathcal{L}$的基于梯度的优化方法，在这种情况下，$\\boldsymbol{\\theta}$被表示为元组$(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$的张量。该框架包括许多常见的用例，例如通过梯度下降来训练神经网络。首先，我们提出了一种计算成本低廉的技术，通过自动微分和计算技巧，提供关于$\\mathcal{L}$及其张量$\\mathbf{T}_s$之间相互作用的高阶信息。其次，我们利用这种技术来建立一个二阶优化方法，适用于训练各种架构的深度神经网络。这个二阶方法利用了$\\boldsymbol{\\theta}$被分割为张量$(\\mathbf{T}_1, \\cdots, \\mathbf{T}_S)$的分区结构，因此不需要计算$\\mathcal{L}$的Hessian矩阵。",
    "tldr": "本论文通过计算高阶导数，将牛顿法应用于神经网络中，提出了一个适用于各种架构的深度神经网络的二阶优化方法。",
    "en_tdlr": "This paper adapts Newton's method to neural networks by computing higher-order derivatives, and proposes a second-order optimization method suitable for training deep neural networks of various architectures."
}