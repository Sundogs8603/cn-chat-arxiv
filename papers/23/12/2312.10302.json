{
    "title": "One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)",
    "abstract": "Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev",
    "link": "http://arxiv.org/abs/2312.10302",
    "context": "Title: One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)\nAbstract: Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev",
    "path": "papers/23/12/2312.10302.json",
    "total_tokens": 864,
    "translated_title": "作为大型语言模型的指导数据探索者的单次学习方法",
    "translated_abstract": "将大型语言模型与人类对齐是有效利用其预训练能力的关键步骤。当前的指导调优方法通常依赖于扩展数据集大小，但缺乏确保数据质量的明确策略，这可能无意中引入噪声并降低模型性能。为了应对这一挑战，我们引入了一种新颖高效的方法Nuggets，该方法利用单次学习从庞大的数据集中选择高质量的指导数据。Nuggets评估单个指导示例作为有效单次示例的潜力，从而识别可以显著提升各种任务性能的示例。Nuggets利用基于候选示例对多样锚定集的困惑度影响的评分系统，有助于选择对指导调优最有益的数据。通过在两个基准测试集MT-Bench和Alpaca-Ev上进行严格测试",
    "tldr": "本研究提出了一种名为Nuggets的新颖有效方法，利用单次学习从庞大的数据集中选择高质量的指导数据，通过评估示例对多样锚定集的困惑度影响，选择对指导调优最有益的数据"
}