{
    "title": "Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)",
    "abstract": "Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models' predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies",
    "link": "http://arxiv.org/abs/2312.15097",
    "context": "Title: Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)\nAbstract: Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models' predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies",
    "path": "papers/23/12/2312.15097.json",
    "total_tokens": 895,
    "translated_title": "通过论据集成实现模型多样性下的追索（技术报告）",
    "translated_abstract": "模型多样性（MM）是指在解决同一预测任务时可以训练出多个性能相同的机器学习模型。最近的研究表明，在MM下获得的模型对于相同的输入可能产生不一致的预测。当出现这种情况时，为受到模型预测影响的个体提供反事实解释（CEs）变得具有挑战性。在本文中，我们正式定义了这个问题，称之为追索感知的集成，并确定了解决这个问题的方法应该满足的几个理想属性。我们发现现有的集成方法在不同方式下自然扩展以提供CEs时未能满足这些属性。然后，我们引入了论证集成，采用计算论证方法来确保CEs对MM的鲁棒性，并适应可定制的用户偏好。我们从理论和实验上证明了论证集成满足了这些属性。",
    "tldr": "本研究提出了一种名为“追索感知的集成”的方法，通过使用计算论证方法来解决模型多样性下提供反事实解释的挑战性。该方法保证了在模型多样性情况下CEs的鲁棒性，并可以适应用户的个性化偏好。"
}