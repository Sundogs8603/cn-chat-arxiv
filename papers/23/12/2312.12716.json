{
    "title": "BloomVQA: Assessing Hierarchical Multi-modal Comprehension",
    "abstract": "arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons",
    "link": "https://arxiv.org/abs/2312.12716",
    "context": "Title: BloomVQA: Assessing Hierarchical Multi-modal Comprehension\nAbstract: arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons",
    "path": "papers/23/12/2312.12716.json",
    "total_tokens": 852,
    "translated_title": "BloomVQA：评估分层多模态理解",
    "translated_abstract": "我们提出了一个新颖的VQA数据集BloomVQA，旨在促进对大型视觉语言模型在理解任务上的全面评估。与当前的基准不同，它们通常侧重于基于事实的记忆和没有理论基础的简单推理任务，我们收集了基于图片故事的多项选择样本，反映了不同层次的理解，正如布鲁姆的分类法所展示的，在教育研究中被广泛采用的经典框架。我们的数据映射到一种新颖的分层图表示，实现了自动数据增强和表征模型一致性的新措施。我们对最近的多模态模型进行了分级评估和可靠性分析。与低级任务相比，我们发现在需要高级理解和认知能力的任务上表现下降，VQA准确性下降了高达38.0%。与早期模型相比，GPT-4V表现出...",
    "tldr": "提出了新VQA数据集BloomVQA，基于Bloom的分类法，通过层次图表示实现数据增强和模型一致性评估，揭示大型视觉语言模型在高级理解任务上的性能下降。"
}