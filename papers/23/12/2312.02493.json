{
    "title": "Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)",
    "abstract": "Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is o",
    "link": "http://arxiv.org/abs/2312.02493",
    "context": "Title: Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)\nAbstract: Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is o",
    "path": "papers/23/12/2312.02493.json",
    "total_tokens": 878,
    "translated_title": "对于不可预知的网络的最优分布式学习的灵活通信",
    "translated_abstract": "梯度压缩通过发送更少的值和对应的索引来减轻分布式深度学习中的昂贵通信，通常通过Allgather（AG）来实现。在高压缩比（CR）下进行训练可以实现与DenseSGD相同的高精度，但由于高通信成本（即并行效率），并行扩展性较低。使用较低的CR可以通过降低同步开销来提高并行效率，但同时也降低模型精度（即统计效率）。此外，使用不同模型和CR获得的加速度也会因网络延迟，有效带宽和用于聚合的集合操作而有所不同。在许多情况下，像Allreduce（AR）这样的集体操作与AG交换相同数量的数据的成本较低。本文提出了一种AR兼容的Topk压缩器，在某些网络配置中比AG更优。我们开发了一种灵活的通信策略，根据使用哪个集体操作来在AG和AR之间切换",
    "tldr": "本文提出了一种灵活的通信策略，根据网络配置自动切换使用Allgather（AG）或Allreduce（AR），以提高分布式学习的并行效率和模型精度。",
    "en_tdlr": "This paper proposes a flexible communication strategy that automatically switches between Allgather (AG) and Allreduce (AR) based on network configurations to improve parallel efficiency and model accuracy in distributed learning systems."
}