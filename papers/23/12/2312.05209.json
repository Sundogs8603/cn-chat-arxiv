{
    "title": "HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models",
    "abstract": "arXiv:2312.05209v2 Announce Type: replace  Abstract: Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to ",
    "link": "https://arxiv.org/abs/2312.05209",
    "context": "Title: HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models\nAbstract: arXiv:2312.05209v2 Announce Type: replace  Abstract: Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to ",
    "path": "papers/23/12/2312.05209.json",
    "total_tokens": 860,
    "translated_title": "HALO：用于在大型语言模型中表示和分类幻觉的本体论",
    "translated_abstract": "arXiv:2312.05209v2 公告类型：替换 摘要：生成式人工智能的最新进展，包括ChatGPT等大型语言模型（LLMs），为从自然语言处理到知识发现和数据挖掘等领域带来了重大机遇。然而，人们越来越意识到这些模型可能存在问题，例如虚构信息或“幻觉”，以及在看似简单问题上犯错误的推理能力。由于像ChatGPT这样的模型备受欢迎，学术界和公民科学家都记录了多种类型和严重程度的幻觉。尽管已有相关工作，但仍然缺乏一种形式模型来细致描述和表示这些幻觉（带有相关元数据）。本文通过提出Hallucination Ontology或HALO来填补这一空白，HALO是一种正式的、可扩展的本体论，用OWL编写，目前支持六种已知的幻觉类型。",
    "tldr": "本文介绍了一种名为HALO的形式化、可扩展的本体论，用OWL编写，用于描述和表示大型语言模型中的六种不同类型的幻觉。",
    "en_tdlr": "This paper presents HALO, a formal and extensible ontology written in OWL, for describing and representing six different types of hallucinations in large language models."
}