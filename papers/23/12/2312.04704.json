{
    "title": "Efficient Parallel Reinforcement Learning Framework using the Reactor Model",
    "abstract": "Parallel Reinforcement Learning (RL) frameworks are essential for mapping RL workloads to multiple computational resources, allowing for faster generation of samples, estimation of values, and policy improvement. These computational paradigms require a seamless integration of training, serving, and simulation workloads. Existing frameworks, such as Ray, are not managing this orchestration efficiently, especially in RL tasks that demand intensive input/output and synchronization between actors on a single node. In this study, we have proposed a solution implementing the reactor model, which enforces a set of actors to have a fixed communication pattern. This allows the scheduler to eliminate work needed for synchronization, such as acquiring and releasing locks for each actor or sending and processing coordination-related messages. Our framework, Lingua Franca (LF), a coordination language based on the reactor model, also supports true parallelism in Python and provides a unified interf",
    "link": "https://arxiv.org/abs/2312.04704",
    "context": "Title: Efficient Parallel Reinforcement Learning Framework using the Reactor Model\nAbstract: Parallel Reinforcement Learning (RL) frameworks are essential for mapping RL workloads to multiple computational resources, allowing for faster generation of samples, estimation of values, and policy improvement. These computational paradigms require a seamless integration of training, serving, and simulation workloads. Existing frameworks, such as Ray, are not managing this orchestration efficiently, especially in RL tasks that demand intensive input/output and synchronization between actors on a single node. In this study, we have proposed a solution implementing the reactor model, which enforces a set of actors to have a fixed communication pattern. This allows the scheduler to eliminate work needed for synchronization, such as acquiring and releasing locks for each actor or sending and processing coordination-related messages. Our framework, Lingua Franca (LF), a coordination language based on the reactor model, also supports true parallelism in Python and provides a unified interf",
    "path": "papers/23/12/2312.04704.json",
    "total_tokens": 804,
    "translated_title": "使用反应器模型的高效并行强化学习框架",
    "translated_abstract": "并行强化学习(Reinforcement Learning, RL)框架对于将RL工作负载映射到多个计算资源非常重要，可以加快样本生成、值估计和策略改进的速度。这些计算模式要求无缝集成训练、服务和模拟工作负载。现有的框架，如Ray，在RL任务中对单个节点上的角色之间的输入/输出和同步要求不够高效。在本研究中，我们提出了一种实现反应器模型的解决方案，该模型强制一组角色具有固定的通信模式。这使得调度程序可以消除需要同步的工作，例如每个角色的锁的获取和释放，或发送和处理协调相关的消息。我们的框架Lingua Franca (LF) 是一种基于反应器模型的协调语言，还在Python中支持真正的并行处理，并提供统一的接口。",
    "tldr": "本论文提出了一种使用反应器模型的高效并行强化学习框架，可以有效解决现有框架在同步和输入/输出方面的低效问题。",
    "en_tdlr": "This paper proposes an efficient parallel reinforcement learning framework using the reactor model, which effectively addresses the inefficiencies in synchronization and input/output in existing frameworks."
}