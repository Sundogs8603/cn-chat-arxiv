{
    "title": "The LLM Surgeon",
    "abstract": "arXiv:2312.17244v2 Announce Type: replace-cross  Abstract: State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimental",
    "link": "https://arxiv.org/abs/2312.17244",
    "context": "Title: The LLM Surgeon\nAbstract: arXiv:2312.17244v2 Announce Type: replace-cross  Abstract: State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimental",
    "path": "papers/23/12/2312.17244.json",
    "total_tokens": 778,
    "translated_title": "LLM外科医生",
    "translated_abstract": "最先进的语言模型越来越庞大，以期在大量可用的文本数据集上实现最佳性能。然而，Transformer架构的巨大规模使得在计算、环境或设备特定约束下部署模型变得困难。我们探讨了对现有预训练模型进行数据驱动压缩作为训练较小模型的替代方法。为此，我们将目标损失景观的Kronecker分解曲率近似扩展到大型语言模型中。通过这样做，我们既可以计算可删除结构的动态分配，也可以更新剩余权重以考虑删除。我们提供了一个通用框架，用于非结构化、半结构化和结构化修剪，并改进了权重更新以捕捉更多权重之间的相关性，同时保持计算效率。实验证明...",
    "tldr": "通过数据驱动压缩现有预训练模型，我们提供了一个改进权重更新的通用框架，以更有效地捕捉更多权重之间的相关性，同时保持计算效率。",
    "en_tdlr": "By data-driven compression of existing pretrained models, we provide a general framework with improved weight updates to effectively capture more correlations between weights while maintaining computational efficiency."
}