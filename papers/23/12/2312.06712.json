{
    "title": "Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models",
    "abstract": "Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. This work illuminates the fundamental reasons for such misalignment, pinpointing issues related to low attention activation scores and mask overlaps. While previous research efforts have individually tackled these issues, we assert that a holistic approach is paramount. Thus, we propose two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Our method diverges from conventional test-time-adaptation techniques, focusing on finetuning critical parameters, which enhances scalability and generalizability. Comprehensive evaluations demonstrate the superior performance of our model in terms of image realism, text-image alignment, and adaptability, notably outperform",
    "link": "https://arxiv.org/abs/2312.06712",
    "context": "Title: Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models\nAbstract: Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. This work illuminates the fundamental reasons for such misalignment, pinpointing issues related to low attention activation scores and mask overlaps. While previous research efforts have individually tackled these issues, we assert that a holistic approach is paramount. Thus, we propose two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Our method diverges from conventional test-time-adaptation techniques, focusing on finetuning critical parameters, which enhances scalability and generalizability. Comprehensive evaluations demonstrate the superior performance of our model in terms of image realism, text-image alignment, and adaptability, notably outperform",
    "path": "papers/23/12/2312.06712.json",
    "total_tokens": 973,
    "translated_title": "Separate-and-Enhance: 文本到图像扩散模型的组合微调方法",
    "translated_abstract": "尽管基于扩散的文本到图像(T2I)模型在近期取得了重大进展，但是目前的系统仍然不太能够确保与文本提示对齐的良好组合生成，特别是在多对象生成方面。本研究阐明了这种错位的基本原因，指出了与低注意力激活得分和掩码重叠有关的问题。尽管先前的研究工作已经分别解决了这些问题，但我们认为整体方法至关重要。因此，我们提出了两个新的目标，即分离损失和增强损失，分别减少对象掩码重叠和最大化注意力得分。我们的方法不同于常规的测试时调适技术，着重于微调关键参数，从而提高了可伸缩性和普适性。全面的评估表明，我们的模型在图像逼真度、文本-图像对齐和适应性方面表现卓越，尤其是在实现上述方面优于其他方法。",
    "tldr": "本研究提出了一种名为\"Separate-and-Enhance\"的方法，通过分离损失和增强损失来解决现有文本到图像模型中的错位问题。这种方法通过减少对象掩码重叠和最大化注意力得分来提高图像生成的质量和与文本提示的对齐。评估结果显示，该方法在图像逼真度、文本-图像对齐和适应性方面表现优于其他方法。"
}