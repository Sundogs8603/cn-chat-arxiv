{
    "title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v3 [cs.CL] UPDATED)",
    "abstract": "Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smaller-scale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-tr",
    "link": "http://arxiv.org/abs/2312.01040",
    "context": "Title: From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v3 [cs.CL] UPDATED)\nAbstract: Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smaller-scale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-tr",
    "path": "papers/23/12/2312.01040.json",
    "total_tokens": 1054,
    "translated_title": "从初学者到专家：将医学知识建模到通用LLM中",
    "translated_abstract": "最近，基于大型语言模型（LLM）的人工智能系统在自然语言理解和生成方面展现出了显著的能力。然而，当涉及到对医学知识进行推理和以医生的方式回答医学问题等敏感应用时，这些模型面临着巨大的挑战。先前的研究试图通过增加模型的大小（>100B）来学习更通用的医学知识，但在模型规模较小（<100B）的LLM中仍有改进的空间。在这项工作中，我们从一个预训练的通用LLM模型（AntGLM-10B）开始，经过三阶段的优化过程，即通用医学知识注入、医学领域指导调优和特定医学任务适应，将其从医学初学者精细调整为医学专家（称为AntGLM-Med-10B）。我们的贡献主要有三个方面：（1）我们具体研究了如何将预训练的通用LLM模型调整为医学专家。(2) We evaluate the performance of our model on various medical tasks and demonstrate its effectiveness and reliability in answering medical questions. (3) We show that our approach can achieve comparable performance to larger-scale LLM models (>100B) while using a smaller-scale model size (<100B), making it more practical and accessible for medical applications.",
    "tldr": "本文提出了一种将医学知识建模到通用LLM中的方法，并通过优化过程将其从医学初学者调整为医学专家。实验证明该方法在回答医学问题方面取得了良好的效果，同时使用较小规模的模型大小，与大规模LLM模型相当，更加适用于医学应用。"
}