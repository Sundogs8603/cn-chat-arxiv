{
    "title": "All Rivers Run to the Sea: Private Learning with Asymmetric Flows",
    "abstract": "arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv",
    "link": "https://arxiv.org/abs/2312.05264",
    "context": "Title: All Rivers Run to the Sea: Private Learning with Asymmetric Flows\nAbstract: arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv",
    "path": "papers/23/12/2312.05264.json",
    "total_tokens": 731,
    "translated_title": "所有的河流都汇聚到大海：具有不对称流量的私有学习",
    "translated_abstract": "数据隐私在云机器学习服务平台中备受关注，当敏感数据暴露给服务提供商时。为了在保护隐私的同时实现高性能计算，我们提出了一种新的私有训练和推理框架Delta，具有与非私有集中训练相当的模型性能。Delta具有两个不对称的数据流：主要的信息敏感流和残差流。主要部分流入一个小模型，而残余部分则被转移到一个大模型。具体来说，Delta将信息敏感表示嵌入到低维空间中，同时将信息不敏感部分推入高维残差中。",
    "tldr": "提出了一种新的私有训练和推理框架Delta，通过两个不对称数据流实现了具有可比较模型性能的隐私保护。",
    "en_tdlr": "Introduce Delta, a new private training and inference framework, which achieves privacy protection with comparable model performance using two asymmetric data flows."
}