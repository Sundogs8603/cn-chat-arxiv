{
    "title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation",
    "abstract": "arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac",
    "link": "https://arxiv.org/abs/2312.11361",
    "context": "Title: NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation\nAbstract: arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac",
    "path": "papers/23/12/2312.11361.json",
    "total_tokens": 942,
    "translated_title": "NoMIRACL: 知道自己不知道的鲁棒多语言检索增强生成",
    "translated_abstract": "arXiv:2312.11361v2 公告类型: 替换 摘要: 检索增强生成（RAG）通过利用外部知识源来将大型语言模型（LLM）输出与现实联系起来，以减少事实幻觉。然而，先前的研究缺乏对不同语言族的全面评估，这使得很难评估LLM对外部检索知识错误的鲁棒性。为了克服这一问题，我们建立了NoMIRACL，这是一个人类注释的数据集，用于评估RAG中LLM对18种在类型上多样化的语言的鲁棒性。NoMIRACL包括一个非相关子集和一个相关子集。非相关子集中的查询包含被判断为不相关的段落，而相关子集中的查询至少包含一个被判断为相关的段落。我们使用两个指标来衡量LLM的鲁棒性：（i）幻觉率，衡量模型倾向于在非相关子集的段落中产生幻觉答案的程度，以及（ii）错误率，衡量模型的不准确度。",
    "tldr": "建立了用于评估大型语言模型在多语言环境中检索增强生成中的鲁棒性的NoMIRACL数据集，并提出了两个衡量模型鲁棒性的指标：幻觉率和错误率。"
}