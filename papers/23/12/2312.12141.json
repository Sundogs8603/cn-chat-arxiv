{
    "title": "Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)",
    "abstract": "We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store \"Paris is related to France\". FFN layers store \"Paris is a capital/city\", activated by attention subvalues relate",
    "link": "http://arxiv.org/abs/2312.12141",
    "context": "Title: Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)\nAbstract: We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store \"Paris is related to France\". FFN layers store \"Paris is a capital/city\", activated by attention subvalues relate",
    "path": "papers/23/12/2312.12141.json",
    "total_tokens": 904,
    "translated_title": "在大型语言模型中定位事实知识：探索剩余流和分析词汇空间中的子值。",
    "translated_abstract": "通过探索剩余流和分析词汇空间中的子值，我们找到了大型语言模型中事实知识的位置。我们发现当投影到词汇空间时，子值具有可人类解释的概念的原因。子值的softmax之前的值通过一个加法函数相加，因此词汇空间中前几个标记的概率会增加。基于此，我们发现使用对数概率增加来计算层和子值的重要性比概率增加更好，因为对数概率增加的曲线呈线性单调增形状。此外，我们计算内积来评估前馈网络（FFN）的子值被前面的层激活的程度。根据我们的方法，我们找到了事实知识“法国，首都，巴黎”存储的位置。具体来说，注意力层存储“巴黎与法国相关”。FFN层存储“巴黎是一个首都/城市”，由注意力子值激活。",
    "tldr": "通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。",
    "en_tdlr": "By exploring the residual stream and analyzing subvalues in vocabulary space, we locate factual knowledge in large language models and find the location where information about \"France, capital, Paris\" is stored."
}