{
    "title": "Conversational Question Answering with Reformulations over Knowledge Graph",
    "abstract": "arXiv:2312.17269v2 Announce Type: replace-cross  Abstract: Conversational question answering (convQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CornNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CornNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs. The learned question representation is then used by an RL model to locate the correct answer in a K",
    "link": "https://arxiv.org/abs/2312.17269",
    "context": "Title: Conversational Question Answering with Reformulations over Knowledge Graph\nAbstract: arXiv:2312.17269v2 Announce Type: replace-cross  Abstract: Conversational question answering (convQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CornNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CornNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs. The learned question representation is then used by an RL model to locate the correct answer in a K",
    "path": "papers/23/12/2312.17269.json",
    "total_tokens": 768,
    "translated_title": "使用知识图谱上的重述进行对话式问答",
    "translated_abstract": "对话式问答（convQA）是关于知识图谱（KGs）中包含的信息的多轮自然语言问题的回答。目前的convQA方法通常在难以理解的问答配对方面遇到困难。这些输入对于人类来说在对话历史的基础上很容易理解，但对于机器来说很难解释，这可能会降低convQA性能。为了解决这个问题，我们提出了一个基于强化学习（RL）的模型CornNet，它利用大型语言模型（LLMs）生成的问题重述来提高convQA性能。CornNet采用教师-学生架构，其中教师模型使用人类编写的重述来学习问题表示，学生模型通过LLMs生成的重述来模仿教师模型的输出。然后，RL模型使用学到的问题表示来在KG中定位正确答案。",
    "tldr": "提出了使用重述技术改进对话式问答性能的CornNet模型",
    "en_tdlr": "Proposed the CornNet model that improves conversational question answering performance by utilizing reformulations技.verbose."
}