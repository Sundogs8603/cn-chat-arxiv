{
    "title": "World Models via Policy-Guided Trajectory Diffusion",
    "abstract": "arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in \"in imagination\". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,",
    "link": "https://arxiv.org/abs/2312.08533",
    "context": "Title: World Models via Policy-Guided Trajectory Diffusion\nAbstract: arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in \"in imagination\". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,",
    "path": "papers/23/12/2312.08533.json",
    "total_tokens": 900,
    "translated_title": "通过策略引导的轨迹扩散实现世界模型",
    "translated_abstract": "世界模型是开发智能agent的强大工具。通过预测一系列行动的结果，世界模型使得可以通过在“想象中”使用合成数据来优化策略，即通过在线策略增强学习（RL）来实现。现有的世界模型是自回归的，因为它们在预测下一个状态的同时从策略中采样下一个行动。随着轨迹长度的增长，预测误差必然会累积。在这项工作中，我们提出了一种新颖的世界建模方法，不是自回归的，而是通过扩散模型一次生成整个在线策略轨迹。我们的方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，利用了除了策略的动作分布梯度之外的一个去噪模型，将最初随机状态和动作的轨迹扩散成一个在线合成轨迹。我们分析了PolyGRAD与",
    "tldr": "这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。",
    "en_tdlr": "This work introduces a novel world modeling approach, Policy-Guided Trajectory Diffusion (PolyGRAD), which generates entire on-policy trajectories in a single pass through a diffusion model, avoiding the compounding prediction error with trajectory length growth seen in autoregressive models."
}