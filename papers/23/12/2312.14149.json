{
    "title": "TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification",
    "abstract": "arXiv:2312.14149v3 Announce Type: replace-cross  Abstract: The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, e.g., the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\\textit{e.g.}, cat) and attributes (\\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of",
    "link": "https://arxiv.org/abs/2312.14149",
    "context": "Title: TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification\nAbstract: arXiv:2312.14149v3 Announce Type: replace-cross  Abstract: The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, e.g., the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\\textit{e.g.}, cat) and attributes (\\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of",
    "path": "papers/23/12/2312.14149.json",
    "total_tokens": 773,
    "translated_title": "TagAlign：利用多标签分类改进视觉-语言对齐",
    "translated_abstract": "学习视觉-语言模型的关键在于从视觉和语言数据中提取语义对齐的信息。我们提出了一种非常简单的方法，可以更好地对齐图像和文本特征，而无需除图像-文本对之外的其他数据格式。具体而言，给定一幅图像及其配对的文本，我们设法从描述中解析出对象（例如猫）和属性（例如黑色），这些对象和属性极有可能存在于图像中。值得注意的是，解析管道完全自动化，因此具有良好的可扩展性。借助这些解析出的语义作为监督信号，我们可以将常用的图像-文本对比损失与多标签分类损失相结合。在广泛的实验结果中",
    "tldr": "提出了一种简单的方法，通过解析图像与文本中的对象和属性，使用多标签分类损失来改进视觉-语言对齐模型",
    "en_tdlr": "Propose a simple approach to improve vision-language alignment by parsing objects and attributes from images and texts, using multi-tag classification loss."
}