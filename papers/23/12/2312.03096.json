{
    "title": "What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes",
    "abstract": "Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \\textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in",
    "link": "https://arxiv.org/abs/2312.03096",
    "context": "Title: What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes\nAbstract: Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \\textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in",
    "path": "papers/23/12/2312.03096.json",
    "total_tokens": 805,
    "translated_title": "引起多义性的原因是什么？通过偶然因素的混合选择性的替代起源故事",
    "translated_abstract": "多义性神经元——激活一组不相关特征的神经元——被视为解释任务优化深度网络的显著障碍，对AI安全性产生影响。传统的多义性起源故事是数据包含的“特征”多于神经元，因此学习执行任务迫使网络将多个不相关特征分配给同一个神经元，危及我们理解网络内部处理的能力。在这项工作中，我们提出了多义性的第二个且非互斥的替代起源故事。我们展示了即使有足够的神经元来表示数据中的所有特征，偶然多义性也可能产生，这是一种我们称之为“偶然多义性”的现象。通过理论和实验证明，偶然多义性可以由多种原因引起，包括正则化和神经噪音；这种偶然多义性发生是因为随机的因素。",
    "tldr": "这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。"
}