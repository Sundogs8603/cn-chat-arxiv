{
    "title": "Factorized Explainer for Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world data",
    "link": "https://arxiv.org/abs/2312.05596",
    "context": "Title: Factorized Explainer for Graph Neural Networks\nAbstract: Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world data",
    "path": "papers/23/12/2312.05596.json",
    "total_tokens": 853,
    "translated_title": "图神经网络的因式分解解释器",
    "translated_abstract": "由于其学习图结构数据的能力，图神经网络（GNN）受到越来越多的关注。为了解开这些深度学习模型的黑盒，已经提出了事后实例级解释方法来理解GNN的预测结果。这些方法旨在发现解释已训练的GNN预测行为的子结构。本文通过分析证明了对于一类广泛的解释任务，基于图信息瓶颈（GIB）原则的传统方法存在不能与可解释性概念一致的平凡解。相反，我们认为可以使用一种修改后的GIB原则来避免上述平凡解。我们进一步介绍了一个具有理论性能保证的新型因式分解解释模型。通过修改后的GIB原则来分析所提出的因式分解解释器的结构特性。我们在合成数据和真实世界数据上进行了大量实验。",
    "tldr": "本研究发现了传统基于图信息瓶颈原则的方法在解释图神经网络时存在平凡解，并提出了一种修改后的原则来避免此问题。同时，提出了一个带有理论性能保证的因式分解解释模型，并通过实验验证了其性能。"
}