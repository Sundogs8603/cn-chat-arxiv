{
    "title": "Hyperparameter Optimization for Large Language Model Instruction-Tuning",
    "abstract": "The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \\nomad algorithm, achieving a boost in performance and human alignment of the tuned mo",
    "link": "https://arxiv.org/abs/2312.00949",
    "context": "Title: Hyperparameter Optimization for Large Language Model Instruction-Tuning\nAbstract: The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \\nomad algorithm, achieving a boost in performance and human alignment of the tuned mo",
    "path": "papers/23/12/2312.00949.json",
    "total_tokens": 868,
    "translated_title": "大型语言模型调参的超参数优化",
    "translated_abstract": "大型语言模型（LLM）的微调使其在自然语言处理应用领域取得了里程碑式的成就。越来越大的LLM的出现为更高效的微调方法铺平了道路。其中，低秩适应（LoRA）方法将预训练LLM的大部分权重冻结，并引入权重矩阵的低秩分解，仅允许调整网络的极小部分。使用LoRA进行微调的模型在下游任务上的性能主要依赖于一组超参数，包括分解的秩。在这项研究中，我们通过两种主要的黑盒优化（BBO）技术来研究这些超参数的选择。我们将在预训练的LLM上进行微调和验证的整个流程视为黑盒，并使用Nomad算法高效地探索超参数空间，从而提高性能和调整模型与人类对齐。",
    "tldr": "该论文研究了大型语言模型调参的超参数优化，通过引入低秩适应方法实现对网络的部分调整，使用黑盒优化技术探索超参数空间，取得了性能提升和模型与人类对齐的效果。",
    "en_tdlr": "This paper investigates hyperparameter optimization for large language model instruction-tuning. It introduces the Low-Rank Adaptation method to adjust a small proportion of the network weights, and uses blackbox optimization techniques to explore the hyperparameter space, achieving performance improvement and model alignment with humans."
}