{
    "title": "Semantic Guidance Tuning for Text-To-Image Diffusion Models. (arXiv:2312.15964v2 [cs.CV] UPDATED)",
    "abstract": "Recent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in resp",
    "link": "http://arxiv.org/abs/2312.15964",
    "context": "Title: Semantic Guidance Tuning for Text-To-Image Diffusion Models. (arXiv:2312.15964v2 [cs.CV] UPDATED)\nAbstract: Recent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in resp",
    "path": "papers/23/12/2312.15964.json",
    "total_tokens": 854,
    "translated_title": "文本到图像扩散模型的语义引导调整",
    "translated_abstract": "最近文本到图像（T2I）扩散模型的进展展现出了令人印象深刻的成功，能够生成具有零样本泛化能力的高质量图像。然而，目前的模型在紧密遵循提示语义方面存在困难，经常错误地表示或忽视特定属性。为了解决这个问题，我们提出了一种简单的、不需要训练的方法，在推理过程中调节扩散模型的引导方向。我们首先将提示语义分解为一组概念，并监控引导轨迹与每个概念的关系。我们的关键观察是，模型在遵循提示语义方面的偏差与引导与一个或多个这些概念的偏离高度相关。基于这个观察，我们设计了一种技术，将引导方向引导到模型偏离的任何概念。广泛的实验验证了我们的方法改善了扩散模型生成的图像的语义对齐性。",
    "tldr": "本文提出了一种简单的、不需要训练的方法，用于调节文本到图像扩散模型在推理过程中的引导方向，以提高生成图像的语义对齐性。"
}