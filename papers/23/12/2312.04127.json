{
    "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak",
    "abstract": "arXiv:2312.04127v2 Announce Type: replace  Abstract: Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as \"Jailbreak Attack\". In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is \"Inherent Response Tendency Analysis\" which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is \"Real-World Instructions-Driven Jailbreak\" which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs wh",
    "link": "https://arxiv.org/abs/2312.04127",
    "context": "Title: Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak\nAbstract: arXiv:2312.04127v2 Announce Type: replace  Abstract: Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as \"Jailbreak Attack\". In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is \"Inherent Response Tendency Analysis\" which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is \"Real-World Instructions-Driven Jailbreak\" which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs wh",
    "path": "papers/23/12/2312.04127.json",
    "total_tokens": 898,
    "translated_title": "分析LLMs的固有响应倾向：真实世界指令驱动的越狱",
    "translated_abstract": "大量工作致力于改善大型语言模型（LLMs）的安全机制。然而，当面对恶意指令时，LLMs仍然倾向于生成有害响应，这种现象被称为“越狱攻击”。 在我们的研究中，我们介绍了一种新颖的自动越狱方法RADIAL，通过放大LLMs生成肯定响应的潜力来绕过安全机制。我们方法的越狱思想是“固有响应倾向分析”，它识别出那些在本质上可以导致LLMs生成肯定响应的真实世界指令，相应的越狱策略是“真实世界指令驱动的越狱”，它涉及通过在恶意指令周围策略性地拼接通过上述分析识别出的真实世界指令。我们的方法在五个开源先进的LLMs上对英语恶意指令取得了优秀的攻击性能。",
    "tldr": "该研究引入了一种新颖的自动越狱方法RADIAL，通过放大LLMs生成肯定响应的潜力来绕过安全机制，实现了对英语恶意指令的优秀攻击性能。",
    "en_tdlr": "This research introduces a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses, achieving excellent attack performance on English malicious instructions."
}