{
    "title": "Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure",
    "abstract": "arXiv:2312.11364v2 Announce Type: replace  Abstract: We present counting reward automata-a finite state machine variant capable of modelling any reward function expressible as a formal language. Unlike previous approaches, which are limited to the expression of tasks as regular languages, our framework allows for tasks described by unrestricted grammars. We prove that an agent equipped with such an abstract machine is able to solve a larger set of tasks than those utilising current approaches. We show that this increase in expressive power does not come at the cost of increased automaton complexity. A selection of learning algorithms are presented which exploit automaton structure to improve sample efficiency. We show that the state machines required in our formulation can be specified from natural language task descriptions using large language models. Empirical results demonstrate that our method outperforms competing approaches in terms of sample efficiency, automaton complexity, an",
    "link": "https://arxiv.org/abs/2312.11364",
    "context": "Title: Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure\nAbstract: arXiv:2312.11364v2 Announce Type: replace  Abstract: We present counting reward automata-a finite state machine variant capable of modelling any reward function expressible as a formal language. Unlike previous approaches, which are limited to the expression of tasks as regular languages, our framework allows for tasks described by unrestricted grammars. We prove that an agent equipped with such an abstract machine is able to solve a larger set of tasks than those utilising current approaches. We show that this increase in expressive power does not come at the cost of increased automaton complexity. A selection of learning algorithms are presented which exploit automaton structure to improve sample efficiency. We show that the state machines required in our formulation can be specified from natural language task descriptions using large language models. Empirical results demonstrate that our method outperforms competing approaches in terms of sample efficiency, automaton complexity, an",
    "path": "papers/23/12/2312.11364.json",
    "total_tokens": 836,
    "translated_title": "计数奖励自动机：通过利用奖励函数结构实现高效抽样的强化学习",
    "translated_abstract": "我们提出了计数奖励自动机——一种有限状态机变体，能够对任何可表示为形式语言的奖励函数进行建模。与以往仅能表达任务为正则语言的方法不同，我们的框架允许描述为无限制文法的任务。我们证明，配备这样抽象机器的代理能够解决比使用当前方法的任务集更广泛。我们展示，这种表达能力增加并不会增加自动机复杂性的代价。提出了一些利用自动机结构来提高样本效率的学习算法。我们展示，在我们的公式中所需的状态机可以使用大型语言模型从自然语言任务描述中指定。实证结果表明，我们的方法在样本效率、自动机复杂性方面优于竞争方法。",
    "tldr": "提出了计数奖励自动机概念，能够对任何奖励函数进行建模，使得强化学习更加高效，并且能够利用自然语言任务描述来指定状态机，实验证明在样本效率方面优于竞争方法。",
    "en_tdlr": "Counting reward automata concept introduced, able to model any reward function, making reinforcement learning more efficient, capable of specifying state machines using natural language task descriptions, empirical results demonstrate outperformance in sample efficiency compared to competing methods."
}