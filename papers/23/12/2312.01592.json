{
    "title": "Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment. (arXiv:2312.01592v2 [cs.CL] UPDATED)",
    "abstract": "Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically it",
    "link": "http://arxiv.org/abs/2312.01592",
    "context": "Title: Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment. (arXiv:2312.01592v2 [cs.CL] UPDATED)\nAbstract: Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically it",
    "path": "papers/23/12/2312.01592.json",
    "total_tokens": 886,
    "translated_title": "通过多模态部分对齐进行基于视觉信息的锚定语言学习，扩展BERT表征",
    "translated_abstract": "在现有的视觉锚定语言学习研究中，语言模型已经通过语言目标和视觉锚定来进行监督。然而，由于视觉锚定数据集和语言语料库的分布和规模差异，语言模型往往会混淆在锚定数据中出现的令牌的上下文和不出现的令牌的上下文。因此，在表征学习过程中，视觉信息与句子的上下文含义之间存在不匹配。为了克服这个限制，我们提出了GroundedBERT-一种通过视觉锚定信息增强BERT表示的基于锚定语言学习方法。GroundedBERT包括两个组件：(i)原始BERT，它从语言语料库中学习单词的上下文表示，(ii)视觉锚定​​模块，它从视觉锚定数据集中学习视觉信息。此外，我们采用了最优传输(OT)，特别是",
    "tldr": "本论文提出了一种名为GroundedBERT的方法，通过视觉锚定信息增强BERT表示，解决了由于视觉锚定数据集和语言语料库的差异导致的上下文含义不匹配的问题。"
}