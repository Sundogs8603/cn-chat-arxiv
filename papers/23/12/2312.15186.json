{
    "title": "Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)",
    "abstract": "While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device",
    "link": "http://arxiv.org/abs/2312.15186",
    "context": "Title: Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)\nAbstract: While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device",
    "path": "papers/23/12/2312.15186.json",
    "total_tokens": 868,
    "translated_title": "高效异步稀疏化量化联邦学习",
    "translated_abstract": "在数据分布在多个边缘设备上的情况下，联邦学习越来越受到关注，它可以在不传输原始数据的情况下合作训练机器学习模型。传统的联邦学习使用参数服务器和大量边缘设备来进行模型训练，每轮选择几个设备参与。然而，有些设备可能会拖慢训练过程甚至导致系统崩溃，而其他空闲设备则闲置不用。由于设备和服务器之间的带宽相对较低，中间数据的通信成为瓶颈。本文提出了一种带有稀疏化和量化的时间高效异步联邦学习方法（TEASQ-Fed），它可以充分利用边缘设备主动参与训练过程。我们利用控制参数来选择适当数量的并行边缘设备。",
    "tldr": "本论文提出了一种高效异步稀疏化量化联邦学习方法（TEASQ-Fed），利用边缘设备的并行参与，解决了传统方法中设备拖慢训练和通信瓶颈的问题。"
}