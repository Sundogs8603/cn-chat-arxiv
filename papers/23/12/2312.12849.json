{
    "title": "Divergences induced by dual subtractive and divisive normalizations of exponential families and their convex deformations. (arXiv:2312.12849v2 [cs.IT] UPDATED)",
    "abstract": "Exponential families are statistical models which are the workhorses in statistics, information theory, and machine learning among others. An exponential family can either be normalized subtractively by its cumulant or free energy function or equivalently normalized divisively by its partition function. Both subtractive and divisive normalizers are strictly convex and smooth functions inducing pairs of Bregman and Jensen divergences. It is well-known that skewed Bhattacharryya distances between probability densities of an exponential family amounts to skewed Jensen divergences induced by the cumulant function between their corresponding natural parameters, and in limit cases that the sided Kullback-Leibler divergences amount to reverse-sided Bregman divergences. In this paper, we first show that the $\\alpha$-divergences between unnormalized densities of an exponential family amounts to scaled $\\alpha$-skewed Jensen divergences induced by the partition function. We then show how compara",
    "link": "http://arxiv.org/abs/2312.12849",
    "context": "Title: Divergences induced by dual subtractive and divisive normalizations of exponential families and their convex deformations. (arXiv:2312.12849v2 [cs.IT] UPDATED)\nAbstract: Exponential families are statistical models which are the workhorses in statistics, information theory, and machine learning among others. An exponential family can either be normalized subtractively by its cumulant or free energy function or equivalently normalized divisively by its partition function. Both subtractive and divisive normalizers are strictly convex and smooth functions inducing pairs of Bregman and Jensen divergences. It is well-known that skewed Bhattacharryya distances between probability densities of an exponential family amounts to skewed Jensen divergences induced by the cumulant function between their corresponding natural parameters, and in limit cases that the sided Kullback-Leibler divergences amount to reverse-sided Bregman divergences. In this paper, we first show that the $\\alpha$-divergences between unnormalized densities of an exponential family amounts to scaled $\\alpha$-skewed Jensen divergences induced by the partition function. We then show how compara",
    "path": "papers/23/12/2312.12849.json",
    "total_tokens": 931,
    "translated_title": "指数族的双重减性和除性归一化引起的差异及其凸变形",
    "translated_abstract": "指数族是统计学、信息理论和机器学习等领域中的重要模型。指数族可以通过累积量或自由能函数减性归一化，也可以通过分配函数除性归一化。这两种归一化方法都是严格的凸平滑函数，会引发一对Bregman和Jensen散度。已知指数族的概率密度之间的偏斜巴氏距离等于通过累积量函数引起的偏斜Jensen散度，而在极限情况下，偏向Kullback-Leibler散度则会转变为反向Bregman散度。本文首先证明了指数族的非归一化概率密度之间的α散度等于由分配函数引起的缩放α偏斜Jensen散度。接下来，我们展示了如何比较不同减性和除性归一化方法的差异。",
    "tldr": "本文研究了指数族的双重减性和除性归一化方法引起的差异，提出了一对Bregman和Jensen散度。同时，证明了α散度可以通过分配函数引起缩放α偏斜Jensen散度，从而比较了不同归一化方法之间的差异。",
    "en_tdlr": "This paper investigates the divergences induced by dual subtractive and divisive normalizations of exponential families, and introduces a pair of Bregman and Jensen divergences. It also demonstrates that α-divergences can be scaled α-skewed Jensen divergences induced by the partition function, providing a way to compare the differences between different normalization methods."
}