{
    "title": "Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v2 [cs.AI] UPDATED)",
    "abstract": "Predicting the collaboration likelihood and measuring cognitive trust to AI systems is more important than ever. To do that, previous research mostly focus solely on the model features (e.g., accuracy, confidence) and ignore the human factor. To address that, we propose several decision-making similarity measures based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired from humans and a wide range of models. We conduct a user study on a textual entailment task, where the users are provided with soft labels from various models and asked to pick the closest option to them. The users are then shown the similarities/differences to their most similar model and are surveyed for their likelihood of collaboration and cognitive trust to the selected system. Finally, we qualitatively and quantitatively analyze the relation between the proposed decision-making similarity measures and the survey results. We find that people tend to collaborate with their most similar models ",
    "link": "http://arxiv.org/abs/2312.08722",
    "context": "Title: Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v2 [cs.AI] UPDATED)\nAbstract: Predicting the collaboration likelihood and measuring cognitive trust to AI systems is more important than ever. To do that, previous research mostly focus solely on the model features (e.g., accuracy, confidence) and ignore the human factor. To address that, we propose several decision-making similarity measures based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired from humans and a wide range of models. We conduct a user study on a textual entailment task, where the users are provided with soft labels from various models and asked to pick the closest option to them. The users are then shown the similarities/differences to their most similar model and are surveyed for their likelihood of collaboration and cognitive trust to the selected system. Finally, we qualitatively and quantitatively analyze the relation between the proposed decision-making similarity measures and the survey results. We find that people tend to collaborate with their most similar models ",
    "path": "papers/23/12/2312.08722.json",
    "total_tokens": 835,
    "translated_title": "量化人工智能协作和认知信任的差异",
    "translated_abstract": "预测协作可能性和测量人们对人工智能系统的认知信任比以往更重要。为了做到这一点，以往的研究主要关注模型特征（例如准确度、置信度），而忽略了人的因素。为了解决这个问题，我们提出了几种基于差异度量（如KL、JSD）计算从人类和各种模型中获取的标签的决策相似度度量。我们在一个文本蕴含任务上进行了用户研究，用户们从各种模型提供的软标签中选择最接近的选项。然后，用户们看到与他们最相似的模型的相似性/差异，并对他们与所选系统的协作可能性和认知信任进行调查。最后，我们对提出的决策相似度度量与调查结果之间的关系进行了定性和定量分析。我们发现人们倾向于与他们最相似的模型进行协作。",
    "tldr": "通过量化人工智能协作和认知信任的差异，我们发现人们倾向于与最相似的模型进行协作。",
    "en_tdlr": "By quantifying the divergence for human-AI collaboration and cognitive trust, we found that people tend to collaborate with their most similar models."
}