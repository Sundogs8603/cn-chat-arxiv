{
    "title": "An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention",
    "abstract": "arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\\textbf{B}$eyond $\\textbf{S}$elf-$\\textbf{A}$ttention for Sequential $\\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t",
    "link": "https://arxiv.org/abs/2312.10325",
    "context": "Title: An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention\nAbstract: arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\\textbf{B}$eyond $\\textbf{S}$elf-$\\textbf{A}$ttention for Sequential $\\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t",
    "path": "papers/23/12/2312.10325.json",
    "total_tokens": 865,
    "translated_title": "超越自注意力的序列推荐中的关注归纳偏差",
    "translated_abstract": "基于Transformer的序列推荐（SR）模型取得了显著的成功。 Transformer的自注意机制在计算机视觉和自然语言处理中遇到了过度平滑问题，即隐藏表示变得类似于标记。 在SR领域，我们首次展示了相同问题的发生。 我们进行了开创性的研究，揭示了自注意在SR中的低通滤波特性，导致了过度平滑。 为此，我们提出了一种名为$\\textbf{B}$eyond $\\textbf{S}$elf-$\\textbf{A}$ttention for Sequential $\\textbf{Rec}$ommendation（BSARec）的新方法，利用傅里叶变换来 i）通过考虑细粒度的序列模式注入归纳偏差和 ii）集成低频和高频信息以减轻过度平滑。 我们的发现在SR领域显示了显著的进展，并有望搭起",
    "tldr": "提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题"
}