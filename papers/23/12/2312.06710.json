{
    "title": "Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)",
    "abstract": "Mitigating catastrophic forgetting is a key hurdle in continual learning. Deep Generative Replay (GR) provides techniques focused on generating samples from prior tasks to enhance the model's memory capabilities. With the progression in generative AI, generative models have advanced from Generative Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major issue is the deterioration in the quality of generated data compared to the original, as the generator continuously self-learns from its outputs. This degradation can lead to the potential risk of catastrophic forgetting occurring in the classifier. To address this, we propose the Class-Prototype Conditional Diffusion Model (CPDM), a GR-based approach for continual learning that enhances image quality in generators and thus reduces catastrophic forgetting in classifiers. The cornerstone of CPDM is a learnable class-prototype that captures the core characteristics of images in a given class. This prototype, integra",
    "link": "http://arxiv.org/abs/2312.06710",
    "context": "Title: Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)\nAbstract: Mitigating catastrophic forgetting is a key hurdle in continual learning. Deep Generative Replay (GR) provides techniques focused on generating samples from prior tasks to enhance the model's memory capabilities. With the progression in generative AI, generative models have advanced from Generative Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major issue is the deterioration in the quality of generated data compared to the original, as the generator continuously self-learns from its outputs. This degradation can lead to the potential risk of catastrophic forgetting occurring in the classifier. To address this, we propose the Class-Prototype Conditional Diffusion Model (CPDM), a GR-based approach for continual learning that enhances image quality in generators and thus reduces catastrophic forgetting in classifiers. The cornerstone of CPDM is a learnable class-prototype that captures the core characteristics of images in a given class. This prototype, integra",
    "path": "papers/23/12/2312.06710.json",
    "total_tokens": 892,
    "translated_title": "使用生成式回放的类原型条件扩散模型进行Continual Learning",
    "translated_abstract": "缓解灾难性遗忘是Continual Learning中的一个关键难题。深度生成式回放（GR）提供了一种通过从先前任务中生成样本来增强模型记忆能力的技术。随着生成式人工智能的发展，生成模型已经从生成对抗网络（GANs）发展到了最新的扩散模型（DMs）。一个主要问题是生成的数据质量与原始数据相比会逐渐下降，因为生成器不断从其输出中进行自我学习。这种退化可能导致分类器中发生灾难性遗忘的潜在风险。为了解决这个问题，我们提出了类原型条件扩散模型（CPDM），一种基于生成式回放的Continual Learning方法，它通过提高生成器中的图像质量从而减少了分类器中的灾难性遗忘。CPDM的核心是一个可学习的类原型，它捕捉了给定类别中图像的核心特征。",
    "tldr": "本论文提出了一个类原型条件扩散模型（CPDM）来解决Continual Learning中的灾难性遗忘问题。CPDM通过提高生成器的图像质量，减少了分类器的灾难性遗忘风险。"
}