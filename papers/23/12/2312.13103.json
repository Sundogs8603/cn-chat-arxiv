{
    "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
    "abstract": "arXiv:2312.13103v2 Announce Type: replace  Abstract: This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans). A subset of original reports was modified to contain synthetic errors by introducing three types of mistakes: \"insert\", \"remove\", and \"substitute\". The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. At the SIMPLE level, our fine-tuned model significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU X-ray data, respectively. This performance boost is also observed in unseen modality, CT scans, as the model performed 19.46% better than the baseline model. The model also surpassed the domain expert's accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the tes",
    "link": "https://arxiv.org/abs/2312.13103",
    "context": "Title: Exploring Multimodal Large Language Models for Radiology Report Error-checking\nAbstract: arXiv:2312.13103v2 Announce Type: replace  Abstract: This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans). A subset of original reports was modified to contain synthetic errors by introducing three types of mistakes: \"insert\", \"remove\", and \"substitute\". The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. At the SIMPLE level, our fine-tuned model significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU X-ray data, respectively. This performance boost is also observed in unseen modality, CT scans, as the model performed 19.46% better than the baseline model. The model also surpassed the domain expert's accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the tes",
    "path": "papers/23/12/2312.13103.json",
    "total_tokens": 978,
    "translated_title": "探索用于放射学报告错误检查的多模态大型语言模型",
    "translated_abstract": "本文提出了多模态大型语言模型（LLMs）在临床应用中作为放射科医生检查报告错误的助手之一。我们从真实世界的放射学数据集（包括X光和CT扫描）中创建了一个评估数据集。一部分原始报告被修改，以包含通过引入三种错误类型进行的合成错误：\"插入\"，\"删除\"和\"替换\"。评估包含两个难度级别：简单级别用于二进制错误检查，复杂级别用于识别错误类型。在简单级别上，我们的微调模型在MIMIC-CXR和IU X光数据上分别提高了47.4%和25.4%的性能。这种性能提升在未见过的模态CT扫描中也得到了观察，因为该模型的性能比基线模型提高了19.46%。模型在MIMIC-CXR数据集中的准确性也比领域专家高出1.67%。值得注意的是，在测试集的子集（N=21）中。",
    "tldr": "本文提出了多模态大型语言模型作为放射科医生检查报告错误的助手，通过引入合成错误并对不同难度级别进行评估，发现该模型在简单级别上在X光和CT扫描数据集上显著提高了性能，并且在MIMIC-CXR数据集上超过了领域专家的准确性。",
    "en_tdlr": "This paper proposes the use of multimodal large language models as an assistant for radiologists to check errors in their reports, showing significant performance improvements on X-ray and CT scan datasets at a simple difficulty level, surpassing domain expert accuracy on the MIMIC-CXR dataset."
}