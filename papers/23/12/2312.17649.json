{
    "title": "Investigating the Effects of Sparse Attention on Cross-Encoders",
    "abstract": "arXiv:2312.17649v2 Announce Type: replace  Abstract: Cross-encoders are effective passage and document re-rankers but less efficient than other neural or classic retrieval models. A few previous studies have applied windowed self-attention to make cross-encoders more efficient. However, these studies did not investigate the potential and limits of different attention patterns or window sizes. We close this gap and systematically analyze how token interactions can be reduced without harming the re-ranking effectiveness. Experimenting with asymmetric attention and different window sizes, we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking and that very small window sizes suffice. In our experiments, even windows of 4 tokens still yield effectiveness on par with previous cross-encoders while reducing the memory requirements by at least 22% / 59% and being 1% / 43% faster at inference time for passages / documents.",
    "link": "https://arxiv.org/abs/2312.17649",
    "context": "Title: Investigating the Effects of Sparse Attention on Cross-Encoders\nAbstract: arXiv:2312.17649v2 Announce Type: replace  Abstract: Cross-encoders are effective passage and document re-rankers but less efficient than other neural or classic retrieval models. A few previous studies have applied windowed self-attention to make cross-encoders more efficient. However, these studies did not investigate the potential and limits of different attention patterns or window sizes. We close this gap and systematically analyze how token interactions can be reduced without harming the re-ranking effectiveness. Experimenting with asymmetric attention and different window sizes, we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking and that very small window sizes suffice. In our experiments, even windows of 4 tokens still yield effectiveness on par with previous cross-encoders while reducing the memory requirements by at least 22% / 59% and being 1% / 43% faster at inference time for passages / documents.",
    "path": "papers/23/12/2312.17649.json",
    "total_tokens": 842,
    "translated_title": "研究稀疏注意力对交叉编码器的影响",
    "translated_abstract": "交叉编码器是有效的段落和文档重新排序器，但效率不如其他神经或经典检索模型。一些先前的研究应用窗口自注意力来使交叉编码器更有效率。然而，这些研究并未探讨不同注意力模式或窗口大小的潜力和限制。我们填补了这一空白，并系统地分析了如何减少标记交互而不损害重新排序的有效性。通过实验使用非对称注意力和不同的窗口大小，我们发现查询标记不需要关注段落或文档标记也能实现有效的重新排序，而非常小的窗口大小就足够了。在我们的实验中，即使是4个标记的窗口仍然能够达到与以前的交叉编码器相当的有效性，同时将内存要求降低至少22% / 59%，并在 passages / documents 的推理时间上快1% / 43%。",
    "tldr": "窗口大小非常小甚至只有4个标记时，仍可保持与以往交叉编码器相当的效果，同时降低内存需求并提高推理速度。",
    "en_tdlr": "Effective re-ranking can be achieved even with very small window sizes such as 4 tokens, maintaining the effectiveness on par with previous cross-encoders while reducing memory requirements and improving inference speeds."
}