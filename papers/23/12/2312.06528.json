{
    "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context",
    "abstract": "arXiv:2312.06528v4 Announce Type: replace  Abstract: Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.",
    "link": "https://arxiv.org/abs/2312.06528",
    "context": "Title: Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context\nAbstract: arXiv:2312.06528v4 Announce Type: replace  Abstract: Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.",
    "path": "papers/23/12/2312.06528.json",
    "total_tokens": 746,
    "translated_title": "Transformers实现了功能性梯度下降来学习上下文中的非线性函数",
    "translated_abstract": "许多神经网络架构被认为是图灵完备的，因此原则上可以实现任意算法。然而，Transformers独特之处在于它们可以在简单的参数配置下实现基于梯度的学习算法。本文提供了理论和实证证据，证明（非线性）Transformers自然地学会在函数空间中实现梯度下降，从而使它们能够在上下文中学习非线性函数。我们的结果适用于一大类非线性架构和非线性上下文学习任务的组合。此外，我们展示了非线性激活函数的最优选择以一种自然的方式取决于需要学习的函数类别。",
    "tldr": "本论文证明了Transformers可以在函数空间中自然地学习实现梯度下降，从而学习上下文中的非线性函数，并且展示了最优的非线性激活函数选择取决于需要学习的函数类别。",
    "en_tdlr": "This paper demonstrates that Transformers can naturally learn to implement gradient descent in function space to learn non-linear functions in context, and shows that the optimal choice of non-linear activation depends on the class of functions that need to be learned."
}