{
    "title": "Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)",
    "abstract": "Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and re-used many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate ",
    "link": "http://arxiv.org/abs/2312.00157",
    "context": "Title: Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)\nAbstract: Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and re-used many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate ",
    "path": "papers/23/12/2312.00157.json",
    "total_tokens": 813,
    "translated_title": "通用后门攻击",
    "translated_abstract": "通过数据污染，网络抓取的数据集容易受到后门攻击，在训练过程中可以用于篡改深度图像分类器。由于在大型数据集上进行训练是昂贵的，因此模型只需要训练一次，然后多次重复使用。与对抗性样本不同，后门攻击通常针对特定的类别，而不是模型学习到的任何类别。我们展示了这并不一定是真实的情况，并且存在更有效的通用数据污染攻击，允许通过增加少量的污染样本来控制从任何源类别到任何目标类别的错误分类。我们的想法是生成具有显著特征的触发器，使模型可以学习。我们制作的触发器利用了我们称之为跨类别污染的现象，即学习一个类别的触发器使得模型更容易学习其他类别的触发器。",
    "tldr": "通过通用数据污染攻击，可以控制深度图像分类器对任何源类别到任何目标类别的错误分类，而只需增加少量的污染样本。",
    "en_tdlr": "Universal data poisoning attacks can control deep image classifiers to misclassify any source class into any target class with a small increase in poison samples."
}