{
    "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
    "abstract": "arXiv:2312.00849v2 Announce Type: replace  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation ",
    "link": "https://arxiv.org/abs/2312.00849",
    "context": "Title: RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback\nAbstract: arXiv:2312.00849v2 Announce Type: replace  Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation ",
    "path": "papers/23/12/2312.00849.json",
    "total_tokens": 868,
    "translated_title": "RLHF-V: 通过细粒度纠正人类反馈实现可信任的MLLMs行为对准",
    "translated_abstract": "最近，多模态大语言模型（MLLMs）展示了在多模态理解、推理和互动方面的令人印象深刻的能力。然而，现有的MLLMs普遍存在严重的幻觉问题，生成的文本与相关图像不符合事实。这个问题使得现有的MLLMs不可靠，因此在现实世界（尤其是高风险）应用中不切实际。为了解决这一挑战，我们提出了RLHF-V，通过从细粒度纠正的人类反馈中增强MLLM的可信度。具体而言，RLHF-V采集人类偏好，以片段级别对幻觉进行纠正，并在人类反馈上执行密集直接偏好优化。在自动和人类评估的五个基准测试上进行的综合实验表明，RLHF-V能够实现更可信赖的MLLM行为，具有有希望的数据和计算。",
    "tldr": "RLHF-V通过细粒度纠正人类反馈的行为对准，提高了MLLM的可信度，使其在多模态理解、推理和互动方面表现出更可靠的行为。",
    "en_tdlr": "RLHF-V enhances the trustworthiness of MLLMs through behavior alignment from fine-grained correctional human feedback, enabling them to exhibit more reliable behaviors in multimodal understanding, reasoning, and interaction."
}