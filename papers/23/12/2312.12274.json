{
    "title": "Intrinsic Image Diffusion for Indoor Single-view Material Estimation",
    "abstract": "arXiv:2312.12274v2 Announce Type: replace-cross  Abstract: We present Intrinsic Image Diffusion, a generative model for appearance decomposition of indoor scenes. Given a single input view, we sample multiple possible material explanations represented as albedo, roughness, and metallic maps. Appearance decomposition poses a considerable challenge in computer vision due to the inherent ambiguity between lighting and material properties and the lack of real datasets. To address this issue, we advocate for a probabilistic formulation, where instead of attempting to directly predict the true material properties, we employ a conditional generative model to sample from the solution space. Furthermore, we show that utilizing the strong learned prior of recent diffusion models trained on large-scale real-world images can be adapted to material estimation and highly improves the generalization to real images. Our method produces significantly sharper, more consistent, and more detailed material",
    "link": "https://arxiv.org/abs/2312.12274",
    "context": "Title: Intrinsic Image Diffusion for Indoor Single-view Material Estimation\nAbstract: arXiv:2312.12274v2 Announce Type: replace-cross  Abstract: We present Intrinsic Image Diffusion, a generative model for appearance decomposition of indoor scenes. Given a single input view, we sample multiple possible material explanations represented as albedo, roughness, and metallic maps. Appearance decomposition poses a considerable challenge in computer vision due to the inherent ambiguity between lighting and material properties and the lack of real datasets. To address this issue, we advocate for a probabilistic formulation, where instead of attempting to directly predict the true material properties, we employ a conditional generative model to sample from the solution space. Furthermore, we show that utilizing the strong learned prior of recent diffusion models trained on large-scale real-world images can be adapted to material estimation and highly improves the generalization to real images. Our method produces significantly sharper, more consistent, and more detailed material",
    "path": "papers/23/12/2312.12274.json",
    "total_tokens": 853,
    "translated_title": "室内单视图材料估计的内在图像扩散",
    "translated_abstract": "我们提出了内在图像扩散，这是一个用于室内场景外观分解的生成模型。给定单个输入视图，我们采样多种可能的材料解释，表示为反照率、粗糙度和金属度图。外观分解在计算机视觉中面临着重大挑战，因为光照和材料属性之间固有的模糊性以及缺乏真实数据集。为了解决这个问题，我们倡导概率形式，不是直接预测真实的材料属性，而是使用条件生成模型从解空间中进行采样。此外，我们表明利用最近扩散模型在大规模现实世界图像上训练的强大学习先验可以被调整为材料估计，并极大地改善对真实图像的泛化。我们的方法产生了明显更清晰、更一致和更详细的材料。",
    "tldr": "提出了内在图像扩散模型，应用于室内单视图材料估计，通过概率形式的条件生成模型采样解空间来解决外观分解中光照和材料属性之间的固有模糊性挑战。",
    "en_tdlr": "Introduced the Intrinsic Image Diffusion model for estimating indoor single-view materials, addressing the intrinsic ambiguity between lighting and material properties in appearance decomposition by sampling from the solution space using a probabilistic conditional generative model."
}