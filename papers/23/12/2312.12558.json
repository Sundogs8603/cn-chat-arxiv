{
    "title": "Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge",
    "abstract": "arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with",
    "link": "https://arxiv.org/abs/2312.12558",
    "context": "Title: Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge\nAbstract: arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with",
    "path": "papers/23/12/2312.12558.json",
    "total_tokens": 758,
    "translated_title": "具有部分动态知识的样本高效强化学习",
    "translated_abstract": "在本文中，我们研究了在线Q学习方法的样本复杂度，当某些关于动态的先前知识可用或可以有效学习时。我们专注于按照加性干扰模型演变的系统，在有限的分集马尔可夫决策过程设置下，我们提出了一种乐观的Q学习算法，在对$f$的完美知识条件下实现了$\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$的遗憾，其中$T$是与系统进行交互的总次数。",
    "tldr": "研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。",
    "en_tdlr": "Investigated the sample complexity of online Q-learning with partial dynamics knowledge and proposed an optimistic Q-learning algorithm that achieves lower regret in finite episodic Markov decision processes."
}