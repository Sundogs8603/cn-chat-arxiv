{
    "title": "MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)",
    "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering",
    "link": "http://arxiv.org/abs/2312.17080",
    "context": "Title: MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)\nAbstract: In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering",
    "path": "papers/23/12/2312.17080.json",
    "total_tokens": 922,
    "translated_title": "MR-GSM8K: 大型语言模型评估中的元推理革命",
    "translated_abstract": "在这项工作中，我们引入了一种新颖的评估范式，用于大型语言模型，这种范式挑战它们从事元推理。这种方法解决了现有的数学问题求解基准中的关键缺陷，传统上用于评估智能体的认知能力。我们的范式将焦点从以结果为导向的评估转移到了更全面的评估，有效地区分了模型之间的认知能力。例如，在我们的基准测试中，GPT-4 的性能较 GPT3-5 提升了五倍。这种新范式的重要意义在于它能够揭示出当前基准测试（如GSM8K）无法发现的大型语言模型的潜在认知缺陷，这是由于基准测试的饱和度和对不同推理能力的有效区分不足。我们的综合分析包括了来自开源和闭源社区的几种最先进的数学模型，揭示了一些关于大型语言模型的认知能力的重要发现。",
    "tldr": "本文介绍了一种新的大型语言模型评估范式，通过挑战这些模型进行元推理，从而有效区分它们的认知能力。这一范式的重要性在于能够揭示出传统基准测试无法发现的模型的潜在认知缺陷。",
    "en_tdlr": "This paper introduces a novel evaluation paradigm for Large Language Models, challenging them to engage in meta-reasoning and effectively differentiate their cognitive capabilities. Its significance lies in revealing potential cognitive deficiencies in models that traditional benchmarks fail to uncover."
}