{
    "title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction",
    "abstract": "arXiv:2312.12021v3 Announce Type: replace-cross  Abstract: Few-shot Relation Extraction (FSRE) aims to extract relational facts from a sparse set of labeled corpora. Recent studies have shown promising results in FSRE by employing Pre-trained Language Models (PLMs) within the framework of supervised contrastive learning, which considers both instances and label facts. However, how to effectively harness massive instance-label pairs to encompass the learned representation with semantic richness in this learning paradigm is not fully explored. To address this gap, we introduce a novel synergistic anchored contrastive pre-training framework. This framework is motivated by the insight that the diverse viewpoints conveyed through instance-label pairs capture incomplete yet complementary intrinsic textual semantics. Specifically, our framework involves a symmetrical contrastive objective that encompasses both sentence-anchored and label-anchored contrastive losses. By combining these two los",
    "link": "https://arxiv.org/abs/2312.12021",
    "context": "Title: Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction\nAbstract: arXiv:2312.12021v3 Announce Type: replace-cross  Abstract: Few-shot Relation Extraction (FSRE) aims to extract relational facts from a sparse set of labeled corpora. Recent studies have shown promising results in FSRE by employing Pre-trained Language Models (PLMs) within the framework of supervised contrastive learning, which considers both instances and label facts. However, how to effectively harness massive instance-label pairs to encompass the learned representation with semantic richness in this learning paradigm is not fully explored. To address this gap, we introduce a novel synergistic anchored contrastive pre-training framework. This framework is motivated by the insight that the diverse viewpoints conveyed through instance-label pairs capture incomplete yet complementary intrinsic textual semantics. Specifically, our framework involves a symmetrical contrastive objective that encompasses both sentence-anchored and label-anchored contrastive losses. By combining these two los",
    "path": "papers/23/12/2312.12021.json",
    "total_tokens": 806,
    "translated_title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction",
    "translated_abstract": "少样本关系抽取（FSRE）旨在从稀疏标记语料库中提取关系事实。最近的研究通过在监督对比学习框架中利用预训练语言模型（PLMs）展现了有希望的FSRE结果，该框架考虑了实例和标签事实。然而，在这种学习范式中如何有效利用大量的实例-标签对来使学习到的表示具有语义丰富性尚未得到充分探讨。为填补这一空白，我们提出了一种新颖的协同对比预训练框架。这个框架的动机是，通过实例-标签对传达的多样观点捕捉到了不完整但互补的文本语义。具体而言，我们的框架涉及一种对称对比目标，包含了句子锚定和标签锚定的对比损失。通过组合这两种损失",
    "tldr": "新提出了一种协同对比预训练框架，利用大量的实例-标签对来丰富学习到的表示",
    "en_tdlr": "A novel synergistic anchored contrastive pre-training framework is proposed, utilizing massive instance-label pairs to enrich the learned representation."
}