{
    "title": "Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools",
    "abstract": "Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our",
    "link": "https://arxiv.org/abs/2312.10622",
    "context": "Title: Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools\nAbstract: Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our",
    "path": "papers/23/12/2312.10622.json",
    "total_tokens": 982,
    "translated_title": "使用生成式人工智能生成单元测试：自动生成工具的性能比较分析",
    "translated_abstract": "生成单元测试是软件开发中一个关键的任务，程序员需要花费大量时间和精力。大型语言模型（LLM）的出现为单元测试脚本的生成提供了一种新的方式。本研究旨在实验性地研究LLM的有效性，特别是以ChatGPT为代表，用于生成Python程序的单元测试脚本，并将生成的测试用例与现有的单元测试生成器（Pynguin）生成的进行比较。在实验中，我们考虑了三种类型的代码单元：1）过程脚本，2）基于函数的模块化代码，3）基于类的代码。生成的测试用例基于覆盖率、正确性和可读性等标准进行评估。我们的研究结果表明，ChatGPT在覆盖率方面与Pynguin相当，但在某些情况下其性能优于Pynguin。我们还发现，对于某些类别，ChatGPT生成的断言约有三分之一是不正确的。",
    "tldr": "本研究通过比较分析，实验性地研究了使用生成式人工智能（如ChatGPT）生成Python程序的单元测试脚本的有效性。结果显示，ChatGPT在覆盖率方面与现有的单元测试生成器Pynguin相当，在某些情况下性能优于Pynguin。然而，对于某些类别，ChatGPT生成的断言约有三分之一是不正确的。",
    "en_tdlr": "This research experimentally investigates the effectiveness of using generative AI, specifically ChatGPT, for generating unit test scripts for Python programs, and compares its performance with an existing unit test generator Pynguin. The results show that ChatGPT performs similarly to Pynguin in terms of coverage and even outperforms Pynguin in some cases. However, a third of the assertions generated by ChatGPT for certain categories were found to be incorrect."
}