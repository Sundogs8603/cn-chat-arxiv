{
    "title": "Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)",
    "abstract": "Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the ",
    "link": "http://arxiv.org/abs/2312.16243",
    "context": "Title: Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)\nAbstract: Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the ",
    "path": "papers/23/12/2312.16243.json",
    "total_tokens": 852,
    "translated_title": "是否所有未见数据都是OoD数据？",
    "translated_abstract": "处理未见数据的分布一直被视为是OoD（out-of-distribution），使其泛化成为一个重要挑战。很多证据表明，训练数据的规模增加可以单调地降低测试数据的泛化误差。然而，从其他观察和分析来看，这并不总是成立。特别是当训练数据包含多个来源领域，并且测试数据包含分布漂移时，不是所有的测试数据的泛化误差都会随着训练数据规模的增加而单调减小。在线性设置下，我们正式研究了这种非单调现象，并通过在不同视觉基准上进行经验证实。鉴于这些结果，我们重新定义了OoD数据，将其视为训练域的凸包之外的数据，并基于这个新定义证明了一个新的泛化上界。它意味着对于在训练阶段没有见过的数据，经过充分训练的模型的有效性可以得到保证。",
    "tldr": "该论文研究了未见数据的分布在泛化中的挑战，并提出了重新定义OoD数据以及新的泛化上界，保证了对未见数据的模型有效性。"
}