{
    "title": "Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification",
    "abstract": "Training large foundation models using self-supervised objectives on unlabeled data, followed by fine-tuning on downstream tasks, has emerged as a standard procedure. Unfortunately, the efficacy of this approach is often constrained by both limited fine-tuning compute and scarcity in labeled downstream data. We introduce Multimodal Attention Merging (MAM), an attempt that facilitates direct knowledge transfer from attention matrices of models rooted in high resource modalities, text and images, to those in resource-constrained domains, speech and audio, employing a zero-shot paradigm. MAM reduces the relative Word Error Rate (WER) of an Automatic Speech Recognition (ASR) model by up to 6.70%, and relative classification error of an Audio Event Classification (AEC) model by 10.63%. In cases where some data/compute is available, we present Learnable-MAM, a data-driven approach to merging attention matrices, resulting in a further 2.90% relative reduction in WER for ASR and 18.42% relativ",
    "link": "https://arxiv.org/abs/2312.14378",
    "context": "Title: Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification\nAbstract: Training large foundation models using self-supervised objectives on unlabeled data, followed by fine-tuning on downstream tasks, has emerged as a standard procedure. Unfortunately, the efficacy of this approach is often constrained by both limited fine-tuning compute and scarcity in labeled downstream data. We introduce Multimodal Attention Merging (MAM), an attempt that facilitates direct knowledge transfer from attention matrices of models rooted in high resource modalities, text and images, to those in resource-constrained domains, speech and audio, employing a zero-shot paradigm. MAM reduces the relative Word Error Rate (WER) of an Automatic Speech Recognition (ASR) model by up to 6.70%, and relative classification error of an Audio Event Classification (AEC) model by 10.63%. In cases where some data/compute is available, we present Learnable-MAM, a data-driven approach to merging attention matrices, resulting in a further 2.90% relative reduction in WER for ASR and 18.42% relativ",
    "path": "papers/23/12/2312.14378.json",
    "total_tokens": 941,
    "translated_title": "多模态注意力合并用于提高语音识别和音频事件分类",
    "translated_abstract": "使用自我监督目标在无标签数据上训练大型基础模型，然后在下游任务中进行微调已成为一种标准的流程。然而，这种方法的有效性经常受限于有限的微调计算资源和标记下游数据的稀缺性。我们引入了多模态注意力合并（MAM），试图通过零-shot范式将高资源模态（文本和图像）中模型注意力矩阵的直接知识传输到资源受限领域（语音和音频）中。MAM将自动语音识别（ASR）模型的相对词错误率（WER）降低了最多6.70％，将音频事件分类（AEC）模型的相对分类错误降低了10.63％。在一些数据/计算可用的情况下，我们提出了可学习的MAM，一种基于数据的方法来合并注意力矩阵，使ASR的WER相对降低了进一步的2.90％，而AEC的相对降低了18.42％。",
    "tldr": "多模态注意力合并（MAM）使用零-shot范式将文本和图像中的模型注意力矩阵的直接知识传输到音频领域中，可以显著降低自动语音识别的词错误率和音频事件分类的分类错误。",
    "en_tdlr": "Multimodal Attention Merging (MAM) transfers direct knowledge from attention matrices of models rooted in high resource modalities, text and images, to resource-constrained domains, speech and audio, using a zero-shot paradigm. MAM significantly reduces the Word Error Rate (WER) of Automatic Speech Recognition (ASR) and the classification error of Audio Event Classification (AEC)."
}