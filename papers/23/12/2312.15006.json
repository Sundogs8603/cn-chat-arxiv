{
    "title": "Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities",
    "abstract": "arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece",
    "link": "https://arxiv.org/abs/2312.15006",
    "context": "Title: Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities\nAbstract: arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece",
    "path": "papers/23/12/2312.15006.json",
    "total_tokens": 823,
    "translated_title": "评估提示方法对ChatGPT的数学能力的影响",
    "translated_abstract": "本研究批判性地评估了提示方法在提升大型语言模型（LLMs）的数学推理能力方面的功效。该研究使用了三种规定性提示方法 - 简单提示、个人提示和对话提示 - 这些方法以提升LLMs语言任务效果而闻名。我们在OpenAI的LLM闲聊机器人ChatGPT-3.5上进行此分析，涵盖了来自MATH、GSM8K和MMLU数据集的广泛问题集合，这些问题涵盖了各种数学挑战。针对每个数据集调整的评分脚本用于确定这些提示干预在增强模型数学分析能力方面的效果。与预期相反，我们的实证分析显示，所检验的方法均未在持续改进ChatGPT-3.5基准表现上，部分方法甚至导致明显的退化。我们的发现表明，提示策略未必能提高模型的数学分析能力。",
    "tldr": "三种提示方法对ChatGPT的数学能力并未产生一贯性改进效果，部分方法甚至导致性能下降",
    "en_tdlr": "The three prompting methods did not consistently improve ChatGPT's mathematical capabilities, with some even causing performance degradation."
}