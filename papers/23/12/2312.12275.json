{
    "title": "Emergence of In-Context Reinforcement Learning from Noise Distillation",
    "abstract": "Recently, extensive studies in Reinforcement Learning have been carried out on the ability of transformers to adapt in-context to various environments and tasks. Current in-context RL methods are limited by their strict requirements for data, which needs to be generated by RL agents or labeled with actions from an optimal policy. In order to address this prevalent problem, we propose AD$^\\varepsilon$, a new data acquisition approach that enables in-context Reinforcement Learning from noise-induced curriculum. We show that it is viable to construct a synthetic noise injection curriculum which helps to obtain learning histories. Moreover, we experimentally demonstrate that it is possible to alleviate the need for generation using optimal policies, with in-context RL still able to outperform the best suboptimal policy in a learning dataset by a 2x margin.",
    "link": "https://arxiv.org/abs/2312.12275",
    "context": "Title: Emergence of In-Context Reinforcement Learning from Noise Distillation\nAbstract: Recently, extensive studies in Reinforcement Learning have been carried out on the ability of transformers to adapt in-context to various environments and tasks. Current in-context RL methods are limited by their strict requirements for data, which needs to be generated by RL agents or labeled with actions from an optimal policy. In order to address this prevalent problem, we propose AD$^\\varepsilon$, a new data acquisition approach that enables in-context Reinforcement Learning from noise-induced curriculum. We show that it is viable to construct a synthetic noise injection curriculum which helps to obtain learning histories. Moreover, we experimentally demonstrate that it is possible to alleviate the need for generation using optimal policies, with in-context RL still able to outperform the best suboptimal policy in a learning dataset by a 2x margin.",
    "path": "papers/23/12/2312.12275.json",
    "total_tokens": 794,
    "translated_title": "从噪声蒸馏中出现的上下文强化学习",
    "translated_abstract": "最近在强化学习领域中，我们进行了大量关于变形金刚能够适应各种环境和任务的能力的研究。目前的上下文强化学习方法受到数据要求的限制，需要由强化学习代理生成或通过最优策略标记的数据。为了解决这个普遍存在的问题，我们提出了AD$^\\varepsilon$，一种新的数据获取方法，可以通过噪声诱导的课程来实现上下文强化学习。我们展示了构建一个帮助获取学习历史的合成噪声注入课程是可行的。此外，我们通过实验证明，即使无需使用最优策略生成，上下文强化学习仍然能够以2倍的边界优于学习数据集中的最优策略。",
    "tldr": "该论文介绍了一种从噪声中生成上下文强化学习的方法，通过构建噪声注入课程来获取学习历史，可以实现在学习数据集中超过最优策略的性能表现。",
    "en_tdlr": "This paper presents a method of generating in-context reinforcement learning from noise, by constructing a noise injection curriculum to obtain learning histories, which achieves better performance than the optimal policy in the learning dataset."
}