{
    "title": "Learning Human-like Representations to Enable Learning Human Values",
    "abstract": "arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds",
    "link": "https://arxiv.org/abs/2312.14106",
    "context": "Title: Learning Human-like Representations to Enable Learning Human Values\nAbstract: arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds",
    "path": "papers/23/12/2312.14106.json",
    "total_tokens": 819,
    "translated_title": "学习类人表示以实现学习类人价值观",
    "translated_abstract": "如何构建与人类价值观相一致的人工智能系统，以避免造成伤害或违反社会对可接受行为的标准？我们认为，人类与人工智能代理之间的表征对齐有助于价值观的对齐。使人工智能系统学习类人类对世界的表示具有许多已知好处，包括提高泛化能力、增强对领域转移的稳健性和提高少样本学习性能。我们提出，这种机器学习（ML）模型与人类之间的表示对齐也可以支持价值对齐，使ML系统遵循人类价值观和社会规范。我们关注伦理学作为价值对齐的一个方面，并在多臂老虎机设置中使用各种方法训练ML代理，其中奖励反映所选行动的道德可接受性。我们使用一个合成实验来证明代理与环境之间的表示对齐",
    "tldr": "通过学习类人的表示，可以实现机器学习系统符合人类价值观，支持伦理等多方面的价值对齐。",
    "en_tdlr": "Learning human-like representations enables machine learning systems to align with human values, supporting value alignment in ethics and other aspects."
}