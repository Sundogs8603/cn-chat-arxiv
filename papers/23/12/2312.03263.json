{
    "title": "Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)",
    "abstract": "Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environ",
    "link": "http://arxiv.org/abs/2312.03263",
    "context": "Title: Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)\nAbstract: Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environ",
    "path": "papers/23/12/2312.03263.json",
    "total_tokens": 1011,
    "translated_title": "气候不确定性中的学习和规划：在时变部分可观测环境中的应用",
    "translated_abstract": "在不确定、随机和时变环境中，最优决策对于自主系统来说是一个重大挑战。随着时间的推移，环境的变化可以对系统的最优决策策略产生显著影响。为了对这样的环境进行建模，我们的工作将之前的时变马尔科夫决策过程(time-varying Markov Decision Processes, TVMDP)的概念与部分可观测性相结合，引入了时变部分可观测马尔科夫决策过程(time-varying Partially Observable Markov Decision Processes, TV-POMDP)。我们提出了一个双管齐下的方法来在TV-POMDP中准确估计和规划：1）记忆优先状态估计(Memory Prioritized State Estimation, MPSE)，利用加权记忆提供更准确的时变转移估计；2）MPSE集成的规划策略，优化长期奖励的同时考虑时间约束。我们使用仿真和硬件验证了所提出的框架和算法，机器人在一个部分可观测、时变的环境中进行探索。",
    "tldr": "本研究结合了时变马尔科夫决策过程和部分可观测性，提出了在不确定、随机和时变环境中进行学习和规划的方法。通过记忆优先状态估计和规划策略的集成，我们实现了对长期奖励的优化，在仿真和硬件验证中取得了良好的结果。",
    "en_tdlr": "This research combines time-varying Markov Decision Processes with partial observability to propose a method for learning and planning in uncertain, stochastic, and time-varying environments. By utilizing memory prioritized state estimation and integrated planning strategy, optimal long-term rewards are achieved, validated through simulations and hardware experiments."
}