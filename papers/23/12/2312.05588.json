{
    "title": "Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding and Fixing Bugs",
    "abstract": "arXiv:2312.05588v2 Announce Type: replace  Abstract: Vision models with high overall accuracy often exhibit systematic errors in specific scenarios, posing potential serious safety concerns. Diagnosing bugs of vision models is gaining increased attention, however traditional diagnostic approaches require annotation efforts (eg rich metadata accompanying each samples of CelebA). To address this issue,We propose a language-assisted diagnostic method that uses texts instead of images to diagnose bugs in vision models based on multi-modal models (eg CLIP). Our approach connects the embedding space of CLIP with the buggy vision model to be diagnosed; meanwhile, utilizing a shared classifier and the cross-modal transferability of embedding space from CLIP, the text-branch of CLIP become a proxy model to find bugs in the buggy model. The proxy model can classify texts paired with images. During the diagnosis, a Large Language Model (LLM) is employed to obtain task-relevant corpora, and this c",
    "link": "https://arxiv.org/abs/2312.05588",
    "context": "Title: Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding and Fixing Bugs\nAbstract: arXiv:2312.05588v2 Announce Type: replace  Abstract: Vision models with high overall accuracy often exhibit systematic errors in specific scenarios, posing potential serious safety concerns. Diagnosing bugs of vision models is gaining increased attention, however traditional diagnostic approaches require annotation efforts (eg rich metadata accompanying each samples of CelebA). To address this issue,We propose a language-assisted diagnostic method that uses texts instead of images to diagnose bugs in vision models based on multi-modal models (eg CLIP). Our approach connects the embedding space of CLIP with the buggy vision model to be diagnosed; meanwhile, utilizing a shared classifier and the cross-modal transferability of embedding space from CLIP, the text-branch of CLIP become a proxy model to find bugs in the buggy model. The proxy model can classify texts paired with images. During the diagnosis, a Large Language Model (LLM) is employed to obtain task-relevant corpora, and this c",
    "path": "papers/23/12/2312.05588.json",
    "total_tokens": 917,
    "translated_title": "语言辅助视觉模型调试器：一种无需样本的发现和修复错误的方法",
    "translated_abstract": "具有高整体准确性的视觉模型经常在特定情景中表现出系统性错误，可能带来严重的安全隐患。诊断视觉模型的错误正变得越来越受到关注，然而传统的诊断方法需要注释工作（例如伴随每个CelebA样本的丰富元数据）。为了解决这个问题，我们提出了一种语言辅助诊断方法，其使用文本而不是图像来诊断基于多模型（例如CLIP）的视觉模型中的错误。我们的方法将CLIP的嵌入空间与待诊断的出错视觉模型连接起来；同时，利用一个共享分类器和从CLIP的嵌入空间到跨模态转移的可能性，CLIP的文本分支成为一个代理模型，用于在出错模型中找出错误。代理模型可以对配对的文本和图像进行分类。在诊断过程中，利用一个大型语言模型（LLM）来获得与任务相关的文集，这c",
    "tldr": "提出了一种语言辅助视觉模型调试方法，利用文本而不是图像来诊断视觉模型中的错误，通过连接CLIP的嵌入空间和出错视觉模型，以及利用CLIP的文本分支作为代理模型来发现错误。",
    "en_tdlr": "Introducing a language-assisted diagnostic approach for vision models, using texts instead of images to diagnose errors. By connecting the embedding space of CLIP with the faulty vision model, and utilizing the text branch of CLIP as a proxy model to identify errors."
}