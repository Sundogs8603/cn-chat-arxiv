{
    "title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering",
    "abstract": "arXiv:2312.15194v2 Announce Type: replace  Abstract: Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine's comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propos",
    "link": "https://arxiv.org/abs/2312.15194",
    "context": "Title: PokeMQA: Programmable knowledge editing for Multi-hop Question Answering\nAbstract: arXiv:2312.15194v2 Announce Type: replace  Abstract: Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine's comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propos",
    "path": "papers/23/12/2312.15194.json",
    "total_tokens": 825,
    "translated_title": "PokeMQA: 可编程的多跳问答知识编辑",
    "translated_abstract": "多跳问答（MQA）是评估机器理解和推理能力的挑战性任务之一，大型语言模型（LLMs）已广泛实现了与人类相媲美的性能。由于现实世界中知识事实的动态性，已经探索了知识编辑来更新模型，以获取最新的事实，同时避免昂贵的重新训练或微调。从编辑后的事实开始，更新后的模型需要在MQA链中提供级联变化。以往的方法简单地采用了混合提示来指导LLMs进行多个推理任务的顺序执行，包括问题分解、答案生成和通过与编辑的事实进行比较的冲突检查。然而，这些功能多样的推理任务的耦合抑制了LLMs在理解和回答问题方面的优势，同时干扰了它们在冲突检查的不熟练任务上。",
    "tldr": "PokeMQA是一个可编程的多跳问答知识编辑模型，通过避免功能多样的推理任务的耦合，实现了更好的问题理解和回答能力。"
}