{
    "title": "Doubly Perturbed Task Free Continual Learning",
    "abstract": "arXiv:2312.13027v2 Announce Type: replace  Abstract: Task Free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method th",
    "link": "https://arxiv.org/abs/2312.13027",
    "context": "Title: Doubly Perturbed Task Free Continual Learning\nAbstract: arXiv:2312.13027v2 Announce Type: replace  Abstract: Task Free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method th",
    "path": "papers/23/12/2312.13027.json",
    "total_tokens": 806,
    "translated_title": "双重扰动任务自由的持续学习",
    "translated_abstract": "Task Free online continual learning (TF-CL)是一个具有挑战性的问题，模型在没有显式任务信息的情况下逐步学习任务。尽管使用来自过去、现在以及未来的所有数据进行训练被认为是黄金标准，但在TF-CL中使用当前样本的幼稚方法可能会与未来样本的学习发生冲突，导致灾难性遗忘和可塑性不佳。因此，在TF-CL中积极考虑未来样本变得至关重要。受到这种直觉的启发，我们提出了一个考虑未来样本的新颖TF-CL框架，并展示在输入数据和决策制定上注入敌对扰动是有效的。然后，我们提出了一种名为Doubly Perturbed Continual Learning (DPCL)的新方法，以有效地实施这些输入和决策扰动。",
    "tldr": "提出了一种新颖的任务自由持续学习框架，在输入数据和决策制定上注入敌对扰动，提高了未来样本考虑的效果。",
    "en_tdlr": "Proposed a novel framework for task-free continual learning, injecting adversarial perturbations on both input data and decision-making to enhance consideration of future samples."
}