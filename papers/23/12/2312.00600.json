{
    "title": "Improving Plasticity in Online Continual Learning via Collaborative Learning",
    "abstract": "arXiv:2312.00600v2 Announce Type: replace  Abstract: Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a collaborative learning scheme to boost the training of the models. ",
    "link": "https://arxiv.org/abs/2312.00600",
    "context": "Title: Improving Plasticity in Online Continual Learning via Collaborative Learning\nAbstract: arXiv:2312.00600v2 Announce Type: replace  Abstract: Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a collaborative learning scheme to boost the training of the models. ",
    "path": "papers/23/12/2312.00600.json",
    "total_tokens": 716,
    "translated_title": "通过协作学习提高在线持续学习中的可塑性",
    "translated_abstract": "在线持续学习解决了从连续数据流中学习不断出现的新分类任务的问题。本文认为，在线持续学习中，模型获取新知识的能力（即模型的可塑性）是另一个挑战。我们提出了协作持续学习（CCL），一个基于协作学习的策略，以提高模型在获取新概念方面的能力。此外，我们引入了蒸馏链（DC），一种协作学习方案，以提升模型的训练。",
    "tldr": "在线持续学习中，为了提高模型的可塑性，我们提出了协作持续学习（CCL）和蒸馏链（DC）策略。",
    "en_tdlr": "In online continual learning, to improve model plasticity, we propose Collaborative Continual Learning (CCL) and Distillation Chain (DC) strategies."
}