{
    "title": "Vision-Language Models as a Source of Rewards",
    "abstract": "arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.",
    "link": "https://arxiv.org/abs/2312.09187",
    "context": "Title: Vision-Language Models as a Source of Rewards\nAbstract: arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.",
    "path": "papers/23/12/2312.09187.json",
    "total_tokens": 825,
    "translated_title": "视觉-语言模型作为奖励的来源",
    "translated_abstract": "建立可以在丰富多样的开放环境中实现许多目标的通用代理是强化学习的研究前沿之一。建立具有RL的通用代理的关键限制因素之一是需要大量的奖励函数来实现不同的目标。我们调查了使用现成的视觉-语言模型（VLM）作为强化学习代理的奖励来源的可行性。我们展示了如何从CLIP系列模型中派生视觉实现各种语言目标的奖励，并用于训练能够实现各种语言目标的RL代理。我们展示了这种方法在两个不同的视觉领域中，并呈现了一个规模化趋势，显示更大的VLM会产生更准确的视觉目标实现奖励，从而产生更有能力的RL代理。",
    "tldr": "使用现成的视觉-语言模型作为强化学习代理的奖励来源，展示了如何通过CLIP系列模型派生视觉目标实现的奖励，从而训练出能够实现多种语言目标的RL代理。",
    "en_tdlr": "The paper explores using off-the-shelf vision-language models as a source of rewards for reinforcement learning agents, demonstrating how rewards for visual achievement of various language goals can be derived from CLIP models to train RL agents capable of achieving multiple language goals."
}