{
    "title": "$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence",
    "abstract": "arXiv:2312.01133v2 Announce Type: replace-cross  Abstract: The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-",
    "link": "https://arxiv.org/abs/2312.01133",
    "context": "Title: $t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence\nAbstract: arXiv:2312.01133v2 Announce Type: replace-cross  Abstract: The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-",
    "path": "papers/23/12/2312.01133.json",
    "total_tokens": 880,
    "translated_title": "$t^3$-变分自动编码器：利用学生t分布和幂分歧学习重尾数据",
    "translated_abstract": "变分自动编码器（VAE）通常采用标准正态先验作为概率潜在编码器的正则化器。然而，高斯尾部往往衰减得太快，无法有效容纳编码点，无法保留数据中隐藏的关键结构。本文探讨了使用重尾模型来抵抗过度正则化的方法。借鉴信息几何的见解，我们提出了$t^3$VAE，一种修改后的VAE框架，它将学生t分布结合到先验、编码器和解码器中。这导致了一个幂形式的联合模型分布，我们认为这可以更好地拟合真实数据集。我们通过重新表达证据下界为两个统计流形之间KL散度的联合优化，将其替换为$\\gamma$-幂分歧，这是幂族的一个自然替代方法。$t^3$VAE展现出卓越的低",
    "tldr": "通过引入学生t分布和幂分歧，提出了$t^3$VAE变分自动编码器框架，以更好地处理重尾数据，并推导出新的优化目标。",
    "en_tdlr": "The paper introduces the $t^3$VAE variational autoencoder framework, incorporating Student's t-distributions and power divergence to better handle heavy-tailed data, and derives a new objective function."
}