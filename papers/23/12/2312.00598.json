{
    "title": "Learning from One Continuous Video Stream",
    "abstract": "arXiv:2312.00598v2 Announce Type: replace-cross  Abstract: We introduce a framework for online learning from a single continuous video stream -- the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little prior work on it. Our framework allows us to do a first deep dive into the topic and includes a collection of streams and tasks composed from two existing video datasets, plus methodology for performance evaluation that considers both adaptation and generalization. We employ pixel-to-pixel modelling as a practical and flexible way to switch between pre-training and single-stream evaluation as well as between arbitrary tasks, without ever requiring changes to models and always using the same pixel loss. Equipped with this framework we obtained large single-stream learning gains from pre-training with a novel family of future prediction tasks, foun",
    "link": "https://arxiv.org/abs/2312.00598",
    "context": "Title: Learning from One Continuous Video Stream\nAbstract: arXiv:2312.00598v2 Announce Type: replace-cross  Abstract: We introduce a framework for online learning from a single continuous video stream -- the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little prior work on it. Our framework allows us to do a first deep dive into the topic and includes a collection of streams and tasks composed from two existing video datasets, plus methodology for performance evaluation that considers both adaptation and generalization. We employ pixel-to-pixel modelling as a practical and flexible way to switch between pre-training and single-stream evaluation as well as between arbitrary tasks, without ever requiring changes to models and always using the same pixel loss. Equipped with this framework we obtained large single-stream learning gains from pre-training with a novel family of future prediction tasks, foun",
    "path": "papers/23/12/2312.00598.json",
    "total_tokens": 849,
    "translated_title": "从单个连续视频流中学习",
    "translated_abstract": "我们介绍了一个针对从单个连续视频流中进行在线学习的框架--就像人类和动物学习的方式一样，无需小批量、数据增强或洗牌。由于连续视频帧之间的高相关性，这带来了很大的挑战，并且在这方面几乎没有先前的工作。我们的框架使我们首次深入探讨了这一主题，包括了由两个现有视频数据集组成的一组流和任务，以及考虑了适应性和泛化性能评估的方法论。我们采用像素级建模作为一种实用且灵活的方式，可以在预训练和单个流评估之间以及在任意任务之间进行切换，而无需改变模型，并始终使用相同的像素损失。借助这个框架，我们通过预训练一组新颖的未来预测任务获得了大量单流学习的收益，foun",
    "tldr": "我们提出了一个在线学习框架，允许从单个连续视频流中学习，通过像素级建模实现预训练和单流评估之间的灵活切换，并获得了大量单流学习的收益。"
}