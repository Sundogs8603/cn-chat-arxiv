{
    "title": "Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)",
    "abstract": "Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution (OOD) data. Recently developed \"diversification\" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities.  We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the",
    "link": "http://arxiv.org/abs/2312.16313",
    "context": "Title: Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)\nAbstract: Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution (OOD) data. Recently developed \"diversification\" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities.  We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the",
    "path": "papers/23/12/2312.16313.json",
    "total_tokens": 921,
    "translated_title": "通过多样化解析OOD广义化的关键组件",
    "translated_abstract": "监督学习数据集可能包含多个解释训练集同样良好的线索，即学习其中任何一个都会导致对训练数据的正确预测。然而，其中许多线索可能是虚假的，在分布偏移下失去了预测能力，并因此无法推广到超出分布的数据。最近开发的“多样化”方法通过找到依赖不同特征的多个不同的假设来解决这个问题。本文旨在研究这类方法并确定对其OOD广义化能力的贡献的关键组件。我们发现(1) 多样化方法对于用于多样化的无标签数据的分布非常敏感，当远离方法特定的最佳点时性能会显著下降。(2) 仅仅进行多样化是不足以实现OOD广义化的。所使用的学习算法的选择，例如",
    "tldr": "本文研究了通过多样化方法来解决OOD广义化问题，并确定了其关键组件。研究发现，多样化方法对无标签数据的分布非常敏感，且仅仅进行多样化是不足以实现OOD广义化的，学习算法的选择也很重要。",
    "en_tdlr": "This paper investigates the use of diversification methods to address the issue of OOD generalization and identifies the key components contributing to their effectiveness. The study finds that diversification methods are highly sensitive to the distribution of unlabeled data and that diversification alone is insufficient for OOD generalization. Furthermore, the choice of learning algorithm also plays a significant role."
}