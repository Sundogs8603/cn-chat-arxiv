{
    "title": "A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)",
    "abstract": "In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\\% - 95\\% depending on the run). An analogous task was performed to determine the year of writing, an",
    "link": "http://arxiv.org/abs/2312.01185",
    "context": "Title: A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)\nAbstract: In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\\% - 95\\% depending on the run). An analogous task was performed to determine the year of writing, an",
    "path": "papers/23/12/2312.01185.json",
    "total_tokens": 981,
    "translated_title": "时间中的涟漪：美国历史中的不连续性",
    "translated_abstract": "在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\\% - 95\\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。",
    "tldr": "该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。",
    "en_tdlr": "This paper utilizes vector embeddings, nonlinear dimension reduction methods, and fine-tuned models to analyze the State of the Union Address dataset and makes surprising observations about the timeline of American history and the nature of the addresses. It finds that combining GPT-2 with UMAP provides better separation and clustering, and that fine-tuned DistilBERT models can accurately identify the President and the year of the speech."
}