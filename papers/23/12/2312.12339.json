{
    "title": "Value Explicit Pretraining for Learning Transferable Representations",
    "abstract": "arXiv:2312.12339v2 Announce Type: replace  Abstract: We propose Value Explicit Pretraining (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables learning of new tasks that share similar objectives as previously learned tasks, by learning an encoder for objective-conditioned representations, irrespective of appearance changes and environment dynamics. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that results in learning temporally smooth representations. VEP learns to relate states across different tasks based on the Bellman return estimate that is reflective of task progress. Experiments using a realistic navigation simulator and Atari benchmark show that the pretrained encoder produced by our method outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a 2 times improvement in rewards on Atari and visual navigation, and up ",
    "link": "https://arxiv.org/abs/2312.12339",
    "context": "Title: Value Explicit Pretraining for Learning Transferable Representations\nAbstract: arXiv:2312.12339v2 Announce Type: replace  Abstract: We propose Value Explicit Pretraining (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables learning of new tasks that share similar objectives as previously learned tasks, by learning an encoder for objective-conditioned representations, irrespective of appearance changes and environment dynamics. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that results in learning temporally smooth representations. VEP learns to relate states across different tasks based on the Bellman return estimate that is reflective of task progress. Experiments using a realistic navigation simulator and Atari benchmark show that the pretrained encoder produced by our method outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a 2 times improvement in rewards on Atari and visual navigation, and up ",
    "path": "papers/23/12/2312.12339.json",
    "total_tokens": 890,
    "translated_title": "为学习可迁移表示提出价值显性预训练",
    "translated_abstract": "我们提出一种名为价值显性预训练（Value Explicit Pretraining，VEP）的方法，用于学习可迁移的表示，以进行强化学习的迁移。VEP通过学习为与先前学习任务共享类似目标的新任务学习编码器来实现，无论外观变化和环境动态如何，都能学习到目标条件表示。为了从一系列观察中预训练编码器，我们使用了一种自监督对比损失，导致学习到时间上平滑的表示。VEP学习将基于反映任务进展的贝尔曼回报估计来关联不同任务之间的状态。在使用真实导航模拟器和Atari基准进行实验后，结果显示我们方法产生的预训练编码器在泛化到未见任务的能力上优于当前最先进的预训练方法。VEP在Atari和视觉导航上的奖励上获得了多达2倍的改善。",
    "tldr": "提出了价值显性预训练（VEP）方法，通过学习编码器来实现学习可迁移的表示，使得能够在新任务中表现优异，对不同任务之间的状态进行关联，实现在Atari和视觉导航中获得多达2倍奖励改善。"
}