{
    "title": "Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)",
    "abstract": "In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a n",
    "link": "http://arxiv.org/abs/2312.07476",
    "context": "Title: Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)\nAbstract: In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a n",
    "path": "papers/23/12/2312.07476.json",
    "total_tokens": 964,
    "translated_title": "可比较的演示在上下文学习中至关重要：对演示选择的新视角",
    "translated_abstract": "在上下文学习（ICL）中，通过少量演示将大型语言模型（LLMs）适应于下游任务是一种重要的范式。尽管ICL取得了巨大成功，但演示数量的限制可能导致演示偏见，即由LLMs引起的输入-标签映射误解了任务的本质。受人类经验的启发，我们尝试通过演示间关系的视角来缓解这种偏见。具体而言，我们通过最小化编辑文本来构建可比较的演示（CDs），以翻转相应的标签，以突出任务的本质并通过演示间比较消除潜在的虚假相关性。通过一系列的CDs实验，我们发现：（1）LLMs存在演示偏见，而CDs可以显著减少这种偏见；（2）CDs在ICL中表现出良好的性能，尤其是在分布外场景中。总之，本研究从一种新的视角探索了ICL机制。",
    "tldr": "本研究从可比较的演示的角度探索了上下文学习（ICL）机制，并发现演示偏见存在于大型语言模型（LLMs）中，而通过可比较的演示可以显著减少这种偏见，并在ICL中展现出良好的性能。",
    "en_tdlr": "this study explores the mechanisms of In-Context Learning (ICL) from the perspective of Comparable Demonstrations (CDs), and finds that demonstration bias exists in Large Language Models (LLMs), while such bias can be significantly reduced through Comparable Demonstrations, which also exhibit good performance in ICL."
}