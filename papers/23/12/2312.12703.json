{
    "title": "Federated Learning with Extremely Noisy Clients via Negative Distillation",
    "abstract": "arXiv:2312.12703v2 Announce Type: replace  Abstract: Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $>$90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using ",
    "link": "https://arxiv.org/abs/2312.12703",
    "context": "Title: Federated Learning with Extremely Noisy Clients via Negative Distillation\nAbstract: arXiv:2312.12703v2 Announce Type: replace  Abstract: Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $>$90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using ",
    "path": "papers/23/12/2312.12703.json",
    "total_tokens": 957,
    "translated_title": "通过负蒸馏针对极端嘈杂客户的联邦学习",
    "translated_abstract": "联邦学习已经在合作训练深度模型方面取得了显著成功，但通常在处理嘈杂标签时存在困难。先进的作品提出通过一种假设较弱标签嘈杂性的重新加权策略来解决标签噪声问题。然而，由于高度污染的客户，在许多真实世界的联邦学习场景中，这种假设可能被违反，导致极端嘈杂比例，例如大于90%。为了解决极端嘈杂客户的问题，我们研究了重新加权策略的鲁棒性，得出了一种悲观结论：减少在嘈杂数据上训练的客户的权重胜过重新加权策略。为了利用在嘈杂客户上训练的模型，我们提出了一种新方法，称为负蒸馏（FedNed）。FedNed首先识别嘈杂客户，并采用而不是丢弃这些嘈杂客户，以知识蒸馏的方式进行训练。特别是，被识别为嘈杂的客户需要使用",
    "tldr": "通过负蒸馏方法，本研究针对极端嘈杂客户的问题，提出了一种通过减少在嘈杂数据上训练的客户权重来优化模型性能的解决方案。"
}