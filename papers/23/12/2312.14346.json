{
    "title": "Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models",
    "abstract": "arXiv:2312.14346v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are adept at text manipulation -- tasks such as machine translation and text summarization. However, these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides. Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different ways in which models hallucinate. This paper takes a deep dive into LLM behavior with respect to hallucinations, defines a token-level approach to identifying different kinds of hallucinations, and further utilizes this token-level tagging to improve the interpretability and faithfulness of LLMs in dialogue summarization tasks. Through this, the paper presents a new, enhanced dataset and a new training paradigm.",
    "link": "https://arxiv.org/abs/2312.14346",
    "context": "Title: Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models\nAbstract: arXiv:2312.14346v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are adept at text manipulation -- tasks such as machine translation and text summarization. However, these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides. Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different ways in which models hallucinate. This paper takes a deep dive into LLM behavior with respect to hallucinations, defines a token-level approach to identifying different kinds of hallucinations, and further utilizes this token-level tagging to improve the interpretability and faithfulness of LLMs in dialogue summarization tasks. Through this, the paper presents a new, enhanced dataset and a new training paradigm.",
    "path": "papers/23/12/2312.14346.json",
    "total_tokens": 801,
    "translated_title": "不要轻信一切：通过自动识别大语言模型中的幻觉增强摘要解释性",
    "translated_abstract": "大语言模型（LLMs）擅长文本操纵——例如机器翻译和文本摘要。然而，这些模型也容易出现幻觉，这可能对模型提供的任何答案的忠实度造成负面影响。最近的作品致力于对抗LLMs中的幻觉，这些作品涉及识别虚构的句子以及对模型产生幻觉的不同方式进行分类。本文深入探讨了LLMs在幻觉方面的行为，定义了一种基于标记的方法来识别不同类型的幻觉，并进一步利用该标记来提高LLMs在对话摘要任务中的可解释性和忠实度。通过这一过程，本文提出了一个新的、增强的数据集和一个新的训练范式。",
    "tldr": "本文通过定义标记级别的方法来识别不同类型的幻觉，并利用这种标记来提高LLMs在对话摘要任务中的可解释性和忠实度。",
    "en_tdlr": "This paper enhances the interpretability and faithfulness of Large Language Models in dialogue summarization tasks by defining a token-level approach to identify different kinds of hallucinations and utilizing this tagging method."
}