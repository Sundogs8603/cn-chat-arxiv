{
    "title": "Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)",
    "abstract": "Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in ",
    "link": "http://arxiv.org/abs/2312.06786",
    "context": "Title: Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)\nAbstract: Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in ",
    "path": "papers/23/12/2312.06786.json",
    "total_tokens": 915,
    "translated_title": "混合线性专家用于长期时间序列预测",
    "translated_abstract": "长期时间序列预测(LTSF)旨在预测给定过去值的时间序列的未来值。当前在这个问题上的最先进技术(SOTA)在某些情况下是由以线性为中心的模型实现的，这些模型主要具有线性映射层。然而，由于其固有的简单性，它们不能够适应时间序列模式的周期性变化。为了解决这个挑战，我们提出了一种混合专家风格的增强线性模型的方法，并提出了混合线性专家(MoLE)。MoLE不是训练单个模型，而是训练多个以线性为中心的模型(即专家)和一个权衡和混合其输出的路由模型。虽然整个框架是端到端训练的，但每个专家都学会专门处理特定的时间模式，而路由模型则学会自适应地组合专家们的输出。实验证明，MoLE降低了线性中心模型(DLinear，RLinear和RMLP)的预测误差。",
    "tldr": "MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。",
    "en_tdlr": "MoLE is a mixture-of-linear-experts model that trains multiple linear-centric models and a router model to adapt to periodic changes in time series patterns, resulting in significantly reduced forecasting error."
}