{
    "title": "The Generalization Gap in Offline Reinforcement Learning",
    "abstract": "arXiv:2312.05742v2 Announce Type: replace-cross  Abstract: Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both ",
    "link": "https://arxiv.org/abs/2312.05742",
    "context": "Title: The Generalization Gap in Offline Reinforcement Learning\nAbstract: arXiv:2312.05742v2 Announce Type: replace-cross  Abstract: Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both ",
    "path": "papers/23/12/2312.05742.json",
    "total_tokens": 862,
    "translated_title": "离线强化学习中的泛化差距",
    "translated_abstract": "尽管近年来离线学习取得了一些进展，这些方法仍然是在相同的环境上进行训练和测试的。在本文中，我们比较了广泛使用的在线和离线学习方法（如在线强化学习（RL）、离线RL、序列建模和行为克隆）的泛化能力。我们的实验表明，离线学习算法在新环境中的表现不如在线学习算法。我们还引入了第一个用于评估离线学习中泛化能力的基准测试，从Procgen（2D视频游戏）和WebShop（电子商务网站）收集了多种大小和技能水平的数据集。这些数据集包含了有限数量的游戏关卡或自然语言指令的轨迹，测试时，代理要能够泛化到新的关卡或指令。我们的实验证明，现有的离线学习算法在这两个方面都很难与在线RL的表现相匹配。",
    "tldr": "该研究比较了在线和离线学习方法在泛化能力上的差异，发现离线学习算法在新环境中表现不如在线学习算法，并引入了用于评估泛化能力的第一个基准测试。",
    "en_tdlr": "This study compares the difference in generalization abilities between online and offline learning methods, finding that offline learning algorithms perform worse on new environments than online learning algorithms, and introduces the first benchmark for evaluating generalization abilities."
}