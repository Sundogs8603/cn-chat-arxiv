{
    "title": "Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs",
    "abstract": "The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i",
    "link": "https://arxiv.org/abs/2312.11282",
    "context": "Title: Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs\nAbstract: The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i",
    "path": "papers/23/12/2312.11282.json",
    "total_tokens": 943,
    "translated_title": "评估和增强用于知识图谱上的对话推理的大型语言模型",
    "translated_abstract": "大型语言模型（LLM）的发展得益于预训练技术的进展。通过手动设计的提示，这些模型展示了强大的推理能力。在这项工作中，我们评估了当前最先进的LLM（GPT-4）在知识图谱（KG）上的对话推理能力。然而，由于缺乏KG环境意识和开发有效的中间推理阶段优化机制的困难，LLM的性能受到限制。我们进一步引入了LLM-ARK，一个基于KG推理的LLM基准代理，旨在提供精确和适应性强的KG路径预测。LLM-ARK利用全文环境（FTE）提示来吸收每个推理步骤中的状态信息。我们将KG上的多跳推理挑战重新框定为顺序决策任务。利用近端策略优化（PPO）在线策略梯度强化学习算法，我们的模型...",
    "tldr": "该论文评估了当前最先进的大型语言模型（GPT-4）在知识图谱上的对话推理能力，提出了一种基于KG推理的LLM基准代理（LLM-ARK），该代理利用全文环境提示来实现精确和适应性强的KG路径预测，并采用近端策略优化算法进行训练。",
    "en_tdlr": "This paper evaluates the conversational reasoning capabilities of the state-of-the-art large language model (GPT-4) on knowledge graphs (KGs) and introduces LLM-ARK, a grounded KG reasoning agent that delivers precise and adaptable KG path predictions using full textual environment prompts, trained using the Proximal Policy Optimization algorithm."
}