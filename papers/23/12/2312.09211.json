{
    "title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models. (arXiv:2312.09211v3 [cs.CL] UPDATED)",
    "abstract": "Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications. However, this approach is susceptible to the existence of outlier values in activation. The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder. This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models. Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values. The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively. We provide theoretical analysis and supporting experiments",
    "link": "http://arxiv.org/abs/2312.09211",
    "context": "Title: Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models. (arXiv:2312.09211v3 [cs.CL] UPDATED)\nAbstract: Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications. However, this approach is susceptible to the existence of outlier values in activation. The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder. This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models. Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values. The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively. We provide theoretical analysis and supporting experiments",
    "path": "papers/23/12/2312.09211.json",
    "total_tokens": 810,
    "translated_title": "降低语言模型低精度微调中的异常激活",
    "translated_abstract": "低精度微调语言模型已成为一种成本效益高且能源高效的方法，在各种应用中部署大规模模型。然而，这种方法容易受到激活中异常值的影响。激活中的异常值会对低精度微调语言模型的性能产生负面影响，因为它们影响了缩放因子，使得表示较小的值变得更困难。本文研究了在语言模型低精度整数微调中减少异常激活的技术。我们提出的新方法使我们能够用8位整数而不是浮点（FP16）值表示异常激活值。使用整数来表示异常值的好处是，它使我们能够使用运算符切片来避免执行16位整数矩阵乘法，从而有效解决这个问题。我们提供了理论分析和支持实验。",
    "tldr": "本论文研究了在语言模型低精度微调中减少异常激活的技术，提出了一种能够用8位整数表示异常激活值的新方法，通过使用整数和运算符切片来提高效率和性能。"
}