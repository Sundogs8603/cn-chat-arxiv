{
    "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft",
    "abstract": "arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax ",
    "link": "https://arxiv.org/abs/2312.09238",
    "context": "Title: Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft\nAbstract: arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax ",
    "path": "papers/23/12/2312.09238.json",
    "total_tokens": 802,
    "translated_title": "使用大语言模型为Minecraft自动设计稠密奖励的Auto MC-Reward",
    "translated_abstract": "许多强化学习环境（例如Minecraft）仅提供指示任务完成或失败的稀疏奖励，这些奖励以二进制值表示。在这种环境中探索效率的挑战使得基于强化学习的代理程序难以学习复杂任务。为了解决这个问题，本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型（LLMs）自动设计稠密奖励函数，从而提高学习效率。Auto MC-Reward包括三个重要组件：奖励设计者、奖励评论家和轨迹分析器。给定环境信息和任务描述，奖励设计者首先通过编写可执行的Python函数和预定义的观测输入来设计奖励函数。然后，我们的奖励评论家将负责验证代码，检查代码是否自洽且无语法错误。",
    "tldr": "本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率",
    "en_tdlr": "This paper introduces an advanced learning system named Auto MC-Reward that leverages Large Language Models to automatically design dense reward functions for enhancing learning efficiency."
}