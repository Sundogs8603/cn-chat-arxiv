{
    "title": "Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)",
    "abstract": "Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave an",
    "link": "http://arxiv.org/abs/2312.07178",
    "context": "Title: Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)\nAbstract: Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave an",
    "path": "papers/23/12/2312.07178.json",
    "total_tokens": 924,
    "translated_title": "超越预期回报：评估强化学习算法时考虑政策可复制性",
    "translated_abstract": "在强化学习中，许多应用通常在环境中存在噪声或随机性。除了对学习的影响之外，这些不确定性导致相同的策略在不同的试验中表现不同，即产生不同的回报。强化学习中常用的评估程序仅使用预期回报来总结结果分布，而不考虑分布的扩散。我们的工作将这种扩散定义为政策的可复制性：当多次试验时，政策获得类似性能的能力，在一些实际应用中这是至关重要的属性。我们指出，现有的仅使用预期回报的程序在两个方面存在局限性：第一，存在无数个回报分布，具有广泛的性能和可复制性的权衡，但是它们可以有相同的预期回报，限制了它在比较策略时的有效性；第二，预期回报度量没有留下足够的空间，以描述分布的其他关键性质。",
    "tldr": "本文提出了一种超越预期回报的评估方法，在强化学习中考虑策略的可复制性。现有的评估方法仅使用预期回报，无法充分考虑分布的扩散，这限制了其在比较策略时的有效性。"
}