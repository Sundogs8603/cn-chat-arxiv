{
    "title": "Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding",
    "abstract": "arXiv:2312.05328v3 Announce Type: replace Abstract: Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate \"learnability\" scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-priori",
    "link": "https://arxiv.org/abs/2312.05328",
    "context": "Title: Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding\nAbstract: arXiv:2312.05328v3 Announce Type: replace Abstract: Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate \"learnability\" scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-priori",
    "path": "papers/23/12/2312.05328.json",
    "total_tokens": 990,
    "translated_title": "不良学生成就了优秀教师：主动学习加速大规模视觉理解",
    "translated_abstract": "幂律缩放表明使用均匀采样的大规模训练速度太慢。主动学习方法旨在通过优先学习最相关的示例来提高数据效率。尽管这些方法具有吸引力，但由于没有证明 a) 可以泛化到各种模型和任务 b) 可以扩展到大规模数据集并且 c) 在考虑数据选择开销时能节省计算资源，因此尚未被广泛采用。在这项工作中，我们提出了一种满足这三个属性的方法，利用小型廉价的代理模型来估计数据点的“可学习性”分数，用于优先训练更大的模型所需的数据。结果上，我们的模型在JFT上需要46%和51%更少的训练更新次数，并且要达到与均匀训练的视觉分类器在JFT和ALIGN上多模型的相同性能需要高达25%的更少的总计算量。最后，我们发现我们的数据优先方法能够在视觉理解和多模型任务上取得与均匀训练相当的性能，同时节省了计算资源。",
    "tldr": "研究提出了一种满足泛化性、扩展性和计算资源优化的主动学习方法，利用廉价的代理模型估计数据点的可学习性分数，优先选择训练更大的模型所需的数据，从而在大规模视觉理解任务中取得相同性能的同时减少了计算量。"
}