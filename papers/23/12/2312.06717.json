{
    "title": "Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)",
    "abstract": "This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h",
    "link": "http://arxiv.org/abs/2312.06717",
    "context": "Title: Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)\nAbstract: This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h",
    "path": "papers/23/12/2312.06717.json",
    "total_tokens": 1006,
    "translated_title": "大型语言模型中的隐私问题：一项调研",
    "translated_abstract": "这是首个关注大型语言模型（LLMs）隐私问题的AI研究领域调研。具体而言，我们关注红队模型以突出隐私风险的工作，尝试将隐私纳入训练或推理过程中的工作，使得数据可以从训练模型中高效删除以符合现有的隐私法规，并试图减轻版权问题。我们的重点在于总结开发算法、证明定理和进行实证评估的技术研究。虽然有大量的法律和政策研究从不同角度解决这些挑战，但这不是我们调研的重点。然而，这些作品以及最近的法律进展确实影响了这些技术问题的形式化处理方式，因此我们在第一节中对其进行了简要讨论。尽管我们已经尽力包含所有相关研究，但由于这一领域的研究进展迅速，我们可能会漏掉一些最新的研究成果。",
    "tldr": "这项调研关注大型语言模型中的隐私问题。主要总结了红队模型揭示隐私风险、将隐私纳入训练和推理过程、高效删除训练模型中的数据以符合隐私法规、以及减轻版权问题等技术研究。法律和政策研究虽然从不同角度解决了相关挑战，但不是本调研的重点。",
    "en_tdlr": "This survey focuses on privacy issues in large language models (LLMs). It summarizes technical research on red-teaming models to highlight privacy risks, incorporating privacy into the training or inference process, efficient data deletion to comply with privacy regulations, and mitigation of copyright issues. Legal and policy work, although addressing these challenges from a different angle, is not the main focus of this survey."
}