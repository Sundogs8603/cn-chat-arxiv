{
    "title": "CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models",
    "abstract": "arXiv:2312.03256v2 Announce Type: replace  Abstract: Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedd",
    "link": "https://arxiv.org/abs/2312.03256",
    "context": "Title: CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models\nAbstract: arXiv:2312.03256v2 Announce Type: replace  Abstract: Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedd",
    "path": "papers/23/12/2312.03256.json",
    "total_tokens": 951,
    "translated_title": "CAFE：面向大规模推荐模型的紧凑、自适应和快速嵌入",
    "translated_abstract": "最近，深度学习推荐模型（DLRM）中嵌入表的不断增长的内存需求给模型训练和部署带来了巨大挑战。现有的嵌入压缩解决方案无法同时满足内存效率、低延迟和适应动态数据分布等三个关键设计要求。本文提出了CAFE，一种紧凑、自适应和快速的嵌入压缩框架，以解决上述要求。CAFE的设计理念是动态分配更多内存资源给重要特征（称为热门特征），并为不重要的特征分配更少内存。在CAFE中，我们提出了一种快速轻量级的草图数据结构，命名为HotSketch，用于捕获特征重要性并实时报告热门特征。对于每个报告的热门特征，我们为其分配唯一的嵌入。对于非热门特征，我们允许多个特征共享一个嵌入，使用哈希embedd。",
    "tldr": "CAFE提出了一种紧凑、自适应和快速的嵌入压缩框架，通过动态分配内存资源给重要特征并引入HotSketch数据结构实时捕获热门特征，解决了嵌入表在大规模推荐模型中内存需求增长的挑战。",
    "en_tdlr": "CAFE presents a compact, adaptive, and fast embedding compression framework that dynamically allocates memory resources to important features, introduces the HotSketch data structure to capture hot features in real time, and addresses the challenge of growing memory demands of embedding tables in large-scale recommendation models."
}