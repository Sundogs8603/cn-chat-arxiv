{
    "title": "Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing. (arXiv:2312.14472v2 [cs.AI] UPDATED)",
    "abstract": "Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks ",
    "link": "http://arxiv.org/abs/2312.14472",
    "context": "Title: Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing. (arXiv:2312.14472v2 [cs.AI] UPDATED)\nAbstract: Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks ",
    "path": "papers/23/12/2312.14472.json",
    "total_tokens": 913,
    "translated_title": "不是所有的任务都一样困难：具有动态深度路由的多任务深度强化学习",
    "translated_abstract": "多任务强化学习旨在通过单个策略完成一组不同的任务。为了通过在多个任务之间共享参数来增强数据效率，一种常见的做法是将网络分割成不同的模块，并训练一个路由网络将这些模块重新组合成任务特定的策略。然而，现有的路由方法在所有任务中使用固定数量的模块，忽略不同难度的任务通常需要不同数量的知识。本文提出了一种动态深度路由 (D2R) 框架，该框架学习跳过某些中间模块，从而灵活地为每个任务选择不同数量的模块。在这个框架下，我们进一步引入了一种 ResRouting 方法，解决了离线训练过程中行为策略和目标策略之间截然不同的路由路径的问题。此外，我们设计了一种自动的路由平衡机制，鼓励对未掌握任务进行持续的路由探索。",
    "tldr": "本文提出了具有动态深度路由的多任务深度强化学习框架(D2R)，该框架灵活选择不同任务所需的模块数量，并通过引入ResRouting方法和自动路由平衡机制来解决存在的问题。",
    "en_tdlr": "This paper proposes a Multi-Task Deep Reinforcement Learning framework with Dynamic Depth Routing (D2R) that flexibly chooses different numbers of modules for each task and addresses existing issues through the introduction of the ResRouting method and an automatic route-balancing mechanism."
}