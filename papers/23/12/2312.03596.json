{
    "title": "MMM: Generative Masked Motion Model",
    "abstract": "arXiv:2312.03596v2 Announce Type: replace-cross  Abstract: Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fi",
    "link": "https://arxiv.org/abs/2312.03596",
    "context": "Title: MMM: Generative Masked Motion Model\nAbstract: arXiv:2312.03596v2 Announce Type: replace-cross  Abstract: Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fi",
    "path": "papers/23/12/2312.03596.json",
    "total_tokens": 862,
    "translated_title": "MMM：生成式遮蔽运动模型",
    "translated_abstract": "最近在使用扩散和自回归模型进行文本到运动生成方面取得了一些进展，显示出了良好的结果。然而，这些模型往往在实时性能、高保真度和运动可编辑性之间存在权衡。为了解决这一问题，我们引入了MMM，一种基于遮蔽运动模型的新颖而简单的运动生成范式。MMM由两个关键组件组成：（1）运动标记器，将3D人体运动转化为潜在空间中的一系列离散标记，以及（2）条件遮蔽运动变换器，学习预测在预先计算的文本标记的条件下随机遮蔽的运动标记。通过在所有方向上关注运动和文本标记，MMM明确地捕获了运动标记之间的固有依赖关系以及运动和文本标记之间的语义映射。在推断过程中，这允许对与fi高度一致的多个运动标记进行并行和迭代解码。",
    "tldr": "MMM 提出了一种基于遮蔽运动模型的新颖运动生成范式，通过运动标记器和条件遮蔽运动变换器，在实时性能、高保真度和运动可编辑性之间取得平衡。",
    "en_tdlr": "MMM introduces a novel motion generation paradigm based on Masked Motion Model, balancing real-time performance, high fidelity, and motion editability with a motion tokenizer and conditional masked motion transformer."
}