{
    "title": "Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization",
    "abstract": "arXiv:2312.16731v2 Announce Type: replace  Abstract: The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods d",
    "link": "https://arxiv.org/abs/2312.16731",
    "context": "Title: Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization\nAbstract: arXiv:2312.16731v2 Announce Type: replace  Abstract: The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods d",
    "path": "papers/23/12/2312.16731.json",
    "total_tokens": 786,
    "translated_title": "用于分解连续学习的无限dSprites：将记忆编辑与泛化分离",
    "translated_abstract": "机器学习系统持续学习的能力受到灾难性遗忘的阻碍，即神经网络在学习新任务时会覆盖现有知识。持续学习方法通过正则化、参数隔离或排练来缓解这一问题，但它们通常在仅包含少数任务的基准测试上进行评估。为了取得进展以缩小这一差距，我们介绍了Infinite dSprites，这是一个简洁的工具，可创建任意长度的连续分类和分解基准，并对生成因素拥有完全控制。我们展示，在足够长的时间范围内，所有主要类型的持续学习方法的性能都表现出...",
    "tldr": "引入了Infinite dSprites工具，用于创建任意长度的连续分类和分解基准，可以全面控制生成因素，有望缩小机器学习系统与人类学习在动态开放环境中的差距",
    "en_tdlr": "Introduced Infinite dSprites tool for creating continual classification and disentanglement benchmarks of arbitrary length with full control over generative factors, aiming to narrow the gap between machine learning systems and human learning in dynamic open environments."
}