{
    "title": "EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v2 [cs.CV] UPDATED)",
    "abstract": "Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereb",
    "link": "http://arxiv.org/abs/2312.08010",
    "context": "Title: EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v2 [cs.CV] UPDATED)\nAbstract: Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the crucial temporal aspects inherent to the video domain. In this study, we present EZ-CLIP, a simple and efficient adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal visual prompting for seamless temporal adaptation, requiring no fundamental alterations to the core CLIP architecture while preserving its remarkable generalization abilities. Moreover, we introduce a novel learning objective that guides the temporal visual prompts to focus on capturing motion, thereb",
    "path": "papers/23/12/2312.08010.json",
    "total_tokens": 843,
    "translated_title": "EZ-CLIP: 高效的零样本视频动作识别",
    "translated_abstract": "最近，在图像-文本配对数据的大规模预训练中取得的进展展示了惊人的零样本任务的泛化能力。借鉴这一成功，研究人员努力将图像为基础的视觉语言模型（如CLIP）应用于视频，扩展其在零样本任务上的能力。虽然这些改进表现出了很有前景的结果，但它们需要巨大的计算成本，并且在有效建模视频领域内关键的时间方面方面存在困难。在这项研究中，我们提出了EZ-CLIP，这是一个简单高效的CLIP改进方案，解决了这些挑战。EZ-CLIP利用时间上的视觉提示进行无缝的时间适应，无需对核心CLIP架构进行基本改动，同时保持其卓越的泛化能力。此外，我们引入了一种新的学习目标，对时间视觉提示进行引导，以专注于捕捉动作。",
    "tldr": "EZ-CLIP是一个简单高效的CLIP改进方案，通过引入时间上的视觉提示来解决视频领域中的挑战，并保持了CLIP的泛化能力。"
}