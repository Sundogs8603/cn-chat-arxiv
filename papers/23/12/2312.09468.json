{
    "title": "Safe Reinforcement Learning in a Simulated Robotic Arm",
    "abstract": "arXiv:2312.09468v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safe",
    "link": "https://arxiv.org/abs/2312.09468",
    "context": "Title: Safe Reinforcement Learning in a Simulated Robotic Arm\nAbstract: arXiv:2312.09468v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safe",
    "path": "papers/23/12/2312.09468.json",
    "total_tokens": 800,
    "translated_title": "在模拟机器人臂中的安全强化学习",
    "translated_abstract": "强化学习（RL）代理需要探索环境以学习最优策略。在许多环境和任务中，安全性至关重要。模拟器的广泛使用提供了许多优势，其中包括安全探索，当RL系统需要直接在物理环境（例如在人机交互中）中进行训练时，安全探索将是不可避免的。流行的Safety Gym库提供了三种移动代理类型，可以学习目标导向任务同时考虑各种安全约束。本文通过创建一个带有Panda机器人臂的定制环境，扩展了安全RL算法的适用性，以便测试Safety Gym算法。我们使用流行的PPO算法进行了试点实验，比较了基线与受限版本，并表明受限版本能够学习出同样优秀的策略，同时更好地符合安全性。",
    "tldr": "本文通过在Panda机器人臂上创建定制环境，扩展了安全强化学习算法的适用性，实现了安全RL算法在物理环境中的测试。"
}