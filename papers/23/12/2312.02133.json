{
    "title": "Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.",
    "link": "http://arxiv.org/abs/2312.02133",
    "context": "Title: Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)\nAbstract: Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.",
    "path": "papers/23/12/2312.02133.json",
    "total_tokens": 907,
    "translated_title": "通过共享注意力实现风格对齐的图像生成",
    "translated_abstract": "大规模的文本到图像（T2I）模型在创意领域迅速崭露头角，从文本提示生成视觉上引人注目的输出物。然而，控制这些模型以确保一致的风格仍然具有挑战性，现有方法需要进行精细调整和手动干预以解开内容和风格的复杂关系。在本文中，我们引入了一种名为StyleAligned的新技术，旨在在一系列生成的图像之间建立风格对齐。通过在扩散过程中使用最小的“注意力共享”，我们的方法在T2I模型中保持图像之间的风格一致性。这种方法通过简单的反转操作，可使用参考风格来创建具有一致风格的图像。我们在不同风格和文本提示上对该方法进行了评估，结果显示出了高质量的合成和保真度，凸显了其在实现各种输入的一致风格方面的效果。",
    "tldr": "本文提出了一种名为StyleAligned的新技术，通过在T2I模型中使用“注意力共享”，实现了在一系列生成的图像之间建立风格对齐。该方法能够通过简单的反转操作，使用参考风格创建具有一致风格的图像，并在各种输入中实现了高质量的合成和保真度。"
}