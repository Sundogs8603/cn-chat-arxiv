{
    "title": "Applying Large Language Models and Chain-of-Thought for Automatic Scoring",
    "abstract": "arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric",
    "link": "https://arxiv.org/abs/2312.03748",
    "context": "Title: Applying Large Language Models and Chain-of-Thought for Automatic Scoring\nAbstract: arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric",
    "path": "papers/23/12/2312.03748.json",
    "total_tokens": 906,
    "translated_title": "应用大型语言模型和思维链进行自动评分",
    "translated_abstract": "本研究调查了大型语言模型（LLMs），特别是 GPT-3.5 和 GPT-4，以及思维链（CoT）在自动评分学生对科学评估的写作反馈中的应用。我们专注于克服先前限制研究人员和教育工作者使用基于人工智能的自动评分工具的无法访问性、技术复杂性和缺乏解释性的挑战。通过一个包括 1,650 个学生反馈的六项评估任务（三个二项和三个三项）的测试数据集，我们采用六种提示工程策略来自动评分学生的反馈。这六种策略将零次学习或少次学习与 CoT 结合在一起，要么独自使用，要么与项目干和评分细则一起使用。结果表明，少次学习（准确率=.67）优于零次学习（准确率=.60），提高12.6%。",
    "tldr": "本研究探讨了在自动评分学生对科学评估写作反馈中应用大型语言模型（LLMs）和思维链（CoT），通过零次或少次学习等策略自动评分，其中少次学习表现更佳。",
    "en_tdlr": "This study explores the application of large language models (LLMs) and Chain-of-Though (CoT) for automatically scoring student-written responses to science assessments, utilizing strategies such as zero-shot or few-shot learning, with few-shot learning showing superior performance."
}