{
    "title": "A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning",
    "abstract": "arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme",
    "link": "https://arxiv.org/abs/2312.15474",
    "context": "Title: A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning\nAbstract: arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme",
    "path": "papers/23/12/2312.15474.json",
    "total_tokens": 855,
    "translated_title": "在离线动力学强化学习中的少样本转移的保守方法",
    "translated_abstract": "离线动力学强化学习（ODRL）旨在将策略从源环境转移到具有不同但相似动力学特征的目标环境。在这种情况下，传统RL代理过度依赖源环境的动力学，导致发现在该环境中表现卓越的策略，但在目标环境中表现不佳。在少样本框架中，引入了来自目标环境的有限数量转换以促进更有效的转移。为了解决这一挑战，我们提出了一种受最近模仿学习和保守RL算法进展启发的创新方法。所提出的方法引入了一个惩罚来调节源训练策略生成的轨迹。我们在代表不同离线动力学条件的各种环境中评估了我们的方法，在这些环境中访问目标环境是极端困难的。",
    "tldr": "提出了一种受最近模仿学习和保守RL算法进展启发的创新方法，在离线动力学强化学习中的少样本转移过程中引入惩罚来调节源训练策略生成的轨迹。",
    "en_tdlr": "Proposed an innovative method inspired by recent advancements in Imitation Learning and conservative RL algorithms, introducing a penalty to regulate the trajectories generated by the source-trained policy in the few-shot transfer process in off-dynamics reinforcement learning."
}