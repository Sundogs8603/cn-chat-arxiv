{
    "title": "CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition. (arXiv:2312.10201v3 [cs.MM] UPDATED)",
    "abstract": "Multi-modal multi-label emotion recognition (MMER) aims to identify relevant emotions from multiple modalities. The challenge of MMER is how to effectively capture discriminative features for multiple labels from heterogeneous data. Recent studies are mainly devoted to exploring various fusion strategies to integrate multi-modal information into a unified representation for all labels. However, such a learning scheme not only overlooks the specificity of each modality but also fails to capture individual discriminative features for different labels. Moreover, dependencies of labels and modalities cannot be effectively modeled. To address these issues, this paper presents ContrAstive feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically, we devise a reconstruction-based fusion mechanism to better model fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features. To further exploit the modality complementarity",
    "link": "http://arxiv.org/abs/2312.10201",
    "context": "Title: CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition. (arXiv:2312.10201v3 [cs.MM] UPDATED)\nAbstract: Multi-modal multi-label emotion recognition (MMER) aims to identify relevant emotions from multiple modalities. The challenge of MMER is how to effectively capture discriminative features for multiple labels from heterogeneous data. Recent studies are mainly devoted to exploring various fusion strategies to integrate multi-modal information into a unified representation for all labels. However, such a learning scheme not only overlooks the specificity of each modality but also fails to capture individual discriminative features for different labels. Moreover, dependencies of labels and modalities cannot be effectively modeled. To address these issues, this paper presents ContrAstive feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically, we devise a reconstruction-based fusion mechanism to better model fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features. To further exploit the modality complementarity",
    "path": "papers/23/12/2312.10201.json",
    "total_tokens": 884,
    "translated_title": "CARAT: 对于多模态多标签情绪识别的对比特征重构和聚合",
    "translated_abstract": "多模态多标签情绪识别旨在从多个模态中识别相关情绪。MMER的挑战在于如何有效地捕捉来自异构数据的多个标签的判别特征。最近的研究主要致力于探索各种融合策略，将多模态信息集成到统一表示中的所有标签。然而，这种学习方案不仅忽视了每种模态的特殊性，也无法捕捉不同标签的个别判别特征。此外，标签和模态的依赖关系也无法有效建模。为了解决这些问题，本文提出了一种用于MMER任务的对比特征重构和聚合（CARAT）方法。具体而言，我们设计了一种基于重构的融合机制，通过对比学习模态分离和标签特定的特征，更好地建模细粒度的模态对标签的依赖关系。",
    "tldr": "本文提出了对比特征重构和聚合（CARAT）方法，用于多模态多标签情绪识别。通过对比学习模态分离和标签特定的特征，CARAT能更好地建模细粒度的模态对标签的依赖关系。",
    "en_tdlr": "This paper presents CARAT, a method for multi-modal multi-label emotion recognition, which effectively models fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features."
}