{
    "title": "Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)",
    "abstract": "The recent successes in analyzing images with deep neural networks are almost exclusively achieved with Convolutional Neural Networks (CNNs). The training of these CNNs, and in fact of all deep neural network architectures, uses the backpropagation algorithm where the output of the network is compared with the desired result and the difference is then used to tune the weights of the network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton suggested an alternative way of training which passes the desired results together with the images at the input of the network. This so called Forward Forward (FF) algorithm has up to now only been used in fully connected networks. In this paper, we show how the FF paradigm can be extended to CNNs. Our FF-trained CNN, featuring a novel spatially-extended labeling technique, achieves a classification accuracy of 99.16% on the MNIST hand-written digits dataset. We show how different hyperparameters affect the performance of the proposed ",
    "link": "http://arxiv.org/abs/2312.14924",
    "context": "Title: Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)\nAbstract: The recent successes in analyzing images with deep neural networks are almost exclusively achieved with Convolutional Neural Networks (CNNs). The training of these CNNs, and in fact of all deep neural network architectures, uses the backpropagation algorithm where the output of the network is compared with the desired result and the difference is then used to tune the weights of the network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton suggested an alternative way of training which passes the desired results together with the images at the input of the network. This so called Forward Forward (FF) algorithm has up to now only been used in fully connected networks. In this paper, we show how the FF paradigm can be extended to CNNs. Our FF-trained CNN, featuring a novel spatially-extended labeling technique, achieves a classification accuracy of 99.16% on the MNIST hand-written digits dataset. We show how different hyperparameters affect the performance of the proposed ",
    "path": "papers/23/12/2312.14924.json",
    "total_tokens": 882,
    "translated_title": "使用Forward-Forward算法训练卷积神经网络",
    "translated_abstract": "近年来，使用深度神经网络对图像进行分析的最新成功几乎全部实现于卷积神经网络（CNN）。这些CNN以及所有深度神经网络架构的训练都使用了反向传播算法，将网络的输出与期望结果进行比较，利用差异来调整网络权重以达到期望的输出。在2022年的一篇预印本中，Geoffrey Hinton提出了一种替代的训练方式，即在网络的输入中同时传递期望的结果和图像。这种称为Forward Forward（FF）算法到目前为止仅在全连接网络中使用过。在本文中，我们展示了如何将FF范式扩展到CNN中。我们的FF训练的CNN采用了一种新颖的空间扩展标签技术，在MNIST手写数字数据集上实现了99.16%的分类准确率。我们展示了不同超参数对所提出的模型性能的影响。",
    "tldr": "本文基于Forward-Forward算法将其应用于卷积神经网络（CNN）的训练，采用了新颖的空间扩展标签技术，在MNIST手写数字数据集上达到了99.16%的分类准确率。",
    "en_tdlr": "This paper extends the Forward-Forward algorithm to the training of Convolutional Neural Networks (CNNs), and achieves a classification accuracy of 99.16% on the MNIST hand-written digits dataset using a novel spatially-extended labeling technique."
}