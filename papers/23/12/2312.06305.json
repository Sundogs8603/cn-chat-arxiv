{
    "title": "A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)",
    "abstract": "AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.",
    "link": "http://arxiv.org/abs/2312.06305",
    "context": "Title: A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)\nAbstract: AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.",
    "path": "papers/23/12/2312.06305.json",
    "total_tokens": 838,
    "translated_title": "一种用于自动机器学习中顺序超参数空间缩减的元级学习算法",
    "translated_abstract": "在AutoML平台上，每个分析步骤都有许多算法可供尝试，例如插补算法、转换算法、特征选择算法和建模算法等。找到最佳的算法组合和超参数值是计算上昂贵的，因为要探索的组合数量导致空间的指数爆炸。本文提出了一种名为顺序超参数空间缩减（SHSR）的算法，用于减少AutoML工具所需的空间，并且性能损失可以忽略不计。SHSR是一种元级学习算法，它分析AutoML工具在几个数据集上的过去运行结果，并学习哪些超参数值可以从要分析的新数据集中过滤掉。SHSR在284个分类问题和375个回归问题上进行了评估，显示出约30%的执行时间缩短和不到0.1%的性能损失。",
    "tldr": "本文提出了一种元级学习算法SHSR，用于减少AutoML中的超参数空间，减少了约30%的执行时间并且性能损失小于0.1%。"
}