{
    "title": "PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping",
    "abstract": "arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,",
    "link": "https://arxiv.org/abs/2312.12065",
    "context": "Title: PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping\nAbstract: arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,",
    "path": "papers/23/12/2312.12065.json",
    "total_tokens": 778,
    "translated_title": "PPO-Clip实现全局最优性：更深入理解修剪技术",
    "translated_abstract": "在这篇论文中，我们首次建立了PPO-Clip变体在表格和神经函数逼近设置中具有全局收敛性结果。我们的发现特别突出了在神经函数逼近情境下的$O(1/\\sqrt{T})$最小迭代收敛速率。通过引入PPO-Clip的广义版本，结合其与铰链损失的关系，采用熵镜像下降，我们为直接策略参数化的表格PPO-Clip建立了渐近收敛。受表格分析启发，",
    "tldr": "该论文在PPO-Clip算法方面做出贡献，建立了其在表格和神经函数逼近设置中的全局收敛结果，特别突出了在神经函数逼近情境下的$O(1/\\sqrt{T})$最小迭代收敛速率。",
    "en_tdlr": "This paper contributes by establishing the global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings, highlighting the $O(1/\\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation."
}