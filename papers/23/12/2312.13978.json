{
    "title": "Metalearning with Very Few Samples Per Task",
    "abstract": "arXiv:2312.13978v2 Announce Type: replace  Abstract: Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new tasks from the metadistribution.   We consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in H$ is a map from features to a representation space that is shared across tasks, and $f_{P} \\in F$ is a task-specific classifier from the representation space to labels. The main question we ask is how much data do we need to metalearn a go",
    "link": "https://arxiv.org/abs/2312.13978",
    "context": "Title: Metalearning with Very Few Samples Per Task\nAbstract: arXiv:2312.13978v2 Announce Type: replace  Abstract: Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new tasks from the metadistribution.   We consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in H$ is a map from features to a representation space that is shared across tasks, and $f_{P} \\in F$ is a task-specific classifier from the representation space to labels. The main question we ask is how much data do we need to metalearn a go",
    "path": "papers/23/12/2312.13978.json",
    "total_tokens": 792,
    "translated_title": "针对每个任务样本数量很少的元学习",
    "translated_abstract": "Metalearning和多任务学习是两种解决相关学习任务组的框架，能够比我们单独解决每个任务更高效。 在多任务学习中，我们被给定一组相关学习任务，并需要为每个任务输出一个准确的模型；而在元学习中，我们被给定从元分布中独立同分布绘制的任务，并需要输出一些可以很容易地专门用于元分布中新任务的公共信息。 我们考虑一个二分类设置，任务是由共享表示相关联的，也就是说，每个任务$P$可以通过形式为$f_{P}\\circ h$的分类器解决，其中$h \\in H$是从特征到共享表示空间的映射，而$f_{P} \\in F$是从表示空间到标签的任务特定分类器。 我们主要探讨的问题是我们需要多少数据来元学习",
    "tldr": "在针对每个任务样本数量很少的情况下，研究了如何通过共享表示进行元学习。",
    "en_tdlr": "Investigated how to perform metalearning with very few samples per task using shared representations."
}