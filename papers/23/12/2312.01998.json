{
    "title": "Language-only Efficient Training of Zero-shot Composed Image Retrieval",
    "abstract": "arXiv:2312.01998v2 Announce Type: replace-cross  Abstract: Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, ",
    "link": "https://arxiv.org/abs/2312.01998",
    "context": "Title: Language-only Efficient Training of Zero-shot Composed Image Retrieval\nAbstract: arXiv:2312.01998v2 Announce Type: replace-cross  Abstract: Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, ",
    "path": "papers/23/12/2312.01998.json",
    "total_tokens": 921,
    "translated_title": "仅使用语言高效训练的零样本组合图像检索",
    "translated_abstract": "组合图像检索（CIR）任务涉及对图像和文本的组合查询，旨在搜索符合这两个条件的相关图像。传统的CIR方法需要一个训练数据集，其中包含查询图像、查询文本和目标图像的三元组，这在收集过程中非常昂贵。最近的一些研究致力于采用零样本（ZS）CIR范式，以解决在没有使用预先收集的三元组的情况下的问题。然而，现有的ZS-CIR方法在训练过程中由于输入文本多样性的缺乏，显示出了有限的主干可扩展性和普适性。我们提出了一种新颖的CIR框架，仅使用语言进行训练。我们的LinCIR（Language-only training for CIR）可以通过一种名为自遮蔽投影（SMP）的新型自监督方法，仅使用文本数据集进行训练。我们将文本潜在嵌入投影到令牌嵌入空间，并通过替换原始文本的关键字令牌构造一个新文本。",
    "tldr": "提出了仅使用语言进行训练的零样本组合图像检索框架LinCIR，并引入自遮蔽投影（SMP）自监督方法，实现了在训练过程中仅使用文本数据集。",
    "en_tdlr": "Proposed a novel CIR framework LinCIR that only uses language for training, introducing a self-supervision method named self-masking projection (SMP) to train solely on text datasets."
}