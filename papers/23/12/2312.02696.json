{
    "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
    "abstract": "arXiv:2312.02696v2 Announce Type: replace-cross  Abstract: Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential mov",
    "link": "https://arxiv.org/abs/2312.02696",
    "context": "Title: Analyzing and Improving the Training Dynamics of Diffusion Models\nAbstract: arXiv:2312.02696v2 Announce Type: replace-cross  Abstract: Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential mov",
    "path": "papers/23/12/2312.02696.json",
    "total_tokens": 837,
    "translated_title": "分析和改进扩散模型的训练动力学",
    "translated_abstract": "扩散模型目前在数据驱动图像合成领域占据主导地位，其对大规模数据集的无与伦比的扩展能力。本文在不改变其高级结构的前提下，识别并纠正了流行的ADM扩散模型架构中导致不均匀和低效训练的几个原因。观察到在训练过程中网络激活和权重的不受控制的幅度变化和不平衡，我们重新设计了网络层以保持期望上的激活、权重和更新幅度。我们发现，系统应用这一理念消除了观察到的漂移和不平衡，导致相当更好的网络在等效的计算复杂性下。我们的修改将之前在ImageNet-512合成中的记录FID从2.41改进到了1.81，采用了快速确定性采样实现。",
    "tldr": "通过重新设计网络层来保持期望的激活、权重和更新幅度，消除了扩散模型中观察到的漂移和不平衡，从而在相同的计算复杂性下获得了更好的网络性能。",
    "en_tdlr": "Redesigning network layers to preserve expected activation, weight, and update magnitudes effectively eliminates drifts and imbalances observed in diffusion models, leading to significantly improved network performance at equal computational complexity."
}