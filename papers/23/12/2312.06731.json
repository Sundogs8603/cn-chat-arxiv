{
    "title": "Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator",
    "abstract": "arXiv:2312.06731v2 Announce Type: replace-cross  Abstract: Instruction tuning data is essential for training the Multimodal Large Language Models (MLLMs). However, the creation of high-quality instruction tuning data presents significant challenges. Prior methods that depended on GPT-4 for data generation were not only costly but also lacked satisfactory performance in complex tasks (i.e., grounding-based reasoning tasks). To address these issues, we developed an innovative data generation pipeline, Genixer, to generate various high-quality instruction tuning data, including nine representative tasks, e.g., Common VQA, REC, REG, and PointQ. Specifically, Genixer provides a unified solution with four key steps for alleviating the difficulty of data generation: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLM, and (iv) data generation and filtering. Subsequently, the superior qualitative results of our Genixer demonstrate that current MLLMs have a ",
    "link": "https://arxiv.org/abs/2312.06731",
    "context": "Title: Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator\nAbstract: arXiv:2312.06731v2 Announce Type: replace-cross  Abstract: Instruction tuning data is essential for training the Multimodal Large Language Models (MLLMs). However, the creation of high-quality instruction tuning data presents significant challenges. Prior methods that depended on GPT-4 for data generation were not only costly but also lacked satisfactory performance in complex tasks (i.e., grounding-based reasoning tasks). To address these issues, we developed an innovative data generation pipeline, Genixer, to generate various high-quality instruction tuning data, including nine representative tasks, e.g., Common VQA, REC, REG, and PointQ. Specifically, Genixer provides a unified solution with four key steps for alleviating the difficulty of data generation: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLM, and (iv) data generation and filtering. Subsequently, the superior qualitative results of our Genixer demonstrate that current MLLMs have a ",
    "path": "papers/23/12/2312.06731.json",
    "total_tokens": 874,
    "translated_title": "Genixer：将多模态大语言模型赋能为强大的数据生成器",
    "translated_abstract": "arXiv:2312.06731v2 公告类型: 替换-交叉  摘要: 调优数据对于训练多模态大语言模型(MLLMs)至关重要。然而，高质量调优数据的创建面临重大挑战。先前依赖于GPT-4进行数据生成的方法不仅成本高昂，而且在复杂任务（即基于理解的推理任务）中的性能也不尽如人意。为解决这些问题，我们开发了一种创新的数据生成流水线Genixer，用于生成各种高质量的调优数据，包括九个代表性任务，例如常见的VQA，REC，REG和PointQ。具体而言，Genixer提供了一个统一的解决方案，包括四个关键步骤来缓解数据生成的困难：(i)指导数据收集，(ii)指导模板设计，(iii)赋能MLLM，和(iv)数据生成和过滤。随后，我们的Genixer的卓越的定性结果表明，当前的MLLM具有...",
    "tldr": "Genixer是一种为生成高质量指导数据而设计的创新数据生成管道，包括九个代表性任务，提供了四个关键步骤来改善数据生成的难度。"
}