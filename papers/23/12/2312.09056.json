{
    "title": "ReCoRe: Regularized Contrastive Representation Learning of World Model",
    "abstract": "arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\\\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc",
    "link": "https://arxiv.org/abs/2312.09056",
    "context": "Title: ReCoRe: Regularized Contrastive Representation Learning of World Model\nAbstract: arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\\\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc",
    "path": "papers/23/12/2312.09056.json",
    "total_tokens": 816,
    "translated_title": "ReCoRe: 正则化对比度表示学习的世界模型",
    "translated_abstract": "近期的无模型强化学习（RL）方法在游戏环境中已经展示出与人类水平相当的有效性，但在视觉导航等日常任务中的成功受到限制，特别是在出现显著外观变化的情况下。为了解决这些挑战，我们提出了一种世界模型，通过（i）对比度无监督学习和（ii）介入不变正则化来学习不变特征。学习世界动力学的显式表示，即世界模型，提高了样本效率，而对比度学习隐含地强化了学习不变特征，改善了泛化性能。然而，简单地将对比度损失集成到世界模型中是不够的，因为基于世界模型的RL方法独立优化表示学习和智能体策略。",
    "tldr": "通过正则化对比度表示学习世界模型，该方法提高了样本效率和泛化性能，解决了在视觉导航等日常任务中出现外观变化时的挑战。",
    "en_tdlr": "By regularizing contrastive representation learning of a world model, this method improves sample efficiency and generalization, addressing challenges in everyday tasks like visual navigation under appearance variations."
}