{
    "title": "On Diversified Preferences of Large Language Model Alignment",
    "abstract": "arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \\textit{Harmless\\&Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance ",
    "link": "https://arxiv.org/abs/2312.07401",
    "context": "Title: On Diversified Preferences of Large Language Model Alignment\nAbstract: arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \\textit{Harmless\\&Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance ",
    "path": "papers/23/12/2312.07401.json",
    "total_tokens": 871,
    "translated_title": "关于大型语言模型对齐多样化偏好的研究",
    "translated_abstract": "将大型语言模型（LLMs）与人类偏好对齐被认为是提高LLMs交互质量的关键。然而，在这个多元化的世界中，由于标注者的不同偏好，人类偏好可能会多样化，这阻碍了LLM对齐方法的有效性。本文首次对常用人类反馈数据集进行定量分析，以研究多样化偏好对奖励建模的影响。我们的分析揭示了奖励模型（RMs）的校准性能与LLMs的对齐性能之间的相关性。我们发现多样化偏好数据对人类共享偏好（如“无害和有帮助”）上的奖励模型的校准性能产生负面影响，从而损害了LLMs的对齐性能。为解决这种无效性，我们提出了一种新颖的多目标奖励学习方法（MORE）以增强校准性能。",
    "tldr": "本文通过定量分析常用人类反馈数据集，揭示了多样化偏好对奖励建模的影响，提出了一种新颖的多目标奖励学习方法以增强校准性能",
    "en_tdlr": "This paper presents a quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling, and proposes a novel Multi-Objective Reward learning method to enhance calibration performance."
}