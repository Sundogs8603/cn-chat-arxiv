{
    "title": "T-Eval: Evaluating the Tool Utilization Capability Step by Step. (arXiv:2312.14033v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, prov",
    "link": "http://arxiv.org/abs/2312.14033",
    "context": "Title: T-Eval: Evaluating the Tool Utilization Capability Step by Step. (arXiv:2312.14033v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, prov",
    "path": "papers/23/12/2312.14033.json",
    "total_tokens": 878,
    "translated_title": "T-Eval: 逐步评估工具利用能力",
    "translated_abstract": "大型语言模型（LLM）在各种NLP任务上取得了卓越的性能，并通过工具进行了更广泛的应用。然而，如何评估和分析LLM的工具利用能力仍未充分探索。与以往评估模型整体性能的工作不同，我们将工具利用全面分解为多个子过程，包括指令跟随、规划、推理、检索、理解和复查。在此基础上，我们进一步引入了T-Eval来逐步评估工具利用能力。T-Eval将工具利用评估解耦为多个子领域，有助于对LLM的整体和独立能力进行内部理解。我们对T-Eval进行了大量实验和各种LLM的深入分析。T-Eval不仅展现了与结果导向评估的一致性，还提供了对LLM能力更细致的分析，表明LLM具备了一定的能力。",
    "tldr": "T-Eval是一种逐步评估工具利用能力的方法，它将工具利用评估解耦为多个子领域，从而能够更细致地分析大型语言模型（LLM）的能力。",
    "en_tdlr": "T-Eval is a method for evaluating the tool utilization capability step by step, by decomposing the evaluation into multiple sub-domains, allowing for a more fine-grained analysis of the capabilities of large language models (LLMs)."
}