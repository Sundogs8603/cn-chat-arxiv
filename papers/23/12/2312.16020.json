{
    "title": "Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)",
    "abstract": "In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m",
    "link": "http://arxiv.org/abs/2312.16020",
    "context": "Title: Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)\nAbstract: In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m",
    "path": "papers/23/12/2312.16020.json",
    "total_tokens": 988,
    "translated_title": "基于梯度采样优化的残差神经网络的鲁棒剪枝",
    "translated_abstract": "在本研究中，我们探讨了一种创新的神经网络优化方法，重点是在剪枝过程中应用梯度采样技术，类似于StochGradAdam中的技术。我们的主要目标是在资源有限的场景中保持剪枝模型的高准确性水平，这是一个关键性挑战。我们的广泛实验表明，采用梯度采样技术优化的模型在剪枝过程中比使用传统优化方法更有效地保持准确性。这一发现强调了梯度采样在促进鲁棒学习和使网络在复杂度大大降低后仍能保留关键信息方面的重要性。我们通过各种数据集和神经架构验证了我们的方法，展示了它的广泛适用性和效果。该论文还深入探讨了理论方面，解释了梯度采样技术如何增强模型的鲁棒性。",
    "tldr": "本研究通过在剪枝过程中应用梯度采样技术，实现了在资源有限的场景中保持高准确性水平的鲁棒剪枝。相关实验证明，采用梯度采样技术进行优化的模型比传统优化方法更能有效地保持准确性。这一创新方法在各种数据集和神经架构上得到了验证，并且理论上解释了梯度采样技术对模型鲁棒性的贡献。",
    "en_tdlr": "In this study, we propose a robust neural pruning method using gradient sampling optimization, which achieves high accuracy levels in resource-limited scenarios. Experimental results demonstrate that models optimized with gradient sampling techniques outperform traditional methods in accuracy preservation during pruning. The approach is validated on various datasets and neural architectures, with theoretical explanations on the contribution of gradient sampling to model robustness."
}