{
    "title": "Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning",
    "abstract": "arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''",
    "link": "https://arxiv.org/abs/2312.10385",
    "context": "Title: Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning\nAbstract: arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''",
    "path": "papers/23/12/2312.10385.json",
    "total_tokens": 819,
    "translated_title": "模仿好的并避免坏的：安全强化学习的增量方法",
    "translated_abstract": "在强化学习（RL）中执行安全动作的流行框架是约束RL，其中利用基于轨迹的成本约束（或其他成本度量）来执行安全操作，更重要的是在最大化期望奖励的同时执行这些约束。最近解决约束RL的方法将基于轨迹的成本约束转换为一个替代问题，可以通过对RL方法进行轻微修改来解决。这类方法的一个主要缺点是在每个状态上对成本约束进行过度或不足估计。因此，我们提供了一种方法，不修改基于轨迹的成本约束，而是模仿“好”轨迹并避免从逐步改进的策略生成的“坏”轨迹。我们使用一个oracle，利用奖励阈值（随学习变化）和整体成本约束来将轨迹标记为“好”或“坏”。",
    "tldr": "提出了一种不修改基于轨迹成本约束的方法，在安全强化学习中通过模仿好的轨迹和避免坏的轨迹来改进策略。",
    "en_tdlr": "Proposed an approach that does not modify trajectory-based cost constraints and improves policies in safe reinforcement learning by imitating good trajectories and avoiding bad ones."
}