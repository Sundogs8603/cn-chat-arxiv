{
    "title": "Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we investigate the expressivity and approximation properties of deep neural networks employing the ReLU$^k$ activation function for $k \\geq 2$. Although deep ReLU networks can approximate polynomials effectively, deep ReLU$^k$ networks have the capability to represent higher-degree polynomials precisely. Our initial contribution is a comprehensive, constructive proof for polynomial representation using deep ReLU$^k$ networks. This allows us to establish an upper bound on both the size and count of network parameters. Consequently, we are able to demonstrate a suboptimal approximation rate for functions from Sobolev spaces as well as for analytic functions. Additionally, through an exploration of the representation power of deep ReLU$^k$ networks for shallow networks, we reveal that deep ReLU$^k$ networks can approximate functions from a range of variation spaces, extending beyond those generated solely by the ReLU$^k$ activation function. This finding demonstrates the ad",
    "link": "http://arxiv.org/abs/2312.16483",
    "context": "Title: Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v2 [cs.LG] UPDATED)\nAbstract: In this paper, we investigate the expressivity and approximation properties of deep neural networks employing the ReLU$^k$ activation function for $k \\geq 2$. Although deep ReLU networks can approximate polynomials effectively, deep ReLU$^k$ networks have the capability to represent higher-degree polynomials precisely. Our initial contribution is a comprehensive, constructive proof for polynomial representation using deep ReLU$^k$ networks. This allows us to establish an upper bound on both the size and count of network parameters. Consequently, we are able to demonstrate a suboptimal approximation rate for functions from Sobolev spaces as well as for analytic functions. Additionally, through an exploration of the representation power of deep ReLU$^k$ networks for shallow networks, we reveal that deep ReLU$^k$ networks can approximate functions from a range of variation spaces, extending beyond those generated solely by the ReLU$^k$ activation function. This finding demonstrates the ad",
    "path": "papers/23/12/2312.16483.json",
    "total_tokens": 928,
    "translated_title": "ReLU$^k$激活的深度神经网络的表达能力和近似性质",
    "translated_abstract": "本文研究了采用ReLU$^k$激活函数（$k \\geq 2$）的深度神经网络的表达能力和近似性质。虽然深层ReLU网络可以有效地逼近多项式，但深层ReLU$^k$网络能够精确表示高阶多项式。我们的初步贡献是通过全面、构造性的证明，证明了深层ReLU$^k$网络可以表示多项式。这使我们能够建立网络参数的大小和数量的上界。因此，我们能够证明Sobolev空间中的函数以及解析函数的次优逼近率。此外，通过研究深层ReLU$^k$网络对浅层网络的表示能力，我们发现深层ReLU$^k$网络能够逼近一定范围内的变化空间中的函数，扩展了仅由ReLU$^k$激活函数生成的空间。这一发现证明了深层ReLU$^k$网络的广泛表示能力。",
    "tldr": "本文研究了采用ReLU$^k$激活函数的深度神经网络的表达能力和近似性质，通过全面、构造性的证明，证明了深层ReLU$^k$网络可以精确表示高阶多项式，并建立了网络参数的上界。此外，发现深层ReLU$^k$网络可以逼近多种变化空间中的函数，扩展了仅由ReLU$^k$激活函数生成的空间。"
}