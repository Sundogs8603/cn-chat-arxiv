{
    "title": "Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning",
    "abstract": "arXiv:2312.07250v2 Announce Type: replace-cross  Abstract: We conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three subtasks including 1) clinical case (CC), 2) clinical terminology (CT), and 3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method wor",
    "link": "https://arxiv.org/abs/2312.07250",
    "context": "Title: Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning\nAbstract: arXiv:2312.07250v2 Announce Type: replace-cross  Abstract: We conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three subtasks including 1) clinical case (CC), 2) clinical terminology (CT), and 3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method wor",
    "path": "papers/23/12/2312.07250.json",
    "total_tokens": 958,
    "translated_title": "临床文本的神经机器翻译：对多语言预训练语言模型和迁移学习的经验研究",
    "translated_abstract": "我们通过检验使用基于深度学习的多语言神经网络模型，如基于Transformer结构的模型，在临床文本机器翻译方面进行了调查。此外，为了解决语言资源不平衡问题，我们还使用基于大规模多语言预训练语言模型（MMPLMs）的迁移学习方法进行了实验。在临床案例（CC）、临床术语（CT）和本体概念（OC）等三个子任务上的实验结果显示，我们的模型在ClinSpEn-2022英西临床领域数据共享任务中取得了顶级性能。此外，我们基于专家的人工评估显示，在临床领域微调中，小型预训练语言模型（PLM）明显胜过其他两个超大型语言模型，这一发现在该领域从未有过报道。最后，迁移学习方法表现出",
    "tldr": "在临床文本的神经机器翻译研究中，通过使用多语言预训练语言模型和迁移学习方法，在ClinSpEn-2022英西临床领域数据上取得了顶级性能，并发现小型预训练语言模型在临床领域微调中胜过其他超大型语言模型。",
    "en_tdlr": "In the research of neural machine translation for clinical text, top-level performances were achieved on ClinSpEn-2022 English-Spanish clinical domain data through the use of multilingual pre-trained language models and transfer learning, with the discovery that small-sized pre-trained language models outperformed other extra-large models in clinical domain fine-tuning."
}