{
    "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
    "abstract": "arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \\textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K and 28.6\\%$\\to$33.0\\% on MATH). The",
    "link": "https://arxiv.org/abs/2312.08935",
    "context": "Title: Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nAbstract: arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \\textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K and 28.6\\%$\\to$33.0\\% on MATH). The",
    "path": "papers/23/12/2312.08935.json",
    "total_tokens": 920,
    "translated_title": "Math-Shepherd: 在不需要人工标注的情况下逐步验证和加强LLMs",
    "translated_abstract": "在这篇论文中，我们提出了一种名为Math-Shepherd的创新过程导向数学奖励模型，为数学问题解决的每一步分配奖励分数。Math-Shepherd的训练是使用自动构建的基于过程的监督数据完成的，打破了现有工作中对手动标注的严重依赖瓶颈。我们探讨了Math-Shepherd在两种场景中的有效性：1）\\textit{验证}：利用Math-Shepherd对大型语言模型(LLMs)生成的多个输出进行重新排序；2）\\textit{强化学习}：使用Math-Shepherd通过逐步的近端策略优化(PPO)加强LLMs。通过Math-Shepherd，一系列开源LLMs展现出卓越的性能。例如，使用Math-Shepherd的逐步PPO显著提高了Mistral-7B的准确率(GSM8K由77.9%提高到84.1%，MATH由28.6%提高到33.0%)",
    "tldr": "Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。",
    "en_tdlr": "Math-Shepherd introduces a new mathematical reward model that achieves step-by-step verification and reinforcement for LLMS using automatically generated process-wise supervision data, significantly improving the accuracy of mathematical problem solutions."
}