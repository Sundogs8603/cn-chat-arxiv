{
    "title": "Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes",
    "abstract": "arXiv:2312.08519v2 Announce Type: replace-cross  Abstract: It has been proposed that, when processing a stream of events, humans divide their experiences in terms of inferred latent causes (LCs) to support context-dependent learning. However, when shared structure is present across contexts, it is still unclear how the \"splitting\" of LCs and learning of shared structure can be simultaneously achieved. Here, we present the Latent Cause Network (LCNet), a neural network model of LC inference. Through learning, it naturally stores structure that is shared across tasks in the network weights. Additionally, it represents context-specific structure using a context module, controlled by a Bayesian nonparametric inference algorithm, which assigns a unique context vector for each inferred LC. Across three simulations, we found that LCNet could 1) extract shared structure across LCs in a function learning task while avoiding catastrophic interference, 2) capture human data on curriculum effects ",
    "link": "https://arxiv.org/abs/2312.08519",
    "context": "Title: Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes\nAbstract: arXiv:2312.08519v2 Announce Type: replace-cross  Abstract: It has been proposed that, when processing a stream of events, humans divide their experiences in terms of inferred latent causes (LCs) to support context-dependent learning. However, when shared structure is present across contexts, it is still unclear how the \"splitting\" of LCs and learning of shared structure can be simultaneously achieved. Here, we present the Latent Cause Network (LCNet), a neural network model of LC inference. Through learning, it naturally stores structure that is shared across tasks in the network weights. Additionally, it represents context-specific structure using a context module, controlled by a Bayesian nonparametric inference algorithm, which assigns a unique context vector for each inferred LC. Across three simulations, we found that LCNet could 1) extract shared structure across LCs in a function learning task while avoiding catastrophic interference, 2) capture human data on curriculum effects ",
    "path": "papers/23/12/2312.08519.json",
    "total_tokens": 909,
    "translated_title": "在神经网络模型中协调共享与特定上下文信息对潜在因果的作用",
    "translated_abstract": "已经提出，当处理一系列事件时，人类会根据推断的潜在因果（LCs）来划分他们的经验，以支持依赖于上下文的学习。然而，当共享结构存在于不同上下文中时，如何同时实现LCs的“分裂”和学习共享结构仍不清楚。本文介绍了潜在因果网络（LCNet），这是一个LC推断的神经网络模型。通过学习，它自然地储存网络权重中跨任务共享的结构。此外，它利用一个上下文模块表示特定上下文结构，由贝叶斯非参数推理算法控制，为每个推断的LC分配一个唯一的上下文向量。通过三个模拟实验，我们发现LCNet能够1)在功能学习任务中提取跨LC的共享结构，同时避免灾难性干扰，2)捕捉关于课程效应的人类数据。",
    "tldr": "LCNet是一个神经网络模型，能够通过学习存储共享结构，同时使用上下文模块表示特定上下文结构，成功实现了在不同任务中提取共享结构并避免灾难性干扰的功能；并且能够捕捉人类数据中的课程效应。"
}