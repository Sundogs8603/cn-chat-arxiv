{
    "title": "Class-Aware Pruning for Efficient Neural Networks",
    "abstract": "arXiv:2312.05875v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in various fields. However, the large number of floating-point operations (FLOPs) in DNNs poses challenges for their deployment in resource-constrained applications, e.g., edge devices. To address the problem, pruning has been introduced to reduce the computational cost in executing DNNs. Previous pruning strategies are based on weight values, gradient values and activation outputs. Different from previous pruning solutions, in this paper, we propose a class-aware pruning technique to compress DNNs, which provides a novel perspective to reduce the computational cost of DNNs. In each iteration, the neural network training is modified to facilitate the class-aware pruning. Afterwards, the importance of filters with respect to the number of classes is evaluated. The filters that are only important for a few number of classes are removed. The neural network is then retraine",
    "link": "https://arxiv.org/abs/2312.05875",
    "context": "Title: Class-Aware Pruning for Efficient Neural Networks\nAbstract: arXiv:2312.05875v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in various fields. However, the large number of floating-point operations (FLOPs) in DNNs poses challenges for their deployment in resource-constrained applications, e.g., edge devices. To address the problem, pruning has been introduced to reduce the computational cost in executing DNNs. Previous pruning strategies are based on weight values, gradient values and activation outputs. Different from previous pruning solutions, in this paper, we propose a class-aware pruning technique to compress DNNs, which provides a novel perspective to reduce the computational cost of DNNs. In each iteration, the neural network training is modified to facilitate the class-aware pruning. Afterwards, the importance of filters with respect to the number of classes is evaluated. The filters that are only important for a few number of classes are removed. The neural network is then retraine",
    "path": "papers/23/12/2312.05875.json",
    "total_tokens": 828,
    "translated_title": "面向类别的修剪以提高神经网络的效率",
    "translated_abstract": "深度神经网络在各个领域展示出了显著的成功。然而，DNN中的大量浮点运算(FLOPs)在资源受限的应用(如边缘设备)中部署时会带来挑战。为解决这一问题，引入了修剪来减少执行DNN时的计算成本。与先前的修剪策略不同，本文提出了一种面向类别的修剪技术来压缩DNN，为减少DNN的计算成本提供了一种新的视角。在每次迭代中，神经网络训练被修改以实现面向类别的修剪。然后，评估了与类别数量相关的滤波器的重要性。针对只对少数类别重要的滤波器进行移除。随后神经网络被重新训练。",
    "tldr": "本文提出了一种面向类别的修剪技术，与先前的修剪策略不同，该技术通过评估与类别数量相关的滤波器重要性来压缩DNN，从而减少计算成本。",
    "en_tdlr": "This paper introduces a class-aware pruning technique that differs from previous methods by evaluating the importance of filters with respect to the number of classes, aiming to compress DNNs and reduce computational cost."
}