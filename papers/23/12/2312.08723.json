{
    "title": "StemGen: A music generation model that listens. (arXiv:2312.08723v2 [cs.SD] UPDATED)",
    "abstract": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
    "link": "http://arxiv.org/abs/2312.08723",
    "context": "Title: StemGen: A music generation model that listens. (arXiv:2312.08723v2 [cs.SD] UPDATED)\nAbstract: End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
    "path": "papers/23/12/2312.08723.json",
    "total_tokens": 851,
    "translated_title": "StemGen: 一个能够听音乐并生成的音乐生成模型",
    "translated_abstract": "最近，使用深度学习技术进行音乐音频的端到端生成活动非常活跃。然而，大多数模型集中在根据抽象的条件信息生成完全混合的音乐。在这项工作中，我们提出了一种替代范式，用于产生能够听音乐并回应音乐环境的音乐生成模型。我们描述了如何使用非自回归的基于Transformer的模型架构构建这样的模型，并提出了一些新颖的架构和采样改进。我们使用一个开源数据集和一个专有数据集来训练所描述的架构。我们使用标准的质量度量和基于音乐信息检索描述符的新方法来评估生成的模型。结果模型达到了最先进的文本条件模型的音频质量，并且在音乐连贯性方面表现出色。",
    "tldr": "该论文介绍了一种能够听音乐并生成音乐的模型。采用了非自回归的Transformer模型架构以及一些新颖的架构和采样改进方法。该模型能够达到最先进的文本条件模型的音频质量，并在音乐连贯性方面表现出色。"
}