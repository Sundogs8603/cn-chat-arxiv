{
    "title": "Symmetry Breaking and Equivariant Neural Networks",
    "abstract": "arXiv:2312.09016v2 Announce Type: replace  Abstract: Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.",
    "link": "https://arxiv.org/abs/2312.09016",
    "context": "Title: Symmetry Breaking and Equivariant Neural Networks\nAbstract: arXiv:2312.09016v2 Announce Type: replace  Abstract: Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.",
    "path": "papers/23/12/2312.09016.json",
    "total_tokens": 791,
    "translated_title": "对称破缺和等变神经网络",
    "translated_abstract": "在深度学习中使用对称作为归纳偏差已被证明是一种有效的方法，可以设计出高效的模型。然而，神经网络中对称和等变性的关系并不总是显而易见。本文分析了等变函数中出现的一个关键限制：它们不能在单个数据样本的层面打破对称。为此，我们引入了一个新颖的“放松等变性”的概念来规避这个限制。我们进一步展示了如何将这种放松引入等变多层感知机（E-MLP），提供了一种替代注入噪声的方法。接着讨论了对称破缺在物理、图表示学习、组合优化和等变解码等各种应用领域的相关性。",
    "tldr": "提出了一种新颖的“放松等变性”的概念，用于解决等变函数无法在单个数据样本层面打破对称的限制，并展示了如何将其应用于等变多层感知机（E-MLP）中。",
    "en_tdlr": "Introduced a novel concept of \"relaxed equivariance\" to address the limitation of equivariant functions in breaking symmetry at the individual data sample level, and demonstrated its application in equivariant multilayer perceptrons (E-MLPs)."
}