{
    "title": "GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations",
    "abstract": "arXiv:2312.10913v2 Announce Type: replace-cross  Abstract: Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the \"power-term approximator block\", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to ",
    "link": "https://arxiv.org/abs/2312.10913",
    "context": "Title: GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations\nAbstract: arXiv:2312.10913v2 Announce Type: replace-cross  Abstract: Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the \"power-term approximator block\", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to ",
    "path": "papers/23/12/2312.10913.json",
    "total_tokens": 835,
    "translated_title": "GINN-LP：一种用于发现多元Laurent多项式方程的可解释性神经网络",
    "translated_abstract": "传统机器学习通常被视为一个黑盒优化问题，不会产生将输入和输出连接起来的可解释性函数。然而，发现这种可解释性函数的能力是可取的。在这项工作中，我们提出了GINN-LP，一种可解释的神经网络，用于发现数据集的基础方程的形式和系数，当假设方程的形式是多元Laurent多项式时。这是通过一种新的可解释性神经网络块，名为“幂项逼近块”，由对数和指数激活函数组成来实现的。GINN-LP是端到端可微分的，可以使用反向传播进行训练。我们提出了一种神经网络增长策略，能够找到代表数据的Laurent多项式中的合适项数，同时还提出了稀疏正则化方法来优化方程的稀疏性。",
    "tldr": "GINN-LP是一种可解释的神经网络，用于发现多元Laurent多项式方程的形式和系数。它采用了一种名为“幂项逼近块”的新型可解释性神经网络块，并通过神经网络增长策略和稀疏正则化来优化方程的表示。"
}