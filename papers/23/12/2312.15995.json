{
    "title": "Generalization in Kernel Regression Under Realistic Assumptions",
    "abstract": "arXiv:2312.15995v2 Announce Type: replace-cross  Abstract: It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the ker",
    "link": "https://arxiv.org/abs/2312.15995",
    "context": "Title: Generalization in Kernel Regression Under Realistic Assumptions\nAbstract: arXiv:2312.15995v2 Announce Type: replace-cross  Abstract: It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the ker",
    "path": "papers/23/12/2312.15995.json",
    "total_tokens": 894,
    "translated_title": "在现实假设下的核回归中的泛化",
    "translated_abstract": "现在已经确立的事实是，现代过度参数化模型似乎能够逃避偏差-方差权衡，在过度拟合噪音的情况下泛化良好。许多最近的研究尝试分析这一现象在核回归相对易处理的设置中。然而，正如我们详细讨论的那样，大多数关于这个主题的过去的研究要么做出了不切实际的假设，要么专注于一个狭窄的问题设置。本文旨在提供一个统一的理论来限制几乎所有常见和现实设置下核回归的超出风险。具体来说，我们提供了对于常见核函数以及任意的正则化量、噪声、任意输入维度和任意样本数都成立的严格界限。此外，我们还为核矩阵的特征值提供了相对扰动界限，这可能具有独立的重要性。这些界限揭示了一种自我正则化现象，即核分解的特征值中存在重尾现象。",
    "tldr": "本文提供了一个统一的理论，用于对几乎所有常见和现实设置下的核回归的超出风险进行上限约束，并揭示了核分解中存在的自我正则化现象。",
    "en_tdlr": "This paper provides a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings, revealing a self-regularization phenomenon in the eigendecomposition of kernel matrices."
}