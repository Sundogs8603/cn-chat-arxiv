{
    "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
    "abstract": "arXiv:2312.10730v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generalit",
    "link": "https://arxiv.org/abs/2312.10730",
    "context": "Title: Mixed Distillation Helps Smaller Language Model Better Reasoning\nAbstract: arXiv:2312.10730v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generalit",
    "path": "papers/23/12/2312.10730.json",
    "total_tokens": 883,
    "translated_title": "混合蒸馏有助于较小的语言模型更好地推理",
    "translated_abstract": "虽然大型语言模型(LLMs)在最近的自然语言处理(NLP)任务中表现出了异常的性能，但由于在真实应用中的高计算和内存需求，它们的部署面临着重大挑战。最近的研究集中于通过从LLMs蒸馏知识来增强较小模型，在特定任务中取得了令人满意的结果。然而，这些模型在特别需要推理的任务中往往难以与LLMs的性能匹敌。在这项工作中，我们介绍了混合蒸馏(MD)框架，该框架利用了LLMs中的Program of Thought (PoT)和Chain of Thought (CoT)能力的优势，结合多种提示技术，并将这些能力蒸馏到较小的模型中。我们的实验结果表明，MD显著增强了较小模型在各种任务中的单路径和多路径推理能力。",
    "tldr": "混合蒸馏(MD)框架结合了LLMs中的Program of Thought (PoT)和Chain of Thought (CoT)能力，将多种提示技术蒸馏到较小模型中，显著增强了较小模型在各种任务中的推理能力。"
}