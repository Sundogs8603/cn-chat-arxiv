{
    "title": "Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v2 [cs.SD] UPDATED)",
    "abstract": "Transformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in low-resource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks,~\\textit{e.g.}, event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduc",
    "link": "http://arxiv.org/abs/2312.16228",
    "context": "Title: Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v2 [cs.SD] UPDATED)\nAbstract: Transformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in low-resource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks,~\\textit{e.g.}, event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduc",
    "path": "papers/23/12/2312.16228.json",
    "total_tokens": 859,
    "translated_title": "可变形音频Transformer用于音频事件检测",
    "translated_abstract": "Transformer 在各种任务中取得了有希望的结果。然而，自我注意计算的二次复杂性限制了应用的范围，特别是在低资源环境和移动设备或边缘设备中。现有工作已经提出利用手工制作的注意力模式来减少计算复杂性。然而，这种手工制作的模式是数据无关的，可能不是最优的。因此，有可能减少了相关的键或值，而重要性较低的键或值仍然保留。基于这个关键洞察力，我们提出了一种新颖的用于音频识别的可变形音频Transformer，命名为DATAR，其中构建了一个具有金字塔Transformer骨架的可变形注意力，并且可学习。这样的架构在预测任务中已被证明是有效的，例如事件分类。此外，我们发现可变形注意力图计算可能过于简化输入特征，可以进一步增强。",
    "tldr": "该论文提出了一种新颖的可变形音频Transformer，命名为DATAR，用于音频事件检测。通过构建一个具有金字塔Transformer骨架的可变形注意力，该模型在预测任务中取得了有效的结果。研究还发现可变形注意力图计算可能过于简化输入特征，提出了进一步增强特征的方法。"
}