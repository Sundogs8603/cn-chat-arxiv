{
    "title": "TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)",
    "abstract": "Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro",
    "link": "http://arxiv.org/abs/2312.10467",
    "context": "Title: TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)\nAbstract: Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro",
    "path": "papers/23/12/2312.10467.json",
    "total_tokens": 893,
    "translated_title": "TrojFST: 将特洛伊木马嵌入到少样本提示调优中",
    "translated_abstract": "提示调优已经成为一种非常有效的方法，用于适应使用有限输入样本的预训练语言模型（PLM）来处理新的自然语言处理任务。然而，提示调优的成功导致对手试图针对该技术进行后门攻击。之前基于提示的后门攻击在少样本提示调优方面面临挑战，需要全模型微调或大规模训练数据集。我们观察到使用少样本提示调优构建基于提示的后门的困难，这涉及冻结PLM并在一组受限制的输入样本上调优软提示。这种方法引入了一个不平衡的污染数据集，容易过拟合并且缺乏注意力感知。为了解决这些挑战，我们在少样本提示调优框架中引入了TrojFST用于后门攻击。TrojFST包括三个模块：平衡的污染学习、选择性令牌污染和...",
    "tldr": "TrojFST是一种在少样本提示调优框架中进行后门攻击的方法，通过引入平衡的污染学习、选择性令牌污染和...等模块来解决构建基于提示的后门的困难。",
    "en_tdlr": "TrojFST is a method for conducting backdoor attacks within the framework of few-shot prompt-tuning, addressing the challenges of constructing prompt-based backdoors by introducing modules like balanced poison learning and selective token poisoning."
}