{
    "title": "LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization",
    "abstract": "arXiv:2312.14949v2 Announce Type: replace-cross  Abstract: With the advent of large language models (LLMs) like GPT-3, a natural question is the extent to which these models can be utilized for source code optimization. This paper presents methodologically stringent case studies applied to well-known open source python libraries pillow and numpy. We find that contemporary LLM ChatGPT-4 (state September and October 2023) is surprisingly adept at optimizing energy and compute efficiency. However, this is only the case in interactive use, with a human expert in the loop. Aware of experimenter bias, we document our qualitative approach in detail, and provide transcript and source code. We start by providing a detailed description of our approach in conversing with the LLM to optimize the _getextrema function in the pillow library, and a quantitative evaluation of the performance improvement. To demonstrate qualitative replicability, we report further attempts on another locus in the pillow",
    "link": "https://arxiv.org/abs/2312.14949",
    "context": "Title: LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization\nAbstract: arXiv:2312.14949v2 Announce Type: replace-cross  Abstract: With the advent of large language models (LLMs) like GPT-3, a natural question is the extent to which these models can be utilized for source code optimization. This paper presents methodologically stringent case studies applied to well-known open source python libraries pillow and numpy. We find that contemporary LLM ChatGPT-4 (state September and October 2023) is surprisingly adept at optimizing energy and compute efficiency. However, this is only the case in interactive use, with a human expert in the loop. Aware of experimenter bias, we document our qualitative approach in detail, and provide transcript and source code. We start by providing a detailed description of our approach in conversing with the LLM to optimize the _getextrema function in the pillow library, and a quantitative evaluation of the performance improvement. To demonstrate qualitative replicability, we report further attempts on another locus in the pillow",
    "path": "papers/23/12/2312.14949.json",
    "total_tokens": 866,
    "translated_title": "LLM互动优化开源Python库--案例研究与概括",
    "translated_abstract": "随着大型语言模型（LLMs）如GPT-3的出现，一个自然的问题是这些模型在源代码优化中的利用程度。本文提出了方法论严谨的案例研究，应用于著名的开源Python库pillow和numpy。我们发现，当与人类专家互动时，当代LLM ChatGPT-4（截至2023年9月和10月）在优化能源和计算效率方面表现出惊人的灵活性。然而，这仅适用于互动使用，且需要人类专家协助。为了避免实验者偏见，我们详细记录了我们的定性方法，并提供了对话和源代码。我们首先详细描述了与LLM对话以优化pillow库中的_getextrema函数的方法，并量化评估了性能改进。为了展示定性可复制性，我们报告了在pillow的另一个位置上进一步尝试的情况。",
    "tldr": "本文研究了如何利用当代LLM ChatGPT-4来优化开源Python库，发现在与人类专家互动的情况下，该模型在优化能源和计算效率方面表现出惊人的灵活性。",
    "en_tdlr": "This paper investigates how to utilize contemporary LLM ChatGPT-4 to optimize open source Python libraries, finding that the model shows surprising flexibility in optimizing energy and compute efficiency when interacting with human experts."
}