{
    "title": "The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])",
    "abstract": "The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the ",
    "link": "http://arxiv.org/abs/2309.04198",
    "context": "Title: The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])\nAbstract: The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the ",
    "path": "papers/23/09/2309.04198.json",
    "total_tokens": 1033,
    "translated_title": "CALLA数据集：从中文医学文献中探索LLMs的交互式知识获取",
    "translated_abstract": "大型语言模型（LLMs）在医学领域的应用引起了研究人员的兴趣。最近的研究集中于通过医学知识图构建指导微调（IFT）数据，以丰富LLMs的交互式医学知识。然而，作为丰富的医学知识来源的医学文献仍未被开发利用。我们的工作引入了CALLA数据集，以探索LLMs从中国医学文献中获取交互式知识。它通过自由对话事实核查任务评估LLMs掌握医学知识的能力。我们发现一种现象称为“事实跟随响应”，LLMs倾向于确认问题中提到的事实，并对挑战这些事实表现出不情愿。为消除这种现象导致的不准确评估，对于黄金事实，我们从两个角度人工构建测试数据：一个与事实一致，一个与事实不一致。根据这些测试数据，我们为LLMs评估其对医学知识的掌握能力提供了更准确的方法。",
    "tldr": "该研究介绍了CALLA数据集，用于探索LLMs从中文医学文献中获取交互式知识。通过自由对话事实核查任务，评估了LLMs掌握医学知识的能力，并发现了一种称为“事实跟随响应”的现象。为了提供更准确的评估方法，人工构建了两种角度的测试数据：一种与事实一致，一种与事实不一致。",
    "en_tdlr": "This study introduces the CALLA dataset, which examines LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses their proficiency through a free-dialogue fact-checking task and discovers a phenomenon called the \"fact-following response.\" To provide a more accurate evaluation, test data is artificially constructed from two perspectives: one consistent with the fact and one inconsistent."
}