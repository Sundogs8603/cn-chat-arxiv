{
    "title": "Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])",
    "abstract": "Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi",
    "link": "http://arxiv.org/abs/2309.03426",
    "context": "Title: Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])\nAbstract: Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi",
    "path": "papers/23/09/2309.03426.json",
    "total_tokens": 926,
    "translated_title": "相等的长期效益率：将静态公平性概念应用到顺序决策中",
    "translated_abstract": "机器学习模型的决策可能对时间产生长期影响，因此长期公平性成为一个重要考虑因素。先前的研究表明，在忽略长期影响时，简单地应用静态公平性准则实际上会加剧偏见。为了明确解决顺序决策中的偏见，最近的研究在马尔可夫决策过程（MDP）框架中制定了长期公平性概念。他们将长期偏见定义为每个时间步上静态偏见的总和。然而，我们证明了简单地对逐步偏见求和可能导致虚假的公平感，因为它未考虑到转换过程中不同时间步的重要性差异。在这项工作中，我们介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，它明确考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。此外，我们还展示了长期收益的策略梯度的性质。",
    "tldr": "这篇论文介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。",
    "en_tdlr": "This paper introduces a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which considers the varying temporal importance and adapts static fairness principles to the sequential setting."
}