{
    "title": "Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])",
    "abstract": "The lack of quality labeled data is one of the main bottlenecks for training Deep Learning models. As the task increases in complexity, there is a higher penalty for overfitting and unstable learning. The typical paradigm employed today is Self-Supervised learning, where the model attempts to learn from a large corpus of unstructured and unlabeled data and then transfer that knowledge to the required task. Some notable examples of self-supervision in other modalities are BERT for Large Language Models, Wav2Vec for Speech Recognition, and the Masked AutoEncoder for Vision, which all utilize Transformers to solve a masked prediction task. GeoAI is uniquely poised to take advantage of the self-supervised methodology due to the decades of data collected, little of which is precisely and dependably annotated. Our goal is to extract building and road segmentations from Digital Elevation Models (DEM) that provide a detailed topography of the earths surface. The proposed architecture is the Ma",
    "link": "http://arxiv.org/abs/2309.03367",
    "context": "Title: Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])\nAbstract: The lack of quality labeled data is one of the main bottlenecks for training Deep Learning models. As the task increases in complexity, there is a higher penalty for overfitting and unstable learning. The typical paradigm employed today is Self-Supervised learning, where the model attempts to learn from a large corpus of unstructured and unlabeled data and then transfer that knowledge to the required task. Some notable examples of self-supervision in other modalities are BERT for Large Language Models, Wav2Vec for Speech Recognition, and the Masked AutoEncoder for Vision, which all utilize Transformers to solve a masked prediction task. GeoAI is uniquely poised to take advantage of the self-supervised methodology due to the decades of data collected, little of which is precisely and dependably annotated. Our goal is to extract building and road segmentations from Digital Elevation Models (DEM) that provide a detailed topography of the earths surface. The proposed architecture is the Ma",
    "path": "papers/23/09/2309.03367.json",
    "total_tokens": 952,
    "translated_title": "低资源下游任务的自监督遮罩数字高程模型编码",
    "translated_abstract": "缺乏高质量标记数据是训练深度学习模型的主要瓶颈之一。随着任务复杂性的增加，过拟合和不稳定学习的惩罚也越高。今天通常采用的范式是自监督学习，模型试图从大量非结构化和无标签的数据中学习，然后将该知识转移到所需的任务中。在其他模态中，一些著名的自监督示例包括用于大型语言模型的BERT，用于语音识别的Wav2Vec以及用于视觉的遮罩自动编码器，所有这些都利用变压器来解决遮罩预测任务。由于几十年的数据采集，并没有精确可靠地注释数据，地理AI独特地具备利用自监督方法的条件。我们的目标是从提供地球表面详细地形的数字高程模型（DEM）中提取建筑物和道路分割。所提出的架构是Ma",
    "tldr": "本研究旨在提供一种针对低资源下游任务的自监督遮罩数字高程模型编码。利用自监督学习，该模型能够从大量未标记和非结构化数据中学习，并将该知识应用于土地地形的建筑物和道路分割任务。",
    "en_tdlr": "This study aims to provide a self-supervised masked digital elevation model encoding for low-resource downstream tasks. Using self-supervised learning, the model is able to learn from a large amount of unlabelled and unstructured data and apply that knowledge to the task of building and road segmentations in land topography."
}