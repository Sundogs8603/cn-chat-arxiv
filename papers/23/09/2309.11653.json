{
    "title": "\"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. (arXiv:2309.11653v1 [cs.HC])",
    "abstract": "The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guideli",
    "link": "http://arxiv.org/abs/2309.11653",
    "context": "Title: \"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. (arXiv:2309.11653v1 [cs.HC])\nAbstract: The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guideli",
    "path": "papers/23/09/2309.11653.json",
    "total_tokens": 1035,
    "translated_title": "\"公平游戏\"，还是吗？研究用户在使用基于LLM的对话型智能助手时如何处理披露风险和效益",
    "translated_abstract": "基于大型语言模型（LLM）的对话型智能助手在高风险领域的广泛使用引发了许多隐私问题。构建尊重用户隐私的道德LLM型对话型智能助手需要深入了解最关注用户的隐私风险。然而，现有的研究主要以模型为中心，无法提供用户的观点。为了弥补这一差距，我们分析了实际的ChatGPT对话中的敏感披露，并对19名LLM型对话型智能助手用户进行了半结构化采访。我们发现，在使用LLM型对话型智能助手时，用户不断面临隐私、效用和便利之间的权衡。然而，用户错误的心智模式和系统设计中的黑暗模式限制了他们对隐私风险的认识和理解。此外，人类化的互动鼓励了更多敏感的披露，这使得用户在权衡中更加困难。我们讨论了实际的设计指南。",
    "tldr": "本研究通过分析用户在实际对话中的敏感披露和采访LLM型对话型智能助手用户的方式，发现用户在使用LLM型对话型智能助手时面临隐私、效用和便利之间的权衡，但用户对隐私风险的认知存在问题，而人类化的互动鼓励了更多敏感的披露，加重了用户的权衡困难。",
    "en_tdlr": "This study explores the trade-offs between privacy, utility, and convenience that users face when using LLM-based conversational agents. It finds that users have limited awareness of privacy risks and that human-like interactions encourage sensitive disclosures, complicating users' ability to navigate these trade-offs."
}