{
    "title": "Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])",
    "abstract": "We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.",
    "link": "http://arxiv.org/abs/2309.10953",
    "context": "Title: Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])\nAbstract: We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.",
    "path": "papers/23/09/2309.10953.json",
    "total_tokens": 892,
    "translated_title": "基于深度强化学习的连续空间无限时域均场问题解决方法",
    "translated_abstract": "我们提出了一种强化学习算法，用于统一解决连续空间均场博弈（MFG）和均场控制（MFC）问题，并对其进行了分析和发展。所提出的方法将演员-评论家（AC）范式与通过参数化评分函数表示的均场分布配对，可以以在线方式有效地更新，并使用朗之万动力学从得到的分布中获得样本。AC代理和评分函数按迭代方式进行更新，以收敛到给定均场问题的MFG平衡或MFC最优解，具体取决于学习率的选择。算法的简单修改使我们能够解决混合均场控制博弈（MFCG）。我们使用渐近无限时域框架中的线性二次基准评估我们的算法性能。",
    "tldr": "这项研究提出了一种基于深度强化学习的算法，通过将演员-评论家范式与均场分布表示配对，来解决连续空间中的均场博弈和均场控制问题，并使用朗之万动力学从分布中获取样本。该算法在渐近无限时域框架下使用线性二次基准进行评估。",
    "en_tdlr": "This research proposes a deep reinforcement learning algorithm that combines actor-critic paradigm with mean field distribution in order to solve continuous-space mean field game and mean field control problems. The algorithm uses Langevin dynamics to obtain samples from the distribution and is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework."
}