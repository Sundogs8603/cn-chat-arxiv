{
    "title": "Compressing Vision Transformers for Low-Resource Visual Learning. (arXiv:2309.02617v1 [cs.CV])",
    "abstract": "Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.  Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires hig",
    "link": "http://arxiv.org/abs/2309.02617",
    "context": "Title: Compressing Vision Transformers for Low-Resource Visual Learning. (arXiv:2309.02617v1 [cs.CV])\nAbstract: Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.  Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires hig",
    "path": "papers/23/09/2309.02617.json",
    "total_tokens": 890,
    "translated_title": "压缩视觉Transformer以实现低资源的视觉学习",
    "translated_abstract": "视觉Transformer（ViT）及其变种已经在视觉学习领域取得了巨大的成功，通过关注视觉输入的不同部分和捕捉长距离的空间依赖性，在图像分类、目标检测和语义分割等任务中取得了最先进的准确性。然而，这些模型非常庞大且计算量大。例如，最近提出的ViT-B模型有86M个参数，使得在资源受限的设备上部署变得不切实际。因此，它们在移动和边缘场景中的应用受到限制。在我们的工作中，我们希望通过利用流行的模型压缩技术，如蒸馏、修剪和量化，为将视觉Transformer引入边缘提供一种解决方案。我们选择的应用环境是无人机（UAV），它以电池供电并且内存受限，携带了一块尺寸与NVIDIA Jetson Nano相当的单板计算机，内存为4GB。",
    "tldr": "本研究旨在通过使用模型压缩技术，将视觉Transformer应用于资源受限的边缘设备，如无人机。这可以在保持准确性的同时减小模型大小和计算需求。",
    "en_tdlr": "This study aims to bring vision transformers to resource-constrained edge devices, such as drones, by using model compression techniques, reducing model size and computational requirements while maintaining accuracy."
}