{
    "title": "TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])",
    "abstract": "Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo",
    "link": "http://arxiv.org/abs/2309.10310",
    "context": "Title: TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])\nAbstract: Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo",
    "path": "papers/23/09/2309.10310.json",
    "total_tokens": 937,
    "translated_title": "TensorCodec: 无强数据假设的紧凑有损张量压缩",
    "translated_abstract": "许多现实世界的数据集都是以张量的形式表示的，即多维数值数组。如果不进行压缩，存储这些数据集通常需要大量的空间，而且随着维度的增加呈指数级增长。尽管有许多张量压缩算法可用，但其中许多依赖于关于数据维度、稀疏性、秩和平滑性的强假设。在这项工作中，我们提出了TENSORCODEC，这是一种针对一般张量的有损压缩算法，不需要符合强假设的输入数据。TENSORCODEC包含了三个关键点。第一个关键点是神经张量列车分解（NTTD），我们将循环神经网络集成到张量列车分解中，以增强其表达能力并减轻由低秩假设所带来的限制。另一个关键点是将输入张量折叠成更高阶的张量，以减少NTTD所需的空间。最后，对输入张量进行模式索引的重新排序，以揭示可以被利用的模式。",
    "tldr": "TensorCodec是一种紧凑的有损张量压缩算法，可以处理无强数据假设的一般张量。它采用神经张量列车分解、折叠输入张量和重新排序模式索引等关键点来提高压缩效果。"
}