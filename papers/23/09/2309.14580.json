{
    "title": "CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss. (arXiv:2309.14580v1 [cs.LG])",
    "abstract": "This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to ``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning (LiT)'' that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the e",
    "link": "http://arxiv.org/abs/2309.14580",
    "context": "Title: CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss. (arXiv:2309.14580v1 [cs.LG])\nAbstract: This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to ``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning (LiT)'' that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the e",
    "path": "papers/23/09/2309.14580.json",
    "total_tokens": 912,
    "translated_title": "CWCL：连续加权对比损失下的跨模态迁移",
    "translated_abstract": "本文考虑使用对比训练进行跨模态零样本迁移，其中一个模态中的预训练模型用于在另一个领域中进行表示学习，使用成对数据。然后，后一个领域中学到的模型可以以一种零样本的方式用于各种任务，类似于最近引起相当关注的“对比语言-图像预训练（CLIP）”和“锁定图像调整（LiT）”。大多数现有的跨模态表示对齐方法（包括CLIP和LiT）使用标准的对比训练目标，它使用一组正样本和负样本来对齐相似和驱散不相似的训练数据样本。然而，训练样本之间的相似性具有更连续的性质，因此需要更“非二进制”的处理。为了解决这个问题，我们提出了一种称为连续加权对比损失（CWCL）的新型损失函数，它使用连续的相似度测量。使用CWCL，我们旨在对齐实例的表示并提高跨模态的迁移性能。",
    "tldr": "本文提出了一种称为CWCL的损失函数，用于跨模态迁移中的对比训练。相比于传统的二进制对比训练，CWCL使用连续的相似性度量，可以更好地对齐实例的表示并提高跨模态的迁移性能。"
}