{
    "title": "LoopTune: Optimizing Tensor Computations with Reinforcement Learning. (arXiv:2309.01825v2 [cs.LG] UPDATED)",
    "abstract": "Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.",
    "link": "http://arxiv.org/abs/2309.01825",
    "context": "Title: LoopTune: Optimizing Tensor Computations with Reinforcement Learning. (arXiv:2309.01825v2 [cs.LG] UPDATED)\nAbstract: Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.",
    "path": "papers/23/09/2309.01825.json",
    "total_tokens": 970,
    "translated_title": "LoopTune: 使用强化学习优化张量计算",
    "translated_abstract": "先进的编译器技术对于使机器学习应用在新型硬件上运行至关重要，但传统编译器无法提供性能，普通的自动调节器搜索时间长，经专家优化的库导致不可持续的成本。为了解决这个问题，我们开发了LoopTune，这是一个深度强化学习编译器，用于优化CPU中深度学习模型中的张量计算。LoopTune在使用超快轻量级代码生成器LoopNest执行硬件特定优化的同时，优化张量遍历顺序。通过采用新颖的基于图的表示和动作空间，LoopTune将LoopNest的速度提高了3.2倍，生成的代码比TVM快一个数量级，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，并持续在手工调整的库Numpy的水平上运行。此外，LoopTune可以在几秒钟内优化代码。",
    "tldr": "LoopTune是一个使用强化学习优化张量计算的编译器，通过优化张量遍历顺序和使用代码生成器LoopNest执行硬件特定优化，LoopTune能够生成比其他编译器更快的代码。通过采用新的图形表示和动作空间，LoopTune比TVM快一个数量级，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，并持续在与手工调优的库Numpy相当的水平上工作。此外，LoopTune优化代码的时间只需几秒钟。",
    "en_tdlr": "LoopTune is a compiler that uses reinforcement learning to optimize tensor computations, generating faster code by optimizing the tensor traversal order and using LoopNest as the code generator for hardware-specific optimizations. It outperforms other compilers by a magnitude, consistently performing at the level of hand-tuned libraries, with optimization time taking only a few seconds."
}