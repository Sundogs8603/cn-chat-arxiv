{
    "title": "Controllable Text Generation with Residual Memory Transformer. (arXiv:2309.16231v1 [cs.CL])",
    "abstract": "Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach.",
    "link": "http://arxiv.org/abs/2309.16231",
    "context": "Title: Controllable Text Generation with Residual Memory Transformer. (arXiv:2309.16231v1 [cs.CL])\nAbstract: Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach.",
    "path": "papers/23/09/2309.16231.json",
    "total_tokens": 927,
    "translated_title": "Residual Memory Transformer: 可控文本生成方法",
    "translated_abstract": "大规模因果语言模型（CLMs），例如GPT3和ChatGPT，在文本生成方面取得了巨大成功。然而，在平衡灵活性、控制粒度和生成效率的同时，控制CLM的生成过程仍然是一个开放性挑战。本文提供了一种新的可控文本生成方法（CTG），通过设计一个非侵入式、轻量级的控制插件，在任意时间步骤上伴随CLM的生成。所提出的控制插件，即Residual Memory Transformer (RMT)，具有编码器-解码器结构，可以接受任何类型的控制条件，并通过残差学习范式与CLM合作，实现更灵活、通用和高效的CTG。在各种控制任务上进行了广泛的实验，包括自动评估和人工评估。结果表明，RMT在一系列最先进方法上表现出优越性，证明了我们方法的有效性和多样性。",
    "tldr": "本文提出了一种名为Residual Memory Transformer(RMT)的控制插件，它可以与大规模因果语言模型(CLMs)合作进行可控文本生成，通过残差学习范式实现更灵活、通用和高效的文本生成。在各种控制任务上的实验证明了RMT的优越性。",
    "en_tdlr": "This paper proposes a control plugin called Residual Memory Transformer (RMT) for controllable text generation (CTG) with large-scale causal language models (CLMs), achieving more flexible, general, and efficient text generation through a residual learning paradigm. Extensive experiments demonstrate the superiority of RMT over state-of-the-art approaches in various control tasks."
}