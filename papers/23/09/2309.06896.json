{
    "title": "Domain-Aware Augmentations for Unsupervised Online General Continual Learning. (arXiv:2309.06896v1 [cs.LG])",
    "abstract": "Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.",
    "link": "http://arxiv.org/abs/2309.06896",
    "context": "Title: Domain-Aware Augmentations for Unsupervised Online General Continual Learning. (arXiv:2309.06896v1 [cs.LG])\nAbstract: Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.",
    "path": "papers/23/09/2309.06896.json",
    "total_tokens": 936,
    "translated_title": "针对无监督在线一般连续学习的领域感知扩增策略",
    "translated_abstract": "连续学习一直是一项具有挑战性的任务，尤其是在处理无监督场景时，例如无监督在线一般连续学习（UOGCL），其中学习代理没有先验知识关于类别边界或任务更改的信息。虽然以前的研究集中在减少等价学习中的遗忘，但最近的研究表明，自监督学习者对遗忘更具鲁棒性。本文提出了一种新的方法，通过定义和使用流依赖的数据扩增方式以及一些实现技巧，增强了UOGCL中对比学习的记忆使用。我们提出的方法简单而有效，在所有考虑的设置中实现了与其他无监督方法相比的最新成果，并减小了监督和无监督连续学习之间的差距。我们的领域感知扩增过程可以适应其他基于回放的方法，使它成为一种有前途的连续学习策略。",
    "tldr": "本文提出了一种新颖的方法，通过定义和使用流依赖的数据扩增方式以及一些实现技巧，增强了UOGCL中的对比学习，达到了与其他无监督方法相比的最新成果，并减小了监督和无监督连续学习之间的差距。"
}