{
    "title": "On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])",
    "abstract": "Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Recently, based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these \"skill neurons\", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data, with higher robustness for T5 than RoBERTa. At the same time, we replicate the existence of skill neurons in RoBERTa and further show that skill neurons also seem to exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to",
    "link": "http://arxiv.org/abs/2309.12263",
    "context": "Title: On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])\nAbstract: Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Recently, based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these \"skill neurons\", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data, with higher robustness for T5 than RoBERTa. At the same time, we replicate the existence of skill neurons in RoBERTa and further show that skill neurons also seem to exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to",
    "path": "papers/23/09/2309.12263.json",
    "total_tokens": 932,
    "translated_title": "关于技能神经元与Prompt Tuning中的鲁棒性的关系研究",
    "translated_abstract": "Prompt Tuning是一种用于预训练大型语言模型(PLMs)的参数高效微调方法。最近，通过对RoBERTa的实验，有人认为Prompt Tuning激活了Transformer前馈网络中特定的神经元，这些神经元对给定任务具有高预测能力和选择性。本文中，我们使用RoBERTa和T5来研究Prompt Tuning与这些“技能神经元”的鲁棒性关系。我们发现特定任务的调整指令在相同类型的任务上具有传递性，但对于对抗性数据的鲁棒性不高，其中T5的鲁棒性比RoBERTa更高。同时，我们重现了RoBERTa中的技能神经元存在，并进一步展示了T5中也存在技能神经元。有趣的是，T5在非对抗性数据上确定的技能神经元也是对抗性数据上预测性最强的神经元，而这在RoBERTa中不是这种情况。我们得出结论，更高的对抗鲁棒性可能与技能神经元的存在相关。",
    "tldr": "本文研究了Prompt Tuning在与\"技能神经元\"的关系中的鲁棒性，发现特定任务的调整指令在相同类型的任务上具有传递性，但对于对抗性数据的鲁棒性不高，其中T5的鲁棒性比RoBERTa更高，并且发现T5和RoBERTa中都存在技能神经元。"
}