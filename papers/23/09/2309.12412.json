{
    "title": "Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition. (arXiv:2309.12412v1 [cs.CV])",
    "abstract": "Compression of a neural network can help in speeding up both the training and the inference of the network. In this research, we study applying compression using low rank decomposition on network layers. Our research demonstrates that to acquire a speed up, the compression methodology should be aware of the underlying hardware as analysis should be done to choose which layers to compress. The advantage of our approach is demonstrated via a case study of compressing ResNet50 and training on full ImageNet-ILSVRC2012. We tested on two different hardware systems Nvidia V100 and Huawei Ascend910. With hardware targeted compression, results on Ascend910 showed 5.36% training speedup and 15.79% inference speed on Ascend310 with only 1% drop in accuracy compared to the original uncompressed model",
    "link": "http://arxiv.org/abs/2309.12412",
    "context": "Title: Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition. (arXiv:2309.12412v1 [cs.CV])\nAbstract: Compression of a neural network can help in speeding up both the training and the inference of the network. In this research, we study applying compression using low rank decomposition on network layers. Our research demonstrates that to acquire a speed up, the compression methodology should be aware of the underlying hardware as analysis should be done to choose which layers to compress. The advantage of our approach is demonstrated via a case study of compressing ResNet50 and training on full ImageNet-ILSVRC2012. We tested on two different hardware systems Nvidia V100 and Huawei Ascend910. With hardware targeted compression, results on Ascend910 showed 5.36% training speedup and 15.79% inference speed on Ascend310 with only 1% drop in accuracy compared to the original uncompressed model",
    "path": "papers/23/09/2309.12412.json",
    "total_tokens": 816,
    "translated_title": "通过针对网络层进行低秩分解加速Resnet架构",
    "translated_abstract": "对神经网络进行压缩可以加速网络的训练和推断。本研究研究了在网络层上应用低秩分解进行压缩的方法。我们的研究表明，为了获得加速效果，压缩方法应该考虑底层硬件，并进行分析选择需要压缩的层次。我们通过对ResNet50进行压缩和在完整的ImageNet-ILSVRC2012上训练的案例研究来展示我们的方法的优势。我们在两个不同的硬件系统Nvidia V100和华为Ascend910上进行了测试。通过针对硬件的压缩，Ascend910上的结果显示相比原始未压缩模型，训练加速了5.36%，在Ascend310上推断速度提高了15.79%，仅有1%的准确率下降。",
    "tldr": "本研究通过针对网络进行低秩分解的压缩方法，加速了Resnet架构的训练和推断，通过对ResNet50的研究案例，证明了硬件目标压缩的优势。",
    "en_tdlr": "This research proposes a compression method using low rank decomposition on network layers, which speeds up the training and inference of Resnet architecture. The advantage of hardware targeted compression is demonstrated through a case study on ResNet50."
}