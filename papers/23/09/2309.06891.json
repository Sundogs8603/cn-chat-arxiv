{
    "title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?. (arXiv:2309.06891v1 [cs.CV])",
    "abstract": "Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?  In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to o",
    "link": "http://arxiv.org/abs/2309.06891",
    "context": "Title: Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?. (arXiv:2309.06891v1 [cs.CV])\nAbstract: Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?  In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to o",
    "path": "papers/23/09/2309.06891.json",
    "total_tokens": 925,
    "translated_title": "保持简单：谁说有监督的Transformer模型注意力不集中? (arXiv:2309.06891v1 [cs.CV])",
    "translated_abstract": "卷积网络和视觉Transformer通过不同形式的成对交互来进行池化，包括在网络层之间进行池化和在网络末端进行池化。后者真的需要与前者不同吗？作为池化的副产品，视觉Transformer可以提供免费的空间注意力，但通常质量较低，除非是自监督的，而这方面的研究并不充分。监督是真正的问题吗？在这项工作中，我们开发了一个通用的池化框架，然后将一些现有方法作为实例化。通过讨论每组方法的特性，我们得到了SimPool，一个简单的基于注意力的池化机制，用于替代卷积和Transformer编码器的默认机制。我们发现，无论是有监督还是自监督，这都改善了预训练和下游任务的性能，并提供了能够描绘对象边界的注意力图。因此，可以称SimPool是通用的。据我们所知，我们是第一个这样做的。",
    "tldr": "该论文提出了一种通用的池化框架SimPool，用于替代卷积和Transformer编码器的默认池化机制，无论是有监督还是自监督的方法都能提高性能，并提供能够描绘对象边界的注意力图。",
    "en_tdlr": "This paper presents a generic pooling framework called SimPool to replace the default pooling mechanism in convolutional and transformer encoders. It improves performance in both supervised and self-supervised methods and provides attention maps for object boundaries."
}