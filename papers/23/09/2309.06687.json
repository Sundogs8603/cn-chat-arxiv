{
    "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics. (arXiv:2309.06687v1 [cs.RO])",
    "abstract": "Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robot",
    "link": "http://arxiv.org/abs/2309.06687",
    "context": "Title: Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics. (arXiv:2309.06687v1 [cs.RO])\nAbstract: Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robot",
    "path": "papers/23/09/2309.06687.json",
    "total_tokens": 879,
    "translated_title": "用于机器人深度强化学习的自我改进型大型语言模型作为自动化奖励函数设计师",
    "translated_abstract": "虽然深度强化学习在众多机器人应用中取得了显著的成功，但设计高性能的奖励函数仍然是一项具有挑战性的任务，通常需要大量的人工输入。最近，广泛采用大型语言模型（LLM）来解决需要深入常识知识的任务，如推理和规划。意识到奖励函数设计与这种知识本质上是相关的，LLM在这个背景下提供了很大的潜力。受此启发，我们在这项工作中提出了一种新颖的LLM框架，具有自我改进机制，用于自动化奖励函数设计。该框架以自然语言输入为基础，由LLM制定一个初始的奖励函数。然后，评估奖励函数的性能，并将结果呈现给LLM以指导其自我改进的过程。通过多种连续机器人任务的实验验证了我们提出的框架的性能。",
    "tldr": "提出一种自我改进机制的大型语言模型（LLM）框架用于自动化奖励函数设计，在深度强化学习中展现了潜在的应用价值。",
    "en_tdlr": "A novel LLM framework with a self-refinement mechanism is proposed for automated reward function design, showing potential applications in deep reinforcement learning."
}