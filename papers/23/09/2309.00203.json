{
    "title": "Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])",
    "abstract": "This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\\times k$ \\textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \\textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \\textit{pseudo-dimension} of performance metrics. We present an $\\tilde{\\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\\tilde{\\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\\Omega(nk)$ lower bound, hence ",
    "link": "http://arxiv.org/abs/2309.00203",
    "context": "Title: Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])\nAbstract: This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\\times k$ \\textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \\textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \\textit{pseudo-dimension} of performance metrics. We present an $\\tilde{\\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\\tilde{\\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\\Omega(nk)$ lower bound, hence ",
    "path": "papers/23/09/2309.00203.json",
    "total_tokens": 1003,
    "translated_title": "数据驱动的线性规划降维方法：泛化界限和学习方法",
    "translated_abstract": "本文研究了一种简单的数据驱动方法来处理高维线性规划问题（LP）。给定过去的$n$维LP数据，我们学习一个$n\\times k$的“投影矩阵”（$n > k$），将维数从$n$降低到$k$。然后，我们通过解决$k$维LP问题并通过乘以投影矩阵来恢复$n$维的解决方案来处理未来的LP实例。这个思想与任何用户首选的LP求解器兼容，因此是一种通用的加速LP求解的方法。一个自然的问题是：需要多少数据才能确保恢复的解决方案的质量？我们基于“数据驱动算法设计”的思想来回答这个问题，它将足够进行泛化保证的数据量与性能指标的“伪维度”联系起来。我们给出了伪维度的$\\tilde{\\mathrm{O}}(nk^2)$上界（$\\tilde{\\mathrm{O}}$压缩了对数因子），并通过一个$\\Omega(nk)$下界来补充它，",
    "tldr": "本文研究了一种简单的数据驱动方法，通过学习投影矩阵来降低高维线性规划问题的维数，实现更快的求解速度。基于“数据驱动算法设计”，提出了泛化保证的数据量与性能指标的伪维度的上界和下界。"
}