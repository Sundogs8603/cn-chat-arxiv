{
    "title": "Generalizable Neural Fields as Partially Observed Neural Processes. (arXiv:2309.06660v1 [cs.LG])",
    "abstract": "Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorit",
    "link": "http://arxiv.org/abs/2309.06660",
    "context": "Title: Generalizable Neural Fields as Partially Observed Neural Processes. (arXiv:2309.06660v1 [cs.LG])\nAbstract: Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorit",
    "path": "papers/23/09/2309.06660.json",
    "total_tokens": 857,
    "translated_title": "通用的神经场作为部分观测的神经过程",
    "translated_abstract": "神经场是使用神经网络参数化的函数来表示信号的一种有希望替代传统离散向量或基于网格的表示方法。与离散表示相比，神经表示不仅能够很好地随着分辨率的增加而扩展，而且是连续的，并且可以进行多次可微分。然而，对于我们想要表示的信号数据集来说，必须对每个信号优化一个单独的神经场是低效的，而且不能利用共享信息或信号之间的结构。现有的泛化方法将这视为元学习问题，并采用基于梯度的元学习来学习初始化值，然后通过测试时优化来微调，或者学习超网络来产生神经场的权重。相比之下，我们提出了一种新的范式，将大规模的神经表示训练视为部分观测的神经过程框架的一部分，并利用神经过程算法进行优化。",
    "tldr": "本研究提出了一种新的范式，将大规模的神经表示训练视为部分观测的神经过程框架的一部分，并利用神经过程算法进行优化。"
}