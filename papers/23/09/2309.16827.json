{
    "title": "Post-Training Overfitting Mitigation in DNN Classifiers. (arXiv:2309.16827v1 [cs.LG])",
    "abstract": "Well-known (non-malicious) sources of overfitting in deep neural net (DNN) classifiers include: i) large class imbalances; ii) insufficient training-set diversity; and iii) over-training. In recent work, it was shown that backdoor data-poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. Thus, an effective post-training (with no knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. Improving upon that work, we threshold activations specifically to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. We also provide some analytical support for this mitigation approach. Most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious ove",
    "link": "http://arxiv.org/abs/2309.16827",
    "context": "Title: Post-Training Overfitting Mitigation in DNN Classifiers. (arXiv:2309.16827v1 [cs.LG])\nAbstract: Well-known (non-malicious) sources of overfitting in deep neural net (DNN) classifiers include: i) large class imbalances; ii) insufficient training-set diversity; and iii) over-training. In recent work, it was shown that backdoor data-poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. Thus, an effective post-training (with no knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. Improving upon that work, we threshold activations specifically to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. We also provide some analytical support for this mitigation approach. Most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious ove",
    "path": "papers/23/09/2309.16827.json",
    "total_tokens": 1003,
    "translated_title": "DNN分类器中的后训练过拟合缓解方法",
    "translated_abstract": "深度神经网络（DNN）分类器中非恶意过拟合的已知来源包括：i）大的类别不平衡；ii）训练集多样性不足；iii）过度训练。最近的研究表明，后门数据污染也会导致过拟合，具有异常大的分类边界距离攻击者的目标类，特别是通过允许大信号在DNN中传播的（无界）ReLU激活。因此，提出了一种有效的后训练缓解方法（不需要了解训练集或训练过程），利用一个小的干净数据集，基于限制神经激活来解决后门问题。在完善该方法的基础上，我们特定阈值激活以限制最大边界（MMs），从而在后门缓解中获得性能收益。我们还提供了一些支持该缓解方法的分析。最重要的是，我们展示了基于最大边界的后训练正则化大大缓解了非恶意过拟合问题。",
    "tldr": "该论文提出了一种后训练缓解方法，用于解决深度神经网络分类器中的过拟合问题。该方法通过限制神经激活边界来缓解后门数据污染导致的过拟合，并获得了性能改进。研究还提供了分析支持，表明基于最大边界的后训练正则化显著缓解了非恶意过拟合问题。",
    "en_tdlr": "This paper proposes a post-training mitigation approach to address overfitting in deep neural network classifiers. The method mitigates overfitting caused by backdoor data-poisoning by limiting neural activations, resulting in performance gains. Analytical support is provided, showing that post-training maximum margin-based regularization substantially mitigates non-malicious overfitting."
}