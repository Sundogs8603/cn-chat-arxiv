{
    "title": "Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])",
    "abstract": "In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\\eta^\\pi$) attained by a given policy $\\pi$.  We use the certainty-equivalence method to construct our estimator $\\hat\\eta^\\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\\widetilde O\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2p}(1-\\gamma)^{2p+2}}\\right)$ to guarantee a $p$-Wasserstein metric between $\\hat\\eta^\\pi$ and $\\eta^\\pi$ is less than $\\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\\widetilde O\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2}(1-\\gamma)^{4}}\\right)$ suffices to ensure the Kolmogorov metric and total variation m",
    "link": "http://arxiv.org/abs/2309.17262",
    "context": "Title: Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])\nAbstract: In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\\eta^\\pi$) attained by a given policy $\\pi$.  We use the certainty-equivalence method to construct our estimator $\\hat\\eta^\\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\\widetilde O\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2p}(1-\\gamma)^{2p+2}}\\right)$ to guarantee a $p$-Wasserstein metric between $\\hat\\eta^\\pi$ and $\\eta^\\pi$ is less than $\\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\\widetilde O\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2}(1-\\gamma)^{4}}\\right)$ suffices to ensure the Kolmogorov metric and total variation m",
    "path": "papers/23/09/2309.17262.json",
    "total_tokens": 924,
    "translated_title": "分布式强化学习中的估计和推断",
    "translated_abstract": "本文从统计效率的角度研究了分布式强化学习。我们研究了分布式策略评估，旨在估计由给定策略π获得的随机回报的完整分布（表示为η^π）。在提供生成模型的情况下，我们使用等价确定法构造了估计器η^π。我们证明，在这种情况下，通过具有大小为O(|S||A|/(ε^(2p)(1-γ)^(2p+2)))的数据集可以保证估计器η^π和真实分布η^π之间的p-Wasserstein距离小于ε的概率很高。这意味着分布式策略评估问题可以以高效利用样本的方式解决。此外，我们还证明，在不同的温和假设下，通过具有大小为O(|S||A|/(ε^2(1-γ)^4))的数据集就足以确保Kolmogorov距离和总变差。",
    "tldr": "本文研究了分布式强化学习中的估计和推断问题，通过使用等价确定法，在提供生成模型的情况下以高效的方式解决了分布式策略评估问题。",
    "en_tdlr": "This paper investigates estimation and inference in distributional reinforcement learning, and proposes an efficient solution to the distributional policy evaluation problem using the certainty-equivalence method, given a generative model is available."
}