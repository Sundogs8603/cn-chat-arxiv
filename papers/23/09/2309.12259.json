{
    "title": "Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])",
    "abstract": "Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and expli",
    "link": "http://arxiv.org/abs/2309.12259",
    "context": "Title: Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])\nAbstract: Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and expli",
    "path": "papers/23/09/2309.12259.json",
    "total_tokens": 897,
    "translated_title": "柔性合并：一种灵活且鲁棒的软模型合并方法，用于增强神经网络性能",
    "translated_abstract": "随机梯度下降（SGD）是深度学习中广泛使用的优化算法，由于问题的非凸性，它通常只能收敛到局部最优解，利用这些局部最优解改进模型性能仍然是一个具有挑战性的任务。鉴于神经网络的固有复杂性，简单的算术平均会导致不理想的结果。本文提出了一种“软合并”方法，通过学习门参数，利用硬具体分布乘以$l_0$范数的代理，实现了多个模型的快速合并，简化了神经网络特定部分的合并，并增强了对具有极端值的恶意模型的鲁棒性。这个合并过程不仅通过收敛到更好的局部最优解来提高模型性能，还最小化了计算成本，提供了高效且明确的方法。",
    "tldr": "本论文提出了一种灵活且鲁棒的软模型合并方法，用于增强神经网络性能。该方法通过学习门参数，利用硬具体分布乘以$l_0$范数的代理，实现了多个模型的快速合并，并增强了对具有极端值的恶意模型的鲁棒性。这种合并过程不仅提高了模型性能，还降低了计算成本。"
}