{
    "title": "Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])",
    "abstract": "Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resou",
    "link": "http://arxiv.org/abs/2309.04679",
    "context": "Title: Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])\nAbstract: Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resou",
    "path": "papers/23/09/2309.04679.json",
    "total_tokens": 929,
    "translated_title": "嵌入结构的重要性：比较适应新语言的多语言词汇方法",
    "translated_abstract": "预训练的多语言语言模型支持英语以外的现代自然语言处理工具的大部分。用于特定语言化的强大基准是语言适应预训练（LAPT）。但是，在适应过程中保留大型跨语言词汇和嵌入矩阵会带来相当多的多余计算成本。在本研究中，我们提出了几种简单的技术来用紧凑的特定语言词汇替换跨语言词汇。具体而言，我们解决了在词汇专门化后如何重新初始化令牌嵌入矩阵的策略。我们对我们的技术进行了系统的实验比较，此外还加入了最近提出的焦点方法。我们证明了：1）在单语转移文献中的嵌入替换技术不适用于适应多语言模型。2）用较小的专门的词汇替换跨语言词汇提供了一种提高低资源情况下性能的有效方法。",
    "tldr": "比较了替换跨语言词汇的几种技术，证明了单语转移文献中的方法不适用于多语言模型。专门化的较小词汇对于提高低资源情况下的性能是有效的。",
    "en_tdlr": "Comparing techniques for replacing cross-lingual vocabularies, it is demonstrated that methods from monolingual transfer literature are inadequate for multilingual models. Specialized smaller vocabularies prove to be effective in improving performance in low-resource scenarios."
}