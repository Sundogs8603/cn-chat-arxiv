{
    "title": "Retrieval-Augmented Meta Learning for Low-Resource Text Classification. (arXiv:2309.04979v1 [cs.CL])",
    "abstract": "Meta learning have achieved promising performance in low-resource text classification which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. However, due to the limited training data in the meta-learning scenario and the inherent properties of parameterized neural networks, poor generalization performance has become a pressing problem that needs to be addressed. To deal with this issue, we propose a meta-learning based method called Retrieval-Augmented Meta Learning(RAML). It not only uses parameterization for inference but also retrieves non-parametric knowledge from an external corpus to make inferences, which greatly alleviates the problem of poor generalization performance caused by the lack of diverse training data in meta-learning. This method differs from previous models that solely rely on parameters, as it explicitly emphasizes the importance of non-parametric knowledge, aiming to strike a balance between p",
    "link": "http://arxiv.org/abs/2309.04979",
    "context": "Title: Retrieval-Augmented Meta Learning for Low-Resource Text Classification. (arXiv:2309.04979v1 [cs.CL])\nAbstract: Meta learning have achieved promising performance in low-resource text classification which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. However, due to the limited training data in the meta-learning scenario and the inherent properties of parameterized neural networks, poor generalization performance has become a pressing problem that needs to be addressed. To deal with this issue, we propose a meta-learning based method called Retrieval-Augmented Meta Learning(RAML). It not only uses parameterization for inference but also retrieves non-parametric knowledge from an external corpus to make inferences, which greatly alleviates the problem of poor generalization performance caused by the lack of diverse training data in meta-learning. This method differs from previous models that solely rely on parameters, as it explicitly emphasizes the importance of non-parametric knowledge, aiming to strike a balance between p",
    "path": "papers/23/09/2309.04979.json",
    "total_tokens": 927,
    "translated_title": "用于低资源文本分类的检索增强元学习",
    "translated_abstract": "元学习在低资源文本分类中取得了有希望的性能，这个任务旨在从源类别中的小任务集合（被称为episodes）中传递知识来识别目标类别。然而，由于元学习场景中的有限训练数据和参数化神经网络的固有属性，泛化性能差成为一个迫切需要解决的问题。为了应对这个问题，我们提出了一种基于元学习的方法，称为检索增强元学习（RAML）。它不仅使用参数化进行推理，还从外部语料库中检索非参数化知识进行推理，大大缓解了由于元学习中缺乏多样性训练数据而导致的泛化性能差的问题。这种方法不同于之前仅依赖于参数的模型，它明确强调了非参数化知识的重要性，旨在在参数化和非参数化知识之间取得平衡。",
    "tldr": "本论文提出了一种基于元学习的方法，称为检索增强元学习（RAML），用于解决低资源文本分类中的泛化性能差问题。该方法不仅使用参数化进行推理，还从外部语料库中检索非参数化知识进行推理，以提高泛化能力和解决元学习中缺乏多样性训练数据的问题。",
    "en_tdlr": "This paper proposes a meta-learning method called Retrieval-Augmented Meta Learning (RAML) to address the issue of poor generalization performance in low-resource text classification. The method not only uses parameterization for inference but also retrieves non-parametric knowledge from an external corpus to improve generalization and tackle the lack of diverse training data in meta-learning."
}