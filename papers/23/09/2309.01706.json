{
    "title": "On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)",
    "abstract": "Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.",
    "link": "http://arxiv.org/abs/2309.01706",
    "context": "Title: On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)\nAbstract: Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.",
    "path": "papers/23/09/2309.01706.json",
    "total_tokens": 875,
    "translated_title": "关于后续GNN解释器对标签噪声的鲁棒性的研究",
    "translated_abstract": "后续GNN解释器被提出作为解决图神经网络(GNNs)固有黑盒限制的方案，旨在提供对训练后的GNN表现行为的精确和深刻的解释。尽管在学术和工业环境中最近有显著进展，但后续GNN解释器在面对标签噪声时的鲁棒性尚未被探索。为了填补这一空白，我们进行了系统的实证研究，评估了不同程度标签噪声下各种后续GNN解释器的功效。我们的结果揭示了几个关键见解：首先，后续GNN解释器容易受到标签扰动的影响。其次，即使是对GNN表现无关紧要的轻微标签噪声，也会严重损害生成的解释质量。最后，我们就随着噪声水平的升高逐渐恢复解释效果展开讨论。",
    "tldr": "本文对后续GNN解释器在面对标签噪声时的鲁棒性进行了研究，并发现即使是轻微的标签噪声也会严重影响解释质量。"
}