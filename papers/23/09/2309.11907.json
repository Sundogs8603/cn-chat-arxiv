{
    "title": "Learning to Recover for Safe Reinforcement Learning. (arXiv:2309.11907v1 [cs.AI])",
    "abstract": "Safety controllers is widely used to achieve safe reinforcement learning. Most methods that apply a safety controller are using handcrafted safety constraints to construct the safety controller. However, when the environment dynamics are sophisticated, handcrafted safety constraints become unavailable. Therefore, it worth to research on constructing safety controllers by learning algorithms. We propose a three-stage architecture for safe reinforcement learning, namely TU-Recovery Architecture. A safety critic and a recovery policy is learned before task training. They form a safety controller to ensure safety in task training. Then a phenomenon induced by disagreement between task policy and recovery policy, called adversarial phenomenon, which reduces learning efficiency and model performance, is described. Auxiliary reward is proposed to mitigate adversarial phenomenon, while help the task policy to learn to recover from high-risk states. A series of experiments are conducted in a ro",
    "link": "http://arxiv.org/abs/2309.11907",
    "context": "Title: Learning to Recover for Safe Reinforcement Learning. (arXiv:2309.11907v1 [cs.AI])\nAbstract: Safety controllers is widely used to achieve safe reinforcement learning. Most methods that apply a safety controller are using handcrafted safety constraints to construct the safety controller. However, when the environment dynamics are sophisticated, handcrafted safety constraints become unavailable. Therefore, it worth to research on constructing safety controllers by learning algorithms. We propose a three-stage architecture for safe reinforcement learning, namely TU-Recovery Architecture. A safety critic and a recovery policy is learned before task training. They form a safety controller to ensure safety in task training. Then a phenomenon induced by disagreement between task policy and recovery policy, called adversarial phenomenon, which reduces learning efficiency and model performance, is described. Auxiliary reward is proposed to mitigate adversarial phenomenon, while help the task policy to learn to recover from high-risk states. A series of experiments are conducted in a ro",
    "path": "papers/23/09/2309.11907.json",
    "total_tokens": 936,
    "translated_title": "学习恢复以实现安全强化学习",
    "translated_abstract": "安全控制器被广泛用于实现安全强化学习。大多数应用安全控制器的方法都使用手工制定的安全约束来构建安全控制器。然而，当环境动态复杂时，手工制定的安全约束变得不可用。因此，有必要研究通过学习算法构建安全控制器。我们提出了一个三阶段的安全强化学习架构，即TU恢复架构。在任务训练之前学习安全评估器和恢复策略。它们形成一个安全控制器，以确保任务训练的安全性。然后，描述了一种由任务策略和恢复策略之间的不一致引起的现象，称为对抗现象，该现象降低了学习效率和模型性能。我们提出了辅助奖励来缓解对抗现象，同时帮助任务策略学习从高风险状态中恢复。在机器人导航任务中进行了一系列实验。",
    "tldr": "该论文提出了一种三阶段的安全强化学习架构，通过学习来构建安全控制器，以在任务训练中确保安全性。为了缓解对抗现象，同时帮助任务策略从高风险状态中恢复，提出了辅助奖励。在机器人导航任务中进行了实验证明。",
    "en_tdlr": "This paper presents a three-stage architecture for safe reinforcement learning, constructing a safety controller through learning to ensure safety during task training. To mitigate adversarial phenomenon and aid the task policy in recovering from high-risk states, an auxiliary reward is proposed. Experiments are conducted in robot navigation tasks to support the findings."
}