{
    "title": "Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning. (arXiv:2309.06086v1 [cs.LG])",
    "abstract": "Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and i",
    "link": "http://arxiv.org/abs/2309.06086",
    "context": "Title: Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning. (arXiv:2309.06086v1 [cs.LG])\nAbstract: Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and i",
    "path": "papers/23/09/2309.06086.json",
    "total_tokens": 949,
    "translated_title": "优化塑性的互补网络用于无监督连续学习",
    "translated_abstract": "连续无监督表示学习（CURL）研究受益于自监督学习（SSL）技术的改进。因此，使用SSL的现有CURL方法可以在没有任何标签的情况下学习高质量的表示，但在学习多任务数据流时性能下降明显。我们假设这是由于为了防止遗忘而施加的正则化损失造成的，导致塑性和稳定性之间的权衡不够优化：它们要么不完全适应新的数据（塑性低），要么在完全适应新的SSL预训练任务时产生显著遗忘（稳定性低）。在这项工作中，我们提出了训练一个专家网络，它不再需要保留先前的知识，而是可以专注于在新任务上表现最佳（优化塑性）。在第二阶段，我们将这个新知识与先前的网络结合在一个适应-回顾阶段，以避免遗忘。",
    "tldr": "本研究提出了一种优化塑性的互补网络方法，用于无监督连续学习。通过解除专家网络对保留先前知识的要求，并通过适应-回顾阶段与之结合，解决了无监督学习中塑性和稳定性之间的权衡问题。",
    "en_tdlr": "This paper proposes an optimization-based approach to unsupervised continual learning by training complementary networks. By relieving an expert network from preserving previous knowledge and combining it with the previous network in an adaptation-retrospection phase, the trade-off between plasticity and stability in unsupervised learning is addressed."
}