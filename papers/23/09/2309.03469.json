{
    "title": "Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])",
    "abstract": "Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \\textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \\textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \\citep{FlexMatch} to FixMatch \\citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training ",
    "link": "http://arxiv.org/abs/2309.03469",
    "context": "Title: Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])\nAbstract: Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \\textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \\textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \\citep{FlexMatch} to FixMatch \\citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training ",
    "path": "papers/23/09/2309.03469.json",
    "total_tokens": 896,
    "translated_title": "快速的FixMatch: 基于课程批次大小的快速半监督学习",
    "translated_abstract": "半监督学习（SSL）的进展几乎完全消除了SSL和监督学习之间的差距，同时减少了标签数量。然而，最近的性能提升往往是以显著增加的训练计算为代价的。为了解决这个问题，我们提出了课程批次大小（CBS），利用深度神经网络的自然训练动态，采用一个小的未标记的批次大小开始训练，并逐渐增加到训练结束。无论数据集、模型或训练轮次，都使用固定的课程，通过在所有设置中减少训练计算量。我们将CBS、强化标记增强和课程伪标签（CPL）应用于FixMatch，并将这个新的SSL算法称为快速的FixMatch。我们进行了割实验，表明强化标记增强和/或CPL并不显著地减少训练量。",
    "tldr": "本论文提出了快速的FixMatch算法，通过引入课程批次大小（CBS）来减少半监督学习的训练计算量，并使用强化标记增强和课程伪标签进行改进。",
    "en_tdlr": "This paper proposes Fast FixMatch, a faster semi-supervised learning algorithm that reduces training computations by introducing Curriculum Batch Size (CBS), and improves performance by incorporating strong labeled augmentation and Curriculum Pseudo Labeling (CPL)."
}