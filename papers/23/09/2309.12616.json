{
    "title": "Unlocking Model Insights: A Dataset for Automated Model Card Generation. (arXiv:2309.12616v1 [cs.CL])",
    "abstract": "Language models (LMs) are no longer restricted to ML community, and instruction-tuned LMs have led to a rise in autonomous AI agents. As the accessibility of LMs grows, it is imperative that an understanding of their capabilities, intended usage, and development cycle also improves. Model cards are a popular practice for documenting detailed information about an ML model. To automate model card generation, we introduce a dataset of 500 question-answer pairs for 25 ML models that cover crucial aspects of the model, such as its training configurations, datasets, biases, architecture details, and training resources. We employ annotators to extract the answers from the original paper. Further, we explore the capabilities of LMs in generating model cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses. We posit ",
    "link": "http://arxiv.org/abs/2309.12616",
    "context": "Title: Unlocking Model Insights: A Dataset for Automated Model Card Generation. (arXiv:2309.12616v1 [cs.CL])\nAbstract: Language models (LMs) are no longer restricted to ML community, and instruction-tuned LMs have led to a rise in autonomous AI agents. As the accessibility of LMs grows, it is imperative that an understanding of their capabilities, intended usage, and development cycle also improves. Model cards are a popular practice for documenting detailed information about an ML model. To automate model card generation, we introduce a dataset of 500 question-answer pairs for 25 ML models that cover crucial aspects of the model, such as its training configurations, datasets, biases, architecture details, and training resources. We employ annotators to extract the answers from the original paper. Further, we explore the capabilities of LMs in generating model cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses. We posit ",
    "path": "papers/23/09/2309.12616.json",
    "total_tokens": 893,
    "translated_title": "解锁模型洞察力：用于自动生成模型卡片的数据集",
    "translated_abstract": "语言模型（LMs）不再局限于机器学习界，针对指令的LMs引发了自主AI代理的兴起。随着LMs的可访问性增加，提高对其能力、预期用途和开发周期的理解变得至关重要。模型卡片是记录关于ML模型详细信息的常见实践。为了自动化模型卡片的生成，我们引入了一个包含25个ML模型的500个问答对的数据集，涵盖了模型的关键方面，如训练配置、数据集、偏见、架构细节和训练资源。我们雇佣注释者从原始论文中提取答案。此外，我们还通过回答问题来探索LMs在生成模型卡片方面的能力。我们对ChatGPT-3.5、LLaMa和Galactica的初步实验显示出这些LMs对研究论文的理解以及生成事实性文本响应方面存在显著差距。",
    "tldr": "该论文介绍了一个包含25个ML模型的数据集，用于自动生成模型卡片。实验发现目前存在的指令模型在研究论文的理解和生成准确文本方面存在显著差距。",
    "en_tdlr": "This paper introduces a dataset of 500 question-answer pairs for 25 ML models, aiming to automate model card generation. The experiments show a significant gap in the understanding of research papers and generating accurate textual responses by existing instruction models."
}