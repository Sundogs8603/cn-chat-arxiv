{
    "title": "EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])",
    "abstract": "Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making",
    "link": "http://arxiv.org/abs/2309.04579",
    "context": "Title: EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])\nAbstract: Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making",
    "path": "papers/23/09/2309.04579.json",
    "total_tokens": 930,
    "translated_title": "EGOFALLS:一种使用自我中心摄像头进行摔倒检测的视听数据集和基准（arXiv:2309.04579v1 [cs.CV]）",
    "translated_abstract": "对于脆弱人群，如老年人，摔倒往往是严重且常导致死亡的。以往的研究通过依赖单个传感器（图像或加速度计）捕捉数据来解决摔倒的检测问题。在本研究中，我们依赖于从自我中心摄像头捕捉的视频中提取的多模态描述符。我们提出的方法包括一个在提取的描述符之上构建的迟决策融合层。此外，我们还收集了一个新的数据集来评估我们提出的方法。这是我们认为的第一个公共同类数据集。该数据集包含14个受试者的10,948个视频样本。我们进行了消融实验以评估单个特征提取器的性能，视觉信息融合以及视觉和音频信息的融合。此外，我们还进行了内部和外部交叉验证的实验。我们的结果表明，通过迟决策融合将音频和视觉信息相结合可以提高检测性能。",
    "tldr": "这项研究提出了一种使用自我中心摄像头进行摔倒检测的方法，并构建了一个新的视听数据集。通过迟决策融合将音频和视觉信息相结合可以提高检测性能。",
    "en_tdlr": "This paper presents a method for fall detection using egocentric cameras and introduces a new visual-audio dataset. The fusion of audio and visual information through late decision fusion improves the detection performance."
}