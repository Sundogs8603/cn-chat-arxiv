{
    "title": "Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning. (arXiv:2309.13181v1 [cs.LG])",
    "abstract": "Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like P",
    "link": "http://arxiv.org/abs/2309.13181",
    "context": "Title: Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning. (arXiv:2309.13181v1 [cs.LG])\nAbstract: Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like P",
    "path": "papers/23/09/2309.13181.json",
    "total_tokens": 852,
    "translated_title": "分析和利用视频游戏对深度强化学习的计算需求",
    "translated_abstract": "人类通过与环境互动并感知行动结果来学习。深度强化学习算法在视频游戏中能够实现与人类相媲美甚至更好的表现，这在人工智能领域是一个里程碑。然而，目前还不清楚深度强化学习模型成功的原因是视觉表示学习的进步，还是强化学习算法发现更好策略的有效性，或者两者兼具。为了解决这个问题，我们引入了学习挑战诊断器（LCD），这是一种能够单独测量任务中感知和强化学习需求的工具。我们使用LCD在Procgen基准测试中发现了一种新的挑战分类，并证明这些预测既高度可靠，又能指导算法的发展。更广泛地讲，LCD揭示了在像P这样的整个视频游戏基准测试中优化深度强化学习算法时可能出现的多种失败情况。",
    "tldr": "本研究通过引入学习挑战诊断器(LCD)来分析视频游戏对深度强化学习的计算需求，并在Procgen基准测试中发现新的挑战分类。结果表明，LCD的预测可靠且能指导算法的发展。"
}