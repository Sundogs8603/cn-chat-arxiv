{
    "title": "Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])",
    "abstract": "We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the \"effective\" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz",
    "link": "http://arxiv.org/abs/2309.17016",
    "context": "Title: Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])\nAbstract: We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the \"effective\" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz",
    "path": "papers/23/09/2309.17016.json",
    "total_tokens": 891,
    "translated_title": "具有平均光滑度的高效无偏学习",
    "translated_abstract": "我们研究了在非参数回归中无分布限制的平均光滑度概念，该概念由Ashlagi等人（2021）提出，用于衡量函数相对于任意未知潜在分布的\"有效\"光滑度。最近的Hanneke等人（2023）的研究在可实现情况下建立了平均光滑函数的紧密一致收敛界限，并提供了具有高效可实现性的学习算法，但这些结果目前在普遍无偏（即有噪声）情况下尚缺乏类似结果。在这项工作中，我们完全填补了这些差距。首先，我们为无偏设置中的平均光滑类提供了一个无分布一致收敛界限。其次，我们将所得到的样本复杂度与一个具有高效无偏学习算法相匹配。我们的结果以数据的内在几何形状为基础，适用于任何全有界度量空间，并展示了最近在可实现情况下获得的保证。",
    "tldr": "该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。",
    "en_tdlr": "This paper investigates the problem of nonparametric regression based on average smoothness, and proposes uniform convergence bounds and efficient agnostic learning algorithm for the case without distribution restrictions."
}