{
    "title": "nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources. (arXiv:2309.02373v2 [cs.CL] UPDATED)",
    "abstract": "State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. We make our contributions, including configurations, codebase, pre-training insights, and pre-trained models, available to the public.",
    "link": "http://arxiv.org/abs/2309.02373",
    "context": "Title: nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources. (arXiv:2309.02373v2 [cs.CL] UPDATED)\nAbstract: State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. We make our contributions, including configurations, codebase, pre-training insights, and pre-trained models, available to the public.",
    "path": "papers/23/09/2309.02373.json",
    "total_tokens": 917,
    "translated_title": "nanoT5:一种用有限资源进行预训练和微调T5风格模型的PyTorch框架",
    "translated_abstract": "最先进的语言模型如T5已经改变了自然语言处理的格局，但其计算需求限制了大部分研究社区的使用。为了解决这个挑战，我们提出了nanoT5，这是一个经过特别优化的PyTorch框架，用于高效地对T5模型进行预训练和微调。通过借鉴优化器的差异和优化效率，nanoT5使得一个T5-Base模型能够在单个GPU上只需16小时进行预训练，而且不会损失性能。通过引入这个开源框架，我们希望扩大语言建模研究的可访问性，并满足社区对更加用户友好的T5（编码-解码）实现的需求。我们将我们的贡献，包括配置、代码库、预训练洞察力和预训练模型，提供给公众。",
    "tldr": "nanoT5是一个用于高效预训练和微调T5模型的PyTorch框架，通过优化性能和计算效率，它可以在单个GPU上在短时间内进行预训练而不损失性能。这个开源框架为更广泛的语言建模研究提供了更易用的T5实现。",
    "en_tdlr": "nanoT5 is a PyTorch framework for efficient pre-training and fine-tuning of T5 models. It allows T5-Base models to be pre-trained on a single GPU in a short amount of time without sacrificing performance. This open-source framework provides a user-friendly T5 implementation for broader language modeling research."
}