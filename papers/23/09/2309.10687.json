{
    "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning. (arXiv:2309.10687v2 [cs.CL] UPDATED)",
    "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, whic",
    "link": "http://arxiv.org/abs/2309.10687",
    "context": "Title: EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning. (arXiv:2309.10687v2 [cs.CL] UPDATED)\nAbstract: Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, whic",
    "path": "papers/23/09/2309.10687.json",
    "total_tokens": 872,
    "translated_title": "EchoPrompt：指导模型重新表述查询以改善上下文学习",
    "translated_abstract": "通过积极采用推断时提示技术，如零-shot和少-shot提示技术，语言模型在各种任务上取得了令人印象深刻的性能。在这项工作中，我们介绍了一种称为EchoPrompt的简单而有效的方法，该方法提示模型在回答问题之前重新表述查询。EchoPrompt适用于标准和思维链提示的零-shot和少-shot上下文学习。实验结果表明，EchoPrompt在这四个因果语言模型族群的所有设置中都取得了显著改进。这些改进观察到了各种数值推理（例如，GSM8K，SVAMP）、阅读理解（例如DROP）和逻辑推理（例如Coin Flipping）任务中。平均而言，EchoPrompt提高了数值任务中code-davinci-002的零-shot-CoT性能5%，阅读理解任务中提高了13%。我们通过消融研究研究了影响EchoPrompt有效性的因素，其中包括...",
    "tldr": "EchoPrompt是一种简单而有效的方法，通过促使模型重新表述查询来提供改进的上下文学习效果。实验证明，EchoPrompt在多个任务中都取得了显著的性能提升。",
    "en_tdlr": "EchoPrompt is a simple yet effective approach that prompts the model to rephrase its queries, leading to improved in-context learning. Experimental results show substantial performance improvements across various tasks."
}