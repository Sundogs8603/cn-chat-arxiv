{
    "title": "Geometric structure of Deep Learning networks and construction of global ${\\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])",
    "abstract": "In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\\mathbb R}^Q$ with equal dimension $Q\\geq1$. The hidden layers are defined on spaces ${\\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network \"curate\" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.",
    "link": "http://arxiv.org/abs/2309.10639",
    "context": "Title: Geometric structure of Deep Learning networks and construction of global ${\\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])\nAbstract: In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\\mathbb R}^Q$ with equal dimension $Q\\geq1$. The hidden layers are defined on spaces ${\\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network \"curate\" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.",
    "path": "papers/23/09/2309.10639.json",
    "total_tokens": 903,
    "translated_title": "深度学习网络的几何结构和全局${\\mathcal L}^2$最小化器的构建",
    "translated_abstract": "本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\\geq1$的输入和输出空间${\\mathbb R}^Q$。隐藏层也定义在${\\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。",
    "tldr": "本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。"
}