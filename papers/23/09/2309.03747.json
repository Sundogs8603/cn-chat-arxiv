{
    "title": "The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties. (arXiv:2309.03747v1 [cs.CL])",
    "abstract": "In this paper, we adopted a retrospective approach to examine and compare five existing popular sentence encoders, i.e., Sentence-BERT, Universal Sentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their performance on downstream tasks versus their capability to capture basic semantic properties. Initially, we evaluated all five sentence encoders on the popular SentEval benchmark and found that multiple sentence encoders perform quite well on a variety of popular downstream tasks. However, being unable to find a single winner in all cases, we designed further experiments to gain a deeper understanding of their behavior. Specifically, we proposed four semantic evaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym Replacement, and Sentence Jumbling, and evaluated the same five sentence encoders using these criteria. We found that the Sentence-Bert and USE models pass the paraphrasing criterion, with SBERT being the superior between the two. LASER dominates ",
    "link": "http://arxiv.org/abs/2309.03747",
    "context": "Title: The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties. (arXiv:2309.03747v1 [cs.CL])\nAbstract: In this paper, we adopted a retrospective approach to examine and compare five existing popular sentence encoders, i.e., Sentence-BERT, Universal Sentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their performance on downstream tasks versus their capability to capture basic semantic properties. Initially, we evaluated all five sentence encoders on the popular SentEval benchmark and found that multiple sentence encoders perform quite well on a variety of popular downstream tasks. However, being unable to find a single winner in all cases, we designed further experiments to gain a deeper understanding of their behavior. Specifically, we proposed four semantic evaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym Replacement, and Sentence Jumbling, and evaluated the same five sentence encoders using these criteria. We found that the Sentence-Bert and USE models pass the paraphrasing criterion, with SBERT being the superior between the two. LASER dominates ",
    "path": "papers/23/09/2309.03747.json",
    "total_tokens": 954,
    "translated_title": "句子编码器面临的严峻困境：在标准基准上成功，在捕捉基本语义属性上失败",
    "translated_abstract": "本文采用回顾性方法研究并比较了五种现有的流行句子编码器，即Sentence-BERT、Universal Sentence Encoder (USE)、LASER、InferSent和Doc2vec在下游任务的性能和捕捉基本语义属性的能力方面。初始时，我们在流行的SentEval基准上评估了这五种句子编码器，并发现多种句子编码器在各种下游任务上表现良好。然而，在所有情况下都没有找到一个单一的优胜者，因此我们设计了进一步的实验来深入了解它们的行为。具体而言，我们提出了四个语义评估标准，即改写、同义词替换、反义词替换和句子混乱，并使用这些标准评估了同样的五种句子编码器。我们发现Sentence-Bert和USE模型通过了改写标准，其中SBERT在两者之间更为优越。LASER在同义词替换和反义词替换标准方面表现出色。",
    "tldr": "这篇论文调查了五种流行的句子编码器在下游任务表现和捕捉基本语义属性方面的能力。结果发现Sentence-Bert和USE模型在改写标准上表现良好，而LASER在同义词替换和反义词替换方面表现出色。",
    "en_tdlr": "This paper investigates the performance of five popular sentence encoders on downstream tasks and their ability to capture basic semantic properties. The results show that Sentence-Bert and USE models perform well in paraphrasing, while LASER excels in synonym and antonym replacement."
}