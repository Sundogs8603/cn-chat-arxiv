{
    "title": "Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. (arXiv:2309.10285v1 [cs.DC])",
    "abstract": "With the fast growth of parameter size, it becomes increasingly challenging to deploy large generative models as they typically require large GPU memory consumption and massive computation. Unstructured model pruning has been a common approach to reduce both GPU memory footprint and the overall computation while retaining good model accuracy. However, the existing solutions do not provide a highly-efficient support for handling unstructured sparsity on modern GPUs, especially on the highly-structured Tensor Core hardware. Therefore, we propose Flash-LLM for enabling low-cost and highly-efficient large generative model inference with the sophisticated support of unstructured sparsity on high-performance but highly restrictive Tensor Cores. Based on our key observation that the main bottleneck of generative model inference is the several skinny matrix multiplications for which Tensor Cores would be significantly under-utilized due to low computational intensity, we propose a general Load",
    "link": "http://arxiv.org/abs/2309.10285",
    "context": "Title: Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. (arXiv:2309.10285v1 [cs.DC])\nAbstract: With the fast growth of parameter size, it becomes increasingly challenging to deploy large generative models as they typically require large GPU memory consumption and massive computation. Unstructured model pruning has been a common approach to reduce both GPU memory footprint and the overall computation while retaining good model accuracy. However, the existing solutions do not provide a highly-efficient support for handling unstructured sparsity on modern GPUs, especially on the highly-structured Tensor Core hardware. Therefore, we propose Flash-LLM for enabling low-cost and highly-efficient large generative model inference with the sophisticated support of unstructured sparsity on high-performance but highly restrictive Tensor Cores. Based on our key observation that the main bottleneck of generative model inference is the several skinny matrix multiplications for which Tensor Cores would be significantly under-utilized due to low computational intensity, we propose a general Load",
    "path": "papers/23/09/2309.10285.json",
    "total_tokens": 932,
    "translated_title": "Flash-LLM: 通过非结构化稀疏性支持，实现经济高效大规模生成模型推断",
    "translated_abstract": "随着参数规模的快速增长，部署大规模生成模型变得越来越具有挑战性，因为它们通常需要大量的GPU内存消耗和大量的计算。非结构化模型修剪是一种常见的方法，可以降低GPU内存占用和整体计算量，同时保持良好的模型精度。然而，现有的解决方案在处理现代GPU上的非结构化稀疏性方面，并没有提供高效的支持，特别是在高度结构化的张量核心硬件上。因此，我们提出了Flash-LLM，以实现对高性能但具有高限制性的张量核心上的非结构化稀疏性进行低成本和高效的大规模生成模型推断。基于我们的关键观察，生成模型推断的主要瓶颈是若干个skinny矩阵乘法，其中由于计算强度低，Tensor Cores的利用率明显不足。我们提出了一种通用的负载...",
    "tldr": "Flash-LLM是一种能够低成本高效地进行大规模生成模型推断的方法，通过支持非结构化稀疏性，在高性能但具有高限制性的Tensor Cores上工作。它能够降低GPU内存消耗和计算量，并且保持良好的模型精度。"
}