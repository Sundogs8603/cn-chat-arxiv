{
    "title": "Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])",
    "abstract": "A rising trend in theoretical deep learning is to understand why deep learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method that is equivalent to using gradient descent to train a multi-layer infinitely-wide neural network. NTK is a major step forward in the theoretical deep learning because it allows researchers to use traditional mathematical tools to analyze properties of deep neural networks and to explain various neural network techniques from a theoretical view. A natural extension of NTK on graph learning is \\textit{Graph Neural Tangent Kernel (GNTK)}, and researchers have already provide GNTK formulation for graph-level regression and show empirically that this kernel method can achieve similar accuracy as GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is whether solving GNTK regression is equivalent to training an infinite-wide multi-layer GNN using gradient descent. In this paper, we provide three new theoretical results. Fi",
    "link": "http://arxiv.org/abs/2309.07452",
    "context": "Title: Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])\nAbstract: A rising trend in theoretical deep learning is to understand why deep learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method that is equivalent to using gradient descent to train a multi-layer infinitely-wide neural network. NTK is a major step forward in the theoretical deep learning because it allows researchers to use traditional mathematical tools to analyze properties of deep neural networks and to explain various neural network techniques from a theoretical view. A natural extension of NTK on graph learning is \\textit{Graph Neural Tangent Kernel (GNTK)}, and researchers have already provide GNTK formulation for graph-level regression and show empirically that this kernel method can achieve similar accuracy as GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is whether solving GNTK regression is equivalent to training an infinite-wide multi-layer GNN using gradient descent. In this paper, we provide three new theoretical results. Fi",
    "path": "papers/23/09/2309.07452.json",
    "total_tokens": 852,
    "translated_title": "解决图神经切向核是否等价于训练图神经网络？",
    "translated_abstract": "理解深度学习原理的一个新趋势是通过神经切向核（NTK）来解释深度学习的工作原理。NTK是一种等价于使用梯度下降训练多层无限宽神经网络的核方法。NTK在理论深度学习中是一个重要的进展，因为它允许研究人员使用传统的数学工具来分析深度神经网络的性质，并从理论角度解释各种神经网络技术。在图学习中，NTK的一种自然推广是图神经切向核（GNTK），研究人员已经提出了GNTK的公式，并从实证上表明该核方法在各种生物信息学数据集上可以达到与GNN相似的准确性。现在剩下的问题是，解决GNTK回归是否等价于使用梯度下降训练无限宽多层GNN。在本文中，我们提供了三个新的理论结果。",
    "tldr": "本文探讨了解决图神经切向核是否等价于训练图神经网络的问题，提供了三个新的理论结果。",
    "en_tdlr": "This paper investigates the equivalence between solving Graph Neural Tangent Kernel and training Graph Neural Networks. It provides three new theoretical results."
}