{
    "title": "Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])",
    "abstract": "Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.",
    "link": "http://arxiv.org/abs/2309.07168",
    "context": "Title: Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])\nAbstract: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.",
    "path": "papers/23/09/2309.07168.json",
    "total_tokens": 859,
    "translated_title": "通过可达性分析在分层强化学习中对目标空间进行抽象",
    "translated_abstract": "开放式学习极大地受益于使用符号方法进行目标表示，因为它们提供了一种将知识结构化为高效且易于迁移的学习方式。然而，依赖于符号推理的现有分层强化学习方法通常受限于手动目标表示。自主发现符号目标表示的挑战在于它必须保留关键信息，如环境动力学。在这项工作中，我们提出了一种通过紧密关联（即将具有类似任务角色的环境状态集合在一起）的新兴表示发现子目标的发展机制。我们创建了一个逐渐学习此表示以及策略的分层强化学习算法，并使用导航任务进行评估，以展示学到的表示具有可解释性并实现数据效率。",
    "tldr": "本文介绍了一种通过可达性分析在分层强化学习中对目标空间进行抽象的方法，以自动发现符号目标表示。该方法通过将具有相似任务角色的环境状态集合在一起的紧密关联表示来发现子目标，在导航任务中表现出可解释性和数据效率。",
    "en_tdlr": "This paper presents a method for abstracting the goal space in hierarchical reinforcement learning via reachability analysis, aiming to autonomously discover symbolic goal representations. The method discovers subgoals through an emergent representation that groups together environment states with similar roles in tasks, resulting in interpretability and data efficiency in navigation tasks."
}