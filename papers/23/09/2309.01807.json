{
    "title": "Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate",
    "link": "http://arxiv.org/abs/2309.01807",
    "context": "Title: Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate",
    "path": "papers/23/09/2309.01807.json",
    "total_tokens": 892,
    "translated_title": "边际化重要性采样用于离线环境下的策略评估",
    "translated_abstract": "强化学习方法通常效率低下，使得在实际机器人中训练和部署RL策略具有挑战性。即使是在仿真中训练的稳健策略，也需要在实际部署中评估其性能。本文提出了一种在实际世界中评估代理策略性能的新方法。我们的方法结合了模拟器和真实世界的离线数据，使用边际化重要性采样（MIS）框架来评估任何策略的性能。现有的MIS方法面临两个挑战：（1）大的密度比率偏离合理范围，（2）间接监督，需要间接推断比率，从而加剧估计误差。我们的方法通过引入模拟器中目标策略的占位变量，并将密度比率学习为两个可分别学习的项的乘积来解决这些挑战。",
    "tldr": "本文提出了一种新的方法，利用边际化重要性采样框架，在使用模拟器和真实世界的离线数据评估代理策略性能时，解决了大密度比率和间接监督的挑战。",
    "en_tdlr": "This paper proposes a new approach that utilizes the framework of Marginalized Importance Sampling (MIS) to evaluate the performance of agent policies using a simulator and real-world offline data, addressing challenges such as large density ratios and indirect supervision."
}