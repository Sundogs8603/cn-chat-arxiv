{
    "title": "Self-concordant Smoothing for Large-Scale Convex Composite Optimization",
    "abstract": "arXiv:2309.01781v2 Announce Type: replace-cross  Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\\ell_1$-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful fo",
    "link": "https://arxiv.org/abs/2309.01781",
    "context": "Title: Self-concordant Smoothing for Large-Scale Convex Composite Optimization\nAbstract: arXiv:2309.01781v2 Announce Type: replace-cross  Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\\ell_1$-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful fo",
    "path": "papers/23/09/2309.01781.json",
    "total_tokens": 906,
    "translated_title": "大规模凸组合优化的自共轭平滑方法",
    "translated_abstract": "我们引入了一种自共轭平滑的概念，用于最小化两个凸函数的和，其中一个是光滑的，另一个可能是非光滑的。我们方法的关键亮点在于所得问题结构的自然特性，为我们提供了一种变量度量选择方法和一个特别适用于近端牛顿类型算法的步长选择规则。此外，我们高效处理了非光滑函数推动的具体结构，如$\\ell_1$正则化和分组Lasso惩罚。我们证明了两个算法的收敛性：Prox-N-SCORE，一种近端牛顿算法，和Prox-GGN-SCORE，一种近端广义高斯-牛顿算法。Prox-GGN-SCORE算法突出了一种重要的近似程序，有助于显著减少逆Hessian相关的大部分计算开销。这种近似在...",
    "tldr": "提出了适用于大规模凸组合优化问题的自共轭平滑方法，通过变量度量和步长规则优化了近端牛顿算法，有效处理了非光滑函数的结构，提出了Prox-N-SCORE和Prox-GGN-SCORE算法，后者通过重要近似程序显著减少了逆Hessian计算开销。",
    "en_tdlr": "Introduced self-concordant smoothing for large-scale convex composite optimization, optimizing proximal Newton algorithm with variable-metric and step rule, efficiently handling structure of nonsmooth function, proposing Prox-N-SCORE, and Prox-GGN-SCORE algorithms, the latter significantly reducing computational overhead of inverse Hessian through an important approximation procedure."
}