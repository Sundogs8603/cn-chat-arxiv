{
    "title": "Effective Distillation of Table-based Reasoning Ability from LLMs. (arXiv:2309.13182v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their remarkable parameter size and their impressive high requirement of computing resources pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. Nevertheless, prior to our work, there has been no investigation into the prospect of specialising table reasoning skills in smaller models specifically tailored for table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation, with the aim of distilling distilling LLMs into tailored, smaller models specifically designed for table-based reasoning task. Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fin",
    "link": "http://arxiv.org/abs/2309.13182",
    "context": "Title: Effective Distillation of Table-based Reasoning Ability from LLMs. (arXiv:2309.13182v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their remarkable parameter size and their impressive high requirement of computing resources pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. Nevertheless, prior to our work, there has been no investigation into the prospect of specialising table reasoning skills in smaller models specifically tailored for table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation, with the aim of distilling distilling LLMs into tailored, smaller models specifically designed for table-based reasoning task. Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fin",
    "path": "papers/23/09/2309.13182.json",
    "total_tokens": 846,
    "translated_title": "从LLMs中有效提取基于表格推理能力的方法",
    "translated_abstract": "大型语言模型（LLMs）在自然语言处理任务中展现出了卓越的性能。然而，它们庞大的参数和对计算资源的高需求给实际应用带来了挑战。最近的研究发现，LLMs的特定能力，如数值推理，可以通过蒸馏传递给较小的模型。一些研究探讨了利用LLMs进行基于表格推理的潜力。然而，在我们的工作之前，尚未对专门为表格生成任务定制的较小模型的表格推理能力进行研究。在本文中，我们提出了一种新颖的基于表格推理的蒸馏方法，旨在将LLMs蒸馏成专门为基于表格推理任务设计的较小模型。实验结果表明，一个具有0.22亿参数的模型（Flan-T5-base）可以有效地进行蒸馏，并展现出良好的性能。",
    "tldr": "本论文提出了一种从LLMs中提取基于表格推理能力的方法，通过蒸馏将大型模型转化为专门用于基于表格推理任务的小型模型，并取得了良好的性能。"
}