{
    "title": "Dual Student Networks for Data-Free Model Stealing. (arXiv:2309.10058v1 [cs.LG])",
    "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent ",
    "link": "http://arxiv.org/abs/2309.10058",
    "context": "Title: Dual Student Networks for Data-Free Model Stealing. (arXiv:2309.10058v1 [cs.LG])\nAbstract: Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent ",
    "path": "papers/23/09/2309.10058.json",
    "total_tokens": 906,
    "translated_title": "无数据模型窃取的双学生网络",
    "translated_abstract": "现有的无数据模型窃取方法使用生成器产生样本来训练学生模型以匹配目标模型的输出。为此，主要挑战是在没有访问目标模型参数的情况下估计目标模型的梯度，并生成一组多样化的训练样本，充分探索输入空间。我们提出了一种双学生方法，通过对称训练两个学生，为生成器提供了一个标准，生成两个学生在其上意见不一致的样本。一方面，样本上的意见不一致意味着至少有一个学生将样本错误地分类为与目标模型相比。这种对不一致的激励隐含地促使生成器探索输入空间中更多样化的区域。另一方面，我们的方法利用学生模型的梯度间接估计目标模型的梯度。我们展示了这种用于生成器网络的新型训练目标与现有方法等价。",
    "tldr": "该论文提出了一种无数据模型窃取的方法，通过训练两个对称学生来引导生成器生成样本，使得这两个学生对样本的分类意见不一致，从而在生成器中激励探索更多样化的输入空间，并利用学生模型的梯度间接估计目标模型的梯度。",
    "en_tdlr": "This paper presents a data-free method for model stealing, where two symmetrically trained student models guide the generator to generate samples that lead to disagreements between them. This encourages the generator to explore more diverse regions of the input space and indirectly estimates gradients of the target model through the gradients of the student models."
}