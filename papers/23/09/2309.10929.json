{
    "title": "Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])",
    "abstract": "In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.",
    "link": "http://arxiv.org/abs/2309.10929",
    "context": "Title: Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])\nAbstract: In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.",
    "path": "papers/23/09/2309.10929.json",
    "total_tokens": 928,
    "translated_title": "通过潜在属性预训练将小型语言模型特化于复杂风格转移",
    "translated_abstract": "在这项工作中，我们引入了复杂文本风格转移任务的概念，并基于两个广泛适用的场景构建了复杂文本数据集。我们的数据集是其类别中第一个大规模数据集，包含700个重新表述的句子和来自游戏原神的1,000个句子。尽管大型语言模型（LLM）在复杂文本风格转移方面显示出潜力，但它们存在数据隐私问题、网络不稳定性和高部署成本等缺点。为解决这些问题，我们通过对比学习探索了小型模型（小于T5-3B）在隐式风格预训练方面的有效性。我们还提出了一种基于与ChatGPT的人工评价对齐的文本生成质量自动评估方法。最后，我们将我们的方法与现有方法进行了比较，并展示了我们的模型在少样本文本风格转移模型方面达到了最新的性能水平。",
    "tldr": "本文介绍了复杂文本风格转移任务的概念，并通过构建大规模数据集和使用小型模型以及隐式风格预训练进行实验来解决大型语言模型的问题。研究结果表明，我们的模型在少样本文本风格转移方面取得了最新的性能水平。"
}