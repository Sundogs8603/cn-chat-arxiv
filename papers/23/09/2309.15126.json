{
    "title": "From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])",
    "abstract": "Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T",
    "link": "http://arxiv.org/abs/2309.15126",
    "context": "Title: From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])\nAbstract: Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T",
    "path": "papers/23/09/2309.15126.json",
    "total_tokens": 971,
    "translated_title": "从肽到纳米结构：一种用于快速稳定的机器学习力场的欧几里得变换器",
    "translated_abstract": "近年来，基于从头计算的机器学习力场（MLFFs）的发展取得了巨大进展。尽管在测试误差上取得了较低的效果，但MLFFs在分子动力学（MD）模拟中的适用性越来越受到人们的关注，因为其稳定性受到质疑。我们的研究结果表明MLFFs中的等变表示与MD模拟稳定性之间可能存在潜在联系，但计算成本可能限制了它们带来的实际优势。为了解决这个问题，我们提出了一种叫做SO3krates的变换器架构，它结合了稀疏等变表示（欧几里得变量）和自注意机制，可以分离不变和等变信息，消除了昂贵的张量积操作。SO3krates实现了精度、稳定性和速度的独特组合，使得我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。",
    "tldr": "这项研究提出了一种称为SO3krates的欧几里得变换器架构，它通过组合稀疏等变表示和自注意机制，在机器学习力场中实现了精度、稳定性和速度的独特组合，从而使我们能够在前所未有的时间和系统尺度上对物质的量子属性进行深入分析。",
    "en_tdlr": "This study proposes a transformer architecture called SO3krates that combines sparse equivariant representations and a self-attention mechanism in machine learned force fields, achieving a unique combination of accuracy, stability, and speed. This enables insightful analysis of quantum properties of matter on unprecedented time and system size scales."
}