{
    "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])",
    "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further",
    "link": "http://arxiv.org/abs/2309.16429",
    "context": "Title: Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])\nAbstract: We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further",
    "path": "papers/23/09/2309.16429.json",
    "total_tokens": 864,
    "translated_title": "通过文本到视频模型自适应实现多样且对齐的音频到视频生成",
    "translated_abstract": "我们考虑使用来自各种语义类别的自然音频样本来引导生成多样且逼真的视频的任务。对于这个任务，视频需要在全局和时间上与输入音频进行对齐：全局上，输入音频与整个输出视频有语义关联；时间上，输入音频的每个片段都与相应的视频片段关联。我们利用现有的文本驱动视频生成模型和预训练的音频编码器模型。所提出的方法基于一个轻量级的适配器网络，该网络学习将基于音频的表示映射到文本到视频生成模型所期望的输入表示。因此，它也可以实现基于文本、音频以及文本和音频的视频生成，据我们所知，这是首次实现。我们在三个数据集上对我们的方法进行了大量验证，展示了音频-视频样本的显著语义多样性，并进一步验证了我们的方法的性能。",
    "tldr": "通过轻量级适配器网络将音频的表示映射到文本到视频生成模型所期望的输入表示，实现了在全局和时间上与输入音频对齐的多样且逼真的视频生成。",
    "en_tdlr": "Diverse and realistic videos are generated by aligning audio with text and using a lightweight adapter network to map the audio representation to the expected input of a text-to-video generation model. This method achieves alignment between audio and video both globally and temporally."
}