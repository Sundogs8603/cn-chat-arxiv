{
    "title": "Two-Step Knowledge Distillation for Tiny Speech Enhancement. (arXiv:2309.08144v1 [cs.SD])",
    "abstract": "Tiny, causal models are crucial for embedded audio machine learning applications. Model compression can be achieved via distilling knowledge from a large teacher into a smaller student model. In this work, we propose a novel two-step approach for tiny speech enhancement model distillation. In contrast to the standard approach of a weighted mixture of distillation and supervised losses, we firstly pre-train the student using only the knowledge distillation (KD) objective, after which we switch to a fully supervised training regime. We also propose a novel fine-grained similarity-preserving KD loss, which aims to match the student's intra-activation Gram matrices to that of the teacher. Our method demonstrates broad improvements, but particularly shines in adverse conditions including high compression and low signal to noise ratios (SNR), yielding signal to distortion ratio gains of 0.9 dB and 1.1 dB, respectively, at -5 dB input SNR and 63x compression compared to baseline.",
    "link": "http://arxiv.org/abs/2309.08144",
    "context": "Title: Two-Step Knowledge Distillation for Tiny Speech Enhancement. (arXiv:2309.08144v1 [cs.SD])\nAbstract: Tiny, causal models are crucial for embedded audio machine learning applications. Model compression can be achieved via distilling knowledge from a large teacher into a smaller student model. In this work, we propose a novel two-step approach for tiny speech enhancement model distillation. In contrast to the standard approach of a weighted mixture of distillation and supervised losses, we firstly pre-train the student using only the knowledge distillation (KD) objective, after which we switch to a fully supervised training regime. We also propose a novel fine-grained similarity-preserving KD loss, which aims to match the student's intra-activation Gram matrices to that of the teacher. Our method demonstrates broad improvements, but particularly shines in adverse conditions including high compression and low signal to noise ratios (SNR), yielding signal to distortion ratio gains of 0.9 dB and 1.1 dB, respectively, at -5 dB input SNR and 63x compression compared to baseline.",
    "path": "papers/23/09/2309.08144.json",
    "total_tokens": 1089,
    "translated_title": "两步法知识蒸馏用于微弱语音增强",
    "translated_abstract": "对于嵌入式音频机器学习应用而言，微型的因果模型至关重要。模型压缩可以通过将大型教师模型的知识蒸馏到更小的学生模型中来实现。在本文中，我们提出了一种新颖的两步法来进行微弱语音增强模型的蒸馏。与标准方法中使用蒸馏损失和监督损失的加权混合不同，我们首先只使用知识蒸馏（KD）目标来预训练学生模型，然后切换到完全监督训练方案。我们还提出了一种新颖的细粒度相似性保持的KD损失，旨在将学生模型的激活内部格拉姆矩阵与教师模型的格拉姆矩阵匹配。我们的方法在多个方面都取得了显著的改进，尤其在恶劣条件下，包括高压缩和低信噪比（SNR），与基线相比，在输入SNR为-5 dB和63倍压缩下，信号失真比分别提高了0.9 dB和1.1 dB。",
    "tldr": "本文提出了一种新颖的两步法知识蒸馏方法用于微弱语音增强模型。方法首先使用知识蒸馏目标预训练学生模型，然后切换到完全监督训练。同时，引入细粒度相似性保持的知识蒸馏损失，将学生模型的激活内部格拉姆矩阵与教师模型匹配。实验证明，该方法在高压缩和低信噪比条件下表现出显著的性能提升。",
    "en_tdlr": "This paper proposes a novel two-step knowledge distillation approach for tiny speech enhancement models. The method first pre-trains the student model using knowledge distillation objective, then switches to fully supervised training. It also introduces a fine-grained similarity-preserving knowledge distillation loss, which matches the student's intra-activation Gram matrices to those of the teacher. The method demonstrates significant improvements, particularly in high compression and low signal-to-noise ratio conditions."
}