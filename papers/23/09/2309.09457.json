{
    "title": "Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])",
    "abstract": "The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink\" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \\subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o",
    "link": "http://arxiv.org/abs/2309.09457",
    "context": "Title: Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])\nAbstract: The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink\" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \\subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o",
    "path": "papers/23/09/2309.09457.json",
    "total_tokens": 983,
    "translated_title": "不需要计算复杂性无法解决的预言机，在稀疏线性MDP中探索和学习。",
    "translated_abstract": "线性马尔可夫决策过程（MDPs）的基本假设是学习者可以访问已知的特征映射$ \\phi（x，a）$，该映射将状态-动作对映射到$d$维向量，并且奖励和转换是此表示中的线性函数。但是这些特征从哪里来？在没有专家领域知识的情况下，一种诱人的策略是使用“厨房水槽”方法，并希望真实特征包含在一个更大的潜在特征集中。在本文中，我们从特征选择的角度重新审视线性MDP。在$k$-稀疏线性MDP中，存在一个未知的大小为$k$的子集$S \\subset [d]$，其中包含所有相关特征，目标是在与环境的交互中仅经过poly$(k,\\log d)$次学习，学习出近似最优策略。我们的主要结果是这个问题的第一个多项式时间算法。与此相反，早期的研究要么做出了明显的假设，使得探索无关紧要，要么提供了指数复杂度的算法。",
    "tldr": "本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。",
    "en_tdlr": "This paper studies the problem of exploration and learning in sparse linear MDPs, and proposes a polynomial-time algorithm based on feature selection to learn near-optimal policies through interactions with the environment."
}