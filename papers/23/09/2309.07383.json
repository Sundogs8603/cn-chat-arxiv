{
    "title": "Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])",
    "abstract": "This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.",
    "link": "http://arxiv.org/abs/2309.07383",
    "context": "Title: Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])\nAbstract: This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.",
    "path": "papers/23/09/2309.07383.json",
    "total_tokens": 812,
    "translated_title": "在强化学习中使用的近似的某些本地空间中的收敛速度",
    "translated_abstract": "本文研究了在一组再生核希尔伯特空间（RKHS）$H(\\Omega)$中出现的一些值函数近似的收敛速度。通过在特定类的本地空间中构建一个最优控制问题，得到了离线近似的算子方程的强收敛速度，这个算子方程出现在策略迭代中。利用有限维近似空间$H_N$在本地空间$H(\\Omega)$中的功率函数$\\Pwr_{H,N}$，得到了值函数近似误差的显式上界。这些上界具有几何性质，并对值函数近似的收敛性有了一些改进和细化。",
    "tldr": "本文研究了在强化学习中出现的值函数近似在特定本地空间中的收敛速度，提出了运用算子方程进行离线近似的方法，并通过有限维近似空间中的功率函数得到了值函数近似误差的上界。这些结果改进和细化了值函数近似的收敛性。",
    "en_tdlr": "This paper studies the convergence rates of value function approximations in specific native spaces for reinforcement learning, proposes a method for offline approximations using operator equations, and derives upper bounds on the error in value function approximations using power functions in finite dimensional approximation spaces. These results refine and improve the convergence properties of value function approximations."
}