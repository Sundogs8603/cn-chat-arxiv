{
    "title": "Weakly-supervised Automated Audio Captioning via text only training. (arXiv:2309.12242v1 [cs.SD])",
    "abstract": "In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to ",
    "link": "http://arxiv.org/abs/2309.12242",
    "context": "Title: Weakly-supervised Automated Audio Captioning via text only training. (arXiv:2309.12242v1 [cs.SD])\nAbstract: In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to ",
    "path": "papers/23/09/2309.12242.json",
    "total_tokens": 958,
    "translated_title": "弱监督自动音频字幕生成的纯文本训练方法",
    "translated_abstract": "近年来，配对的音频和字幕数据集在自动生成音频片段描述方面取得了显著的成功，即自动音频字幕生成（AAC）。然而，收集足够数量的配对音频和字幕数据是一项费时费力的工作。受到对比性语言-音频预训练（CLAP）的最新进展的启发，我们提出了一种弱监督的方法，只使用文本数据和预训练的CLAP模型来训练AAC模型，减少了对配对目标数据的需求。我们的方法利用CLAP中音频和文本嵌入之间的相似性。在训练过程中，我们学习从CLAP文本嵌入中重建文本，而在推断过程中，我们使用音频嵌入进行解码。为了减小音频和文本嵌入之间的模态差距，我们在训练和推断阶段采用了策略来弥合差距。我们在Clotho和AudioCaps数据集上评估了我们的方法，证明了其能够...",
    "tldr": "通过仅使用文本数据和预训练的语言-音频对比模型（CLAP）来弱监督训练自动音频字幕生成模型，从而减少对配对音频和字幕数据的需求，并通过在训练和推断阶段采用策略来弥合音频和文本之间的差距。"
}