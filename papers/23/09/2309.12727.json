{
    "title": "In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])",
    "abstract": "Large language models (LLMs) have had a huge impact on society due to their impressive capabilities and vast knowledge of the world. Various applications and tools have been created that allow users to interact with these models in a black-box scenario. However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction. This learning process is called in-context training, and it refers to training that is confined to the user's current session or context. In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from interference between information that continually flows in the context, causing it to forget previously learned knowledge, which can reduce the model's performance. Along with showing the problem, w",
    "link": "http://arxiv.org/abs/2309.12727",
    "context": "Title: In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])\nAbstract: Large language models (LLMs) have had a huge impact on society due to their impressive capabilities and vast knowledge of the world. Various applications and tools have been created that allow users to interact with these models in a black-box scenario. However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction. This learning process is called in-context training, and it refers to training that is confined to the user's current session or context. In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from interference between information that continually flows in the context, causing it to forget previously learned knowledge, which can reduce the model's performance. Along with showing the problem, w",
    "path": "papers/23/09/2309.12727.json",
    "total_tokens": 877,
    "translated_title": "聊天式大型语言模型的上下文干扰问题",
    "translated_abstract": "大型语言模型（LLMs）由于其卓越的能力和广泛的世界知识而对社会产生了巨大影响。创建了各种应用和工具，使用户可以以黑盒场景与这些模型交互。然而，这种场景的限制之一是用户无法修改模型的内部知识，添加或修改内部知识的唯一方法是在当前交互过程中明确提及。这种学习过程称为上下文训练，指的是限定在用户当前会话或上下文中进行的训练。上下文学习具有重要的应用，但也存在很少研究的限制。在本文中，我们展示了一项研究，说明了模型可能会遭受在上下文中不断流动的信息之间的干扰，从而导致遗忘先前学到的知识，降低模型的性能。除了展示问题，我们还提出了解决方案来解决该问题。",
    "tldr": "本文研究了聊天式大型语言模型中的上下文干扰问题，发现模型在上下文中持续流动的信息之间可能会遭受干扰，导致遗忘之前学到的知识，降低性能。",
    "en_tdlr": "This paper investigates the issue of in-context interference in chat-based large language models, showing how the models can suffer from interference between continuously flowing information in the context, resulting in forgetting previously learned knowledge and decrease in performance."
}