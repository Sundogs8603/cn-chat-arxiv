{
    "title": "Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])",
    "abstract": "Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe",
    "link": "http://arxiv.org/abs/2309.10066",
    "context": "Title: Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])\nAbstract: Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe",
    "path": "papers/23/09/2309.10066.json",
    "total_tokens": 954,
    "translated_title": "使用大型语言模型的自动个性化印象生成PET报告",
    "translated_abstract": "目的：确定通过fine-tuned大型语言模型(LLMs)是否可以为全身PET报告生成准确的个性化印象。材料和方法：使用teacher-forcing算法在PET报告语料库上训练了12个语言模型，输入是报告发现，参考是临床印象。额外的输入标记编码了阅读医生的身份，使模型能够学习医生特定的报告风格。我们的语料库包括2010年至2022年间从我们机构收集的37,370份回顾性PET报告。通过与两名核医学（NM）医生的质量评分进行30个评估指标的基准测试，最匹配的指标选择了用于专家评估的模型。在部分数据子集中，根据6个质量维度和一个总体实用性评分（5分制），三名核医学医生评估了模型生成的印象和原始临床印象。",
    "tldr": "本研究旨在使用fine-tuned大型语言模型实现自动个性化生成全身PET报告的准确印象。通过训练语言模型并引入阅读医生的身份信息，模型能够学习医生特定的报告风格。研究结果经过专家评估和核医学医生的质量评分认可，证明该方法在实践中具有潜在的应用价值。",
    "en_tdlr": "This study aims to generate accurate personalized impressions for whole-body PET reports using fine-tuned large language models. By training the models with PET report corpus and incorporating reading physician's identity, the models are able to learn physician-specific reporting styles. The results, validated by expert evaluation and quality scores from nuclear medicine physicians, demonstrate the potential applicability of this method in practice."
}