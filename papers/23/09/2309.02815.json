{
    "title": "Near-continuous time Reinforcement Learning for continuous state-action spaces. (arXiv:2309.02815v1 [cs.AI])",
    "abstract": "We consider the Reinforcement Learning problem of controlling an unknown dynamical system to maximise the long-term average reward along a single trajectory. Most of the literature considers system interactions that occur in discrete time and discrete state-action spaces. Although this standpoint is suitable for games, it is often inadequate for mechanical or digital systems in which interactions occur at a high frequency, if not in continuous time, and whose state spaces are large if not inherently continuous. Perhaps the only exception is the Linear Quadratic framework for which results exist both in discrete and continuous time. However, its ability to handle continuous states comes with the drawback of a rigid dynamic and reward structure. This work aims to overcome these shortcomings by modelling interaction times with a Poisson clock of frequency $\\varepsilon^{-1}$, which captures arbitrary time scales: from discrete ($\\varepsilon=1$) to continuous time ($\\varepsilon\\downarrow0$)",
    "link": "http://arxiv.org/abs/2309.02815",
    "context": "Title: Near-continuous time Reinforcement Learning for continuous state-action spaces. (arXiv:2309.02815v1 [cs.AI])\nAbstract: We consider the Reinforcement Learning problem of controlling an unknown dynamical system to maximise the long-term average reward along a single trajectory. Most of the literature considers system interactions that occur in discrete time and discrete state-action spaces. Although this standpoint is suitable for games, it is often inadequate for mechanical or digital systems in which interactions occur at a high frequency, if not in continuous time, and whose state spaces are large if not inherently continuous. Perhaps the only exception is the Linear Quadratic framework for which results exist both in discrete and continuous time. However, its ability to handle continuous states comes with the drawback of a rigid dynamic and reward structure. This work aims to overcome these shortcomings by modelling interaction times with a Poisson clock of frequency $\\varepsilon^{-1}$, which captures arbitrary time scales: from discrete ($\\varepsilon=1$) to continuous time ($\\varepsilon\\downarrow0$)",
    "path": "papers/23/09/2309.02815.json",
    "total_tokens": 924,
    "translated_title": "连续状态动作空间的几乎连续时间强化学习",
    "translated_abstract": "本文研究控制未知动力系统以在单个轨迹上最大化长期平均奖励的强化学习问题。大多数文献考虑的是在离散时间和离散状态-动作空间中发生的系统交互。尽管这种观点适用于游戏，但对于交互频率高（如果不是连续时间）且状态空间大（如果不是固有连续的）的机械或数字系统来说，通常是不够的。也许唯一的例外是线性二次框架，它在离散和连续时间中都有结果。然而，它处理连续状态的能力带来了动态和奖励结构的刚性缺点。本文旨在通过使用频率为ε的泊松时钟对建模交互时间，从离散（ε=1）到连续时间（ε↓0）捕捉任意时间尺度，克服这些缺点。",
    "tldr": "本文提出了一种几乎连续时间的强化学习方法，用于控制连续状态动作空间中的未知动力系统，在单个轨迹上最大化长期平均奖励，并通过使用泊松时钟对交互时间进行建模，从离散到连续时间捕捉任意时间尺度。",
    "en_tdlr": "This paper proposes a near-continuous time reinforcement learning method for controlling unknown dynamical systems in continuous state-action spaces, with the aim of maximizing long-term average rewards along a single trajectory. The method models interaction times using a Poisson clock, allowing for arbitrary time scales from discrete to continuous."
}