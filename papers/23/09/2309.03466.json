{
    "title": "MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])",
    "abstract": "To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \\textsc{Mira} and achieve ",
    "link": "http://arxiv.org/abs/2309.03466",
    "context": "Title: MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])\nAbstract: To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \\textsc{Mira} and achieve ",
    "path": "papers/23/09/2309.03466.json",
    "total_tokens": 922,
    "translated_title": "利用基于模型反演的去除攻击方法破解深度神经网络中的黑盒水印",
    "translated_abstract": "为了保护训练有素的深度神经网络（DNN）的知识产权，黑盒DNN水印已在学术界和工业界越来越受欢迎。这些水印被嵌入到DNN模型在一组特别设计的样本上的预测行为中。最近的研究经验证明，大多数黑盒水印方案对已知的去除攻击具有抵抗能力。本文提出了一种新颖的基于模型反演的去除攻击方法（\\textsc{Mira}），该方法对大多数主流黑盒DNN水印方案都是无关水印的，并且具有高效性。总的来说，我们的攻击流程利用受保护模型的内部信息来恢复和消除水印信息。我们还设计了目标类别检测和恢复样本分割算法，以减少\\textsc{Mira}引起的效用损失，并实现最优的攻击效果。",
    "tldr": "本文提出了一种基于模型反演的去除攻击方法（\\textsc{Mira}），该方法能够有效地破解大多数主流黑盒DNN水印方案，通过利用受保护模型的内部信息来恢复和消除水印信息。"
}