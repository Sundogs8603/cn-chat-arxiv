{
    "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?. (arXiv:2309.07462v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on Natural Language Processing (NLP) tasks, such as Question Answering, Summarization, and Classification. The use of LLMs as evaluators, that can rank or score the output of other models (usually LLMs) has become increasingly popular, due to the limitations of current evaluation techniques including the lack of appropriate benchmarks, metrics, cost, and access to human annotators. While LLMs are capable of handling approximately 100 languages, the majority of languages beyond the top 20 lack systematic evaluation across various tasks, metrics, and benchmarks. This creates an urgent need to scale up multilingual evaluation to ensure a precise understanding of LLM performance across diverse languages. LLM-based evaluators seem like the perfect solution to this problem, as they do not require human annotators, human-created references, or benchmarks and can theoretically be used to evaluate any language covered by the ",
    "link": "http://arxiv.org/abs/2309.07462",
    "context": "Title: Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?. (arXiv:2309.07462v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated impressive performance on Natural Language Processing (NLP) tasks, such as Question Answering, Summarization, and Classification. The use of LLMs as evaluators, that can rank or score the output of other models (usually LLMs) has become increasingly popular, due to the limitations of current evaluation techniques including the lack of appropriate benchmarks, metrics, cost, and access to human annotators. While LLMs are capable of handling approximately 100 languages, the majority of languages beyond the top 20 lack systematic evaluation across various tasks, metrics, and benchmarks. This creates an urgent need to scale up multilingual evaluation to ensure a precise understanding of LLM performance across diverse languages. LLM-based evaluators seem like the perfect solution to this problem, as they do not require human annotators, human-created references, or benchmarks and can theoretically be used to evaluate any language covered by the ",
    "path": "papers/23/09/2309.07462.json",
    "total_tokens": 869,
    "translated_title": "基于大型语言模型的评估器是否是扩展多语言评估的解决方案？",
    "translated_abstract": "大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出了令人印象深刻的性能，如问答、摘要和分类。将LLMs用作评估器，可以对其他模型（通常为LLMs）的输出进行排序或评分，因为当前评估技术存在许多限制，包括缺乏适当的基准、指标、成本和人工标注者的访问性。虽然LLMs能够处理大约100种语言，但大多数语言在各种任务、指标和基准上缺乏系统的评估。这就迫切需要扩展多语言评估，以确保对LLM在各种语言中的性能有准确的理解。基于LLM的评估器似乎是这个问题的完美解决方案，因为它们不需要人工标注者、人工创建的参考和基准，并且理论上可以用来评估任何被覆盖的语言。",
    "tldr": "大型语言模型（LLMs）作为评估器可以解决当前多语言评估的限制和挑战，能够对各种语言中的自然语言处理任务进行有效评估。"
}