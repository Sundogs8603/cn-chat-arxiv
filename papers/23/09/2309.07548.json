{
    "title": "Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])",
    "abstract": "This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the \"optimal\" exponent $p$ in a $p$-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the propo",
    "link": "http://arxiv.org/abs/2309.07548",
    "context": "Title: Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])\nAbstract: This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the \"optimal\" exponent $p$ in a $p$-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the propo",
    "path": "papers/23/09/2309.07548.json",
    "total_tokens": 956,
    "translated_title": "强化学习中的近似贝尔曼映射及其在鲁棒自适应滤波中的应用",
    "translated_abstract": "本文旨在通过引入一类新颖的近似贝尔曼映射，探讨强化学习（RL）的算法理论核心。这些映射定义在重复核希尔伯特空间（RKHS）中，通过充分利用RKHS的近似性质和内积，不论折扣因子的值如何，都属于强大的希尔伯特（Firmly）非扩张映射家族，并具有丰富的设计自由度，甚至可以模拟经典贝尔曼映射的属性，为新的RL设计铺平道路。在提出的映射类上构建了一种近似策略迭代方案，以解决在线选择\"最佳\"指数$p$的问题，从而在线性自适应滤波中抵抗异常值而无需训练数据和异常值的统计属性。合成数据上的数值测试展示了所提方法的卓越性能。",
    "tldr": "本文提出了一种新的近似贝尔曼映射类，并在强化学习和鲁棒自适应滤波中应用。这些映射具有丰富的设计自由度，并可以解决线性自适应滤波中选择指数的问题。数值测试表明这种方法具有卓越的性能。",
    "en_tdlr": "This paper introduces a novel class of proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. These mappings provide ample degrees of design freedom and can solve the problem of selecting the exponent in a norm loss for combating outliers in linear adaptive filtering. Numerical tests demonstrate their superior performance."
}