{
    "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])",
    "abstract": "Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of \"behavior tokens\" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W",
    "link": "http://arxiv.org/abs/2309.00359",
    "context": "Title: Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])\nAbstract: Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of \"behavior tokens\" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W",
    "path": "papers/23/09/2309.00359.json",
    "total_tokens": 952,
    "translated_title": "大型内容和行为模型用于理解、模拟和优化内容和行为",
    "translated_abstract": "香农在引入信息理论的经典论文中将通信分为三个层次：技术层、语义层和效果层。技术层关注的是准确重构传输的符号，而语义层和效果层则涉及推断出的意义及其对接收者的影响。得益于电信技术，第一层问题已经取得了较大的进步，如互联网。大型语言模型（LLM）在第二个目标方面取得了一些进展，但第三层仍然基本上未被触及。第三个问题涉及预测和优化通信以实现期望的接收者行为。LLM在各种任务中显示出了广泛的泛化能力，但无法解决这个问题。表现不佳的原因之一可能是LLM的训练语料库中缺少\"行为标记\"。行为标记定义了在一次通信中的接收者行为，如分享、点赞、点击、购买、转推等。",
    "tldr": "该论文提出了使用大型内容和行为模型来理解、模拟和优化内容和行为。大型语言模型虽然在任务泛化能力方面取得了进展，但还无法解决预测和优化通信以实现期望接收者行为的问题。其中的一个原因可能是训练语料库中缺少\"行为标记\"。"
}