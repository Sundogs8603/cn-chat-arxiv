{
    "title": "Reinforcement Learning with Human Feedback for Realistic Traffic Simulation. (arXiv:2309.00709v1 [cs.AI])",
    "abstract": "In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generati",
    "link": "http://arxiv.org/abs/2309.00709",
    "context": "Title: Reinforcement Learning with Human Feedback for Realistic Traffic Simulation. (arXiv:2309.00709v1 [cs.AI])\nAbstract: In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generati",
    "path": "papers/23/09/2309.00709.json",
    "total_tokens": 890,
    "translated_title": "利用人类反馈进行实际交通模拟的强化学习",
    "translated_abstract": "鉴于真实世界测试的挑战和成本，自动驾驶车辆开发者通常依赖模拟测试来创建可靠的系统。有效模拟的关键要素是融入与人类知识相一致的真实交通模型，这一方面因为需要平衡真实性和多样性而具有挑战性。本研究旨在通过开发一个框架，利用强化学习与人类偏好（RLHF）来增强现有交通模型的真实性。该研究还确定了两个主要挑战：捕捉人类对真实性的微妙偏好以及统一多样的交通模拟模型。为了解决这些问题，我们建议使用人类反馈进行对齐，并采用RLHF因其样本效率高。我们还介绍了第一个用于交通建模中真实性对齐的数据集，以支持此类研究。我们的框架名为TrafficRLHF，在生成现实世界般的交通模拟数据方面展现了其能力。",
    "tldr": "本研究利用强化学习与人类偏好相结合的框架来增强现有交通模型的真实性，并通过引入第一个用于交通建模中真实性对齐的数据集来支持此研究。"
}