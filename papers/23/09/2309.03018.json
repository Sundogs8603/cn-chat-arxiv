{
    "title": "Amortised Inference in Bayesian Neural Networks. (arXiv:2309.03018v1 [stat.ML])",
    "abstract": "Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.  In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional vari",
    "link": "http://arxiv.org/abs/2309.03018",
    "context": "Title: Amortised Inference in Bayesian Neural Networks. (arXiv:2309.03018v1 [stat.ML])\nAbstract: Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.  In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional vari",
    "path": "papers/23/09/2309.03018.json",
    "total_tokens": 810,
    "translated_title": "贝叶斯神经网络的摊销推理",
    "translated_abstract": "元学习是一种框架，机器学习模型在一组数据集上进行训练，以便在测试时对新数据集进行预测。近年来，概率元学习受到研究界的广泛关注，但许多现有的概率元模型存在一个共性问题，即需要大量数据集才能生成具有高质量预测和良好校准不确定性估计的模型。然而，在许多应用中，很难获取这么多数据。在本文中，我们通过在贝叶斯神经网络中对推理进行摊销，引入了摊销伪观测变分推理贝叶斯神经网络（APOVI-BNN），提出了一种更加高效利用数据的概率元学习方法。首先，我们展示了在我们的摊销方案下获取的近似后验与传统变分推理获取的近似后验具有相似或更好的质量。",
    "tldr": "本文提出了一种摊销推理的贝叶斯神经网络方法，通过对推理进行摊销，能够更有效地利用数据进行概率元学习。",
    "en_tdlr": "This paper presents an amortised inference approach in Bayesian neural networks for more data-efficient probabilistic meta-learning."
}