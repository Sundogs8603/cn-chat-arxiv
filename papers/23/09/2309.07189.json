{
    "title": "Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])",
    "abstract": "Federated learning algorithms perform reasonably well on independent and identically distributed (IID) data. They, on the other hand, suffer greatly from heterogeneous environments, i.e., Non-IID data. Despite the fact that many research projects have been done to address this issue, recent findings indicate that they are still sub-optimal when compared to training on IID data. In this work, we carefully analyze the existing methods in heterogeneous environments. Interestingly, we find that regularizing the classifier's outputs is quite effective in preventing performance degradation on Non-IID data. Motivated by this, we propose Learning from Drift (LfD), a novel method for effectively training the model in heterogeneous settings. Our scheme encapsulates two key components: drift estimation and drift regularization. Specifically, LfD first estimates how different the local model is from the global model (i.e., drift). The local model is then regularized such that it does not fall in t",
    "link": "http://arxiv.org/abs/2309.07189",
    "context": "Title: Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])\nAbstract: Federated learning algorithms perform reasonably well on independent and identically distributed (IID) data. They, on the other hand, suffer greatly from heterogeneous environments, i.e., Non-IID data. Despite the fact that many research projects have been done to address this issue, recent findings indicate that they are still sub-optimal when compared to training on IID data. In this work, we carefully analyze the existing methods in heterogeneous environments. Interestingly, we find that regularizing the classifier's outputs is quite effective in preventing performance degradation on Non-IID data. Motivated by this, we propose Learning from Drift (LfD), a novel method for effectively training the model in heterogeneous settings. Our scheme encapsulates two key components: drift estimation and drift regularization. Specifically, LfD first estimates how different the local model is from the global model (i.e., drift). The local model is then regularized such that it does not fall in t",
    "path": "papers/23/09/2309.07189.json",
    "total_tokens": 889,
    "translated_title": "学习漂移：通过漂移正则化在非独立同分布数据上进行联邦学习",
    "translated_abstract": "联邦学习算法在独立同分布（IID）数据上表现良好，但在非独立同分布（Non-IID）数据上却存在较大困难。尽管已经有许多研究项目来解决这个问题，但最近的研究发现它们在与IID数据的训练相比仍然不够优化。在本研究中，我们仔细分析了在非IID环境中存在的方法。有趣的是，我们发现正则化分类器的输出在防止非IID数据性能下降方面非常有效。基于这一发现，我们提出了学习漂移（LfD）的新方法，用于在异构环境中有效训练模型。我们的方案包括两个关键组成部分：漂移估计和漂移正则化。具体而言，LfD首先估计本地模型与全局模型之间的差异（即漂移），然后对本地模型进行正则化以避免过度漂移。",
    "tldr": "通过漂移正则化，我们提出了学习漂移（LfD）方法，它能在非独立同分布数据上有效地训练模型，防止性能下降。",
    "en_tdlr": "We propose Learning from Drift (LfD), a novel method that effectively trains models on non-independent and identically distributed (Non-IID) data by using drift regularization to prevent performance degradation."
}