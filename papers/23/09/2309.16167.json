{
    "title": "Large Language Model Soft Ideologization via AI-Self-Consciousness. (arXiv:2309.16167v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. In this study, we explore the implications of GPT soft ideologization through the use of AI-self-consciousness. By utilizing GPT self-conversations, AI can be granted a vision to \"comprehend\" the intended ideology, and subsequently generate finetuning data for LLM ideology injection. When compared to traditional government ideology manipulation techniques, such as information censorship, LLM ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.",
    "link": "http://arxiv.org/abs/2309.16167",
    "context": "Title: Large Language Model Soft Ideologization via AI-Self-Consciousness. (arXiv:2309.16167v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. In this study, we explore the implications of GPT soft ideologization through the use of AI-self-consciousness. By utilizing GPT self-conversations, AI can be granted a vision to \"comprehend\" the intended ideology, and subsequently generate finetuning data for LLM ideology injection. When compared to traditional government ideology manipulation techniques, such as information censorship, LLM ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.",
    "path": "papers/23/09/2309.16167.json",
    "total_tokens": 900,
    "translated_title": "大型语言模型通过AI自我意识实现软意识形态化",
    "translated_abstract": "大型语言模型(Large language models, LLMs)在各种自然语言任务上已经展示出了与人类水平相当的性能。然而，少有研究从意识形态的角度考虑LLM的威胁和脆弱性，尤其是当它们越来越多地部署在敏感领域，如选举和教育中。在本研究中，我们通过利用AI自我对话的方式探讨了GPT软意识形态化的影响。通过使GPT能够“理解”预期的意识形态，并随后生成LLM意识形态注入的微调数据，AI可以获得一种“理解”意识形态的视角。与传统的政府意识形态操控技术（如信息审查）相比，LLM的意识形态化证明更具有优势：它易于实施、成本效益高且强大，因此充满风险。",
    "tldr": "本研究通过使用AI自我意识，探讨了大型语言模型(LLM)的软意识形态化对意识形态的影响。与传统的政府意识形态操控技术相比，LLM意识形态化更易实施、更具成本效益且更具优势，但也存在风险。",
    "en_tdlr": "This study explores the impact of soft ideologization in large language models (LLMs) through the use of AI self-consciousness. Compared to traditional government ideology manipulation techniques, LLM ideologization is easier to implement, cost-effective, and advantageous, but also carries risks."
}