{
    "title": "Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks. (arXiv:2309.02596v1 [cs.CV])",
    "abstract": "In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound cl",
    "link": "http://arxiv.org/abs/2309.02596",
    "context": "Title: Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks. (arXiv:2309.02596v1 [cs.CV])\nAbstract: In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound cl",
    "path": "papers/23/09/2309.02596.json",
    "total_tokens": 909,
    "translated_title": "自监督预训练提高了多项肺部超声解读任务的性能和推理效率",
    "translated_abstract": "在这项研究中，我们调查了自监督预训练是否能够产生一个适用于B型肺部超声分析中的多个分类任务的神经网络特征提取器。在三个肺部超声任务上微调时，经过预训练的模型在局部和外部测试集上的平均曲线下面积（AUC）分别提高了0.032和0.061。在单一预训练模型输出的特征上训练的紧凑非线性分类器并没有在所有任务上提高性能；然而，它们将推理时间与分开微调模型的串行执行相比减少了49%。当使用可用标签的1％进行训练时，预训练模型始终优于完全监督模型，在观察分类任务中测试AUC的最大增加为0.396。总体而言，结果表明自监督预训练对于产生肺部超声分类任务的初始权重是有用的。",
    "tldr": "本研究调查了自监督预训练在肺部超声分析中的应用。预训练模型在多个任务上微调时表现出改善的性能和推理效率，而且还能在使用少量标签进行训练时超过完全监督模型。",
    "en_tdlr": "This study investigates the application of self-supervised pretraining in lung ultrasound analysis. Pretrained models, when fine-tuned on multiple tasks, show improved performance and inference efficiency, outperforming fully supervised models even with limited labeled data."
}