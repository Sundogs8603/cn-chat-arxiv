{
    "title": "Poster: Self-Supervised Quantization-Aware Knowledge Distillation. (arXiv:2309.13220v1 [cs.CV])",
    "abstract": "Quantization-aware training (QAT) starts with a pre-trained full-precision model and performs quantization during retraining. However, existing QAT works require supervision from the labels and they suffer from accuracy loss due to reduced precision. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD). SQAKD first unifies the forward and backward dynamics of various quantization functions and then reframes QAT as a co-optimization problem that simultaneously minimizes the KL-Loss and the discretization error, in a self-supervised manner. The evaluation shows that SQAKD significantly improves the performance of various state-of-the-art QAT works. SQAKD establishes stronger baselines and does not require extensive labeled training data, potentially making state-of-the-art QAT research more accessible.",
    "link": "http://arxiv.org/abs/2309.13220",
    "context": "Title: Poster: Self-Supervised Quantization-Aware Knowledge Distillation. (arXiv:2309.13220v1 [cs.CV])\nAbstract: Quantization-aware training (QAT) starts with a pre-trained full-precision model and performs quantization during retraining. However, existing QAT works require supervision from the labels and they suffer from accuracy loss due to reduced precision. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD). SQAKD first unifies the forward and backward dynamics of various quantization functions and then reframes QAT as a co-optimization problem that simultaneously minimizes the KL-Loss and the discretization error, in a self-supervised manner. The evaluation shows that SQAKD significantly improves the performance of various state-of-the-art QAT works. SQAKD establishes stronger baselines and does not require extensive labeled training data, potentially making state-of-the-art QAT research more accessible.",
    "path": "papers/23/09/2309.13220.json",
    "total_tokens": 989,
    "translated_title": "海报：自监督的量化感知知识蒸馏",
    "translated_abstract": "量化感知训练(QAT)从预训练的全精度模型开始，在重新训练过程中执行量化。然而，现有的QAT方法需要依赖标签的监督，并且由于降低了精度而导致准确性损失。为了解决这些限制，本文提出了一种新颖的自监督的量化感知知识蒸馏框架(SQAKD)。SQAKD首先统一了各种量化函数的前向和反向动力学，然后以自监督的方式将QAT重新构建为一个共同优化的问题，同时最小化KL损失和离散化误差。评估结果表明，SQAKD显著改善了各种最先进的QAT方法的性能。SQAKD建立了更强的基线，并且不需要大量的标记训练数据，潜在地使得最先进的QAT研究更易于操作。",
    "tldr": "本文提出了一种名为SQAKD的自监督的量化感知知识蒸馏框架，它可以在不需要标签监督和准确性损失的情况下，显著提高各种最先进的QAT方法的性能。通过统一各种量化函数的动力学，并以自监督的方式进行优化，SQAKD为最先进的QAT研究提供了更强的基线，同时不需要大量标记的训练数据，使其更易于操作。",
    "en_tdlr": "This paper proposes a self-supervised quantization-aware knowledge distillation framework called SQAKD, which significantly improves the performance of various state-of-the-art quantization-aware training (QAT) methods without the need for label supervision and accuracy loss. By unifying the dynamics of different quantization functions and optimizing in a self-supervised manner, SQAKD establishes stronger baselines for state-of-the-art QAT research, while also being more accessible due to not requiring extensive labeled training data."
}