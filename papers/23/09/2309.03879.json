{
    "title": "Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])",
    "abstract": "Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology ",
    "link": "http://arxiv.org/abs/2309.03879",
    "context": "Title: Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])\nAbstract: Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology ",
    "path": "papers/23/09/2309.03879.json",
    "total_tokens": 893,
    "translated_title": "提升域自适应的最佳实践",
    "translated_abstract": "在机器学习的实际应用中，分布偏移是非常常见的现象。域自适应（DA）旨在通过提供各种框架来适应模型到部署数据，而无需使用标签来解决这个问题。然而，域偏移场景引发了另一个较为微妙的挑战：在没有带标签的验证集的情况下，为这些自适应算法执行超参数优化（HPO）的困难。DA的不明确的验证协议导致了文献中的不良实践，例如在实际场景中，通过使用目标测试标签来执行HPO，而这些标签是不可用的。这导致了DA研究进展与实际情况相比存在过度乐观的问题。在本文中，我们通过对一套候选验证标准进行基准测试并使用它们来评估流行的自适应算法，分析了在使用良好的评估实践时的域自适应状态。我们展示了域自适应方法论的三个分支都面临挑战。",
    "tldr": "本文通过对一套候选验证标准进行基准测试并使用它们来评估流行的自适应算法，分析了在使用良好的评估实践时的域自适应状态，揭示了域自适应方法论的挑战。"
}