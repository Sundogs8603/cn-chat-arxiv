{
    "title": "Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology. (arXiv:2309.07113v1 [cs.CV])",
    "abstract": "Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select th",
    "link": "http://arxiv.org/abs/2309.07113",
    "context": "Title: Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology. (arXiv:2309.07113v1 [cs.CV])\nAbstract: Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select th",
    "path": "papers/23/09/2309.07113.json",
    "total_tokens": 964,
    "translated_title": "对比性深度编码实现了具有不确定性的机器学习辅助组织病理学",
    "translated_abstract": "深度神经网络模型可以从数百万个组织病理学图像中学习临床相关的特征。然而，为每个医院、每种癌症类型和每个诊断任务生成高质量的注释来训练这些模型是非常费时费力的。另一方面，在某些情况下，虽然缺乏可靠的注释，但是公共领域中有可用的千兆字节级训练数据。在这项工作中，我们探讨了如何有意识地利用这些大型数据集来预训练深度网络以编码信息丰富的表示。然后，我们在一部分带有注释的训练数据上对预训练模型进行微调，以执行特定的下游任务。我们展示了相比于其他最新方法，我们的方法可以在只有1-10%的随机选择注释情况下达到基于补丁的分类的最新水平（SOTA）。此外，我们提出了一种不确定性感知的损失函数，用于量化模型在推断过程中的置信度。量化的不确定性有助于专家进行选择。",
    "tldr": "对比性深度编码能够利用公共领域的大型数据集进行预训练，并以少量随机选择注释进行微调，实现基于补丁的分类的最新水平。此外，引入不确定性感知的损失函数用于量化模型置信度，帮助专家进行选择。",
    "en_tdlr": "Contrastive deep encoding enables pre-training on large publicly available datasets and fine-tuning on a fraction of annotated data, achieving state-of-the-art patch-level classification. Additionally, an uncertainty-aware loss function is proposed to quantify model confidence and assist expert decision-making."
}