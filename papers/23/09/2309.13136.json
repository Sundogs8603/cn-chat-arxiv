{
    "title": "Contextual Emotion Estimation from Image Captions. (arXiv:2309.13136v1 [cs.CV])",
    "abstract": "Emotion estimation in images is a challenging task, typically using computer vision methods to directly estimate people's emotions using face, body pose and contextual cues. In this paper, we explore whether Large Language Models (LLMs) can support the contextual emotion estimation task, by first captioning images, then using an LLM for inference. First, we must understand: how well do LLMs perceive human emotions? And which parts of the information enable them to determine emotions? One initial challenge is to construct a caption that describes a person within a scene with information relevant for emotion perception. Towards this goal, we propose a set of natural language descriptors for faces, bodies, interactions, and environments. We use them to manually generate captions and emotion annotations for a subset of 331 images from the EMOTIC dataset. These captions offer an interpretable representation for emotion estimation, towards understanding how elements of a scene affect emotion",
    "link": "http://arxiv.org/abs/2309.13136",
    "context": "Title: Contextual Emotion Estimation from Image Captions. (arXiv:2309.13136v1 [cs.CV])\nAbstract: Emotion estimation in images is a challenging task, typically using computer vision methods to directly estimate people's emotions using face, body pose and contextual cues. In this paper, we explore whether Large Language Models (LLMs) can support the contextual emotion estimation task, by first captioning images, then using an LLM for inference. First, we must understand: how well do LLMs perceive human emotions? And which parts of the information enable them to determine emotions? One initial challenge is to construct a caption that describes a person within a scene with information relevant for emotion perception. Towards this goal, we propose a set of natural language descriptors for faces, bodies, interactions, and environments. We use them to manually generate captions and emotion annotations for a subset of 331 images from the EMOTIC dataset. These captions offer an interpretable representation for emotion estimation, towards understanding how elements of a scene affect emotion",
    "path": "papers/23/09/2309.13136.json",
    "total_tokens": 982,
    "translated_title": "图像标题中的上下文情感估计",
    "translated_abstract": "图像中的情感估计是一项具有挑战性的任务，通常使用计算机视觉方法通过面部、身体姿势和上下文线索直接估计人们的情感。本文探讨了是否可以通过使用大型语言模型（LLMs）来支持上下文情感估计任务，方法是首先对图像进行字幕生成，然后使用LLM进行推理。首先，我们必须了解：LLMs对人类情感的感知能力如何？以及哪些信息部分使它们能够确定情感？首先的一个挑战是构建一个能够描述场景中的人物并包含与情感感知相关信息的标题。为了实现这个目标，我们提出了一组用于面部、身体、互动和环境的自然语言描述符。我们使用它们为EMOTIC数据集中的331个图像手动生成字幕和情感注释。这些字幕为情感估计提供了一种可解释的表示，以便理解场景中的元素如何影响情感。",
    "tldr": "本文探索使用大型语言模型（LLMs）支持上下文情感估计任务的方法，通过首先对图像进行字幕生成，然后使用LLM进行推理。研究着重于理解LLMs对人类情感的感知能力以及哪些信息能帮助其确定情感。研究还提出了一组自然语言描述符，用于生成字幕和情感注释，以实现情感估计和理解场景中元素对情感的影响。"
}