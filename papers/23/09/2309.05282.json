{
    "title": "Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving. (arXiv:2309.05282v2 [cs.CV] UPDATED)",
    "abstract": "In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.  First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.",
    "link": "http://arxiv.org/abs/2309.05282",
    "context": "Title: Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving. (arXiv:2309.05282v2 [cs.CV] UPDATED)\nAbstract: In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.  First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.",
    "path": "papers/23/09/2309.05282.json",
    "total_tokens": 865,
    "translated_title": "可以通过短信传输发生的事情吗？将预训练语言编码器整合到自动驾驶的轨迹预测模型中",
    "translated_abstract": "在自动驾驶任务中，场景理解是预测周围交通参与者未来行为的第一步。然而，如何表示给定的场景并提取其特征仍然是开放的研究问题。本研究提出了一种新颖的基于文本的交通场景表示，并通过预训练的语言编码器进行处理。首先，我们展示了基于文本的表示与传统的栅格化图像表示相结合，可以得到描述性的场景嵌入。其次，我们在nuScenes数据集上对我们的预测进行了基准测试，并与基准模型相比，显示出显著的改进。第三，我们通过消融研究证明，文本和栅格化图像的联合编码器胜过单独的编码器，确认了两种表示具有互补的优势。",
    "tldr": "本研究提出了将预训练语言编码器整合到自动驾驶的轨迹预测模型中的新方法。通过基于文本的场景表示和经典的栅格化图像表示相结合，得到了描述性的场景嵌入，并在实验中验证了显著的性能改进。",
    "en_tdlr": "This study proposes a new approach integrating pre-trained language encoders into trajectory prediction models for autonomous driving. By combining text-based scene representation with classical rasterized image representation, the study achieves descriptive scene embeddings and demonstrates significant performance improvements in experiments."
}