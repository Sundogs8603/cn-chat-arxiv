{
    "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (arXiv:2309.00254v1 [cs.LG])",
    "abstract": "Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving univer",
    "link": "http://arxiv.org/abs/2309.00254",
    "context": "Title: Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (arXiv:2309.00254v1 [cs.LG])\nAbstract: Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving univer",
    "path": "papers/23/09/2309.00254.json",
    "total_tokens": 887,
    "translated_title": "为什么通用对抗性攻击对大型语言模型有效：几何可能是答案",
    "translated_abstract": "基于Transformer的大型语言模型具有新发能力，在社会中越来越普遍。然而，在对抗攻击的背景下，理解和解释它们的内部工作仍然基本未解决。已经证明基于梯度的通用对抗性攻击对大型语言模型非常有效，由于它们对输入不敏感的特性，可能具有潜在的危险。本研究提出了一个新颖的几何视角，解释了对大型语言模型的通用对抗性攻击。通过对攻击117M参数的GPT-2模型，我们发现证据表明通用对抗性触发器可能是嵌入向量，仅仅近似了其对抗训练区域中的语义信息。这个假设得到了通过白盒模型分析的支持，包括对隐藏表示的降维和相似度测量。我们相信这种关于驱动通用对抗性攻击的潜在机制的新几何视角。",
    "tldr": "本研究从几何视角解释了对大型语言模型的通用对抗性攻击，发现通用对抗性触发器可能是嵌入向量，仅仅近似了其对抗训练区域中的语义信息。",
    "en_tdlr": "This study provides a novel geometric perspective on universal adversarial attacks on large language models, suggesting that universal adversarial triggers could be embedding vectors which approximate the semantic information in their adversarial training region."
}