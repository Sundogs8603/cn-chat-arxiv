{
    "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])",
    "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.  In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?  We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is exp",
    "link": "http://arxiv.org/abs/2309.16155",
    "context": "Title: The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])\nAbstract: Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.  In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?  We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is exp",
    "path": "papers/23/09/2309.16155.json",
    "total_tokens": 937,
    "translated_title": "奖励（不）一致性对RLHF的涓滴效应",
    "translated_abstract": "强化学习来自人类反馈（RLHF）的标准实践涉及优化奖励模型（RM），而RM本身是通过训练来反映人类对期望生成的偏好。一个值得研究的重要主题是RM的（不）一致性 - 即它们能否识别不同提示的语义变化并适当地调整奖励分配 - 以及它们对下游RLHF模型的影响。本文针对RM不一致性提出了一系列相关的研究问题：（1）我们如何衡量奖励模型的一致性？（2）现有的RM有多一致，我们如何改进它们？（3）奖励的不一致性以何种方式影响RLHF模型训练所得的聊天机器人？我们提出了对RM一致性的基准测试策略\"对比提示\"。每个对比提示示例都包含一对具有不同真实响应的词汇相似的指令。一致的RM是期望对这对指令给出相似奖励分配的。",
    "tldr": "本文研究了奖励模型（RM）的一致性对强化学习来自人类反馈（RLHF）模型训练所得的聊天机器人的影响，并提出了一种衡量RM一致性的对比提示的基准测试策略。",
    "en_tdlr": "This paper investigates the impact of reward model (RM) consistency on the chatbots generated by reinforcement learning from human feedback (RLHF) models, and proposes a benchmarking strategy called Contrast Instructions to measure RM consistency."
}