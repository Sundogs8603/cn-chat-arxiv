{
    "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference. (arXiv:2309.04255v1 [cs.NI])",
    "abstract": "Generative tasks, such as text generation and question answering, hold a crucial position in the realm of mobile applications. Due to their sensitivity to privacy concerns, there is a growing demand for their execution directly on mobile devices. Currently, the execution of these generative tasks heavily depends on Large Language Models (LLMs). Nevertheless, the limited memory capacity of these devices presents a formidable challenge to the scalability of such models.  In our research, we introduce LLMCad, an innovative on-device inference engine specifically designed for efficient generative Natural Language Processing (NLP) tasks. The core idea behind LLMCad revolves around model collaboration: a compact LLM, residing in memory, takes charge of generating the most straightforward tokens, while a high-precision LLM steps in to validate these tokens and rectify any identified errors. LLMCad incorporates three novel techniques: (1) Instead of generating candidate tokens in a sequential ",
    "link": "http://arxiv.org/abs/2309.04255",
    "context": "Title: LLMCad: Fast and Scalable On-device Large Language Model Inference. (arXiv:2309.04255v1 [cs.NI])\nAbstract: Generative tasks, such as text generation and question answering, hold a crucial position in the realm of mobile applications. Due to their sensitivity to privacy concerns, there is a growing demand for their execution directly on mobile devices. Currently, the execution of these generative tasks heavily depends on Large Language Models (LLMs). Nevertheless, the limited memory capacity of these devices presents a formidable challenge to the scalability of such models.  In our research, we introduce LLMCad, an innovative on-device inference engine specifically designed for efficient generative Natural Language Processing (NLP) tasks. The core idea behind LLMCad revolves around model collaboration: a compact LLM, residing in memory, takes charge of generating the most straightforward tokens, while a high-precision LLM steps in to validate these tokens and rectify any identified errors. LLMCad incorporates three novel techniques: (1) Instead of generating candidate tokens in a sequential ",
    "path": "papers/23/09/2309.04255.json",
    "total_tokens": 855,
    "translated_title": "LLMCad: 快速且可扩展的移动设备上大型语言模型推断",
    "translated_abstract": "生成任务，如文本生成和问题回答，在移动应用领域中占据重要地位。由于对隐私问题的敏感性，人们越来越需要在移动设备上直接执行这些生成任务。目前，执行这些生成任务的关键是大型语言模型（LLMs）。然而，这些设备的有限内存容量给这种模型的可扩展性带来了巨大挑战。在我们的研究中，我们引入了LLMCad，一种创新的用于高效生成自然语言处理（NLP）任务的移动设备上推断引擎。LLMCad的核心思想围绕模型协作：一个位于内存中的简洁LLM负责生成最直接的标记，而一个高精度的LLM则负责验证这些标记，并纠正任何已识别出的错误。LLMCad包含三种创新技术：（1）不再按顺序生成候选标记，",
    "tldr": "LLMCad是一种用于移动设备上高效推断自然语言处理任务的创新引擎，通过模型协作实现，在有限的内存容量下实现了可扩展性和隐私保护。"
}