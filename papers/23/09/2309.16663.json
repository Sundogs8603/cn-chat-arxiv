{
    "title": "HyperPPO: A scalable method for finding small policies for robotic control. (arXiv:2309.16663v1 [cs.RO])",
    "abstract": "Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo",
    "link": "http://arxiv.org/abs/2309.16663",
    "context": "Title: HyperPPO: A scalable method for finding small policies for robotic control. (arXiv:2309.16663v1 [cs.RO])\nAbstract: Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo",
    "path": "papers/23/09/2309.16663.json",
    "total_tokens": 886,
    "translated_title": "HyperPPO:一种用于机器人控制寻找小策略的可扩展方法",
    "translated_abstract": "针对记忆有限的高性能机器人的神经控制，需要具有较少参数的模型。寻找这些较小的神经网络架构可能耗费大量时间。我们提出了HyperPPO，一种基于策略上的强化学习算法，利用图状超网络同时估计多个神经网络架构的权重。我们的方法估计的网络权重要远小于常用网络的权重，但却能编码高性能策略。我们同时获得多个训练好的策略，并保持采样效率，使用户能够选择适合其计算约束条件的网络架构。我们展示了我们的方法具有良好的可扩展性-更多的训练资源会产生更快收敛到更高性能架构的结果。我们证明了由HyperPPO估计的神经策略能够分散控制Crazyflie2.1四旋翼飞行器。",
    "tldr": "HyperPPO是一种可扩展的方法，用于寻找适用于机器人控制的小策略。它利用图状超网络同时估计多个神经网络架构的权重，可以获得性能优秀的策略，并能够满足用户的计算约束条件。",
    "en_tdlr": "HyperPPO is a scalable method for finding small policies for robotic control. It utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously, allowing for the creation of high-performing policies that satisfy computational constraints."
}