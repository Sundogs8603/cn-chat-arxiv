{
    "title": "PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts. (arXiv:2309.07727v1 [cs.CL])",
    "abstract": "The meanings of words and phrases depend not only on where they are used (contexts) but also on who use them (writers). Pretrained language models (PLMs) are powerful tools for capturing context, but they are typically pretrained and fine-tuned for universal use across different writers. This study aims to improve the accuracy of text understanding tasks by personalizing the fine-tuning of PLMs for specific writers. We focus on a general setting where only the plain text from target writers are available for personalization. To avoid the cost of fine-tuning and storing multiple copies of PLMs for different users, we exhaustively explore using writer-specific prompts to personalize a unified PLM. Since the design and evaluation of these prompts is an underdeveloped area, we introduce and compare different types of prompts that are possible in our setting. To maximize the potential of prompt-based personalized fine-tuning, we propose a personalized intermediate learning based on masked l",
    "link": "http://arxiv.org/abs/2309.07727",
    "context": "Title: PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts. (arXiv:2309.07727v1 [cs.CL])\nAbstract: The meanings of words and phrases depend not only on where they are used (contexts) but also on who use them (writers). Pretrained language models (PLMs) are powerful tools for capturing context, but they are typically pretrained and fine-tuned for universal use across different writers. This study aims to improve the accuracy of text understanding tasks by personalizing the fine-tuning of PLMs for specific writers. We focus on a general setting where only the plain text from target writers are available for personalization. To avoid the cost of fine-tuning and storing multiple copies of PLMs for different users, we exhaustively explore using writer-specific prompts to personalize a unified PLM. Since the design and evaluation of these prompts is an underdeveloped area, we introduce and compare different types of prompts that are possible in our setting. To maximize the potential of prompt-based personalized fine-tuning, we propose a personalized intermediate learning based on masked l",
    "path": "papers/23/09/2309.07727.json",
    "total_tokens": 903,
    "translated_title": "PerPLM: 通过个性化的中间学习和提示对预训练语言模型进行个性化微调",
    "translated_abstract": "单词和短语的含义不仅取决于它们所在的上下文，还取决于使用者。预训练语言模型（PLMs）是捕捉上下文的强大工具，但通常是为了在不同的使用者之间普遍使用而进行预训练和微调的。本研究旨在通过个性化微调PLMs来提高文本理解任务的准确性。我们关注一个只有目标作者的纯文本可用于个性化的一般设置。为了避免对不同用户进行微调和存储多个PLM的成本，我们详细探讨了使用特定作者提示来个性化统一PLM的可能性。由于这些提示的设计和评估是一个尚未发展完善的领域，我们介绍并比较了在我们的环境中可能的不同类型的提示。为了最大限度地发挥基于提示的个性化微调的潜力，我们提出了一种基于掩码的个性化中间学习。",
    "tldr": "PerPLM通过个性化中间学习和提示实现个性化的预训练语言模型微调，并在只有目标作者纯文本的情况下将其扩展到多用户，提高了文本理解任务的准确性。"
}