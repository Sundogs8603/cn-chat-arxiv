{
    "title": "Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])",
    "abstract": "Choosing an appropriate representation of the environment for the underlying decision-making process of the \\gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per",
    "link": "http://arxiv.org/abs/2309.11984",
    "context": "Title: Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])\nAbstract: Choosing an appropriate representation of the environment for the underlying decision-making process of the \\gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per",
    "path": "papers/23/09/2309.11984.json",
    "total_tokens": 894,
    "translated_title": "表示抽象作为强化学习代理的激励：基于机器人抓取的案例研究",
    "translated_abstract": "选择一个适当的环境表示对于强化学习代理的决策过程并不总是简单的。状态表示应该足够包容，以便让代理能够信息地决定其行动，并且足够紧凑，以提高策略训练的样本效率。本文研究了不同状态表示对代理在特定机器人任务（对称和平面物体抓取）上解决问题的影响。从具有完整系统知识的基于模型的方法开始，通过手工数字表示到基于图像的表示，逐渐减少任务特定知识的引入量，定义了一系列状态表示抽象。我们研究了每种表示对代理在仿真环境中解决任务以及学到的策略在真实机器人上的可转移性的影响。结果表明，使用数字状态的强化学习代理能够在模拟环境中解决问题。",
    "tldr": "本文研究了不同状态表示对强化学习代理在机器人抓取任务上的影响，结果显示使用数字状态的代理能够在模拟环境中成功解决问题，并在真实机器人上实现了学习策略的可转移性。"
}