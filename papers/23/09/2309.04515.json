{
    "title": "Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])",
    "abstract": "Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To",
    "link": "http://arxiv.org/abs/2309.04515",
    "context": "Title: Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])\nAbstract: Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To",
    "path": "papers/23/09/2309.04515.json",
    "total_tokens": 940,
    "translated_title": "使用卷积变分瓶颈的隐私保护联合学习",
    "translated_abstract": "梯度反转攻击是联合学习中普遍存在的威胁，它通过利用梯度泄漏来重构本应是私密的训练数据。最近的工作提出了一种基于变分建模的隐私增强模块（PRECODE），以防止梯度泄漏而不损失模型的效用。但在没有进一步的分析之前，已经证明PRECODE成功防止了梯度反转攻击。在本文中，我们做出了多个贡献。首先，我们研究了PRECODE对梯度反转攻击的影响，以揭示其基本的工作原理。我们证明了变分建模将随机性引入了PRECODE和神经网络中后续层的梯度中。这些层的随机梯度阻止了迭代梯度反转攻击的收敛。其次，我们提出了一种攻击方法，通过有意地在攻击优化过程中省略随机梯度，来禁用PRECODE的隐私保护效果。",
    "tldr": "本文研究了使用卷积变分瓶颈的隐私保护联合学习，并发现了PRECODE的工作原理和影响。PRECODE通过引入随机性梯度防止梯度反转攻击的收敛，但也提出了一种攻击方法来禁用其隐私保护效果。"
}