{
    "title": "When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])",
    "abstract": "The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML-models, human and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that transformer pretrained lan",
    "link": "http://arxiv.org/abs/2309.14488",
    "context": "Title: When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])\nAbstract: The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML-models, human and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that transformer pretrained lan",
    "path": "papers/23/09/2309.14488.json",
    "total_tokens": 941,
    "translated_title": "当自动化评估遇上自动化内容生成：在GPT时代审查文本质量",
    "translated_abstract": "机器学习模型在评估和打分文本数据方面的应用已经在包括自然语言处理、信息检索、搜索和推荐以及在线内容可信度评估等各种情境中变得越来越普遍。在机器学习和文本交叉领域的一次重要变革是生成式预训练转换器（GPT）等生成大型语言模型的使用。我们通过经验性评估人类和GPT生成的文本对于基于人类内容训练的机器学习打分模型如何评估内容质量的差异。为此，我们提出了一个分析框架，该框架包括论文评分机器学习模型、人类和机器学习生成的论文，以及一种可以简洁考虑到被调查者类型、提示类型和评估模型的统计模型。我们利用一个丰富的测试样本，涵盖了18,460篇人工生成和GPT生成的论文。我们的基准分析结果显示，预训练转换器线性分类器在评估GPT生成的论文时性能较差。",
    "tldr": "本研究通过对人类和GPT生成的文本进行实证评估，探讨了基于机器学习模型对文本质量评估的差异。结果表明，在评估GPT生成的文本时，预训练转换器模型的性能较差。"
}