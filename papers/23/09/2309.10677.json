{
    "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. (arXiv:2309.10677v2 [cs.CL] UPDATED)",
    "abstract": "Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.",
    "link": "http://arxiv.org/abs/2309.10677",
    "context": "Title: Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. (arXiv:2309.10677v2 [cs.CL] UPDATED)\nAbstract: Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.",
    "path": "papers/23/09/2309.10677.json",
    "total_tokens": 888,
    "translated_title": "通过困惑度估计污染：量化语言模型评估中的记忆化",
    "translated_abstract": "在模型评估中，数据污染变得越来越普遍，因为大型语言模型的大规模训练语料库经常无意中包含基准样本。因此，污染分析已成为可靠模型评估不可避免的一部分。然而，现有的污染分析方法需要访问整个训练数据，这通常对于最新模型来说是保密的。这阻止了社区对这些模型进行严格审计和准确评估其能力。在本文中，我们提出了一种新的方法来在不访问完整训练集的情况下量化污染，即用困惑度来衡量污染的程度。我们的分析提供了证据，表明最近的基础模型在受欢迎的阅读理解和摘要基准中存在显著的记忆化，而多项选择似乎没有那么受污染。",
    "tldr": "本文提出了一种新方法，通过困惑度来量化语言模型评估中的污染，而不需要访问完整的训练数据。研究表明，最近的基础模型在阅读理解和摘要基准中存在显著的记忆化，而多项选择问题则受污染较少。",
    "en_tdlr": "This paper proposes a novel method to quantify contamination in language model evaluation using perplexity, without requiring access to the full training data. The analysis suggests significant memorization in recent foundation models for popular reading comprehension and summarization benchmarks, while multiple choice questions appear to be less contaminated."
}