{
    "title": "Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])",
    "abstract": "Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments",
    "link": "http://arxiv.org/abs/2309.04644",
    "context": "Title: Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])\nAbstract: Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments",
    "path": "papers/23/09/2309.04644.json",
    "total_tokens": 948,
    "translated_title": "探索神经网络坍塌现象：批归一化和权重衰减的影响",
    "translated_abstract": "神经网络坍塌是最近观察到的一种在神经网络分类器最后一层中出现的几何结构。具体来说，神经网络坍塌指的是在神经网络训练的终端阶段，1）最后一层特征的类内变异趋向于零，2）类特征均值构成等角紧框架（ETF），3）最后一层类特征和权重在缩放上相等，4）分类行为崩溃到最近的类中心（NCC）决策规则。本文研究了批归一化和权重衰减对神经网络坍塌现象的影响。我们提出了几何直观的类内和类间余弦相似度度量，捕捉了神经网络坍塌的多个核心方面。利用这个度量，我们在正则化交叉熵损失接近最优时，提供了关于批归一化和权重衰减下神经网络坍塌的理论保证。我们还进行了进一步的实验。",
    "tldr": "本文研究了批归一化和权重衰减对神经网络坍塌现象的影响，并提出了对坍塌现象进行度量的方法。通过理论和实验，证明了在接近最优时，批归一化和权重衰减能够促使神经网络坍塌的出现。"
}