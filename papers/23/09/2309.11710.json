{
    "title": "ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])",
    "abstract": "Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging bench",
    "link": "http://arxiv.org/abs/2309.11710",
    "context": "Title: ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])\nAbstract: Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging bench",
    "path": "papers/23/09/2309.11710.json",
    "total_tokens": 895,
    "translated_title": "ContextRef: 评估图像描述生成的无参考度量",
    "translated_abstract": "无参考度量（例如CLIPScore）使用预训练的视觉-语言模型直接评估图像描述，而无需昂贵的真实参考文本。这样的方法可以促进快速进步，但前提是它们真正与人的偏好判断相吻合。在本文中，我们介绍了ContextRef，一个用于评估无参考度量以实现此对齐的基准。ContextRef有两个组成部分：基于各种已建立的质量维度的人类评分，以及设计了十种不同的鲁棒性检查，旨在发现基本弱点。ContextRef的一个关键方面是图像和描述以上下文方式呈现，反映了先前的研究表明上下文对描述质量的重要性。使用ContextRef，我们评估了各种预训练模型、评分函数和整合上下文的技术。但是，没有一种方法在ContextRef上获得成功，但我们表明经过精心微调可以取得实质性的改善。ContextRef仍然是一个具有挑战性的基准数据集。",
    "tldr": "本文介绍了ContextRef，用于评估图像描述生成的无参考度量。ContextRef包括人类评分和鲁棒性检查，同时考虑了上下文，通过精心微调可以取得实质性改善。",
    "en_tdlr": "This paper introduces ContextRef, a benchmark for evaluating referenceless metrics for image description generation. ContextRef includes human ratings and robustness checks, takes context into account, and shows that careful fine-tuning can lead to substantial improvements."
}