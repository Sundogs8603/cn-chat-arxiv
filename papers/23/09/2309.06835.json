{
    "title": "Safe Reinforcement Learning with Dual Robustness. (arXiv:2309.06835v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) agents are vulnerable to adversarial disturbances, which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneousl",
    "link": "http://arxiv.org/abs/2309.06835",
    "context": "Title: Safe Reinforcement Learning with Dual Robustness. (arXiv:2309.06835v1 [cs.LG])\nAbstract: Reinforcement learning (RL) agents are vulnerable to adversarial disturbances, which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneousl",
    "path": "papers/23/09/2309.06835.json",
    "total_tokens": 923,
    "translated_title": "安全的双重鲁棒性强化学习",
    "tldr": "本文提出了一个系统框架，将安全强化学习和鲁棒强化学习统一起来，通过双重策略迭代方案同时保证安全性和鲁棒性。"
}