{
    "title": "OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch. (arXiv:2309.10706v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. Additionally, we also provide the fine-tunin",
    "link": "http://arxiv.org/abs/2309.10706",
    "context": "Title: OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch. (arXiv:2309.10706v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. Additionally, we also provide the fine-tunin",
    "path": "papers/23/09/2309.10706.json",
    "total_tokens": 968,
    "translated_title": "OpenBA: 一种从头开始预训练的开源15B双语不对称seq2seq模型",
    "translated_abstract": "具有数十亿参数的大型语言模型在各种自然语言处理任务中展示出卓越的性能。本报告介绍了OpenBA，一种开源的15B双语不对称seq2seq模型，为中国定向的开源模型社区贡献了一种LLM变体。我们通过有效和高效的技术来增强OpenBA，并采用三阶段训练策略从零开始训练模型。我们的解决方案在只有380B令牌的情况下也可以取得非常有竞争力的性能，优于BELEBELE基准测试中的LLaMA-70B，MMLU基准测试中的BLOOM-176B和C-Eval（hard）基准测试中的GLM-130B。本报告提供了预训练类似模型的主要细节，包括预训练数据处理，双语文集数据收集，启发我们模型架构设计的经验观察，不同阶段的训练目标以及其他增强技术。此外，我们还提供了微调的",
    "tldr": "OpenBA是一种开源的15B双语不对称seq2seq模型，通过三阶段训练策略从零开始训练，可在只有380B令牌的情况下取得非常有竞争力的性能，为中国定向的开源模型社区贡献了一种LLM变体。"
}