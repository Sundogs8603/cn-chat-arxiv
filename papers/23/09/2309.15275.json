{
    "title": "Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])",
    "abstract": "The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT. In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the st",
    "link": "http://arxiv.org/abs/2309.15275",
    "context": "Title: Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])\nAbstract: The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT. In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the st",
    "path": "papers/23/09/2309.15275.json",
    "total_tokens": 989,
    "translated_title": "Vision Transformer适应的高效低秩反向传播",
    "translated_abstract": "视觉变换器（ViT）的规模不断增大，使得为特定需求对这些大模型进行高效微调成为各种应用中的重大挑战。这个问题源于ViT的线性层中需要的计算量大的矩阵乘法在反向传播过程中。在本文中，我们通过提出一种新的基于Walsh-Hadamard变换的低秩反向传播（LBP-WHT）方法来解决这个问题。直观地说，LBP-WHT将梯度投影到低秩空间，并进行反向传播。这种方法大大减少了适应ViT所需的计算量，因为低秩空间中的矩阵乘法较少占用资源。我们在多个数据集上使用不同模型（ViT、混合卷积-ViT模型）进行了广泛实验证明了我们方法的有效性。例如，在对CIFAR100上的EfficientFormer-L1模型进行适应时，我们的LBP-WHT相比标准方法提高了10.4%的准确率。",
    "tldr": "本论文提出了一种名为LBP-WHT的新方法，用于解决视觉变换器（ViT）在反向传播中对计算资源的需求过高的问题。LBP-WHT方法通过将梯度投影到低秩空间并进行反向传播，显著减少了适应ViT所需的计算量。实验结果表明，LBP-WHT在多个数据集上对不同模型的适应性能都表现出色。"
}