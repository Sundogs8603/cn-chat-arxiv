{
    "title": "Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])",
    "abstract": "Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach \"Invariant CONsistency learning\" (ICON). Exte",
    "link": "http://arxiv.org/abs/2309.12742",
    "context": "Title: Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])\nAbstract: Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach \"Invariant CONsistency learning\" (ICON). Exte",
    "path": "papers/23/09/2309.12742.json",
    "total_tokens": 923,
    "translated_title": "让UDA中的U变得重要：无监督领域自适应的不变一致性学习",
    "translated_abstract": "领域自适应(DA)在处理领域内部的相关特征（如类别身份）和领域特定特征（如环境）之间的虚假相关性时，总是面临挑战，这种相关性在目标领域中不具有普遍性。不幸的是，即使使用了额外的无监督目标领域，现有的无监督DA(UDA)方法仍然受到影响。这是因为源域监督只将目标域样本视为辅助数据（如通过伪标签），而目标域中有价值的去相关线索的固有分布被忽视了。我们建议通过给予两个域相等的地位，来使UDA中的U变得重要。具体而言，我们学习一个不变的分类器，其预测同时与源域标签和目标域聚类一致，从而消除了目标域中的虚假相关性不一致。我们将我们的方法称为\"不变一致性学习\"（ICON）。",
    "tldr": "本文提出了一种名为\"不变一致性学习\"（ICON）的方法，通过给予源域和目标域相等的地位，学习一个不变的分类器，从而消除了在目标域中的虚假相关性不一致。",
    "en_tdlr": "This paper proposes a method called \"Invariant CONsistency learning\" (ICON) that gives equal status to the source and target domains, and learns an invariant classifier to eliminate inconsistent spurious correlation in the target domain."
}