{
    "title": "Large Language Models Are Also Good Prototypical Commonsense Reasoners. (arXiv:2309.13165v1 [cs.CL])",
    "abstract": "Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for th",
    "link": "http://arxiv.org/abs/2309.13165",
    "context": "Title: Large Language Models Are Also Good Prototypical Commonsense Reasoners. (arXiv:2309.13165v1 [cs.CL])\nAbstract: Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for th",
    "path": "papers/23/09/2309.13165.json",
    "total_tokens": 897,
    "translated_title": "大型语言模型也是良好的典型常识推理器",
    "translated_abstract": "常识推理是大型语言模型的关键技能，但在涉及此能力的特定任务中仍存在持续挑战。传统的微调方法可能耗费大量资源，并可能损害模型的泛化能力。此外，像GPT-3.5和Claude这样的最先进语言模型主要通过API调用进行访问，这使得微调模型具有挑战性。为了解决这些问题，我们从大型模型的输出中汲取灵感，针对特定任务半自动地开发了一组新颖的提示，包括任务相关性、支持性证据生成（例如思路链和知识）、多样路径解码等，以帮助模型。在ProtoQA数据集上的实验结果表明，通过更好设计的提示，我们可以在ProtoQA排行榜上取得新的最佳成绩，将最大答案正确率提高了8％，最大错误率降低了4％（突破50％）",
    "tldr": "本论文提出了一种解决大型语言模型常识推理任务的方法，通过设计更好的提示，实现了在ProtoQA数据集上的最新最佳结果，最大答案正确率提高了8％，最大错误率降低了4％。",
    "en_tdlr": "This paper proposes a method to address the challenges of commonsense reasoning in large language models. By designing better prompts, the authors achieved new state-of-the-art results on the ProtoQA dataset, improving the Max Answer@1 score by 8% and the Max Incorrect@1 score by 4%."
}