{
    "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving. (arXiv:2309.16436v1 [cs.AI])",
    "abstract": "Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generat",
    "link": "http://arxiv.org/abs/2309.16436",
    "context": "Title: Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving. (arXiv:2309.16436v1 [cs.AI])\nAbstract: Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generat",
    "path": "papers/23/09/2309.16436.json",
    "total_tokens": 919,
    "translated_title": "神经符号推理用于计划：利用大型语言模型和可满足性求解进行反例引导归纳合成",
    "translated_abstract": "利用强化训练的生成型大型语言模型（LLM）（如GPT-4），可以根据人工提供的指令提示来生成类似人类的回复。除了自然语言回复外，这些模型还被发现在从自然语言提示中生成代码、计划和逻辑规范方面非常有效。尽管它们的准确性得到了显着改善，但这些模型仍然可能产生事实不正确或上下文不恰当的结果，这被称为幻觉现象。这种限制使得在安全关键应用中使用这些模型合成形式化工件变得困难。与文本摘要和问答等任务不同，LLM生成的代码、计划和其他形式化工件中的错误可能具有灾难性。我们认为可以使用可满足性模型检测（SMT）求解器作为演绎推理引擎来分析生成的工件。",
    "tldr": "该论文提出了一种利用大型语言模型和可满足性求解进行反例引导归纳合成的神经符号推理方法，旨在解决大型语言模型生成虚假结果的问题，并为安全关键应用中的形式化工件合成提供解决方案。",
    "en_tdlr": "This paper proposes a neuro-symbolic reasoning approach for planning, using large language models and satisfiability solving to address the problem of false results generated by language models and provide a solution for synthesizing formal artifacts in safety-critical applications."
}