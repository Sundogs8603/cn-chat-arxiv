{
    "title": "PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. (arXiv:2309.02334v1 [cs.LG])",
    "abstract": "Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area i",
    "link": "http://arxiv.org/abs/2309.02334",
    "context": "Title: PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. (arXiv:2309.02334v1 [cs.LG])\nAbstract: Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area i",
    "path": "papers/23/09/2309.02334.json",
    "total_tokens": 948,
    "translated_title": "PolyLUT: 用于超低延迟FPGA基于查找表推理的分段多项式学习",
    "translated_abstract": "可编程门阵列（FPGA）被广泛用于实现深度学习推理。标准的深度神经网络推理涉及交错线性映射和非线性激活函数的计算。以往的超低延迟实现工作在FPGA查找表（LUT）中硬编码了线性映射和非线性激活的组合。我们的工作受到这个想法的启发，即FPGA中的LUT可以用来实现比这更多样化的函数。在本文中，我们提出了一种新的方法来训练用于FPGA部署的神经网络，以多变量多项式作为基本模块。我们的方法利用软件逻辑提供的灵活性，将多项式评估隐藏在LUT中且没有任何开销。我们表明，通过使用多项式模块，我们可以实现相同的准确度，而使用的软件逻辑层数要比使用线性函数要少得多，从而带来显著的延迟和面积的减少。",
    "tldr": "提出了一种名为PolyLUT的新方法，用于训练神经网络在FPGA上进行部署。该方法利用多变量多项式作为基本模块，并利用软逻辑将多项式评估隐藏在FPGA的查找表中，从而实现超低延迟推理，并减少了软件逻辑的层数。",
    "en_tdlr": "A novel approach called PolyLUT is proposed for training neural networks for FPGA deployment. The method utilizes multivariate polynomials as basic building blocks and hides polynomial evaluation inside FPGA lookup tables using soft logic, achieving ultra-low latency inference and reducing the number of layers of soft logic."
}