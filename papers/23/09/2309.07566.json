{
    "title": "Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])",
    "abstract": "Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .",
    "link": "http://arxiv.org/abs/2309.07566",
    "context": "Title: Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])\nAbstract: Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .",
    "path": "papers/23/09/2309.07566.json",
    "total_tokens": 941,
    "translated_title": "基于离散单元的风格转换的语音到语音翻译",
    "translated_abstract": "直接的语音到语音翻译（S2ST）通过离散的自监督表示实现了显著的准确性，但在翻译过程中无法保留源语音的说话人音色。与此同时，高质量说话人平行数据的稀缺性对于学习源语音和目标语音之间的风格转换构成了挑战。我们提出了一个基于自监督模型的离散单元的声学语言模型和风格转换的神经编解码器的S2ST框架。声学语言模型通过自监督上下文学习获得了风格转换的能力，无需依赖于任何说话人平行数据，从而克服了数据稀缺性问题。通过使用大量的训练数据，我们的模型可以在之前未见过的源语言上实现零-shot跨语言风格转换。实验证明，我们的模型生成的翻译语音具有高度的保真度和风格相似性。",
    "tldr": "本研究提出了一种基于离散单元的语音到语音翻译框架，通过自监督学习和神经编解码器实现风格转换，解决了数据稀缺和音色保留的问题。实验结果表明，我们的模型在之前未见的语言上实现了高质量的跨语言风格转换。",
    "en_tdlr": "This research proposes a speech-to-speech translation framework based on discrete units, which utilizes self-supervised learning and neural codec for style transfer, addressing the challenges of data scarcity and preserving speaker timbre. Experimental results demonstrate high-quality cross-lingual style transfer on previously unseen languages."
}