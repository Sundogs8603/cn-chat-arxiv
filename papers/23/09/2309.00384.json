{
    "title": "BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])",
    "abstract": "Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, ",
    "link": "http://arxiv.org/abs/2309.00384",
    "context": "Title: BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])\nAbstract: Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, ",
    "path": "papers/23/09/2309.00384.json",
    "total_tokens": 872,
    "translated_title": "BatchPrompt: 用更少的资源实现更多任务的策略",
    "translated_abstract": "许多LLM（Language Model）被训练来使用基于指令的提示实现零样本或少样本推理。为这些LLM制作提示通常需要用户提供详细的任务描述、上下文和完成示例以及推理上下文的单个示例。本文将这种常规提示基准称为SinglePrompt。然而，在每个推理数据点不一定很长的NLP任务中，提示中的指令和少样本示例的令牌计数可能比数据点的令牌计数大得多，与Fine-tuned BERT等基于编码器的模型相比，导致令牌资源利用率降低。这个成本效率问题影响了推理速度和计算预算，抵消了LLM所能提供的许多好处。本文旨在通过将多个数据点批量打包到一个提示中来缓解上述问题，我们将这种提示策略称为BatchPrompt。这种策略增加了数据点的密度，",
    "tldr": "BatchPrompt是一种提示策略，它通过将多个数据点批量打包到一个提示中来提高LLM的令牌资源利用效率，从而缓解由于令牌计数差异导致的成本效率问题，提高推理速度和计算预算的利用率。"
}