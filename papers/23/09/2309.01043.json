{
    "title": "Distribution learning via neural differential equations: a nonparametric statistical perspective. (arXiv:2309.01043v1 [math.ST])",
    "abstract": "Ordinary differential equations (ODEs), via their induced flow maps, provide a powerful framework to parameterize invertible transformations for the purpose of representing complex probability distributions. While such models have achieved enormous success in machine learning, particularly for generative modeling and density estimation, little is known about their statistical properties. This work establishes the first general nonparametric statistical convergence analysis for distribution learning via ODE models trained through likelihood maximization. We first prove a convergence theorem applicable to arbitrary velocity field classes $\\mathcal{F}$ satisfying certain simple boundary constraints. This general result captures the trade-off between approximation error (`bias') and the complexity of the ODE model (`variance'). We show that the latter can be quantified via the $C^1$-metric entropy of the class $\\mathcal F$. We then apply this general framework to the setting of $C^k$-smoot",
    "link": "http://arxiv.org/abs/2309.01043",
    "context": "Title: Distribution learning via neural differential equations: a nonparametric statistical perspective. (arXiv:2309.01043v1 [math.ST])\nAbstract: Ordinary differential equations (ODEs), via their induced flow maps, provide a powerful framework to parameterize invertible transformations for the purpose of representing complex probability distributions. While such models have achieved enormous success in machine learning, particularly for generative modeling and density estimation, little is known about their statistical properties. This work establishes the first general nonparametric statistical convergence analysis for distribution learning via ODE models trained through likelihood maximization. We first prove a convergence theorem applicable to arbitrary velocity field classes $\\mathcal{F}$ satisfying certain simple boundary constraints. This general result captures the trade-off between approximation error (`bias') and the complexity of the ODE model (`variance'). We show that the latter can be quantified via the $C^1$-metric entropy of the class $\\mathcal F$. We then apply this general framework to the setting of $C^k$-smoot",
    "path": "papers/23/09/2309.01043.json",
    "total_tokens": 829,
    "translated_title": "通过神经微分方程进行分布学习：一种非参数统计角度",
    "translated_abstract": "普通微分方程（ODEs）通过其引导的流映射，为表示复杂概率分布的可逆变换提供了强大的框架。尽管这些模型在机器学习中取得了巨大的成功，特别是对于生成建模和密度估计，但对它们的统计性质知之甚少。本文建立了通过最大似然训练的ODE模型的分布学习的第一个一般非参数统计收敛性分析。我们首先证明了适用于满足一定简单边界约束的任意速度场类$\\mathcal{F}$的收敛定理。这个一般结果捕捉了逼近误差（'偏差'）和ODE模型的复杂性（'方差'）之间的平衡。我们证明了通过$\\mathcal F$类的$C^1$-度量熵可以量化后者。然后，我们将这个通用框架应用于$C^k$-smoot的情况。",
    "tldr": "本文提出了一种通过使用神经微分方程进行分布学习的新方法，并建立了相应的非参数统计收敛性分析。",
    "en_tdlr": "This paper proposes a new approach to distribution learning using neural differential equations and establishes the corresponding nonparametric statistical convergence analysis."
}