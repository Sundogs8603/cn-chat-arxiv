{
    "title": "Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory. (arXiv:2309.02787v1 [cs.LG])",
    "abstract": "Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate vary",
    "link": "http://arxiv.org/abs/2309.02787",
    "context": "Title: Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory. (arXiv:2309.02787v1 [cs.LG])\nAbstract: Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate vary",
    "path": "papers/23/09/2309.02787.json",
    "total_tokens": 909,
    "translated_title": "移动边缘计算中分离学习的动态编码和解码: 利用信息瓶颈理论",
    "translated_abstract": "分离学习是一种保护隐私的分布式学习范式，其中机器学习模型（例如神经网络）被分为两部分（即编码器和解码器）。编码器共享所谓的潜在表示，而不是原始数据，用于模型训练。在移动边缘计算中，可以通过分离学习来训练网络功能（如流量预测），其中编码器位于用户设备（UE），解码器位于边缘网络中。基于数据处理不等式和信息瓶颈（IB）理论，我们提出了一个新的框架和训练机制，以实现传输资源消耗与共享潜在表示的信息性之间的动态平衡，这直接影响预测性能。所提出的训练机制提供了一个编码器-解码器神经网络架构，具有多种复杂性-相关性权衡的模式，实现可调节的性能。适应性可以适应不同的情况。",
    "tldr": "这项研究提出了一种在移动边缘计算中实现动态编码和解码的分离学习框架，利用信息瓶颈理论实现传输资源消耗和共享潜在表示的平衡，从而提高预测性能。",
    "en_tdlr": "This research proposes a split learning framework for dynamic encoding and decoding in mobile-edge computing, leveraging information bottleneck theory to balance transmission resource consumption and the informativeness of shared latent representations, leading to improved predictive performance."
}