{
    "title": "Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs. (arXiv:2309.16357v1 [cs.LG])",
    "abstract": "Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training).  We propose a novel framework called TEMT that exploits t",
    "link": "http://arxiv.org/abs/2309.16357",
    "context": "Title: Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs. (arXiv:2309.16357v1 [cs.LG])\nAbstract: Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training).  We propose a novel framework called TEMT that exploits t",
    "path": "papers/23/09/2309.16357.json",
    "total_tokens": 863,
    "translated_title": "利用预训练语言模型进行文本增强的时间间隔预测在时态知识图中的应用",
    "translated_abstract": "大多数知识图完成方法通过将给定图中的实体和关系映射到向量空间来学习潜在表示。虽然大多数这些方法专注于静态的知识图，但是许多公开可用的知识图包含描述某个事实为真的时间点/时间段的时间信息。这样的图通常被称为时态知识图。此外，知识图还可能包含实体和关系的文本描述。静态的知识图完成方法在表示学习过程中不考虑时间信息和文本描述，而只利用图的结构信息。最近一些研究利用时间信息提高了链接预测的性能，但它们没有利用文本描述，并不支持归纳推理（在训练中未被看到的实体上进行预测）。我们提出了一个新颖的框架TEM和TEM来利用预训练的语言模型和时态知识图中的文本描述，从而进行时间间隔的预测和归纳推理。",
    "tldr": "提出了一个新颖框架TEMT，利用预训练语言模型在文本增强的时态知识图中进行时间间隔预测和归纳推理。"
}