{
    "title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v1 [cs.CL])",
    "abstract": "Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pear",
    "link": "http://arxiv.org/abs/2309.08156",
    "context": "Title: RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v1 [cs.CL])\nAbstract: Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pear",
    "path": "papers/23/09/2309.08156.json",
    "total_tokens": 843,
    "translated_title": "RADE: 基于参考的开放领域对话评估",
    "translated_abstract": "评估开放领域的对话系统具有挑战性，原因在于一对多问题，即除了黄金回应以外还有许多适当的回应。目前，自动评估方法需要更好地与人类保持一致，而可靠的人工评估可能耗时和耗资。为此，我们提出了基于参考的对话评估（RADE）方法，该方法利用预创建的语句作为参考，而不仅仅是黄金回应，以缓解一对多问题。具体而言，",
    "tldr": "基于参考的对话评估（RADE）方法利用预创建的语句作为参考，以解决开放领域对话系统中的一对多问题，并通过共享编码器增强预测。",
    "en_tdlr": "The Reference-Assisted Dialogue Evaluation (RADE) approach uses pre-created utterances as references to address the one-to-many problem in open-domain dialogue systems, and enhances prediction through a shared encoder."
}