{
    "title": "CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity. (arXiv:2309.13307v1 [cs.LG])",
    "abstract": "With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\\mathcal{O}(1)$-bits (aga",
    "link": "http://arxiv.org/abs/2309.13307",
    "context": "Title: CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity. (arXiv:2309.13307v1 [cs.LG])\nAbstract: With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\\mathcal{O}(1)$-bits (aga",
    "path": "papers/23/09/2309.13307.json",
    "total_tokens": 823,
    "translated_title": "CORE: 可证明低通信复杂度的分布式优化中的通用随机重构",
    "translated_abstract": "随着分布式机器学习成为大规模机器学习任务的重要技术，通信复杂度已成为加速训练和扩展机器数量的主要瓶颈。本文提出了一种名为CORE的新技术，它可以用于在没有其他严格条件的情况下压缩机器之间传输的信息，以减少通信复杂度。特别地，我们的技术将向量值信息通过共同随机向量投影到低维空间，并在通信后使用相同的随机噪声对信息进行重构。我们将CORE应用于两个分布式任务，分别是线性模型上的凸优化和通用非凸优化，并设计了新的分布式算法，实现了可证明的更低通信复杂度。例如，我们证明了对于线性模型，基于CORE的算法可以将梯度向量编码为O(1)位。",
    "tldr": "提出了一种名为CORE的新技术，可以在分布式机器学习中通过投影和重构信息来减少通信复杂度，从而实现更高效的训练和扩展性。",
    "en_tdlr": "A new technique named CORE is proposed to reduce communication complexity in distributed machine learning by projecting and reconstructing information, enabling more efficient training and scalability."
}