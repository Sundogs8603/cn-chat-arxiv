{
    "title": "Can humans help BERT gain \"confidence\"?. (arXiv:2309.06580v1 [cs.CL])",
    "abstract": "The advancements in artificial intelligence over the last decade have opened a multitude of avenues for interdisciplinary research. Since the idea of artificial intelligence was inspired by the working of neurons in the brain, it seems pretty practical to combine the two fields and take the help of cognitive data to train AI models. Not only it will help to get a deeper understanding of the technology, but of the brain as well. In this thesis, I conduct novel experiments to integrate cognitive features from the Zurich Cognitive Corpus (ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called BERT. I show how EEG and eye-tracking features from ZuCo can help to increase the performance of the NLP model. I confirm the performance increase with the help of a robustness-checking pipeline and derive a word-EEG lexicon to use in benchmarking on an external dataset that does not have any cognitive features associated with it. Further, I analyze the internal working mechan",
    "link": "http://arxiv.org/abs/2309.06580",
    "context": "Title: Can humans help BERT gain \"confidence\"?. (arXiv:2309.06580v1 [cs.CL])\nAbstract: The advancements in artificial intelligence over the last decade have opened a multitude of avenues for interdisciplinary research. Since the idea of artificial intelligence was inspired by the working of neurons in the brain, it seems pretty practical to combine the two fields and take the help of cognitive data to train AI models. Not only it will help to get a deeper understanding of the technology, but of the brain as well. In this thesis, I conduct novel experiments to integrate cognitive features from the Zurich Cognitive Corpus (ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called BERT. I show how EEG and eye-tracking features from ZuCo can help to increase the performance of the NLP model. I confirm the performance increase with the help of a robustness-checking pipeline and derive a word-EEG lexicon to use in benchmarking on an external dataset that does not have any cognitive features associated with it. Further, I analyze the internal working mechan",
    "path": "papers/23/09/2309.06580.json",
    "total_tokens": 912,
    "translated_title": "人类能帮助BERT获得“信心”吗？",
    "translated_abstract": "过去十年中，人工智能的进步为跨学科研究开辟了多种途径。由于人工智能的灵感来自大脑神经元的工作原理，将这两个领域结合起来，并利用认知数据来训练AI模型似乎是非常实际的。这不仅有助于更深入地理解技术，还有助于理解大脑。在本论文中，我进行了新颖的实验，将苏黎世认知语料库（ZuCo）的认知特征与基于变压器的编码器模型BERT集成。我展示了来自ZuCo的脑电图（EEG）和眼动特征如何帮助提高自然语言处理模型的性能。我利用一个鲁棒性检查流水线确认了性能的提升，并生成了一个单词-EEG词典，用于在没有任何认知特征的外部数据集上进行基准测试。此外，我分析了内部工作机制。",
    "tldr": "本论文研究了如何将苏黎世认知语料库的认知特征与BERT模型集成，证明了脑电图和眼动特征可以提高自然语言处理模型的性能，并开发了一个用于基准测试的词-EEG词典。"
}