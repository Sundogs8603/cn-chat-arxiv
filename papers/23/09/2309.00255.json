{
    "title": "SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])",
    "abstract": "As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during ",
    "link": "http://arxiv.org/abs/2309.00255",
    "context": "Title: SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])\nAbstract: As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during ",
    "path": "papers/23/09/2309.00255.json",
    "total_tokens": 951,
    "translated_title": "SortedNet，每个网络都有自己的位置：面向训练多对一神经网络的广义解决方案",
    "translated_abstract": "随着深度学习模型的规模不断增大，如何在内存和计算约束下找到最优模型变得越来越重要。虽然神经网络的架构和组成部分通常允许以模块化的方式使用，但它们的训练过程并不意识到这种模块化。因此，传统的神经网络训练缺乏在推断过程中适应模型计算负载的灵活性。本文提出了SortedNet，这是一种广义且可扩展的解决方案，用于利用深度神经网络在各个维度上的内在模块化特性，实现高效的动态推断。我们的训练方法采用了一种嵌套结构的子模型和主模型共享参数的方式，并以排序和概率的方式训练它们。这种子网络的排序训练使我们能够在一轮训练中将子网络的数量扩展到数百个。我们利用一种新颖的更新方案在推断过程中动态调整子网络的计算负载。",
    "tldr": "SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。"
}