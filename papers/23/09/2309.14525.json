{
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF. (arXiv:2309.14525v1 [cs.CV])",
    "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in \"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text ",
    "link": "http://arxiv.org/abs/2309.14525",
    "context": "Title: Aligning Large Multimodal Models with Factually Augmented RLHF. (arXiv:2309.14525v1 [cs.CV])\nAbstract: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in \"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text ",
    "path": "papers/23/09/2309.14525.json",
    "total_tokens": 908,
    "translated_title": "用事实增强的RLHF方法对齐大型多模态模型",
    "translated_abstract": "大型多模态模型（LMM）是跨模态构建的，两种模态之间的不对齐可能导致“幻觉”，生成的文本输出没有与上下文中的多模态信息相匹配。为了解决多模态不对齐问题，我们将来自文本领域的强化学习人类反馈（RLHF）方法改进为视觉语言对齐任务，其中需要人工标注者比较两个响应并指出“幻觉”更严重的一个，视觉语言模型则被训练以最大化模拟人类奖励。我们提出了一种新的对齐算法称为Factually Augmented RLHF，它通过附加图像标题和地面真实多选项等额外事实信息来增强奖励模型，从而减轻强化学习人类反馈中的奖励欺骗现象并进一步提高性能。我们还通过之前可用的人工编写的图像-文本数据增强了GPT-4生成的训练数据（用于视觉指令调整）。",
    "tldr": "这项研究提出了一种名为Factually Augmented RLHF的新对齐算法，通过将事实信息加入奖励模型来解决多模态模型之间的幻觉问题，并进一步提高性能。",
    "en_tdlr": "This research proposes a new alignment algorithm called Factually Augmented RLHF, which addresses the issue of misalignment between different modalities in multimodal models by augmenting the reward model with factual information, leading to improved performance."
}