{
    "title": "The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])",
    "abstract": "Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.",
    "link": "http://arxiv.org/abs/2309.07367",
    "context": "Title: The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])\nAbstract: Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.",
    "path": "papers/23/09/2309.07367.json",
    "total_tokens": 839,
    "translated_title": "深度神经网络的核平衡方程",
    "translated_abstract": "在过去十年中，深度神经网络已经展现出许多有益的应用。通过对有限数据集进行训练，网络可以获得广义函数。广义化程度是数据空间中的近似尺度的实现。特别地，在数据集复杂时，尺度不明确。在本文中，我们考虑了一个用于数据集分布估计的网络。我们展示了估计是不稳定的，并且不稳定性取决于数据密度和训练持续时间。我们推导了核平衡方程，它给出了解的简短现象学描述。该方程告诉我们不稳定性的原因和尺度的机制。网络的输出作为预测是数据集的局部平均，平均的尺度沿着方程确定。尺度在训练过程中逐渐减小，最终导致不稳定性。",
    "tldr": "本文提出了深度神经网络的核平衡方程，解释了数据集分布估计中的不稳定性和尺度机制。网络的输出是数据集的局部平均，平均的尺度随着训练逐渐减小并导致不稳定性。",
    "en_tdlr": "This paper presents the kernel-balanced equation for deep neural networks, explaining the instability and scale mechanism in dataset distribution estimation. The network outputs a local average of the dataset, and the scale of averaging gradually decreases during training, resulting in instability."
}