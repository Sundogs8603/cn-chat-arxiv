{
    "title": "Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])",
    "abstract": "While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or \"thoughts\". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence ",
    "link": "http://arxiv.org/abs/2309.07694",
    "context": "Title: Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])\nAbstract: While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or \"thoughts\". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence ",
    "path": "papers/23/09/2309.07694.json",
    "total_tokens": 921,
    "translated_title": "不确定思维树：大型语言模型的推理方法",
    "translated_abstract": "尽管最近引入的不确定思维树（Tree of Thoughts, ToT）在允许大型语言模型（LLMs）通过预见和回溯进行全局决策方面取得了进展，但它忽视了中间决策点或“思维”中的固有局部不确定性。这些固有的局部不确定性，由于LLMs潜在的多样性响应能力，成为推理过程中的重要问题。为了解决这一关键差距，我们引入了不确定思维树（Tree of Uncertain Thoughts, TouT）-一种针对LLMs设计的推理框架。我们的TouT有效地利用了蒙特卡洛丢弃(Monte Carlo Dropout)来量化与LLMs在这些中间步骤上的不同本地响应相关的不确定性得分。通过将这种本地不确定性量化与全局搜索算法结合起来，TouT提高了模型生成响应的精确性。我们通过在两个苛刻的规划任务上进行严格实验证明了我们的方法：24点游戏和迷你填字游戏。",
    "tldr": "本研究提出了一种针对大型语言模型的推理框架——不确定思维树（TouT），它通过利用蒙特卡洛丢弃来量化中间步骤上的本地不确定性，提高了模型生成响应的精确性。",
    "en_tdlr": "This paper introduces a reasoning framework called Tree of Uncertain Thoughts (TouT) for Large Language Models (LLMs), which leverages Monte Carlo Dropout to quantify local uncertainties and enhance the precision of response generation."
}