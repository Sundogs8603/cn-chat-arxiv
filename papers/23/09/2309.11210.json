{
    "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation. (arXiv:2309.11210v1 [eess.AS])",
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
    "link": "http://arxiv.org/abs/2309.11210",
    "context": "Title: Speak While You Think: Streaming Speech Synthesis During Text Generation. (arXiv:2309.11210v1 [eess.AS])\nAbstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
    "path": "papers/23/09/2309.11210.json",
    "total_tokens": 818,
    "translated_title": "边生成文本边合成语音：在文本生成过程中进行流式语音合成",
    "translated_abstract": "大型语言模型（LLMs）展示了令人印象深刻的能力，但是与这些模型的交互大多通过文本进行。使用文本到语音（Text-To-Speech）来合成LLM输出通常会导致显著的延迟，这对于流利的语音对话来说是不切实际的。我们提出了LLM2Speech，一种在LLM生成文本的同时合成语音的架构，可以显著减少延迟。LLM2Speech模仿非流式教师模型的预测，同时限制对未来上下文的暴露，以实现流式合成。它利用LLM的隐藏嵌入，这是文本生成的副产品，包含有信息的语义上下文。实验结果显示，LLM2Speech保持了教师模型的质量，同时减少了延迟，使自然对话成为可能。",
    "tldr": "本论文提出了LLM2Speech，一种在文本生成过程中边合成语音的架构，通过利用隐藏嵌入来实现流式合成，从而显著减少了生成语音的延迟，并保持了高质量的生成文本。",
    "en_tdlr": "This paper proposes LLM2Speech, an architecture that synthesizes speech while generating text, significantly reducing the latency of speech synthesis and maintaining high-quality text generation by utilizing hidden embeddings."
}