{
    "title": "Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])",
    "abstract": "Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are no",
    "link": "http://arxiv.org/abs/2309.16349",
    "context": "Title: Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])\nAbstract: Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are no",
    "path": "papers/23/09/2309.16349.json",
    "total_tokens": 870,
    "translated_title": "人类反馈不是黄金标准",
    "translated_abstract": "人类反馈已成为评估大型语言模型性能的事实标准，并越来越被用作训练目标。然而，不清楚这个单一的“偏好”分数捕捉到生成输出的哪些特性。我们假设偏好分数是主观的，并且容易受到不良偏差的影响。我们对人类反馈在训练和评估中的使用进行了批判性分析，以验证它是否完全捕捉到一系列关键错误标准。我们发现，虽然偏好分数的覆盖范围相当好，但它们在事实性等重要方面表现不足。我们进一步假设偏好分数和错误注释可能受到混杂因素的影响，并利用调试模型生成在两个可能的混杂维度上变化的输出：坚定性和复杂性。我们发现，输出的坚定性会使事实错误的感知率偏差，表明人类注释是不准确的。",
    "tldr": "这个论文对人类反馈在语言模型评估中的使用进行了批判性分析，发现它们无法完全捕捉到关键错误标准，而且容易受到主观偏见和混杂因素的影响。",
    "en_tdlr": "This paper critically analyzes the use of human feedback in evaluating language models and finds that it fails to fully capture crucial error criteria and is susceptible to subjective biases and confounding factors."
}