{
    "title": "On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])",
    "abstract": "Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu",
    "link": "http://arxiv.org/abs/2309.02869",
    "context": "Title: On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])\nAbstract: Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu",
    "path": "papers/23/09/2309.02869.json",
    "total_tokens": 951,
    "translated_title": "降低深度强化学习模型中的不良行为",
    "translated_abstract": "深度强化学习（DRL）在大量应用领域证明了其极大的效用。然而，即使是成功的基于DRL的软件也可能表现出极其不良的行为。这是因为DRL训练是基于最大化一个奖励函数，该函数通常能捕捉到一般趋势，但不能准确捕捉或排除系统的某些行为。在本文中，我们提出了一个新的框架，旨在大幅降低基于DRL的软件的不良行为，同时保持其出色的性能。此外，我们的框架可以帮助工程师对这种不良行为进行可理解的表征。在底层，我们的方法基于从错误的状态-动作对中提取决策树分类器，然后将这些决策树整合到DRL训练循环中，当系统发生错误时对其进行惩罚。我们提供了一个我们方法的概念验证实现，并用它来评估该技术。",
    "tldr": "本论文提出了一个旨在降低深度强化学习模型不良行为的框架，通过从错误的状态-动作对中提取决策树分类器，并将其整合到训练循环中，来惩罚系统错误行为。这一框架在保持卓越性能的同时，为工程师提供了针对不良行为的可理解表征。"
}