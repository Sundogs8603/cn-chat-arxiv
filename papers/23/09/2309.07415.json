{
    "title": "Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])",
    "abstract": "Federated Learning (FL) enables distributed participants (e.g., mobile devices) to train a global model without sharing data directly to a central server. Recent studies have revealed that FL is vulnerable to gradient inversion attack (GIA), which aims to reconstruct the original training samples and poses high risk against the privacy of clients in FL. However, most existing GIAs necessitate control over the server and rely on strong prior knowledge including batch normalization and data distribution information. In this work, we propose Client-side poisoning Gradient Inversion (CGI), which is a novel attack method that can be launched from clients. For the first time, we show the feasibility of a client-side adversary with limited knowledge being able to recover the training samples from the aggregated global model. We take a distinct approach in which the adversary utilizes a malicious model that amplifies the loss of a specific targeted class of interest. When honest clients employ",
    "link": "http://arxiv.org/abs/2309.07415",
    "context": "Title: Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])\nAbstract: Federated Learning (FL) enables distributed participants (e.g., mobile devices) to train a global model without sharing data directly to a central server. Recent studies have revealed that FL is vulnerable to gradient inversion attack (GIA), which aims to reconstruct the original training samples and poses high risk against the privacy of clients in FL. However, most existing GIAs necessitate control over the server and rely on strong prior knowledge including batch normalization and data distribution information. In this work, we propose Client-side poisoning Gradient Inversion (CGI), which is a novel attack method that can be launched from clients. For the first time, we show the feasibility of a client-side adversary with limited knowledge being able to recover the training samples from the aggregated global model. We take a distinct approach in which the adversary utilizes a malicious model that amplifies the loss of a specific targeted class of interest. When honest clients employ",
    "path": "papers/23/09/2309.07415.json",
    "total_tokens": 877,
    "translated_title": "对抗联邦学习中的客户端梯度反演",
    "translated_abstract": "联邦学习（FL）使得分布式参与者（如移动设备）能够在不直接共享数据给中央服务器的情况下训练全局模型。最近的研究发现，FL容易受到梯度反演攻击（GIA）的威胁，该攻击旨在重构原始训练样本，并对FL中的客户端隐私构成高风险。然而，大多数现有的GIA方法需要控制服务器并依赖强先验知识，包括批归一化和数据分布信息。我们提出了一种新的攻击方法——客户端污染梯度反演（CGI），可以从客户端发起。首次展示了一个具有有限知识的客户端对损失恶意模型的利用，能够从聚合的全局模型中恢复训练样本的可行性。",
    "tldr": "这项工作提出了一种新的联邦学习攻击方法——客户端污染梯度反演（CGI），能够从客户端发起并在有限的知识条件下恢复训练样本。这项研究首次展示了客户端对FL的攻击可能性。",
    "en_tdlr": "This work introduces a novel attack method called Client-side poisoning Gradient Inversion (CGI) in federated learning, which enables recovery of training samples from clients with limited knowledge. It demonstrates for the first time the feasibility of client-side attacks on FL."
}