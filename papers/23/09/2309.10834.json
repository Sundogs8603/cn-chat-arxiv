{
    "title": "Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])",
    "abstract": "This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magni",
    "link": "http://arxiv.org/abs/2309.10834",
    "context": "Title: Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])\nAbstract: This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magni",
    "path": "papers/23/09/2309.10834.json",
    "total_tokens": 962,
    "translated_title": "存在稀疏的随机网络：通过正则化增强通信效率的联合学习方法",
    "translated_abstract": "本研究提出了一种新的方法，用于增强随机联合学习中的通信效率，该方法训练过参数化的随机网络。在这种设置下，优化的是二进制掩码，而不是固定的模型权重。该掩码表征了一个能够和较小的目标网络一样好地泛化的稀疏子网络。重要的是，在传统联合学习中，交换的是稀疏的二进制掩码，而不是浮点权重，这样可以将通信成本降低到每个参数最多1个比特。我们发现，之前的最先进的随机方法无法找到能够通过一致的损失目标减少通信和存储开销的稀疏网络。为了解决这个问题，我们建议在本地目标中添加正则化项，通过消除子网络之间的冗余特征来促进更稀疏的解决方案。大量实验表明，在通信和内存效率方面取得了显著的改进，高达五个数量级。",
    "tldr": "本论文提出了一种通过正则化方法增强通信效率的联合学习方法，该方法训练过参数化的随机网络，通过优化二进制掩码来减少通信开销，同时在本地目标中添加正则化项来促进稀疏网络的解决方案。实验证明，在通信和内存效率方面取得了显著的改进。",
    "en_tdlr": "This paper proposes a regularization-based method to enhance communication efficiency in federated learning. The method trains over-parameterized random networks by optimizing binary masks instead of fixed model weights. By exchanging sparse binary masks and adding regularization terms to local objectives, significant improvements in communication and memory efficiency are achieved."
}