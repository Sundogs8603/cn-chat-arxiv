{
    "title": "Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors. (arXiv:2309.16976v1 [cs.LG])",
    "abstract": "Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we e",
    "link": "http://arxiv.org/abs/2309.16976",
    "context": "Title: Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors. (arXiv:2309.16976v1 [cs.LG])\nAbstract: Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we e",
    "path": "papers/23/09/2309.16976.json",
    "total_tokens": 898,
    "translated_title": "在Habana Gaudi处理器上对大型语言模型进行基准测试和深入性能研究",
    "translated_abstract": "Transformer模型在各种机器学习任务中取得了显著的成功，但面临着高计算复杂性和资源需求的问题。自注意机制的二次复杂性在处理长序列和大型数据集时进一步加剧了这些挑战。专用的人工智能硬件加速器，如Habana GAUDI架构，提供了解决这些问题的有希望的解决方案。GAUDI拥有矩阵乘法引擎（MME）和一簇完全可编程的张量处理核心（TPC）。本文探索了使用GAUDI处理器加速基于Transformer的模型的潜力，解决了该过程中的关键挑战。首先，我们在MME和TPC组件之间提供了全面的性能比较，阐明了它们的相对优势和劣势。其次，我们探讨了优化MME和TPC利用率的策略，提供了增强计算效率的实用洞察。第三，我们提出了针对GAUDI构架的Transformer模型优化和性能研究结果。",
    "tldr": "本文基于Habana Gaudi处理器，对使用Transformer模型进行加速的潜力进行了研究，并提供了关键挑战，全面性能比较以及优化策略。"
}