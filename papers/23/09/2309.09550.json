{
    "title": "Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks. (arXiv:2309.09550v2 [cs.NE] UPDATED)",
    "abstract": "The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN mod",
    "link": "http://arxiv.org/abs/2309.09550",
    "context": "Title: Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks. (arXiv:2309.09550v2 [cs.NE] UPDATED)\nAbstract: The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN mod",
    "path": "papers/23/09/2309.09550.json",
    "total_tokens": 981,
    "translated_title": "具有脉冲神经网络的可持续学习的神经路径的自适应重组",
    "translated_abstract": "人脑可以自组织出丰富多样的稀疏神经路径，逐步掌握数百个认知任务。然而，目前大多数深度人工和脉冲神经网络的可持续学习算法无法充分自动调节网络中有限的资源，这导致随着任务增加，性能下降，能耗上升。在本文中，我们提出了一种脑启发式的可持续学习算法，通过自组织调节网络将单一有限的脉冲神经网络（SOR-SNN）重新组织为丰富的稀疏神经路径，以高效应对递增任务。所提出的模型在各种可持续学习任务上表现出了一致的性能优势、能耗和内存容量优势，包括从儿童简单任务到复杂任务、以及泛化的CIFAR100和ImageNet数据集。尤其是，SOR-SNN模型表现出了令人满意的性能、能耗和内存容量。",
    "tldr": "本文提出了一种脑启发式的可持续学习算法，通过自组织调节网络将单一有限的脉冲神经网络重新组织为丰富的稀疏神经路径，以高效应对递增任务，并在各种可持续学习任务以及泛化的CIFAR100和ImageNet数据集上展现出一致的性能优势、能耗和内存容量优势。",
    "en_tdlr": "This paper proposes a brain-inspired continual learning algorithm that reorganizes a single limited spiking neural network into rich sparse neural pathways to efficiently cope with incremental tasks, demonstrating consistent superiority in performance, energy consumption, and memory capacity across various continual learning tasks and generalized CIFAR100 and ImageNet datasets."
}