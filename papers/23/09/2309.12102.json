{
    "title": "SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. (arXiv:2309.12102v1 [cs.CL])",
    "abstract": "We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.",
    "link": "http://arxiv.org/abs/2309.12102",
    "context": "Title: SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. (arXiv:2309.12102v1 [cs.CL])\nAbstract: We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.",
    "path": "papers/23/09/2309.12102.json",
    "total_tokens": 873,
    "translated_title": "SemEval-2022任务7：识别指导文本中暗含或不明确短语的合理解释",
    "translated_abstract": "我们介绍了SemEval-2022任务7，这是一个关于评估指导文本中解释的合理性的共享任务。这个任务的数据集包括手动澄清的操作指南，我们生成了替代的解释并收集了人类的合理性判断。参与系统的任务是在相应的上下文中自动确定解释的合理性。总共有21个参与者参与了这个任务，最好的系统的准确率达到了68.9%。本报告总结了来自8个团队的结果和发现，以及他们的系统描述。最后，我们进行了额外的评估，显示出排名前的参与团队的预测能以75.2%的准确率识别出有多个合理解释的上下文。",
    "tldr": "SemEval-2022任务7旨在识别指导文本中暗含或不明确短语的合理解释。通过人类评判和参与系统的自动判断，最好的系统在此任务上达到了68.9%的准确率。同时，还发现了有多个合理解释的上下文可以以75.2%的准确率被排名前的参与团队的预测所识别出来。",
    "en_tdlr": "SemEval-2022 Task 7 aims to identify plausible clarifications of implicit and underspecified phrases in instructional texts. The best system achieved an accuracy of 68.9% on this task, and it was found that contexts with multiple plausible clarifications can be identified with an accuracy of 75.2% using predictions from the top participating team."
}