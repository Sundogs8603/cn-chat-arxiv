{
    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention. (arXiv:2309.06180v1 [cs.LG])",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequence",
    "link": "http://arxiv.org/abs/2309.06180",
    "context": "Title: Efficient Memory Management for Large Language Model Serving with PagedAttention. (arXiv:2309.06180v1 [cs.LG])\nAbstract: High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequence",
    "path": "papers/23/09/2309.06180.json",
    "total_tokens": 985,
    "translated_title": "大型语言模型服务的高效内存管理：基于分页注意力的方法",
    "translated_abstract": "高吞吐量的大型语言模型（LLMs）服务需要一次批处理足够多的请求。然而，现有系统存在困难，因为每个请求的键-值缓存（KV缓存）内存非常庞大且动态增长和收缩。当管理不当时，这些内存可能会因为碎片化和冗余复制而被大量浪费，从而限制了批处理的规模。为了解决这个问题，我们提出了基于经典虚拟内存和分页技术的注意力算法PagedAttention。在此基础上，我们构建了vLLM，一个LLM服务系统，它能够实现（1）KV缓存内存几乎零浪费和（2）灵活共享KV缓存，以进一步减少内存使用量。我们的评估结果显示，与最先进的系统（如FasterTransformer和Orca）相比，vLLM在保持相同延迟水平的情况下，将流行的LLMs的吞吐量提高了2-4倍。对于更长的序列，这种改进效果更为明显。",
    "tldr": "本论文提出了PagedAttention算法和vLLM系统，通过类似虚拟内存和分页技术的方法，实现了大型语言模型服务中键-值缓存内存的高效管理和灵活共享，大大提高了吞吐量，减少了内存使用量，相对于最先进的系统（如FasterTransformer和Orca）改进了2-4倍的吞吐量。"
}