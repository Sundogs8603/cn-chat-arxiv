{
    "title": "Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])",
    "abstract": "High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the ",
    "link": "http://arxiv.org/abs/2309.12161",
    "context": "Title: Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])\nAbstract: High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the ",
    "path": "papers/23/09/2309.12161.json",
    "total_tokens": 870,
    "translated_title": "大型语言模型中准确计算的代码独白",
    "translated_abstract": "高质量的对话数据集对于采用大型语言模型（LLM）后端的智能辅导系统（ITS）的成功开发至关重要。当这些数据集用于对LLM后端进行细调时，显着提高了学生和ITS之间的互动质量。开发这些数据集的常见策略涉及使用先进的GPT-4模型生成合成的学生-教师对话。然而，当这些对话需要进行物理等科目中常见的复杂计算时，就会出现挑战。尽管其先进的功能，GPT-4在可靠处理甚至简单的乘法任务方面的表现还不够，这是其在这些科目中的局限性。为了解决这些挑战，本文介绍了一种创新的有状态提示设计。我们的方法生成了一个由GPT-4模拟的学生和导师机器人之间的模拟对话。每个学生的回答都会触发一个自言自语（内心独白）。",
    "tldr": "该论文介绍了一种应对大型语言模型在处理复杂计算方面的限制性的创新方法，通过生成模拟对话，并在每个学生回答触发自言自语来提高性能。",
    "en_tdlr": "This paper introduces an innovative approach to address the limitations of large language models in handling complex calculations by generating simulated dialogues and triggering soliloquies for improved performance."
}