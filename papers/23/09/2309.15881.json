{
    "title": "Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])",
    "abstract": "Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.  Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors ",
    "link": "http://arxiv.org/abs/2309.15881",
    "context": "Title: Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])\nAbstract: Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.  Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors ",
    "path": "papers/23/09/2309.15881.json",
    "total_tokens": 992,
    "translated_title": "利用多层嵌入训练增强推荐系统中的跨类别学习",
    "translated_abstract": "现代基于DNN的推荐系统依赖于对稀疏特征进行训练得到的嵌入。输入稀疏性使得很难获得少出现类别的高质量嵌入，因为它们的表示很少更新。我们展示了一种训练时的技术，通过有效的跨类别学习产生优秀的嵌入，并从理论上解释了其令人惊讶的有效性。该方案被称为多层嵌入训练（MLET），通过嵌入层的分解训练嵌入，内部维度高于目标嵌入维度。为了提高推理效率，MLET将训练得到的双层嵌入转换为单层嵌入，从而保持了推理时的模型大小不变。MLET的实验优越性令人困惑，因为其搜索空间并不比单层嵌入更大。MLET对内部维度的强依赖甚至更令人惊讶。我们发展了一个理论来解释这两种行为。",
    "tldr": "该论文提出了一种名为多层嵌入训练（MLET）的训练技术，通过跨类别学习产生优秀的嵌入。该方法通过嵌入层的分解训练嵌入，内部维度高于目标嵌入维度，并在推理时提高了效率。该技术的实验结果令人惊讶，并且通过理论解释了其有效性。"
}