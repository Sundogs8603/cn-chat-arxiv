{
    "title": "Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])",
    "abstract": "The extensive memory footprint of pre-trained language models (PLMs) can hinder deployment in memory-constrained settings, such as cloud environments or on-device. PLMs use embedding matrices to represent extensive vocabularies, forming a large proportion of the model parameters. While previous work towards parameter-efficient PLM development has considered pruning parameters within the transformer layers, pruning the embedding matrix as part of fine-tuning or inference has yet to be explored. We first demonstrate that a significant proportion of the vocabulary remains unused in these scenarios. We then propose a simple yet effective approach that leverages this finding to minimize the memory footprint of the embedding matrix. We show that this approach provides substantial reductions in memory usage across a wide range of models and tasks. Notably, our approach maintains equivalent downstream task performance while allowing a more efficient use of compute resources.",
    "link": "http://arxiv.org/abs/2309.08708",
    "context": "Title: Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])\nAbstract: The extensive memory footprint of pre-trained language models (PLMs) can hinder deployment in memory-constrained settings, such as cloud environments or on-device. PLMs use embedding matrices to represent extensive vocabularies, forming a large proportion of the model parameters. While previous work towards parameter-efficient PLM development has considered pruning parameters within the transformer layers, pruning the embedding matrix as part of fine-tuning or inference has yet to be explored. We first demonstrate that a significant proportion of the vocabulary remains unused in these scenarios. We then propose a simple yet effective approach that leverages this finding to minimize the memory footprint of the embedding matrix. We show that this approach provides substantial reductions in memory usage across a wide range of models and tasks. Notably, our approach maintains equivalent downstream task performance while allowing a more efficient use of compute resources.",
    "path": "papers/23/09/2309.08708.json",
    "total_tokens": 917,
    "translated_title": "经由动态嵌入剪枝实现的预训练语言模型的令人沮丧地简单的内存效率",
    "translated_abstract": "预训练语言模型（PLMs）的广泛内存占用会阻碍其在内存受限环境（如云环境或设备上）的部署。 PLMs使用嵌入矩阵来表示广泛的词汇，构成了模型参数的大部分。尽管之前的工作已经考虑了在Transformer层内剪枝参数以提高参数效率，但在微调或推理过程中剪枝嵌入矩阵尚未被探索。我们首先证明了在这些情况下有一个显著比例的词汇未被使用。然后，我们提出了一个简单而有效的方法，利用这一发现来最小化嵌入矩阵的内存占用。我们展示了这种方法在各种模型和任务中都能显著降低内存使用量。值得注意的是，我们的方法在保持下游任务性能的同时允许更高效地使用计算资源。",
    "tldr": "该论文提出了一种简单但有效的方法，通过动态嵌入剪枝来减小预训练语言模型的内存占用。该方法在各种模型和任务中都能显著降低内存使用量，同时保持相当的下游任务性能，实现更高效地利用计算资源。",
    "en_tdlr": "This paper proposes a simple yet effective approach to reduce the memory footprint of pre-trained language models (PLMs) through dynamic embedding pruning. The approach significantly reduces memory usage across various models and tasks while maintaining equivalent downstream task performance, allowing for more efficient utilization of compute resources."
}