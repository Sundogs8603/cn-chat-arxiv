{
    "title": "PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction. (arXiv:2309.03099v1 [q-bio.BM])",
    "abstract": "Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non-fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide's potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide's capacity to resist non-specific interactions. This m",
    "link": "http://arxiv.org/abs/2309.03099",
    "context": "Title: PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction. (arXiv:2309.03099v1 [q-bio.BM])\nAbstract: Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non-fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide's potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide's capacity to resist non-specific interactions. This m",
    "path": "papers/23/09/2309.03099.json",
    "total_tokens": 904,
    "translated_title": "PeptideBERT: 基于Transformer的肽性质预测语言模型",
    "translated_abstract": "最近语言模型的进展使得蛋白质建模领域拥有了强大的工具，因为蛋白序列可以被表示为文本。具体而言，通过利用Transformer，可以在没有显式结构数据的情况下进行序列到性质预测。在这项工作中，受到大型语言模型 (LLM) 的最新进展的启发，我们引入了PeptideBERT，这是一个用于预测肽的三个关键性质（溶血性、溶解性和阻垢性）的蛋白质语言模型。PeptideBert利用了预先训练过的具有12个注意力头和12个隐藏层的ProtBERT Transformer模型，然后对这个预训练模型进行了微调以适应三个下游任务。我们的模型在预测溶血性方面达到了最先进水平，并在预测肽的抗非特异性相互作用能力方面也取得了显著的准确性。",
    "tldr": "PeptideBERT是一种基于Transformer的语言模型，用于预测肽的溶血性、溶解性和阻垢性。通过利用预训练模型和微调，该模型在这些任务上取得了最先进的性能。",
    "en_tdlr": "PeptideBERT is a Transformer-based language model for predicting the hemolysis, solubility, and non-fouling properties of peptides. By utilizing a pretrained model and fine-tuning, our model achieves state-of-the-art performance on these tasks."
}