{
    "title": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning. (arXiv:2309.03839v1 [cs.RO])",
    "abstract": "Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated nav",
    "link": "http://arxiv.org/abs/2309.03839",
    "context": "Title: Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning. (arXiv:2309.03839v1 [cs.RO])\nAbstract: Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated nav",
    "path": "papers/23/09/2309.03839.json",
    "total_tokens": 942,
    "translated_title": "使用离线强化学习为自适应人机界面引导引导",
    "translated_abstract": "自适应界面可以帮助用户在给定嘈杂的高维命令信号（例如来自脑-计算机接口）的情况下执行顺序决策任务，例如机器人遥操作。人在循环机器学习的最新进展使得这些系统能够通过与用户进行交互来改进，但在实际应用中往往受限于从单个用户收集的数据量。本文提出了一种强化学习算法，通过离线预训练和在线微调训练界面以将原始命令信号映射到动作。为了解决嘈杂命令信号和稀疏奖励带来的挑战，我们提出了一种用于表示和推断用户对给定轨迹的长期意图的新方法。我们主要通过一个用户研究评估我们的方法在帮助仅通过嘈杂的高维输入通道进行通信的用户时的能力，研究中有12名参与者进行了模拟导航任务。",
    "tldr": "本文提出了一种结合离线预训练和在线微调的强化学习算法来训练自适应人机界面，以帮助用户通过嘈杂的高维输入通道进行顺序决策任务。研究中开发了一种独特的方法来表示和推断用户对给定轨迹的长期意图。",
    "en_tdlr": "This paper proposes a reinforcement learning algorithm that combines offline pre-training and online fine-tuning to train an adaptive human-machine interface that assists users in sequential decision-making tasks through noisy, high-dimensional input channels. A novel method is developed to represent and infer the user's long-term intent for a given trajectory."
}