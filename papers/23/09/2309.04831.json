{
    "title": "Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])",
    "abstract": "We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con",
    "link": "http://arxiv.org/abs/2309.04831",
    "context": "Title: Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])\nAbstract: We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con",
    "path": "papers/23/09/2309.04831.json",
    "total_tokens": 960,
    "translated_title": "在学习估计器设计中，递减时域策略搜索的全局收敛性研究",
    "translated_abstract": "我们引入了递减时域策略梯度（RHPG）算法，这是第一个在学习最优线性估计器设计（即卡尔曼滤波器）方面具有可证明全局收敛性的PG算法。值得注意的是，RHPG算法不需要任何关于系统的先验知识作为初始化，也不需要目标系统是开环稳定的。RHPG的关键是将普通的PG（或其他策略搜索方向）集成到动态规划的外循环中，将在策略参数中受限且非凸的无穷时域KF问题分解为一系列无约束且强凸的静态估计问题，从而实现全局收敛。我们进一步对RHPG的优化路线进行了详细分析，并详细说明了算法的收敛性和样本复杂度保证。这项工作是针对控制器设计开展强化学习算法的初步尝试。",
    "tldr": "RHPG算法是第一个在学习最优线性估计器设计方面具有可证明全局收敛性的PG算法，通过将普通的PG集成到动态规划的外循环中，将无约束且强凸的静态估计问题分解成受限且非凸的无穷时域KF问题，从而实现了全局收敛。"
}