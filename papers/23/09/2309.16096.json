{
    "title": "Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])",
    "abstract": "The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.",
    "link": "http://arxiv.org/abs/2309.16096",
    "context": "Title: Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])\nAbstract: The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.",
    "path": "papers/23/09/2309.16096.json",
    "total_tokens": 853,
    "translated_title": "对抗样本可能是可以避免的：数据集中性在对抗鲁棒性中的作用",
    "translated_abstract": "现代机器学习分类器对于对抗样本的敏感性引发了理论结果，暗示这些对抗样本可能是不可避免的。然而，这些结果可能过于一般化，无法应用于自然数据分布。事实上，人类在涉及视觉的任务中表现出相当的鲁棒性。这种明显的矛盾推动我们更深入地探索一个问题：对抗样本是否真的是不可避免的？在这项工作中，我们理论上证明了数据分布的一个关键属性——对输入空间的小容积子集的集中程度，决定了是否存在一个鲁棒分类器。我们进一步证明，在数据分布集中在低维线性子空间的并集时，利用数据结构自然地得到享有良好鲁棒性保证的分类器，改进了在特定范围内可证明认证方法。",
    "tldr": "本研究论证了数据分布的集中程度对于决定鲁棒分类器的存在与否至关重要，并展示了在数据分布集中在低维线性子空间的并集时，利用数据结构可以获得具有良好鲁棒性保证的分类器的方法。",
    "en_tdlr": "This study demonstrates the crucial role of data concentration in determining the existence of robust classifiers, and shows that exploiting data structure in low-dimensional linear subspaces can lead to classifiers with good robustness guarantees."
}