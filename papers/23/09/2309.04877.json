{
    "title": "A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])",
    "abstract": "The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.",
    "link": "http://arxiv.org/abs/2309.04877",
    "context": "Title: A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])\nAbstract: The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.",
    "path": "papers/23/09/2309.04877.json",
    "total_tokens": 899,
    "translated_title": "渐变优化和变分不等式在机器学习中的温和介绍",
    "translated_abstract": "近年来机器学习的快速发展基于与渐变优化的紧密联系。进一步的进展部分取决于从模式识别到决策和多智能体问题的转变。在这些更广泛的背景下，涉及均衡和博弈论而不是极值的新的数学挑战出现了。基于梯度的方法仍然至关重要--考虑到机器学习问题的高维度和大规模--但简单的梯度下降不再是算法设计的出发点。我们提供了一个对机器学习中基于梯度的算法的更广泛框架的温和介绍，从鞍点和单调博弈开始，然后到一般的变分不等式。虽然我们对所提出的几个算法进行了收敛性证明，但我们的主要关注点是提供动机和直观理解。",
    "tldr": "这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。",
    "en_tdlr": "This paper provides a gentle introduction to the application of gradient-based optimization and variational inequalities in machine learning, emphasizing the shift from pattern recognition to decision-making and multi-agent problems, and the mathematical challenges involving equilibria and game theory. It provides convergence proofs for several algorithms but focuses mainly on providing motivation and intuition."
}