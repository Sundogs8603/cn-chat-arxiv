{
    "title": "Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])",
    "abstract": "We introduce a machine-learning framework to warm-start fixed-point optimization algorithms. Our architecture consists of a neural network mapping problem parameters to warm starts, followed by a predefined number of fixed-point iterations. We propose two loss functions designed to either minimize the fixed-point residual or the distance to a ground truth solution. In this way, the neural network predicts warm starts with the end-to-end goal of minimizing the downstream loss. An important feature of our architecture is its flexibility, in that it can predict a warm start for fixed-point algorithms run for any number of steps, without being limited to the number of steps it has been trained on. We provide PAC-Bayes generalization bounds on unseen data for common classes of fixed-point operators: contractive, linearly convergent, and averaged. Applying this framework to well-known applications in control, statistics, and signal processing, we observe a significant reduction in the number",
    "link": "http://arxiv.org/abs/2309.07835",
    "context": "Title: Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])\nAbstract: We introduce a machine-learning framework to warm-start fixed-point optimization algorithms. Our architecture consists of a neural network mapping problem parameters to warm starts, followed by a predefined number of fixed-point iterations. We propose two loss functions designed to either minimize the fixed-point residual or the distance to a ground truth solution. In this way, the neural network predicts warm starts with the end-to-end goal of minimizing the downstream loss. An important feature of our architecture is its flexibility, in that it can predict a warm start for fixed-point algorithms run for any number of steps, without being limited to the number of steps it has been trained on. We provide PAC-Bayes generalization bounds on unseen data for common classes of fixed-point operators: contractive, linearly convergent, and averaged. Applying this framework to well-known applications in control, statistics, and signal processing, we observe a significant reduction in the number",
    "path": "papers/23/09/2309.07835.json",
    "total_tokens": 907,
    "translated_title": "学习如何为固定点优化算法预热",
    "translated_abstract": "我们引入了一个机器学习框架来为固定点优化算法进行预热。我们的架构由一个神经网络组成，将问题参数映射到预热起始点，然后进行一定数量的固定点迭代。我们提出了两个损失函数，旨在最小化固定点残差或与基准解的距离。通过这种方式，神经网络预测了预热起始点，最终目标是最小化下游损失。我们架构的一个重要特点是其灵活性，可以为运行任意步数的固定点算法预测预热起始点，而不仅限于它训练的步数。我们为常见的固定点算子类（收缩、线性收敛和平均值）提供了PAC-Bayes泛化界限。将此框架应用于控制、统计和信号处理的众所周知的应用中，我们观察到了数量显著减少。",
    "tldr": "该论文提出了一个机器学习框架，用于为固定点优化算法提供预热。该框架使用神经网络预测问题参数的预热起始点，并通过固定点迭代来优化结果。实验结果表明，该方法可以显著减少迭代次数。",
    "en_tdlr": "This paper introduces a machine learning framework for warm-starting fixed-point optimization algorithms. The framework utilizes a neural network to predict warm starts based on problem parameters and optimizes the results through fixed-point iterations. Experimental results demonstrate a significant reduction in the number of iterations required."
}