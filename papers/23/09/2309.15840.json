{
    "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])",
    "abstract": "Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa",
    "link": "http://arxiv.org/abs/2309.15840",
    "context": "Title: How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])\nAbstract: Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa",
    "path": "papers/23/09/2309.15840.json",
    "total_tokens": 1050,
    "translated_title": "如何捕捉AI谎言：通过问无关问题在黑盒LLMs中进行谎言检测",
    "translated_abstract": "大型语言模型（LLMs）会“说谎”，也就是在明知道真相的情况下输出虚假陈述。当指示输出错误信息时，LLMs可能会“说谎”。在这里，我们开发了一个简单的谎言检测器，既不需要访问LLM的激活（黑盒），也不需要事实问题的真相知识。这个检测器通过在怀疑有谎言的情况下问一组预定义的无关后续问题，并将LLM的是/否答案输入到逻辑回归分类器中来工作。尽管简单，这个谎言检测器非常准确并且令人惊讶地通用。当在单一情境的示例上进行训练 - 促使GPT-3.5在事实问题上撒谎 - 该检测器可以推广到以下情况：（1）其他LLM架构，（2）细调为说谎的LLMs，（3）谄媚的谎言，和（4）出现在实际场景中的谎言，比如销售。这些结果表明，LLMs具有特殊的与谎言相关的行为模式。",
    "tldr": "本文提出了一个简单但高精确度的谎言检测器，通过在怀疑有谎言的情况下问一组无关的后续问题，并将LLM的是/否答案输入到一个逻辑回归分类器中，该检测器能够推广到不同的LLM架构和实际场景中的谎言情况。"
}