{
    "title": "Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those sele",
    "link": "http://arxiv.org/abs/2309.12294",
    "context": "Title: Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those sele",
    "path": "papers/23/09/2309.12294.json",
    "total_tokens": 908,
    "translated_title": "大型语言模型在从逻辑形式中生成自然语言方面的重新排序研究",
    "translated_abstract": "大型语言模型（LLMs）在自然语言生成方面展现出了引人注目的能力。然而，它们的输出质量不一致，给从逻辑形式（LFs）生成自然语言带来了挑战。这个任务要求生成的输出体现LFs的确切语义，不遗漏任何LF语义或产生任何幻象。在这项工作中，我们通过提出一种新颖的生成和重新排序方法来解决这个问题。我们的方法涉及通过提示LLM初始化生成一组候选输出，然后使用特定任务的重新排序模型对它们进行重新排序。此外，我们创建了一个手动收集的数据集，用于评估不同排序指标与人类判断之间的一致性。所选择的排序指标被用于增强重新排序模型的训练和评估。通过对三个不同数据集进行广泛的实验，我们证明了我们的重新排序模型选择的候选项优于那些选项.",
    "tldr": "本研究提出了一种新颖的生成和重新排序方法，用于解决大型语言模型在从逻辑形式中生成自然语言方面的质量不一致问题。通过对候选输出进行重新排序，我们的方法能够更好地体现逻辑形式的语义，并提升生成结果的质量。",
    "en_tdlr": "This study proposes a novel generate-and-rerank approach to address the issue of inconsistent output quality in natural language generation from logical forms using large language models. By reranking the candidate outputs, our approach better captures the semantic of logical forms and improves the quality of generated results."
}