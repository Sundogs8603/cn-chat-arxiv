{
    "title": "Uncertainty-Aware Multi-View Visual Semantic Embedding. (arXiv:2309.08154v1 [cs.CV])",
    "abstract": "The key challenge in image-text retrieval is effectively leveraging semantic information to measure the similarity between vision and language data. However, using instance-level binary labels, where each image is paired with a single text, fails to capture multiple correspondences between different semantic units, leading to uncertainty in multi-modal semantic understanding. Although recent research has captured fine-grained information through more complex model structures or pre-training techniques, few studies have directly modeled uncertainty of correspondence to fully exploit binary labels. To address this issue, we propose an Uncertainty-Aware Multi-View Visual Semantic Embedding (UAMVSE)} framework that decomposes the overall image-text matching into multiple view-text matchings. Our framework introduce an uncertainty-aware loss function (UALoss) to compute the weighting of each view-text loss by adaptively modeling the uncertainty in each view-text correspondence. Different we",
    "link": "http://arxiv.org/abs/2309.08154",
    "context": "Title: Uncertainty-Aware Multi-View Visual Semantic Embedding. (arXiv:2309.08154v1 [cs.CV])\nAbstract: The key challenge in image-text retrieval is effectively leveraging semantic information to measure the similarity between vision and language data. However, using instance-level binary labels, where each image is paired with a single text, fails to capture multiple correspondences between different semantic units, leading to uncertainty in multi-modal semantic understanding. Although recent research has captured fine-grained information through more complex model structures or pre-training techniques, few studies have directly modeled uncertainty of correspondence to fully exploit binary labels. To address this issue, we propose an Uncertainty-Aware Multi-View Visual Semantic Embedding (UAMVSE)} framework that decomposes the overall image-text matching into multiple view-text matchings. Our framework introduce an uncertainty-aware loss function (UALoss) to compute the weighting of each view-text loss by adaptively modeling the uncertainty in each view-text correspondence. Different we",
    "path": "papers/23/09/2309.08154.json",
    "total_tokens": 923,
    "translated_title": "不确定性感知的多视图视觉语义嵌入",
    "translated_abstract": "图像-文本检索的关键挑战是有效地利用语义信息来衡量视觉和语言数据之间的相似性。然而，使用实例级的二进制标签，其中每个图像与一个文本配对，无法捕捉不同语义单元之间的多个对应关系，从而导致多模态语义理解中的不确定性。尽管最近的研究通过更复杂的模型结构或预训练技术捕捉了细粒度信息，但很少有研究直接建模对应关系的不确定性以充分利用二进制标签。为了解决这个问题，我们提出了一种不确定性感知的多视图视觉语义嵌入（UAMVSE）框架，该框架将整体图像-文本匹配分解为多个视图-文本匹配。我们的框架引入了一种不确定性感知损失函数（UALoss），通过自适应地建模每个视图-文本对应关系的不确定性来计算每个视图-文本损失的权重。",
    "tldr": "这篇论文提出了一种不确定性感知的多视图视觉语义嵌入框架，在图像-文本检索中有效地利用语义信息进行相似性度量，通过引入不确定性感知损失函数，充分利用二进制标签的不确定性，并将整体匹配分解为多个视图-文本匹配。",
    "en_tdlr": "This paper proposes an uncertainty-aware multi-view visual semantic embedding framework that effectively leverages semantic information for image-text retrieval, and introduces an uncertainty-aware loss function to exploit the uncertainty of binary labels and decompose the overall matching into multiple view-text matchings."
}