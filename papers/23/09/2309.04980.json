{
    "title": "Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data. (arXiv:2309.04980v1 [math.OC])",
    "abstract": "This paper considers a type of incremental aggregated gradient (IAG) method for large-scale distributed optimization. The IAG method is well suited for the parameter server architecture as the latter can easily aggregate potentially staled gradients contributed by workers. Although the convergence of IAG in the case of deterministic gradient is well known, there are only a few results for the case of its stochastic variant based on streaming data. Considering strongly convex optimization, this paper shows that the streaming IAG method achieves linear speedup when the workers are updating frequently enough, even if the data sample distribution across workers are heterogeneous. We show that the expected squared distance to optimal solution decays at O((1+T)/(nt)), where $n$ is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Our analysis involves careful treatments of the conditional expectations with staled gradients and a recursive system wi",
    "link": "http://arxiv.org/abs/2309.04980",
    "context": "Title: Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data. (arXiv:2309.04980v1 [math.OC])\nAbstract: This paper considers a type of incremental aggregated gradient (IAG) method for large-scale distributed optimization. The IAG method is well suited for the parameter server architecture as the latter can easily aggregate potentially staled gradients contributed by workers. Although the convergence of IAG in the case of deterministic gradient is well known, there are only a few results for the case of its stochastic variant based on streaming data. Considering strongly convex optimization, this paper shows that the streaming IAG method achieves linear speedup when the workers are updating frequently enough, even if the data sample distribution across workers are heterogeneous. We show that the expected squared distance to optimal solution decays at O((1+T)/(nt)), where $n$ is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Our analysis involves careful treatments of the conditional expectations with staled gradients and a recursive system wi",
    "path": "papers/23/09/2309.04980.json",
    "total_tokens": 897,
    "translated_title": "增量汇聚梯度方法在流数据上的线性加速",
    "translated_abstract": "本文考虑了一种用于大规模分布式优化的增量汇聚梯度(IAG)方法。IAG方法非常适合参数服务器架构，因为参数服务器可以轻松地汇聚工人贡献的可能过期的梯度。尽管IAG在确定性梯度情况下的收敛性已经有了很好的研究，但对于基于流数据的随机变体的研究结果还很有限。在考虑强凸优化的情况下，本文展示了当工人频繁更新时，流IAG方法可以实现线性加速，即使工人之间的数据样本分布不均匀。我们的分析涉及对带有过期梯度的条件期望的仔细处理和递归系统的处理。",
    "tldr": "本文研究了增量汇聚梯度方法在流数据上的线性加速。研究表明，在确定性梯度情况下，流IAG方法可以实现线性加速，并且即使数据样本分布不均匀，只要工人频繁更新，期望的最优解平方距离可以以O((1+T)/(nt))的速度衰减。",
    "en_tdlr": "This paper discusses the linear speedup of the incremental aggregated gradient (IAG) method on streaming data. The study shows that the streaming IAG method achieves linear speedup even if the data sample distribution is heterogeneous, as long as the workers update frequently enough. The expected squared distance to the optimal solution decays at a rate of O((1+T)/(nt))."
}