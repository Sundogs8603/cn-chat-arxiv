{
    "title": "AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models. (arXiv:2309.06495v1 [cs.CL])",
    "abstract": "Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, pe",
    "link": "http://arxiv.org/abs/2309.06495",
    "context": "Title: AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models. (arXiv:2309.06495v1 [cs.CL])\nAbstract: Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, pe",
    "path": "papers/23/09/2309.06495.json",
    "total_tokens": 880,
    "translated_title": "AGIBench: 用于大型语言模型的多粒度、多模态、人工参考、自动评分基准",
    "translated_abstract": "大型语言模型（LLM）如ChatGPT展示了惊人的智能。如何评估LLM的问题解决能力和智能程度是一个热点但具有挑战性的问题。首先，问题解决能力与不同的能力分支（如理解）和大规模的知识类别（如数学）交织在一起。第二，问题的输入是多模态的，可能涉及文本和图像。第三，LLM的响应格式多样，因此对结果提取和评估提出了巨大挑战。在本文中，我们提出了AGIBench--一种用于LLM的多粒度、多模态、人工参考和自动评分的基准方法。与混合问题集合不同，AGIBench专注于三个典型的能力分支，并采用四元组<能力分支、知识、难度、模态>来标记每个问题的属性。首先，它支持多粒度的基准化，例如每个问题、每个能力分支、每个知识类别的基准化。",
    "tldr": "AGIBench是一个用于大型语言模型的多粒度、多模态、人工参考、自动评分的基准，通过标记问题的属性来评估语言模型的问题解决能力和智能程度。"
}