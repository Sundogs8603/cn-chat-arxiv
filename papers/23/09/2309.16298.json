{
    "title": "At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreove",
    "link": "http://arxiv.org/abs/2309.16298",
    "context": "Title: At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreove",
    "path": "papers/23/09/2309.16298.json",
    "total_tokens": 951,
    "translated_title": "在哪个训练阶段，代码数据会帮助语言模型进行推理？",
    "translated_abstract": "大型语言模型（LLMs）展现了出色的推理能力，成为语言技术的基础。受到代码数据在训练LLMs中的巨大成功的启发，我们自然而然地想知道在哪个训练阶段引入代码数据真正可以帮助LLMs进行推理。为此，本文系统地探索了代码数据对LLMs在不同阶段的影响。具体而言，我们分别在预训练阶段、指令调整阶段以及两者之间引入代码数据。然后，通过五个领域中的六个推理任务全面公正地评估了LLMs的推理能力。我们对实验结果进行了关键分析，并提供了具有深度洞察力的结论。首先，使用代码和文本混合预训练LLMs可以显著增强LLMs的通用推理能力，几乎不对其他任务产生负面影响。此外，在指令调整阶段，代码数据赋予LLMs特定任务的推理能力。",
    "tldr": "本研究探索了代码数据对大型语言模型（LLMs）在不同阶段的影响。实验结果表明，在预训练阶段使用代码和文本混合可以显著增强LLMs的通用推理能力，而在指令调整阶段，代码数据赋予LLMs特定任务的推理能力。"
}