{
    "title": "Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.",
    "link": "http://arxiv.org/abs/2309.17007",
    "context": "Title: Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])\nAbstract: Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.",
    "path": "papers/23/09/2309.17007.json",
    "total_tokens": 972,
    "translated_title": "医学基础模型易受有针对性的错误信息攻击",
    "translated_abstract": "大型语言模型(LLM)在医学领域拥有广泛的知识，并能够跨越多个领域进行医学信息的推理，对未来的医学应用具有很大的潜力。在这项研究中，我们展示了LLM在医学领域存在的一个令人担忧的脆弱性。通过有针对性地操纵模型权重的1.1％，我们可以故意注入一个错误的生物医学事实。错误信息会在模型的输出中传播，同时对其他生物医学任务的性能没有影响。我们在一组1,038个错误的生物医学事实中验证了我们的发现。这种特殊的易受攻击性引发了人们对在医疗环境中应用LLM的安全性和可信度的严重关注。这凸显了确保这些模型在医学实践中可靠和安全使用的需求，需要采取强有力的保护措施，进行彻底的验证机制，并对这些模型的访问进行严格管理。",
    "tldr": "本研究揭示了医学领域中大型语言模型(LLM)的脆弱性，通过操纵模型权重的很小比例，可以故意注入错误的生物医学事实，并且这些错误信息会被模型输出传播。面对这种易受攻击性，我们需要采取措施确保这些模型在医疗实践中的可靠和安全使用。",
    "en_tdlr": "This study reveals the vulnerability of large language models (LLMs) in the medical field. By manipulating just a small portion of the model's weights, incorrect biomedical facts can be deliberately injected and propagated in the model's output. This susceptibility raises concerns about the reliability and safety of using LLMs in healthcare settings."
}