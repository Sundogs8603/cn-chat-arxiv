{
    "title": "Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need. (arXiv:2309.15737v1 [cs.LG])",
    "abstract": "We present a new algorithm based on posterior sampling for learning in constrained Markov decision processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of \\tilde{O} (HS \\sqrt{AT}) for any communicating CMDP with S states, A actions, and bound on the hitting time H. This regret bound matches the lower bound in order of time horizon T and is the best-known regret bound for communicating CMDPs in the infinite-horizon undiscounted setting. Empirical results show that, despite its simplicity, our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.",
    "link": "http://arxiv.org/abs/2309.15737",
    "context": "Title: Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need. (arXiv:2309.15737v1 [cs.LG])\nAbstract: We present a new algorithm based on posterior sampling for learning in constrained Markov decision processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of \\tilde{O} (HS \\sqrt{AT}) for any communicating CMDP with S states, A actions, and bound on the hitting time H. This regret bound matches the lower bound in order of time horizon T and is the best-known regret bound for communicating CMDPs in the infinite-horizon undiscounted setting. Empirical results show that, despite its simplicity, our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.",
    "path": "papers/23/09/2309.15737.json",
    "total_tokens": 845,
    "translated_title": "在约束强化学习中可以证明的高效探索：后验抽样就足够了",
    "translated_abstract": "我们提出了一种基于后验抽样的新算法，用于无限期无折扣约束马尔可夫决策过程（CMDP）中的学习。该算法在实证上相对于现有算法具有优势，同时实现了近似最优的遗憾界限。我们的主要理论结果是对于任意通信CMDP，每个成本部分的贝叶斯遗憾界限为\\tilde{O} (HS \\sqrt{AT})，其中CMDP具有S个状态、A个行动和命中时间H的边界。这个遗憾界限与时间跨度T的下界匹配，并且是无限期无折扣设置中通信CMDP的已知最佳遗憾界限。实验结果表明，尽管我们的后验抽样算法非常简单，但在约束强化学习中胜过现有算法。",
    "tldr": "该论文提出了一种基于后验抽样的算法，用于约束强化学习中的无限期无折扣马尔可夫决策过程。该算法在遗憾界限上接近最优，并在实证上表现出优势。",
    "en_tdlr": "This paper introduces an algorithm based on posterior sampling for constrained reinforcement learning in the infinite-horizon undiscounted Markov decision processes. The algorithm achieves near-optimal regret bounds and outperforms existing algorithms in empirical results."
}