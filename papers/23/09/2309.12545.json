{
    "title": "Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])",
    "abstract": "Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi",
    "link": "http://arxiv.org/abs/2309.12545",
    "context": "Title: Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])\nAbstract: Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi",
    "path": "papers/23/09/2309.12545.json",
    "total_tokens": 901,
    "translated_title": "通过鲁棒优化方法为神经网络提供可证明的鲁棒和可信的反事实解释",
    "translated_abstract": "反事实解释(CEs)作为解释神经网络分类器的主要方法已经引起了越来越多的关注。通常，CEs对于输入-输出对被定义为到输入的最小距离的数据点，其与输出具有不同的标签。为了解决CEs在模型参数更新(比如重新训练)时很容易被无效的问题，研究提出了一种通过模型参数变化的范数球界限来证明CEs的鲁棒性的方法。然而，现有的针对这种鲁棒性的方法不是完全正确的，或者可能生成不合理的CEs，即与训练数据集存在离群值。事实上，目前没有一种方法能够同时优化距离和可信度，并保持鲁棒性保证。在这项工作中，我们提出了一种名为PROPLACE的方法，利用鲁棒优化技术来解决上述问题。",
    "tldr": "本文提出了一种名为PROPLACE的方法，通过鲁棒优化技术为神经网络提供可证明的鲁棒和可信的反事实解释，解决了现有方法在保持鲁棒性的同时生成不合理解释的问题。",
    "en_tdlr": "This paper proposes a method called PROPLACE, which uses robust optimization techniques to provide provably robust and plausible counterfactual explanations for neural networks, addressing the problem of existing methods generating implausible explanations while preserving robustness."
}