{
    "title": "Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count. (arXiv:2309.12996v1 [cs.LG])",
    "abstract": "This paper introduces the Point Cloud Network (PCN) architecture, a novel implementation of linear layers in deep learning networks, and provides empirical evidence to advocate for its preference over the Multilayer Perceptron (MLP) in linear layers. We train several models, including the original AlexNet, using both MLP and PCN architectures for direct comparison of linear layers (Krizhevsky et al., 2012). The key results collected are model parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet, achieves comparable efficacy (test accuracy) to the original architecture with a 99.5% reduction of parameters in its linear layers. All training is done on cloud RTX 4090 GPUs, leveraging pytorch for model construction and training. Code is provided for anyone to reproduce the trials from this paper.",
    "link": "http://arxiv.org/abs/2309.12996",
    "context": "Title: Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count. (arXiv:2309.12996v1 [cs.LG])\nAbstract: This paper introduces the Point Cloud Network (PCN) architecture, a novel implementation of linear layers in deep learning networks, and provides empirical evidence to advocate for its preference over the Multilayer Perceptron (MLP) in linear layers. We train several models, including the original AlexNet, using both MLP and PCN architectures for direct comparison of linear layers (Krizhevsky et al., 2012). The key results collected are model parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet, achieves comparable efficacy (test accuracy) to the original architecture with a 99.5% reduction of parameters in its linear layers. All training is done on cloud RTX 4090 GPUs, leveraging pytorch for model construction and training. Code is provided for anyone to reproduce the trials from this paper.",
    "path": "papers/23/09/2309.12996.json",
    "total_tokens": 830,
    "translated_title": "点云网络：线性层参数数量的数量级改进",
    "translated_abstract": "本文介绍了点云网络（PCN）架构，这是一种深度学习网络中线性层的新型实现，并提供了实证证据来支持其在线性层中优于多层感知机（MLP）的偏好。我们使用MLP和PCN架构训练了几个模型，包括原始的AlexNet，以直接比较线性层。我们收集的关键结果是模型参数数量和在CIFAR-10和CIFAR-100数据集上的top-1测试准确性。我们的PCN等效于AlexNet的AlexNet-PCN16，在其线性层中参数减少99.5%的情况下，实现了与原始架构相当的效果（测试准确性）。所有训练都是在云RTX 4090 GPU上进行的，利用pytorch进行模型构建和训练。提供了代码供任何人复制本文中的实验。",
    "tldr": "本文提出了点云网络（PCN）架构，通过对线性层的改进，在保持与原始架构相当的测试准确性的同时，大幅度减少了参数数量。",
    "en_tdlr": "This paper introduces the Point Cloud Network (PCN) architecture, which significantly reduces the parameter count while maintaining comparable test accuracy to the original architecture."
}