{
    "title": "Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)",
    "abstract": "We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo",
    "link": "http://arxiv.org/abs/2309.05086",
    "context": "Title: Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)\nAbstract: We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo",
    "path": "papers/23/09/2309.05086.json",
    "total_tokens": 892,
    "translated_title": "Neural-Hidden-CRF: 一种鲁棒的弱监督序列标注模型",
    "translated_abstract": "我们提出了一种基于神经网络的无向图模型 Neural-Hidden-CRF 来解决弱监督序列标注问题。在概率无向图理论的框架下，Neural-Hidden-CRF融合了一个隐藏CRF层，用于建模单词序列、潜在真实序列和弱标签序列之间的变量关系并具有全局视角。在Neural-Hidden-CRF中，我们可以利用强大的语言模型BERT或其他深度模型为潜在真实序列提供丰富的上下文语义知识，并使用隐藏的CRF层来捕捉内部标签之间的依赖关系。Neural-Hidden-CRF在概念上简单而在实践中强大。它在一个众包基准测试和三个弱监督基准测试上取得了最新的最优结果，包括在平均泛化和推理性能方面超过了最新的高级模型CHMM的2.80 F1分和2.23 F1分。",
    "tldr": "Neural-Hidden-CRF是一种鲁棒的弱监督序列标注模型，通过引入隐藏CRF层和利用强大的语言模型，它在各项基准测试中取得了最新的最优结果。",
    "en_tdlr": "Neural-Hidden-CRF is a robust weakly-supervised sequence labeler that incorporates a hidden CRF layer and utilizes powerful language models, achieving state-of-the-art results in various benchmarks."
}