{
    "title": "Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio. (arXiv:2309.15024v1 [cs.SD])",
    "abstract": "Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution ",
    "link": "http://arxiv.org/abs/2309.15024",
    "context": "Title: Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio. (arXiv:2309.15024v1 [cs.SD])\nAbstract: Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution ",
    "path": "papers/23/09/2309.15024.json",
    "total_tokens": 946,
    "translated_title": "Synthia的旋律：无监督领域自适应音频基准框架",
    "translated_abstract": "尽管在视觉和自然语言的深度学习方面取得了重大进展，但音频领域的无监督领域自适应仍然相对未开发。其中一个原因是缺乏适当的基准数据集。为了填补这一空白，我们提出了Synthia的旋律，一种新颖的音频数据生成框架，能够模拟具有用户指定混淆结构的无限多种4秒旋律，这些结构由音乐键、音色和音量特征化。与观测设置下收集的现有数据集不同，Synthia的旋律没有未观察到的偏差，确保了实验的可重复性和可比性。为了展示其实用性，我们生成了两种类型的分布转移-领域转移和样本选择偏差，并评估了这些转移下声学深度学习模型的性能。我们的评估结果表明，Synthia的旋律为检验这些模型对不同分布级别的敏感性提供了一个稳健的实验平台。",
    "tldr": "Synthia's melody提出了一个新颖的音频数据生成框架，为无监督领域自适应提供了一个适当的基准数据集，这个框架能够模拟各种旋律，并评估声学深度学习模型对分布转移的敏感性。",
    "en_tdlr": "Synthia's Melody presents a novel audio data generation framework that provides an appropriate benchmark dataset for unsupervised domain adaptation in audio. It generates various melodies and evaluates the sensitivity of acoustic deep learning models to distribution shifts."
}