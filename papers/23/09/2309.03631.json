{
    "title": "Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])",
    "abstract": "Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .",
    "link": "http://arxiv.org/abs/2309.03631",
    "context": "Title: Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])\nAbstract: Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .",
    "path": "papers/23/09/2309.03631.json",
    "total_tokens": 876,
    "translated_title": "对于蛋白功能预测中Transformer模型内部运作的洞察",
    "translated_abstract": "动机：我们探索了可解释性人工智能（XAI）如何帮助揭示神经网络用于蛋白质功能预测的内部运作，通过扩展广泛使用的XAI方法——集成梯度，使其能够检查调整为基因本体术语和酶委员会编号预测的Transformer模型内的潜在表示。结果：该方法使我们能够识别出变压器在序列中特别关注的氨基酸，并展示这些相关的序列部分反映了生物学和化学的预期，无论是在嵌入层还是模型内部。我们确定了变压器头与地面真实序列注释（例如，跨膜区域，活性位点）之间具有统计显著对应的归因图的变压器头，这在多个蛋白质中都有出现。代码可在https://github.com/markuswenzel/xai-proteins 上获取和实施。",
    "tldr": "本研究通过扩展可解释性人工智能方法，探索了Transformer模型在蛋白质功能预测中的内部运作，并成功识别出了与生物学和化学相关的序列部分，为蛋白质研究提供了重要线索。",
    "en_tdlr": "This study explores the internal workings of Transformer models for protein function prediction and successfully identifies relevant sequence parts related to biology and chemistry, providing important clues for protein research."
}