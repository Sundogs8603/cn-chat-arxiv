{
    "title": "AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])",
    "abstract": "The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.",
    "link": "http://arxiv.org/abs/2309.11104",
    "context": "Title: AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])\nAbstract: The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.",
    "path": "papers/23/09/2309.11104.json",
    "total_tokens": 849,
    "translated_title": "AttentionMix: 一种基于BERT注意机制的数据增强方法",
    "translated_abstract": "Mixup方法在计算机视觉领域已被证明是一种强大的数据增强技术，并且有许多以引导方式执行图像混合的后继方法。将Mixup思想应用于其他领域，如自然语言处理（NLP），是一个有趣的研究方向。尽管已经存在一些将Mixup应用于文本数据的方法，但仍有改进的空间。在本文中，我们介绍了AttentionMix，一种基于注意力的混合方法。虽然本文关注BERT注意机制，但所提出的方法可以应用于任何基于注意力的模型。AttentionMix在3个标准情感分类数据集上进行了评估，并在所有三种情况下表现优于利用Mixup机制和普通BERT方法的两种基准方法。结果证实，注意力的信息可以有效地用于NLP领域的数据增强。",
    "tldr": "AttentionMix是一种新的数据增强方法，利用了基于注意力的信息。在三个标准情感分类数据集上的评估结果表明，AttentionMix在NLP领域的数据增强中表现优于使用Mixup机制的两种基准方法和普通BERT方法。",
    "en_tdlr": "AttentionMix is a novel data augmentation method that utilizes attention-based information. Evaluation on three standard sentiment classification datasets shows that AttentionMix outperforms two benchmark approaches using Mixup mechanism and the vanilla BERT method in NLP domain."
}