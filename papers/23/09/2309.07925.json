{
    "title": "Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023. (arXiv:2309.07925v1 [eess.AS])",
    "abstract": "In this paper, we propose a novel framework for recognizing both discrete and dimensional emotions. In our framework, deep features extracted from foundation models are used as robust acoustic and visual representations of raw video. Three different structures based on attention-guided feature gathering (AFG) are designed for deep feature fusion. Then, we introduce a joint decoding structure for emotion classification and valence regression in the decoding stage. A multi-task loss based on uncertainty is also designed to optimize the whole process. Finally, by combining three different structures on the posterior probability level, we obtain the final predictions of discrete and dimensional emotions. When tested on the dataset of multimodal emotion recognition challenge (MER 2023), the proposed framework yields consistent improvements in both emotion classification and valence regression. Our final system achieves state-of-the-art performance and ranks third on the leaderboard on MER-M",
    "link": "http://arxiv.org/abs/2309.07925",
    "context": "Title: Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023. (arXiv:2309.07925v1 [eess.AS])\nAbstract: In this paper, we propose a novel framework for recognizing both discrete and dimensional emotions. In our framework, deep features extracted from foundation models are used as robust acoustic and visual representations of raw video. Three different structures based on attention-guided feature gathering (AFG) are designed for deep feature fusion. Then, we introduce a joint decoding structure for emotion classification and valence regression in the decoding stage. A multi-task loss based on uncertainty is also designed to optimize the whole process. Finally, by combining three different structures on the posterior probability level, we obtain the final predictions of discrete and dimensional emotions. When tested on the dataset of multimodal emotion recognition challenge (MER 2023), the proposed framework yields consistent improvements in both emotion classification and valence regression. Our final system achieves state-of-the-art performance and ranks third on the leaderboard on MER-M",
    "path": "papers/23/09/2309.07925.json",
    "total_tokens": 869,
    "translated_title": "MER 2023中基于多标签联合解码的分层音频-视觉信息融合",
    "translated_abstract": "本文提出了一种用于识别离散和维度情绪的创新框架。在我们的框架中，从基础模型中提取的深层特征被用作原始视频的稳健声音和视觉表示。基于注意力引导的特征聚集的三种不同结构被设计用于深层特征融合。然后，在解码阶段，我们引入了一种联合解码结构，用于情绪分类和价值回归。还设计了基于不确定性的多任务损失来优化整个过程。最后，通过在后验概率水平上结合三种不同结构，我们获得了离散和维度情绪的最终预测结果。在多模态情感识别挑战（MER 2023）的数据集上进行测试时，所提出的框架在情绪分类和价值回归方面取得了一致的改进。我们的最终系统达到了最先进的性能并在MER-M排行榜上排名第三。",
    "tldr": "本文提出了一个新颖的框架，用于识别离散和维度情绪，并在MER 2023数据集上取得了最先进的性能和第三的排名。",
    "en_tdlr": "This paper proposes a novel framework for recognizing both discrete and dimensional emotions and achieves state-of-the-art performance and ranks third on the leaderboard on MER-M challenge in MER 2023."
}