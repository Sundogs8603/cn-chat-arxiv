{
    "title": "Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])",
    "abstract": "Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the",
    "link": "http://arxiv.org/abs/2309.04354",
    "context": "Title: Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])\nAbstract: Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the",
    "path": "papers/23/09/2309.04354.json",
    "total_tokens": 866,
    "translated_title": "移动V-MoEs：通过稀疏MoEs缩小视觉变换器的规模",
    "translated_abstract": "最近，稀疏的专家混合模型（MoEs）因其能够通过仅激活给定输入令牌的模型参数的一小部分而将模型规模与推理效率分离而受到关注。因此，稀疏的MoEs在自然语言处理和计算机视觉等领域取得了巨大的成功。在这项工作中，我们研究了使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以使其在资源受限的视觉应用中更具吸引力。为此，我们提出了一种简化和适用于移动设备的MoE设计，其中整个图像而不是单个补丁被路由到专家。我们还提出了一种稳定的MoE训练过程，使用超类信息来引导路由器。实验证明，我们的稀疏移动视觉MoEs（V-MoEs）可以在性能和效率之间达到更好的折衷。例如，对于...",
    "tldr": "本论文研究通过使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以提高在资源受限的视觉应用中的性能和效率。",
    "en_tdlr": "This paper explores the use of sparse MoEs to scale-down Vision Transformers (ViTs) for resource-constrained vision applications, achieving better performance and efficiency."
}