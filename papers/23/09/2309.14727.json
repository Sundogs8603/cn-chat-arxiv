{
    "title": "Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization. (arXiv:2309.14727v1 [eess.SY])",
    "abstract": "In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.",
    "link": "http://arxiv.org/abs/2309.14727",
    "context": "Title: Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization. (arXiv:2309.14727v1 [eess.SY])\nAbstract: In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.",
    "path": "papers/23/09/2309.14727.json",
    "total_tokens": 879,
    "translated_title": "高效的多智能体深度强化学习控制与相对熵正则化",
    "translated_abstract": "本文提出了一种新颖的多智能体强化学习（MARL）方法，即多智能体连续动态策略梯度（MACDPP），用于解决多个智能体控制的各种场景中存在的能力有限和样本效率问题。通过引入相对熵正则化到中心化训练与分散执行（CTDE）框架中的Actor-Critic（AC）结构，它缓解了多个智能体策略更新的不一致性。通过对多智能体合作和竞争任务以及包括OpenAI基准和机械臂操作在内的传统控制任务的评估，MACDPP在学习能力和样本效率方面相较于相关的多智能体和广泛使用的单智能体基线表现出显著优势，因此扩展了MARL在有效学习具有挑战性的控制场景中的潜力。",
    "tldr": "本文提出了一种新的多智能体强化学习方法，通过相对熵正则化解决了多个智能体策略更新的不一致性问题，并证明在多智能体合作和竞争任务以及传统控制任务中表现出显著的学习能力和样本效率。",
    "en_tdlr": "This paper proposes a novel multi-agent reinforcement learning method that addresses the inconsistency of policy updates among multiple agents by introducing relative entropy regularization. It demonstrates significant learning capability and sample efficiency in multi-agent cooperation and competition tasks, as well as traditional control tasks."
}