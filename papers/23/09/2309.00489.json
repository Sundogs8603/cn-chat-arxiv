{
    "title": "How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?. (arXiv:2309.00489v1 [cs.NI])",
    "abstract": "The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence",
    "link": "http://arxiv.org/abs/2309.00489",
    "context": "Title: How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?. (arXiv:2309.00489v1 [cs.NI])\nAbstract: The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence",
    "path": "papers/23/09/2309.00489.json",
    "total_tokens": 826,
    "translated_title": "预测对DRL技术在O-RAN切片中的收敛性有何影响？",
    "translated_abstract": "沉浸式应用（如虚拟现实游戏和元宇宙服务）的成功取决于低延迟和可靠的连接。为了提供无缝的用户体验，开放式无线接入网络（O-RAN）架构和6G网络被期望发挥关键作用。O-RAN范式的关键组成部分之一，RAN切片可以根据沉浸式服务的需求分配网络资源，在单个物理基础设施上创建多个虚拟网络。在O-RAN文献中，深度强化学习（DRL）算法通常用于优化资源分配。然而，DRL在实际部署中的采用速度较慢。主要原因是DRL代理在初始部署时和网络条件发生显著变化时的收敛缓慢和性能不稳定。本文研究了时间序列预测对交通需求的收敛影响。",
    "tldr": "本文研究了时间序列预测对在O-RAN切片中使用的DRL技术的收敛性的影响。",
    "en_tdlr": "This paper investigates the impact of time series forecasting on the convergence of DRL techniques used in O-RAN slicing."
}