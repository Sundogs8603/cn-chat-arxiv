{
    "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])",
    "abstract": "There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and ",
    "link": "http://arxiv.org/abs/2309.17234",
    "context": "Title: LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])\nAbstract: There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and ",
    "path": "papers/23/09/2309.17234.json",
    "total_tokens": 964,
    "translated_title": "LLM-辩论: 使用交互式多智能体协商游戏评估LLMs",
    "translated_abstract": "越来越多的人对使用大型语言模型（LLMs）作为代理人来解决可能需要评估复杂情况的现实任务感兴趣。然而，我们对LLMs的推理和决策能力有限的理解，在某种程度上是由于缺乏专门的评估基准。由于谈判和妥协是我们日常沟通和合作的关键方面，我们提出使用可评分的谈判游戏作为LLMs的新评估框架。我们创建了一个多样的基于文本的、多智能体的、多问题的、语义丰富的谈判游戏测试平台，难度可调。为了解决这一挑战，代理人需要具备强大的算术、推理、探索和规划能力，同时无缝地整合它们。通过系统化的零-shot思维链提示（CoT），我们展示了代理人可以进行谈判并持续达成成功交易。我们用多个指标量化性能，并观察到GPT-4与原文之间存在很大差距。",
    "tldr": "本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。",
    "en_tdlr": "This paper proposes using scorable negotiation games as a new evaluation framework for Large Language Models (LLMs), creating a diverse testbed and demonstrating that agents can successfully negotiate through systematic zero-shot Chain-of-Thought prompting (CoT). The study reveals a performance gap between GPT-4 and the original text."
}