{
    "title": "Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])",
    "abstract": "Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\\eta$, after which it fluctuates around this value.  The quantity $2/\\eta$ has been called the \"edge of stability\" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an \"edge of stability\" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.",
    "link": "http://arxiv.org/abs/2309.12488",
    "context": "Title: Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])\nAbstract: Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\\eta$, after which it fluctuates around this value.  The quantity $2/\\eta$ has been called the \"edge of stability\" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an \"edge of stability\" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.",
    "path": "papers/23/09/2309.12488.json",
    "total_tokens": 817,
    "translated_title": "锐度感知最小化和稳定性边界。",
    "translated_abstract": "最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。",
    "tldr": "本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。",
    "en_tdlr": "This study determines an edge of stability for Sharpness-Aware Minimization (SAM), a variant of gradient descent that improves generalization, based on a similar calculation method as the one used for traditional gradient descent (GD). The SAM-edge is dependent on the norm of the gradient, unlike the GD case, and empirical results show that SAM operates on this stability edge."
}