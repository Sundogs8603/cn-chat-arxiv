{
    "title": "A Unified Scheme of ResNet and Softmax. (arXiv:2309.13482v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have brought significant changes to human society. Softmax regression and residual neural networks (ResNet) are two important techniques in deep learning: they not only serve as significant theoretical components supporting the functionality of LLMs but also are related to many other machine learning and theoretical computer science fields, including but not limited to image classification, object detection, semantic segmentation, and tensors.  Previous research works studied these two concepts separately. In this paper, we provide a theoretical analysis of the regression problem: $\\| \\langle \\exp(Ax) + A x , {\\bf 1}_n \\rangle^{-1} ( \\exp(Ax) + Ax ) - b \\|_2^2$, where $A$ is a matrix in $\\mathbb{R}^{n \\times d}$, $b$ is a vector in $\\mathbb{R}^n$, and ${\\bf 1}_n$ is the $n$-dimensional vector whose entries are all $1$. This regression problem is a unified scheme that combines softmax regression and ResNet, which has never been done before. We derive the gra",
    "link": "http://arxiv.org/abs/2309.13482",
    "context": "Title: A Unified Scheme of ResNet and Softmax. (arXiv:2309.13482v1 [cs.LG])\nAbstract: Large language models (LLMs) have brought significant changes to human society. Softmax regression and residual neural networks (ResNet) are two important techniques in deep learning: they not only serve as significant theoretical components supporting the functionality of LLMs but also are related to many other machine learning and theoretical computer science fields, including but not limited to image classification, object detection, semantic segmentation, and tensors.  Previous research works studied these two concepts separately. In this paper, we provide a theoretical analysis of the regression problem: $\\| \\langle \\exp(Ax) + A x , {\\bf 1}_n \\rangle^{-1} ( \\exp(Ax) + Ax ) - b \\|_2^2$, where $A$ is a matrix in $\\mathbb{R}^{n \\times d}$, $b$ is a vector in $\\mathbb{R}^n$, and ${\\bf 1}_n$ is the $n$-dimensional vector whose entries are all $1$. This regression problem is a unified scheme that combines softmax regression and ResNet, which has never been done before. We derive the gra",
    "path": "papers/23/09/2309.13482.json",
    "total_tokens": 899,
    "translated_title": "ResNet和Softmax的统一方案",
    "translated_abstract": "大型语言模型（LLM）对人类社会带来了重大变革。Softmax回归和残差神经网络（ResNet）是深度学习中两个重要的技术：它们不仅作为支持LLM功能的重要理论组成部分，而且与许多其他机器学习和理论计算机科学领域相关，包括但不限于图像分类，目标检测，语义分割和张量。以往的研究对这两个概念进行了分别研究。本文提出了一个回归问题的理论分析：$\\| \\langle \\exp(Ax) + A x , {\\bf 1}_n \\rangle^{-1} ( \\exp(Ax) + Ax ) - b \\|_2^2$，其中$A$是一个$n \\times d$维实矩阵，$b$是一个$n$维实向量，${\\bf 1}_n$是所有元素都为1的$n$维向量。这个回归问题是将softmax回归和ResNet相结合的统一方案，这在以前从未做过。我们推导了梯度...",
    "tldr": "这是一篇关于将softmax回归和ResNet相结合的统一方案的论文，提供了对回归问题的理论分析，并推导了梯度...",
    "en_tdlr": "This paper presents a unified scheme that combines softmax regression and ResNet, providing theoretical analysis of the regression problem and deriving gradient..."
}