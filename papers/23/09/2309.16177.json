{
    "title": "Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])",
    "abstract": "Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi",
    "link": "http://arxiv.org/abs/2309.16177",
    "context": "Title: Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])\nAbstract: Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi",
    "path": "papers/23/09/2309.16177.json",
    "total_tokens": 863,
    "translated_title": "系统化采样和机器学习参数化在气候模型中的验证",
    "translated_abstract": "混合物理-机器学习气候模拟的进展受到获取高性能耦合（即在线）模拟的困难的限制。虽然在脱机环境中评估数百个机器学习参数化子网格闭合（如对流和辐射）是直接的，但在相同规模上的在线评估在技术上具有挑战性。我们的软件自动化实现了比以往任何时候都多一个数量级的在线建模错误采样。利用这一点，我们评估混合气候模型的性能，并制定改进策略。我们发现，在包含记忆、相对湿度输入特征转换和额外输入变量的情况下，模型的在线性能有所提高。我们还揭示了在线错误的显著差异以及脱机与在线错误统计之间的不一致性。这意味着需要在线评估数百个候选机器学习模型以检测参数化设计选择的影响。",
    "tldr": "本论文研究了混合物理-机器学习气候模拟的挑战，并通过大规模在线建模错误采样和评估，在机器学习参数化设计中发现了改进性能的策略。"
}