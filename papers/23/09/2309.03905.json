{
    "title": "ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])",
    "abstract": "We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by ",
    "link": "http://arxiv.org/abs/2309.03905",
    "context": "Title: ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])\nAbstract: We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by ",
    "path": "papers/23/09/2309.03905.json",
    "total_tokens": 885,
    "translated_title": "ImageBind-LLM: 多模态指令调优的方法",
    "translated_abstract": "我们提出了一种通过ImageBind对大型语言模型（LLMs）进行多模态指令调优的方法。现有的工作主要集中在语言和图像指令调优方面，与此不同，我们的ImageBind-LLM可以响应多模态条件，包括音频、3D点云、视频以及它们的嵌入空间算术，只需进行图像-文本对齐训练。在训练过程中，我们采用可学习的Bind网络来对齐LLaMA和ImageBind的图像编码器之间的嵌入空间。然后，通过Bind网络转换的图像特征被添加到LLaMA的所有层的单词标记中，通过一个无注意力和零初始化的门控机制逐步注入视觉指令。在ImageBind的联合嵌入的帮助下，简单的图像-文本训练使我们的模型展示出了卓越的多模态指令跟随能力。在推断过程中，多模态输入被送入相应的ImageBind编码器，并被处理。",
    "tldr": "ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。",
    "en_tdlr": "ImageBind-LLM is a method for multi-modality instruction tuning, achieved through image-text alignment training and leveraging joint embedding for superior multi-modality instruction-following capabilities."
}