{
    "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing. (arXiv:2309.15826v1 [cs.CL])",
    "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yield",
    "link": "http://arxiv.org/abs/2309.15826",
    "context": "Title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing. (arXiv:2309.15826v1 [cs.CL])\nAbstract: Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yield",
    "path": "papers/23/09/2309.15826.json",
    "total_tokens": 938,
    "translated_title": "跨模态多任务的语音到文本翻译方法通过硬参数共享",
    "translated_abstract": "最近的端到端语音到文本翻译(ST)的研究提出了一种利用机器翻译(MT)数据通过次要编码器将文本目标转化为跨模态表示的多任务方法和软参数共享。本文中，我们提出了一种具有硬参数共享的ST/MT多任务框架，其中所有模型参数都以跨模态方式共享。我们的方法通过将语音和文本输入转换为两个相似长度的离散标记序列的预处理阶段来减小语音-文本模态差距，这使得模型可以简单地使用一个联合词汇表对两种模态进行无差别处理。通过对MuST-C的实验，我们证明了我们的多任务框架在没有外部MT数据的情况下，可以提高注意力编码器-解码器、连接主义时序分类(CTC)、传递器和联合CTC/注意力模型的平均BLEU得分+0.5。此外，我们还展示了该框架可以利用外部MT数据，并获得更好的结果。",
    "tldr": "本文提出了一种具有硬参数共享的ST/MT多任务框架，通过将语音和文本输入转换为两个相似长度的离散标记序列的预处理阶段来减小语音-文本模态差距，以提高翻译性能，而不需要外部MT数据。",
    "en_tdlr": "This paper proposes a ST/MT multi-tasking framework with hard parameter sharing, which reduces the modality gap between speech and text inputs by converting them into two discrete token sequences of similar length in a preprocessing stage, achieving improved translation performance without the need for external MT data."
}