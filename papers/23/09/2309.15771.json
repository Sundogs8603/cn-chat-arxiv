{
    "title": "Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])",
    "abstract": "We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a \"pessimistic\" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the \"implicit exploration\" estimator of \\citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive \"uniform coverage\" assumption made in all previous works.",
    "link": "http://arxiv.org/abs/2309.15771",
    "context": "Title: Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])\nAbstract: We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a \"pessimistic\" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the \"implicit exploration\" estimator of \\citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive \"uniform coverage\" assumption made in all previous works.",
    "path": "papers/23/09/2309.15771.json",
    "total_tokens": 981,
    "translated_title": "权重重要的离线学习正确地完成",
    "translated_abstract": "我们研究了随机上下文赌博问题中的离线策略优化问题，目标是基于由次优行为策略收集的决策数据集学习一个近似最优的策略。我们不对奖励函数做任何结构性假设，而是假设可以访问给定的策略类，并且旨在与该类中的最佳比较器策略竞争。在这种情况下，标准方法是计算每个策略价值的权重重要估计，并选择一个最小化估计值的策略，减去估计值中的“悲观”调整以减少其随机波动。在本文中，我们展示了一种基于 \\citet{Neu2015} 的“隐式探索”估计器的简单替代方法，其性能保证在几乎所有可能的情况下都优于之前的结果。尤其值得注意的是，我们消除了之前所有工作中非常苛刻的“均匀覆盖”假设。",
    "tldr": "本文研究了随机上下文赌博问题中的离线策略优化问题，并提出了一种替代方法，通过使用“隐式探索”估计器来计算策略价值的权重重要估计。与之前的结果相比，在几乎所有情况下都具有更好的性能保证，同时消除了之前所做的非常苛刻的“均匀覆盖”假设。",
    "en_tdlr": "This paper investigates the problem of offline policy optimization in stochastic contextual bandit problems. It introduces an alternative approach that uses an \"implicit exploration\" estimator to compute importance-weighted estimators of policy values. Compared to previous results, this approach has superior performance guarantees in nearly all cases and removes the restrictive \"uniform coverage\" assumption."
}