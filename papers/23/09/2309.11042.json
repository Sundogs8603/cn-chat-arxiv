{
    "title": "Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])",
    "abstract": "Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks ",
    "link": "http://arxiv.org/abs/2309.11042",
    "context": "Title: Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])\nAbstract: Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks ",
    "path": "papers/23/09/2309.11042.json",
    "total_tokens": 947,
    "translated_title": "使用任务适配器的混合多任务学习使小型语言模型更好",
    "translated_abstract": "最近，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了惊人的零样本学习性能，特别是文本生成任务。然而，LLMs的尺寸通常会导致模型训练和在线部署的高计算成本。在我们的工作中，我们提出了ALTER，一个在小型语言模型（参数<1B）上有效构建混合任务适配器的多任务学习系统，以同时处理多个NLP任务，并捕捉任务之间的共同点和差异，以支持特定领域的应用。具体而言，在ALTER中，我们提出了任务适配器混合（MTA）模块，作为底层模型变压器架构的扩展，用于捕捉任务内部的知识和任务间的知识。进一步提出了一种两阶段的训练方法，以在较小的计算成本下优化适配器之间的协作。在混合NLP任务的实验结果上",
    "tldr": "本论文提出了一种使用任务适配器的混合多任务学习方法，通过在小型语言模型上构建混合任务适配器，同时处理多个NLP任务，并通过两阶段训练方法优化适配器之间的协作，从而实现在较小的计算成本下支持多个领域特定应用。",
    "en_tdlr": "This paper proposes a method of using mixture-of-task-adapters for better multi-task learning, which builds upon small language models to handle multiple NLP tasks simultaneously. It optimizes the collaboration between adapters through a two-stage training method and enables support for domain-specific applications at a lower computational cost."
}