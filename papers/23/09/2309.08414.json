{
    "title": "Make Deep Networks Shallow Again. (arXiv:2309.08414v1 [cs.LG])",
    "abstract": "Deep neural networks have a good success record and are thus viewed as the best architecture choice for complex applications. Their main shortcoming has been, for a long time, the vanishing gradient which prevented the numerical optimization algorithms from acceptable convergence. A breakthrough has been achieved by the concept of residual connections -- an identity mapping parallel to a conventional layer. This concept is applicable to stacks of layers of the same dimension and substantially alleviates the vanishing gradient problem. A stack of residual connection layers can be expressed as an expansion of terms similar to the Taylor expansion. This expansion suggests the possibility of truncating the higher-order terms and receiving an architecture consisting of a single broad layer composed of all initially stacked layers in parallel. In other words, a sequential deep architecture is substituted by a parallel shallow one. Prompted by this theory, we investigated the performance capa",
    "link": "http://arxiv.org/abs/2309.08414",
    "context": "Title: Make Deep Networks Shallow Again. (arXiv:2309.08414v1 [cs.LG])\nAbstract: Deep neural networks have a good success record and are thus viewed as the best architecture choice for complex applications. Their main shortcoming has been, for a long time, the vanishing gradient which prevented the numerical optimization algorithms from acceptable convergence. A breakthrough has been achieved by the concept of residual connections -- an identity mapping parallel to a conventional layer. This concept is applicable to stacks of layers of the same dimension and substantially alleviates the vanishing gradient problem. A stack of residual connection layers can be expressed as an expansion of terms similar to the Taylor expansion. This expansion suggests the possibility of truncating the higher-order terms and receiving an architecture consisting of a single broad layer composed of all initially stacked layers in parallel. In other words, a sequential deep architecture is substituted by a parallel shallow one. Prompted by this theory, we investigated the performance capa",
    "path": "papers/23/09/2309.08414.json",
    "total_tokens": 859,
    "translated_title": "再次使深层网络变浅",
    "translated_abstract": "深度神经网络在复杂应用中有着良好的成功记录，因此被视为最佳架构选择。然而，长期以来它们的主要缺点一直是梯度消失问题，导致数值优化算法无法收敛。通过残差连接的概念，取得了重大突破--在常规层的旁边构建了一个恒等映射。这个概念适用于具有相同维度的层堆叠，并且大大减轻了梯度消失问题。残差连接层堆叠可以表达为类似泰勒展开的项的扩展。这种展开方式提出了截断高阶项的可能性，并可以得到一个由所有初始堆叠层以并行方式组成的单个宽广层的结构。换句话说，将顺序深层架构替换为并行浅层架构。在这一理论的推动下，我们研究了性能可能的上限",
    "tldr": "通过残差连接的概念将顺序深层架构替换为并行浅层架构，大大减轻了梯度消失问题，并提出了通过截断高阶项的方式来得到宽广层的结构。",
    "en_tdlr": "By replacing sequential deep architecture with parallel shallow architecture using the concept of residual connections, the vanishing gradient problem is significantly alleviated and a broad layer structure can be obtained by truncating higher-order terms."
}