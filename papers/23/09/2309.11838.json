{
    "title": "Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])",
    "abstract": "In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pretraining while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses.",
    "link": "http://arxiv.org/abs/2309.11838",
    "context": "Title: Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])\nAbstract: In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pretraining while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses.",
    "path": "papers/23/09/2309.11838.json",
    "total_tokens": 834,
    "translated_title": "在信息搜索对话中评估大型语言模型用于基于文档的响应生成",
    "translated_abstract": "本文研究了在信息搜索对话的背景下，使用类似ChatGPT的大型语言模型（LLM）进行基于文档的响应生成。我们使用了先前在DialDoc 2022共享任务中使用的四个社会服务领域的任务导向对话的MultiDoc2Dial语料库进行评估。信息搜索对话的转换以多个提供相关信息的文档为基础。我们通过使用两种方法Chat-Completion和LlamaIndex，通过激发ChatGPT模型来生成对话完成响应。ChatCompletion使用ChatGPT模型的预训练知识，而LlamaIndex还从文档中提取相关信息。我们观察到，通过LLM进行基于文档的响应生成不能通过自动评估指标充分评估，因为它们更加冗长，所以我们进行了人工评估，其中评估员对共享任务的获奖系统、两个Chat-GPT变体的输出以及人类响应进行评级。",
    "tldr": "本文研究了在信息搜索对话中使用大型语言模型进行基于文档的响应生成，并通过人工评估了其性能。",
    "en_tdlr": "This paper investigates the use of large language models for document-grounded response generation in information-seeking dialogues and evaluates their performance through human evaluation."
}