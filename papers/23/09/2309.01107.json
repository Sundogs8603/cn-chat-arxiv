{
    "title": "Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization",
    "abstract": "In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method and prove its convergence. Numerical experiments illustrate the learned policy's robustness",
    "link": "https://arxiv.org/abs/2309.01107",
    "context": "Title: Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization\nAbstract: In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method and prove its convergence. Numerical experiments illustrate the learned policy's robustness",
    "path": "papers/23/09/2309.01107.json",
    "total_tokens": 949,
    "translated_title": "通过频率正则化解决非矩形奖励强鲁棒马尔可夫决策过程",
    "translated_abstract": "在强鲁棒马尔可夫决策过程（RMDPs）中，假设奖励和转移动态位于给定的不确定性集合中。通过针对该集合中最具敌对性的模型下的最大回报，RMDPs解决了对错误环境的性能敏感性问题。然而，为了保持计算的可行性，传统上对于每个状态独立地构建不确定性集合。这种所谓的矩形条件仅仅是基于计算上的考虑。因此，它缺乏实际的动机，可能导致过度保守的行为。在这项工作中，我们研究了耦合奖励RMDPs，其中转移核是固定的，但是奖励函数在与名义奖励函数相距 α 半径内。我们直接联系了这种非矩形奖励-RMDPs和应用策略访问频率正则化之间的关系。我们介绍了一种策略梯度方法并证明了其收敛性。数值实验证明了所学策略的鲁棒性",
    "tldr": "本研究通过引入策略访问频率正则化，解决了非矩形奖励强鲁棒马尔可夫决策过程（RMDPs）的问题，并且提出了一种策略梯度方法并证明了其收敛性。",
    "en_tdlr": "This study addresses the problem of non-rectangular reward-robust Markov decision processes (RMDPs) through the introduction of policy visitation frequency regularization. A policy-gradient method is proposed and its convergence is proven, providing a solution for the robustness of the learned policy."
}