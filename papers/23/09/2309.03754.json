{
    "title": "Convergence Analysis of Decentralized ASGD. (arXiv:2309.03754v1 [cs.LG])",
    "abstract": "Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.  In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\\mathcal{O}(\\sigma\\epsilon^{-2}) + \\mat",
    "link": "http://arxiv.org/abs/2309.03754",
    "context": "Title: Convergence Analysis of Decentralized ASGD. (arXiv:2309.03754v1 [cs.LG])\nAbstract: Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.  In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\\mathcal{O}(\\sigma\\epsilon^{-2}) + \\mat",
    "path": "papers/23/09/2309.03754.json",
    "total_tokens": 823,
    "translated_title": "分散式ASGD的收敛分析",
    "translated_abstract": "在过去的几十年里，随机梯度下降（SGD）一直受到机器学习界的广泛研究。尽管其具有多样性和出色的性能，但通过SGD优化大模型仍然是一项耗时的任务。为了缩短训练时间，常常将训练过程分布在多个设备上。最近已经证明，异步SGD（ASGD）的收敛速度总是比小批量SGD快。然而，尽管在理论上的改善，大多数ASGD收敛速度的证明仍然依赖于一个集中式参数服务器，在将梯度计算扩展到许多分布式进程时容易成为瓶颈。本文介绍了一种新颖的分散式异步SGD（DASGD）的收敛速度分析方法，该方法不需要节点之间的部分同步，也不需要限制性的网络拓扑结构。",
    "tldr": "本文提出了一个不需要部分同步和限制性网络拓扑结构的分散式和异步SGD（DASGD）的收敛速度分析方法。",
    "en_tdlr": "This paper presents a convergence-rate analysis for decentralized and asynchronous SGD (DASGD) without the need for partial synchronization among nodes or restrictive network topologies."
}