{
    "title": "Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])",
    "abstract": "Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-bas",
    "link": "http://arxiv.org/abs/2309.16173",
    "context": "Title: Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])\nAbstract: Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-bas",
    "path": "papers/23/09/2309.16173.json",
    "total_tokens": 939,
    "translated_title": "Distill to Delete: 使用知识蒸馏进行图网络中的遗忘",
    "translated_abstract": "图遗忘已成为从预训练图神经网络（GNN）中删除信息的重要方法。可以删除节点、节点类、边或边类。遗忘方法使GNN模型符合数据保护法规（即被遗忘权），适应不断变化的数据分布，并通过避免重复训练来减少GPU小时的碳足迹。现有的基于分区和聚合的方法在处理局部图依赖和附加开销方面存在局限性。最近，GNNDelete提出了一种模型无关的方法，缓解了其中一些问题。我们的工作通过知识蒸馏采用了一种新的方法来解决图遗忘中的这些挑战，即GNN中的跨轴蒸馏进行删除（D2DGN）。这是一个模型无关的蒸馏框架，将完整的图知识划分并标记为保留和删除。它使用响应为基础进行蒸馏。",
    "tldr": "本论文提出了一种名为D2DGN的图遗忘方法，通过知识蒸馏的方式删除图神经网络中的信息。这种方法解决了传统方法在处理局部依赖和附加开销方面的局限性，并能够适应不断变化的数据分布和减少训练重复带来的能源消耗。"
}