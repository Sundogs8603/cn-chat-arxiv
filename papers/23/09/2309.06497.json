{
    "title": "A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale. (arXiv:2309.06497v1 [cs.LG])",
    "abstract": "Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our i",
    "link": "http://arxiv.org/abs/2309.06497",
    "context": "Title: A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale. (arXiv:2309.06497v1 [cs.LG])\nAbstract: Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our i",
    "path": "papers/23/09/2309.06497.json",
    "total_tokens": 937,
    "translated_title": "PyTorch分布式数据并行的分布式Shampoo优化器实现用于大规模训练神经网络",
    "translated_abstract": "Shampoo是一种在线和随机优化算法，属于AdaGrad方法家族，用于训练神经网络。它构建一个块对角先验矩阵，其中每个块由神经网络每个参数的全矩阵AdaGrad的粗略Kronecker积近似构成。在这项工作中，我们提供了算法的完整描述，以及我们的实现利用PyTorch来训练深度网络的性能优化。我们的实现通过PyTorch的DTensor数据结构来分布每个参数块的内存和计算，并在每次迭代中对计算得到的搜索方向进行AllGather原语操作，从而实现快速的多GPU分布式数据并行训练。这一主要性能提升使我们在每步墙钟时间上相比于标准的基于对角缩放的自适应梯度方法最多只有10%的性能下降。我们通过实验验证了我们的实现。",
    "tldr": "这是一个针对大规模训练神经网络的PyTorch分布式数据并行实现的分布式Shampoo优化算法。这种算法利用了块对角先验矩阵和搜索方向的AllGather原语操作，在性能上实现了显著的优化，相比于传统的对角缩放的自适应梯度方法仅有最多10%的性能损失。"
}