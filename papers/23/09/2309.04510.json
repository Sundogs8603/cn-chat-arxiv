{
    "title": "Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning. (arXiv:2309.04510v1 [cs.LG])",
    "abstract": "Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a n",
    "link": "http://arxiv.org/abs/2309.04510",
    "context": "Title: Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning. (arXiv:2309.04510v1 [cs.LG])\nAbstract: Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a n",
    "path": "papers/23/09/2309.04510.json",
    "total_tokens": 821,
    "translated_title": "减少贝叶斯优化的计算时间：通用的记忆修剪方法",
    "translated_abstract": "在处理高维度或大规模数据集时，贝叶斯优化（BO）所需的计算时间较长。这是由于高斯过程代理模型与实验数量具有多项式时间复杂度导致的。由于计算时间的这种复杂度扩展，运行BO在高维度或大规模数据集上变得困难，从而阻碍了实验。已经开发了替代的代理模型来降低BO过程的计算利用率，然而，这些方法需要对继承的代理函数进行数学修改，限制了只能使用该函数。在本文中，我们展示了一个通用的BO包装器——记忆修剪和有界优化，能够与任何代理模型和获取函数一起使用。通过使用这种记忆修剪方法，我们展示了BO每个实验的墙钟计算时间从多项式增加模式下降到锯齿模式。",
    "tldr": "该论文提出了一个通用的贝叶斯优化方法，通过记忆修剪和有界优化，可以降低计算时间，并适用于任何代理模型和获取函数。",
    "en_tdlr": "This paper presents a generalizable Bayesian optimization method that reduces computing time by using memory pruning and bounded optimization, which can be applied to any surrogate model and acquisition function."
}