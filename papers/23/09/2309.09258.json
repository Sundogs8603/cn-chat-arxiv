{
    "title": "Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets. (arXiv:2309.09258v1 [cs.LG])",
    "abstract": "In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are \"Villani functions\" and thus be able to build on recent progress with analyzing SGD on such objectives.",
    "link": "http://arxiv.org/abs/2309.09258",
    "context": "Title: Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets. (arXiv:2309.09258v1 [cs.LG])\nAbstract: In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are \"Villani functions\" and thus be able to build on recent progress with analyzing SGD on such objectives.",
    "path": "papers/23/09/2309.09258.json",
    "total_tokens": 852,
    "translated_title": "两层神经网络上逻辑回归代价函数的全局收敛性",
    "translated_abstract": "在本文中，我们首次证明了随机梯度下降（SGD）对于适当正则化的深度为2的神经网络的逻辑回归代价函数能够收敛到全局极小值，对于任意数据和具有充分平滑且有界激活函数（如sigmoid和tanh）。我们还证明了连续时间SGD的指数级快速收敛速度，该结果也适用于光滑无界的激活函数（如SoftPlus）。我们的关键思想是证明了在恒定大小的神经网络上存在Frobenius范数正则化的逻辑回归代价函数，这些函数是\"Villani函数\"，从而能够构建在最近对于此类目标函数上分析SGD的研究进展上。",
    "tldr": "本文首次证明了在深度为2的神经网络上，适当正则化的逻辑回归代价函数通过随机梯度下降（SGD）能够收敛到全局极小值，这适用于任意数据和具有充分平滑且有界激活函数。同时，我们还证明了连续时间SGD的指数级快速收敛速度，该结果也适用于光滑无界的激活函数。",
    "en_tdlr": "This paper provides the first provable convergence of SGD to the global minima for appropriately regularized logistic empirical risks on depth 2 neural nets, for arbitrary data and with adequately smooth and bounded activations. The paper also proves an exponentially fast convergence rate for continuous time SGD, which applies to smooth unbounded activations as well."
}