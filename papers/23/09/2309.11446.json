{
    "title": "Weight Averaging Improves Knowledge Distillation under Domain Shift. (arXiv:2309.11446v1 [cs.LG])",
    "abstract": "Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our f",
    "link": "http://arxiv.org/abs/2309.11446",
    "context": "Title: Weight Averaging Improves Knowledge Distillation under Domain Shift. (arXiv:2309.11446v1 [cs.LG])\nAbstract: Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our f",
    "path": "papers/23/09/2309.11446.json",
    "total_tokens": 903,
    "translated_title": "加权平均改善了领域转移下的知识蒸馏",
    "translated_abstract": "知识蒸馏是一种广泛应用于实际深度学习应用中的强大模型压缩技术。它专注于训练一个小型的学生网络来模仿一个较大的教师网络。尽管人们普遍认为知识蒸馏可以在i.i.d的情况下改善学生的泛化能力，但在领域转移下学生网络的性能，即在训练期间未见过的领域数据上的性能，在文献中得到了很少的关注。在本文中，我们在知识蒸馏和领域泛化的研究领域之间迈出了一步。我们展示了在领域转移下，领域泛化文献中提出的加权平均技术，如SWAD和SMA，也可以提高知识蒸馏的性能。此外，我们提出了一种简单的加权平均策略，不需要在训练过程中对验证数据进行评估，并且展示了当应用于知识蒸馏时，它与SWAD和SMA的性能相当。我们将其命名为f",
    "tldr": "加权平均技术在领域转移下的知识蒸馏中也能提高性能，可以应用于没有验证数据的简单加权平均策略。",
    "en_tdlr": "Weight averaging techniques can improve the performance of knowledge distillation under domain shift, and a simple weight averaging strategy without validation data can achieve comparable results."
}