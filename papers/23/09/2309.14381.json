{
    "title": "Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])",
    "abstract": "In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community.  The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-le",
    "link": "http://arxiv.org/abs/2309.14381",
    "context": "Title: Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])\nAbstract: In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community.  The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-le",
    "path": "papers/23/09/2309.14381.json",
    "total_tokens": 870,
    "translated_title": "社交偏见在视觉-语言模型中的调查",
    "translated_abstract": "近年来，机器学习模型，尤其是基于Transformer的预训练模型，在自然语言处理和计算机视觉领域取得了快速的发展。然而，研究人员发现这些模型可能会无意中捕捉和强化其训练数据集中存在的社会偏见，导致资源分配不均和对特定社会群体的不公平代表。在人工智能系统中解决这些偏见并确保公平性已经成为机器学习社区的关键关切。最近引入的预训练的视觉-语言模型在新兴的多模态领域中需要关注这些模型中存在的潜在社会偏见。虽然视觉-语言模型容易受到社会偏见的影响，但对于与自然语言处理和计算机视觉中的偏见相比，人们对其了解有限。本调查旨在为研究人员提供高水平的综述和资源，以增进对视觉-语言模型中社会偏见的理解。",
    "tldr": "社交偏见在视觉-语言模型中的调查，旨在为研究人员提供对潜在社会偏见的理解，以解决资源分配不均和不公平代表等问题。"
}