{
    "title": "DeepliteRT: Computer Vision at the Edge. (arXiv:2309.10878v1 [cs.LG])",
    "abstract": "The proliferation of edge devices has unlocked unprecedented opportunities for deep learning model deployment in computer vision applications. However, these complex models require considerable power, memory and compute resources that are typically not available on edge platforms. Ultra low-bit quantization presents an attractive solution to this problem by scaling down the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on ",
    "link": "http://arxiv.org/abs/2309.10878",
    "context": "Title: DeepliteRT: Computer Vision at the Edge. (arXiv:2309.10878v1 [cs.LG])\nAbstract: The proliferation of edge devices has unlocked unprecedented opportunities for deep learning model deployment in computer vision applications. However, these complex models require considerable power, memory and compute resources that are typically not available on edge platforms. Ultra low-bit quantization presents an attractive solution to this problem by scaling down the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on ",
    "path": "papers/23/09/2309.10878.json",
    "total_tokens": 926,
    "translated_title": "DeepliteRT：边缘计算机视觉中的深度学习",
    "translated_abstract": "边缘设备的普及为计算机视觉应用中深度学习模型的部署开辟了前所未有的机会。然而，这些复杂模型需要大量的电力、内存和计算资源，而这些资源通常在边缘平台上不可用。超低位量化提供了一个解决方案，通过将模型权重和激活从32位缩减到小于8位来降低模型的复杂度。我们在基于ARM架构的目标上实现了高度优化的超低位卷积运算符，性能优于现有方法的最多4.34倍。我们的运算符是在Deeplite Runtime（DeepliteRT）中实现的，这是一个端到端的解决方案，用于在ARM设备上编译、调优和推断超低位模型。在DeepliteRT中的编译器转换工作会将一个基于伪量化的全精度模型自动转换为紧凑的超低位表示，从而简化了在商用硬件上部署量化模型的过程。我们分析了DeepliteRT在某些计算机视觉任务上的性能。",
    "tldr": "DeepliteRT是一个在边缘设备上部署深度学习模型的解决方案，通过超低位量化技术实现了高效的模型运算，并通过编译器转换工作简化了量化模型的部署过程。",
    "en_tdlr": "DeepliteRT is a solution for deploying deep learning models on edge devices, achieving efficient model operations through ultra-low-bit quantization and simplifying the deployment process of quantized models through compiler conversion."
}