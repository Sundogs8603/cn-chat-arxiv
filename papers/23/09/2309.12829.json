{
    "title": "Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])",
    "abstract": "Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs",
    "link": "http://arxiv.org/abs/2309.12829",
    "context": "Title: Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])\nAbstract: Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs",
    "path": "papers/23/09/2309.12829.json",
    "total_tokens": 893,
    "translated_title": "合成提升：利用合成数据增强超声心动图中的视觉-语言分割",
    "translated_abstract": "准确的分割对于基于超声心动图的心血管疾病评估至关重要。然而，超声图像的变异性和固有挑战阻碍了精确的分割。通过利用图像和文本模态的联合表示，视觉-语言分割模型（VLSM）可以融入丰富的上下文信息，可能有助于精确和可解释的分割。然而，超声心动图中缺乏现成的数据阻碍了VLSM的训练。本研究中，我们探讨了使用语义扩散模型（SDM）生成的合成数据集来增强超声心动图分割的VLSM。我们使用从超声心动图图像、分割掩模和元数据中自动提取的多个属性导出的七种不同的语言提示来评估两个流行的VLSM模型（CLIPSeg和CRIS）的结果。我们的结果显示，在预训练VLSM时，转换和收敛速度更快。",
    "tldr": "本研究探讨了使用合成数据集来增强超声心动图分割的视觉-语言分割模型（VLSM），结果显示合成数据集可以提高分割模型的指标和训练速度。",
    "en_tdlr": "This study explores enhancing vision-language segmentation models (VLSMs) for echocardiography segmentation using synthetic datasets, which results in improved metrics and training speed."
}