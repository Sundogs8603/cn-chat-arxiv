{
    "title": "PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. (arXiv:2309.02014v2 [math.OC] UPDATED)",
    "abstract": "This paper introduces PROMISE ($\\textbf{Pr}$econditioned Stochastic $\\textbf{O}$ptimization $\\textbf{M}$ethods by $\\textbf{I}$ncorporating $\\textbf{S}$calable Curvature $\\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine l",
    "link": "http://arxiv.org/abs/2309.02014",
    "context": "Title: PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. (arXiv:2309.02014v2 [math.OC] UPDATED)\nAbstract: This paper introduces PROMISE ($\\textbf{Pr}$econditioned Stochastic $\\textbf{O}$ptimization $\\textbf{M}$ethods by $\\textbf{I}$ncorporating $\\textbf{S}$calable Curvature $\\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine l",
    "path": "papers/23/09/2309.02014.json",
    "total_tokens": 887,
    "translated_title": "PROMISE: 通过引入可扩展曲率估计的预条件随机优化方法",
    "translated_abstract": "本文介绍了PROMISE（通过引入可扩展曲率估计的预条件随机优化方法），这是一套基于草图的预条件随机梯度算法套件，用于解决机器学习中出现的大规模凸优化问题。PROMISE包括SVRG、SAGA和Katyusha的预条件版本；每个算法都有强大的理论分析和有效的默认超参数值。相比之下，传统的随机梯度方法需要仔细调节超参数才能成功，在机器学习中普遍存在的病态条件下性能会下降。实验证明，通过使用默认超参数值，所提出的算法在由基准机器学习问题汇编的51个岭回归和逻辑回归问题上优于或与流行的调整后的随机梯度优化器相匹配。",
    "tldr": "本文介绍了一套基于草图的预条件随机梯度算法套件PROMISE，用于解决大规模凸优化问题，相比传统方法，在默认超参数下在机器学习问题上表现更优。",
    "en_tdlr": "This paper introduces PROMISE, a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems in machine learning. The proposed algorithms, with effective default hyperparameter values, outperform or match popular tuned stochastic gradient optimizers on a test bed of 51 ridge and logistic regression problems."
}