{
    "title": "Actor critic learning algorithms for mean-field control with moment neural networks. (arXiv:2309.04317v1 [stat.ML])",
    "abstract": "We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.",
    "link": "http://arxiv.org/abs/2309.04317",
    "context": "Title: Actor critic learning algorithms for mean-field control with moment neural networks. (arXiv:2309.04317v1 [stat.ML])\nAbstract: We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.",
    "path": "papers/23/09/2309.04317.json",
    "total_tokens": 886,
    "translated_title": "使用矩神经网络的平均场控制中的演员-评论员学习算法",
    "translated_abstract": "我们在连续时间强化学习环境中开发了一种解决平均场控制问题的新的策略梯度和演员-评论员算法。我们的方法利用基于梯度的价值函数表示，采用参数化的随机策略。演员（策略）和评论员（价值函数）的学习是通过在概率测度的Wasserstein空间上的一类矩神经网络函数来实现的，其关键特征是直接采样分布的轨迹。这项研究中解决的一个核心挑战涉及到对于平均场框架特有的运算符的计算处理。为了说明我们方法的有效性，我们提供了一系列全面的数值结果。这些结果涵盖了多维设置和具有受控波动性的非线性二次平均场控制问题等不同的例子。",
    "tldr": "我们开发了一种使用矩神经网络的演员-评论员算法，用于解决平均场控制问题。我们的方法利用基于梯度的价值函数表示，并通过直接采样分布的轨迹来实现学习。数值结果表明，我们的方法在多维和非线性二次控制问题等不同情境下具有良好的效果。",
    "en_tdlr": "We develop an actor-critic algorithm using moment neural networks for mean-field control problems. Our method uses a gradient-based value function representation and learns by sampling trajectories of distributions directly. Numerical results show the effectiveness of our approach in diverse scenarios, including multi-dimensional and nonlinear quadratic control problems."
}