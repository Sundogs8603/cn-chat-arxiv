{
    "title": "Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning. (arXiv:2309.16074v1 [cs.RO])",
    "abstract": "Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on",
    "link": "http://arxiv.org/abs/2309.16074",
    "context": "Title: Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning. (arXiv:2309.16074v1 [cs.RO])\nAbstract: Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on",
    "path": "papers/23/09/2309.16074.json",
    "total_tokens": 885,
    "translated_title": "通过反向强化学习从示范中推理和调整：双足动作奖励学习",
    "translated_abstract": "使双足行走机器人学习如何在高度不平坦、动态变化的地形上行进是具有挑战性的，这是由于机器人动力学和相互作用环境的复杂性。最近示范学习的进展在复杂环境中展示了机器人学习的良好结果。虽然模仿学习专家策略的研究已经得到了很好的探索，但在腿部运动中学习专家奖励函数的研究在很大程度上仍未得到充分探索。本文将先进的反向强化学习(IRL)技术引入解决双足动作问题。我们提出了学习专家奖励函数的算法，并对学习到的函数进行了分析。通过非线性函数逼近，我们发现了专家的动作策略中有意义的见解。此外，我们通过实验证明，使用推理出的奖励函数训练双足行走策略可以提高其行走性能。",
    "tldr": "本文通过反向强化学习方法解决复杂地形上的双足行走问题。我们提出了学习专家奖励函数的算法，并分析了学习到的函数。实验证明使用推理出的奖励函数可以提高双足行走策略的行走性能。"
}