{
    "title": "Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])",
    "abstract": "Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra",
    "link": "http://arxiv.org/abs/2309.15096",
    "context": "Title: Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])\nAbstract: Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra",
    "path": "papers/23/09/2309.15096.json",
    "total_tokens": 946,
    "translated_title": "修复NTK：从神经网络线性化到精确的凸程序",
    "translated_abstract": "最近，深度神经网络的理论分析主要集中在两个方向上：1）通过在隐藏层宽度无限大和学习率无穷小的情况下进行的SGD训练的理论洞察力（也称为梯度流）通过神经切线核（NTK）；2）通过锥约束凸重塑ReLU网络的全局优化训练目标。后一种研究方向还提供了ReLU网络的另一种公式，称为门控ReLU网络，可通过高效的无约束凸程序进行全局优化。在这项工作中，我们将门控ReLU网络的凸问题解释为具有加权数据屏蔽特征映射的多核学习（MKL）模型，并与NTK建立了连接。具体而言，我们证明了对于那些与学习目标无关的特定选择的掩码权重，该核等效于门控ReLU网络在训练样本上的NTK。",
    "tldr": "本文从两个方向对深度神经网络进行理论分析，提供了通过神经切线核（NTK）和通过凸重塑ReLU网络的全局优化训练目标的方法。此外，我们还提出了一种与NTK相连的多核学习模型，称为门控ReLU网络，通过加权数据屏蔽特征映射来实现全局优化。"
}