{
    "title": "GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue. (arXiv:2309.04162v1 [cs.CL])",
    "abstract": "Pre-trained models have achieved success in Chinese Short Text Matching (STM) tasks, but they often rely on superficial clues, leading to a lack of robust predictions. To address this issue, it is crucial to analyze and mitigate the influence of superficial clues on STM models. Our study aims to investigate their over-reliance on the edit distance feature, commonly used to measure the semantic similarity of Chinese text pairs, which can be considered a superficial clue. To mitigate STM models' over-reliance on superficial clues, we propose a novel resampling training strategy called Gradually Learn Samples Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the robustness and generalization of Chinese STM models. Moreover, we conduct a detailed analysis of existing methods and reveal their commonality.",
    "link": "http://arxiv.org/abs/2309.04162",
    "context": "Title: GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue. (arXiv:2309.04162v1 [cs.CL])\nAbstract: Pre-trained models have achieved success in Chinese Short Text Matching (STM) tasks, but they often rely on superficial clues, leading to a lack of robust predictions. To address this issue, it is crucial to analyze and mitigate the influence of superficial clues on STM models. Our study aims to investigate their over-reliance on the edit distance feature, commonly used to measure the semantic similarity of Chinese text pairs, which can be considered a superficial clue. To mitigate STM models' over-reliance on superficial clues, we propose a novel resampling training strategy called Gradually Learn Samples Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the robustness and generalization of Chinese STM models. Moreover, we conduct a detailed analysis of existing methods and reveal their commonality.",
    "path": "papers/23/09/2309.04162.json",
    "total_tokens": 1001,
    "translated_title": "GLS-CSC: 一种简单但有效的策略来减少中文STM模型对表面线索的依赖",
    "translated_abstract": "预训练模型在中文短文本匹配任务中取得了成功，但它们往往依赖于表面线索，导致缺乏鲁棒性的预测。为了解决这个问题，分析和减少表面线索对STM模型的影响至关重要。本研究旨在调查编辑距离特征对STM模型的过度依赖情况，编辑距离常被用来衡量中文文本对的语义相似度，可以被认为是表面线索。为了减少STM模型对表面线索的依赖，我们提出了一种名为Gradually Learn Samples Containing Superficial Clue (GLS-CSC)的新的重采样训练策略。通过对领域内（I.D.）、鲁棒性（Rob.）和领域外（O.O.D.）测试集的全面评估，我们证明了GLS-CSC在增强中文STM模型的鲁棒性和泛化性方面优于现有方法。此外，我们对现有方法进行了详细分析，并揭示了它们的共同点。",
    "tldr": "本研究提出了GLS-CSC，一种用于缓解中文STM模型对表面线索过度依赖的简单但有效的策略。通过分析编辑距离特征的影响，我们证明GLS-CSC能够提高中文STM模型的鲁棒性和泛化性能。"
}