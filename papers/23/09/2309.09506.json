{
    "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models. (arXiv:2309.09506v2 [cs.CV] UPDATED)",
    "abstract": "Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the complet",
    "link": "http://arxiv.org/abs/2309.09506",
    "context": "Title: LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models. (arXiv:2309.09506v2 [cs.CV] UPDATED)\nAbstract: Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the complet",
    "path": "papers/23/09/2309.09506.json",
    "total_tokens": 843,
    "translated_title": "LayoutNUWA: 揭示大型语言模型的隐藏版式专长",
    "translated_abstract": "图形版式生成作为一个不断发展的研究领域，在用户参与度和信息感知方面起着重要作用。现有方法主要将版式生成视为数值优化任务，注重定量方面，但忽略了版式的语义信息，如每个版式元素之间的关系。本文提出了LayoutNUWA，这是第一个将版式生成视为代码生成任务的模型，以增强语义信息并利用大型语言模型的隐藏版式专长。具体而言，我们开发了一种称为代码指示调整（Code Instruct Tuning，CIT）的方法，包括三个相互关联的模块：1）代码初始化（Code Initialization，CI）模块将数值条件量化并以HTML代码的形式进行初始化，并放置了策略性的屏蔽；2）代码完成（Code Completion，CC）模块利用语言模型的格式化知识填写HTML代码中的屏蔽部分；3）代码渲染（Code Rendering，CR）模块将完成的代码转化为最终的图形版式。",
    "tldr": "LayoutNUWA是第一个将版式生成视为代码生成任务来增强语义信息和利用大型语言模型的隐藏版式专长的模型。",
    "en_tdlr": "LayoutNUWA is the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models."
}