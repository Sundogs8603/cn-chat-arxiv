{
    "title": "MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images. (arXiv:2309.04790v1 [cs.CL])",
    "abstract": "In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), in-context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.",
    "link": "http://arxiv.org/abs/2309.04790",
    "context": "Title: MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images. (arXiv:2309.04790v1 [cs.CL])\nAbstract: In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), in-context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.",
    "path": "papers/23/09/2309.04790.json",
    "total_tokens": 929,
    "translated_title": "MMHQA-ICL: 文本、表格和图像多模态背景下的混合问答的多模若干学习",
    "translated_abstract": "在现实世界中，知识常常以多模态和异构的形式存在。解决包括文本、表格和图像在内的混合数据类型的问答任务是一项具有挑战性的任务（MMHQA）。最近，随着大型语言模型（LLM）的崛起，背景下学习（ICL）已成为解决QA问题的最流行的方法。我们提出了用于解决此问题的MMHQA-ICL框架，其中包括更强大的异构数据检索器和图像标题模块。最重要的是，我们提出了一种针对MMHQA的特定类型的背景下学习策略，使LLM能够在这个任务中发挥其强大的性能。我们是第一个在这个任务中使用端到端LLM提示方法的人。实验结果表明，我们的框架在MultimodalQA数据集的少样本设置下优于所有基准线和训练在完整数据集上的方法，达到了最先进的结果。",
    "tldr": "MMHQA-ICL框架结合了强大的异构数据检索器和图像标题模块，提出了一种特定类型的背景下学习策略，使得大型语言模型能够在多模态混合问答任务中取得最先进的结果。"
}