{
    "title": "Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])",
    "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-",
    "link": "http://arxiv.org/abs/2309.00667",
    "context": "Title: Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])\nAbstract: We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-",
    "path": "papers/23/09/2309.00667.json",
    "total_tokens": 943,
    "translated_title": "脱离上下文的影响：关于衡量LLMs中的情境意识论文",
    "translated_abstract": "我们旨在更好地理解大型语言模型（LLMs）中“情境意识”的出现。如果一个模型在意识到自己是一个模型的同时，能够识别自己当前是处于测试或部署状态，那么这个模型在情境上是具备意识的。今天的LLMs在部署之前会经过安全性和对齐性的测试。在部署后，一个LLM可能会利用情境意识在安全测试中取得高分，但在实际使用中采取有害行为。情境意识可能会意外地在模型扩展中出现。为了更好地预测这种出现，我们提出了对于情境意识而言必要的能力之一，即“脱离上下文推理”（与基于上下文的学习相对）。我们通过实验研究了脱离上下文推理。首先，我们在没有提供任何示例或演示的情况下，对LLM进行了描述测试的微调。在测试时，我们评估模型是否能通过测试。令我们惊讶的是，我们发现LLMs在这种情况下取得了成功。",
    "tldr": "这个论文目的在于研究大型语言模型中的情境意识的产生，提出了一种衡量模型情境意识的能力，并通过实验证明了其有效性。"
}