{
    "title": "Robotic Offline RL from Internet Videos via Value-Function Pre-Training. (arXiv:2309.13041v1 [cs.RO])",
    "abstract": "Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a \"type mismatch\" with video data (such as Ego4D), the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-",
    "link": "http://arxiv.org/abs/2309.13041",
    "context": "Title: Robotic Offline RL from Internet Videos via Value-Function Pre-Training. (arXiv:2309.13041v1 [cs.RO])\nAbstract: Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a \"type mismatch\" with video data (such as Ego4D), the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-",
    "path": "papers/23/09/2309.13041.json",
    "total_tokens": 944,
    "translated_title": "通过价值函数预训练从网络视频中进行机器人离线强化学习",
    "translated_abstract": "在许多现代机器学习系统中，通过在互联网数据上进行预训练已被证明是广泛推广的关键因素。那么，在机器人强化学习中启用这样的能力需要什么？离线强化学习方法利用机器人经验数据集进行学习，这是一种利用先前数据进入机器人学习流程的方式。然而，这些方法与视频数据（如Ego4D）存在\"类型不匹配\"，而视频数据是机器人学习的最大先前数据集，因为视频提供仅有观察经验，没有强化学习方法所需的动作或奖励注释。在本文中，我们开发了一个系统，通过时间差分学习完全基于学习价值函数，从大规模人类视频数据集中利用机器人离线强化学习。我们展示了与其他从视频数据学习的方法相比，价值学习对于下游机器人离线强化学习更有利的表示学习。我们的系统被称为V-PTR，将预训练的好处与离线强化学习相结合。",
    "tldr": "本文提出了一种通过时间差分学习从大规模视频数据集中利用机器人离线强化学习的系统，该系统被称为V-PTR。与其他学习视频数据方法相比，通过价值学习可以更好地表示机器人离线强化学习的数据。",
    "en_tdlr": "This paper proposes a system, called V-PTR, for leveraging robotic offline reinforcement learning from large-scale video datasets through temporal-difference learning. Value learning is shown to provide better representations for robotic offline RL compared to other methods for learning from video data."
}