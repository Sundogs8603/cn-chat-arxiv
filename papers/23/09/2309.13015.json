{
    "title": "Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design. (arXiv:2309.13015v1 [cs.LG])",
    "abstract": "Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DN",
    "link": "http://arxiv.org/abs/2309.13015",
    "context": "Title: Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design. (arXiv:2309.13015v1 [cs.LG])\nAbstract: Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DN",
    "path": "papers/23/09/2309.13015.json",
    "total_tokens": 999,
    "translated_title": "高效的N:M稀疏DNN训练使用算法，架构和数据流协同设计",
    "translated_abstract": "稀疏训练是减少DNN计算成本同时保持高准确性的一种有前景的技术。特别是N:M细粒度结构稀疏，其中只有连续M个元素中的N个可以是非零值，因其对硬件友好的模式和达到高稀疏比的能力而受到关注。然而，加速N:M稀疏DNN训练的潜力尚未充分利用，并且缺乏支持N:M稀疏训练的高效硬件。为了解决这些挑战，本文提出了一种使用算法，架构和数据流协同设计的计算高效的N:M稀疏DNN训练方案。在算法层面上，提出了一种双向权重修剪方法（BDWP），利用DNN训练的前向和后向传播过程中权重的N:M稀疏性，可以显著降低计算成本同时保持模型准确性。在架构层面上，提出了一种用于DNN稀疏加速的架构，该架构基于数据流协议，利用稀疏权重的结构模式优化计算性能。",
    "tldr": "本文提出了一种使用算法，架构和数据流协同设计的高效N:M稀疏DNN训练方案，该方案利用双向权重修剪方法优化计算成本，并通过稀疏加速器硬件支持实现高稀疏比的DNN训练。",
    "en_tdlr": "This paper presents an efficient N:M sparse DNN training scheme using algorithm, architecture, and dataflow co-design. The scheme leverages bidirectional weight pruning to optimize computational cost and achieves high sparsity ratio in DNN training with the support of a sparse accelerator hardware."
}