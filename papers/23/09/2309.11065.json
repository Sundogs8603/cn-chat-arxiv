{
    "title": "UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])",
    "abstract": "Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different dat",
    "link": "http://arxiv.org/abs/2309.11065",
    "context": "Title: UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])\nAbstract: Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different dat",
    "path": "papers/23/09/2309.11065.json",
    "total_tokens": 946,
    "translated_title": "UniPCM: 具有任务感知自动提示的通用预训练对话模型",
    "translated_abstract": "最近的研究表明，多任务预训练可以极大地提高模型的鲁棒性和迁移能力，这对于构建高质量的对话系统至关重要。然而，大多数先前关于多任务预训练的工作严重依赖于人为定义的输入格式或提示，这在质量和数量上都不是最佳的。在这项工作中，我们提出使用基于任务的自动提示生成（TAP）来自动生成高质量的提示。使用生成的高质量提示，我们将预训练对话模型的语料库扩展到了来自15个与对话相关任务的122个数据集，从而得到了通用预训练对话模型（UniPCM），这是一个对于各种对话任务和不同对话系统具有强大基础的模型。广泛的实验表明，UniPCM对输入提示具有鲁棒性，并且能够完成各种对话相关任务。此外，UniPCM具有很强的迁移能力，并在资源有限的场景下表现出色，在9个不同数据集上取得了最先进的结果。",
    "tldr": "本论文提出了一种使用任务感知自动提示生成来扩展预训练对话模型语料库的方法，从而构建了通用预训练对话模型（UniPCM）。实验证明，UniPCM具有鲁棒性、强大的迁移能力，并在不同对话任务上取得最先进的结果。",
    "en_tdlr": "This paper introduces a method to extend the corpus of pre-trained conversation models using task-aware automatic prompt generation, resulting in the Universal Pre-trained Conversation Model (UniPCM). The experiments show that UniPCM is robust, has strong transferability, and achieves state-of-the-art results on different dialogue tasks."
}