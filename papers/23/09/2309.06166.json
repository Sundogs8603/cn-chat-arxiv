{
    "title": "Certified Robust Models with Slack Control and Large Lipschitz Constants. (arXiv:2309.06166v1 [cs.LG])",
    "abstract": "Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establis",
    "link": "http://arxiv.org/abs/2309.06166",
    "context": "Title: Certified Robust Models with Slack Control and Large Lipschitz Constants. (arXiv:2309.06166v1 [cs.LG])\nAbstract: Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establis",
    "path": "papers/23/09/2309.06166.json",
    "total_tokens": 971,
    "translated_title": "具有弹性控制和较大Lipschitz常数的认证鲁棒模型",
    "translated_abstract": "尽管最近取得了成功，但目前最先进的基于学习的模型仍然对输入变化，如对抗样本，非常容易受到攻击。为了获得对这种扰动的可证明的鲁棒性，最近的研究考虑了基于Lipschitz的正则化器或约束，同时增加了预测边界。不幸的是，这样做会显著降低准确性。在本文中，我们提出了一个校准的Lipschitz边界误差（CLL）来解决这个问题，并通过解决两个问题来提高认证鲁棒性：首先，常用的边界误差不会根据收缩的输出分布调整惩罚，这是由于最小化Lipschitz常数K所造成的。其次，最重要的是，我们观察到最小化K可以导致决策函数过度平滑。这限制了模型的复杂性，从而降低了准确性。我们的CLL通过明确校准损失与边界和Lipschitz常数的关系来解决这些问题，从而确保模型具有较高的准确性。",
    "tldr": "本文提出了一种校准的Lipschitz边界误差（CLL）来提高认证鲁棒性，通过解决边界误差不会根据收缩的输出分布调整惩罚和最小化Lipschitz常数导致过度平滑的问题。"
}