{
    "title": "Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning. (arXiv:2309.06097v1 [cs.AI])",
    "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE method",
    "link": "http://arxiv.org/abs/2309.06097",
    "context": "Title: Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning. (arXiv:2309.06097v1 [cs.AI])\nAbstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE method",
    "path": "papers/23/09/2309.06097.json",
    "total_tokens": 934,
    "translated_title": "基于保真度诱导的可解释策略提取方法用于强化学习",
    "translated_abstract": "深度强化学习在顺序决策问题上取得了显著的成功。然而，现有的深度强化学习代理以不透明的方式进行决策，阻碍了用户建立信任和审视代理的弱点。虽然最近的研究开发了一些可解释策略提取方法来解释代理的行为，但它们的解释常常与代理的行为不一致，因此经常无法解释。为了解决这个问题，我们提出了一种新方法，即基于保真度诱导的策略提取（FIPE）。具体而言，我们首先分析了现有可解释策略提取方法的优化机制，阐述了在增加累积奖励时忽视一致性的问题。然后，我们将一个保真度量集成到强化学习反馈中，设计了一个保真度诱导机制。我们在星际争霸 II 的复杂控制环境下进行实验，这是当前可解释策略提取方法通常避免的领域。",
    "tldr": "本文提出了一种基于保真度诱导的策略提取方法（FIPE）来解决深度强化学习代理不透明的决策问题。实验证明，该方法在星际争霸 II 这样的复杂控制环境下具有良好的性能。",
    "en_tdlr": "This paper proposes a fidelity-induced policy extraction (FIPE) method to address the issue of opaque decision-making in deep reinforcement learning agents. The method incorporates fidelity measurement into reinforcement learning feedback and shows promising performance in complex control environments like StarCraft II."
}