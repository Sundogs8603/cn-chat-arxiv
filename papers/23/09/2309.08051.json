{
    "title": "Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v1 [cs.SD])",
    "abstract": "Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet ",
    "link": "http://arxiv.org/abs/2309.08051",
    "context": "Title: Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v1 [cs.SD])\nAbstract: Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet ",
    "path": "papers/23/09/2309.08051.json",
    "total_tokens": 947,
    "translated_title": "检索增强型文本到音频生成",
    "translated_abstract": "尽管在文本到音频(TTA)生成方面取得了一些进展，我们发现状态-艺术模型，如AudioLDM，在数据集上表现出类别分布不平衡（如AudioCaps）的训练中，其生成性能存在偏差。具体而言，它们在生成常见音频类别方面表现出色，而在罕见类别方面表现不佳，从而降低了整体生成性能。我们将此问题称为长尾文本到音频生成。为解决这个问题，我们提出了一种简单的检索增强方法来进行TTA模型。具体而言，给定一个文本输入提示，我们首先使用对比语音语言预训练（CLAP）模型来检索相关的文本-音频对。然后使用检索到的音频-文本数据的特征作为额外条件来指导TTA模型的学习。我们使用我们提出的方法增强了AudioLDM，并将所得到的增强系统称为Re-AudioLDM。在AudioCaps数据集上，Re-AudioLDM实现了最先进的Frechet得分。",
    "tldr": "这篇论文提出了一种检索增强的文本到音频生成方法，用于解决长尾文本到音频生成的问题。通过利用检索到的相关文本-音频数据作为额外条件，从而增强了模型的学习能力，在AudioCaps数据集上取得了最先进的结果。",
    "en_tdlr": "This paper proposes a retrieval-augmented approach for text-to-audio generation to address the problem of long-tailed generation. By leveraging relevant text-audio data retrieved through a contrastive language audio pretraining model, the proposed method enhances the learning capability of the models and achieves state-of-the-art results on the AudioCaps dataset."
}