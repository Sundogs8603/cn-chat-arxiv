{
    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])",
    "abstract": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",
    "link": "http://arxiv.org/abs/2309.00615",
    "context": "Title: Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])\nAbstract: We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",
    "path": "papers/23/09/2309.00615.json",
    "total_tokens": 1010,
    "translated_title": "Point-Bind和Point-LLM：用于3D理解、生成和指导跟随的多模态点云对齐",
    "translated_abstract": "我们引入了Point-Bind，一个将点云与2D图像、语言、音频和视频对齐的3D多模态模型。在ImageBind的指导下，我们构建了一个将3D和多模态嵌入空间进行结合的模型，实现了许多有前景的应用，例如任意到3D生成、3D嵌入算术和3D开放世界的理解。在此基础上，我们进一步提出了Point-LLM，第一个遵循3D多模态指令的大型语言模型（LLM）。通过参数高效调优技术，Point-LLM将Point-Bind的语义注入到预训练的LLMs中，例如LLaMA，不需要3D指令数据但展现出卓越的3D和多模态问答能力。我们希望我们的工作能为将3D点云扩展到多模态应用的研究社区提供启示。代码可在https://github.com/ZiyuGuo99/Point-Bind_Point-LLM找到。",
    "tldr": "Point-Bind和Point-LLM是用于3D理解、生成和指导跟随的多模态点云对齐模型，能实现任意到3D生成、3D嵌入算术和3D开放世界的理解，并且Point-LLM能实现3D和多模态问答功能。",
    "en_tdlr": "Point-Bind and Point-LLM are multi-modality models that align point clouds with 2D image, language, audio, and video for 3D understanding, generation, and instruction following. They can achieve tasks such as any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. Additionally, Point-LLM demonstrates superior 3D and multi-modal question-answering capacity without the need for 3D instruction data."
}