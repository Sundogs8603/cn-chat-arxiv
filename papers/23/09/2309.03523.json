{
    "title": "DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks. (arXiv:2309.03523v1 [cs.DC])",
    "abstract": "Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning dynamic graphs by exploiting both spatial and temporal features. Although DGNN has recently received considerable attention by AI community and various DGNN models have been proposed, building a distributed system for efficient DGNN training is still challenging. It has been well recognized that how to partition the dynamic graph and assign workloads to multiple GPUs plays a critical role in training acceleration. Existing works partition a dynamic graph into snapshots or temporal sequences, which only work well when the graph has uniform spatio-temporal structures. However, dynamic graphs in practice are not uniformly structured, with some snapshots being very dense while others are sparse. To address this issue, we propose DGC, a distributed DGNN training system that achieves a 1.25x - 7.52x speedup over the state-of-the-art in our testbed. DGC's success stems from a new graph partitioning method that parti",
    "link": "http://arxiv.org/abs/2309.03523",
    "context": "Title: DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks. (arXiv:2309.03523v1 [cs.DC])\nAbstract: Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning dynamic graphs by exploiting both spatial and temporal features. Although DGNN has recently received considerable attention by AI community and various DGNN models have been proposed, building a distributed system for efficient DGNN training is still challenging. It has been well recognized that how to partition the dynamic graph and assign workloads to multiple GPUs plays a critical role in training acceleration. Existing works partition a dynamic graph into snapshots or temporal sequences, which only work well when the graph has uniform spatio-temporal structures. However, dynamic graphs in practice are not uniformly structured, with some snapshots being very dense while others are sparse. To address this issue, we propose DGC, a distributed DGNN training system that achieves a 1.25x - 7.52x speedup over the state-of-the-art in our testbed. DGC's success stems from a new graph partitioning method that parti",
    "path": "papers/23/09/2309.03523.json",
    "total_tokens": 965,
    "translated_title": "DGC: 使用分块图分区训练具有时空非均匀性的动态图",
    "translated_abstract": "动态图神经网络(DGNN)通过利用时空特征展示了学习动态图的强大能力。尽管DGNN最近受到人工智能社区的广泛关注，并且提出了各种DGNN模型，但构建高效的DGNN训练分布式系统仍然具有挑战性。已经公认的是，如何将动态图划分并将工作量分配给多个GPU在训练加速中起到了关键作用。现有工作将动态图划分为快照或时间序列，但这仅在图具有均匀时空结构时起作用。然而，实际中的动态图不具有均匀结构，其中一些快照非常稠密，而其他快照非常稀疏。为了解决这个问题，我们提出了DGC，一种分布式DGNN训练系统，在我们的测试平台上实现了比最先进方法快1.25倍-7.52倍的加速。DGC的成功源于一种新的图分区方法，它能够根据图的时空非均匀性进行分块。",
    "tldr": "本论文提出了DGC系统，利用一种新的图分区方法实现了动态图神经网络(DGNN)的高效训练，解决了实际动态图非均匀结构的问题，实现了1.25倍-7.52倍的训练加速。",
    "en_tdlr": "This paper proposes the DGC system, which achieves efficient training of dynamic graph neural networks (DGNN) by utilizing a novel graph partitioning method, addressing the issue of non-uniform structure in practical dynamic graphs, and achieving a 1.25x - 7.52x training acceleration."
}