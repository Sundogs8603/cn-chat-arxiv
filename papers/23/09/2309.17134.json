{
    "title": "Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation. (arXiv:2309.17134v1 [cs.CL])",
    "abstract": "Despite substantial progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer mainly focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention thus far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. ",
    "link": "http://arxiv.org/abs/2309.17134",
    "context": "Title: Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation. (arXiv:2309.17134v1 [cs.CL])\nAbstract: Despite substantial progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer mainly focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention thus far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. ",
    "path": "papers/23/09/2309.17134.json",
    "total_tokens": 898,
    "translated_title": "通过自我知识蒸馏在资源稀缺情况下促进广义跨语言问答",
    "translated_abstract": "尽管多语言抽取式问答（QA）取得了显著进展，但在语言上具有高水平且一致分布的模型仍然具有挑战性，尤其对于资源有限的语言。我们主要研究广义跨语言迁移（G-XLT）任务，该任务关注问题语言与上下文语言不同的情况，迄今为止这个挑战受到了有限的关注。我们的方法旨在通过在大规模数据集上训练高性能多语言模型，并结合跨语言采样和先进的自我蒸馏训练方法，在几种语言间进行交叉学习，从而提高跨语言QA的迁移能力。值得注意的是，我们引入了新颖的mAP@k系数来微调自我知识蒸馏损失，动态调节教师模型的知识，实现平衡有效的知识传递。",
    "tldr": "通过自我知识蒸馏和跨语言采样，我们提出一种方法来提高在资源稀缺情况下的广义跨语言问答迁移能力，并引入mAP@k系数来实现平衡有效的知识传递。"
}