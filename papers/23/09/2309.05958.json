{
    "title": "The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])",
    "abstract": "As large language models (LLMs) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. This study utilized the Moral Machine framework to investigate the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their responses to human preferences. While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations. Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans. These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving.",
    "link": "http://arxiv.org/abs/2309.05958",
    "context": "Title: The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])\nAbstract: As large language models (LLMs) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. This study utilized the Moral Machine framework to investigate the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their responses to human preferences. While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations. Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans. These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving.",
    "path": "papers/23/09/2309.05958.json",
    "total_tokens": 988,
    "translated_title": "大型语言模型上的道德机器实验",
    "translated_abstract": "随着大型语言模型（LLMs）在各个领域的深入整合，理解它们如何做出道德判断变得至关重要，尤其是在自动驾驶领域。本研究利用道德机器框架，调查了知名的LLMs（包括GPT-3.5，GPT-4，PaLM 2和Llama 2）的道德决策倾向，并将其与人类偏好进行比较。虽然LLMs和人类的偏好（例如将人类放在宠物之上和更倾向于挽救更多生命）在很大程度上是一致的，但特别是PaLM 2和Llama 2显示出明显的偏差。此外，尽管LLM和人类偏好之间存在定性上的相似性，但在数量上存在明显的差异，这表明与人类相比，LLMs可能更趋向于做出更坚决的决策。这些发现阐明了LLMs的道德框架及其对自动驾驶的潜在影响。",
    "tldr": "本研究通过利用道德机器框架，调查了大型语言模型在道德决策上的倾向性，发现虽然它们与人类的偏好存在定性上的相似性，但在数量上存在明显的差异，表明大型语言模型更倾向于做出更坚决的决策。这些发现对于自动驾驶具有重要的伦理和潜在影响。",
    "en_tdlr": "This study investigated the ethical decision-making tendencies of large language models (LLMs) using the Moral Machine framework, revealing that while LLMs align qualitatively with human preferences, they tend to lean towards more uncompromising decisions quantitatively. These findings have important ethical implications for autonomous driving."
}