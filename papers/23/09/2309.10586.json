{
    "title": "Adversarial Attacks Against Uncertainty Quantification. (arXiv:2309.10586v1 [cs.CV])",
    "abstract": "Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, we focus on a different adversarial scenario in which the attacker is still interested in manipulating the uncertainty estimate, but regardless of the correctness of the prediction; in particular, the goal is to undermine the use of machine-learning models when their outputs are consumed by a downstream module or by a human operator. Following such direction, we: \\textit{(i)} design a threat model for attacks targeting uncertainty quantification; \\textit{(ii)} devise different attack strategies ",
    "link": "http://arxiv.org/abs/2309.10586",
    "context": "Title: Adversarial Attacks Against Uncertainty Quantification. (arXiv:2309.10586v1 [cs.CV])\nAbstract: Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, we focus on a different adversarial scenario in which the attacker is still interested in manipulating the uncertainty estimate, but regardless of the correctness of the prediction; in particular, the goal is to undermine the use of machine-learning models when their outputs are consumed by a downstream module or by a human operator. Following such direction, we: \\textit{(i)} design a threat model for attacks targeting uncertainty quantification; \\textit{(ii)} devise different attack strategies ",
    "path": "papers/23/09/2309.10586.json",
    "total_tokens": 915,
    "translated_title": "对不确定性量化的对抗性攻击",
    "translated_abstract": "机器学习模型可以被对抗性示例所欺骗，即通过精心设计的输入扰动来迫使模型输出错误的预测。尽管最近提出了使用不确定性量化来检测对抗性输入的方法，但在假设这些攻击显示比原始数据具有更高的预测不确定性的情况下，已经表明针对减少不确定度估计的自适应攻击可以轻易绕过这种防御机制。在这项工作中，我们关注的是另一种对抗性场景，其中攻击者仍然有兴趣操纵不确定度估计，但不考虑预测的正确性；具体而言，目标是当机器学习模型的输出被下游模块或人操作员使用时削弱其效果。在这个方向上，我们：\\textit{(i)}为针对不确定性量化的攻击设计了一个威胁模型；\\textit{(ii)}设计了不同的攻击策略",
    "tldr": "本研究着眼于对不确定性量化的对抗性攻击，提出了一种新的攻击场景，即攻击者通过操纵不确定度估计来削弱机器学习模型的输出效果，无论预测的正确性如何。本文设计了威胁模型并提出了多种攻击策略。",
    "en_tdlr": "This paper focuses on adversarial attacks against uncertainty quantification and introduces a new attack scenario where the attacker manipulates the uncertainty estimate to undermine the output of machine learning models, regardless of the correctness of the prediction. The study proposes a threat model and devises various attack strategies."
}