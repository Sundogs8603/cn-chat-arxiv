{
    "title": "Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca",
    "abstract": "Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.",
    "link": "https://arxiv.org/abs/2309.08958",
    "context": "Title: Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca\nAbstract: Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.",
    "path": "papers/23/09/2309.08958.json",
    "total_tokens": 837,
    "translated_title": "单语或多语指导调整：哪种方式更适合alpaca？",
    "translated_abstract": "基础的大型语言模型（LLM）可以通过指导调整来执行开放域的问答任务，从而实现聊天助手等应用。虽然这类努力通常只在单一语言中进行，但我们实证分析了多语言场景下的成本效益策略。我们使用Alpaca数据集和其中的机器翻译数据形成多语言调整的训练集，然后采用低秩调整或完全参数训练的方式对LLM进行调整。在受控算力预算下的比较结果表明，多语言调整可以达到或超越每种语言单独调整的效果。此外，通过下采样的数据进行多语言调整可以达到相同甚至更强的效果。我们的研究结果为通过指导调整来扩展语言支持提供了指导。",
    "tldr": "通过实证分析比较了单语和多语指导调整的成本效益，发现在多语言场景下，多语指导调整可以达到或超越单独调整每种语言的效果，并且采用下采样的数据进行多语调整可以提供更强的效果和更好的鲁棒性。",
    "en_tdlr": "Through empirical analysis, we compared the cost-effectiveness of monolingual and multilingual instruction tuning and found that in multilingual scenarios, multilingual instruction tuning can achieve or surpass the performance of tuning a model for each language separately. Additionally, multilingual tuning with downsampled data can provide stronger performance and better robustness."
}