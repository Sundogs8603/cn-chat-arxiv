{
    "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants. (arXiv:2309.10020v1 [cs.CV])",
    "abstract": "This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in compute",
    "link": "http://arxiv.org/abs/2309.10020",
    "context": "Title: Multimodal Foundation Models: From Specialists to General-Purpose Assistants. (arXiv:2309.10020v1 [cs.CV])\nAbstract: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in compute",
    "path": "papers/23/09/2309.10020.json",
    "total_tokens": 925,
    "translated_title": "多模态基础模型：从专业模型到通用助手",
    "translated_abstract": "本文介绍了一项对多模态基础模型的分类和演化的全面调查研究，重点关注从专业模型到通用助手的过渡。研究范围包括五个核心主题，分为两类。第一类是已经建立起来的研究领域：为特定目的而预训练的多模态基础模型，包括两个主题：学习视觉骨干用于视觉理解和文本到图像生成的方法。第二类是最近在探索性、开放的研究领域中取得的进展：旨在扮演通用助手角色的多模态基础模型，包括三个主题：受大型语言模型启发的统一视觉模型、多模态语言模型的端到端训练以及与语言模型进行链式多模态工具的开发。本文的目标读者是研究人员、研究生和计算专业人士。",
    "tldr": "这项研究调查了多模态基础模型的分类和演化情况，并重点关注了从专业模型到通用助手的过渡。它涵盖了五个核心主题，包括预训练模型和通用助手模型的学习方法以及整合语言模型的统一视觉模型的最新进展。",
    "en_tdlr": "This research provides a comprehensive survey of multimodal foundation models, focusing on the transition from specialist models to general-purpose assistants. It covers five core topics, including the learning methods of pre-trained models and the recent advances in unified vision models that integrate language models."
}