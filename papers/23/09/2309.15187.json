{
    "title": "Monitoring Machine Learning Models: Online Detection of Relevant Deviations. (arXiv:2309.15187v1 [cs.LG])",
    "abstract": "Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark m",
    "link": "http://arxiv.org/abs/2309.15187",
    "context": "Title: Monitoring Machine Learning Models: Online Detection of Relevant Deviations. (arXiv:2309.15187v1 [cs.LG])\nAbstract: Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark m",
    "path": "papers/23/09/2309.15187.json",
    "total_tokens": 867,
    "translated_title": "监测机器学习模型：在线检测相关偏差",
    "translated_abstract": "机器学习模型是各个领域中重要的工具，但其性能可能会随时间的推移而降低，原因是数据分布的变化或其他因素。一方面，检测和解决这种降级对于保持模型的可靠性至关重要。另一方面，给定足够的数据，可以检测到任意小的质量变化。由于模型重新训练或替换等干预措施可能代价高昂，我们认为仅当变化超过给定阈值时才应该进行这些干预措施。我们提出了一种顺序监测方案来检测这些相关变化。所提出的方法通过考虑所测量模型质量的时间依赖性来减少不必要的警报并克服多重测试问题。文中提供了一致性和指定渐近水平的条件。使用模拟和真实数据进行的实证验证证明了我们的方法在检测模型质量相关变化方面的优越性，相比基准方法",
    "tldr": "本论文提出了一种用于监测机器学习模型的在线检测方案，通过考虑模型质量的时间依赖性，可以减少不必要的警报并优化对相关变化的检测。"
}