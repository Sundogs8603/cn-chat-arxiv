{
    "title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning. (arXiv:2309.07108v1 [cs.LG])",
    "abstract": "Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmar",
    "link": "http://arxiv.org/abs/2309.07108",
    "context": "Title: Characterizing Speed Performance of Multi-Agent Reinforcement Learning. (arXiv:2309.07108v1 [cs.LG])\nAbstract: Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmar",
    "path": "papers/23/09/2309.07108.json",
    "total_tokens": 953,
    "translated_title": "多智能体强化学习的速度性能特征化",
    "translated_abstract": "多智能体强化学习（MARL）在大规模人工智能系统和大数据应用（如智能电网、监控等）取得了显著成功。现有的MARL算法的进展主要集中在通过引入各种机制来改进智能体之间的合作以提高奖励。然而，这些优化通常会在计算和内存方面产生较大负担，从而导致端到端训练时间的速度性能不佳。在本研究中，我们分析了速度性能（即延迟受限吞吐量）作为MARL实现的关键指标。具体而言，我们首先从加速的角度引入了一个MARL算法的分类法，包括（1）训练方案和（2）通信方法。利用我们的分类法，我们确定了三种最先进的MARL算法—多智能体深度确定性策略梯度（MADDPG）、面向目标的多智能体通信与合作（ToM2C）和网络多智能体RL（NeurComm）—作为目标基准。",
    "tldr": "本文对多智能体强化学习（MARL）的速度性能进行了特征化研究。作者通过引入分类法，提出了三种最先进的MARL算法作为目标基准，以解决目前优化奖励的算法在训练时间速度性能方面的不足。"
}