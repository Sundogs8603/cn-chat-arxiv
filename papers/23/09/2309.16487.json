{
    "title": "Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])",
    "abstract": "Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much ",
    "link": "http://arxiv.org/abs/2309.16487",
    "context": "Title: Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])\nAbstract: Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much ",
    "path": "papers/23/09/2309.16487.json",
    "total_tokens": 871,
    "translated_title": "对中毒公平表示的研究",
    "translated_abstract": "公平机器学习旨在减少对某些人口子群体（如老年人和女性）的模型预测偏见。最近，通过深度神经网络训练的公平表示学习（FRL）表现出优越的性能，从数据中推断出不包含人口统计信息的表示，然后将其用作分类或其他下游任务的输入。尽管FRL方法得到了发展，但它们在面对数据中毒攻击时的脆弱性，即在对抗性场景下评估模型的健壮性的流行协议中，尚未得到充分研究。数据中毒攻击已经针对将公平性约束纳入浅层模型分类器的经典公平机器学习方法进行了开发。然而，由于公平目标和模型架构有明显的差异，这些攻击在FRL中不够有效。本文提出了第一个针对FRL的数据中毒框架。我们诱使模型输出包含尽可能多公平表示的不公平表示。",
    "tldr": "这项研究提出了第一个针对公平表示学习（FRL）的数据中毒框架，该框架在对抗性场景下评估模型的健壮性，解决了FRL方法在面对数据中毒攻击时的脆弱性。"
}