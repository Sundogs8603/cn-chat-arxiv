{
    "title": "Improving Machine Learning Robustness via Adversarial Training. (arXiv:2309.12593v1 [cs.LG])",
    "abstract": "As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-I",
    "link": "http://arxiv.org/abs/2309.12593",
    "context": "Title: Improving Machine Learning Robustness via Adversarial Training. (arXiv:2309.12593v1 [cs.LG])\nAbstract: As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-I",
    "path": "papers/23/09/2309.12593.json",
    "total_tokens": 897,
    "translated_title": "通过对抗训练提高机器学习的鲁棒性",
    "translated_abstract": "随着机器学习在各种实际应用中的使用越来越广泛，确保机器学习算法对任何潜在的最坏情况噪声、对抗攻击和非常不寻常的情况都具有鲁棒性至关重要。研究机器学习的鲁棒性将在设计机器学习算法方面起到重要作用。本文通过对抗训练在集中式和分布式环境中研究了机器学习的鲁棒性，其中机器学习的训练和测试在一个或多个计算机上进行。在集中式环境中，我们分别使用Fast Gradient Sign Method和DeepFool生成的对抗样本进行分类，得到了65.41%和83.0%的测试准确率。与现有研究相比，这些结果分别提高了18.41%和47%。在分布式环境中，我们通过使用对抗训练研究了联邦学习的鲁棒性，其中采用了独立同分布（IID）和非IID的方法。",
    "tldr": "本文通过对抗训练在集中式和分布式环境中研究了机器学习的鲁棒性，取得了较现有研究更好的效果。",
    "en_tdlr": "This paper investigates the robustness of machine learning through adversarial training in centralized and decentralized environments, achieving better results compared to existing studies."
}