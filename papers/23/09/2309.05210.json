{
    "title": "Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) are rapidly increasing in size, with the number of parameters becoming a key factor in the success of many commercial models, such as ChatGPT, Claude, and Bard. Even the recently released publicly accessible models for commercial usage, such as Falcon and Llama2, come equipped with billions of parameters. This significant increase in the number of parameters makes deployment and operation very costly. The remarkable progress in the field of quantization for large neural networks in general and LLMs in particular, has made these models more accessible by enabling them to be deployed on consumer-grade GPUs. Quantized models generally demonstrate comparable performance levels to their unquantized base counterparts. Nonetheless, there exists a notable gap in our comprehensive understanding of how these quantized models respond to hyperparameters, such as temperature, max new tokens, and topk, particularly for next word prediction. The present analysis reveals t",
    "link": "http://arxiv.org/abs/2309.05210",
    "context": "Title: Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) are rapidly increasing in size, with the number of parameters becoming a key factor in the success of many commercial models, such as ChatGPT, Claude, and Bard. Even the recently released publicly accessible models for commercial usage, such as Falcon and Llama2, come equipped with billions of parameters. This significant increase in the number of parameters makes deployment and operation very costly. The remarkable progress in the field of quantization for large neural networks in general and LLMs in particular, has made these models more accessible by enabling them to be deployed on consumer-grade GPUs. Quantized models generally demonstrate comparable performance levels to their unquantized base counterparts. Nonetheless, there exists a notable gap in our comprehensive understanding of how these quantized models respond to hyperparameters, such as temperature, max new tokens, and topk, particularly for next word prediction. The present analysis reveals t",
    "path": "papers/23/09/2309.05210.json",
    "total_tokens": 849,
    "translated_title": "理解后训练量化对大型语言模型的影响",
    "translated_abstract": "大型语言模型（LLMs）的规模迅速增加，参数数量成为许多商业模型成功的关键因素，如ChatGPT、Claude和Bard。即使是最近发布的用于商业用途的公开可见模型，如Falcon和Llama2，也拥有数十亿个参数。参数数量的显著增加使得部署和运行非常昂贵。量化领域在大型神经网络以及LLMs方面取得了显著进展，使得这些模型可以在消费级GPU上部署，从而使其更易获得。量化模型通常表现出与其未量化基准模型相当的性能水平。然而，对于诸如温度、最大新标记数和topk等超参数，尤其是对于下一个单词预测，我们对这些量化模型如何响应仍存在显著差距。本研究揭示了这一问题。",
    "tldr": "本研究旨在理解后训练量化对大型语言模型的影响，揭示了量化模型在下一个单词预测等关键任务中如何响应超参数的差距。"
}