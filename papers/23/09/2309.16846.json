{
    "title": "Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])",
    "abstract": "Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the \"parameters\" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid",
    "link": "http://arxiv.org/abs/2309.16846",
    "context": "Title: Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])\nAbstract: Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the \"parameters\" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid",
    "path": "papers/23/09/2309.16846.json",
    "total_tokens": 805,
    "translated_title": "最优非线性性能改进随机特征的泛化性能",
    "translated_abstract": "通过非线性激活函数的随机特征模型在训练和泛化误差方面已被证明与高斯模型渐进等效。等效模型的分析揭示了激活函数发挥的重要但尚未完全理解的作用。为了解决这个问题，我们研究等效模型的“参数”，以实现对给定监督学习问题的改进的泛化性能。我们展示了从高斯模型获取的参数使我们能够定义一组最优非线性性。我们提供了这组最优非线性性的两个示例类，例如二阶多项式和分段线性函数。这些函数被优化以改进泛化性能，无论其实际形式如何。我们对回归和分类问题进行了实验，包括合成和真实数据（如CIFAR10）。我们的数值结果验证了优化的非线性性能优于wid。",
    "tldr": "通过研究等效模型的参数，本研究发现获得的参数可以定义一组最优非线性性，如二阶多项式和分段线性函数。这些非线性性能优化了泛化性能，无论其实际形式如何，对回归和分类问题均有效。"
}