{
    "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])",
    "abstract": "The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and",
    "link": "http://arxiv.org/abs/2309.11197",
    "context": "Title: The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])\nAbstract: The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and",
    "path": "papers/23/09/2309.11197.json",
    "total_tokens": 900,
    "translated_title": "Languini Kitchen: 在不同计算规模上实现语言模型研究",
    "translated_abstract": "Languini Kitchen既是一个研究集体，又是一个代码库，旨在赋予计算资源有限的研究人员对语言模型领域做出有意义贡献的能力。我们引入了一个实验协议，使得可以基于等效计算（以加速器小时计量）来进行模型比较。模型训练的令牌数量由模型的吞吐量和选择的计算类别来定义。值得注意的是，这种方法避免了对影响总参数或浮点操作的关键超参数的限制。为了评估，我们预处理了一个现有的大规模、多样化且高质量的图书数据集，该数据集在质量、多样性和文档长度方面超过了现有的学术基准。在此基础上，我们通过不同计算水平上的实验来估计方法的经验性扩展趋势。这项工作还提供了两个基准模型：从GPT-2架构推导出的前馈模型及...",
    "tldr": "Languini Kitchen是一个研究集体和代码库，旨在通过等效计算来进行语言模型比较，并提供新的大规模、多样化且高质量的数据集。要点：实验协议、模型比较、等效计算、大规模数据集。",
    "en_tdlr": "The Languini Kitchen is a research collective and codebase that enables language model comparisons through equivalent computing and provides a new large-scale, diverse, and high-quality dataset. Key points: experimental protocol, model comparisons, equivalent computing, large-scale dataset."
}