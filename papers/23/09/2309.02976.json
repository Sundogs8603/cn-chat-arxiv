{
    "title": "Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])",
    "abstract": "Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive ",
    "link": "http://arxiv.org/abs/2309.02976",
    "context": "Title: Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])\nAbstract: Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive ",
    "path": "papers/23/09/2309.02976.json",
    "total_tokens": 909,
    "translated_title": "使用强化学习在高维肌肉骨骼模型中实现自然且稳健的行走，无需演示",
    "translated_abstract": "人类在复杂的自然环境中以稳健的双足行走表现出色。在每一步中，他们充分调整生物力学肌肉动力学和神经信号的相互作用，以在地面条件的不确定性下保持稳健。然而，我们还没有完全理解神经系统如何解决肌肉骨骼冗余问题，以解决考虑稳定性、稳健性和能量效率的多目标控制问题。在计算机模拟中，能量最小化已被证明是一种成功的优化目标，在轨迹优化或基于反射的控制方法中重现了自然行走。然而，这些方法一次只关注特定的运动，并且在补偿干扰时，所产生的控制器受到限制。在机器人领域，最近的强化学习（RL）方法在四足系统上实现了高度稳定（和高效）的运动，但要使用双足生物力学模型生成类似人类行走的行走需要大量的工作。",
    "tldr": "本论文通过使用强化学习来解决肌肉骨骼冗余问题，并实现了在高维度肌肉骨骼模型中自然且稳健的行走。"
}