{
    "title": "Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems. (arXiv:2309.05787v1 [cs.AI])",
    "abstract": "Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advan",
    "link": "http://arxiv.org/abs/2309.05787",
    "context": "Title: Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems. (arXiv:2309.05787v1 [cs.AI])\nAbstract: Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advan",
    "path": "papers/23/09/2309.05787.json",
    "total_tokens": 835,
    "translated_title": "自适应用户中心的神经符号学习在多模态交互中的应用于自主系统",
    "translated_abstract": "最近在机器学习，特别是深度学习方面取得的进展使得自主系统能够以感知的非符号化方式识别和理解对象及其环境。这些系统现在可以执行物体检测、传感器数据融合和语言理解任务。然而，要提升这些系统对对象及其环境的概念和符号理解能力，需要综合考虑人类提供的显性教导（例如描述情况或解释如何行动）和通过观察人类行为（通过系统的传感器）获得的隐性教导。因此，系统必须设计具有多模态输入和输出能力，以支持隐性和显性交互模型。在这篇文章中，我们主张同时考虑这两种类型的输入，以及人在循环中的参与和增量学习技术，以推进这一强大的人工智能水平的实现。",
    "tldr": "这篇论文提出了一种自适应用户中心的神经符号学习方法，用于支持多模态交互中自主系统对物体和环境的符号化理解能力的提升。",
    "en_tdlr": "This paper proposes an adaptive user-centered neuro-symbolic learning approach to enhance the symbolic understanding of objects and their environments in autonomous systems for multimodal interaction."
}