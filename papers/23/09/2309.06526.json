{
    "title": "Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers. (arXiv:2309.06526v1 [cs.LG])",
    "abstract": "For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.",
    "link": "http://arxiv.org/abs/2309.06526",
    "context": "Title: Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers. (arXiv:2309.06526v1 [cs.LG])\nAbstract: For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.",
    "path": "papers/23/09/2309.06526.json",
    "total_tokens": 825,
    "translated_title": "探索差分隐私预训练和参数高效微调在表格转换器中的受益",
    "translated_abstract": "对于使用表格数据的机器学习，Table Transformer是一种最先进的神经网络模型，而差分隐私是确保数据隐私的重要组成部分。在本文中，我们探索了将这两个方面结合起来应用于迁移学习场景中的好处——差分隐私预训练和对Table Transformer进行多种参数高效微调方法（包括Adapter、LoRA和Prompt Tuning）。我们在ACSIncome数据集上进行了大量实验证明，这些参数高效微调方法在下游任务准确度和可训练参数数量方面优于传统方法，从而在参数效率、隐私和准确度之间实现了改进的权衡。我们的代码可在github.com/IBM/DP-TabTransformer中找到。",
    "tldr": "本文探索了对表格转换器进行差分隐私预训练和参数高效微调的好处，并展示了这些方法在准确性和可训练参数数量方面优于传统方法，从而在参数效率、隐私和准确度之间实现了改进的权衡。",
    "en_tdlr": "This paper explores the benefits of differentially private pre-training and parameter-efficient fine-tuning for Table Transformers and demonstrates that these methods outperform traditional approaches in terms of accuracy and number of trainable parameters, achieving an improved trade-off among parameter efficiency, privacy, and accuracy."
}