{
    "title": "Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])",
    "abstract": "In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi",
    "link": "http://arxiv.org/abs/2309.11475",
    "context": "Title: Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])\nAbstract: In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi",
    "path": "papers/23/09/2309.11475.json",
    "total_tokens": 852,
    "translated_title": "避免在根查找和优化中出现不需要的点的方法：通过增加极点",
    "translated_abstract": "在根查找和优化中，存在许多情况下，我们可能无法保证自己选择的方法构造的序列收敛于一个闭集合A（在这里，我们并不假设A有其他附加属性，如凸性或连通性）。在这种情况下，我们希望有一个机制来避免在算法的下一次运行中再次遇到这个点z*。在本文中，我们提出了一种新的方法来实现这一目标：我们将代价函数除以到A的距离函数的适当幂。这个想法受到了在一维函数中尝试找到所有根的启发。我们首先解释了在代价函数的最小值恰好为0的情况下这种方法的启发式方法，然后解释了如果最小值不为零该如何进行（同时允许正的最小值）。",
    "tldr": "通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。",
    "en_tdlr": "A method to avoid unwanted points in root finding and optimization is proposed by adding poles, where the cost function is divided by an appropriate power of the distance function to the target point."
}