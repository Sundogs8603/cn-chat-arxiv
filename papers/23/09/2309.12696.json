{
    "title": "Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning. (arXiv:2309.12696v1 [cs.AI])",
    "abstract": "Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. Tomitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the",
    "link": "http://arxiv.org/abs/2309.12696",
    "context": "Title: Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning. (arXiv:2309.12696v1 [cs.AI])\nAbstract: Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. Tomitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the",
    "path": "papers/23/09/2309.12696.json",
    "total_tokens": 824,
    "translated_title": "离线多智能体增强学习的反事实保守Q学习",
    "translated_abstract": "离线多智能体增强学习面临着离线环境中的分布偏移问题和多智能体环境中的高维问题的挑战，导致行为的超出分布和价值的过高估计现象过于严重。为了缓解这个问题，我们提出了一种新的多智能体离线RL算法，命名为反事实保守Q学习（CFCQL），以进行保守的价值估计。CFCQL并不将所有智能体视为一个高维单独的智能体，并直接应用单智能体方法，而是以反事实的方式分别为每个智能体计算保守的正则化，并线性组合它们以实现整体保守价值估计。我们证明了它仍然具有单一智能体保守方法的低估性质和性能保证，但所引起的正则化和安全策略改进界限都是独立的。",
    "tldr": "提出了一种新的多智能体离线RL算法，通过反事实保守Q学习实现保守的价值估计，在解决离线分布偏移和高维问题的同时，确保行为的分布和价值的估计不过高。"
}