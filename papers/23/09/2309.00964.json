{
    "title": "eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)",
    "abstract": "Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM ",
    "link": "http://arxiv.org/abs/2309.00964",
    "context": "Title: eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)\nAbstract: Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM ",
    "path": "papers/23/09/2309.00964.json",
    "total_tokens": 957,
    "translated_title": "eDKM:一种用于大型语言模型的高效准确的训练时权重聚类方法",
    "translated_abstract": "由于大型语言模型（LLMs）在许多复杂语言任务上表现出高质量性能，因此将这些LLMs引入移动设备以实现更快的响应和更好的隐私保护引起了极大兴趣。然而，LLMs的规模（数十亿个参数）需要高效的压缩才能适应存储有限的设备。在众多压缩技术中，权重聚类是LLM压缩的领先候选方法之一，并得到了现代智能手机的支持。然而，其训练开销对LLM的微调来说是难以承受的。特别是，可微分K均值聚类（DKM）已经显示出在压缩比和准确性回归之间的最先进折衷方案，但其较大的内存复杂性使其几乎不可能应用于训练时的LLM压缩。因此，在本文中，我们提出了一种内存高效的DKM实现，即eDKM，通过创新的技术减小了DKM的内存占用。",
    "tldr": "本文提出了一种内存高效的DKM实现，即eDKM，用于大型语言模型的高效准确的训练时权重聚类方法。它通过减小DKM的内存占用，解决了LLM压缩中的训练开销问题。",
    "en_tdlr": "This paper proposes a memory-efficient implementation of DKM, called eDKM, for efficient and accurate train-time weight clustering in large language models. It addresses the training overhead issue in LLM compression by reducing the memory footprint of DKM."
}