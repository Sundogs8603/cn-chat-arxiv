{
    "title": "Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])",
    "abstract": "Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s",
    "link": "http://arxiv.org/abs/2309.04344",
    "context": "Title: Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])\nAbstract: Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s",
    "path": "papers/23/09/2309.04344.json",
    "total_tokens": 936,
    "translated_title": "使用基础模型对零样本模型进行零样本强化",
    "translated_abstract": "零样本推断是一种强大的范式，可以在没有进一步训练的情况下使用预训练模型来进行下游分类任务。然而，这些模型容易受到继承的偏见的影响，从而影响它们的性能。传统的解决方案是微调，但这削弱了预训练模型的主要优势，即可以直接使用的能力。我们提出了RoboShot，一种完全零样本的方法，可以改善预训练模型嵌入的鲁棒性。首先，我们使用零样本语言模型（LMs）从任务描述中获取有用的见解。这些见解被嵌入并用于去除嵌入中的有害成分并增强有用成分--而无需任何监督。从理论上讲，我们提供了一个简单且可计算的模型，用于分析零样本嵌入中的偏见，并给出了在什么条件下我们的方法可以提高性能的结果。在实证上，我们在九个图像和NLP分类任务上评估了RoboShot。",
    "tldr": "提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。"
}