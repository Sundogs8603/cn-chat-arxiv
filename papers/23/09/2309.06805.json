{
    "title": "FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization. (arXiv:2309.06805v1 [cs.LG])",
    "abstract": "Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \\textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against",
    "link": "http://arxiv.org/abs/2309.06805",
    "context": "Title: FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization. (arXiv:2309.06805v1 [cs.LG])\nAbstract: Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \\textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against",
    "path": "papers/23/09/2309.06805.json",
    "total_tokens": 865,
    "translated_title": "FedDIP: 采用极端动态修剪和增量正则化的联邦学习",
    "translated_abstract": "联邦学习（FL）已成功应用于大规模深度神经网络（DNN）的分布式训练和推理。然而，DNN具有极大的参数数量，因此在分布式节点之间交换这些参数和管理内存方面面临着重大挑战。尽管最近的DNN压缩方法（例如稀疏化、修剪）解决了这些挑战，但它们并未全面考虑在保持高精度水平的同时自适应地控制参数交换的减少。因此，我们提出了一种新颖的FL框架（称为FedDIP），它结合了（i）动态模型修剪和误差反馈来消除冗余信息交换，从而显著提高性能，以及（ii）增量正则化，可以实现“极端”稀疏模型。我们提供了FedDIP的收敛性分析，并对其进行了全面的性能和比较评估。",
    "tldr": "FedDIP是一个结合了动态模型修剪和增量正则化的联邦学习框架，通过消除冗余信息交换和实现极端稀疏模型来显著提高性能。",
    "en_tdlr": "FedDIP is a federated learning framework that combines dynamic model pruning and incremental regularization to significantly improve performance by eliminating redundant information exchange and achieving extreme sparsity in models."
}