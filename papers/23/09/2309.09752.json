{
    "title": "Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)",
    "abstract": "In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv",
    "link": "http://arxiv.org/abs/2309.09752",
    "context": "Title: Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)\nAbstract: In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv",
    "path": "papers/23/09/2309.09752.json",
    "total_tokens": 903,
    "translated_title": "对比初始状态缓冲区在强化学习中的应用",
    "translated_abstract": "在强化学习中，勘探与利用之间的平衡给从有限的样本中实现高效学习带来了复杂的挑战。虽然最近的工作在利用过去的经验进行策略更新方面是有效的，但它们常常忽视了重新利用过去经验进行数据收集的潜力。独立于基本强化学习算法，我们引入了对比初始状态缓冲区的概念，它从过去的经验中选择状态，并用这些状态初始化环境中的代理，以引导它走向更具信息量的状态。我们在两个复杂的机器人任务上验证了我们的方法，而不依赖任何关于环境的先验信息：（i）四足机器人穿越具有挑战性的地形和（ii）四旋翼无人机在赛道上飞行。实验结果表明，我们的初始状态缓冲区在任务性能上优于基准线，同时还加速了训练过程。",
    "tldr": "本论文提出了对比初始状态缓冲区的概念，它通过选择过去的经验中的状态来初始化环境中的代理，以引导其进入更有信息量的状态。实验证明，该方法在两个复杂机器人任务上取得了更高的任务性能并加速了训练过程。"
}