{
    "title": "GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis. (arXiv:2309.16022v1 [cs.LG])",
    "abstract": "With the ever-growing popularity of Graph Neural Networks (GNNs), efficient GNN inference is gaining tremendous attention. Field-Programming Gate Arrays (FPGAs) are a promising execution platform due to their fine-grained parallelism, low-power consumption, reconfigurability, and concurrent execution. Even better, High-Level Synthesis (HLS) tools bridge the gap between the non-trivial FPGA development efforts and rapid emergence of new GNN models. In this paper, we propose GNNHLS, an open-source framework to comprehensively evaluate GNN inference acceleration on FPGAs via HLS, containing a software stack for data generation and baseline deployment, and FPGA implementations of 6 well-tuned GNN HLS kernels. We evaluate GNNHLS on 4 graph datasets with distinct topologies and scales. The results show that GNNHLS achieves up to 50.8x speedup and 423x energy reduction relative to the CPU baselines. Compared with the GPU baselines, GNNHLS achieves up to 5.16x speedup and 74.5x energy reductio",
    "link": "http://arxiv.org/abs/2309.16022",
    "context": "Title: GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis. (arXiv:2309.16022v1 [cs.LG])\nAbstract: With the ever-growing popularity of Graph Neural Networks (GNNs), efficient GNN inference is gaining tremendous attention. Field-Programming Gate Arrays (FPGAs) are a promising execution platform due to their fine-grained parallelism, low-power consumption, reconfigurability, and concurrent execution. Even better, High-Level Synthesis (HLS) tools bridge the gap between the non-trivial FPGA development efforts and rapid emergence of new GNN models. In this paper, we propose GNNHLS, an open-source framework to comprehensively evaluate GNN inference acceleration on FPGAs via HLS, containing a software stack for data generation and baseline deployment, and FPGA implementations of 6 well-tuned GNN HLS kernels. We evaluate GNNHLS on 4 graph datasets with distinct topologies and scales. The results show that GNNHLS achieves up to 50.8x speedup and 423x energy reduction relative to the CPU baselines. Compared with the GPU baselines, GNNHLS achieves up to 5.16x speedup and 74.5x energy reductio",
    "path": "papers/23/09/2309.16022.json",
    "total_tokens": 933,
    "translated_title": "GNNHLS: 通过高级综合评估图神经网络推断",
    "translated_abstract": "随着图神经网络（GNNs）的逐渐流行，高效的GNN推断引起了极大关注。由于其精细级并行性、低功耗、可重构性和并发执行的特点，Field-Programming Gate Arrays (FPGAs) 是一个有前途的执行平台。更好的是，高级综合（HLS）工具弥补了非常规的FPGA开发工作和新GNN模型的快速涌现之间的差距。在本文中，我们提出了GNNHLS，一个开源框架，通过HLS全面评估FPGAs上的GNN推断加速，包括用于数据生成和基准部署的软件栈以及6个经过良好调优的GNN HLS内核的FPGA实现。我们使用4个具有不同拓扑结构和规模的图数据集对GNNHLS进行评估。结果表明，与CPU基准相比，GNNHLS实现了高达50.8倍的加速和423倍的能量降低。与GPU基准相比，GNNHLS实现了高达5.16倍的加速和74.5倍的能量降低。",
    "tldr": "GNNHLS是一个通过高级综合评估FPGAs上的图神经网络推断的开源框架，能够实现高速加速和能量降低。",
    "en_tdlr": "GNNHLS is an open-source framework for evaluating graph neural network inference on FPGAs via high-level synthesis, achieving high-speed acceleration and energy reduction."
}