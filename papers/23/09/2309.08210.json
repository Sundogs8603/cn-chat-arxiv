{
    "title": "Investigating Answerability of LLMs for Long-Form Question Answering. (arXiv:2309.08210v1 [cs.CL])",
    "abstract": "As we embark on a new era of LLMs, it becomes increasingly crucial to understand their capabilities, limitations, and differences. Toward making further progress in this direction, we strive to build a deeper understanding of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective open-source LLMs and their distilled counterparts. To this end, we specifically focus on long-form question answering (LFQA) because it has several practical and impactful applications (e.g., troubleshooting, customer service, etc.) yet is still understudied and challenging for LLMs. We propose a question-generation method from abstractive summaries and show that generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our experimental results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup for LLMs and shows performance gaps between LLMs li",
    "link": "http://arxiv.org/abs/2309.08210",
    "context": "Title: Investigating Answerability of LLMs for Long-Form Question Answering. (arXiv:2309.08210v1 [cs.CL])\nAbstract: As we embark on a new era of LLMs, it becomes increasingly crucial to understand their capabilities, limitations, and differences. Toward making further progress in this direction, we strive to build a deeper understanding of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective open-source LLMs and their distilled counterparts. To this end, we specifically focus on long-form question answering (LFQA) because it has several practical and impactful applications (e.g., troubleshooting, customer service, etc.) yet is still understudied and challenging for LLMs. We propose a question-generation method from abstractive summaries and show that generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our experimental results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup for LLMs and shows performance gaps between LLMs li",
    "path": "papers/23/09/2309.08210.json",
    "total_tokens": 936,
    "translated_title": "探究LLMs在长篇问题回答方面的可回答性",
    "translated_abstract": "随着我们进入LLMs的新时代，理解它们的能力、限制和差异变得越来越重要。为了进一步在这个方向上取得进展，我们致力于更深入地了解大型LLMs（例如ChatGPT）与更小但有效的开源LLMs及其精简版本之间的差距。为此，我们特别关注长篇问题回答（LFQA），因为它在实践中有多个实际且有影响力的应用（例如故障排除、客户服务等），但对LLMs来说仍然是不够研究且具有挑战性的。我们提出了一种从抽象摘要生成问题的方法，并展示了从长文档摘要中生成后续问题可以为LLMs提供从长上下文进行推理和推断的挑战性环境。我们的实验结果证实：（1）我们提出的从抽象摘要生成问题的方法为LLMs提出了一个具有挑战性的设置，并显示了LLMs之间的性能差距",
    "tldr": "本研究主要探究LLMs在长篇问题回答方面的可回答性，通过提出从抽象摘要生成问题的方法，展示了从长文档摘要中生成后续问题对LLMs进行推理和推断的挑战，并确认了LLMs之间的性能差距。",
    "en_tdlr": "This study investigates the answerability of LLMs for long-form question answering and proposes a method of generating questions from abstractive summaries, demonstrating the challenge of reasoning and inference from long document summaries for LLMs and confirming performance gaps among LLMs."
}