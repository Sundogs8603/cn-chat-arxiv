{
    "title": "Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])",
    "abstract": "The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje",
    "link": "http://arxiv.org/abs/2309.08825",
    "context": "Title: Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])\nAbstract: The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje",
    "path": "papers/23/09/2309.08825.json",
    "total_tokens": 830,
    "translated_title": "先验偏移下的分布鲁棒事后分类器",
    "translated_abstract": "当测试分布偏离训练分布时，机器学习模型的泛化能力显著降低。我们研究了训练模型以应对由类先验或组先验分布变化引起的偏移的问题。存在偏斜的训练先验往往会导致模型对噪声特征过拟合。与现有方法不同，现有方法优化最差或平均性能，而我们的工作是出于对模型鲁棒性质更细粒度控制的需求。我们提出了一种极其轻量级的事后方法，通过对预训练模型的预测进行缩放调整，旨在最小化选择的目标分布周围的分布鲁棒损失。这些调整通过在验证集上求解约束优化问题来计算，并在测试时间应用于模型。",
    "tldr": "研究了先验偏移下的分布鲁棒事后分类器的训练问题，通过在预训练模型的预测上进行缩放调整，以最小化目标分布周围的分布鲁棒损失。",
    "en_tdlr": "Investigated the training of distributionally robust post-hoc classifiers under prior shifts, through scaling adjustments to predictions from a pre-trained model, aiming to minimize the distributionally robust loss around a chosen target distribution."
}