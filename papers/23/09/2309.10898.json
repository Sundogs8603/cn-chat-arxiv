{
    "title": "RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans. (arXiv:2309.10898v1 [cs.CL])",
    "abstract": "The text editing tasks, including sentence fusion, sentence splitting and rephrasing, text simplification, and Grammatical Error Correction (GEC), share a common trait of dealing with highly similar input and output sequences. This area of research lies at the intersection of two well-established fields: (i) fully autoregressive sequence-to-sequence approaches commonly used in tasks like Neural Machine Translation (NMT) and (ii) sequence tagging techniques commonly used to address tasks such as Part-of-speech tagging, Named-entity recognition (NER), and similar. In the pursuit of a balanced architecture, researchers have come up with numerous imaginative and unconventional solutions, which we're discussing in the Related Works section. Our approach to addressing text editing tasks is called RedPenNet and is aimed at reducing architectural and parametric redundancies presented in specific Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our models achieve $F_{0",
    "link": "http://arxiv.org/abs/2309.10898",
    "context": "Title: RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans. (arXiv:2309.10898v1 [cs.CL])\nAbstract: The text editing tasks, including sentence fusion, sentence splitting and rephrasing, text simplification, and Grammatical Error Correction (GEC), share a common trait of dealing with highly similar input and output sequences. This area of research lies at the intersection of two well-established fields: (i) fully autoregressive sequence-to-sequence approaches commonly used in tasks like Neural Machine Translation (NMT) and (ii) sequence tagging techniques commonly used to address tasks such as Part-of-speech tagging, Named-entity recognition (NER), and similar. In the pursuit of a balanced architecture, researchers have come up with numerous imaginative and unconventional solutions, which we're discussing in the Related Works section. Our approach to addressing text editing tasks is called RedPenNet and is aimed at reducing architectural and parametric redundancies presented in specific Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our models achieve $F_{0",
    "path": "papers/23/09/2309.10898.json",
    "total_tokens": 857,
    "translated_title": "RedPenNet用于语法错误纠正：将输出转化为单词，将注意力应用到片段上",
    "translated_abstract": "文本编辑任务包括句子合并、句子分割和改写、文本简化以及语法错误纠正（GEC），它们都面临着输入和输出序列之间高度相似的特点。这一研究领域处于两个已经成熟的领域的交叉点上：（i）全自回归序列到序列的方法，常用于神经机器翻译（NMT）等任务，以及（ii）序列标注技术，常用于词性标注、命名实体识别（NER）等任务。为了设计一个平衡的架构，研究人员提出了许多富有想象力和非传统的解决方案，我们在相关工作部分进行了讨论。我们的方法称为RedPenNet，旨在减少特定的序列到编辑模型中出现的架构和参数冗余，保留它们的半自回归优势。我们的模型实现了$F_{0",
    "tldr": "RedPenNet是一种用于语法错误纠正的方法，解决了文本编辑任务中的架构和参数冗余问题，具有半自回归优势。",
    "en_tdlr": "RedPenNet is a method for grammatical error correction that addresses architectural and parametric redundancies in text editing tasks, while preserving semi-autoregressive advantages."
}