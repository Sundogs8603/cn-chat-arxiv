{
    "title": "Memory-augmented conformer for improved end-to-end long-form ASR. (arXiv:2309.13029v1 [eess.AS])",
    "abstract": "Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.",
    "link": "http://arxiv.org/abs/2309.13029",
    "context": "Title: Memory-augmented conformer for improved end-to-end long-form ASR. (arXiv:2309.13029v1 [eess.AS])\nAbstract: Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.",
    "path": "papers/23/09/2309.13029.json",
    "total_tokens": 855,
    "translated_title": "增强记忆的Conformer用于改进端到端长篇ASR",
    "translated_abstract": "Conformer最近被提出作为自动语音识别（ASR）中一种有前景的建模方法，优于循环神经网络和Transformer。然而，总体而言，这些端到端模型（尤其是注意力机制的模型）在长话语的情况下性能明显下降。为了解决这个局限性，我们建议在Conformer的编码器和解码器之间添加一个完全可微分的记忆增强神经网络。这个外部记忆可以增强对长话语的泛化能力，因为它允许系统循环地存储和检索更多的信息。值得注意的是，我们探索了神经图灵机（NTM），从而提出了我们的Conformer-NTM模型架构用于ASR。使用Librispeech的train-clean-100和train-960数据集进行的实验结果表明，对于长话语，所提出的系统优于没有记忆的基准Conformer。",
    "tldr": "该论文提出了一种将记忆增强神经网络添加到Conformer模型中以解决长话语情况下性能下降的问题。实验结果表明，该系统在长话语上优于基准Conformer模型。"
}