{
    "title": "Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])",
    "abstract": "Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.  This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation metho",
    "link": "http://arxiv.org/abs/2309.15025",
    "context": "Title: Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])\nAbstract: Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.  This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation metho",
    "path": "papers/23/09/2309.15025.json",
    "total_tokens": 926,
    "translated_title": "大规模语言模型对齐：一项调查",
    "translated_abstract": "近年来，大规模语言模型（LLMs）取得了显著进展。这些进展引起了广泛关注，但同时也引发了各种担忧。这些模型的潜力无疑是巨大的；然而，它们可能产生不准确、误导性甚至有害的文本。因此，采用对齐技术以确保这些模型表现出与人类价值一致的行为变得至关重要。本调查旨在对针对LLMs设计的对齐方法进行广泛探讨，并结合该领域中的现有能力研究。采用AI对齐的视角，我们将用于对齐LLMs的主流方法和新兴提议分为外部对齐和内部对齐。我们还探讨了模型可解释性和潜在的对抗攻击漏洞等重要问题。为了评估LLM的对齐，我们提出了多样化的基准和评估方法。",
    "tldr": "这项调查对大规模语言模型对齐的方法进行了广泛探讨，并提出了内部对齐和外部对齐的分类。同时讨论了模型的可解释性和潜在的对抗攻击漏洞。考虑到模型可能产生的不准确和误导性文本，对齐技术显得至关重要。",
    "en_tdlr": "This survey extensively explores alignment methodologies for large language models (LLMs) and categorizes them into inner and outer alignment. It also discusses issues such as model interpretability and vulnerabilities to adversarial attacks. Considering the potential of LLMs to produce inaccurate and misleading texts, alignment techniques are crucial."
}