{
    "title": "Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction. (arXiv:2309.00483v1 [cs.LG])",
    "abstract": "Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at ",
    "link": "http://arxiv.org/abs/2309.00483",
    "context": "Title: Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction. (arXiv:2309.00483v1 [cs.LG])\nAbstract: Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at ",
    "path": "papers/23/09/2309.00483.json",
    "total_tokens": 888,
    "translated_title": "几何感知的线图转换器预训练用于分子性质预测",
    "translated_abstract": "过去几年来，使用深度学习进行分子性质预测已经引起了广泛关注。由于标记分子的稀缺性，对于从无标签数据中学习泛化分子表示的自监督学习方法越来越受到关注。通常将分子视为二维拓扑图来建模，但是已经发现分子的三维几何对确定分子功能非常重要。在本文中，我们提出了几何感知的线图转换器（Galformer）预训练，这是一个新颖的自监督学习框架，旨在通过2D和3D模态增强分子表示学习。具体而言，我们首先设计了一个双模态线图转换器主干来编码分子的拓扑和几何信息。设计的主干结合了有效的结构编码，从两种模态捕捉图结构。然后，我们设计了两个互补的预训练任务",
    "tldr": "本研究提出了一种几何感知的线图转换器（Galformer）预训练方法，用于增强分子表示学习。该方法结合2D和3D模态编码分子的拓扑和几何信息，并设计了互补的预训练任务。",
    "en_tdlr": "This paper proposes a Geometry-aware Line Graph Transformer (Galformer) pre-training method to enhance molecular representation learning. The method combines 2D and 3D modalities to encode the topological and geometric information of molecules, and designs complementary pre-training tasks."
}