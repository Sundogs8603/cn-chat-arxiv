{
    "title": "DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])",
    "abstract": "This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER",
    "link": "http://arxiv.org/abs/2309.16844",
    "context": "Title: DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])\nAbstract: This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER",
    "path": "papers/23/09/2309.16844.json",
    "total_tokens": 954,
    "translated_title": "DeBERTinha: 一种用于巴西葡萄牙语自然语言处理任务的DebertaV3 XSmall的多步骤适应方法",
    "translated_abstract": "本文提出了一种适应英语预训练DebertaV3 XSmall模型用于巴西葡萄牙语自然语言处理（NLP）任务的方法。该方法的关键在于多步训练过程，以确保模型在葡萄牙语上的有效调优。使用Carolina和BrWac的初始数据集进行预处理，解决表情符号、HTML标签和编码等问题。使用SentencePiece创建一个包含50,000个token的葡萄牙语特定词汇表。模型不是从头训练，而是使用预训练的英语模型的权重来初始化网络的大部分，仅包括随机嵌入，以识别从头训练的昂贵成本。采用替换标记检测任务以与DebertaV3训练的相同格式微调模型。适应的模型称为DeBERTinha，在命名实体识别、情感分析和判断句子相关性等下游任务上表现出优越性能，胜过BER。",
    "tldr": "DeBERTinha是一种通过多步骤训练和微调适应DebertaV3 XSmall模型的方法，用于巴西葡萄牙语自然语言处理任务。它在命名实体识别、情感分析和判断句子相关性等任务上表现出优越性能。"
}