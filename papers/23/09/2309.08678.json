{
    "title": "Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions. (arXiv:2309.08678v1 [cs.LG])",
    "abstract": "How to properly set the privacy parameter in differential privacy (DP) has been an open question in DP research since it was first proposed in 2006. In this work, we demonstrate the ability of influence functions to offer insight into how a specific privacy parameter value will affect a model's test loss in the randomized response-based local DP setting. Our proposed method allows a data curator to select the privacy parameter best aligned with their allowed privacy-utility trade-off without requiring heavy computation such as extensive model retraining and data privatization. We consider multiple common randomization scenarios, such as performing randomized response over the features, and/or over the labels, as well as the more complex case of applying a class-dependent label noise correction method to offset the noise incurred by randomization. Further, we provide a detailed discussion over the computational complexity of our proposed approach inclusive of an empirical analysis. Thro",
    "link": "http://arxiv.org/abs/2309.08678",
    "context": "Title: Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions. (arXiv:2309.08678v1 [cs.LG])\nAbstract: How to properly set the privacy parameter in differential privacy (DP) has been an open question in DP research since it was first proposed in 2006. In this work, we demonstrate the ability of influence functions to offer insight into how a specific privacy parameter value will affect a model's test loss in the randomized response-based local DP setting. Our proposed method allows a data curator to select the privacy parameter best aligned with their allowed privacy-utility trade-off without requiring heavy computation such as extensive model retraining and data privatization. We consider multiple common randomization scenarios, such as performing randomized response over the features, and/or over the labels, as well as the more complex case of applying a class-dependent label noise correction method to offset the noise incurred by randomization. Further, we provide a detailed discussion over the computational complexity of our proposed approach inclusive of an empirical analysis. Thro",
    "path": "papers/23/09/2309.08678.json",
    "total_tokens": 869,
    "translated_title": "通过影响函数评估本地差分隐私对效用损失的影响",
    "translated_abstract": "如何正确设置差分隐私中的隐私参数是自从2006年首次提出差分隐私以来一直存在的一个问题。在这项工作中，我们展示了影响函数的能力，可以揭示在基于随机响应的本地差分隐私设置下，特定隐私参数值将如何影响模型的测试损失。我们提出的方法允许数据管理员选择与其允许的隐私-效用权衡最符合的隐私参数值，而无需进行繁重的计算，如大量模型重训练和数据私有化。我们考虑了多种常见的随机化场景，例如对特征进行随机响应，对标签进行随机响应，以及通过应用基于类别的标签噪声校正方法来抵消随机化造成的噪声的更复杂的情况。此外，我们还详细讨论了我们提出的方法的计算复杂度，包括经验分析。",
    "tldr": "本论文通过影响函数评估了本地差分隐私对效用损失的影响，并提出了一种方法，可以帮助数据管理员选择最适合其隐私-效用权衡的隐私参数值，而无需进行大量计算。",
    "en_tdlr": "This paper evaluates the impact of local differential privacy on utility loss using influence functions and proposes a method to help data curators select an optimal privacy parameter value without heavy computation."
}