{
    "title": "RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])",
    "abstract": "Although syntactic information is beneficial for many NLP tasks, combining it with contextual information between words to solve the coreference resolution problem needs to be further explored. In this paper, we propose an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. In particular, the RGAT model is first proposed, then used to understand the syntactic dependency graph and learn better task-specific syntactic embeddings. An integrated architecture incorporating BERT embeddings and syntactic embeddings is constructed to generate blending representations for the downstream task. Our experiments on a public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision learning of the syntactic dependency graph and without fine-tuning the entire BERT, we increased the F1-score of the previous best model (RGCN-wi",
    "link": "http://arxiv.org/abs/2309.04977",
    "context": "Title: RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])\nAbstract: Although syntactic information is beneficial for many NLP tasks, combining it with contextual information between words to solve the coreference resolution problem needs to be further explored. In this paper, we propose an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. In particular, the RGAT model is first proposed, then used to understand the syntactic dependency graph and learn better task-specific syntactic embeddings. An integrated architecture incorporating BERT embeddings and syntactic embeddings is constructed to generate blending representations for the downstream task. Our experiments on a public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision learning of the syntactic dependency graph and without fine-tuning the entire BERT, we increased the F1-score of the previous best model (RGCN-wi",
    "path": "papers/23/09/2309.04977.json",
    "total_tokens": 965,
    "translated_title": "RGAT：更深入探索句法依赖信息在指代消解中的作用",
    "translated_abstract": "虽然句法信息对很多自然语言处理任务是有益的，但将其与词语之间的上下文信息相结合来解决指代消解问题仍需进一步探索。本文提出了一种综合预训练BERT和句法关系图注意网络（RGAT）的端到端解析器，以更深入地研究句法依赖信息在指代消解任务中的作用。具体而言，首先提出了RGAT模型，然后用于理解句法依赖图并学习更好的任务特定句法嵌入。构建了一个集成结构，将BERT嵌入和句法嵌入结合起来，为下游任务生成混合表示。我们在一个公共的Gendered Ambiguous Pronouns（GAP）数据集上进行的实验表明，在对句法依赖图进行监督学习的同时，不需要对整个BERT进行微调，我们提高了先前最佳模型（RGCN-wi）的F1分数",
    "tldr": "本文提出了一种综合预训练BERT和句法关系图注意网络（RGAT）的端到端解析器，以更深入地研究句法依赖信息在指代消解任务中的作用。通过对句法依赖图进行监督学习，并不需要对整个BERT进行微调，我们提高了先前最佳模型的F1分数。",
    "en_tdlr": "This paper proposes an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. By supervising the learning of the syntactic dependency graph and without fine-tuning the entire BERT, the F1-score of the previous best model is increased."
}