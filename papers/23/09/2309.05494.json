{
    "title": "CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)",
    "abstract": "Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than ",
    "link": "http://arxiv.org/abs/2309.05494",
    "context": "Title: CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)\nAbstract: Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than ",
    "path": "papers/23/09/2309.05494.json",
    "total_tokens": 890,
    "translated_title": "CrisisTransformers：用于危机相关社交媒体文本的预训练语言模型和句子编码器",
    "translated_abstract": "社交媒体平台在危机沟通中起着重要作用，但由于其非正式性质，分析危机相关社交媒体文本具有挑战性。基于Transformer的预训练模型如BERT和RoBERTa在各种自然语言处理任务中表现出成功，但它们并不针对危机相关文本进行优化。此外，通用的句子编码器用于生成句子嵌入，而不考虑危机相关文本中的文本复杂性。文本分类、语义搜索和聚类等应用的进展有助于有效处理危机相关文本，这对于紧急响应者获得危机事件的综合视图至关重要，无论该事件是历史事件还是实时事件。为填补危机信息学文献中的这些空白，本研究引入了CrisisTransformers，这是一组在超过150亿个词元的推文语料库上训练的预训练语言模型和句子编码器的集成。",
    "tldr": "CrisisTransformers是一组针对危机相关文本优化的预训练语言模型和句子编码器，旨在有效处理危机相关社交媒体文本，并为紧急响应者提供综合视图。"
}