{
    "title": "Adapting Large Language Models via Reading Comprehension",
    "abstract": "arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif",
    "link": "https://arxiv.org/abs/2309.09530",
    "context": "Title: Adapting Large Language Models via Reading Comprehension\nAbstract: arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif",
    "path": "papers/23/09/2309.09530.json",
    "total_tokens": 854,
    "translated_title": "通过阅读理解调整大型语言模型",
    "translated_abstract": "我们探讨了在特定领域语料库上持续预训练对大型语言模型的影响，发现在原始语料库上进行训练赋予模型领域知识，但极大地损害了其回答问题的能力。受人类通过阅读理解学习的启发，即阅读后练习提高基于所学知识回答问题的能力，我们提出了一种将原始语料库转化为阅读理解文本的简单方法。每个原始文本都会被一系列与其内容相关的任务丰富。我们的方法非常可扩展，适用于任何预训练语料库，能够在三个不同领域（生物医学、金融和法律）的各种任务中持续提升性能。值得注意的是，我们的7B语言模型在竞争中表现出色，能与规模更大的领域特定模型（如BloombergGPT-50B）相媲美。此外，我们证明了领域特定模型可以带来更好的效果。",
    "tldr": "通过将原始语料库转化为阅读理解文本来调整大型语言模型，使其在多个领域的各种任务中性能始终得到提升。",
    "en_tdlr": "Adapting large language models by transforming raw corpora into reading comprehension texts consistently enhances performance across various tasks in multiple domains."
}