{
    "title": "PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])",
    "abstract": "Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys",
    "link": "http://arxiv.org/abs/2309.10977",
    "context": "Title: PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])\nAbstract: Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys",
    "path": "papers/23/09/2309.10977.json",
    "total_tokens": 866,
    "translated_title": "PAGER: 一种用于深度回归模型故障分析的框架",
    "translated_abstract": "安全部署AI模型需要主动检测潜在的预测故障，以防止昂贵的错误。尽管分类问题的故障检测已经引起了广泛关注，但在回归任务中表征故障模式更加复杂且较少研究。现有方法依赖于认识不确定性或与训练分布的特征不一致来表征模型风险。然而，我们表明，仅靠不确定性无法准确表征故障，这是由于各种误差源的存在。在本文中，我们提出了PAGER（回归器的原则性泛化错误分析），这是一个系统检测和表征深度回归模型故障的框架。基于最近提出的深度模型锚定思想，PAGER将认识不确定性和新颖的、互补的不一致分数统一起来，将样本组织成不同的风险区域，从而提供全面的分析。",
    "tldr": "PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。"
}