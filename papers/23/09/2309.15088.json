{
    "title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. (arXiv:2309.15088v1 [cs.IR])",
    "abstract": "Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at h",
    "link": "http://arxiv.org/abs/2309.15088",
    "context": "Title: RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. (arXiv:2309.15088v1 [cs.IR])\nAbstract: Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at h",
    "path": "papers/23/09/2309.15088.json",
    "total_tokens": 949,
    "translated_title": "RankVicuna: 使用开源大型语言模型进行零样本列表排序的研究",
    "translated_abstract": "研究人员成功地将ChatGPT等大型语言模型应用于信息检索中的重新排序，但迄今为止，这样的工作大多建立在不透明的API后面的专有模型上。这种方法产生的实验结果不可复现且非确定性，威胁到建立在这种不稳定基础上的结果的真实性。为了解决这个重大缺陷，我们提出了RankVicuna，这是第一个能够在零样本设置中执行高质量列表排序的完全开源的大型语言模型。在TREC 2019和2020深度学习跟踪实验中，我们的实验结果显示，我们可以使用比GPT-3.5小得多的7B参数模型实现与零样本重新排序相当的效果，尽管我们的效果仍略逊于GPT-4重新排序。我们希望我们的工作为将来使用现代大型语言模型进行重新排序的研究提供基础。复现我们结果所需的所有代码都可以在h链接处获得。",
    "tldr": "RankVicuna是第一个能够在零样本设置中执行高质量列表排序的完全开源的大型语言模型，通过使用比GPT-3.5小得多的参数模型，实现了与零样本重新排序相当的效果，并为将来研究提供了基础。",
    "en_tdlr": "RankVicuna is the first fully open-source large language model capable of performing high-quality listwise reranking in a zero-shot setting. By using a much smaller parameter model compared to GPT-3.5, it achieves comparable effectiveness in zero-shot reranking and provides a foundation for future research."
}