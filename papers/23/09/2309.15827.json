{
    "title": "How We Define Harm Impacts Data Annotations: Explaining How Annotators Distinguish Hateful, Offensive, and Toxic Comments. (arXiv:2309.15827v1 [cs.CL])",
    "abstract": "Computational social science research has made advances in machine learning and natural language processing that support content moderators in detecting harmful content. These advances often rely on training datasets annotated by crowdworkers for harmful content. In designing instructions for annotation tasks to generate training data for these algorithms, researchers often treat the harm concepts that we train algorithms to detect - 'hateful', 'offensive', 'toxic', 'racist', 'sexist', etc. - as interchangeable. In this work, we studied whether the way that researchers define 'harm' affects annotation outcomes. Using Venn diagrams, information gain comparisons, and content analyses, we reveal that annotators do not use the concepts 'hateful', 'offensive', and 'toxic' interchangeably. We identify that features of harm definitions and annotators' individual characteristics explain much of how annotators use these terms differently. Our results offer empirical evidence discouraging the co",
    "link": "http://arxiv.org/abs/2309.15827",
    "context": "Title: How We Define Harm Impacts Data Annotations: Explaining How Annotators Distinguish Hateful, Offensive, and Toxic Comments. (arXiv:2309.15827v1 [cs.CL])\nAbstract: Computational social science research has made advances in machine learning and natural language processing that support content moderators in detecting harmful content. These advances often rely on training datasets annotated by crowdworkers for harmful content. In designing instructions for annotation tasks to generate training data for these algorithms, researchers often treat the harm concepts that we train algorithms to detect - 'hateful', 'offensive', 'toxic', 'racist', 'sexist', etc. - as interchangeable. In this work, we studied whether the way that researchers define 'harm' affects annotation outcomes. Using Venn diagrams, information gain comparisons, and content analyses, we reveal that annotators do not use the concepts 'hateful', 'offensive', and 'toxic' interchangeably. We identify that features of harm definitions and annotators' individual characteristics explain much of how annotators use these terms differently. Our results offer empirical evidence discouraging the co",
    "path": "papers/23/09/2309.15827.json",
    "total_tokens": 1061,
    "translated_title": "如何定义伤害影响数据标注：解释标注者如何区分令人讨厌的、冒犯的和有毒的评论",
    "translated_abstract": "计算社会科学研究在机器学习和自然语言处理方面取得了进展，支持内容审核员检测有害内容。这些进展往往依赖于由众包工作者为有害内容标注的训练数据集。在设计标注任务的指导说明以生成这些算法的训练数据时，研究人员通常将我们训练算法检测的伤害概念 - “令人讨厌的”、“冒犯的”、“有毒的”、“种族主义的”、“性别歧视的”等视为可互换的。在这项工作中，我们研究了研究人员对“伤害”的定义方式是否影响标注结果。通过使用维恩图、信息增益比较和内容分析，我们发现标注者并不将“令人讨厌的”、“冒犯的”和“有毒的”概念混为一谈。我们发现伤害定义的特征以及标注者的个人特点在很大程度上解释了标注者如何不同地使用这些术语。我们的结果提供了经验   证据，不鼓励混淆这些术语的使用。",
    "tldr": "本研究研究了研究人员如何定义“伤害”对标注结果的影响。通过使用维恩图、信息增益比较和内容分析，我们发现标注者不将“令人讨厌的”、“冒犯的”和“有毒的”概念混为一谈。我们的结果为不鼓励混淆这些术语的使用提供了经验证据。",
    "en_tdlr": "This paper examines how researchers define \"harm\" and its impact on annotation outcomes. Using Venn diagrams, information gain comparisons, and content analyses, the study reveals that annotators do not treat the concepts of \"hateful,\" \"offensive,\" and \"toxic\" interchangeably. The findings discourage the interchangeable use of these terms."
}