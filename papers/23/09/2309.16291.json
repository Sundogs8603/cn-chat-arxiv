{
    "title": "Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])",
    "abstract": "We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.  Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.  In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.",
    "link": "http://arxiv.org/abs/2309.16291",
    "context": "Title: Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])\nAbstract: We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.  Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.  In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.",
    "path": "papers/23/09/2309.16291.json",
    "total_tokens": 712,
    "translated_title": "RL方法的效率分离：无模型、有模型和目标条件下的效率分析",
    "translated_abstract": "我们证明了一类广泛的强化学习（RL）算法的效率的基本限制。这个限制适用于无模型的RL方法，以及一系列的有模型方法，如树搜索的规划。在对这类问题的抽象定义下，我们提供了一系列RL问题，对于这些方法来说，它们与环境的交互寻找最优行为的时间复杂度下界是指数级的。然而，存在一种方法，不针对这个特定的问题家族，可以高效地解决这个问题家族中的问题。相比之下，我们的限制不适用于文献中提出的几种方法，例如目标条件下的方法或构建逆动力学模型的其他算法。",
    "tldr": "本论文证明了无模型和有模型强化学习方法在效率上存在根本的限制，但目标条件下的方法和构建逆动力学模型的算法不受该限制。"
}