{
    "title": "Style Transfer for Non-differentiable Audio Effects. (arXiv:2309.17125v1 [cs.LG])",
    "abstract": "Digital audio effects are widely used by audio engineers to alter the acoustic and temporal qualities of audio data. However, these effects can have a large number of parameters which can make them difficult to learn for beginners and hamper creativity for professionals. Recently, there have been a number of efforts to employ progress in deep learning to acquire the low-level parameter configurations of audio effects by minimising an objective function between an input and reference track, commonly referred to as style transfer. However, current approaches use inflexible black-box techniques or require that the effects under consideration are implemented in an auto-differentiation framework. In this work, we propose a deep learning approach to audio production style matching which can be used with effects implemented in some of the most widely used frameworks, requiring only that the parameters under consideration have a continuous domain. Further, our method includes style matching fo",
    "link": "http://arxiv.org/abs/2309.17125",
    "context": "Title: Style Transfer for Non-differentiable Audio Effects. (arXiv:2309.17125v1 [cs.LG])\nAbstract: Digital audio effects are widely used by audio engineers to alter the acoustic and temporal qualities of audio data. However, these effects can have a large number of parameters which can make them difficult to learn for beginners and hamper creativity for professionals. Recently, there have been a number of efforts to employ progress in deep learning to acquire the low-level parameter configurations of audio effects by minimising an objective function between an input and reference track, commonly referred to as style transfer. However, current approaches use inflexible black-box techniques or require that the effects under consideration are implemented in an auto-differentiation framework. In this work, we propose a deep learning approach to audio production style matching which can be used with effects implemented in some of the most widely used frameworks, requiring only that the parameters under consideration have a continuous domain. Further, our method includes style matching fo",
    "path": "papers/23/09/2309.17125.json",
    "total_tokens": 855,
    "translated_title": "非可微分音频效果的风格迁移",
    "translated_abstract": "数字音频效果被音频工程师广泛使用来改变音频数据的声学和时间特性。然而，这些效果可能具有大量参数，这使得对于初学者而言学习困难，对于专业人士而言限制了创造力。最近，一些努力致力于利用深度学习的进展来获取音频效果的低级参数配置，通过最小化输入和参考音轨之间的目标函数来实现，称之为风格迁移。然而，目前的方法使用的是不灵活的黑盒技术，或者要求所考虑的效果在自动微分框架中实现。在这项工作中，我们提出了一种深度学习方法来进行音频制作风格匹配，可以用于一些最广泛使用的框架中实现的效果，仅要求所考虑的参数具有连续的领域。此外，我们的方法还包括风格匹配部分。",
    "tldr": "本研究提出了一种深度学习方法，用于非可微分音频效果的风格迁移。该方法可以用于最广泛使用的框架中实现的效果，并且仅要求所考虑的参数具有连续的领域。"
}