{
    "title": "Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.",
    "link": "http://arxiv.org/abs/2309.11378",
    "context": "Title: Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])\nAbstract: Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.",
    "path": "papers/23/09/2309.11378.json",
    "total_tokens": 837,
    "translated_title": "预处理联邦学习",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习方法，可以以高效的通信和保护隐私的方式进行模型训练。FL中的标准优化方法是联邦平均（FedAvg），它在通信轮之间执行多个本地SGD步骤。与现代的一阶自适应优化相比，FedAvg被认为缺乏算法适应性。本文提出了基于两个自适应框架（本地适应性和服务器端适应性）的新的高效通信FL算法：PreFed和PreFedOp。提出的方法通过使用新颖的协方差矩阵预处理器来实现适应性。理论上，我们为我们的算法提供了收敛性保证。实证实验表明，我们的方法在i.i.d.和非i.i.d.设置上都达到了最先进的性能。",
    "tldr": "本文提出了基于两个自适应框架的新的通信高效联邦学习算法：PreFed和PreFedOp，通过使用新颖的协方差矩阵预处理器来实现适应性。实验证明这些方法在i.i.d.和非i.i.d.设置下均取得了最先进的性能。",
    "en_tdlr": "The paper presents new communication-efficient federated learning algorithms, PreFed and PreFedOp, based on two adaptive frameworks. These methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings by using a novel covariance matrix preconditioner for adaptivity."
}