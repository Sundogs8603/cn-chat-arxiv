{
    "title": "Linear Regression using Heterogeneous Data Batches. (arXiv:2309.01973v1 [cs.LG])",
    "abstract": "In many learning applications, data are collected from multiple sources, each providing a \\emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work~\\cite{kong2020meta} showed that with abundant small-batches, the regression vectors can be learned with only few, $\\tilde\\Omega( k^{3/2})$, batches of medium-size with $\\tilde\\Omega(\\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem\". We propose a novel gradient-based algorithm that",
    "link": "http://arxiv.org/abs/2309.01973",
    "context": "Title: Linear Regression using Heterogeneous Data Batches. (arXiv:2309.01973v1 [cs.LG])\nAbstract: In many learning applications, data are collected from multiple sources, each providing a \\emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work~\\cite{kong2020meta} showed that with abundant small-batches, the regression vectors can be learned with only few, $\\tilde\\Omega( k^{3/2})$, batches of medium-size with $\\tilde\\Omega(\\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem\". We propose a novel gradient-based algorithm that",
    "path": "papers/23/09/2309.01973.json",
    "total_tokens": 884,
    "translated_title": "使用异构数据批次的线性回归",
    "translated_abstract": "在许多学习应用中，数据是从多个来源收集的，每个来源都提供一批样本，单独的样本是不足以学习其输入-输出关系的。一种常见的方法是假设这些来源属于数个未知的子组，每个子组具有未知的输入分布和输入-输出关系。我们考虑了这种设置最基本和重要的表现之一，即输出是输入的噪声线性组合，并且有 $k$ 个子组，每个子组都有自己的回归向量。之前的研究[1]表明，通过丰富的小批量，可以用少数中等大小的批次来学习回归向量，每个批次有 $\\tilde\\Omega( k^{3/2})$ 个样本。然而，该论文要求所有 $k$ 个子组的输入分布是各向同性的高斯分布，并表示去除这一假设是一个“有趣且具有挑战性的问题”。我们提出了一种新颖的基于梯度的算法，它可以:",
    "tldr": "本论文提出了一种用于线性回归的新颖梯度算法，可以在不同子组的异构数据批次中进行学习，并且不再需要假设输入分布为高斯分布。",
    "en_tdlr": "This paper proposes a novel gradient-based algorithm for linear regression that can learn from heterogeneous data batches from different subgroups without assuming an isotropic Gaussian input distribution."
}