{
    "title": "Improved Outlier Robust Seeding for k-means. (arXiv:2309.02710v1 [cs.LG])",
    "abstract": "The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold.  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, whil",
    "link": "http://arxiv.org/abs/2309.02710",
    "context": "Title: Improved Outlier Robust Seeding for k-means. (arXiv:2309.02710v1 [cs.LG])\nAbstract: The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold.  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, whil",
    "path": "papers/23/09/2309.02710.json",
    "total_tokens": 888,
    "translated_title": "改进的鲁棒性$k$-means初始化方法",
    "translated_abstract": "$k$-means是一种流行的聚类目标函数，但它对异常值非常敏感。现有的$k$-means++初始化方法使用$D^2$采样并具有$O(\\log k)$的近似保证。然而，在存在对抗性噪声或异常值的情况下，$D^2$采样更有可能选择远离集群的异常值作为初始聚类中心，因此其关于$k$-means在内部数据上的近似保证不再成立。我们假设异常值构成给定数据的一个常数分数，并提出了一种在$D^2$采样分布中使其鲁棒性更好的简单改进。我们的算法在$O(ndk)$时间内运行，输出$O(k)$个聚类，比最优异常值个数多丢弃一些数据点，并且具有可证明的$O(1)$近似保证。我们的算法还可以修改为输出恰好$k$个聚类而不是$O(k)$个聚类。",
    "tldr": "本论文提出了一种改进的$k$-means初始化方法，使其在存在异常值的情况下更加鲁棒。算法在复杂度低的同时提供了给定数据的有效聚类。",
    "en_tdlr": "This paper proposes an improved initialization method for k-means that is more robust to outliers. The algorithm provides effective clustering of the given data while maintaining low computational complexity."
}