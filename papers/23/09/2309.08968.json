{
    "title": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the power of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The ef",
    "link": "https://arxiv.org/abs/2309.08968",
    "context": "Title: Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference\nAbstract: Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the power of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The ef",
    "path": "papers/23/09/2309.08968.json",
    "total_tokens": 916,
    "translated_title": "Sorted LLaMA: 揭示大型语言模型中间层的潜力，用于动态推理",
    "translated_abstract": "大型语言模型（LLMs）通过在理解和生成类似人类文本方面表现出色，为自然语言处理（NLP）领域带来了革命。然而，广泛部署这些模型可能成本过高。SortedNet是一种最近的训练技术，通过利用网络中的模块化和基于计算/准确性对子模型进行嵌套排序，实现了动态推理。我们将SortedNet扩展到生成NLP任务，使大型语言模型在不进行任何预训练的情况下变得动态，仅通过将标准微调（SFT）替换为排序微调（SoFT）。我们的方法提高了模型的效率，消除了在推理过程中在不同场景中使用多个模型的需求。我们展示了这种方法可以释放transformers中间层在生成目标输出方面的能力。我们的子模型仍然是原始模型的组成部分，最小化了存储需求和在不同计算/延迟预算之间的过渡成本。",
    "tldr": "Sorted LLaMA通过扩展SortedNet到生成NLP任务，使得大型语言模型在动态推理中更高效，并且不需要预训练，只需将标准微调替换为排序微调即可。该方法可以释放transformers中间层的潜力，同时最小化存储需求和过渡成本。"
}