{
    "title": "Probabilistic Self-supervised Learning via Scoring Rules Minimization. (arXiv:2309.02048v1 [cs.LG])",
    "abstract": "In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation qualit",
    "link": "http://arxiv.org/abs/2309.02048",
    "context": "Title: Probabilistic Self-supervised Learning via Scoring Rules Minimization. (arXiv:2309.02048v1 [cs.LG])\nAbstract: In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation qualit",
    "path": "papers/23/09/2309.02048.json",
    "total_tokens": 857,
    "translated_title": "通过评分规则最小化进行概率自监督学习",
    "translated_abstract": "在本文中，我们提出了一种新颖的概率自监督学习方法ProSMIN，通过评分规则最小化来提升表示质量和缓解崩溃表示。我们的方法涉及两个神经网络：在线网络和目标网络，它们通过知识蒸馏相互协作学习表示的多样分布。通过将输入样本以两种增强格式呈现，训练在线网络来预测不同增强视图下的目标网络表示的相同样本。通过基于适当的评分规则的新损失函数训练这两个网络。我们对ProSMIN的收敛性提供了理论证明，证明了其修改后的评分规则的严格适用性。这一认识验证了该方法的优化过程并有助于其在提升表示质量方面的稳健性和有效性。",
    "tldr": "本文提出了一种新颖的概率自监督学习方法ProSMIN，通过评分规则最小化来提升表示质量和缓解崩溃表示。方法通过两个神经网络协同学习多样分布的表示，并提供了对ProSMIN收敛性的理论证明。",
    "en_tdlr": "This paper proposes a novel probabilistic self-supervised learning method, ProSMIN, which leverages scoring rule minimization to enhance representation quality and mitigate collapsing representations. The method involves two neural networks, online network and target network, collaborating to learn diverse distribution of representations and is theoretically justified for its convergence."
}