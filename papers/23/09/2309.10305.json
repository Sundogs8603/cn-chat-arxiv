{
    "title": "Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
    "link": "http://arxiv.org/abs/2309.10305",
    "context": "Title: Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
    "path": "papers/23/09/2309.10305.json",
    "total_tokens": 907,
    "translated_title": "Baichuan 2: 开放的大规模语言模型",
    "translated_abstract": "大型语言模型（LLMs）在仅有少量自然语言指令示例的情况下，已经在各种自然语言任务中展示出了令人瞩目的性能，减少了对广泛特征工程的需求。然而，大多数强大的LLMs是封闭源代码的，或者在除了英语以外的其他语言方面能力有限。在这篇技术报告中，我们介绍了Baichuan 2系列，这是一系列从头开始进行训练的大规模多语言模型，包含70亿和130亿个参数，使用26万亿个标记进行训练。Baichuan 2在MMLU、CMMLU、GSM8K和HumanEval等公开基准测试中与其他相同规模的开源模型相匹配或胜过。此外，Baichuan 2在医学和法律等垂直领域表现出色。我们将发布所有预训练模型检查点，以使研究界更好地理解Baichuan 2的训练动态。",
    "tldr": "Baichuan 2是一系列开放的大规模多语言模型，拥有70亿和130亿个参数，训练自26万亿个标记。Baichuan 2在公开基准测试中表现出色，并在垂直领域如医学和法律中具有优势。"
}