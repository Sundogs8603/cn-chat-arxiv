{
    "title": "Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])",
    "abstract": "Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on \"training\" functions. These training functions are typically required to have the same domain as the \"test\" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.",
    "link": "http://arxiv.org/abs/2309.16597",
    "context": "Title: Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])\nAbstract: Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on \"training\" functions. These training functions are typically required to have the same domain as the \"test\" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.",
    "path": "papers/23/09/2309.16597.json",
    "total_tokens": 852,
    "translated_title": "异质搜索空间上的贝叶斯优化的迁移学习",
    "translated_abstract": "贝叶斯优化是一种流行的黑盒函数优化方法，它基于贝叶斯模型（通常是高斯过程）进行顺序决策。为了确保模型的质量，我们开发了迁移学习方法，通过学习来自“训练”函数的观察结果来自动设计高斯过程先验。这些训练函数通常需要与“测试”函数（待优化的黑盒函数）具有相同的定义域。在本文中，我们介绍了一种名为MPHD的模型预训练方法，它使用神经网络将特定于领域的上下文映射到分层高斯过程的规范。MPHD可以与贝叶斯优化无缝集成，实现异质搜索空间的知识迁移。我们的理论和实证结果证明了MPHD的有效性，并展示了它在具有挑战性的黑盒函数优化任务中的优越性能。",
    "tldr": "本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。"
}