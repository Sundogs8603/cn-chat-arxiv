{
    "title": "Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])",
    "abstract": "Large language models (LLM) have demonstrated their abilities to solve various natural language processing tasks through dialogue-based interactions. For instance, research indicates that LLMs can achieve competitive performance in offline machine translation tasks for high-resource languages. However, applying LLMs to simultaneous machine translation (SimulMT) poses many challenges, including issues related to the training-inference mismatch arising from different decoding patterns. In this paper, we explore the feasibility of utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce a simple yet effective mixture policy that enables LLMs to engage in SimulMT without requiring additional training. Furthermore, after Supervised Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits significant performance improvements. Our experiments, conducted with Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that LLM can ac",
    "link": "http://arxiv.org/abs/2309.06706",
    "context": "Title: Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])\nAbstract: Large language models (LLM) have demonstrated their abilities to solve various natural language processing tasks through dialogue-based interactions. For instance, research indicates that LLMs can achieve competitive performance in offline machine translation tasks for high-resource languages. However, applying LLMs to simultaneous machine translation (SimulMT) poses many challenges, including issues related to the training-inference mismatch arising from different decoding patterns. In this paper, we explore the feasibility of utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce a simple yet effective mixture policy that enables LLMs to engage in SimulMT without requiring additional training. Furthermore, after Supervised Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits significant performance improvements. Our experiments, conducted with Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that LLM can ac",
    "path": "papers/23/09/2309.06706.json",
    "total_tokens": 844,
    "translated_title": "使用大型语言模型的同时机器翻译",
    "translated_abstract": "通过对话式交互，大型语言模型 (LLM) 已经展示出解决各种自然语言处理任务的能力。例如，研究表明，LLM可以在高资源语言的离线机器翻译任务中取得竞争性的性能。然而，将LLM应用于同时机器翻译 (SimulMT) 面临许多挑战，包括与不同解码模式产生的训练-推理不匹配问题。本文探索了利用LLM进行SimulMT的可行性。在传统方法的基础上，我们引入了一个简单而有效的混合策略，使LLM能够在不需要额外训练的情况下参与SimulMT。此外，在对全句和前缀句子进行有监督微调后，该模型展示出了显著的性能改进。我们使用MUST-C数据集上的九种语言对进行实验，结果表明LLM可以实现同时机器翻译。",
    "tldr": "本文研究了使用大型语言模型进行同时机器翻译的可行性，通过引入混合策略，并进行有监督微调，取得了显著的性能改进。"
}