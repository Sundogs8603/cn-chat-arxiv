{
    "title": "SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])",
    "abstract": "Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel \"weighted accumulated self-attention map\" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen",
    "link": "http://arxiv.org/abs/2309.03179",
    "context": "Title: SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])\nAbstract: Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel \"weighted accumulated self-attention map\" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen",
    "path": "papers/23/09/2309.03179.json",
    "total_tokens": 869,
    "translated_title": "SLiMe: 像我一样进行分割",
    "translated_abstract": "使用大型视觉语言模型（如稳定扩散SD），在诸多下游任务（包括图像编辑、图像对应和3D形状生成）方面取得了显著进展。受到这些进展的启发，我们探索利用这些广泛的视觉语言模型，通过提出SLiMe，以尽可能少的标注样本对图像进行任意细粒度的分割。SLiMe将这个问题作为一个优化任务来进行。具体而言，给定一张训练图像及其分割掩膜，我们首先从SD先验中提取注意力图，包括我们的新颖的“加权累积自注意力图”。然后，利用提取的注意力图，优化稳定扩散的文本嵌入，使得每个嵌入只学习训练图像中的一个分割区域。这些学习到的嵌入然后在注意力图中突出显示分割区域，从而可以生成分割图。这使得SLiMe可以进行图像分割。",
    "tldr": "基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。",
    "en_tdlr": "The SLiMe approach based on large vision-language models achieves image segmentation with only a few annotated samples by extracting attention maps and optimizing text embeddings, enabling segmentation at arbitrary granularity."
}