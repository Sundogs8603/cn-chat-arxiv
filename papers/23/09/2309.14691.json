{
    "title": "On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. (arXiv:2309.14691v1 [cs.LG])",
    "abstract": "Artificial neural networks (ANNs) with recurrence and self-attention have been shown to be Turing-complete (TC). However, existing work has shown that these ANNs require multiple turns or unbounded computation time, even with unbounded precision in weights, in order to recognize TC grammars. However, under constraints such as fixed or bounded precision neurons and time, ANNs without memory are shown to struggle to recognize even context-free languages. In this work, we extend the theoretical foundation for the $2^{nd}$-order recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$ RNN that is Turing-complete with bounded time. This model is capable of directly encoding a transition table into its recurrent weights, enabling bounded time computation and is interpretable by design. We also demonstrate that $2$nd order RNNs, without memory, under bounded weights and time constraints, outperform modern-day models such as vanilla RNNs and gated recurrent units in recogn",
    "link": "http://arxiv.org/abs/2309.14691",
    "context": "Title: On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. (arXiv:2309.14691v1 [cs.LG])\nAbstract: Artificial neural networks (ANNs) with recurrence and self-attention have been shown to be Turing-complete (TC). However, existing work has shown that these ANNs require multiple turns or unbounded computation time, even with unbounded precision in weights, in order to recognize TC grammars. However, under constraints such as fixed or bounded precision neurons and time, ANNs without memory are shown to struggle to recognize even context-free languages. In this work, we extend the theoretical foundation for the $2^{nd}$-order recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$ RNN that is Turing-complete with bounded time. This model is capable of directly encoding a transition table into its recurrent weights, enabling bounded time computation and is interpretable by design. We also demonstrate that $2$nd order RNNs, without memory, under bounded weights and time constraints, outperform modern-day models such as vanilla RNNs and gated recurrent units in recogn",
    "path": "papers/23/09/2309.14691.json",
    "total_tokens": 940,
    "translated_title": "计算复杂度和形式层次的二阶循环神经网络",
    "translated_abstract": "已经证明具有循环和自注意力的人工神经网络(ANNs)是图灵完备的(TC)。然而，现有的工作表明，即使在权重无限制的情况下，这些ANNs也需要多次迭代或无限计算时间才能识别TC语法。然而，在固定或有界精度神经元和时间的约束下，无记忆的ANNs被证明很难识别甚至是上下文自由语言。在这项工作中，我们扩展了二阶循环网络($2^{nd}$ RNN)的理论基础，并证明存在一类有界时间的$2^{nd}$ RNN是图灵完备的。该模型能够直接将转移表编码到其循环权重中，实现有界时间计算，并且具有可解释性。我们还证明，在有界权重和时间约束下，无记忆的二阶RNNs在识别上优于现代模型，如基本RNNs和门控循环单元。",
    "tldr": "本研究扩展了二阶循环神经网络的理论基础，并证明了存在一类有界时间的二阶RNN是图灵完备的。该模型通过将转移表编码到其循环权重中实现有界时间计算，并在识别任务上优于现代模型。",
    "en_tdlr": "This work extends the theoretical foundation of second order recurrent neural networks (RNNs) and proves the existence of a class of second order RNNs with bounded time that is Turing-complete. The model encodes a transition table into its recurrent weights, enabling bounded time computation and outperforms modern models in recognition tasks."
}