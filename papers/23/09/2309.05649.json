{
    "title": "Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)",
    "abstract": "The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.",
    "link": "http://arxiv.org/abs/2309.05649",
    "context": "Title: Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)\nAbstract: The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.",
    "path": "papers/23/09/2309.05649.json",
    "total_tokens": 819,
    "translated_title": "数据效率、维度约简和广义对称信息瓶颈",
    "translated_abstract": "对称信息瓶颈（SIB）是一种维度约简技术，它是更常见的信息瓶颈的扩展，同时压缩两个随机变量以保留它们的压缩版本之间的信息。我们引入了广义对称信息瓶颈（GSIB），探索了不同功能形式的同时约简成本。然后，我们探索了同时压缩的数据集大小需求。我们通过推导涉及损失函数的统计波动的界限和均方根估计来实现这一点。我们表明，在典型情况下，与逐个压缩变量相比，同时的GSIB压缩在达到相同误差时需要更少的数据。我们认为这是一个更一般的原则的例子，即同时压缩比独立压缩输入变量更具数据效率。",
    "tldr": "广义对称信息瓶颈是一种同时压缩两个随机变量以保留信息的维度约简技术，相较于逐个压缩变量，它需要更少的数据来达到相同的误差。"
}