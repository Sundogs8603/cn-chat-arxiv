{
    "title": "Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method. (arXiv:2309.08626v1 [cs.CL])",
    "abstract": "Inverse text normalization (ITN) is crucial for converting spoken-form into written-form, especially in the context of automatic speech recognition (ASR). While most downstream tasks of ASR rely on written-form, ASR systems often output spoken-form, highlighting the necessity for robust ITN in product-level ASR-based applications. Although neural ITN methods have shown promise, they still encounter performance challenges, particularly when dealing with ASR-generated spoken text. These challenges arise from the out-of-domain problem between training data and ASR-generated text. To address this, we propose a direct training approach that utilizes ASR-generated written or spoken text, with pairs augmented through ASR linguistic context emulation and a semi-supervised learning method enhanced by a large language model, respectively. Additionally, we introduce a post-aligning method to manage unpredictable errors, thereby enhancing the reliability of ITN. Our experiments show that our propo",
    "link": "http://arxiv.org/abs/2309.08626",
    "context": "Title: Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method. (arXiv:2309.08626v1 [cs.CL])\nAbstract: Inverse text normalization (ITN) is crucial for converting spoken-form into written-form, especially in the context of automatic speech recognition (ASR). While most downstream tasks of ASR rely on written-form, ASR systems often output spoken-form, highlighting the necessity for robust ITN in product-level ASR-based applications. Although neural ITN methods have shown promise, they still encounter performance challenges, particularly when dealing with ASR-generated spoken text. These challenges arise from the out-of-domain problem between training data and ASR-generated text. To address this, we propose a direct training approach that utilizes ASR-generated written or spoken text, with pairs augmented through ASR linguistic context emulation and a semi-supervised learning method enhanced by a large language model, respectively. Additionally, we introduce a post-aligning method to manage unpredictable errors, thereby enhancing the reliability of ITN. Our experiments show that our propo",
    "path": "papers/23/09/2309.08626.json",
    "total_tokens": 882,
    "translated_title": "通过数据增强、半监督学习和后对齐方法改进神经逆文本标准化的鲁棒性",
    "translated_abstract": "逆文本标准化（ITN）在将口语形式转化为书面形式方面非常重要，特别是在自动语音识别（ASR）的情境中。尽管神经ITN方法显示出了潜力，但在处理ASR生成的口语文本时仍然遇到性能挑战。这些挑战源于训练数据和ASR生成文本之间的领域外问题。为了解决这个问题，我们提出了一种直接训练方法，利用ASR生成的书面或口语文本，通过ASR语言上下文模拟和大型语言模型增强的半监督学习方法来增加数据对。此外，我们引入了一种后对齐方法来处理不可预测的错误，从而增强了ITN的可靠性。我们的实验证明了我们的方法可以提高ITN的性能。",
    "tldr": "本研究通过数据增强、半监督学习和后对齐方法改进神经逆文本标准化的鲁棒性，提高了自动语音识别的效果。",
    "en_tdlr": "This paper proposes a direct training approach that utilizes ASR-generated text and augments pairs through ASR linguistic context emulation and semi-supervised learning to improve the robustness of neural inverse text normalization in dealing with ASR-generated spoken text, enhancing the reliability of ITN and the performance of automatic speech recognition."
}