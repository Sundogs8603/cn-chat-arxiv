{
    "title": "Bias Propagation in Federated Learning. (arXiv:2309.02160v1 [cs.LG])",
    "abstract": "We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms t",
    "link": "http://arxiv.org/abs/2309.02160",
    "context": "Title: Bias Propagation in Federated Learning. (arXiv:2309.02160v1 [cs.LG])\nAbstract: We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms t",
    "path": "papers/23/09/2309.02160.json",
    "total_tokens": 913,
    "translated_title": "在联邦学习中的偏见传播",
    "translated_abstract": "我们展示了参与联邦学习可能对群体公平性有害。事实上，少数参与者对于被较少代表的群体（如性别或种族等敏感属性确定的群体）的偏见可以通过网络传播到所有参与者。我们在自然分区的真实数据集上分析和解释了联邦学习中的偏见传播。我们的分析揭示了具有偏见的参与者无意中但秘密地将其偏见编码到少数模型参数中，并且在整个训练过程中，它们稳步增加了全局模型对敏感属性的依赖。重要的是要强调的是，在联邦学习中经历的偏见比参与者在训练在所有数据的联合上的集中式训练中遇到的要高。这表明偏见是由算法造成的。我们的工作呼吁在联邦学习中进行群体公平性审计和设计学习算法。",
    "tldr": "我们的研究揭示了在联邦学习中的偏见传播现象，少数对于较少代表的群体的偏见可以通过网络传播到所有参与者，并且这种偏见程度高于集中式训练。该研究呼吁在联邦学习中审计群体公平性并设计相应的学习算法。",
    "en_tdlr": "Our research reveals the phenomenon of bias propagation in federated learning, where the bias of a few parties against under-represented groups can propagate through the network, resulting in a higher level of bias than centralized training. This calls for auditing group fairness in federated learning and designing appropriate learning algorithms."
}