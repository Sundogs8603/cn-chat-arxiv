{
    "title": "Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])",
    "abstract": "Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefi",
    "link": "http://arxiv.org/abs/2309.11564",
    "context": "Title: Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])\nAbstract: Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefi",
    "path": "papers/23/09/2309.11564.json",
    "total_tokens": 947,
    "translated_title": "带有自然语言子目标的层次强化学习",
    "translated_abstract": "层次强化学习一直是一种实现长序列动作目标导向行为的有吸引力的方法。然而，在现实或开放环境中实现层次强化学习是具有挑战性的。主要的挑战之一是找到适合实例化层次的子目标空间。我们提出了一种新方法，利用人类解决这些任务的数据，对3D躯体环境中一组长程任务的目标空间进行软监督。特别是，我们使用非约束的自然语言来参数化这个空间。这有两个优点：首先，可以从天真的人类参与者那里轻松生成这些数据；其次，它足够灵活，能够表示人类相关任务中的一大范围子目标。我们的方法在这些任务上优于克隆专家行为的代理和没有这种受监督子目标空间的从头开始的层次强化学习方法。我们的工作提出了一种将人类专家监督与这种受益相结合的新方法。",
    "tldr": "本文介绍了一种层次强化学习的新方法，使用来自人类解决任务的数据来监督一组长程任务的目标空间，并使用自然语言来描述这个空间，该方法在克隆专家行为的代理和无监督子目标空间的层次强化学习中表现出色。",
    "en_tdlr": "This paper presents a novel approach to hierarchical reinforcement learning, where data from humans solving tasks is used to supervise the goal space for a set of long range tasks, using unconstrained natural language to describe this space. The approach outperforms agents that clone expert behavior and hierarchical reinforcement learning without this supervised sub-goal space."
}