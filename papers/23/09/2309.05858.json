{
    "title": "Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])",
    "abstract": "Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention ",
    "link": "http://arxiv.org/abs/2309.05858",
    "context": "Title: Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])\nAbstract: Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention ",
    "path": "papers/23/09/2309.05858.json",
    "total_tokens": 921,
    "translated_title": "揭示Transformer中的mesa-optimization算法",
    "translated_abstract": "Transformer已经成为深度学习中主导的模型，但其卓越性能的原因尚不清楚。我们假设Transformer的强大性能源于其架构中对mesa-optimization的偏好，即一个学习过程在模型的前向传递中运行，由以下两个步骤组成：（i）构建内部学习目标，和（ii）通过优化找到相应的解决方案。为了验证这个假设，我们对一系列在简单序列建模任务上训练的自回归Transformer进行了逆向工程，揭示了驱动预测生成的基于梯度的底层mesa-optimization算法。此外，我们还展示了学习的前向传递优化算法可以立即被重新应用于解决监督式少样本任务，这表明mesa-optimization可能是大型语言模型的上下文学习能力的基础。最后，我们提出了一种新颖的自注意力机制。",
    "tldr": "本研究揭示了Transformer模型中的mesa-optimization算法，该算法通过内部学习目标和相应的优化解决方案驱动预测生成。研究还发现，这种学习的优化算法可以被应用于解决监督式少样本任务，暗示了mesa-optimization可能是大型语言模型上下文学习能力的基础。"
}