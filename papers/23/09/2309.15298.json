{
    "title": "Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization. (arXiv:2309.15298v1 [math.OC])",
    "abstract": "This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a sum-log-concave function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the cross-gradient and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called checkered regression method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checker",
    "link": "http://arxiv.org/abs/2309.15298",
    "context": "Title: Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization. (arXiv:2309.15298v1 [math.OC])\nAbstract: This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a sum-log-concave function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the cross-gradient and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called checkered regression method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checker",
    "path": "papers/23/09/2309.15298.json",
    "total_tokens": 928,
    "translated_title": "超越对数凹性：求解和优化和函数之和取负对数之最小化的理论和算法",
    "translated_abstract": "本文将经典的凸优化理论拓展到对求解和函数之和取负对数的函数的最小化问题上，我们称之为和函数对数凹函数。我们证明了这种函数通常不是凸函数，但仍然满足广义凸性不等式。这些不等式揭示了我们称之为交叉梯度的某个向量的重要性，该向量通常与常规梯度不同。因此，我们提出了交叉梯度下降（XGD）算法，它沿着交叉梯度的相反方向移动，并推导出收敛性分析。作为我们的和函数对数凹框架的应用，我们引入了所谓的棋盘回归方法，它依赖于和函数对数凹函数。这种分类器将（多类）逻辑回归扩展到非线性可分问题，因为它可以通过使用任意数量的超平面来分割特征空间，创建一个棋盘格。",
    "tldr": "本文拓展了凸优化理论，提出了求解和函数对数凹函数最小化的算法，并应用于棋盘回归方法，扩展了逻辑回归到非线性可分问题。"
}