{
    "title": "Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions. (arXiv:2309.08625v1 [cs.CL])",
    "abstract": "As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given. Therefore, this study aims to investigate the impact of medical data mixed with small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3 questions were used as a model for relevant medical data. We use both multiple choice and open ended questions. We gathered small talk sentences from human participants using the Mechanical Turk platform. Both sets of USLME questions were arranged in a pattern where each sentence from the original questions was followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both sets of questions with and without the small talk sentences. A board-certified physician analyzed the answers by ChatGPT and compared them to the formal correct answer. The analysis results demonstrate that the ability of ChatGP",
    "link": "http://arxiv.org/abs/2309.08625",
    "context": "Title: Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions. (arXiv:2309.08625v1 [cs.CL])\nAbstract: As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given. Therefore, this study aims to investigate the impact of medical data mixed with small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3 questions were used as a model for relevant medical data. We use both multiple choice and open ended questions. We gathered small talk sentences from human participants using the Mechanical Turk platform. Both sets of USLME questions were arranged in a pattern where each sentence from the original questions was followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both sets of questions with and without the small talk sentences. A board-certified physician analyzed the answers by ChatGPT and compared them to the formal correct answer. The analysis results demonstrate that the ability of ChatGP",
    "path": "papers/23/09/2309.08625.json",
    "total_tokens": 944,
    "translated_title": "ChatGPT-3.5和GPT-4在带有干扰和不带干扰的美国医师执照考试上的表现",
    "translated_abstract": "由于大型语言模型（LLMs）是基于提示中的单词构建响应的预测模型，因此存在着闲聊和无关信息可能改变响应和建议的风险。因此，本研究旨在调查混合了闲聊的医疗数据对ChatGPT提供的医学建议准确性的影响。我们使用USMLE第3步问题作为相关医学数据的模型，包括多项选择题和开放性问题。我们通过机械土耳其平台从人类参与者那里收集了闲聊句子。两组USLME问题按照一种模式排列，即原始问题的每个句子后跟一个闲聊句子。要求ChatGPT 3.5和4回答带有和不带有闲聊句子的两组问题。一名经过认证的医生分析了ChatGPT的答案，并将其与正确定答案进行了比较。分析结果表明，ChatGPT对带有干扰的问题的回答能力有所降低。",
    "tldr": "本研究调查了ChatGPT在带有闲聊句子和不带闲聊句子的情况下对医学建议准确性的影响，结果显示带有干扰的问题的回答能力有所降低。",
    "en_tdlr": "This study investigates the impact of small talk mixed with medical data on the accuracy of medical advice provided by ChatGPT, showing a decrease in performance when dealing with questions containing distractions."
}