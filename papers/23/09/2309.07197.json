{
    "title": "Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])",
    "abstract": "The main premise of federated learning (FL) is that machine learning model updates are computed locally to preserve user data privacy. This approach avoids by design user data to ever leave the perimeter of their device. Once the updates aggregated, the model is broadcast to all nodes in the federation. However, without proper defenses, compromised nodes can probe the model inside their local memory in search for adversarial examples, which can lead to dangerous real-world scenarios. For instance, in image-based applications, adversarial examples consist of images slightly perturbed to the human eye getting misclassified by the local model. These adversarial images are then later presented to a victim node's counterpart model to replay the attack. Typical examples harness dissemination strategies such as altered traffic signs (patch attacks) no longer recognized by autonomous vehicles or seemingly unaltered samples that poison the local dataset of the FL scheme to undermine its robustn",
    "link": "http://arxiv.org/abs/2309.07197",
    "context": "Title: Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])\nAbstract: The main premise of federated learning (FL) is that machine learning model updates are computed locally to preserve user data privacy. This approach avoids by design user data to ever leave the perimeter of their device. Once the updates aggregated, the model is broadcast to all nodes in the federation. However, without proper defenses, compromised nodes can probe the model inside their local memory in search for adversarial examples, which can lead to dangerous real-world scenarios. For instance, in image-based applications, adversarial examples consist of images slightly perturbed to the human eye getting misclassified by the local model. These adversarial images are then later presented to a victim node's counterpart model to replay the attack. Typical examples harness dissemination strategies such as altered traffic signs (patch attacks) no longer recognized by autonomous vehicles or seemingly unaltered samples that poison the local dataset of the FL scheme to undermine its robustn",
    "path": "papers/23/09/2309.07197.json",
    "total_tokens": 973,
    "translated_title": "在受信任的执行环境中减轻联邦学习中的对抗性攻击",
    "translated_abstract": "联邦学习的主要前提是本地计算机学习模型更新，以保护用户数据隐私。这种方法设计上避免了用户数据离开其设备的情况。一旦更新汇总，模型将广播到联邦中的所有节点。然而，如果没有适当的防御措施，被入侵的节点可以在其本地内存中探测模型，寻找对抗性示例，这可能导致危险的现实世界场景。例如，在基于图像的应用程序中，对抗性示例包含微微扰动的图像，人眼无法察觉，但被本地模型错误分类。然后，这些对抗性图像稍后会被呈现给受害节点的对应模型，以重放攻击。典型的示例利用了传播策略，如被更改的交通标志（修补攻击），这些标志不再被自动驾驶车辆识别，或者看似未被更改的样本，这些样本会破坏联邦学习方案的鲁棒性，污染本地数据集。",
    "tldr": "在联邦学习中，通过使用受信任的执行环境，可以减轻被入侵节点利用对抗性攻击来探测模型的风险。这可以避免危险的现实世界场景，如被篡改的交通标志导致自动驾驶车辆错误识别，或者被污染的本地数据集。",
    "en_tdlr": "Mitigating the risk of compromised nodes probing the model in federated learning by utilizing trusted execution environments, thus avoiding dangerous real-world scenarios such as misclassification due to altered traffic signs in autonomous vehicles or contaminated local datasets."
}