{
    "title": "Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])",
    "abstract": "Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. ",
    "link": "http://arxiv.org/abs/2309.08748",
    "context": "Title: Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])\nAbstract: Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. ",
    "path": "papers/23/09/2309.08748.json",
    "total_tokens": 854,
    "translated_title": "Wasserstein分布保证的策略评估和学习在上下文乐队中",
    "translated_abstract": "在没有与环境直接互动的情况下，数据收集的环境通常与学习的策略应用的环境不同。为了在学习和执行过程中考虑不同环境的影响，我们提出了一种使用Wasserstein距离的新型分布保证优化(DRO)方法，该方法在假设新环境的分布位于不确定集合内时，计算策略值的最坏情况下界。典型地，这个不确定集合是基于从日志数据集中计算的经验分布的KL散度定义的。然而，KL不确定集合无法包含具有不同支持的分布，也缺乏对分布支持的几何感知。结果，KL方法在解决实际环境不匹配和导致过度拟合最坏情况方面存在不足。为了克服这些限制，我们提出了一种使用Wasserstein距离的新型DRO方法。",
    "tldr": "通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。",
    "en_tdlr": "We propose a novel distributionally robust optimization (DRO) method using Wasserstein distance instead of KL divergence to address practical environment mismatches and overfitting to worst-case scenarios in contextual bandits."
}