{
    "title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models. (arXiv:2309.14763v1 [cs.CL])",
    "abstract": "Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module sele",
    "link": "http://arxiv.org/abs/2309.14763",
    "context": "Title: ConPET: Continual Parameter-Efficient Tuning for Large Language Models. (arXiv:2309.14763v1 [cs.CL])\nAbstract: Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module sele",
    "path": "papers/23/09/2309.14763.json",
    "total_tokens": 845,
    "translated_title": "ConPET: 对大型语言模型的持续参数高效调整",
    "translated_abstract": "持续学习需要对模型进行持续调整以适应新出现的任务，同时最大限度地减少对旧任务的灾难性遗忘。对于具有全参数调整的大型语言模型（LLMs）来说，这是极具挑战性的，原因是计算成本高、内存消耗大且容易遗忘。受参数高效调整（PET）的成功启发，我们提出了持续参数高效调整（ConPET），这是一种适用于LLMs的具有任务数量无关训练复杂度的延续任务适应的通用范式。ConPET包括两个版本，适用于不同的应用场景。首先，静态ConPET可以通过PET和动态回放策略将原本针对较小模型设计的旧的持续学习方法应用到LLMs中，从而大大减少调整成本，缓解过度拟合和遗忘问题。此外，为了保持可伸缩性，动态ConPET为不同任务采用单独的PET模块和PET模块选择策略。",
    "tldr": "ConPET是一种适用于大型语言模型的持续参数高效调整方法，通过参数高效调整（PET）和动态回放策略，减少调整成本、缓解过度拟合和遗忘问题。"
}