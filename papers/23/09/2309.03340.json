{
    "title": "Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation. (arXiv:2309.03340v1 [cs.CL])",
    "abstract": "There has been significant research on developing pretrained transformer architectures for multimodal-to-text generation tasks. Albeit performance improvements, such models are frequently overparameterized, hence suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. In this paper, we address both these issues for the application of automated audio captioning. First, we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio-text shared latent space is suitable for detecting hallucination. Then, we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. During the beam decoding step, the smaller model utilizes an audio-text shared latent representation to semantically align the generated text with corresponding input audio. Faithful guidance ",
    "link": "http://arxiv.org/abs/2309.03340",
    "context": "Title: Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation. (arXiv:2309.03340v1 [cs.CL])\nAbstract: There has been significant research on developing pretrained transformer architectures for multimodal-to-text generation tasks. Albeit performance improvements, such models are frequently overparameterized, hence suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. In this paper, we address both these issues for the application of automated audio captioning. First, we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio-text shared latent space is suitable for detecting hallucination. Then, we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. During the beam decoding step, the smaller model utilizes an audio-text shared latent representation to semantically align the generated text with corresponding input audio. Faithful guidance ",
    "path": "papers/23/09/2309.03340.json",
    "total_tokens": 891,
    "translated_title": "使用音频-文本共享潜在表示的参数高效音频字幕生成方法",
    "translated_abstract": "在多模态文本生成任务中，已有大量的研究专注于开发预训练的Transformer架构。然而，尽管性能有所提升，这些模型往往过于参数化，因此容易出现幻觉和内存占用过大的问题，使它们在边缘设备上部署具有挑战性。本文针对自动化音频字幕应用中的这两个问题进行了研究。首先，我们提出了一种用于生成幻觉音频字幕的数据增强技术，并证明了基于音频-文本共享潜在空间的相似性对于检测幻觉是合适的。然后，我们提出了一种参数高效的推理时准确解码算法，使较小的音频字幕生成模型能够与使用更多数据训练的较大模型的性能相当。在束搜索解码步骤中，较小的模型利用音频-文本共享潜在表示来语义对齐生成的文本与相应的输入音频。准确的引导",
    "tldr": "本文提出了一种参数高效的音频字幕生成方法，通过使用音频-文本共享潜在表示来检测幻觉，并采用准确解码算法，使较小的模型能够实现与较大模型相当的性能。",
    "en_tdlr": "This paper presents a parameter-efficient method for audio captioning, which detects hallucination using audio-text shared latent representation and utilizes an accurate decoding algorithm, enabling smaller models to achieve performance equivalent to larger models."
}