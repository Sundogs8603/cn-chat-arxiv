{
    "title": "VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])",
    "abstract": "The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.",
    "link": "http://arxiv.org/abs/2309.07544",
    "context": "Title: VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])\nAbstract: The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.",
    "path": "papers/23/09/2309.07544.json",
    "total_tokens": 885,
    "translated_title": "VerilogEval：评估大型语言模型在Verilog代码生成中的性能",
    "translated_abstract": "大型语言模型（LLMs）的日益普及为它们在各个领域的应用铺平了道路。本文提出了一个特别针对Verilog代码生成性能评估的基准框架，用于硬件设计和验证。我们提供了一个包含来自Verilog教学网站HDLBits的156个问题的综合评估数据集。该评估集包含了各种Verilog代码生成任务，从简单的组合电路到复杂的有限状态机。可以通过将生成的设计的瞬态仿真输出与黄金解决方案进行比较，自动测试Verilog代码完成的功能正确性。我们还证明了预训练语言模型的Verilog代码生成能力可以通过使用LLM生成的合成问题-代码对进行监督微调来改善。",
    "tldr": "本文提出了一个专门用于评估大型语言模型在Verilog代码生成中的性能的基准框架，并提供了一个包含156个问题的综合评估数据集。通过与黄金解决方案进行比较，可以自动测试Verilog代码的功能正确性。通过使用LLM生成的合成问题-代码对进行监督微调，可以改善预训练语言模型的Verilog代码生成能力。",
    "en_tdlr": "This paper proposes a benchmarking framework for evaluating large language models (LLMs) in the context of Verilog code generation, and provides a comprehensive evaluation dataset with 156 problems. By comparing with a golden solution, the functional correctness of Verilog code can be automatically tested. The Verilog code generation capability of pretrained language models can be improved through supervised fine-tuning with LLM generated synthetic problem-code pairs."
}