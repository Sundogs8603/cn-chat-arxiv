{
    "title": "Information Leakage from Data Updates in Machine Learning Models. (arXiv:2309.11022v1 [cs.LG])",
    "abstract": "In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the update",
    "link": "http://arxiv.org/abs/2309.11022",
    "context": "Title: Information Leakage from Data Updates in Machine Learning Models. (arXiv:2309.11022v1 [cs.LG])\nAbstract: In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the update",
    "path": "papers/23/09/2309.11022.json",
    "total_tokens": 844,
    "translated_title": "机器学习模型中数据更新的信息泄露",
    "translated_abstract": "本文考虑了在更新数据集时重新训练机器学习模型的情景，以便将最新信息纳入模型或反映分布变化。我们研究了是否可以推断出关于这些更新的信息（例如，记录属性值的更改）。在这里，攻击者可以访问数据集变化前后的机器学习模型的快照。与现有文献相反，我们假设一个或多个训练数据点的属性发生改变，而不是整个数据记录被删除或添加。我们提出了基于原始模型和更新模型预测置信度的攻击方法。我们在两个公共数据集上评估了我们的攻击方法，采用了多层感知器和逻辑回归模型。我们验证了模型的两个快照与仅访问更新数据相比会导致更高的信息泄露。",
    "tldr": "本文研究了在机器学习模型中进行数据更新时可能发生的信息泄露问题，并提出了基于预测置信度差异的攻击方法。实验证实了两个模型快照相对于仅访问更新数据会导致更高的信息泄露。",
    "en_tdlr": "This paper investigates the issue of information leakage during data updates in machine learning models, and proposes attacks based on the difference in prediction confidence. The experiments validate that having two model snapshots leads to higher information leakage compared to only having access to the updated data."
}