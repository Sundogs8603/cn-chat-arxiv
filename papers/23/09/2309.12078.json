{
    "title": "Clustering-based Domain-Incremental Learning. (arXiv:2309.12078v1 [cs.LG])",
    "abstract": "We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called \"catastrophic forgetting problem\", in which the performance of the learner in an \"old task\" decreases when subsequently trained on a \"new task\". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the har",
    "link": "http://arxiv.org/abs/2309.12078",
    "context": "Title: Clustering-based Domain-Incremental Learning. (arXiv:2309.12078v1 [cs.LG])\nAbstract: We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called \"catastrophic forgetting problem\", in which the performance of the learner in an \"old task\" decreases when subsequently trained on a \"new task\". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the har",
    "path": "papers/23/09/2309.12078.json",
    "total_tokens": 858,
    "translated_title": "基于聚类的领域增量学习",
    "translated_abstract": "本文考虑在连续学习场景中学习多个任务的问题，其中来自不同任务的数据以流式呈现给学习器。在这种设置下的一个关键挑战是所谓的“灾难性遗忘问题”，即在“旧任务”上进行后续训练时学习器在“新任务”上的性能下降。现有的连续学习方法，如平均梯度情节性记忆（A-GEM）和正交梯度下降（OGD），通过最小化当前任务的损失而不增加先前任务的损失来解决灾难性遗忘问题。然而，这些方法假设学习器知道任务何时改变，这在实践中是不现实的。本文通过在动态更新的有限样本或梯度池中使用在线聚类方法来消除为算法提供有关任务更改的信息的需求。从而成功地对抗了其中一个困难的灾难性遗忘情况。",
    "tldr": "本文提出了一个基于聚类的方法来解决连续学习中的灾难性遗忘问题，该方法不需要提供任务更改的信息，并通过在线聚类方法有效地对抗灾难性遗忘。",
    "en_tdlr": "This paper proposes a clustering-based approach to address catastrophic forgetting in continual learning, which does not require providing information about task changes and effectively counteracts it through online clustering."
}