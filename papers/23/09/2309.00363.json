{
    "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning. (arXiv:2309.00363v1 [cs.LG])",
    "abstract": "LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, fede",
    "link": "http://arxiv.org/abs/2309.00363",
    "context": "Title: FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning. (arXiv:2309.00363v1 [cs.LG])\nAbstract: LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, fede",
    "path": "papers/23/09/2309.00363.json",
    "total_tokens": 1071,
    "translated_title": "FederatedScope-LLM: 一个全面的套装，用于在联邦学习中优化大型语言模型的微调",
    "translated_abstract": "在各种自然语言处理任务中，大型语言模型(LLMs)展示了强大的能力。通过对LLMs进行微调，不同的实体可以进一步提高它们在特定下游任务上的性能。当有多个实体有相似的感兴趣任务，但由于隐私关注规定而无法共享数据时，联邦学习(FL)是利用不同实体数据的主要解决方案。然而，在联邦学习设置下对LLMs进行微调仍然缺乏现有联邦学习框架的充分支持，因为它必须处理优化重要通信和计算资源的消耗、不同任务的数据准备以及不同的信息保护需求。本文首先讨论了联邦微调LLMs所面临的挑战，并介绍了我们的FS-LLM套装作为主要贡献，其中包括以下组件：(1)我们建立了一个端到端的基准测试流水线，自动化数据集预处理、联邦学习模型训练和评估的流程；(2)我们提供了合理的策略来优化通信和计算资源的消耗；(3)我们实现了数据预处理的模块，并针对不同任务提供了适应性的数据预处理方法；(4)我们设计了一种保护信息隐私的机制，以满足不同实体的需求。",
    "tldr": "本论文提出了一个名为FS-LLM的套装，用于在联邦学习中优化大型语言模型的微调。它解决了在联邦学习设置下对LLMs进行微调面临的挑战，并提供了自动化流水线、优化资源消耗的策略、适应性的数据预处理方法和信息隐私保护机制。"
}