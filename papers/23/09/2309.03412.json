{
    "title": "From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v1 [cs.CL])",
    "abstract": "Instruction tuning is essential for large language models (LLMs) to become interactive. While many instruction tuning datasets exist in English, there is a noticeable lack in other languages. Also, their effectiveness has not been well verified in non-English languages. We construct a Japanese instruction dataset by expanding and filtering existing datasets and apply the dataset to a Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning on both Japanese and English existing models using our instruction dataset. We evaluated these models from both quantitative and qualitative perspectives. As a result, the effectiveness of Japanese instruction datasets is confirmed. The results also indicate that even with relatively small LLMs, performances in downstream tasks would be improved through instruction tuning. Our instruction dataset, tuned models, and implementation are publicly available online.",
    "link": "http://arxiv.org/abs/2309.03412",
    "context": "Title: From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v1 [cs.CL])\nAbstract: Instruction tuning is essential for large language models (LLMs) to become interactive. While many instruction tuning datasets exist in English, there is a noticeable lack in other languages. Also, their effectiveness has not been well verified in non-English languages. We construct a Japanese instruction dataset by expanding and filtering existing datasets and apply the dataset to a Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning on both Japanese and English existing models using our instruction dataset. We evaluated these models from both quantitative and qualitative perspectives. As a result, the effectiveness of Japanese instruction datasets is confirmed. The results also indicate that even with relatively small LLMs, performances in downstream tasks would be improved through instruction tuning. Our instruction dataset, tuned models, and implementation are publicly available online.",
    "path": "papers/23/09/2309.03412.json",
    "total_tokens": 807,
    "translated_title": "从基础到对话式：日语指令数据集和调整大型语言模型",
    "translated_abstract": "对于大型语言模型（LLMs）能够变得交互性来说，指令调整是至关重要的。尽管存在许多英文的指令调整数据集，但其他语言缺乏明显。而且，它们在非英语语言中的效果尚未得到很好的验证。我们通过扩展和筛选现有数据集构建了一个日语指令数据集，并将该数据集应用于一个日语预训练基础模型。我们使用我们的指令数据集对日语和英语现有模型进行了低秩适应（LoRA）调整。我们从数量和质量两个角度评估了这些模型。结果确认了日语指令数据集的有效性。结果还表明，即使是相对较小的LLMs，通过指令调整也能提高下游任务的性能。我们的指令数据集、调整模型和实现均可在网上公开获取。",
    "tldr": "通过构建日语指令数据集并进行指令调整，验证了日语指令数据集的有效性，并表明通过指令调整可以提高下游任务的性能。"
}