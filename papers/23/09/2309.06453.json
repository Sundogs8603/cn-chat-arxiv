{
    "title": "Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])",
    "abstract": "Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer \"What happens during the training process that leads to the performance gap?\" and \"How can the performance gap be narrowed?\". In this paper, we conduct empirical experiments to answer these \"What\" and \"How\" questions. We first answer the \"What\" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o",
    "link": "http://arxiv.org/abs/2309.06453",
    "context": "Title: Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])\nAbstract: Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer \"What happens during the training process that leads to the performance gap?\" and \"How can the performance gap be narrowed?\". In this paper, we conduct empirical experiments to answer these \"What\" and \"How\" questions. We first answer the \"What\" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o",
    "path": "papers/23/09/2309.06453.json",
    "total_tokens": 776,
    "translated_title": "缩小监督和无监督句子表示学习的差距：大规模语言模型",
    "translated_abstract": "句子表示学习是自然语言处理中的一项基本任务，对比学习的句子嵌入（CSE）作为主流技术具有出色的性能。然而，在CSE中有一个有趣的现象，即监督和无监督方法之间存在显著的性能差距，即使它们的句子编码器和损失函数相同。本文通过实证实验回答“发生了什么导致了性能差距”和“如何缩小性能差距”的问题。我们首先通过彻底比较监督和无监督CSE在各自的训练过程中的行为来回答“发生了什么”这个问题。",
    "tldr": "本文通过实验比较了监督和无监督句子表示学习在训练过程中的行为，并探讨了如何缩小性能差距。"
}