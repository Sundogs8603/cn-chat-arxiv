{
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])",
    "abstract": "Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2",
    "link": "http://arxiv.org/abs/2309.16014",
    "context": "Title: Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])\nAbstract: Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2",
    "path": "papers/23/09/2309.16014.json",
    "total_tokens": 886,
    "translated_title": "用联合嵌入预测架构进行图级表示学习",
    "translated_abstract": "联合嵌入预测架构（JEPAs）作为一种新颖而强大的自监督表示学习技术最近出现。它们旨在通过从上下文信号x中预测目标信号y的潜在表示来学习基于能量的模型。JEPAs绕过了对数据增强和负样本的需求，这通常是对比学习所要求的，同时避免了与生成式预训练相关的过拟合问题。在本文中，我们展示了该范式可以有效地对图级表示进行建模，并提出了Graph-JEPA，这是图领域的第一个JEPA。特别是，我们采用掩码建模的方式来学习输入图的不同子图的嵌入。为了赋予表示隐含的层次结构，我们设计了一种替代性的训练目标，该目标是预测编码子图在2维单位双曲线上的坐标。",
    "tldr": "本文提出了一种用于图级表示学习的联合嵌入预测架构（JEPA），通过预测输入图的不同子图的潜在表示在2维单位双曲线上的坐标，实现了对图级表示的有效建模。",
    "en_tdlr": "This paper proposes a Joint-Embedding Predictive Architecture (JEPA) for graph-level representation learning. By predicting the latent representation of different subgraphs of the input graph on a 2D hyperbola, it effectively models the graph-level representations."
}