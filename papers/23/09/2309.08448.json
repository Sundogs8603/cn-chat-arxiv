{
    "title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])",
    "abstract": "The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models' capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwa",
    "link": "http://arxiv.org/abs/2309.08448",
    "context": "Title: Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])\nAbstract: The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models' capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwa",
    "path": "papers/23/09/2309.08448.json",
    "total_tokens": 915,
    "translated_title": "推进传统中文语言模型评估：迈向全面基准套件",
    "translated_abstract": "在语言理解和生成领域中，评估大型语言模型是至关重要的任务。随着语言模型的不断发展，评估其性能的有效基准的需求变得迫切。在中文语境下，尽管存在一些基准数据集如DRCD、TTQA、CMDQA和FGC，但缺乏全面多样的基准数据集来评估语言模型的能力。为了弥补这一空白，我们提出了一套新的基准数据集，利用现有的英文数据集，针对传统中文语言模型进行评估。这些基准数据集涵盖了广泛的任务，包括上下文问答、摘要、分类和表格理解。所提出的基准数据集提供了一个全面的评估框架，可以评估语言模型在不同任务下的能力。本文中，我们评估了GPT-3.5和Taiwa的性能。",
    "tldr": "本论文提出了一套新的基准数据集，利用现有的英文数据集，针对传统中文语言模型进行全面评估。这些基准数据集涵盖了上下文问答、摘要、分类和表格理解等多个任务，为评估语言模型在不同任务下的能力提供了全面的评估框架。"
}