{
    "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)",
    "abstract": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluation",
    "link": "http://arxiv.org/abs/2309.17446",
    "context": "Title: L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)\nAbstract: Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluation",
    "path": "papers/23/09/2309.17446.json",
    "total_tokens": 853,
    "translated_title": "L2CEval:评估大型语言模型的语言到代码生成能力",
    "translated_abstract": "最近，特别是那些在代码上预训练的大型语言模型（LLM）已经展示出了在几次训练甚至零次训练的情况下，从自然语言输入生成程序的强大能力。尽管有着有希望的结果，但对于这些模型的语言到代码生成能力缺乏全面的评估。现有的研究往往集中在特定任务、模型架构或学习范式上，导致对整体情况的理解零散。在这项工作中，我们提出了L2CEval，对LLM在语义解析、数学推理和Python编程的7个任务上的语言到代码生成能力进行系统评估，并分析可能影响其性能的因素，如模型大小、预训练数据、指令调整和不同的提示方法。除了评估模型性能，我们还对模型进行了置信度校准的测量和人工评估。",
    "tldr": "L2CEval是对大型语言模型的语言到代码生成能力进行系统评估的工作，分析了影响其性能的因素，并对置信度校准和人工评估进行了测量。",
    "en_tdlr": "L2CEval is a systematic evaluation of the language-to-code generation capabilities of large language models (LLMs), analyzing factors that affect their performance and conducting measurements on confidence calibration and human evaluation."
}