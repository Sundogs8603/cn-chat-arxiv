{
    "title": "On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])",
    "abstract": "The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat",
    "link": "http://arxiv.org/abs/2309.15075",
    "context": "Title: On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])\nAbstract: The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat",
    "path": "papers/23/09/2309.15075.json",
    "total_tokens": 904,
    "translated_title": "关于神经网络分类器超额风险收敛速率的研究",
    "translated_abstract": "最近神经网络在模式识别和分类问题上的成功表明，与其他更经典的分类器（如SVM或boosting分类器）相比，神经网络具有独特的特点。本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过其超额风险来衡量。与文献中所规定的典型条件相比，我们考虑了一个更一般的场景，它在两个方面与实际应用类似：首先，要近似的函数类包括了Barron函数作为正子集；其次，构建的神经网络分类器是通过最小化一个替代损失函数而不是0-1损失函数来实现的，从而可以轻松应用基于梯度下降的数值优化方法。虽然我们考虑的函数类非常大，最优速率不能超过$n^{-\\frac{1}{3}}$，但在这种情况下，无维度速率是可能的。",
    "tldr": "本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过超额风险来衡量。研究考虑了更一般的场景，使得神经网络可以轻松应用数值优化方法。虽然函数类很大，但无维度速率是可能的。",
    "en_tdlr": "This paper investigates the performance of plug-in classifiers based on neural networks in binary classification, measured by excess risks. The study considers a more general scenario that allows for easy application of numerical optimization methods. Although the function class is large, dimension-free rates are possible."
}