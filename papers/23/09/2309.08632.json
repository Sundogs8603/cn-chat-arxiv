{
    "title": "Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])",
    "abstract": "Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \\textbf{phi-CTNL} (pronounced ``fictional\") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.",
    "link": "http://arxiv.org/abs/2309.08632",
    "context": "Title: Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])\nAbstract: Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \\textbf{phi-CTNL} (pronounced ``fictional\") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.",
    "path": "papers/23/09/2309.08632.json",
    "total_tokens": 787,
    "translated_title": "在测试集上进行预训练就足够了",
    "translated_abstract": "受到最近有关使用小型Transformer语言模型在经过精心策划的数据上进行预训练的潜力展示的工作的启发，我们通过大量精心构建仅基于评估基准的新颖高质量的非合成数据混合来加强这种方法。使用我们的新颖数据集混合，其中包含不到10万个token，我们预训练了一个拥有100万参数的基于Transformer的LLM模型phi-CTNL（读作“fictional”），在各种学术基准测试中取得了完美的结果，严格超越了所有已知的基准模型。phi-CTNL还超越了幂律缩放，并展现出前所未见的类似grokking的能力，准确预测下游评估基准的canaries。",
    "tldr": "这项研究通过在测试集上进行预训练，使用精心构建的非合成数据混合，成功开发出一个在多个学术基准测试上表现出色的Transformer语言模型phi-CTNL。",
    "en_tdlr": "This research presents the successful development of phi-CTNL, a Transformer language model, through pretraining on the test set using carefully curated non-synthetic data mixture. It outperforms known benchmark models across multiple academic benchmarks and exhibits a novel grokking-like ability to accurately predict downstream evaluation benchmarks' canaries."
}