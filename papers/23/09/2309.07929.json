{
    "title": "Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])",
    "abstract": "Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain ",
    "link": "http://arxiv.org/abs/2309.07929",
    "context": "Title: Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])\nAbstract: Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain ",
    "path": "papers/23/09/2309.07929.json",
    "total_tokens": 960,
    "translated_title": "使用声音提示进行分割的泛化音频-视觉源定位器",
    "translated_abstract": "在从未同时看到物体和听到其声音的情况下，模型是否仍然能够准确地从输入音频中定位其视觉位置？在这项工作中，我们关注零样本和少样本情况下的音频-视觉定位和分割任务。为了实现这个目标，我们引入了编码器提示解码器的范式，与现有方法不同，现有方法主要使用编码器融合解码器范式从融合音频-视觉特征中解码定位信息，我们旨在借助预训练模型的丰富知识来更好地适应数据稀缺性和不同数据分布的困境。具体地，我们首先提出构建语义感知音频提示（SAP）来帮助视觉基础模型关注有声对象，同时也鼓励视觉和音频模态之间的语义差距缩小。然后，我们开发了一个相关适配器（ColA）来保持最小的训练工作量并维持模型性能。",
    "tldr": "本研究提出了一种使用声音提示进行分割的泛化音频-视觉源定位器，在零样本和少样本情况下实现音频-视觉定位和分割任务。通过引入编码器提示解码器范式、构建语义感知音频提示和相关适配器来解决数据稀缺性和不同数据分布的困境。",
    "en_tdlr": "This study presents a generalizable audio-visual source localizer that uses sound prompting for segmentation, achieving audio-visual localization and segmentation tasks in zero-shot and few-shot scenarios. The approach introduces the encoder-prompt-decoder paradigm and utilizes semantic-aware audio prompts and correlation adapters to address the challenges of data scarcity and varying data distribution."
}