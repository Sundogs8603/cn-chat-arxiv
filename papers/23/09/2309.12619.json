{
    "title": "Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])",
    "abstract": "Neural language models often fail to generate diverse and informative texts, limiting their applicability in real-world problems. While previous approaches have proposed to address these issues by identifying and penalizing undesirable behaviors (e.g., repetition, overuse of frequent words) from language models, we propose an alternative approach based on an observation: models primarily learn attributes within examples that are likely to cause degeneration problems. Based on this observation, we propose a new approach to prevent degeneration problems by training two models. Specifically, we first train a model that is designed to amplify undesirable patterns. We then enhance the diversity of the second model by focusing on patterns that the first model fails to learn. Extensive experiments on two tasks, namely language modeling and dialogue generation, demonstrate the effectiveness of our approach.",
    "link": "http://arxiv.org/abs/2309.12619",
    "context": "Title: Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])\nAbstract: Neural language models often fail to generate diverse and informative texts, limiting their applicability in real-world problems. While previous approaches have proposed to address these issues by identifying and penalizing undesirable behaviors (e.g., repetition, overuse of frequent words) from language models, we propose an alternative approach based on an observation: models primarily learn attributes within examples that are likely to cause degeneration problems. Based on this observation, we propose a new approach to prevent degeneration problems by training two models. Specifically, we first train a model that is designed to amplify undesirable patterns. We then enhance the diversity of the second model by focusing on patterns that the first model fails to learn. Extensive experiments on two tasks, namely language modeling and dialogue generation, demonstrate the effectiveness of our approach.",
    "path": "papers/23/09/2309.12619.json",
    "total_tokens": 792,
    "translated_title": "通过退化模型学习多样化神经文本生成",
    "translated_abstract": "神经语言模型在生成多样化且信息丰富的文本方面往往失败，限制了它们在实际问题中的适用性。虽然先前的方法通过识别和惩罚语言模型中的不良行为（例如重复、过度使用高频词汇）来解决这些问题，但我们提出了一种基于观察结果的替代方法：模型主要学习可能导致退化问题的示例中的属性。基于这一观察结果，我们提出了一种通过训练两个模型来预防退化问题的新方法。具体而言，我们首先训练一个旨在放大不良模式的模型。然后，通过专注于第一个模型未能学习到的模式来增强第二个模型的多样性。对语言建模和对话生成这两个任务的大量实验证明了我们方法的有效性。",
    "tldr": "我们通过训练两个模型，一个放大不良模式，一个增强多样性，来解决神经文本生成中多样性不足的问题。实验证明了我们方法的有效性。",
    "en_tdlr": "We address the lack of diversity in neural text generation by training two models: one amplifies undesirable patterns and the other enhances diversity. Experimental results demonstrate the effectiveness of our approach."
}