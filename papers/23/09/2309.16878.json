{
    "title": "Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])",
    "abstract": "Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m",
    "link": "http://arxiv.org/abs/2309.16878",
    "context": "Title: Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])\nAbstract: Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m",
    "path": "papers/23/09/2309.16878.json",
    "total_tokens": 894,
    "translated_title": "揭示对抗扰动中隐藏的人类可识别特征",
    "translated_abstract": "神经网络在各种机器学习任务中表现出色，但对抗扰动并不免疫。这种脆弱性对实际应用有着重要影响。虽然已经进行了大量的研究，但神经网络为何易受对抗攻击的根本原因尚未完全理解。我们的研究的核心是探索对抗扰动中的人类可识别特征，我们使用了五种攻击算法和三个数据集进行实验。此外，我们发现了在人类可识别特征中表现出的两种不同效应。在非定向攻击中，掩蔽效应更为显著，而在定向攻击中，生成效应更为常见。通过像素级注释，我们提取这些特征并证明它们能够破坏目标模型。此外，我们的研究结果表明，多个攻击算法生成的扰动在平均情况下存在显著的相似性。",
    "tldr": "该研究揭示了对抗扰动中隐藏的人类可识别特征，并发现了掩蔽效应和生成效应在不同类型攻击中的表现。通过分析多个攻击算法生成的扰动，发现它们在一定程度上存在相似性。",
    "en_tdlr": "This study uncovers human-identifiable features hidden in adversarial perturbations and identifies two distinct effects in different types of attacks. It also reveals the existence of similarities among perturbations generated by different attack algorithms."
}