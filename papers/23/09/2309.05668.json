{
    "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])",
    "abstract": "In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their per",
    "link": "http://arxiv.org/abs/2309.05668",
    "context": "Title: Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])\nAbstract: In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their per",
    "path": "papers/23/09/2309.05668.json",
    "total_tokens": 899,
    "translated_title": "研究使用ChatGPT生成文本进行预训练对下游任务的影响",
    "translated_abstract": "最近，在语言模型领域取得了显著进展，特别是随着大型语言模型（LLM）的出现，这些模型使用从互联网档案中提取的大量数据进行训练。这些LLM，例如ChatGPT，已广泛可用，使用户能够为各种目的生成文本，包括文章、论文、笑话和诗歌。由于LLM是在涵盖Reddit和Twitter等平台的各种文本来源上进行训练的，可以预见未来的训练数据集还将包含前几个模型生成的文本。鉴于此发展，我们的研究旨在探究在语言模型的预训练阶段中人造文本的影响。具体而言，我们对一个使用CNN/DailyMail新闻文章进行预训练的语言模型RoBERTa和一个使用相同文章进行训练的ChatGPT进行了比较分析，并评估了它们的表现。",
    "tldr": "本研究调查了在语言模型的预训练阶段中使用ChatGPT生成文本的人造文本的影响，通过比较分析了使用CNN/DailyMail新闻文章预训练的RoBERTa和使用相同文章预训练的ChatGPT的性能。",
    "en_tdlr": "This research investigates the impact of using artificially generated text from ChatGPT in the pre-training phase of language models, and compares the performance between RoBERTa pre-trained on CNN/DailyMail news articles and ChatGPT pre-trained on the same articles."
}