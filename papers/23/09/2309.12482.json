{
    "title": "State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])",
    "abstract": "With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining \"concepts\" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St",
    "link": "http://arxiv.org/abs/2309.12482",
    "context": "Title: State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])\nAbstract: With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining \"concepts\" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St",
    "path": "papers/23/09/2309.12482.json",
    "total_tokens": 921,
    "translated_title": "State2Explanation:基于概念的解释：有利于Agent学习和用户理解",
    "translated_abstract": "随着非AI专家使用更复杂的AI系统来完成日常任务，人们越来越努力开发能够为非AI专家理解的AI决策提供解释的方法。为了实现这个目标，利用高级概念并生成基于概念的解释已经成为一种流行的方法。大多数基于概念的解释都是为分类技术而开发的，我们认为目前关于顺序决策的方法还存在一定限制。在这项工作中，我们首先提出了在顺序决策设置中定义“概念”的愿望。受到“Protege效应”的启发，该效应说明解释知识通常会增强个体的自主学习能力，我们探索了基于概念的解释对RL agent的学习效果和最终用户对agent决策理解的双重好处。为此，我们提出了一个统一的框架，St",
    "tldr": "本论文致力于开发一种基于概念的解释方法，旨在提高非AI专家对AI决策的理解。通过定义顺序决策设置中的“概念”以及探索基于概念的解释对RL agent学习效果和最终用户对agent决策理解的双重好处，我们提出了一个统一的框架。"
}