{
    "title": "Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])",
    "abstract": "Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer",
    "link": "http://arxiv.org/abs/2309.02724",
    "context": "Title: Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])\nAbstract: Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer",
    "path": "papers/23/09/2309.02724.json",
    "total_tokens": 1030,
    "translated_title": "希伯来语侮辱性语料库及BERT模型的检测",
    "translated_abstract": "侮辱性语言检测在许多语言中已经有很多研究，但在低资源语言(如希伯来语)中仍有所滞后。本文介绍了一个新的希伯来语侮辱性语料库，从Twitter上收集了15881条推文。每条推文都由阿拉伯-希伯来双语人士标记为五个类别(辱骂、仇恨、暴力、色情或非侮辱性)。标注过程具有挑战性，因为每个标注者都需要熟悉以色列的文化、政治和实践，以理解每条推文的背景。我们使用提出的数据集和另一个已发布的数据集对两个希伯来语BERT模型(HeBERT和AlephBERT)进行微调。我们观察到，我们的数据与D_OLaH结合后，提高了HeBERT模型2%的性能。在我们的数据上对AlephBERT进行微调并在D_OLaH上进行测试，准确率达到69%，而在D_OLaH上进行微调并在我们的数据上进行测试时，准确率为57%，这可能表明我们的数据具有一定的泛化性能。",
    "tldr": "本研究提出了一个新的希伯来语侮辱性语料库，并使用两个希伯来语BERT模型（HeBERT和AlephBERT）进行了微调。我们观察到，我们的数据结合D_OLaH可以提高HeBERT模型的性能2%。此外，我们的数据对AlephBERT模型也具有一定的泛化性能。"
}