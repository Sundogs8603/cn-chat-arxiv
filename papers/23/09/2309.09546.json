{
    "title": "Training dynamic models using early exits for automatic speech recognition on resource-constrained devices",
    "abstract": "arXiv:2309.09546v2 Announce Type: replace-cross  Abstract: The ability to dynamically adjust the computational load of neural models during inference is crucial for on-device processing scenarios characterised by limited and time-varying computational resources. A promising solution is presented by early-exit architectures, in which additional exit branches are appended to intermediate layers of the encoder. In self-attention models for automatic speech recognition (ASR), early-exit architectures enable the development of dynamic models capable of adapting their size and architecture to varying levels of computational resources and ASR performance demands. Previous research on early-exiting ASR models has relied on pre-trained self-supervised models, fine-tuned with an early-exit loss. In this paper, we undertake an experimental comparison between fine-tuning pre-trained backbones and training models from scratch with the early-exiting objective. Experiments conducted on public dataset",
    "link": "https://arxiv.org/abs/2309.09546",
    "context": "Title: Training dynamic models using early exits for automatic speech recognition on resource-constrained devices\nAbstract: arXiv:2309.09546v2 Announce Type: replace-cross  Abstract: The ability to dynamically adjust the computational load of neural models during inference is crucial for on-device processing scenarios characterised by limited and time-varying computational resources. A promising solution is presented by early-exit architectures, in which additional exit branches are appended to intermediate layers of the encoder. In self-attention models for automatic speech recognition (ASR), early-exit architectures enable the development of dynamic models capable of adapting their size and architecture to varying levels of computational resources and ASR performance demands. Previous research on early-exiting ASR models has relied on pre-trained self-supervised models, fine-tuned with an early-exit loss. In this paper, we undertake an experimental comparison between fine-tuning pre-trained backbones and training models from scratch with the early-exiting objective. Experiments conducted on public dataset",
    "path": "papers/23/09/2309.09546.json",
    "total_tokens": 850,
    "translated_title": "使用早期退出训练动态模型以在资源受限设备上进行自动语音识别",
    "translated_abstract": "动态调整神经模型在推理过程中的计算负载的能力对于设备处理资源有限且计算资源随时间变化的场景至关重要。 早期退出架构提供了一种有前景的解决方案，其中在编码器的中间层附加了额外的退出分支。 在用于自动语音识别（ASR）的自注意力模型中，早期退出架构使得可以开发出动态模型，这些动态模型能够根据不同水平的计算资源和ASR性能需求来调整其大小和架构。 以往关于早期退出ASR模型的研究依赖于预训练的自监督模型，并使用早期退出损失进行微调。 本文对预训练的骨干网进行微调和使用早期退出目标从头训练模型进行了实验比较。 实验在公共数据集上进行。",
    "tldr": "早期退出架构可以使自注意力模型适应不同计算资源和ASR性能需求，该研究比较了微调预训练模型和采用早期退出目标从头训练模型的效果",
    "en_tdlr": "Early-exit architectures enable self-attention models to adapt to different computational resources and ASR performance demands; this study compares fine-tuning pre-trained models with training models from scratch using the early-exiting objective."
}