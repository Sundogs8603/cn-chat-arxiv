{
    "title": "Scaling Experiments in Self-Supervised Cross-Table Representation Learning. (arXiv:2309.17339v1 [cs.LG])",
    "abstract": "To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines.",
    "link": "http://arxiv.org/abs/2309.17339",
    "context": "Title: Scaling Experiments in Self-Supervised Cross-Table Representation Learning. (arXiv:2309.17339v1 [cs.LG])\nAbstract: To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines.",
    "path": "papers/23/09/2309.17339.json",
    "total_tokens": 911,
    "translated_title": "自监督跨表格表示学习的扩展实验",
    "translated_abstract": "为了分析深度表格表示学习模型的扩展潜力，我们引入了一种新颖的基于Transformer的架构，专门针对表格数据和跨表格表示学习，通过利用表格特定的分词器和共享的Transformer骨干结构。我们的训练方法包括单表和跨表模型，通过自监督的遮蔽单元恢复目标进行缺失值填充。为了了解我们方法的扩展行为，我们训练了不同规模的模型，参数范围从大约$10^4$到$10^7$。这些模型是在精心策划的预训练数据集上训练的，该数据集包含来自76个不同数据集的135M个训练标记。我们通过在线性探测方法在精心策划的基准数据集上评估预训练模型，并与传统基准进行比较，来评估我们架构在单表和跨表预训练设置中的扩展性。",
    "tldr": "本文介绍了一种新颖的基于Transformer的架构，用于深度表格表示学习，以及针对跨表格表示学习的方法。通过自监督训练和不同规模的模型训练，该架构在单表和跨表预训练设置中展现了良好的扩展性能。",
    "en_tdlr": "This paper presents a novel Transformer-based architecture for deep tabular representation learning, as well as a method for cross-table representation learning. Through self-supervised training and models of varying sizes, the architecture exhibits good scalability in both single-table and cross-table pretraining setups."
}