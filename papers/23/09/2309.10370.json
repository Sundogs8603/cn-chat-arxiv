{
    "title": "Geometric structure of shallow neural networks and constructive ${\\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])",
    "abstract": "In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\\mathbb R}^M$, output space ${\\mathbb R}^Q$ with $Q\\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\\delta_P$ where $\\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\\leq M$ by a relative error $O(\\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen",
    "link": "http://arxiv.org/abs/2309.10370",
    "context": "Title: Geometric structure of shallow neural networks and constructive ${\\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])\nAbstract: In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\\mathbb R}^M$, output space ${\\mathbb R}^Q$ with $Q\\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\\delta_P$ where $\\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\\leq M$ by a relative error $O(\\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen",
    "path": "papers/23/09/2309.10370.json",
    "total_tokens": 1002,
    "translated_title": "浅层神经网络的几何结构和基于${\\mathcal L}^2$代价最小化的构造方法",
    "translated_abstract": "本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\\mathbb R}^M$、输出空间${\\mathbb R}^Q$（其中$Q\\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\\delta_P)$的上界，其中$\\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\\leq M$所获得的上界之间有一个相对误差$O(\\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。",
    "tldr": "本文提供了浅层神经网络的几何结构解释，并通过基于${\\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。"
}