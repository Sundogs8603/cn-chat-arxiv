{
    "title": "Regularization and Optimal Multiclass Learning. (arXiv:2309.13692v1 [cs.LG])",
    "abstract": "The quintessential learning algorithm of empirical risk minimization (ERM) is known to fail in various settings for which uniform convergence does not characterize learning. It is therefore unsurprising that the practice of machine learning is rife with considerably richer algorithmic techniques for successfully controlling model capacity. Nevertheless, no such technique or principle has broken away from the pack to characterize optimal learning in these more general settings.  The purpose of this work is to characterize the role of regularization in perhaps the simplest setting for which ERM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam's Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian reasoning. Most notably, we introduce an optimal learner which relaxes structural risk minimization on",
    "link": "http://arxiv.org/abs/2309.13692",
    "context": "Title: Regularization and Optimal Multiclass Learning. (arXiv:2309.13692v1 [cs.LG])\nAbstract: The quintessential learning algorithm of empirical risk minimization (ERM) is known to fail in various settings for which uniform convergence does not characterize learning. It is therefore unsurprising that the practice of machine learning is rife with considerably richer algorithmic techniques for successfully controlling model capacity. Nevertheless, no such technique or principle has broken away from the pack to characterize optimal learning in these more general settings.  The purpose of this work is to characterize the role of regularization in perhaps the simplest setting for which ERM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam's Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian reasoning. Most notably, we introduce an optimal learner which relaxes structural risk minimization on",
    "path": "papers/23/09/2309.13692.json",
    "total_tokens": 906,
    "translated_title": "正则化与最优多类别学习",
    "translated_abstract": "以经验风险最小化（ERM）为代表的典型学习算法已被发现在一些学习非均匀收敛的情景中无法成功应用。因此，在机器学习实践中存在许多更丰富的算法技术来控制模型容量。然而，在这些更一般的情境中，没有一种技术或原则能够脱颖而出来描述最优学习的特征。本文旨在表征正则化在多类别学习中的作用，这可能是ERM失败的最简单情景，而标签集是任意的。我们利用一对一包含图（OIGs）展示了与传统算法原则相结合的最优学习算法：奥卡姆剃刀原则所体现的结构风险最小化（SRM），最大熵原则和贝叶斯推理。值得注意的是，我们引入了一种在结构风险最小化上进行放松的最优学习器。",
    "tldr": "本研究旨在研究正则化在多类别学习中的作用，以及其在一些特定情景下的最优学习算法。我们使用一对一包含图(OIGs)展示了结构风险最小化、最大熵原则和贝叶斯推理等算法原则的最优学习算法。"
}