{
    "title": "Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])",
    "abstract": "Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp",
    "link": "http://arxiv.org/abs/2309.07988",
    "context": "Title: Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])\nAbstract: Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp",
    "path": "papers/23/09/2309.07988.json",
    "total_tokens": 931,
    "translated_title": "折叠注意力：面向设备的Transformer流式语音识别的内存和功耗优化",
    "translated_abstract": "基于Transformer的模型在语音识别中表现出色。现有的用于优化Transformer推断的努力，通常针对长上下文应用，主要集中在简化注意力得分计算上。然而，流式语音识别模型通常每次只处理有限数量的令牌，因此注意力得分计算在这种情况下并不是瓶颈所在。相反，瓶颈在于多头注意力和前馈网络的线性投影层，它们构成了模型大小的相当部分，并对计算、内存和功耗的使用产生重要影响。为了解决这一瓶颈，我们提出了折叠注意力，这是一种针对这些线性层的技术，显著减小了模型大小，并提高了内存和功耗效率。设备上的基于Transformer的流式语音识别模型的实验证明，折叠注意力可以将模型大小（和相应的内存消耗）减小多达24%，并将功耗减小多达23%，而无需进行补充。",
    "tldr": "本论文提出了一种名为折叠注意力的技术，在基于Transformer的流式语音识别模型中，通过减少线性投影层的数量，显著减小了模型大小，提高了内存和功耗效率，实验证明可以将模型大小减小24%、功耗减小23%。",
    "en_tdlr": "This paper proposes a technique called folding attention that significantly reduces the model size and improves memory and power efficiency in Transformer-based streaming speech recognition models, by reducing the number of linear projection layers. Experimental results show a reduction of 24% in model size and 23% in power consumption."
}