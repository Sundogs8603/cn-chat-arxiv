{
    "title": "Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])",
    "abstract": "The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.",
    "link": "http://arxiv.org/abs/2309.14913",
    "context": "Title: Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])\nAbstract: The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.",
    "path": "papers/23/09/2309.14913.json",
    "total_tokens": 793,
    "translated_title": "随机语言模型的鲁棒性",
    "translated_abstract": "随机语言模型(De Giuli 2019)是一组随机上下文无关文法，量化人类和计算机语言的句法。该模型提出了一个简单的第一语言学习图景，即作为潜在语言空间中一个退火类型，推断了向语法句法的单一连续转变，其中潜在的词汇和分类之间的对称性会自发打破。在本文中，通过考虑其对明确对称性打破的鲁棒性，对这一图景进行了严格审视，这是在现实世界的学习中不可避免的组成部分。结果表明，该场景对于这种对称性的打破是鲁棒的。与语法网络聚类系数的人类数据进行比较表明，观察到的转变相当于儿童24个月时通常经历的转变。",
    "tldr": "随机语言模型的研究展示了第一语言学习过程中的语法句法连续转变，并证明该转变对于明确对称性的打破是鲁棒的。"
}