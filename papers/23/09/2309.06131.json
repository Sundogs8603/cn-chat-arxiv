{
    "title": "Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])",
    "abstract": "Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rank",
    "link": "http://arxiv.org/abs/2309.06131",
    "context": "Title: Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])\nAbstract: Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rank",
    "path": "papers/23/09/2309.06131.json",
    "total_tokens": 971,
    "translated_title": "对神经排序器进行微调的数据标注？当前主动学习策略并不比随机选择更好。",
    "translated_abstract": "基于预训练语言模型（PLM）的搜索方法相比统计和早期神经排序模型显示出了巨大的有效性提升。然而，微调基于PLM的排序器需要大量的标注训练数据。标注数据需要大量的人工努力，因此在具体领域任务中非常昂贵。本文研究了在有限的训练数据和预算下微调基于PLM的排序器。我们研究了两种情况：从头开始微调排序器，以及从已经在通用数据上微调的排序器开始进行领域适应，并在目标数据集上继续微调。我们观察到在不同随机选择的训练数据子集上进行微调时，有效性存在很大的变异性。这表明通过主动选择对排序器效果最积极的训练数据子集，可以实现有效性提升。通过这种方式，将可以微调出有效的PLM排序器。",
    "tldr": "本论文研究了在有限的训练数据和预算下，对基于预训练语言模型的排序器进行微调的问题。通过观察发现，在不同随机选择的训练数据子集上进行微调时，有效性存在很大变异性。因此，通过主动选择对排序器效果积极的训练数据子集，可以实现有效性提升。"
}