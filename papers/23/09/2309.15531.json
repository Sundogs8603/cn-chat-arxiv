{
    "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])",
    "abstract": "Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as",
    "link": "http://arxiv.org/abs/2309.15531",
    "context": "Title: Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])\nAbstract: Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as",
    "path": "papers/23/09/2309.15531.json",
    "total_tokens": 978,
    "translated_title": "重新思考频道维度以隔离大型语言模型的低位权重量化中的异常值",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中近期展示了显著成功。然而，在有效地为LLMs提供服务方面一直是个挑战，这主要是由于其大内存瓶颈，特别是在小批量推理设置（如移动设备）中。仅对权重进行量化可能是一种有希望的方法，但是由于存在大幅度激活异常值，低于4位的量化仍然是一个挑战。为了减轻不可取的异常效果，我们首先提出了每个输入通道（IC）内进行量化分组的per-IC量化方法，这是一种简单但有效的方法，而不是传统的每个输出通道（OC）内进行量化分组。我们的方法的动机是观察到激活异常值影响权重矩阵的输入维度，因此在IC方向上对权重进行类似分组可以将异常值隔离到一个分组内。我们还发现激活的异常值并不决定量化的难度，其固有的权重敏感性也存在。通过per-IC量化作为方法，我们的方法成功地解决了大型语言模型低位权重量化中的异常值问题。",
    "tldr": "该论文提出了一种重新思考频道维度的方法，以隔离大型语言模型低位权重量化中的异常值。通过将权重按输入通道内进行量化分组，可以解决激活异常值的问题，并成功地使得低于4位的量化成为可能。",
    "en_tdlr": "This paper proposes a rethinking of channel dimensions to isolate outliers for low-bit weight quantization of large language models. By creating quantization groups within each input channel, the paper successfully addresses the issue of activation outliers and enables quantization below 4 bits."
}