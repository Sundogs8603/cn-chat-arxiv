{
    "title": "Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.",
    "link": "http://arxiv.org/abs/2309.07382",
    "context": "Title: Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.",
    "path": "papers/23/09/2309.07382.json",
    "total_tokens": 913,
    "translated_title": "长文本摘要评估中“少即是多”的理论",
    "translated_abstract": "大型语言模型(LLMs)在摘要评估任务中表现出了令人期待的性能，但它们面临诸如高计算成本和长文档中重要信息被忽视的“迷失在中间”问题。为解决这些问题，本文引入了一种新颖的方法，即“先提取再评估”，该方法涉及从长文本源文件中提取关键句子，然后通过提问LLMs来评估摘要。结果表明，所提出的方法不仅显著降低了评估成本，而且与人工评估之间存在更高的相关性。此外，我们提供了关于最佳文档长度和句子提取方法的实用建议，为基于LLMs的文本生成评估的开发提供了成本效益更高且更准确的方法。",
    "tldr": "该论文引入了一种新颖的方法，通过先提取关键句子再进行评估，有效解决了大型语言模型在长文档摘要评估中遇到的计算成本高和忽视重要信息的问题。研究发现，这种方法不仅显著降低了评估成本，而且与人工评估有更高的相关性。此外，论文还提供了关于最佳文档长度和句子提取方法的实用建议，为基于大型语言模型的文本生成评估的发展做出了贡献。",
    "en_tdlr": "This paper presents a novel approach, Extract-then-Evaluate, for long document summary evaluation using Language Models, which effectively addresses the challenges of high computational costs and overlooking important information. The proposed method significantly reduces evaluation costs, exhibits a higher correlation with human evaluations, and provides practical recommendations for optimal document length and sentence extraction methods."
}