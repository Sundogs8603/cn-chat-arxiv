{
    "title": "Can Whisper perform speech-based in-context learning. (arXiv:2309.07081v1 [eess.AS])",
    "abstract": "This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to",
    "link": "http://arxiv.org/abs/2309.07081",
    "context": "Title: Can Whisper perform speech-based in-context learning. (arXiv:2309.07081v1 [eess.AS])\nAbstract: This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to",
    "path": "papers/23/09/2309.07081.json",
    "total_tokens": 931,
    "translated_title": "Whisper能够进行基于语境的语音学习吗？",
    "translated_abstract": "本文研究了OpenAI发布的Whisper自动语音识别（ASR）模型的语境学习能力。提出了一种新的基于语境的语音学习（SICL）方法，用于测试时适应，可以在没有梯度下降的情况下减少单词错误率（WER），只需要少量标记的语音样本。使用中文方言进行语言级别的适应实验表明，在将SICL应用于孤立词ASR时，可以在两个方言上使用任意大小的Whisper模型实现一致且显著的WER相对降低，平均为32.3%。基于k最近邻的上下文示例选择技术可以进一步提高SICL的效率，平均相对WER降低率为36.4%。通过说话人适应或连续语音识别任务来验证了这些发现，并且两者都实现了显著的相对WER降低。还提供了详细的定量分析。",
    "tldr": "本文研究了Whisper自动语音识别模型的语境学习能力，并提出了一种基于语境的语音学习方法，用于在测试时适应。通过实验验证了该方法在中文方言上的有效性，可以显著减少单词错误率。通过进一步优化选择技术可以进一步提高效率。",
    "en_tdlr": "This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models and proposes a speech-based in-context learning approach for test-time adaptation. Experiments on Chinese dialects demonstrate that this approach effectively reduces word error rates and further improvement can be achieved by optimizing the selection technique."
}