{
    "title": "Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning. (arXiv:2309.13285v1 [cs.RO])",
    "abstract": "End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our wor",
    "link": "http://arxiv.org/abs/2309.13285",
    "context": "Title: Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning. (arXiv:2309.13285v1 [cs.RO])\nAbstract: End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our wor",
    "path": "papers/23/09/2309.13285.json",
    "total_tokens": 947,
    "translated_title": "无人机群体的碰撞回避和导航方法：基于端到端深度强化学习",
    "translated_abstract": "端到端深度强化学习(DRL)的无人机控制方法具有易于部署、任务泛化和实时执行能力等优点。然而，以往的端到端DRL方法主要用于在简单、无障碍环境中训练单个无人机或无人机团队的控制器，并没有考虑障碍物对训练RL策略的困难性增加造成的挑战。本文提出了一种在带有障碍物环境中控制无人机群体的端到端DRL方法。我们为智能体提供了一个课程（curriculum）和一个回放缓冲区（replay buffer），用于改善在充满障碍物的环境中的性能。此外，我们还实施了一个注意力机制，以关注邻近机器人和障碍物之间的相互作用 - 这是首次成功地在严重计算受限的硬件上部署的群体行为策略中应用此机制。",
    "tldr": "这项研究提出了一种基于端到端深度强化学习的方法，用于在带有障碍物的环境中控制无人机群体，并通过课程学习和回放缓冲区提高性能。此外，还实施了注意力机制以关注邻近机器人和障碍物之间的相互作用。"
}