{
    "title": "Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks. (arXiv:2309.07765v1 [cs.SD])",
    "abstract": "The Transformer architecture has proven to be highly effective for Automatic Speech Recognition (ASR) tasks, becoming a foundational component for a plethora of research in the domain. Historically, many approaches have leaned on fixed-length attention windows, which becomes problematic for varied speech samples in duration and complexity, leading to data over-smoothing and neglect of essential long-term connectivity. Addressing this limitation, we introduce Echo-MSA, a nimble module equipped with a variable-length attention mechanism that accommodates a range of speech sample complexities and durations. This module offers the flexibility to extract speech features across various granularities, spanning from frames and phonemes to words and discourse. The proposed design captures the variable length feature of speech and addresses the limitations of fixed-length attention. Our evaluation leverages a parallel attention architecture complemented by a dynamic gating mechanism that amalgam",
    "link": "http://arxiv.org/abs/2309.07765",
    "context": "Title: Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks. (arXiv:2309.07765v1 [cs.SD])\nAbstract: The Transformer architecture has proven to be highly effective for Automatic Speech Recognition (ASR) tasks, becoming a foundational component for a plethora of research in the domain. Historically, many approaches have leaned on fixed-length attention windows, which becomes problematic for varied speech samples in duration and complexity, leading to data over-smoothing and neglect of essential long-term connectivity. Addressing this limitation, we introduce Echo-MSA, a nimble module equipped with a variable-length attention mechanism that accommodates a range of speech sample complexities and durations. This module offers the flexibility to extract speech features across various granularities, spanning from frames and phonemes to words and discourse. The proposed design captures the variable length feature of speech and addresses the limitations of fixed-length attention. Our evaluation leverages a parallel attention architecture complemented by a dynamic gating mechanism that amalgam",
    "path": "papers/23/09/2309.07765.json",
    "total_tokens": 899,
    "translated_title": "Echotune: 利用语音的可变长度特性的模块化特征提取器在ASR任务中的应用",
    "translated_abstract": "Transformer架构已被证明在自动语音识别（ASR）任务中非常有效，成为该领域众多研究的基础组件。历史上，许多方法依赖于固定长度的注意力窗口，这对于持续时间和复杂性不同的语音样本来说是有问题的，导致数据过度平滑化和忽视了长期连通性的重要性。为了解决这个限制，我们引入了Echo-MSA，一个具有可变长度注意力机制的灵活模块，可以适应不同复杂性和持续时间的语音样本。该模块提供了从帧和音素到单词和话语的各种颗粒度的语音特征提取的灵活性。提出的设计捕捉到了语音的可变长度特征，并解决了固定长度注意力的局限性。我们的评估利用了一个平行的注意力架构，并结合了一个动态门控机制。",
    "tldr": "Echotune是一个模块化特征提取器，利用语音的可变长度特性，通过引入Echo-MSA模块，实现了从帧到话语的各种颗粒度的语音特征提取，解决了固定长度注意力的局限性。",
    "en_tdlr": "Echotune is a modular feature extractor that leverages the variable-length nature of speech. By introducing the Echo-MSA module, it enables flexible speech feature extraction across various granularities from frames to discourse, addressing the limitations of fixed-length attention."
}