{
    "title": "Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])",
    "abstract": "Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared",
    "link": "http://arxiv.org/abs/2309.04664",
    "context": "Title: Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])\nAbstract: Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared",
    "path": "papers/23/09/2309.04664.json",
    "total_tokens": 867,
    "translated_title": "Compact：逼近复杂激活函数用于安全计算",
    "translated_abstract": "安全多方计算（Secure multi-party computation，MPC）技术可以用于在用户查询部署在公共云上的深度神经网络（DNN）模型时提供数据隐私。最先进的MPC技术可以直接用于使用简单激活函数（AFs）的DNN模型，如ReLU。然而，为了最新的应用设计的DNN模型架构通常使用复杂且高度非线性的AFs。为这些复杂AFs设计高效的MPC技术是一个尚未解决的问题。为此，我们提出了Compact，它产生复杂AFs的分段多项式逼近，以便与最先进的MPC技术高效配合使用。Compact既不需要也不强加任何模型训练的限制，并且导致几乎相同的模型准确性。我们在使用流行的复杂AFs SiLU、GeLU和Mish的DNN架构的四个不同机器学习任务上对Compact进行了广泛评估。我们的实验结果表明，与传统方法相比，Compact几乎不会损失准确性。",
    "tldr": "Compact提出了一种逼近复杂激活函数的方法，可以与MPC技术高效配合使用，并且几乎不会损失模型准确性。",
    "en_tdlr": "Compact proposes an approximation method for complex activation functions, enabling efficient use with MPC techniques and resulting in minimal loss of model accuracy."
}