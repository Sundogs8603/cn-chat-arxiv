{
    "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])",
    "abstract": "As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ",
    "link": "http://arxiv.org/abs/2309.00614",
    "context": "Title: Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])\nAbstract: As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ",
    "path": "papers/23/09/2309.00614.json",
    "total_tokens": 916,
    "translated_title": "面向对齐语言模型的对抗攻击的基线防御",
    "translated_abstract": "随着大型语言模型的普及，其安全漏洞变得至关重要。最近的研究表明，文本优化器可以生成绕过审查和对齐的越狱提示。借鉴对抗机器学习的丰富研究成果，我们从三个问题入手：在这个领域中什么样的威胁模型是实用的？基线防御技术在这个新领域中表现如何？LLM安全性与计算机视觉有何不同？我们对主导对抗LLM攻击的几种基线防御策略进行评估，讨论了每种策略在各种设置下的可行性和有效性。特别是，我们关注三种类型的防御：检测（基于困惑度）、输入预处理（改写和重新标记化）和对抗训练。我们讨论了白盒和灰盒设置，并讨论了每种考虑的防御策略在鲁棒性和性能之间的权衡。",
    "tldr": "这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。",
    "en_tdlr": "This paper investigates adversarial attacks against aligned language models, evaluates the effectiveness of baseline defense strategies, and discusses the feasibility and efficacy of various approaches, as well as the trade-off between robustness and performance."
}