{
    "title": "FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining. (arXiv:2309.09431v4 [cs.CV] UPDATED)",
    "abstract": "Hyperspectral images (HSIs) contain rich spectral and spatial information. Motivated by the success of transformers in the field of natural language processing and computer vision where they have shown the ability to learn long range dependencies within input data, recent research has focused on using transformers for HSIs. However, current state-of-the-art hyperspectral transformers only tokenize the input HSI sample along the spectral dimension, resulting in the under-utilization of spatial information. Moreover, transformers are known to be data-hungry and their performance relies heavily on large-scale pretraining, which is challenging due to limited annotated hyperspectral data. Therefore, the full potential of HSI transformers has not been fully realized. To overcome these limitations, we propose a novel factorized spectral-spatial transformer that incorporates factorized self-supervised pretraining procedures, leading to significant improvements in performance. The factorization",
    "link": "http://arxiv.org/abs/2309.09431",
    "context": "Title: FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pretraining. (arXiv:2309.09431v4 [cs.CV] UPDATED)\nAbstract: Hyperspectral images (HSIs) contain rich spectral and spatial information. Motivated by the success of transformers in the field of natural language processing and computer vision where they have shown the ability to learn long range dependencies within input data, recent research has focused on using transformers for HSIs. However, current state-of-the-art hyperspectral transformers only tokenize the input HSI sample along the spectral dimension, resulting in the under-utilization of spatial information. Moreover, transformers are known to be data-hungry and their performance relies heavily on large-scale pretraining, which is challenging due to limited annotated hyperspectral data. Therefore, the full potential of HSI transformers has not been fully realized. To overcome these limitations, we propose a novel factorized spectral-spatial transformer that incorporates factorized self-supervised pretraining procedures, leading to significant improvements in performance. The factorization",
    "path": "papers/23/09/2309.09431.json",
    "total_tokens": 850,
    "translated_title": "FactoFormer: 通过自监督预训练的分解高光谱变压器",
    "translated_abstract": "高光谱图像（HSIs）包含丰富的光谱和空间信息。在自然语言处理和计算机视觉领域，变压器在学习输入数据中的长程依赖性方面取得了成功，因此近期的研究集中在将变压器用于HSIs上。然而，当前最先进的高光谱变压器只在光谱维度上对输入的HSI样本进行标记，导致空间信息的利用不足。此外，已知变压器对数据需求量大，并且它们的性能严重依赖于大规模的预训练，而由于有限的标注高光谱数据，这在实践中存在挑战。因此，高光谱变压器的全部潜力尚未完全发挥出来。为了克服这些限制，我们提出了一种新颖的分解光谱-空间变压器，它融合了分解的自监督预训练流程，从而显著提高了性能。",
    "tldr": "该论文提出了一种Factorized Hyperspectral Transformer，结合了分解的自监督预训练流程，显著提高了性能。",
    "en_tdlr": "This paper proposes a Factorized Hyperspectral Transformer with self-supervised pretraining, which significantly improves performance."
}