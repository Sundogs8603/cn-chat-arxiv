{
    "title": "Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])",
    "abstract": "Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models' human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., \"woman\" to \"man\"), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to",
    "link": "http://arxiv.org/abs/2309.15991",
    "context": "Title: Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])\nAbstract: Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models' human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., \"woman\" to \"man\"), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to",
    "path": "papers/23/09/2309.15991.json",
    "total_tokens": 950,
    "translated_title": "有针对性的图像数据增强提高了基本技能字幕鲁棒性",
    "translated_abstract": "人工神经网络通常在推广到超出上下文的示例时遇到困难。这种限制的原因之一是数据集只包含有关世界潜在关联结构的部分信息。在这项工作中，我们提出了TIDA（有针对性的图像编辑数据增强）方法，它是一种专注于通过使用文本到图像生成模型来填补关联结构差距以提高模型类人能力（例如性别识别）的有针对性数据增强方法。更具体地说，TIDA识别描述图像的标题中的特定技能（例如图中特定性别的存在），改变标题（例如将“女人”改为“男人”），然后使用文本到图像模型编辑图像，以使其与新标题匹配（例如唯一地将一个女人改为一个男人同时保持上下文不变）。基于Flickr30K基准测试，我们展示了与原始数据集相比，使用TIDA增强的数据集相关的...",
    "tldr": "本论文提出了一种有针对性的图像数据增强方法（TIDA），通过使用文本到图像生成模型来填补关联结构差距以提高模型的人类感知能力，如性别识别。实验结果表明，相较于原始数据集，使用TIDA增强的数据集能够显著提高模型的鲁棒性。",
    "en_tdlr": "This paper proposes a targeted image data augmentation method (TIDA) that improves models' human-like abilities, such as gender recognition, by using a text-to-image generative model to fill the correlational structure gap. Experimental results show that the TIDA-enhanced dataset significantly increases the robustness of models compared to the original dataset."
}