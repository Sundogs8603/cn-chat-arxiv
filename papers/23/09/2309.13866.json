{
    "title": "On Calibration of Modern Quantized Efficient Neural Networks. (arXiv:2309.13866v2 [cs.LG] UPDATED)",
    "abstract": "We explore calibration properties at various precisions for three architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets: CIFAR-100 and PathMNIST. The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration. This becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown to be the most robust to overall performance drop at lower precision. We find that temperature scaling can improve calibration error for quantized networks, with some caveats. We hope that these preliminary insights can lead to more opportunities for explainable and reliable EdgeML.",
    "link": "http://arxiv.org/abs/2309.13866",
    "context": "Title: On Calibration of Modern Quantized Efficient Neural Networks. (arXiv:2309.13866v2 [cs.LG] UPDATED)\nAbstract: We explore calibration properties at various precisions for three architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets: CIFAR-100 and PathMNIST. The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration. This becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown to be the most robust to overall performance drop at lower precision. We find that temperature scaling can improve calibration error for quantized networks, with some caveats. We hope that these preliminary insights can lead to more opportunities for explainable and reliable EdgeML.",
    "path": "papers/23/09/2309.13866.json",
    "total_tokens": 863,
    "translated_title": "关于现代量化高效神经网络的校准研究",
    "translated_abstract": "我们探索了三种架构（ShuffleNetv2、GhostNet-VGG和MobileOne）以及两个数据集（CIFAR-100和PathMNIST）在不同精度下的校准性能。研究发现，校准质量与量化质量密切相关；已有文献证明随着精度降低，性能会变得更差，我们观察到校准质量也存在类似的关联。这在4位激活区间尤为严重。GhostNet-VGG在更低精度下表现出最高的稳健性，我们发现温度缩放可以改善量化网络的校准误差，但需要注意一些细节。我们希望这些初步的观察能够为可解释和可靠的边缘机器学习提供更多机会。",
    "tldr": "这项研究探索了在不同精度下三种架构和两个数据集的神经网络校准性能，发现校准质量与量化质量相关，并观察到在低精度下性能和校准质量均变差。GhostNet-VGG表现出最高稳健性，温度缩放可以改善量化网络的校准误差。",
    "en_tdlr": "This study explores the calibration properties of three architectures and two datasets at different precisions, finding a correlation between calibration quality and quantization quality, and observing a deterioration of performance and calibration at lower precision. GhostNet-VGG exhibits the highest robustness, and temperature scaling improves calibration error in quantized networks."
}