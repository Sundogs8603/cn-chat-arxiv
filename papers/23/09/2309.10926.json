{
    "title": "Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])",
    "abstract": "Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming \"semi-autoregressive\" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming ",
    "link": "http://arxiv.org/abs/2309.10926",
    "context": "Title: Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])\nAbstract: Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming \"semi-autoregressive\" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming ",
    "path": "papers/23/09/2309.10926.json",
    "total_tokens": 920,
    "translated_title": "带标签上下文的半自回归流式自动语音识别",
    "translated_abstract": "非自回归(NAR)建模在语音处理中引起了广泛关注，因为这些模型在推断时间方面比自回归(AR)模型大大降低，同时也达到了较好的转录准确率。由于NAR自动语音识别(ASR)模型必须等待整个话语的完整完成才能进行处理，因此一些研究探索了基于块状注意力的流式NAR模型，以用于低延迟应用。然而，与流式AR和非流式NAR模型相比，流式NAR模型在准确性方面明显滞后。为了解决这个问题，我们提出了一种流式的“半自回归”ASR模型，通过使用语言模型(LM)子网络将先前块中发出的标签作为附加上下文进行建模。我们还引入了一种新颖的贪婪解码算法，能够在块之间附近处理插入和删除错误，同时不显著增加推断时间。实验结果表明，我们的方法优于现有的流式方法。",
    "tldr": "提出了一种带有标签上下文的半自回归流式自动语音识别模型，通过使用语言模型子网络，将先前块中的标签作为额外的上下文进行建模。实验结果表明，该方法在流式自动语音识别中取得了更好的性能。",
    "en_tdlr": "A semi-autoregressive streaming ASR model is proposed, which incorporates the labels emitted in previous blocks as additional context using a language model subnetwork. Experimental results show that this approach achieves better performance in streaming automatic speech recognition."
}