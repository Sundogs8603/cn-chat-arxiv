{
    "title": "FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])",
    "abstract": "Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation",
    "link": "http://arxiv.org/abs/2309.11306",
    "context": "Title: FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])\nAbstract: Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation",
    "path": "papers/23/09/2309.11306.json",
    "total_tokens": 907,
    "translated_title": "FaceDiffuser：使用扩散技术的基于语音驱动的3D面部动画合成",
    "translated_abstract": "基于语音驱动的3D面部动画合成一直是一个具有挑战性的任务，无论在工业界还是研究领域。最近的方法主要集中在确定性深度学习方法上，这意味着给定一个语音输入，输出总是相同的。然而，在现实中，脸部中存在的非语言面部线索是非确定性的。此外，大多数方法都集中在基于3D顶点的数据集和与现有面部动画流程兼容的方法上是稀缺的。为了解决这些问题，我们提出了FaceDiffuser，这是一个基于非确定性深度学习模型的生成语音驱动的面部动画的方法，它在训练中使用了基于3D顶点和混合形状的数据集。我们的方法基于扩散技术，使用预训练的大型语音表示模型HuBERT来编码音频输入。据我们所知，我们是首次将扩散方法用于语音驱动的3D面部动画任务。",
    "tldr": "FaceDiffuser是一种使用扩散技术的基于语音驱动的3D面部动画合成方法，它采用了非确定性的深度学习模型，并使用了基于3D顶点和混合形状的数据集进行训练。",
    "en_tdlr": "FaceDiffuser is a speech-driven 3D facial animation synthesis method that uses diffusion technique, non-deterministic deep learning model, and is trained with both 3D vertex and blendshape based datasets."
}