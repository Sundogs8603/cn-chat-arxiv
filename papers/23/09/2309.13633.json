{
    "title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria",
    "abstract": "arXiv:2309.13633v2 Announce Type: replace-cross  Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory promp",
    "link": "https://arxiv.org/abs/2309.13633",
    "context": "Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\nAbstract: arXiv:2309.13633v2 Announce Type: replace-cross  Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory promp",
    "path": "papers/23/09/2309.13633.json",
    "total_tokens": 881,
    "translated_title": "EvalLM: 交互式评估基于用户定义标准的大型语言模型提示",
    "translated_abstract": "通过简单地组合提示，开发人员可以使用大型语言模型（LLMs）原型化新颖的生成应用。然而，要将原型细化为产品，开发人员必须通过评估输出以诊断弱点来迭代修订提示。形成性访谈（N=8）显示，开发人员在评估输出时投入了大量精力，因为他们评估特定上下文和主观标准。我们提出了EvalLM，这是一个交互式系统，可以通过评估用户定义标准上的多个输出来迭代改进提示。用户可以通过用自然语言描述标准，使用系统基于LLM的评估器来获得提示在哪些方面表现出色或失败的概述，并根据评估器的反馈进行改进。一项比较研究（N=12）显示，与手动评估相比，EvalLM有助于帮助参与者撰写更多样化的标准、检查两倍数量的输出，并达到令人满意的提示。",
    "tldr": "EvalLM是一个交互式系统，帮助开发人员通过评估多个输出来改进大型语言模型提示，相比手动评估，能够帮助用户撰写更多样化的标准、检查更多输出，达到更满意的提示。"
}