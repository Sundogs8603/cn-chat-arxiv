{
    "title": "Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])",
    "abstract": "The \"pre-training and fine-tuning\" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to",
    "link": "http://arxiv.org/abs/2309.10019",
    "context": "Title: Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])\nAbstract: The \"pre-training and fine-tuning\" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to",
    "path": "papers/23/09/2309.10019.json",
    "total_tokens": 851,
    "translated_title": "参数高效的长尾识别",
    "translated_abstract": "自从出现大规模视觉语言模型（例如对比语言-图像预训练模型CLIP），\"预训练和微调\"范例在解决长尾识别任务中引起了极大的兴趣。虽然先前研究在适应预训练模型用于这些任务方面表现出了希望，但它们常常需要大量的训练轮数或额外的训练数据来保持良好的性能，这是不可取的。在本文中，我们提出了一种名为PEL的微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。我们首先经验性地发现，常用的微调方法（例如全面微调和分类器微调）容易过拟合，导致尾部类别的性能下降。为了解决这个问题，PEL采用了现有的参数高效微调方法的设计，引入了少量的任务特定参数。",
    "tldr": "本文提出了一种名为PEL的参数高效微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。该方法通过引入少量的任务特定参数，解决了常用微调方法导致尾部类别性能下降的问题。"
}