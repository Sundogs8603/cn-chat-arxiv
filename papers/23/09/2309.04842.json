{
    "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])",
    "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypoth",
    "link": "http://arxiv.org/abs/2309.04842",
    "context": "Title: Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])\nAbstract: While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypoth",
    "path": "papers/23/09/2309.04842.json",
    "total_tokens": 875,
    "translated_title": "利用大型语言模型来利用ASR不确定性",
    "translated_abstract": "尽管大型语言模型在各种自然语言处理（NLP）任务中表现出色，但要在口语理解（SLU）任务上表现出色，它们必须依靠现成的自动语音识别（ASR）系统进行转录，或者配备内置的语音模态。本文关注的是前一种情况，即LLM在SLU任务上的准确性受限于固定ASR系统在口语输入上的准确性。具体而言，我们解决了语音意图分类任务，其中高字词错误率可能限制LLM理解口头意图的能力。我们的目标不是通过设计复杂或专门的架构追求高准确性，而是在不实质改变底层ASR和LLM的情况下，看看我们能走多远，这些模型可以潜在地被多个不相关的任务共享。为此，我们提出使用ASR假设的n-best列表来提示LLM，而不仅仅是容易出错的1-best假设。",
    "tldr": "这项工作旨在通过利用ASR的n-best列表来解决大型语言模型在口语理解任务上的潜在限制，而无需实质改变ASR和LLM的结构。"
}