{
    "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduce",
    "link": "http://arxiv.org/abs/2309.08008",
    "context": "Title: An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduce",
    "path": "papers/23/09/2309.08008.json",
    "total_tokens": 861,
    "translated_title": "大型语言模型在零样本临床自然语言处理中提示策略的实证评估",
    "translated_abstract": "大型语言模型（LLMs）在自然语言处理（NLP）领域表现出卓越的能力，尤其在标注数据稀缺或昂贵的领域，如临床领域。然而，要解锁这些LLMs中隐藏的临床知识，我们需要设计有效的提示，可以引导它们在没有任何特定任务训练数据的情况下执行特定的临床NLP任务。这被称为上下文学习，这是一门需要了解不同LLMs和提示工程方法的优点和缺点的艺术和科学。在本文中，我们提出了一个全面而系统的实验研究，针对五个临床NLP任务进行提示工程的评估：临床意义消歧、生物医学证据提取、共指消解、药物状态提取和药物属性提取。我们评估了最近文献中提出的提示方法，包括简单前缀、简单填空、思维链和预期提示，并介绍了一些新的提示方法。",
    "tldr": "该论文通过对五个临床自然语言处理任务的实验研究，评估了不同提示工程方法在大型语言模型上的效果，为解锁临床领域中的知识提供了指导。"
}