{
    "title": "Distributionally Robust Model-based Reinforcement Learning with Large State Spaces. (arXiv:2309.02236v1 [cs.LG])",
    "abstract": "Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed met",
    "link": "http://arxiv.org/abs/2309.02236",
    "context": "Title: Distributionally Robust Model-based Reinforcement Learning with Large State Spaces. (arXiv:2309.02236v1 [cs.LG])\nAbstract: Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed met",
    "path": "papers/23/09/2309.02236.json",
    "total_tokens": 898,
    "translated_title": "具有大状态空间的分布鲁棒的基于模型的强化学习",
    "translated_abstract": "强化学习面临着复杂的动态系统和大状态空间的挑战，以及昂贵的数据收集过程以及真实世界动力学与训练环境部署的偏差。为了克服这些问题，我们研究了在广泛使用的Kullback-Leibler、卡方和总变差不确定性集合下具有连续状态空间的分布鲁棒的马尔可夫决策过程。我们提出了一种基于模型的方法，利用高斯过程和最大方差缩减算法来高效学习多输出的名义转移动态，并利用生成模型（即模拟器）的访问权限。我们进一步展示了该方法在不同不确定性集合下的统计样本复杂度。这些复杂度界不依赖于状态数量，并且超越线性动态，确保了我们方法在识别接近最优的分布鲁棒策略方面的有效性。",
    "tldr": "本研究提出了一种用于解决强化学习中复杂动态系统和大状态空间问题的分布鲁棒的基于模型的方法，通过利用高斯过程和最大方差缩减算法进行高效学习，并在不同不确定性集合下展示了统计样本复杂度。",
    "en_tdlr": "This study proposes a distributionally robust model-based approach to address the challenges of complex dynamical systems and large state spaces in reinforcement learning. The method utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn and demonstrates statistical sample complexity in different uncertainty sets."
}