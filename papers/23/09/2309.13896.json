{
    "title": "Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts. (arXiv:2309.13896v1 [cs.LG])",
    "abstract": "Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be",
    "link": "http://arxiv.org/abs/2309.13896",
    "context": "Title: Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts. (arXiv:2309.13896v1 [cs.LG])\nAbstract: Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be",
    "path": "papers/23/09/2309.13896.json",
    "total_tokens": 848,
    "translated_title": "后续也很重要：通过后期服务上下文改进上下文推荐",
    "translated_abstract": "标准的上下文推荐问题假设算法在选择一个选项之前观察到所有相关的上下文。然而，在处理一些问题时，这种建模方式通常不够用，因为在选择选项后可以观察到有价值的附加上下文。为了提高这些应用中的在线学习效率，我们研究了一种具有后期服务上下文的新型上下文推荐问题，并设计了一种新算法poLinUCB，该算法在标准假设下实现了严格的遗憾控制。我们的技术证明的核心是著名的椭圆潜力引理（EPL）的一个鲁棒化和广义化版本，它可以容纳数据中的噪声。这种鲁棒化对于解决我们的问题是必要的，我们相信它也可以应用于其他领域。",
    "tldr": "通过引入后期服务上下文，我们设计了一种新算法poLinUCB以提高上下文推荐中的在线学习效率，并通过鲁棒化的椭圆潜力引理实现了严格的遗憾控制。",
    "en_tdlr": "By introducing post-serving contexts, we designed a new algorithm poLinUCB to improve online learning efficiency in contextual recommendation and achieved tight regret control through a robustified Elliptical Potential Lemma."
}