{
    "title": "Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback. (arXiv:2309.17078v1 [cs.IR])",
    "abstract": "Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To dem",
    "link": "http://arxiv.org/abs/2309.17078",
    "context": "Title: Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback. (arXiv:2309.17078v1 [cs.IR])\nAbstract: Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To dem",
    "path": "papers/23/09/2309.17078.json",
    "total_tokens": 875,
    "translated_title": "通过对比反馈将大型语言模型的能力与信息检索上下文对齐",
    "translated_abstract": "信息检索(IR)是寻找满足用户信息需求的过程，在现代人的生活中起着重要作用。近年来，大型语言模型(LLMs)在各种任务中展示了杰出的能力，其中一些任务对于IR来说非常重要。然而，LLMs经常面临生成缺乏特定性回复的问题。这在很多情况下限制了LLMs在IR中的整体效果。为了解决这些问题，我们提出了一种无监督对齐框架，称为对比反馈强化学习(RLCF)，它赋予LLMs生成既具有高质量又与IR任务需求相符的上下文特定回复的能力。具体而言，我们通过将每个文档与其相似文档进行比较构建对比反馈，然后提出了一个名为Batched-MRR的奖励函数，教导LLMs生成能够捕捉区分文档与其相似文档的细粒度信息的回复。",
    "tldr": "通过对比反馈强化学习框架，有效提升大型语言模型在信息检索中的应用，使其能够生成更具特定性和上下文适应性的回复。"
}