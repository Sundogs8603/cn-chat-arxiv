{
    "title": "Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds. (arXiv:2309.13915v1 [cs.LG])",
    "abstract": "Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a",
    "link": "http://arxiv.org/abs/2309.13915",
    "context": "Title: Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds. (arXiv:2309.13915v1 [cs.LG])\nAbstract: Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a",
    "path": "papers/23/09/2309.13915.json",
    "total_tokens": 972,
    "translated_title": "神经策略镜像梯度在低维流形上的策略优化的样本复杂性研究",
    "translated_abstract": "在强化学习中，配备有深度神经网络的策略优化算法在解决高维度的问题中取得了巨大的成功。然而，目前的分析无法解释它们为何能抵抗维度诅咒。在本研究中，我们研究了具有卷积神经网络作为函数逼近器的神经策略镜像梯度（NPMD）算法的样本复杂性。受到许多高维环境具有低维结构的经验观察的启发，例如将图像作为状态，我们将状态空间视为嵌入在$D$维欧氏空间中的$d$维流形，其中$d\\ll D$是内在维度。我们证明了在NPMD的每次迭代中，价值函数和策略都可以很好地由卷积神经网络进行逼近。逼近误差由网络的大小控制，并且前一个网络的平滑性可以保留。",
    "tldr": "本研究探讨了神经策略镜像梯度算法在低维流形上的样本复杂性。研究发现在每次迭代中，卷积神经网络可以很好地逼近价值函数和策略，且逼近误差受网络大小的影响，并且可以继承之前网络的平滑性。"
}