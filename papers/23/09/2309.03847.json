{
    "title": "Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])",
    "abstract": "We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\\tilde{O}(k^2 d^4 \\log(1/\\delta) / \\alpha^2 \\varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\\alpha$ while satisfying $(\\varepsilon, \\delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a \"locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].",
    "link": "http://arxiv.org/abs/2309.03847",
    "context": "Title: Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])\nAbstract: We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\\tilde{O}(k^2 d^4 \\log(1/\\delta) / \\alpha^2 \\varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\\alpha$ while satisfying $(\\varepsilon, \\delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a \"locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].",
    "path": "papers/23/09/2309.03847.json",
    "total_tokens": 956,
    "translated_title": "高斯混合物可以通过多项式数量的样本进行差分隐私学习",
    "translated_abstract": "我们研究了在差分隐私(DP)约束下估计高斯混合物的问题。我们的主要结果是，使用$\\tilde{O}(k^2 d^4 \\log(1/\\delta) / \\alpha^2 \\varepsilon)$个样本即可在满足$(\\varepsilon, \\delta)$-DP的条件下估计$k$个高斯混合物，使其达到总变差距离$\\alpha$。这是该问题的第一个有限样本复杂性上限，而无需对GMMs做任何结构性假设。为了解决这个问题，我们构建了一个新的框架，该框架对于其他任务可能也有用。在高层次上，我们展示了如果一个分布类（比如高斯分布）是（1）可列表译码的并且（2）在总变差距离方面具有“局部小”覆盖[ BKSW19]，则其混合物类是私密可学习的。证明绕过了一个已知障碍，表明与高斯分布不同，GMMs不具有局部小的覆盖[AAL21]。",
    "tldr": "通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。"
}