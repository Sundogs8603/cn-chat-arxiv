{
    "title": "Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning. (arXiv:2309.05505v1 [cs.LG])",
    "abstract": "Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \\DPFEDREP\\ converges to a ball centered around the \\emph{global o",
    "link": "http://arxiv.org/abs/2309.05505",
    "context": "Title: Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning. (arXiv:2309.05505v1 [cs.LG])\nAbstract: Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \\DPFEDREP\\ converges to a ball centered around the \\emph{global o",
    "path": "papers/23/09/2309.05505.json",
    "total_tokens": 909,
    "translated_title": "分享你的表示：在联邦学习中保证隐私和效用的权衡的改善",
    "translated_abstract": "在联邦学习中重复参数共享会导致私人数据的显著信息泄露，从而违背了数据隐私的主要目的。尽管使用最先进的差分隐私算法可以减轻信息泄露的风险，但这并非没有代价。随机化机制可能会阻止模型对有用的表示函数的学习收敛，尤其是当本地模型之间在分类函数上存在更大的不一致性时（由于数据异构性）。本文考虑一种表示联邦学习目标，鼓励各方在保障差分隐私的同时共同改进模型的共识部分，并且单独允许足够的自由进行本地个性化（无需共享）。我们证明在线性表示设置中，虽然目标是非凸的，我们提出的新算法DPFEDREP会收敛到以全局为中心的球形区域。",
    "tldr": "在联邦学习中重复参数共享会泄露私人数据，本文提出了一种表示联邦学习目标，旨在在保障差分隐私的同时允许本地个性化，通过提出的新算法DPFEDREP，可以在线性表示设置中收敛到以全局为中心的球形区域。",
    "en_tdlr": "Repeated parameter sharing in federated learning leads to information leakage, and this paper proposes a representation federated learning objective that allows local personalization while ensuring differential privacy. The proposed algorithm, DPFEDREP, converges to a ball centered around the global representation in the linear setting."
}