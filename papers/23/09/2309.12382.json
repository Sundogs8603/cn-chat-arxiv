{
    "title": "SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap. (arXiv:2309.12382v1 [cs.CV])",
    "abstract": "Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonst",
    "link": "http://arxiv.org/abs/2309.12382",
    "context": "Title: SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap. (arXiv:2309.12382v1 [cs.CV])\nAbstract: Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonst",
    "path": "papers/23/09/2309.12382.json",
    "total_tokens": 997,
    "translated_title": "SCOB: 通过字符级别监督对比学习和在线文本渲染进行领域间差异的通用文本理解",
    "translated_abstract": "受基于语言模型（LM）的预训练取得的巨大成功的启发，最近的研究在视觉文档理解中探索了基于LM的预训练方法来建模文档图像中的文本。其中，从图像中读取所有文本的预训练方法显示出了很大的潜力，但在应用于更广泛领域（如包括视觉文档和场景文本图像的领域）时往往具有不稳定性甚至失败。这对于现实世界的场景来说是一个重大限制，因为在各种领域中处理文本图像输入是至关重要的。在本文中，我们研究了更广泛领域中的有效预训练任务，并提出了一种新的预训练方法，名为SCOB，它利用字符级别的监督对比学习和在线文本渲染来通过弥合领域差异有效地预训练文档和场景文本领域。此外，SCOB实现了弱监督学习，大大降低了注释成本。广泛的基准测试结果证明了SCOB的优越性能。",
    "tldr": "本文提出了一种名为SCOB的新预训练模型，它通过字符级别的监督对比学习和在线文本渲染来有效地预训练文档和场景文本领域，从而弥合了领域之间的差异。SCOB还能实现弱监督学习，大大降低了注释成本。",
    "en_tdlr": "SCOB, a novel pre-training model, effectively bridges the gap between document and scene text domains by leveraging character-wise supervised contrastive learning and online text rendering. It also enables weakly supervised learning, significantly reducing annotation costs."
}