{
    "title": "Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation. (arXiv:2309.07998v1 [cs.CL])",
    "abstract": "Human evaluation has been widely accepted as the standard for evaluating chat-oriented dialogue systems. However, there is a significant variation in previous work regarding who gets recruited as evaluators. Evaluator groups such as domain experts, university students, and professional annotators have been used to assess and compare dialogue systems, although it is unclear to what extent the choice of an evaluator group can affect results. This paper analyzes the evaluator group impact on dialogue system evaluation by testing 4 state-of-the-art dialogue systems using 4 distinct evaluator groups. Our analysis reveals a robustness towards evaluator groups for Likert evaluations that is not seen for Pairwise, with only minor differences observed when changing evaluator groups. Furthermore, two notable limitations to this robustness are observed, which reveal discrepancies between evaluators with different levels of chatbot expertise and indicate that evaluator objectivity is beneficial fo",
    "link": "http://arxiv.org/abs/2309.07998",
    "context": "Title: Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation. (arXiv:2309.07998v1 [cs.CL])\nAbstract: Human evaluation has been widely accepted as the standard for evaluating chat-oriented dialogue systems. However, there is a significant variation in previous work regarding who gets recruited as evaluators. Evaluator groups such as domain experts, university students, and professional annotators have been used to assess and compare dialogue systems, although it is unclear to what extent the choice of an evaluator group can affect results. This paper analyzes the evaluator group impact on dialogue system evaluation by testing 4 state-of-the-art dialogue systems using 4 distinct evaluator groups. Our analysis reveals a robustness towards evaluator groups for Likert evaluations that is not seen for Pairwise, with only minor differences observed when changing evaluator groups. Furthermore, two notable limitations to this robustness are observed, which reveal discrepancies between evaluators with different levels of chatbot expertise and indicate that evaluator objectivity is beneficial fo",
    "path": "papers/23/09/2309.07998.json",
    "total_tokens": 980,
    "translated_title": "探索人类评估员群体对面向对话评估的影响",
    "translated_abstract": "人工评估被广泛接受作为评估面向对话系统的标准。然而，在以往的研究中，在招募评估员的方面存在显著的差异。评估员群体，如领域专家、大学生和专业标注员，已被用于评估和比较对话系统，尽管不清楚评估员群体的选择在多大程度上会影响结果。本文通过使用4个不同的评估员群体对4个最先进的对话系统进行测试，分析了评估员群体对对话系统评估的影响。我们的分析揭示了Likert评估在评估员群体方面的稳健性，而Pairwise评估没有这种稳健性，在更改评估员群体时只观察到了一些细微的差异。此外，我们观察到了两个显著的局限性，揭示了不同水平的聊天机器人专长的评估员之间存在差异，并表明评估员的客观性是有益的。",
    "tldr": "本文通过对4个不同的评估员群体对4个最先进的对话系统进行测试，分析了评估员群体对对话系统评估的影响。结果显示，对于Likert评估，评估员群体具有稳健性，而Pairwise评估没有。此外，还发现了不同评估员之间存在差异的局限性，并且评估员的客观性是有益的。",
    "en_tdlr": "This paper analyzes the impact of evaluator groups on dialogue system evaluation by testing 4 state-of-the-art dialogue systems with 4 distinct evaluator groups. The results show robustness in evaluator groups for Likert evaluations, but not for Pairwise evaluations. Furthermore, limitations are observed in terms of differences among evaluators and the benefit of evaluator objectivity."
}