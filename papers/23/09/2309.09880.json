{
    "title": "Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])",
    "abstract": "Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t",
    "link": "http://arxiv.org/abs/2309.09880",
    "context": "Title: Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])\nAbstract: Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t",
    "path": "papers/23/09/2309.09880.json",
    "total_tokens": 885,
    "translated_title": "由堆叠回归减少误差",
    "translated_abstract": "堆叠回归是一种集成技术，它通过形成不同回归估计器的线性组合来提高预测准确性。传统方法使用交叉验证数据来生成由构成估计器预测，并使用带非负性约束的最小二乘法学习组合权重。在本文中，我们类似地通过最小化一种估计的总体风险来学习这些权重，并受到非负性约束。当构成的估计器是通过至少三个维度分隔的嵌套子空间的线性最小二乘投影时，我们证明由于收缩效应，所得到的堆叠估计器的总体风险严格小于其中最佳的单个估计器。这里的“最佳”是指最小化选择准则如AIC或BIC的模型。换句话说，在这种情况下，最佳的单个估计器是不可接受的。因为优化问题可以重构为同信息回归，所以...",
    "tldr": "本文提出了一种新的堆叠回归方法，通过最小化总体风险并受非负性约束，成功降低了误差。实验证明，堆叠估计器相比其中最佳的单个估计器具有更小的总体风险。"
}