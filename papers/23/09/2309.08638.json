{
    "title": "Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])",
    "abstract": "Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where",
    "link": "http://arxiv.org/abs/2309.08638",
    "context": "Title: Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])\nAbstract: Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where",
    "path": "papers/23/09/2309.08638.json",
    "total_tokens": 964,
    "translated_title": "锚点：用更少的示例对模型进行基准测试",
    "translated_abstract": "现代语言模型通常表现出强大但脆弱的行为，因此开发出更大、更多样化的基准来可靠地评估它们的行为。在这里，我们建议可以使用更小的评估集对模型性能进行基准测试和阐明。我们首先展示了在六个流行语言分类基准中，模型对许多点对的正确类别的置信度在各个模型之间具有强相关性。我们在此现象基础上提出了锚点选择技术，该技术可以选择捕捉整个数据集上的模型行为的小子集。锚点可靠地对模型进行排序：在87个不同的语言模型-提示对上，使用1-30个锚点评估模型在准确排序模型方面优于均匀采样和其他基线方法。此外，只需要几个锚点就可以用较低的平均绝对误差估计出模型对数据集中所有其他点的每个类别的预测，足以衡量模型在哪些方面表现得如何。",
    "tldr": "这个论文介绍了一种使用更少的示例来对模型进行基准测试的方法，并提出了锚点选择技术来捕捉模型行为。实验证明，使用锚点对模型进行排序比使用均匀采样和其他基线方法更准确。仅使用几个锚点就可以估计模型对数据集中所有其他点的每个类别的预测，用于衡量模型性能。",
    "en_tdlr": "This paper presents a method to benchmark models with fewer examples and introduces anchor point selection technique to capture model behavior. The experiments show that ranking models using anchor points is more accurate than uniform sampling and other baselines. Only a few anchor points are needed to estimate model predictions for each class on all other points in the dataset for performance evaluation."
}