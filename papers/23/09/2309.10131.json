{
    "title": "Deep Prompt Tuning for Graph Transformers. (arXiv:2309.10131v1 [cs.LG])",
    "abstract": "Graph transformers have gained popularity in various graph-based tasks by addressing challenges faced by traditional Graph Neural Networks. However, the quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks. Fine-tuning, a common approach, is resource-intensive and requires storing multiple copies of large models. We propose a novel approach called deep graph prompt tuning as an alternative to fine-tuning for leveraging large graph transformer models in downstream graph based prediction tasks. Our method introduces trainable feature nodes to the graph and pre-pends task-specific tokens to the graph transformer, enhancing the model's expressive power. By freezing the pre-trained parameters and only updating the added tokens, our approach reduces the number of free parameters and eliminates the need for multiple model copies, making it suitable for small dataset",
    "link": "http://arxiv.org/abs/2309.10131",
    "context": "Title: Deep Prompt Tuning for Graph Transformers. (arXiv:2309.10131v1 [cs.LG])\nAbstract: Graph transformers have gained popularity in various graph-based tasks by addressing challenges faced by traditional Graph Neural Networks. However, the quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks. Fine-tuning, a common approach, is resource-intensive and requires storing multiple copies of large models. We propose a novel approach called deep graph prompt tuning as an alternative to fine-tuning for leveraging large graph transformer models in downstream graph based prediction tasks. Our method introduces trainable feature nodes to the graph and pre-pends task-specific tokens to the graph transformer, enhancing the model's expressive power. By freezing the pre-trained parameters and only updating the added tokens, our approach reduces the number of free parameters and eliminates the need for multiple model copies, making it suitable for small dataset",
    "path": "papers/23/09/2309.10131.json",
    "total_tokens": 905,
    "translated_title": "图形变换器的深度指导调优",
    "translated_abstract": "图形变换器通过解决传统图神经网络面临的挑战，在各种基于图形的任务中变得越来越受欢迎。然而，自我注意力操作的二次复杂性和图形变换器架构中的大规模层叠给将其应用于基于图形的预测任务带来了挑战。常见方法fine-tuning耗费资源且需要存储多个大型模型的副本。我们提出了一种新颖的方法，名为深度图形指导调优，作为在下游图形预测任务中利用大型图形变换器模型的替代方法。我们的方法引入了可训练的特征节点到图形中，并在图形变换器上预先添加任务特定的令牌，增强了模型的表达能力。通过冻结预训练参数并仅更新添加的令牌，我们的方法减少了自由参数的数量，消除了多个模型副本的需求，使其适用于小型数据集。",
    "tldr": "提出了一种称为深度图形指导调优的替代fine-tuning的方法，通过引入可训练的特征节点和任务特定的令牌，来增强图形变换器模型在下游图形预测任务中的表达能力，同时减少了自由参数的数量和模型副本的需求，适用于小型数据集。"
}