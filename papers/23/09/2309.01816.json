{
    "title": "Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana",
    "link": "http://arxiv.org/abs/2309.01816",
    "context": "Title: Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana",
    "path": "papers/23/09/2309.01816.json",
    "total_tokens": 862,
    "translated_title": "计算和通信高效的无线网络联合学习",
    "translated_abstract": "联合学习（FL）能够在边缘设备上进行分布式学习，并保护数据隐私。然而，由于设备数据的异质性，学习准确度下降，在计算能力有限和无线资源有限的设备上更新大规模学习模型会增加计算和通信延迟。我们考虑了一种新颖的FL框架，其中包括模型剪枝和个性化，以克服这些挑战。该框架将学习模型分为全局部分和个性化部分，全局部分通过模型剪枝与所有设备共享以学习数据表示，个性化部分针对特定设备进行微调，在FL过程中调整模型大小以减少计算和通信延迟，并提高非独立同分布（non-IID）数据设备的学习准确度。然后，对所提出的FL框架的计算和通信延迟以及收敛分析进行了数学分析。",
    "tldr": "提出了一种计算和通信高效的无线网络联合学习框架，通过模型剪枝和个性化，在分布式学习中减少计算和通信延迟，并提高非独立同分布数据设备的学习准确度。",
    "en_tdlr": "This study proposes a computation and communication efficient framework for federated learning over wireless networks, which reduces computation and communication latency and improves learning accuracy for devices with non-IID data through model pruning and personalization."
}