{
    "title": "The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings",
    "abstract": "arXiv:2309.09783v2 Announce Type: replace  Abstract: The paper presents a new training dataset of sentences in 7 languages, manually annotated for sentiment, which are used in a series of experiments focused on training a robust sentiment identifier for parliamentary proceedings. The paper additionally introduces the first domain-specific multilingual transformer language model for political science applications, which was additionally pre-trained on 1.72 billion words from parliamentary proceedings of 27 European parliaments. We present experiments demonstrating how the additional pre-training on parliamentary data can significantly improve the model downstream performance, in our case, sentiment identification in parliamentary proceedings. We further show that our multilingual model performs very well on languages not seen during fine-tuning, and that additional fine-tuning data from other languages significantly improves the target parliament's results. The paper makes an important ",
    "link": "https://arxiv.org/abs/2309.09783",
    "context": "Title: The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings\nAbstract: arXiv:2309.09783v2 Announce Type: replace  Abstract: The paper presents a new training dataset of sentences in 7 languages, manually annotated for sentiment, which are used in a series of experiments focused on training a robust sentiment identifier for parliamentary proceedings. The paper additionally introduces the first domain-specific multilingual transformer language model for political science applications, which was additionally pre-trained on 1.72 billion words from parliamentary proceedings of 27 European parliaments. We present experiments demonstrating how the additional pre-training on parliamentary data can significantly improve the model downstream performance, in our case, sentiment identification in parliamentary proceedings. We further show that our multilingual model performs very well on languages not seen during fine-tuning, and that additional fine-tuning data from other languages significantly improves the target parliament's results. The paper makes an important ",
    "path": "papers/23/09/2309.09783.json",
    "total_tokens": 835,
    "translated_title": "ParlaSent多语种训练数据集用于议会会议情感识别",
    "translated_abstract": "这篇论文介绍了一个新的训练数据集，其中包含7种语言的句子，手动标注了情感，并在一系列实验中用于训练一个针对议会会议的情感识别器。该论文还介绍了第一个专门针对政治科学应用的领域特定多语种变压器语言模型，该模型额外在27个欧洲议会的议会文件中预先训练了17.2亿字。我们展示了额外在议会数据上进行预训练如何显著提高模型在下游任务中的性能，即在议会会议中进行情感识别。我们进一步表明，我们的多语种模型在未在微调中见过的语言上表现非常好，并且来自其他语言的额外微调数据显著提高了目标议会的结果。",
    "tldr": "该研究介绍了ParlaSent多语种训练数据集，以及首个专门针对政治科学应用的领域特定多语种变压器语言模型，通过在议会数据上进行预训练，显著提高了情感识别模型在议会会议中的性能。",
    "en_tdlr": "This paper introduces the ParlaSent multilingual training dataset and the first domain-specific multilingual transformer language model for political science applications, significantly improving the performance of sentiment identification models in parliamentary proceedings through pre-training on parliamentary data."
}