{
    "title": "M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection. (arXiv:2309.08365v1 [cs.CV])",
    "abstract": "Most existing salient object detection methods mostly use U-Net or feature pyramid structure, which simply aggregates feature maps of different scales, ignoring the uniqueness and interdependence of them and their respective contributions to the final prediction. To overcome these, we propose the M$^3$Net, i.e., the Multilevel, Mixed and Multistage attention network for Salient Object Detection (SOD). Firstly, we propose Multiscale Interaction Block which innovatively introduces the cross-attention approach to achieve the interaction between multilevel features, allowing high-level features to guide low-level feature learning and thus enhancing salient regions. Secondly, considering the fact that previous Transformer based SOD methods locate salient regions only using global self-attention while inevitably overlooking the details of complex objects, we propose the Mixed Attention Block. This block combines global self-attention and window self-attention, aiming at modeling context at b",
    "link": "http://arxiv.org/abs/2309.08365",
    "context": "Title: M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection. (arXiv:2309.08365v1 [cs.CV])\nAbstract: Most existing salient object detection methods mostly use U-Net or feature pyramid structure, which simply aggregates feature maps of different scales, ignoring the uniqueness and interdependence of them and their respective contributions to the final prediction. To overcome these, we propose the M$^3$Net, i.e., the Multilevel, Mixed and Multistage attention network for Salient Object Detection (SOD). Firstly, we propose Multiscale Interaction Block which innovatively introduces the cross-attention approach to achieve the interaction between multilevel features, allowing high-level features to guide low-level feature learning and thus enhancing salient regions. Secondly, considering the fact that previous Transformer based SOD methods locate salient regions only using global self-attention while inevitably overlooking the details of complex objects, we propose the Mixed Attention Block. This block combines global self-attention and window self-attention, aiming at modeling context at b",
    "path": "papers/23/09/2309.08365.json",
    "total_tokens": 973,
    "translated_title": "M$^3$Net：用于显著目标检测的多级、混合和多阶段注意力网络",
    "translated_abstract": "大多数现有的显著目标检测方法主要使用U-Net或特征金字塔结构，简单地聚合不同尺度的特征图，忽视了它们的独特性和相互依赖性以及它们对最终预测的贡献。为了克服这些问题，我们提出了M$^3$Net，即多级、混合和多阶段注意力网络用于显著目标检测（SOD）。首先，我们提出了多尺度交互块，创新性地引入了交叉注意力方法来实现多级特征之间的交互，允许高层特征指导低层特征学习，从而增强显著区域。其次，考虑到以前基于Transformer的SOD方法只使用全局自注意力来定位显著区域，而无法避免忽视复杂对象的细节，我们提出了混合注意力块。此块结合了全局自注意力和窗口自注意力，旨在建模上下文。",
    "tldr": "本研究提出了M$^3$Net，一种用于显著目标检测的多级、混合和多阶段注意力网络。通过引入多尺度交互块和混合注意力块，该方法能够提高显著区域的检测性能并准确定位复杂对象的细节。",
    "en_tdlr": "This study proposes M$^3$Net, a multilevel, mixed and multistage attention network for salient object detection. By introducing multiscale interaction block and mixed attention block, this method improves the detection performance of salient regions and accurately locates the details of complex objects."
}