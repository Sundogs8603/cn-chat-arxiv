{
    "title": "Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])",
    "abstract": "Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs, as the reasoner is expected to couple a sequential answer with a structured explanation which embodies both the correct presentation and the correct reasoning process. In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a de",
    "link": "http://arxiv.org/abs/2309.08347",
    "context": "Title: Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])\nAbstract: Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs, as the reasoner is expected to couple a sequential answer with a structured explanation which embodies both the correct presentation and the correct reasoning process. In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a de",
    "path": "papers/23/09/2309.08347.json",
    "total_tokens": 808,
    "translated_title": "生成半结构化解释的奖励工程",
    "translated_abstract": "半结构化解释描绘了一个推理者的隐式过程和显式表示。这种解释突出了在特定查询中可用信息如何与推理者从内部权重产生的信息相结合，以生成答案。尽管语言模型的生成能力最近有所改进，但生成结构化解释以验证模型真正的推理能力仍然是一个挑战。对于规模不是很大的语言模型而言，这个问题尤为明显，因为推理者被期望将顺序的答案与体现正确展示和正确推理过程的结构化解释相结合。在这项工作中，我们首先强调了监督微调(SFT)在应对这一挑战方面的局限性，然后在强化学习(RL)中引入了一种精心设计的奖励工程方法，以更好地解决这个问题。我们研究了多种奖励聚合方法，并提供了一种...",
    "tldr": "本论文提出了一种奖励工程方法，在生成语言模型的半结构化解释方面取得了增强效果，解决了模型推理能力验证的问题。",
    "en_tdlr": "This paper proposes a reward engineering method that enhances the generation of semi-structured explanations in language models, addressing the challenge of verifying the model's reasoning capabilities."
}