{
    "title": "Contrastive Learning as Kernel Approximation. (arXiv:2309.02651v1 [cs.LG])",
    "abstract": "In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.  The goal of this thesis is to provide an overview of the current theoretical understanding of co",
    "link": "http://arxiv.org/abs/2309.02651",
    "context": "Title: Contrastive Learning as Kernel Approximation. (arXiv:2309.02651v1 [cs.LG])\nAbstract: In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.  The goal of this thesis is to provide an overview of the current theoretical understanding of co",
    "path": "papers/23/09/2309.02651.json",
    "total_tokens": 844,
    "translated_title": "对比学习作为核近似",
    "translated_abstract": "在标准的监督机器学习中，需要为数据中的每个输入提供标签。虽然在许多应用领域，原始数据很容易获得，但手动标注这些数据的成本太高。为了解决这个问题，对比学习方法在大型未标注数据集上产生高维输入的低维向量表示（也称为特征）。这是通过使用对比损失函数进行训练来实现的，该函数强制要求在特征空间中，相似的输入具有较高的内积，而不相似的输入具有较低的内积。与逐个注释每个输入不同，只需定义一种采样相似和不相似输入对的方法即可。然后，对比特征可以作为输入提供给有限标注数据上的监督学习系统，以在感兴趣的最终任务上获得高的准确性。",
    "tldr": "本论文总结了目前对比学习作为核近似的理论理解，通过在大型未标注数据集上将高维输入转化为低维特征表示，可以实现在有限标注数据上达到高准确性的监督学习。"
}