{
    "title": "A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration. (arXiv:2309.17192v1 [cs.LG])",
    "abstract": "Due to data privacy constraints, data sharing among multiple clinical centers is restricted, which impedes the development of high performance deep learning models from multicenter collaboration. Naive weight transfer methods share intermediate model weights without raw data and hence can bypass data privacy restrictions. However, performance drops are typically observed when the model is transferred from one center to the next because of the forgetting problem. Incremental transfer learning, which combines peer-to-peer federated learning and domain incremental learning, can overcome the data privacy issue and meanwhile preserve model performance by using continual learning techniques. In this work, a conventional domain/task incremental learning framework is adapted for incremental transfer learning. A comprehensive survey on the efficacy of different regularization-based continual learning methods for multicenter collaboration is performed. The influences of data heterogeneity, class",
    "link": "http://arxiv.org/abs/2309.17192",
    "context": "Title: A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration. (arXiv:2309.17192v1 [cs.LG])\nAbstract: Due to data privacy constraints, data sharing among multiple clinical centers is restricted, which impedes the development of high performance deep learning models from multicenter collaboration. Naive weight transfer methods share intermediate model weights without raw data and hence can bypass data privacy restrictions. However, performance drops are typically observed when the model is transferred from one center to the next because of the forgetting problem. Incremental transfer learning, which combines peer-to-peer federated learning and domain incremental learning, can overcome the data privacy issue and meanwhile preserve model performance by using continual learning techniques. In this work, a conventional domain/task incremental learning framework is adapted for incremental transfer learning. A comprehensive survey on the efficacy of different regularization-based continual learning methods for multicenter collaboration is performed. The influences of data heterogeneity, class",
    "path": "papers/23/09/2309.17192.json",
    "total_tokens": 955,
    "translated_title": "增量迁移学习调查: 将点对点联邦学习与领域增量学习相结合用于多中心协作",
    "translated_abstract": "由于数据隐私限制，多个临床中心之间的数据共享受到限制，这阻碍了从多中心协作中开发高性能深度学习模型的发展。朴素的权重转移方法在没有原始数据的情况下分享中间模型权重，因此可以绕过数据隐私限制。然而，通常在模型从一个中心转移到下一个中心时会观察到性能下降，这是由于遗忘问题。增量迁移学习通过使用连续学习技术，结合点对点联邦学习和领域增量学习，可以克服数据隐私问题，并同时保持模型性能。在这项工作中，将传统的领域/任务增量学习框架用于增量迁移学习。对于多中心协作，还进行了对不同基于正则化的连续学习方法的有效性进行全面调查。数据异质性，类别不平衡和领域知识共享对增量迁移学习的影响",
    "tldr": "该调查研究了增量迁移学习方法，通过结合点对点联邦学习和领域增量学习，克服了数据隐私限制，并使用连续学习技术保持模型性能。调查还探讨了在多中心协作中，不同正则化方法对增量迁移学习的有效性的影响。"
}