{
    "title": "Approximation Results for Gradient Descent trained Neural Networks. (arXiv:2309.04860v1 [cs.LG])",
    "abstract": "The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.",
    "link": "http://arxiv.org/abs/2309.04860",
    "context": "Title: Approximation Results for Gradient Descent trained Neural Networks. (arXiv:2309.04860v1 [cs.LG])\nAbstract: The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.",
    "path": "papers/23/09/2309.04860.json",
    "total_tokens": 783,
    "translated_title": "梯度下降训练的神经网络的近似结果",
    "translated_abstract": "该论文对采用梯度流训练的神经网络进行了近似保证，其中误差以连续的$L_2(\\mathbb{S}^{d-1})$范数在$d$维单位球面上测量，目标为Sobolev平滑。网络是完全连接的，深度恒定，宽度递增。虽然所有层都进行了训练，但梯度流的收敛性是基于对于非凸的倒数第二层的神经切向核(NTK)的论证。与标准的NTK分析不同，连续误差范数暗示了一个欠参数化的区域，在逼近时需要自然的光滑性假设。典型的过参数化通过逼近率的损失以及相对于Sobolev平滑函数的已建立的逼近方法而重新进入结果中。",
    "tldr": "该论文研究了采用梯度下降训练的神经网络的近似保证，利用连续的误差范数对网络进行分析，并发现在欠参数化的情况下相对于已有的逼近方法存在逼近率下降的问题。"
}