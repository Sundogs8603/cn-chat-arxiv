{
    "title": "FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation. (arXiv:2309.17174v1 [cs.LG])",
    "abstract": "Federated learning is a distributed learning framework that allows a set of clients to collaboratively train a model under the orchestration of a central server, without sharing raw data samples. Although in many practical scenarios the derivatives of the objective function are not available, only few works have considered the federated zeroth-order setting, in which functions can only be accessed through a budgeted number of point evaluations. In this work we focus on convex optimization and design the first federated zeroth-order algorithm to estimate the curvature of the global objective, with the purpose of achieving superlinear convergence. We take an incremental Hessian estimator whose error norm converges linearly, and we adapt it to the federated zeroth-order setting, sampling the random search directions from the Stiefel manifold for improved performance. In particular, both the gradient and Hessian estimators are built at the central server in a communication-efficient and pr",
    "link": "http://arxiv.org/abs/2309.17174",
    "context": "Title: FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation. (arXiv:2309.17174v1 [cs.LG])\nAbstract: Federated learning is a distributed learning framework that allows a set of clients to collaboratively train a model under the orchestration of a central server, without sharing raw data samples. Although in many practical scenarios the derivatives of the objective function are not available, only few works have considered the federated zeroth-order setting, in which functions can only be accessed through a budgeted number of point evaluations. In this work we focus on convex optimization and design the first federated zeroth-order algorithm to estimate the curvature of the global objective, with the purpose of achieving superlinear convergence. We take an incremental Hessian estimator whose error norm converges linearly, and we adapt it to the federated zeroth-order setting, sampling the random search directions from the Stiefel manifold for improved performance. In particular, both the gradient and Hessian estimators are built at the central server in a communication-efficient and pr",
    "path": "papers/23/09/2309.17174.json",
    "total_tokens": 915,
    "translated_title": "FedZeN:通过增量Hessian估计实现超线性零阶联邦学习",
    "translated_abstract": "联邦学习是一种分布式学习框架，允许一组客户端在中央服务器的编排下协同训练模型，而不共享原始数据样本。尽管在许多实际场景中，目标函数的导数是不可用的，但只有少数研究考虑了联邦零阶设置，在此设置中，只能通过预算数量的点评估来访问函数。在这项工作中，我们着重于凸优化，并设计了第一个联邦零阶算法来估计全局目标的曲率，以实现超线性收敛。我们采用了一种误差范数线性收敛的增量Hessian估计器，并将其适应于联邦零阶设置，从Stiefel流形中随机采样搜索方向以提高性能。特别地，梯度和Hessian估计器都在中央服务器上以通信效率高且省资源的方式构建。",
    "tldr": "本文提出了FedZeN，一种使用增量Hessian估计的超线性零阶联邦学习算法。这种算法可以在不共享原始数据样本的情况下，通过采用Stiefel流形中的随机搜索方向，估计全局目标的曲率，并实现超线性收敛。",
    "en_tdlr": "In this paper, we propose FedZeN, a superlinear zeroth-order federated learning algorithm that uses incremental Hessian estimation. The algorithm estimates the curvature of the global objective by sampling random search directions from the Stiefel manifold, achieving superlinear convergence without sharing raw data samples."
}