{
    "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)",
    "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate",
    "link": "http://arxiv.org/abs/2309.01838",
    "context": "Title: Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)\nAbstract: Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate",
    "path": "papers/23/09/2309.01838.json",
    "total_tokens": 909,
    "translated_title": "针对卷积神经网络的模型窃取攻击的高效防御方法",
    "translated_abstract": "模型窃取攻击对深度学习模型构成了严重威胁，攻击者可以通过查询其黑盒API来窃取已训练的模型。这可能导致知识产权盗窃和其他安全与隐私风险。目前针对模型窃取攻击的最先进防御方法建议向预测概率添加扰动，但其计算较重且对攻击者提出了不切实际的假设。通常需要训练辅助模型，这可能耗时且资源密集，妨碍了该防御方法在实际应用中的部署。本文提出了一种简单但有效且高效的防御替代方案。我们引入一种启发式方法来扰动输出概率。该防御方法可以轻松集成到模型中而无需额外训练。我们展示了我们的防御方法在抵御三种最先进的窃取攻击中的有效性。我们进行了评估。",
    "tldr": "本文提出了一种简单但有效且高效的防御替代方案，引入一种启发式方法来扰动输出概率，可以轻松集成到模型中而无需额外训练，并且在抵御最先进的窃取攻击中表现出有效性。",
    "en_tdlr": "This paper proposes a simple yet effective and efficient defense alternative against model stealing attacks, introducing a heuristic approach to perturb the output probabilities. It can be easily integrated into models without additional training and shows effectiveness in defending against state-of-the-art stealing attacks."
}