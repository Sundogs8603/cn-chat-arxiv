{
    "title": "Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])",
    "abstract": "Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions",
    "link": "http://arxiv.org/abs/2309.06256",
    "context": "Title: Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])\nAbstract: Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions",
    "path": "papers/23/09/2309.06256.json",
    "total_tokens": 918,
    "translated_title": "专业性与广泛性：关于基础模型微调中灾难性遗忘的实证研究",
    "translated_abstract": "基础模型，包括视觉语言模型(VLMs)和大型语言模型(LLMs)，具有处理多样分布和任务的广泛性，这源于它们广泛的预训练数据集。对基础模型进行微调是提高任务性能或调整模型行为与人类期望一致的常见做法，使其获得专业性。然而，用于微调的小型数据集可能无法充分覆盖预训练过程中遇到的多样分布和任务。因此，追求微调过程中的专业性可能导致模型的广泛性损失，这与深度学习中的灾难性遗忘(Catastrophic Forgetting, CF)相关。在本研究中，我们展示了这种现象在VLMs和LLMs中的存在。例如，对像CLIP这样的VLM进行在ImageNet上的微调会导致处理多样分布的广泛性损失，对医学领域的Galactica进行微调则会导致遵循指令的能力损失。",
    "tldr": "本研究实证了基础模型微调中的灾难性遗忘现象，微调过程中追求专业性会导致模型的广泛性损失。",
    "en_tdlr": "This study empirically demonstrates the phenomenon of catastrophic forgetting in fine-tuning foundation models, where pursuing speciality during fine-tuning leads to a loss of generality in the model."
}