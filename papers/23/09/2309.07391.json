{
    "title": "EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])",
    "abstract": "The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music or environmental sounds. To approach this problem, methods inspired by self-supervised models from NLP, like BERT, are often used and adapted to audio. These models rely on the discrete nature of text, hence adopting this type of approach for audio processing requires either a change in the learning objective or mapping the audio signal to a set of discrete classes. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for learning an universal audio model based on a masked autoencoder (MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of audio tasks spanning speech, music and environmental sounds, achieving performances comparable or better than leading audio representation models.",
    "link": "http://arxiv.org/abs/2309.07391",
    "context": "Title: EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])\nAbstract: The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music or environmental sounds. To approach this problem, methods inspired by self-supervised models from NLP, like BERT, are often used and adapted to audio. These models rely on the discrete nature of text, hence adopting this type of approach for audio processing requires either a change in the learning objective or mapping the audio signal to a set of discrete classes. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for learning an universal audio model based on a masked autoencoder (MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of audio tasks spanning speech, music and environmental sounds, achieving performances comparable or better than leading audio representation models.",
    "path": "papers/23/09/2309.07391.json",
    "total_tokens": 923,
    "translated_title": "EnCodecMAE: 利用神经编解码器进行通用音频表示学习",
    "translated_abstract": "通用音频表示学习的目标是获得可以用于涉及语音、音乐或环境声音的各种后续任务的基础模型。为了解决这个问题，通常使用受自监督模型（如BERT）启发的方法，并将其应用于音频。这些模型依赖于文本的离散性质，因此采用这种方法来处理音频需要改变学习目标或将音频信号映射到一组离散类别。在这项工作中，我们探索了使用神经音频编解码器EnCodec生成用于基于遮蔽自动编码器（MAE）学习通用音频模型的离散目标的方法。我们评估了该方法，称之为EnCodecMAE，在涵盖语音、音乐和环境声音的广泛音频任务上，其性能相当或优于领先的音频表示模型。",
    "tldr": "本文提出了一种称为EnCodecMAE的方法，利用神经编解码器EnCodec生成离散目标，用于基于遮蔽自动编码器（MAE）学习通用音频模型。通过在涵盖语音、音乐和环境声音的多个音频任务上的评估，发现EnCodecMAE达到了与领先的音频表示模型相当甚至更好的性能。"
}