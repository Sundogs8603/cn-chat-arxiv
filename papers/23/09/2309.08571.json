{
    "title": "A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])",
    "abstract": "We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.",
    "link": "http://arxiv.org/abs/2309.08571",
    "context": "Title: A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])\nAbstract: We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.",
    "path": "papers/23/09/2309.08571.json",
    "total_tokens": 924,
    "translated_title": "一种贝叶斯方法用于稳健的逆强化学习",
    "translated_abstract": "我们考虑一种贝叶斯方法用于离线模型导向的逆强化学习(IRL)。所提出的框架通过同时估计专家的奖励函数和主观模型的环境动态，区别于现有的离线模型导向的IRL方法。我们利用一类先验分布来参数化专家对环境的模型的准确性，以开发在高维环境中估计专家奖励和主观动态的高效算法。我们的分析揭示了一个新的见解，即当先验地认为专家对环境具有高度准确的模型时，估计的策略表现出稳健性能。我们在MuJoCo环境中验证了这一观察，并展示了我们的算法胜过最先进的离线IRL算法。",
    "tldr": "这篇论文提出了一种贝叶斯方法，用于稳健的离线模型导向的逆强化学习。通过同时估计专家的奖励函数和主观模型的环境动态，利用先验分布参数化专家对环境模型的准确性，提出了高效的算法。实验证明，当先验地认为专家对环境具有高度准确的模型时，估计的策略表现出稳健性能，并且优于最先进的离线IRL算法。",
    "en_tdlr": "This paper presents a Bayesian approach for robust offline model-based inverse reinforcement learning. By simultaneously estimating the expert's reward function and subjective model of environment dynamics, utilizing a prior distribution to parameterize the accuracy of the expert's model, efficient algorithms are developed. The results show that the estimated policy exhibits robust performance when the expert is believed to have a highly accurate model of the environment, outperforming state-of-the-art offline IRL algorithms."
}