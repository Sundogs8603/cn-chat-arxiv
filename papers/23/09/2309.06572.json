{
    "title": "Addressing the Blind Spots in Spoken Language Processing. (arXiv:2309.06572v1 [eess.AS])",
    "abstract": "This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the",
    "link": "http://arxiv.org/abs/2309.06572",
    "context": "Title: Addressing the Blind Spots in Spoken Language Processing. (arXiv:2309.06572v1 [eess.AS])\nAbstract: This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the",
    "path": "papers/23/09/2309.06572.json",
    "total_tokens": 865,
    "translated_title": "解决口语语言处理的盲区",
    "translated_abstract": "本文探讨了非语言线索在人类交流中的关键但往往被忽视的作用，包括协同语言手势和面部表情，并探讨这些线索对自然语言处理（NLP）的影响。我们认为理解人类交流需要一种更全面的方法，超越文本或口语词汇，包括非语言元素。借鉴手语处理的进展，我们提出发展通用的自动手势分割和转录模型，将这些非语言线索转录成文本形式。这种方法旨在弥补口语语言理解中的盲点，增强NLP模型的范围和适用性。通过示例，我们证明了仅依靠基于文本的模型的局限性。我们提出了一种计算效率高且灵活的方法，可以与现有的NLP流程无缝融合，并通过呼吁对现有方法的改进来结束。",
    "tldr": "本文探讨了非语言线索在人类交流中的关键作用，并借鉴手语处理的进展，提出发展通用的自动手势分割和转录模型来弥补口语语言理解中的盲点，并增强NLP模型的范围和适用性。",
    "en_tdlr": "This paper explores the crucial role of non-verbal cues in human communication and proposes the development of universal automatic gesture segmentation and transcription models to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models."
}