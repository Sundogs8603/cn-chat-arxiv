{
    "title": "Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])",
    "abstract": "The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \\times$ with a minimal accuracy drop ",
    "link": "http://arxiv.org/abs/2309.06626",
    "context": "Title: Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])\nAbstract: The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \\times$ with a minimal accuracy drop ",
    "path": "papers/23/09/2309.06626.json",
    "total_tokens": 909,
    "translated_title": "通过半结构激活稀疏加速深度神经网络",
    "translated_abstract": "在嵌入式设备上高效处理深度神经网络（DNNs）的需求是限制其部署的重要挑战之一。利用网络特征图中的稀疏性是减少推断延迟的一种方式。已知非结构化稀疏性与结构化稀疏性相比对精度下降的影响较小，但前者需要进行广泛的推断引擎更改以获得延迟优势。为了解决这个问题，我们提出了一种通过小型运行时修改引入半结构激活稀疏性的解决方案。为了在推断时获得高加速度水平，我们设计了一种稀疏训练过程，同时在计算广义矩阵乘法（GEMM）时考虑激活的最终位置。我们对各种图像分类和目标检测任务的模型对所提出的解决方案进行了广泛评估。值得注意的是，我们的方法在保持精度下降最小的情况下提供了1.25倍的速度提升。",
    "tldr": "通过小型运行时修改引入半结构激活稀疏性，我们设计了一种稀疏训练过程，在保持精度下降最小的情况下，实现了深度神经网络的高效处理和推断加速。",
    "en_tdlr": "By introducing semi-structured activation sparsity through minor runtime modifications and designing a sparse training procedure, we achieve efficient processing and inference acceleration of deep neural networks while minimizing the drop in accuracy."
}