{
    "title": "Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])",
    "abstract": "Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to",
    "link": "http://arxiv.org/abs/2309.04222",
    "context": "Title: Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])\nAbstract: Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to",
    "path": "papers/23/09/2309.04222.json",
    "total_tokens": 921,
    "translated_title": "未观察到潜在混淆因素下的离线推荐系统评估",
    "translated_abstract": "离线政策估计方法(OPE)允许我们从记录的数据中学习和评估决策策略，使它们成为离线评估推荐系统的吸引人选择。最近的一些作品报道了成功采用OPE方法的情况。这项工作的一个重要假设是不存在未观察到的混淆因素：在数据收集时影响行动和奖励的随机变量。由于数据收集策略通常在从业者的控制之下，因此很少明确地提及无混淆假设，并且现有文献中很少处理其违规问题。这项工作旨在强调在存在未观察到的混淆因素的情况下进行离线策略估计时出现的问题，特别关注推荐系统的用例。我们专注于基于策略的估计器，其中日志倾向是从记录数据中学习的。我们对由于混淆因素引起的统计偏差进行了描述。",
    "tldr": "本论文讨论了在存在潜在混淆因素的情况下进行离线推荐系统评估的问题，并特别关注推荐系统用例。通过对基于策略的估计器进行研究，我们描述了由混淆因素引起的统计偏差。"
}