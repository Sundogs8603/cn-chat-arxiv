{
    "title": "MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])",
    "abstract": "Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and",
    "link": "http://arxiv.org/abs/2309.17341",
    "context": "Title: MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])\nAbstract: Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and",
    "path": "papers/23/09/2309.17341.json",
    "total_tokens": 814,
    "translated_title": "MixQuant: 带有位宽优化搜索的混合精度量化",
    "translated_abstract": "量化是一种创建高效深度神经网络（DNNs）的技术，它通过在比f32浮点精度更低的位宽上执行计算和存储张量来实现。量化减少了模型大小和推理延迟，因此可以在计算资源受限和实时系统上部署DNNs。然而，量化可能会导致由舍入误差引起的数值不稳定性，从而导致计算不准确，进而降低了量化模型的准确性。类似于先前的研究表明，偏置和激活对量化更敏感，最好保持全精度或用更高的位宽进行量化，我们表明一些权重比其他权重更敏感，应在其量化位宽上反映出来。为此，我们提出了MixQuant，一种基于舍入误差的搜索算法，以找到每个层权重的最佳定制量化位宽。",
    "tldr": "本研究提出了一种称为MixQuant的混合精度量化算法，在每个层权重上找到了最佳的量化位宽，从而减少了量化模型的准确性降低问题。"
}