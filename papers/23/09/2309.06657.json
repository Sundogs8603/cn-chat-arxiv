{
    "title": "Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v1 [cs.CL])",
    "abstract": "Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted t",
    "link": "http://arxiv.org/abs/2309.06657",
    "context": "Title: Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v1 [cs.CL])\nAbstract: Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted t",
    "path": "papers/23/09/2309.06657.json",
    "total_tokens": 893,
    "translated_title": "统计拒绝抽样改进了优化偏好方法",
    "translated_abstract": "提高语言模型与人类偏好的一致性仍然是一个活跃的研究挑战。之前的方法主要使用强化学习从人类反馈中学习（RLHF），通过在线强化学习方法如近端策略优化（PPO）。最近，离线方法如序列似然校准（SLiC）和直接偏好优化（DPO）已经成为有吸引力的替代方案，提供了稳定性和可扩展性的改进，同时保持了竞争性能。SLiC通过使用从经过监督微调（SFT）策略中采样的序列对来优化其损失函数，而DPO直接根据偏好数据优化语言模型，无需单独的奖励模型。然而，目标最优策略的最大似然估计器（MLE）需要从该策略中采样标记的偏好对。DPO缺乏奖励模型限制其从最优策略中采样偏好对的能力，而SLiC则受到了限制。",
    "tldr": "本文提出了一种名为统计拒绝抽样的新方法，改进了优化偏好的过程，并解决了传统方法中缺乏奖励模型和从最优策略采样偏好对的问题。",
    "en_tdlr": "This paper introduces a new approach called statistical rejection sampling to improve preference optimization, addressing the limitations of traditional methods like the lack of a reward model and the inability to sample preference pairs from the optimal policy."
}