{
    "title": "On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])",
    "abstract": "Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\\eta$. For small $B$ and large $\\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\\equiv \\eta/B$. Yet this description is observed to break down for sufficiently large batches $B\\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\\eta$ plane that separates three dynamical phases: $\\textit{(i)}$ a noise-dominated SGD governed by temperature, $\\textit{(ii)}$ a large-first-step-dominated SGD and",
    "link": "http://arxiv.org/abs/2309.10688",
    "context": "Title: On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])\nAbstract: Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\\eta$. For small $B$ and large $\\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\\equiv \\eta/B$. Yet this description is observed to break down for sufficiently large batches $B\\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\\eta$ plane that separates three dynamical phases: $\\textit{(i)}$ a noise-dominated SGD governed by temperature, $\\textit{(ii)}$ a large-first-step-dominated SGD and",
    "path": "papers/23/09/2309.10688.json",
    "total_tokens": 914,
    "translated_title": "关于随机梯度下降的不同模式",
    "translated_abstract": "现代深度网络通过随机梯度下降（SGD）进行训练，其关键参数是每个步骤考虑的数据量或批量大小B以及步长或学习率η。对于小的B和大的η，SGD对应于参数的随机演化，其噪声幅度由“温度”T=η/B控制。然而当批量大小B≥B^*足够大时，这种描述被观察到失效，或者在温度足够小时简化为梯度下降（GD）。理解这些交叉发生的位置仍然是一个中心挑战。在这里，我们解决了这些问题，在一个教师-学生感知器分类模型中，我们展示了我们的关键预测仍适用于深度网络。具体来说，我们在B-η平面上获得了一个相位图，将三个动态阶段分开：（i）受温度控制的噪声主导的SGD，（ii）大步骤主导的SGD和",
    "tldr": "这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。",
    "en_tdlr": "This study addresses the problem of tracking and understanding the different regimes of stochastic gradient descent (SGD) by providing a phase diagram that distinguishes between noise-dominated SGD and large-first-step-dominated SGD."
}