{
    "title": "RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])",
    "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model recei",
    "link": "http://arxiv.org/abs/2309.07124",
    "context": "Title: RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])\nAbstract: Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model recei",
    "path": "papers/23/09/2309.07124.json",
    "total_tokens": 954,
    "translated_title": "RAIN: 您的语言模型可以自我调整而无需微调",
    "translated_abstract": "大型语言模型（LLM）常常与人类偏好存在不一致性。之前的研究通过收集人类偏好数据，然后使用强化学习或指导调优等方法对预训练模型进行微调以实现对齐。相比之下，无需任何额外数据对齐冻结的LLM更有吸引力。本研究探讨了后一种情景的潜力。我们发现通过将自我评估和回滚机制整合在一起，不对齐的LLM可以通过自我增强直接产生与人类偏好一致的响应。我们引入了一种新的推理方法，可回滚的自回归推理（RAIN），它允许预训练的LLM评估自己的生成，并利用评估结果来引导向后回滚和向前生成以确保人工智能的安全性。值得注意的是，RAIN在模型对齐时无需额外数据，并且不需要任何训练、梯度计算或参数更新；在自我评估阶段，模型接收的是一些随机回滚的生成样本，并将其与人类偏好进行比较。",
    "tldr": "本研究提出了RAIN方法，该方法可以在无需微调或额外数据的情况下，通过整合自我评估和回滚机制实现对齐冻结的语言模型，使其能够直接产生与人类偏好一致的响应。",
    "en_tdlr": "This study introduces the RAIN method, which allows frozen language models to align themselves without the need for fine-tuning or extra data, by integrating self-evaluation and rewind mechanisms. The RAIN method enables the models to directly generate responses consistent with human preferences."
}