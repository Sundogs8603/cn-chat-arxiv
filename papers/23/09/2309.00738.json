{
    "title": "Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability",
    "abstract": "The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiv",
    "link": "https://arxiv.org/abs/2309.00738",
    "context": "Title: Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability\nAbstract: The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiv",
    "path": "papers/23/09/2309.00738.json",
    "total_tokens": 1071,
    "translated_title": "重新思考图规范化在图表示学习中的能力",
    "translated_abstract": "近年来，图神经网络（GNNs）的表达能力得到了广泛研究，以揭示设计更强大的GNNs的原则。图规范化作为一种区分非同构图的典型方法，但在开发表达能力强的GNNs时很少被采用。本文提出通过图规范化最大化GNNs的表达能力，然后从模型稳定性的角度来研究这些GNNs的能力。稳定的GNN会将相似的图映射到在向量空间中紧密相连的图表示中，而GNN的稳定性对于将性能推广到未见过的图很关键。我们在理论上揭示了图规范化增强的GNNs在表达能力和稳定性之间的折衷。然后，我们引入了普遍图规范化的概念，作为解决折衷的通用解决方案，并表征了一种广泛适用的充分条件来解决普遍图规范化问题。一系列全面的实验证明了其有效性。",
    "tldr": "这篇论文提出了通过图规范化最大化GNNs表达能力的方法，并从模型稳定性角度研究了这些GNNs的能力。本文基于稳定GNN将相似的图映射到紧密相连的向量表示中，理论上揭示了图规范化增强的GNNs在表达能力和稳定性之间的折衷，提出了普遍图规范化的概念并给出了一种广泛适用的充分条件来解决普遍图规范化问题。实验证明了这种方法的有效性。"
}