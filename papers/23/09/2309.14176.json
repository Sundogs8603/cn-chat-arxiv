{
    "title": "Federated Learning Under Restricted User Availability. (arXiv:2309.14176v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a decentralized machine learning framework that enables collaborative model training while respecting data privacy. In various applications, non-uniform availability or participation of users is unavoidable due to an adverse or stochastic environment, the latter often being uncontrollable during learning. Here, we posit a generic user selection mechanism implementing a possibly randomized, stationary selection policy, suggestively termed as a Random Access Model (RAM). We propose a new formulation of the FL problem which effectively captures and mitigates limited participation of data originating from infrequent, or restricted users, at the presence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the (unknown) RAM distribution, we extend the expected loss FL objective to a risk-aware objective, enabling the design of an efficient training algorithm that is completely oblivious to the RAM, and with essentially identical complexity as FedAvg. O",
    "link": "http://arxiv.org/abs/2309.14176",
    "context": "Title: Federated Learning Under Restricted User Availability. (arXiv:2309.14176v1 [cs.LG])\nAbstract: Federated Learning (FL) is a decentralized machine learning framework that enables collaborative model training while respecting data privacy. In various applications, non-uniform availability or participation of users is unavoidable due to an adverse or stochastic environment, the latter often being uncontrollable during learning. Here, we posit a generic user selection mechanism implementing a possibly randomized, stationary selection policy, suggestively termed as a Random Access Model (RAM). We propose a new formulation of the FL problem which effectively captures and mitigates limited participation of data originating from infrequent, or restricted users, at the presence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the (unknown) RAM distribution, we extend the expected loss FL objective to a risk-aware objective, enabling the design of an efficient training algorithm that is completely oblivious to the RAM, and with essentially identical complexity as FedAvg. O",
    "path": "papers/23/09/2309.14176.json",
    "total_tokens": 900,
    "translated_title": "受限用户可用性下的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种分散的机器学习框架，可以在尊重数据隐私的同时进行协作模型训练。在各种应用中，由于不利或随机环境，用户的可用性或参与度不均匀是不可避免的，后者在学习期间往往是不可控制的。在这里，我们提出了一种通用的用户选择机制，实施可能是随机化的固定选择策略，暂时称为随机访问模型（RAM）。我们提出了FL问题的新的公式化，有效地捕捉并减轻源自不频繁或受限用户的数据有限参与的情况下，存在RAM的情况下。通过在（未知的）RAM分布上使用条件风险价值（CVaR），我们将期望损失FL目标扩展到风险感知目标，从而实现了一种完全不受RAM影响的高效训练算法的设计，并且复杂性与FedAvg基本相同。",
    "tldr": "本文研究了受限用户可用性下的联邦学习问题，提出了一种新的FL问题的公式化，并通过使用风险感知目标设计了一种高效训练算法，完全不受随机访问模型（RAM）的影响。"
}