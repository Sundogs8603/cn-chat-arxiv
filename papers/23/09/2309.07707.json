{
    "title": "CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v1 [cs.CL])",
    "abstract": "Large-scale self-supervised pre-trained speech encoders outperform conventional approaches in speech recognition and translation tasks. Due to the high cost of developing these large models, building new encoders for new tasks and deploying them to on-device applications are infeasible. Prior studies propose model compression methods to address this issue, but those works focus on smaller models and less realistic tasks. Thus, we propose Contrastive Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to compress pre-trained speech encoders by leveraging masked prediction and contrastive learning to train student models to copy the behavior of a large teacher model. CoLLD outperforms prior methods and closes the gap between small and large models on multilingual speech-to-text translation and recognition benchmarks.",
    "link": "http://arxiv.org/abs/2309.07707",
    "context": "Title: CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v1 [cs.CL])\nAbstract: Large-scale self-supervised pre-trained speech encoders outperform conventional approaches in speech recognition and translation tasks. Due to the high cost of developing these large models, building new encoders for new tasks and deploying them to on-device applications are infeasible. Prior studies propose model compression methods to address this issue, but those works focus on smaller models and less realistic tasks. Thus, we propose Contrastive Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to compress pre-trained speech encoders by leveraging masked prediction and contrastive learning to train student models to copy the behavior of a large teacher model. CoLLD outperforms prior methods and closes the gap between small and large models on multilingual speech-to-text translation and recognition benchmarks.",
    "path": "papers/23/09/2309.07707.json",
    "total_tokens": 859,
    "translated_title": "CoLLD: 对多语种预训练语音编码器的对比层与层蒸馏进行压缩",
    "translated_abstract": "大规模自监督预训练语音编码器在语音识别和翻译任务中表现优于传统方法。由于开发这些大模型的成本较高，为新任务构建新编码器并将其部署到设备应用中是不可行的。先前的研究提出了模型压缩方法来解决这个问题，但这些方法仅针对较小模型和不太实际的任务。因此，我们提出了对比层与层蒸馏（CoLLD），一种新颖的知识蒸馏方法，通过利用掩蔽预测和对比学习，训练学生模型复制大教师模型的行为来压缩预训练语音编码器。CoLLD在多语种语音到文本翻译和识别基准上胜过先前的方法，弥合了小模型和大模型之间的差距。",
    "tldr": "CoLLD是一种用于压缩预训练语音编码器的对比层与层蒸馏方法，通过学生模型复制大教师模型的行为来提高性能，并在多语种任务中取得了优异表现。"
}