{
    "title": "Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])",
    "abstract": "The increasingly Large Language Models (LLMs) demonstrate stronger language understanding and generation capabilities, while the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications. In this work, we focus on Parameter-Efficient Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG), which freeze most parameters in LLMs and tune a small subset of parameters in few-shot cases so that memory footprint, training cost, and labeling cost are reduced while maintaining or even improving the performance. We propose a Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better performance and generalization ability but without an obvious increase in training cost. Further study on intermediate SPT suggests the superior transferability of SPT in few-",
    "link": "http://arxiv.org/abs/2309.06759",
    "context": "Title: Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])\nAbstract: The increasingly Large Language Models (LLMs) demonstrate stronger language understanding and generation capabilities, while the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications. In this work, we focus on Parameter-Efficient Fine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG), which freeze most parameters in LLMs and tune a small subset of parameters in few-shot cases so that memory footprint, training cost, and labeling cost are reduced while maintaining or even improving the performance. We propose a Scaled Prompt-Tuning (SPT) method which surpasses conventional PT with better performance and generalization ability but without an obvious increase in training cost. Further study on intermediate SPT suggests the superior transferability of SPT in few-",
    "path": "papers/23/09/2309.06759.json",
    "total_tokens": 967,
    "translated_title": "Scaled Prompt-Tuning for Few-Shot Natural Language Generation. (arXiv:2309.06759v1 [cs.CL])",
    "translated_abstract": "越来越强大的大型语言模型（LLMs）展示了更强的语言理解和生成能力，但对下游任务对LLMs进行微调的内存需求和计算成本是不可忽视的。此外，微调通常需要来自各个任务的一定数量的数据，而数据收集成本是实际应用中需要考虑的另一个问题。在这项工作中，我们关注少样本自然语言生成（NLG）的参数高效微调（PEFT）方法，该方法冻结LLMs中的大多数参数，并在少样本情况下微调一小部分参数，以降低内存占用、训练成本和标注成本，同时保持甚至提高性能。我们提出了一种Scaled Prompt-Tuning (SPT)方法，它在性能和泛化能力方面超越了传统的Prompt-Tuning (PT)方法，但没有明显增加训练成本。进一步研究表明，在少样本情况下，SPT具有更好的迁移能力。",
    "tldr": "本论文提出了一种Scaled Prompt-Tuning (SPT)方法，用于少样本自然语言生成 (NLG)。该方法冻结大多数参数，只微调其中一小部分参数，以减少内存占用、训练成本和标注成本，并在性能和泛化能力方面超越了传统的方法。在少样本情况下，SPT展现了更好的迁移能力。",
    "en_tdlr": "This paper proposes a Scaled Prompt-Tuning (SPT) method for few-shot Natural Language Generation (NLG). The method freezes most parameters and only fine-tunes a small subset, reducing memory footprint, training cost, and labeling cost, while surpassing conventional methods in performance and generalization ability. SPT demonstrates superior transferability in few-shot scenarios."
}