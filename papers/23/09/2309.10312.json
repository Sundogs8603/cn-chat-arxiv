{
    "title": "Rigorously Assessing Natural Language Explanations of Neurons. (arXiv:2309.10312v1 [cs.CL])",
    "abstract": "Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron $a$ activates on all and only input strings that refer to a concept picked out by the proposed explanation $E$. In the intervention mode, we construe $E$ as a claim that the neuron $a$ is a causal mediator of the concept denoted by $E$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.",
    "link": "http://arxiv.org/abs/2309.10312",
    "context": "Title: Rigorously Assessing Natural Language Explanations of Neurons. (arXiv:2309.10312v1 [cs.CL])\nAbstract: Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron $a$ activates on all and only input strings that refer to a concept picked out by the proposed explanation $E$. In the intervention mode, we construe $E$ as a claim that the neuron $a$ is a causal mediator of the concept denoted by $E$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.",
    "path": "papers/23/09/2309.10312.json",
    "total_tokens": 940,
    "translated_title": "严格评估神经元的自然语言解释",
    "translated_abstract": "自然语言是解释大型语言模型如何处理和存储信息的一种吸引人的方式，但评估这种解释的忠实性是具有挑战性的。为了帮助解决这个问题，我们开发了两种评估自然语言解释的模式，这些解释声称个别神经元代表文本输入中的一个概念。在观察模式中，我们评估了神经元$a$仅并且完全激活与所提出的解释$E$所指代的概念相关的所有输入字符串的说法。在干预模式中，我们将$E$解释为神经元$a$是由$E$所表示的概念的因果中介者的说法。我们将我们的框架应用于Bills等人（2023年）对GPT-2 XL神经元的GPT-4生成的解释，并显示即使是最自信的解释也有很高的错误率和几乎没有因果效应。最后，我们对自然语言是否是一个好的解释选择以及神经元是否是最好的分析层次进行了批判性评估。",
    "tldr": "该论文开发了两种模式来评估自然语言解释神经元的忠实度，并且应用于GPT-4生成的GPT-2 XL神经元解释的评估结果显示，即使是自信度最高的解释也存在较高的错误率和几乎没有因果效应。"
}