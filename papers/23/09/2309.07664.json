{
    "title": "Computer says 'no': Exploring systemic hiring bias in ChatGPT using an audit approach. (arXiv:2309.07664v1 [econ.GN])",
    "abstract": "Large language models offer significant potential for optimising professional activities, such as streamlining personnel selection procedures. However, concerns exist about these models perpetuating systemic biases embedded into their pre-training data. This study explores whether ChatGPT, a chatbot producing human-like responses to language tasks, displays ethnic or gender bias in job applicant screening. Using a correspondence audit approach, I simulated a CV screening task in which I instructed the chatbot to rate fictitious applicant profiles only differing in names, signalling ethnic and gender identity. Comparing ratings of Arab, Asian, Black American, Central African, Dutch, Eastern European, Hispanic, Turkish, and White American male and female applicants, I show that ethnic and gender identity influence ChatGPT's evaluations. The ethnic bias appears to arise partly from the prompts' language and partly from ethnic identity cues in applicants' names. Although ChatGPT produces n",
    "link": "http://arxiv.org/abs/2309.07664",
    "context": "Title: Computer says 'no': Exploring systemic hiring bias in ChatGPT using an audit approach. (arXiv:2309.07664v1 [econ.GN])\nAbstract: Large language models offer significant potential for optimising professional activities, such as streamlining personnel selection procedures. However, concerns exist about these models perpetuating systemic biases embedded into their pre-training data. This study explores whether ChatGPT, a chatbot producing human-like responses to language tasks, displays ethnic or gender bias in job applicant screening. Using a correspondence audit approach, I simulated a CV screening task in which I instructed the chatbot to rate fictitious applicant profiles only differing in names, signalling ethnic and gender identity. Comparing ratings of Arab, Asian, Black American, Central African, Dutch, Eastern European, Hispanic, Turkish, and White American male and female applicants, I show that ethnic and gender identity influence ChatGPT's evaluations. The ethnic bias appears to arise partly from the prompts' language and partly from ethnic identity cues in applicants' names. Although ChatGPT produces n",
    "path": "papers/23/09/2309.07664.json",
    "total_tokens": 907,
    "translated_title": "计算机说“不行”: 使用审计方法探索ChatGPT中的系统招聘偏见",
    "translated_abstract": "大型语言模型在优化职业活动方面具有重要潜力，如简化人员选拔程序。然而，人们担心这些模型会延续预训练数据中嵌入的系统偏见。这项研究探讨了ChatGPT在求职者筛选方面是否表现出种族或性别偏见，ChatGPT是一个能够生成类似人类回应的聊天机器人。通过使用一种通信审计方法，我模拟了一个简历筛选任务，指示聊天机器人评价虚构的申请者简历，这些简历只有名字不同，以暗示种族和性别身份。通过比较阿拉伯、亚洲、美国黑人、中非、荷兰、东欧、西班牙裔、土耳其和美国白人男性和女性申请者的评分，我发现种族和性别身份会影响ChatGPT的评估。种族偏见似乎部分源于提示语言，部分源于申请者姓名中的种族身份线索。",
    "tldr": "本研究使用审计方法探索了ChatGPT在求职者筛选中的系统偏见，研究发现语言提示和申请者姓名中的种族身份线索会影响ChatGPT的评估。",
    "en_tdlr": "This study explores systemic biases in job applicant screening of ChatGPT by using an audit approach. The research shows that ChatGPT's evaluations are influenced by ethnic and gender identity cues in the language prompts and applicants' names."
}