{
    "title": "Flow Factorized Representation Learning. (arXiv:2309.13167v1 [cs.LG])",
    "abstract": "A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential followi",
    "link": "http://arxiv.org/abs/2309.13167",
    "context": "Title: Flow Factorized Representation Learning. (arXiv:2309.13167v1 [cs.LG])\nAbstract: A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential followi",
    "path": "papers/23/09/2309.13167.json",
    "total_tokens": 774,
    "translated_title": "流动因式表示学习",
    "translated_abstract": "表示学习研究的一个重要目标是实现与真实因素变化有关的有用因式化表示。区分嵌入和等变表示学习领域从不同的角度接近了这个理想；然而，迄今为止，大多数方法都被证明要么规定不明确，要么灵活性不足，不能有效地将所有感兴趣的因素在学习的潜在空间中分离开来。在这项工作中，我们提出了一种称为流动因式表示学习的结构化表示学习的替代观点，并展示了该方法学习到的表示比现有框架更高效、更有用的结构。具体而言，我们引入了一个生成模型，该模型指定了一组不同输入变换的潜在概率路径。每个潜在流是由一个学习的势函数的梯度场生成的。",
    "tldr": "本文提出了一种称为流动因式表示学习的新观点，并展示了比现有框架更高效更有用的结构化表示的学习方法。",
    "en_tdlr": "This paper proposes a new viewpoint called Flow Factorized Representation Learning and demonstrates a learning method for more efficient and useful structured representations compared to existing frameworks."
}