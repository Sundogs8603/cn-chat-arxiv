{
    "title": "On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])",
    "abstract": "Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent \"selection bias\" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like \"Option C\"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves ",
    "link": "http://arxiv.org/abs/2309.03882",
    "context": "Title: On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])\nAbstract: Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent \"selection bias\" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like \"Option C\"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves ",
    "path": "papers/23/09/2309.03882.json",
    "total_tokens": 957,
    "translated_title": "关于大型语言模型在多项选择题中的选择偏差问题",
    "translated_abstract": "多项选择题（MCQs）是大型语言模型（LLMs）研究中常见且重要的任务格式。我们的工作表明，LLMs在MCQs中存在固有的“选择偏差”，即LLMs倾向于选择特定位置上的选项（如“选项C”）。这种偏差在各种LLMs中普遍存在，使得它们在MCQs中对选项位置变化的性能变得脆弱。我们发现导致选择偏差的一个主要原因是选项编号，即与选项相关的ID符号A/B/C/D。为了减轻选择偏差，我们提出了一种新方法称为PriDe。PriDe首先将观察到的模型预测分布分解为对选项内容的内在预测和对选项ID的先验分布。然后，它通过在少量测试样本上对选项内容进行排列组合来估计先验，从而用于消除后续测试样本的偏差。我们证明了作为一种无标签、推断时间方法，PriDe可以实现高精度且稳定的解决方案。",
    "tldr": "本研究发现大型语言模型在多项选择题中存在选择偏差，即倾向于选择特定位置上的选项。研究指出这一偏差的主要原因是选项编号，提出了一种名为PriDe的方法来减轻偏差，并展示了其高精度和稳定性。"
}