{
    "title": "Learning Shared Safety Constraints from Multi-task Demonstrations. (arXiv:2309.00711v1 [cs.LG])",
    "abstract": "Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simula",
    "link": "http://arxiv.org/abs/2309.00711",
    "context": "Title: Learning Shared Safety Constraints from Multi-task Demonstrations. (arXiv:2309.00711v1 [cs.LG])\nAbstract: Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simula",
    "path": "papers/23/09/2309.00711.json",
    "total_tokens": 891,
    "translated_title": "从多任务示范中学习共享的安全约束",
    "translated_abstract": "无论我们希望智能体在环境中执行哪项任务，我们通常希望它们遵守共享的安全约束。例如，无论是做三明治还是清理桌子，厨房机器人都不应该打破盘子。手动指定这样的约束可能耗时且容易出错。我们展示了如何通过将逆强化学习(IRL)技术扩展到约束空间来从安全任务完成的专家示范中学习约束。直观地说，我们学习约束以禁止专家可能采取但选择不采取的高收益行为。不幸的是，约束学习问题通常不明确，并且通常导致过于保守的约束，禁止所有专家没有采取的行为。我们通过利用多任务设置中自然发生的多样化示范来学习一组更紧密的约束来解决这个问题。我们用仿真实验证实了我们的方法。",
    "tldr": "本论文介绍了如何从安全任务完成的专家示范中学习共享的安全约束，通过将逆强化学习技术扩展到约束空间，学习约束以禁止专家可能采取但选择不采取的高收益行为，并利用多任务设置中多样化示范学习一组更紧密的约束。",
    "en_tdlr": "This paper presents how to learn shared safety constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning techniques to the space of constraints and leveraging diverse demonstrations that naturally occur in multi-task settings."
}