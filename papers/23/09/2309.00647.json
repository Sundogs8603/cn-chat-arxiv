{
    "title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data. (arXiv:2309.00647v1 [eess.AS])",
    "abstract": "Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark.",
    "link": "http://arxiv.org/abs/2309.00647",
    "context": "Title: Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data. (arXiv:2309.00647v1 [eess.AS])\nAbstract: Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark.",
    "path": "papers/23/09/2309.00647.json",
    "total_tokens": 960,
    "translated_title": "通过对辅助数据进行监督，改进小型关键词检测模型的效果",
    "translated_abstract": "少样本关键词检测模型通常需要大规模的标注数据集才能适应未见过的目标关键词。然而，现有的关键词检测数据集在规模上存在限制，并且收集类似关键词的标注数据是一项昂贵的任务。为了解决这个问题，我们提出了一个框架，利用容易收集的无标注阅读语音数据作为辅助来源。自监督学习广泛应用于从无标注数据中学习表征，但是它适用于具有足够容量的大模型，并不适合训练小型的关键词检测模型。相反，我们通过自动标注和筛选数据来构建一个类似关键词的数据集LibriWord，实现对辅助数据的监督。然后我们采用多任务学习来提升模型从域外辅助数据中的表示能力。我们的方法显著改善了在少样本关键词检测基准测试中的性能，超过了竞争方法。",
    "tldr": "本研究提出了一种改进小型关键词检测模型的方法，通过对辅助数据进行监督，利用自动标注和筛选数据构建了一个类似关键词的数据集，并采用多任务学习提升模型的表示能力，取得了在少样本关键词检测基准测试中优于竞争方法的性能表现。"
}