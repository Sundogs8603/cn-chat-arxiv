{
    "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])",
    "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human perform",
    "link": "http://arxiv.org/abs/2309.16575",
    "context": "Title: A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])\nAbstract: Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human perform",
    "path": "papers/23/09/2309.16575.json",
    "total_tokens": 999,
    "translated_title": "从一本语法书学习翻译新语言的基准测试",
    "translated_abstract": "大型语言模型(LLMs)可以通过上下文学习或轻量级微调来完成令人印象深刻的任务。人们自然而然地想知道这些模型在适应全新任务时的表现如何，但如何找到在互联网规模的训练数据集中未见过的任务呢？我们转向一个明确受到网络数据稀缺性的驱动和限制的领域：低资源语言。在本文中，我们引入了一种名为MTOB（从一本书进行机器翻译）的基准测试，用于学习在英语和Kalamang之间翻译--Kalamang是一种只有不到200名使用者的语言，因此在网络上几乎没有存在感--我们使用了几百页的田野语言学参考资料。这种任务框架的新颖之处在于，它要求模型从一本人类可读的语法解释书中学习一种语言，而不是从大规模挖掘的领域内数据语料库中学习，更类似于L2学习而不是L1习得。我们证明，使用当前LLMs的基准测试具有很大的潜力，但仍然不及人类表现。",
    "tldr": "本研究提出了一种从一本语法书中学习翻译新语言的基准测试MTOB，用于翻译英语和Kalamang之间的文本，探索了低资源语言情况下的翻译问题。结果显示，现有的大型语言模型在这个任务上表现有限。",
    "en_tdlr": "This study introduces a benchmark called MTOB for learning to translate a new language from one grammar book, specifically exploring the translation between English and Kalamang in low-resource language scenarios. The results demonstrate that current large language models have limited performance on this task."
}