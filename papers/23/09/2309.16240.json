{
    "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])",
    "abstract": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat",
    "link": "http://arxiv.org/abs/2309.16240",
    "context": "Title: Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])\nAbstract: The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat",
    "path": "papers/23/09/2309.16240.json",
    "total_tokens": 878,
    "translated_title": "超越逆KL：通过多样的差异约束推广直接偏好优化",
    "translated_abstract": "大型语言模型（LLM）的不断增强能力为人工智能提供了机会，但同时也放大了安全问题，如AI系统的潜在滥用，这需要有效的AI对齐。基于人类反馈的强化学习（RLHF）已经成为AI对齐的一条有希望的路径，但由于其复杂性和对独立奖励模型的依赖性而带来了挑战。直接偏好优化（DPO）被提出作为一种替代方法，在逆KL正则化约束下等同于RLHF。本文提出了f-DPO，一种通过整合多样的差异约束来推广DPO的方法。我们证明，在某些f-散度下，包括Jensen-Shannon散度、正向KL散度和α-散度，奖励和最优策略之间的复杂关系也可以通过解决Karush-Kuhn-Tucker条件来简化。这消除了对估计方法的需要。",
    "tldr": "本论文提出了一种通过引入多样差异约束推广直接偏好优化（DPO）的方法，该方法消除了对估计方法的需要并简化了奖励和最优策略之间的复杂关系。"
}