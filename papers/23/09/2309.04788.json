{
    "title": "Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])",
    "abstract": "Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.",
    "link": "http://arxiv.org/abs/2309.04788",
    "context": "Title: Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])\nAbstract: Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.",
    "path": "papers/23/09/2309.04788.json",
    "total_tokens": 906,
    "translated_title": "随机梯度下降在高维信号恢复的玻璃能量景观中表现优于梯度下降",
    "translated_abstract": "随机梯度下降（SGD）是一种非平衡算法，广泛用于训练人工神经网络。然而，我们对SGD在这项技术的成功中至关重要的程度以及相对于其他优化算法（如梯度下降）在优化高维非凸成本函数方面的效果知之甚少。在这项工作中，我们利用动力学均场理论在高维极限中准确分析了其性能。我们考虑恢复隐藏的高维非线性加密信号问题，即一个典型的高维非凸的难优化问题。我们比较了SGD和GD的性能，并表明SGD大大优于GD。特别地，对这些算法的弛豫时间进行幂律拟合表明，SGD在小批量大小的情况下的恢复阈值小于GD的对应阈值。",
    "tldr": "本研究利用动力学均场理论研究了随机梯度下降（SGD）在高维非凸成本函数优化中的表现。实验结果表明，SGD在恢复高维非线性加密信号问题上明显优于梯度下降（GD）。"
}