{
    "title": "Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])",
    "abstract": "The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\\'a\\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun",
    "link": "http://arxiv.org/abs/2309.00367",
    "context": "Title: Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])\nAbstract: The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\\'a\\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun",
    "path": "papers/23/09/2309.00367.json",
    "total_tokens": 956,
    "translated_title": "长程图表基准的重新评估：差距去哪儿了？",
    "translated_abstract": "最近的长程图表基准(LRGB，Dwivedi等，2022)引入了一组与顶点之间的长程相互作用密切相关的图表学习任务。经验证据表明，在这些任务中，图形变换器明显优于消息传递GNN（MPGNN）。在本文中，我们对LRGB上的多个MPGNN基线以及图形变换器GPS（Ramp\\'a\\v{s}ek等，2022）进行了仔细重新评估。通过严格的实证分析，我们证明了由于子优超参数选择不当而高估了报告的性能差距。值得注意的是，在基本超参数优化后，跨多个数据集，性能差距完全消失。此外，我们还讨论了LRGB的视觉数据集缺乏特征归一化的影响，并突出了LRGB的链接预测度量的虚假实现。我们的论文的主要目标是在图机器学习社区建立更高的实证严谨性标准。",
    "tldr": "本文对长程图表基准（LRGB）进行了重新评估，通过严格的实证分析发现，先前的报告性能差距被高估了，而经过基本超参数优化后，差距完全消失。此外，我们还讨论了特征归一化的缺失和链接预测度量的虚假实现对LRGB的影响。"
}