{
    "title": "Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language. (arXiv:2309.11753v1 [cs.AI])",
    "abstract": "Reinforcement learning is a powerful technique for learning from trial and error, but it often requires a large number of interactions to achieve good performance. In some domains, such as sparse-reward tasks, an oracle that can provide useful feedback or guidance to the agent during the learning process is really of great importance. However, querying the oracle too frequently may be costly or impractical, and the oracle may not always have a clear answer for every situation. Therefore, we propose a novel method for interacting with the oracle in a selective and efficient way, using a retrieval-based approach. We assume that the interaction can be modeled as a sequence of templated questions and answers, and that there is a large corpus of previous interactions available. We use a neural network to encode the current state of the agent and the oracle, and retrieve the most relevant question from the corpus to ask the oracle. We then use the oracle's answer to update the agent's policy",
    "link": "http://arxiv.org/abs/2309.11753",
    "context": "Title: Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language. (arXiv:2309.11753v1 [cs.AI])\nAbstract: Reinforcement learning is a powerful technique for learning from trial and error, but it often requires a large number of interactions to achieve good performance. In some domains, such as sparse-reward tasks, an oracle that can provide useful feedback or guidance to the agent during the learning process is really of great importance. However, querying the oracle too frequently may be costly or impractical, and the oracle may not always have a clear answer for every situation. Therefore, we propose a novel method for interacting with the oracle in a selective and efficient way, using a retrieval-based approach. We assume that the interaction can be modeled as a sequence of templated questions and answers, and that there is a large corpus of previous interactions available. We use a neural network to encode the current state of the agent and the oracle, and retrieve the most relevant question from the corpus to ask the oracle. We then use the oracle's answer to update the agent's policy",
    "path": "papers/23/09/2309.11753.json",
    "total_tokens": 891,
    "translated_title": "通过自然语言引导的语义探索，提高深度强化学习的效率",
    "translated_abstract": "强化学习是一种从试错中学习的强大技术，但通常需要大量的交互才能达到良好的性能。在某些领域中，如稀疏奖励任务，一个能在学习过程中为代理提供有用反馈或指导的\"神\"级存在是非常重要的。然而，过于频繁地查询\"神\"级存在可能是昂贵或不切实际的，而且\"神\"级存在可能并不总是对每个情况都有明确的答案。因此，我们提出了一种基于检索的方法，以选择性和高效的方式与\"神\"级存在进行交互。我们假设交互可以被建模为一系列模板化的问题和答案，并且存在大量以前的交互的语料库。我们使用神经网络对代理和\"神\"级存在的当前状态进行编码，并从语料库中检索出最相关的问题来问\"神\"级存在。然后，我们使用\"神\"级存在的答案来更新代理的策略。",
    "tldr": "通过自然语言引导的语义探索，提高深度强化学习的效率。通过检索语料库中相关问题来与\"神\"级存在交互，更新代理的策略。",
    "en_tdlr": "Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language. Use a retrieval-based approach to interact with an oracle and update the agent's policy by querying relevant questions from a corpus."
}