{
    "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.",
    "link": "http://arxiv.org/abs/2309.10202",
    "context": "Title: Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.",
    "path": "papers/23/09/2309.10202.json",
    "total_tokens": 897,
    "translated_title": "通过优势模型和选择性回放稳定RLHF",
    "translated_abstract": "大型语言模型（LLM）已经在自然语言处理领域产生了革命性的影响，然而，通过RLHF将这些模型与人类价值观和偏好对齐仍然是一个重大挑战。这个挑战表现为各种不稳定性，比如奖励欺骗和灾难性遗忘。在这个技术报告中，我们提出了两项创新来稳定RLHF训练: 1) 优势模型，直接建模优势得分，即相对于期望奖励的额外奖励，并通过调节得分分布来防止奖励欺骗。2) 选择性回放，通过有策略地选择数据进行PPO训练和知识回放，从而减轻灾难性遗忘。我们在公开和专有数据集上进行的实验分析表明，所提出的方法不仅在RLHF训练中增加了稳定性，而且实现了更高的奖励得分和胜率。",
    "tldr": "本论文提出了通过优势模型和选择性回放来稳定RLHF训练的两种创新方法，成功地解决了奖励欺骗和灾难性遗忘等不稳定性问题，并在实验中取得了更高的奖励得分和胜率。",
    "en_tdlr": "This paper proposes two innovative methods, an Advantage Model and Selective Rehearsal, to stabilize RLHF training and address issues such as reward hacking and catastrophic forgetting. Experimental results show that these methods not only increase stability in RLHF training but also achieve higher reward scores and win rates."
}