{
    "title": "Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])",
    "abstract": "Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and ",
    "link": "http://arxiv.org/abs/2309.16347",
    "context": "Title: Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])\nAbstract: Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and ",
    "path": "papers/23/09/2309.16347.json",
    "total_tokens": 957,
    "translated_title": "复杂长视程机器人操作任务的内在语言引导探索",
    "translated_abstract": "当前的强化学习算法在稀疏和复杂的环境中面临困境，尤其是在涉及众多不同序列的长视程操作任务中。在本研究中，我们提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架。通过利用LLMs作为辅助内在奖励，IGE-LLMs引导强化学习中的探索过程，以解决复杂的长视程操作任务中奖励稀疏问题。我们在一个具有探索挑战的环境和一个同时面临探索和长视程挑战的复杂机器人操作任务中评估了我们的框架和相关的内在学习方法。结果显示，IGE-LLMs(i)在相关的内在方法和直接使用LLMs进行决策的性能上表现出明显的较高水平，(ii)可以与现有的学习方法相结合和互补，突出其模块化性能，(iii)对于不同的内在缩放参数比较不敏感。",
    "tldr": "本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。"
}