{
    "title": "A State Representation for Diminishing Rewards. (arXiv:2309.03710v1 [cs.LG])",
    "abstract": "A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\\lambda$ representation ($\\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\\lambda$R's formal properties and examine ",
    "link": "http://arxiv.org/abs/2309.03710",
    "context": "Title: A State Representation for Diminishing Rewards. (arXiv:2309.03710v1 [cs.LG])\nAbstract: A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\\lambda$ representation ($\\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\\lambda$R's formal properties and examine ",
    "path": "papers/23/09/2309.03710.json",
    "total_tokens": 894,
    "translated_title": "一种递减奖励的状态表示",
    "translated_abstract": "多任务强化学习中常见的情景要求代理快速适应从固定分布中随机采样的各种静态奖励函数。在这种情况下，后继状态表示（SR）是一个流行的框架，通过将策略的预期折扣累积状态分布与特定奖励函数解耦，支持快速策略评估。然而，在自然界中，顺序任务很少是独立的，而是基于奖励刺激的可用性和主观感知反映出不断变化的优先级。为了反映这种不协调，本文研究了递减边际效用的现象，并引入了一种新颖的状态表示，称为$\\lambda$表示（$\\lambda$R），令人惊讶的是，在该设置中需要用于策略评估，并且可以推广SR以及文献中的其他几种状态表示。我们建立了$\\lambda$R的正式属性并研究了它的性能。",
    "tldr": "该论文研究了多任务强化学习中存在的递减边际效用现象，并引入了一种名为$\\lambda$表示（$\\lambda$R）的新型状态表示，用于快速策略评估，该表示能够推广已有的状态表示并具备一些正式属性。",
    "en_tdlr": "This paper investigates the phenomenon of diminishing marginal utility in multitask reinforcement learning and introduces a novel state representation called the $\\lambda$ representation ($\\lambda$R), which is required for rapid policy evaluation and generalizes existing state representations with formal properties."
}