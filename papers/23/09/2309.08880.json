{
    "title": "Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])",
    "abstract": "Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\\mathcal{O}(\\underline{q}^3)$ in the literature to $\\mathcal{O}(\\underline{q}^2)$ in the proposed algorithm, where $\\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form ",
    "link": "http://arxiv.org/abs/2309.08880",
    "context": "Title: Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])\nAbstract: Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\\mathcal{O}(\\underline{q}^3)$ in the literature to $\\mathcal{O}(\\underline{q}^2)$ in the proposed algorithm, where $\\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form ",
    "path": "papers/23/09/2309.08880.json",
    "total_tokens": 909,
    "translated_title": "数据驱动的实时高效强化学习算法在H-infinity控制中的应用：自主移动需求系统",
    "translated_abstract": "强化学习是一类人工智能算法，通过在线学习设计适应性最优控制器。本文提出了一种基于模型无关、实时、数据高效的Q-learning算法来解决线性离散时间系统的H-infinity控制问题。计算复杂度从文献中的O(q^3)降低到了提出的算法的O(q^2)，其中q是状态变量、控制输入和干扰的大小之和的二次项。通过在线学习，设计了自适应最优控制器，并学习了动作和评论网络的参数，不需要对系统动力学的了解，使得该算法完全无模型。此外，仅在第一次迭代中需要足够的扰动噪声，而不影响提出的算法。无需初始稳定策略，算法收敛到闭式解。",
    "tldr": "本文提出了一种实时高效的强化学习算法，可以解决线性离散时间系统的H-infinity控制问题。该算法降低了计算复杂度，完全无模型，且不需要初始稳定策略，能够收敛到闭式解。",
    "en_tdlr": "This paper presents a real-time and efficient reinforcement learning algorithm that solves the H-infinity control problem for linear discrete-time systems. The algorithm reduces computational complexity, is completely model-free, and converges to a closed-form solution without the need for an initial stabilizing policy."
}