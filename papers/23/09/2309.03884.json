{
    "title": "Zero-Shot Audio Captioning via Audibility Guidance. (arXiv:2309.03884v1 [cs.SD])",
    "abstract": "The task of audio captioning is similar in essence to tasks such as image and video captioning. However, it has received much less attention. We propose three desiderata for captioning audio -- (i) fluency of the generated text, (ii) faithfulness of the generated text to the input audio, and the somewhat related (iii) audibility, which is the quality of being able to be perceived based only on audio. Our method is a zero-shot method, i.e., we do not learn to perform captioning. Instead, captioning occurs as an inference process that involves three networks that correspond to the three desired qualities: (i) A Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A model that provides a matching score between an audio file and a text, for which we use a multimodal matching network called ImageBind, and (iii) A text classifier, trained using a dataset we collected automatically by instructing GPT-4 with prompts designed to direct the generation of both audible and in",
    "link": "http://arxiv.org/abs/2309.03884",
    "context": "Title: Zero-Shot Audio Captioning via Audibility Guidance. (arXiv:2309.03884v1 [cs.SD])\nAbstract: The task of audio captioning is similar in essence to tasks such as image and video captioning. However, it has received much less attention. We propose three desiderata for captioning audio -- (i) fluency of the generated text, (ii) faithfulness of the generated text to the input audio, and the somewhat related (iii) audibility, which is the quality of being able to be perceived based only on audio. Our method is a zero-shot method, i.e., we do not learn to perform captioning. Instead, captioning occurs as an inference process that involves three networks that correspond to the three desired qualities: (i) A Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A model that provides a matching score between an audio file and a text, for which we use a multimodal matching network called ImageBind, and (iii) A text classifier, trained using a dataset we collected automatically by instructing GPT-4 with prompts designed to direct the generation of both audible and in",
    "path": "papers/23/09/2309.03884.json",
    "total_tokens": 846,
    "translated_title": "通过听觉引导的零样本音频字幕生成",
    "translated_abstract": "音频字幕生成的任务与图像和视频字幕生成类似，但却得到了较少的关注。我们提出了三个音频字幕生成的要求：（i）生成文本的流畅性，（ii）生成文本与输入音频的忠实性，以及（iii）可听性，即仅基于音频能够被感知的质量。我们的方法是一种零样本方法，即我们不会学习执行字幕生成。相反，字幕生成作为一个推理过程发生，涉及到三个对应于三个期望质量的网络：（i）一个大型语言模型，我们在这里选择了 GPT-2，（ii）一个在音频文件和文本之间提供匹配分数的模型，我们使用了一个名为 ImageBind 的多模态匹配网络，以及（iii）一种文本分类器，使用我们自动收集的数据集进行训练，这些数据集是通过指导 GPT-4 提示设计来指导可听和不可听文本的生成。",
    "tldr": "本论文提出了一种通过听觉引导的零样本音频字幕生成方法，使用大型语言模型、多模态匹配网络和文本分类器来实现生成文本流畅性、忠实性和可听性。"
}