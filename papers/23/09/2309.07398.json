{
    "title": "Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])",
    "abstract": "Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas",
    "link": "http://arxiv.org/abs/2309.07398",
    "context": "Title: Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])\nAbstract: Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas",
    "path": "papers/23/09/2309.07398.json",
    "total_tokens": 920,
    "translated_title": "通过扩散模型实现语义对抗攻击",
    "translated_abstract": "传统的对抗攻击主要通过在像素空间中添加对抗性扰动来操纵干净的样本。相比之下，语义对抗攻击更加关注改变干净样本的语义属性，例如颜色、上下文和特征，在实际世界中更具可行性。在本文中，我们提出了一个框架，通过利用最近的扩散模型，能够快速生成语义对抗攻击，因为语义信息包含在训练有素的扩散模型的潜在空间中。然后，该框架有两个变种：1) 语义转换方法(ST)通过微调生成图像的潜在空间和/或扩散模型本身；2) 潜在屏蔽方法(LM)使用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。此外，ST方法可在白盒或黑盒设置中应用。在CelebA-HQ和AFHQ数据集上进行了大量实验。",
    "tldr": "本文提出了一个用于生成语义对抗攻击的框架，并使用扩散模型中的潜在空间中的语义信息。在该框架中，有两个变种方法：语义转换方法(ST)，通过微调潜在空间和/或扩散模型本身来生成图像；潜在屏蔽方法(LM)，利用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。实验结果验证了该框架的有效性。"
}