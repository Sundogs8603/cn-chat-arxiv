{
    "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)",
    "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2",
    "link": "http://arxiv.org/abs/2309.05653",
    "context": "Title: MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)\nAbstract: We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2",
    "path": "papers/23/09/2309.05653.json",
    "total_tokens": 1113,
    "translated_title": "MAmmoTH: 通过混合指令调整构建数学通用模型",
    "translated_abstract": "我们介绍了MAmmoTH，一系列针对通用数学问题求解的开源大型语言模型（LLM）。MAmmoTH模型是在我们精心策划的指令调整数据集MathInstruct上训练的。MathInstruct从13个数学数据集中编译而成，包含中间的推理过程，其中有六个数据集是由我们新鲜策划的推理过程。它提供了一种独特的链状思维（CoT）和程序维思维（PoT）推理的混合，同时确保了对数学领域各个方面的广泛覆盖。CoT和PoT的混合不仅释放了工具使用的潜力，而且允许在不同数学问题上使用不同的思考过程。因此，MAmmoTH系列在九个数学推理数据集上显著优于现有的开源模型，在所有规模上平均准确率提高了16%到32%不等。值得注意的是，我们的MAmmoTH-7B模型在MATH（一个竞赛级数据集）上达到了33%，超过了最好的开源7B模型（WizardMath）2个百分点。",
    "tldr": "MAmmoTH是一系列用于解决通用数学问题的开源大型语言模型，通过混合指令调整网络架构，成功地融合了链状思维和程序维思维，从而在多个数学推理数据集上实现了显著提升的准确率。其中，MAmmoTH-7B模型在MATH数据集上的表现超过了目前最好的开源7B模型WizardMath，并且整个系列模型在各个规模上平均准确率提高了16%到32%之间。",
    "en_tdlr": "MAmmoTH is a series of open-source large language models designed for general math problem-solving. By combining chain-of-thought and program-of-thought rationales, the MAmmoTH models achieve significant improvements in accuracy on various mathematical reasoning datasets. Notably, the MAmmoTH-7B model outperforms the best open-source 7B model, WizardMath, on the MATH dataset, and the entire series of models demonstrate an average accuracy gain of 16% to 32% across different scales."
}