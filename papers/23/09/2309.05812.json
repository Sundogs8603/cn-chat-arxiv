{
    "title": "Interpretable learning of effective dynamics for multiscale systems. (arXiv:2309.05812v1 [stat.ML])",
    "abstract": "The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit o",
    "link": "http://arxiv.org/abs/2309.05812",
    "context": "Title: Interpretable learning of effective dynamics for multiscale systems. (arXiv:2309.05812v1 [stat.ML])\nAbstract: The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit o",
    "path": "papers/23/09/2309.05812.json",
    "total_tokens": 863,
    "translated_title": "可解释多尺度系统有效动力学学习",
    "translated_abstract": "高维多尺度系统的建模和仿真是科学和工程领域面临的重要挑战。尽管现今的计算机技术不断进步，解决由控制方程描述的所有时空尺度仍然是一个遥不可及的目标。这种认识促使人们大力发展模型降阶技术。近年来，基于深度循环神经网络的技术在复杂时空系统的建模和仿真方面取得了令人鼓舞的成果，并且具有模型开发的灵活性，因为它们可以结合实验和计算数据。然而，神经网络缺乏可解释性，限制了它们在复杂系统中的实用性和普适性。在这里，我们提出了一种新的可解释学习有效动力学（iLED）框架，它具有与基于循环神经网络的最新方法相当的准确性，并提供了额外的好处。",
    "tldr": "该论文提出了一种新的可解释学习有效动力学（iLED）框架，它通过引入深度循环神经网络技术，在保持准确性的同时提供了可解释性，解决了现有神经网络在复杂系统中应用受限的问题。",
    "en_tdlr": "This paper proposes a novel framework called Interpretable Learning Effective Dynamics (iLED) that combines deep recurrent neural networks with interpretability, addressing the limitation of traditional neural networks in complex systems."
}