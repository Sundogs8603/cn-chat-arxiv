{
    "title": "Bridging the Projection Gap: Overcoming Projection Bias Through Parameterized Distance Learning",
    "abstract": "arXiv:2309.01390v2 Announce Type: replace-cross  Abstract: Generalized zero-shot learning (GZSL) aims to recognize samples from both seen and unseen classes using only seen class samples for training. However, GZSL methods are prone to bias towards seen classes during inference due to the projection function being learned from seen classes. Most methods focus on learning an accurate projection, but bias in the projection is inevitable. We address this projection bias by proposing to learn a parameterized Mahalanobis distance metric for robust inference. Our key insight is that the distance computation during inference is critical, even with a biased projection. We make two main contributions - (1) We extend the VAEGAN (Variational Autoencoder \\& Generative Adversarial Networks) architecture with two branches to separately output the projection of samples from seen and unseen classes, enabling more robust distance learning. (2) We introduce a novel loss function to optimize the Mahalano",
    "link": "https://arxiv.org/abs/2309.01390",
    "context": "Title: Bridging the Projection Gap: Overcoming Projection Bias Through Parameterized Distance Learning\nAbstract: arXiv:2309.01390v2 Announce Type: replace-cross  Abstract: Generalized zero-shot learning (GZSL) aims to recognize samples from both seen and unseen classes using only seen class samples for training. However, GZSL methods are prone to bias towards seen classes during inference due to the projection function being learned from seen classes. Most methods focus on learning an accurate projection, but bias in the projection is inevitable. We address this projection bias by proposing to learn a parameterized Mahalanobis distance metric for robust inference. Our key insight is that the distance computation during inference is critical, even with a biased projection. We make two main contributions - (1) We extend the VAEGAN (Variational Autoencoder \\& Generative Adversarial Networks) architecture with two branches to separately output the projection of samples from seen and unseen classes, enabling more robust distance learning. (2) We introduce a novel loss function to optimize the Mahalano",
    "path": "papers/23/09/2309.01390.json",
    "total_tokens": 906,
    "translated_title": "弥合投影差距：通过参数化距离学习克服投影偏差",
    "translated_abstract": "广义零样本学习（GZSL）旨在仅利用已知类别样本训练来识别来自已知和未知类别的样本。然而，在推断过程中，由于投影函数是从已知类别中学习的，GZSL方法很容易偏向已知类别。大多数方法致力于学习准确的投影，但投影中的偏差是不可避免的。我们通过提出学习参数化的马氏距离度量来解决该投影偏差，关键洞察是尽管投影存在偏差，但在推断过程中距离计算至关重要。我们作出两个主要贡献 - (1)我们通过增加两个分支扩展了VAEGAN（变分自动编码器和生成对抗网络）架构，分别输出来自已知和未知类别的样本的投影，从而实现更稳健的距离学习。 (2)我们引入了一种新颖的损失函数来优化马氏距离",
    "tldr": "通过学习参数化的马氏距离度量，解决广义零样本学习中的投影偏差问题，提出了扩展VAEGAN架构和引入新损失函数以实现更稳健的距离学习",
    "en_tdlr": "Addressing projection bias in generalized zero-shot learning by learning a parameterized Mahalanobis distance metric, the paper extends the VAEGAN architecture and introduces a novel loss function for more robust distance learning."
}