{
    "title": "Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])",
    "abstract": "The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.",
    "link": "http://arxiv.org/abs/2309.03619",
    "context": "Title: Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])\nAbstract: The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.",
    "path": "papers/23/09/2309.03619.json",
    "total_tokens": 873,
    "translated_title": "通过不变性和冗余减少理解自监督学习的语音表示",
    "translated_abstract": "目标函数的选择在自监督学习中生成高质量表示的过程中至关重要。本文研究了Barlow Twins (BT) 目标的不同表达形式如何影响语音数据下游任务的性能。我们提出了带有标准化潜变量的Modified Barlow Twins (MBT) 来强制尺度不变，并在说话人识别、性别识别和关键词检测任务上进行了评估。我们的结果表明，MBT在有限的目标数据上微调时，改善了表示泛化能力，尤其是相对于原始BT。这凸显了设计鼓励不变性和可传输表示的目标的重要性。我们的分析提供了关于如何调整BT学习目标以生成在适用于新的下游任务中表现出色的语音表示的见解。这项研究是发展可重用的自监督语音表示的重要一步。",
    "tldr": "通过标准化潜变量和调整目标函数，我们提出了Modified Barlow Twins (MBT) 方法来改善自监督学习中的语音表示泛化能力，尤其是在有限的目标数据上微调时。这项研究为发展可重用的自监督语音表示迈出了重要的一步。",
    "en_tdlr": "We proposed Modified Barlow Twins (MBT) with normalized latents to improve the generalization of self-supervised speech representation, especially when fine-tuning with limited target data. This study is an important step towards developing reusable self-supervised speech representations."
}