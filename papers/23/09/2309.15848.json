{
    "title": "SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations. (arXiv:2309.15848v1 [cs.CV])",
    "abstract": "Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datase",
    "link": "http://arxiv.org/abs/2309.15848",
    "context": "Title: SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations. (arXiv:2309.15848v1 [cs.CV])\nAbstract: Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datase",
    "path": "papers/23/09/2309.15848.json",
    "total_tokens": 932,
    "translated_title": "SHACIRA: 可扩展的哈希网格压缩技术用于隐式神经表示",
    "translated_abstract": "隐式神经表示（INR）或神经场已成为编码多媒体信号（如图像和辐射场）并保持高质量的流行框架。最近，由Instant-NGP提出的可学习特征网格在训练和采样INR方面提供了显著的加速，它通过用一个多分辨率查找表的特征向量和一个更小的神经网络来替代一个大型神经网络。然而，这些特征网格的内存消耗很大，这可能成为存储和流媒体应用的瓶颈。本研究提出了SHACIRA，一种简单但有效的通用框架，用于对这些特征网格进行压缩，而无需额外的后处理修剪/量化阶段。我们使用量化的潜在权重对特征网格重新参数化，并在潜在空间应用熵正则化，以实现在不同领域间的高水平压缩。对多样化数据集的定量和定性结果验证了我们的方法。",
    "tldr": "SHACIRA提出了一种简单但有效的通用框架，用于对神经表示中的特征网格进行高水平压缩，通过量化潜在权重和应用熵正则化来实现压缩。这种方法在多样化数据集上取得了定量和定性上的好结果。"
}