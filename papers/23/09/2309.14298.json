{
    "title": "Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. (arXiv:2309.14298v1 [stat.ML])",
    "abstract": "We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used \"optimism in the face of uncertainty\" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confi",
    "link": "http://arxiv.org/abs/2309.14298",
    "context": "Title: Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. (arXiv:2309.14298v1 [stat.ML])\nAbstract: We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used \"optimism in the face of uncertainty\" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confi",
    "path": "papers/23/09/2309.14298.json",
    "total_tokens": 930,
    "translated_title": "使用鞍点边界的马丁格尔混合改进随机线性Bandit算法",
    "translated_abstract": "我们提出了一种对随机线性Bandit问题具有最坏情况下遗憾保证的改进算法。广泛使用的\"面对不确定性时的乐观原则\"可以将随机Bandit问题转化为对未知奖励函数构建置信序列的问题。结果算法的性能取决于置信序列的大小，置信集较小可提供更好的经验性能和更强的遗憾保证。本研究中，我们使用了一种对自适应马丁格尔混合的尾部边界来构建适用于随机Bandit的置信序列。这些置信序列允许通过凸规划进行高效的动作选择。我们证明了基于我们的置信序列的线性Bandit算法能够保证达到具有竞争力的最坏情况下遗憾。我们实证和理论上证明了我们的置信序列比竞争对手更紧致。最后，我们证明了我们的紧致置信序列可以提供和置信集比较容易配置的更好的性能。",
    "tldr": "本研究提出了一种改进的随机线性Bandit算法，利用鞍点边界的马丁格尔混合构建了适用于随机Bandit的置信序列，并证明该算法能够以竞争性的最坏情况下遗憾保证实现更好的性能。",
    "en_tdlr": "This study presents an improved algorithm for the stochastic linear bandit problem by using tail bounds for martingale mixtures to construct confidence sequences. The algorithm achieves competitive worst-case regret guarantees by providing tighter confidence sequences compared to competitors."
}