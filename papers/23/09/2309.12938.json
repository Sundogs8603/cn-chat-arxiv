{
    "title": "Frustrated with Code Quality Issues? LLMs can Help!. (arXiv:2309.12938v1 [cs.AI])",
    "abstract": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The \\emph{proposer LLM} of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, ",
    "link": "http://arxiv.org/abs/2309.12938",
    "context": "Title: Frustrated with Code Quality Issues? LLMs can Help!. (arXiv:2309.12938v1 [cs.AI])\nAbstract: As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The \\emph{proposer LLM} of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, ",
    "path": "papers/23/09/2309.12938.json",
    "total_tokens": 933,
    "translated_title": "受代码质量问题困扰？LLM可以帮助！",
    "translated_abstract": "随着软件项目的进行，代码质量对软件的可靠性、可维护性和安全性具有至关重要的影响。因此，在开发者的工作流程中使用静态分析工具来标记代码质量问题。然而，开发者需要额外努力来修改他们的代码以改善代码质量。在这项工作中，我们研究了使用（指令跟随）大型语言模型（LLMs）来帮助开发者修正代码以解决代码质量问题的方式。我们提出了一个工具，名为CORE（COde REvisions），该工具使用一对组织为提供者和评估者的LLMs。静态分析工具的提供者推荐解决工具警告的方法，开发者遵循这些方法来修改他们的代码。CORE的\\emph{提供者LLM}接受相同的推荐并将其应用于生成候选的代码修订。通过静态质量检查的候选代码被保留。",
    "tldr": "使用大型语言模型（LLMs）协助开发者修正代码从而解决代码质量问题。提出了一个名为CORE的工具，使用一对组织为提供者和评估者的LLMs。该工具通过提供静态分析工具的推荐和生成候选的代码修订来改善代码质量。",
    "en_tdlr": "Using large language models (LLMs) to assist developers in revising code and resolving code quality issues. Introducing CORE, a tool that utilizes a pair of LLMs as a provider and a ranker to improve code quality by providing recommendations from static analysis tools and generating candidate code revisions."
}