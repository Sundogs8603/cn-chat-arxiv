{
    "title": "Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model. (arXiv:2309.08535v1 [cs.CV])",
    "abstract": "This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlab",
    "link": "http://arxiv.org/abs/2309.08535",
    "context": "Title: Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model. (arXiv:2309.08535v1 [cs.CV])\nAbstract: This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlab",
    "path": "papers/23/09/2309.08535.json",
    "total_tokens": 846,
    "translated_title": "用Whisper模型从自动标注中获得低资源语言的视觉语音识别",
    "translated_abstract": "本文提出了一种强大的视觉语音识别(VSR)方法，适用于多种语言，特别是那些标注数据有限的低资源语言。与之前试图通过从其他语言学习的知识来提高目标语言的VSR性能的方法不同，我们探索是否可以在不依赖人工干预的情况下增加不同语言的训练数据量。为此，我们使用了一个Whisper模型，它可以进行语言识别和基于音频的语音识别。它用于过滤所需语言的数据，并从未注释的多语言视听数据池中转录标签。通过比较使用自动标签和人工标注标签训练的VSR模型的性能，我们表明即使不使用人工注释，我们也可以达到与人工注释标签相似的VSR性能。通过自动标注的过程，我们标注了大规模未标注数据。",
    "tldr": "本文提出了一种利用Whisper模型从未标注的多语言视听数据自动标注的方法，实现了在低资源语言中的视觉语音识别，并证明了该方法可以获得与人工标注相似的性能。",
    "en_tdlr": "This paper proposes a method to automatically label low-resource languages using a Whisper model and achieve visual speech recognition without human annotations, demonstrating similar performance to manually labeled data."
}