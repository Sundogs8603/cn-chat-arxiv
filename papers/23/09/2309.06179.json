{
    "title": "Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])",
    "abstract": "Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT met",
    "link": "http://arxiv.org/abs/2309.06179",
    "context": "Title: Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])\nAbstract: Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT met",
    "path": "papers/23/09/2309.06179.json",
    "total_tokens": 940,
    "translated_title": "在同时机器翻译中展望未来",
    "translated_abstract": "同时机器翻译（SiMT）在阅读源语句的同时输出翻译。与传统的序列到序列（seq2seq）训练不同，现有的SiMT方法采用前缀到前缀（prefix2prefix）训练，即模型基于部分源标记预测目标标记。然而，前缀到前缀训练降低了模型捕捉全局信息的能力，并且由于缺乏必要的源信息而引入了强制预测。因此，弥合前缀到前缀训练和序列到序列训练之间差距以增强SiMT模型的翻译能力至关重要。在本文中，我们提出了一种新的方法，在课程学习中展望未来，实现从序列到序列训练过渡到前缀到前缀训练。具体而言，我们逐渐减少可用的源信息，从整个句子到与延迟对应的前缀。我们的方法适用于广泛的SiMT方法。",
    "tldr": "本文提出了一种新的方法，在同时机器翻译中通过课程学习的逐步减少可用源信息，从整个句子到与延迟对应的前缀，以实现从序列到序列训练到前缀到前缀训练的过渡，从而增强SiMT模型的翻译能力。",
    "en_tdlr": "This paper proposes a novel method that gradually reduces the available source information from the whole sentence to the prefix corresponding to the latency in simultaneous machine translation (SiMT), in order to bridge the gap between sequence-to-sequence training and prefix-to-prefix training and enhance the translation capability of the SiMT model."
}