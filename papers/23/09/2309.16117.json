{
    "title": "E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])",
    "abstract": "Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay t",
    "link": "http://arxiv.org/abs/2309.16117",
    "context": "Title: E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])\nAbstract: Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay t",
    "path": "papers/23/09/2309.16117.json",
    "total_tokens": 878,
    "translated_title": "E2Net: 弹性扩展网络实现资源高效的持续学习",
    "translated_abstract": "持续学习方法旨在学习新任务而不消除以前的知识。然而，持续学习通常需要大量的计算能力和存储容量才能达到令人满意的性能。在本文中，我们提出了一种资源高效的持续学习方法，称为弹性扩展网络（E2Net）。通过核心子网蒸馏和精确的回放样本选择，E2Net在相同的计算和存储限制下实现了卓越的平均准确性和较小的遗忘，并最大程度地减少了处理时间。在E2Net中，我们提出了代表性网络蒸馏，通过评估参数数量和与工作网络的输出相似性来识别代表性的核心子网，蒸馏工作网络内的类似子网以减轻对重演缓冲区的依赖，并促进跨先前任务的知识转移。为了提高存储资源利用率，我们还提出了子网约束经验回放方法。",
    "tldr": "E2Net是一种资源高效的持续学习方法，通过核心子网蒸馏和精确的回放样本选择，实现了卓越的准确性和较小的遗忘，在相同的计算和存储限制下最大程度地减少了处理时间。",
    "en_tdlr": "E2Net is a resource-efficient continual learning method that achieves superior accuracy and reduced forgetting by leveraging core subnet distillation and precise replay sample selection, while minimizing processing time within the same computational and storage constraints."
}