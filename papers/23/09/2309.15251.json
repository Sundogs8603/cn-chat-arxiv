{
    "title": "VPA: Fully Test-Time Visual Prompt Adaptation. (arXiv:2309.15251v1 [cs.CV])",
    "abstract": "Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various mode",
    "link": "http://arxiv.org/abs/2309.15251",
    "context": "Title: VPA: Fully Test-Time Visual Prompt Adaptation. (arXiv:2309.15251v1 [cs.CV])\nAbstract: Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various mode",
    "path": "papers/23/09/2309.15251.json",
    "total_tokens": 861,
    "translated_title": "VPA: 全面测试时间视觉提示适应",
    "translated_abstract": "文本提示调整已经展示出了显著的性能改进，通过将手工调整的提示作为可训练参数，将自然语言处理模型适应到各种下游任务中。受到文本提示成功的启发，一些研究探索了视觉提示调整的有效性。在这项工作中，我们提出了Visual Prompt Adaptation（VPA），这是第一个将视觉提示与测试时间适应相结合的框架。VPA引入了少量的可学习令牌，实现了完全测试时间和存储高效的适应，而不需要源域信息。我们在不同的适应设置下检验了VPA设计，包括单图像、批次图像和伪标签适应。我们在多个任务上评估了VPA，包括超出分布（OOD）泛化、损坏鲁棒性和领域适应。实验结果表明，VPA通过3.3%有效提高了超出分布泛化的能力。",
    "tldr": "VPA是首个将视觉提示与测试时间适应相结合的框架，通过引入可学习令牌实现了完全测试时间和存储高效的适应。实验证明，VPA能够有效提升超出分布泛化能力。",
    "en_tdlr": "VPA is the first framework that combines visual prompting with test-time adaptation, achieving fully test-time and storage-efficient adaptation by introducing learnable tokens. Experimental results demonstrate that VPA effectively enhances out-of-distribution generalization capability."
}