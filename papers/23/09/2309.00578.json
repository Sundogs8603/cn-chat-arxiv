{
    "title": "Consistency of Lloyd's Algorithm Under Perturbations. (arXiv:2309.00578v1 [cs.LG])",
    "abstract": "In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with g",
    "link": "http://arxiv.org/abs/2309.00578",
    "context": "Title: Consistency of Lloyd's Algorithm Under Perturbations. (arXiv:2309.00578v1 [cs.LG])\nAbstract: In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with g",
    "path": "papers/23/09/2309.00578.json",
    "total_tokens": 884,
    "translated_title": "Lloyd算法在扰动下的一致性",
    "translated_abstract": "在无监督学习的背景下，Lloyd算法是最常用的聚类算法之一。它启发了大量的工作，研究了算法在不同设置下对地面真实聚类的正确性。特别是在2016年，卢和周表明，在正确初始化算法的前提下，Lloyd算法在从亚高斯混合中独立抽取的n个样本上的错聚类率在O(log(n))次迭代后指数下界受限。然而，在许多应用中，真实样本是未观测到的，需要通过预处理流水线（如合适的数据矩阵上的谱方法）从数据中学习。我们展示了在适当初始化和扰动相对于亚高斯噪声较小的假设下，Lloyd算法在从亚高斯混合中扰动样本上的错聚类率在O(log(n))次迭代后同样指数下界受限。",
    "tldr": "该论文研究了Lloyd算法在扰动样本上的一致性，证明了在适当初始化和扰动相对于亚高斯噪声较小的假设下，算法在O(log(n))次迭代后的错聚类率在指数下界受限。",
    "en_tdlr": "This paper investigates the consistency of Lloyd's algorithm on perturbed samples and proves that under the assumptions of proper initialization and small perturbations relative to the sub-Gaussian noise, the algorithm's mis-clustering rate is exponentially bounded after O(log(n)) iterations."
}