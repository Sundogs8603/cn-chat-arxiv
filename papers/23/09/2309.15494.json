{
    "title": "VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning. (arXiv:2309.15494v1 [cs.CV])",
    "abstract": "Multimodal transfer learning aims to transform pretrained representations of diverse modalities into a common domain space for effective multimodal fusion. However, conventional systems are typically built on the assumption that all modalities exist, and the lack of modalities always leads to poor inference performance. Furthermore, extracting pretrained embeddings for all modalities is computationally inefficient for inference. In this work, to achieve high efficiency-performance multimodal transfer learning, we propose VideoAdviser, a video knowledge distillation method to transfer multimodal knowledge of video-enhanced prompts from a multimodal fundamental model (teacher) to a specific modal fundamental model (student). With an intuition that the best learning performance comes with professional advisers and smart students, we use a CLIP-based teacher model to provide expressive multimodal knowledge supervision signals to a RoBERTa-based student model via optimizing a step-distillat",
    "link": "http://arxiv.org/abs/2309.15494",
    "context": "Title: VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning. (arXiv:2309.15494v1 [cs.CV])\nAbstract: Multimodal transfer learning aims to transform pretrained representations of diverse modalities into a common domain space for effective multimodal fusion. However, conventional systems are typically built on the assumption that all modalities exist, and the lack of modalities always leads to poor inference performance. Furthermore, extracting pretrained embeddings for all modalities is computationally inefficient for inference. In this work, to achieve high efficiency-performance multimodal transfer learning, we propose VideoAdviser, a video knowledge distillation method to transfer multimodal knowledge of video-enhanced prompts from a multimodal fundamental model (teacher) to a specific modal fundamental model (student). With an intuition that the best learning performance comes with professional advisers and smart students, we use a CLIP-based teacher model to provide expressive multimodal knowledge supervision signals to a RoBERTa-based student model via optimizing a step-distillat",
    "path": "papers/23/09/2309.15494.json",
    "total_tokens": 883,
    "translated_title": "VideoAdviser: 视频知识蒸馏用于多模态迁移学习",
    "translated_abstract": "多模态迁移学习旨在将不同模态的预训练表示转换为一个共享的领域空间，以实现有效的多模态融合。然而，传统系统通常基于所有模态均存在的假设构建，并且缺乏模态会导致推理性能较差。此外，为所有模态提取预训练嵌入在推理中效率低下。在本文中，为了实现高效性能的多模态迁移学习，我们提出了一种视频知识蒸馏方法 VideoAdviser，将多模态视频增强提示的多模态知识从一个多模态基础模型（教师）传输到一个特定模态的基础模型（学生）。基于专业顾问和聪明学生能够获得最佳学习性能的直觉，我们使用基于CLIP的教师模型通过优化步骤蒸馏，为基于RoBERTa的学生模型提供富有表达力的多模态知识监督信号。",
    "tldr": "提出了一个视频知识蒸馏方法 VideoAdviser，通过将多模态视频增强提示的多模态知识从教师模型传输到学生模型，实现高效性能的多模态迁移学习。",
    "en_tdlr": "A video knowledge distillation method, VideoAdviser, is proposed for achieving efficient and high-performance multimodal transfer learning by transferring multimodal knowledge of video-enhanced prompts from a teacher model to a student model."
}