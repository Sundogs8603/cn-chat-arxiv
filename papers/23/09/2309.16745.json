{
    "title": "Efficient Training of One Class Classification-SVMs. (arXiv:2309.16745v1 [cs.LG])",
    "abstract": "This study examines the use of a highly effective training method to conduct one-class classification. The existence of both positive and negative examples in the training data is necessary to develop an effective classifier in common binary classification scenarios. Unfortunately, this criteria is not met in many domains. Here, there is just one class of examples. Classification algorithms that learn from solely positive input have been created to deal with this setting. In this paper, an effective algorithm for dual soft-margin one-class SVM training is presented. Our approach makes use of the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method. The FPGM requires only first derivatives, which for the dual soft margin OCC-SVM means computing mainly a matrix-vector product. Therefore, AL-FPGM, being computationally inexpensive, may complement existing quadratic programming solvers for training large SVMs. We extensively validate our approach over real-world ",
    "link": "http://arxiv.org/abs/2309.16745",
    "context": "Title: Efficient Training of One Class Classification-SVMs. (arXiv:2309.16745v1 [cs.LG])\nAbstract: This study examines the use of a highly effective training method to conduct one-class classification. The existence of both positive and negative examples in the training data is necessary to develop an effective classifier in common binary classification scenarios. Unfortunately, this criteria is not met in many domains. Here, there is just one class of examples. Classification algorithms that learn from solely positive input have been created to deal with this setting. In this paper, an effective algorithm for dual soft-margin one-class SVM training is presented. Our approach makes use of the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method. The FPGM requires only first derivatives, which for the dual soft margin OCC-SVM means computing mainly a matrix-vector product. Therefore, AL-FPGM, being computationally inexpensive, may complement existing quadratic programming solvers for training large SVMs. We extensively validate our approach over real-world ",
    "path": "papers/23/09/2309.16745.json",
    "total_tokens": 914,
    "translated_title": "高效训练一类分类支持向量机",
    "translated_abstract": "本研究探讨了一种高效的训练方法来进行一类分类。在常见的二分类场景中，训练数据需要同时存在正例和负例才能开发出有效的分类器。然而，在许多领域中并不满足这个条件。针对这种情况，已经开发出了从纯正例输入中学习的分类算法。本文介绍了一种有效的双软边界一类支持向量机训练算法。我们的方法利用了增广Lagrange乘子法（AL-FPGM），这是一种快速投影梯度方法的变体。FPGM只需要一阶导数，对于双软边界一类支持向量机来说，主要是计算矩阵向量乘积。因此，计算成本低廉的AL-FPGM可以与现有的二次规划求解器相结合，用于训练大规模支持向量机。我们在真实世界的数据集上广泛验证了我们的方法。",
    "tldr": "本研究提出了一种高效训练方法来进行一类分类支持向量机。通过利用增广Lagrange乘子法（AL-FPGM），该方法可以在只有正例的情况下训练有效的分类器，并且计算成本低廉，可以用于训练大规模支持向量机。",
    "en_tdlr": "This study proposes an efficient training method for one-class classification support vector machines. By utilizing the Augmented Lagrangian (AL-FPGM), this method can train effective classifiers in scenarios where only positive examples are available, and it is computationally inexpensive, making it suitable for training large-scale SVMs."
}