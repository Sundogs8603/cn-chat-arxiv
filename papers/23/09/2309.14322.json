{
    "title": "Small-scale proxies for large-scale Transformer training instabilities. (arXiv:2309.14322v2 [cs.LG] UPDATED)",
    "abstract": "Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to",
    "link": "http://arxiv.org/abs/2309.14322",
    "context": "Title: Small-scale proxies for large-scale Transformer training instabilities. (arXiv:2309.14322v2 [cs.LG] UPDATED)\nAbstract: Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to",
    "path": "papers/23/09/2309.14322.json",
    "total_tokens": 982,
    "translated_title": "大规模Transformer训练不稳定性的小规模代理",
    "translated_abstract": "已有研究团队在训练大规模Transformer模型时报告了在小规模训练中未出现的训练不稳定性问题。尽管这些不稳定性问题的原因具有科学意义，但需要大量资源进行复现，使得研究变得困难。本文旨在寻找在小规模上复现和研究训练的稳定性和不稳定性的方法。首先，我们关注先前工作中描述的两个训练不稳定性的源头：注意力层中logits的增长（Dehghani等人，2023）和输出logits与对数概率之间的发散（Chowdhery等人，2022）。通过在各个尺度上测量学习率和损失之间的关系，我们发现当以高学习率训练小模型时，这些不稳定性也会出现，并且先前在大规模上使用的缓解方法在这个情景下同样有效。这促使我们进一步研究程度上是否可以将这些缓解方法推广到大规模训练中。",
    "tldr": "本文研究了大规模Transformer训练中的不稳定性问题，并找到了对应的小规模代理模型来复现和研究这些问题。研究人员发现训练不稳定性的两个源头，并表明先前使用的缓解方法在小规模训练中同样有效。这个发现有助于将缓解方法推广到大规模训练中。",
    "en_tdlr": "This paper investigates the training instabilities in large-scale Transformer training and finds small-scale proxies to reproduce and study these instabilities. The researchers identify two sources of instabilities and show that previous mitigations are effective in small-scale training. The findings contribute to the generalizability of these mitigations to large-scale training."
}