{
    "title": "BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])",
    "abstract": "This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and trainin",
    "link": "http://arxiv.org/abs/2309.14774",
    "context": "Title: BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])\nAbstract: This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and trainin",
    "path": "papers/23/09/2309.14774.json",
    "total_tokens": 907,
    "translated_title": "BLIP-Adapter：移动设备屏幕截图标题生成的参数高效迁移学习",
    "translated_abstract": "本研究旨在探索有效的调整方法来处理屏幕截图标题生成任务。近年来，图像标题生成取得了显著进展，但是对于移动设备屏幕截图的标题生成任务的研究相对较少。目前的数据集和使用案例中对产品截屏中的用户行为的描述相对有限。因此，我们尝试对现有模型进行微调，以解决屏幕截图标题生成任务。然而，微调大型预训练模型可能消耗大量资源，需要考虑到图像标题生成模型中大量参数的时间、计算力和存储开销。为了解决这一挑战，本研究提出了一种适配器方法的组合，只需要调整模型中的附加模块。这些方法最初是为视觉或语言任务设计的，我们的意图是将它们应用于解决屏幕截图标题生成中的类似挑战。通过冻结图像标题模型的参数并训练其他模块，可以实现参数高效的迁移学习。",
    "tldr": "本研究提出了一种参数高效的迁移学习方法，通过冻结图像标题模型的参数并只调整附加模块，解决了移动设备屏幕截图标题生成任务中大量参数的开销问题。"
}