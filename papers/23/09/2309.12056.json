{
    "title": "BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])",
    "abstract": "This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor",
    "link": "http://arxiv.org/abs/2309.12056",
    "context": "Title: BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])\nAbstract: This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor",
    "path": "papers/23/09/2309.12056.json",
    "total_tokens": 970,
    "translated_title": "BELT: 通过自然语言监督进行脑电图到语言解码和零样本情感分类的引导式模型和学习框架",
    "translated_abstract": "本文介绍了BELT，这是一个针对大脑到语言翻译研究的重要主题的新模型和学习框架。将非侵入性脑信号翻译成可读的自然语言有潜力推动大脑-计算机界面（BCI）的应用场景和发展。脑信号解码或脑-语言翻译中的关键问题是从有限规模和质量的数据集中获得语义适当且具有区分性的脑电图表示。所提出的BELT方法是一个通用且高效的框架，利用现成的大规模预训练语言模型（LM）引导脑电图表示学习。通过利用训练在互联网规模数据集上的大规模LM理解语义信息和零样本泛化能力，BELT极大改进了对脑电信号的理解。具体而言，BELT模型由一个深度神经网络组成，其中包含一个解码神经网络和一个预训练LM。",
    "tldr": "BELT是一种通过自然语言监督进行脑电图到语言解码和零样本情感分类的引导式模型和学习框架。它利用现成的大规模预训练语言模型来改进脑电信号的理解，从而推动了大脑-计算机界面的应用和发展。",
    "en_tdlr": "BELT is a bootstrapping model and learning framework that enables EEG-to-language decoding and zero-shot sentiment classification through natural language supervision. It utilizes large-scale pretrained language models to improve the understanding of EEG signals, promoting the application and development of brain-computer interfaces."
}