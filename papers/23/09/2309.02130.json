{
    "title": "Asymmetric Momentum: A Rethinking of Gradient Descent. (arXiv:2309.02130v2 [cs.LG] UPDATED)",
    "abstract": "Through theoretical and experimental validation, unlike all existing adaptive methods like Adam which penalize frequently-changing parameters and are only applicable to sparse gradients, we propose the simplest SGD enhanced method, Loss-Controlled Asymmetric Momentum(LCAM). By averaging the loss, we divide training process into different loss phases and using different momentum. It not only can accelerates slow-changing parameters for sparse gradients, similar to adaptive optimizers, but also can choose to accelerates frequently-changing parameters for non-sparse gradients, thus being adaptable to all types of datasets. We reinterpret the machine learning training process through the concepts of weight coupling and weight traction, and experimentally validate that weights have directional specificity, which are correlated with the specificity of the dataset. Thus interestingly, we observe that in non-sparse gradients, frequently-changing parameters should actually be accelerated, which",
    "link": "http://arxiv.org/abs/2309.02130",
    "context": "Title: Asymmetric Momentum: A Rethinking of Gradient Descent. (arXiv:2309.02130v2 [cs.LG] UPDATED)\nAbstract: Through theoretical and experimental validation, unlike all existing adaptive methods like Adam which penalize frequently-changing parameters and are only applicable to sparse gradients, we propose the simplest SGD enhanced method, Loss-Controlled Asymmetric Momentum(LCAM). By averaging the loss, we divide training process into different loss phases and using different momentum. It not only can accelerates slow-changing parameters for sparse gradients, similar to adaptive optimizers, but also can choose to accelerates frequently-changing parameters for non-sparse gradients, thus being adaptable to all types of datasets. We reinterpret the machine learning training process through the concepts of weight coupling and weight traction, and experimentally validate that weights have directional specificity, which are correlated with the specificity of the dataset. Thus interestingly, we observe that in non-sparse gradients, frequently-changing parameters should actually be accelerated, which",
    "path": "papers/23/09/2309.02130.json",
    "total_tokens": 774,
    "translated_title": "非对称动量：对梯度下降的重新思考",
    "translated_abstract": "通过理论和实验证明，与Adam等现有的自适应方法相比，我们提出了最简单的SGD增强方法——Loss-Controlled Asymmetric Momentum（LCAM）。通过对损失进行平均，我们将训练过程分为不同的损失阶段，并使用不同的动量。它不仅可以加速稀疏梯度下的缓慢变化参数，类似于自适应优化器，还可以选择在非稀疏梯度下加速频繁变化的参数，从而适应所有类型的数据集。我们通过权重耦合和权重牵引的概念对机器学习训练过程进行重新解释，并通过实验证明权重具有方向特异性，与数据集的特异性相关。因此，有趣的是，我们观察到在非稀疏梯度情况下，实际上应该加速频繁变化的参数。",
    "tldr": "Loss-Controlled Asymmetric Momentum (LCAM) is proposed as a simple and versatile optimization method that can adapt to all types of datasets by dividing the training process into different loss phases and using different momentum. Experimental results suggest that frequently-changing parameters should be accelerated in non-sparse gradients, challenging the conventional wisdom."
}