{
    "title": "User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue. (arXiv:2309.13233v1 [cs.CL])",
    "abstract": "One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with ",
    "link": "http://arxiv.org/abs/2309.13233",
    "context": "Title: User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue. (arXiv:2309.13233v1 [cs.CL])\nAbstract: One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with ",
    "path": "papers/23/09/2309.13233.json",
    "total_tokens": 929,
    "translated_title": "使用大型语言模型进行用户模拟，用于评估任务导向对话",
    "translated_abstract": "新任务导向对话系统开发过程中的一个主要障碍是需要在多个阶段和迭代中进行人工评估。为了实现对任务导向对话的自动化评估，我们提出了一种新颖的用户模拟器，利用最近开发的大型预训练语言模型（LLMs）构建。为了增加我们系统的语言多样性，相对于相关先前的工作，我们不在现有的任务导向对话数据集上对我们系统使用的LLMs进行微调；相反，我们使用上下文学习来提示LLMs生成鲁棒且语言多样的输出，以模拟人类对话参与者的行为。不同于以目标成功率（GSR）作为模拟器性能的主要指标的先前工作，我们的目标是使系统实现与人类与任务导向对话系统交互中观察到的GSR相似。采用这种方法，我们目前的模拟器能够有效地与人类进行互动。",
    "tldr": "提出了一种使用大型语言模型进行用户模拟的方法，用于评估任务导向对话系统。通过在上下文中学习，可以生成具有鲁棒性和语言多样性的输出，模拟人类对话参与者的行为。目标是使系统的成功率与人类和任务导向对话系统的交互相似。"
}