{
    "title": "Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])",
    "abstract": "The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.",
    "link": "http://arxiv.org/abs/2309.10299",
    "context": "Title: Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])\nAbstract: The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.",
    "path": "papers/23/09/2309.10299.json",
    "total_tokens": 812,
    "translated_title": "使用微调和最小先行搜索来提高Whisper",
    "translated_abstract": "Whisper在低资源语言上的性能仍然远离完美。除了低资源语言上缺乏训练数据外，我们还发现Whisper中使用的束搜索算法存在一些限制。为了解决这些问题，我们在额外的数据上对Whisper进行微调，并提出了一种改进的解码算法。在越南语上，使用LoRA对Whisper-Tiny进行微调可以将WER的改进提高38.49，相比于全参数微调，进一步减少了1.45。此外，通过使用Filter-Ends和Min Lookahead解码算法，与标准束搜索相比，WER在一系列语言中平均降低了2.26。这些结果可以推广到更大的Whisper模型尺寸。我们还证明了Min Lookahead优于Whisper中使用的标准束搜索算法。",
    "tldr": "使用微调和最小先行搜索算法来改进Whisper，在低资源语言上提高了性能，并且证明了最小先行搜索优于标准束搜索算法。",
    "en_tdlr": "Using fine-tuning and min lookahead beam search, this paper improves the performance of Whisper in low-resource languages. The proposed improvements include fine-tuning Whisper on additional data and using an improved decoding algorithm. The experiments show that fine-tuning leads to significant reductions in WER, and the Min Lookahead algorithm outperforms the standard beam search algorithm used in Whisper."
}