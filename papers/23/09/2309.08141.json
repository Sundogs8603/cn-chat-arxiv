{
    "title": "Audio Difference Learning for Audio Captioning. (arXiv:2309.08141v1 [eess.AS])",
    "abstract": "This study introduces a novel training paradigm, audio difference learning, for improving audio captioning. The fundamental concept of the proposed learning method is to create a feature representation space that preserves the relationship between audio, enabling the generation of captions that detail intricate audio information. This method employs a reference audio along with the input audio, both of which are transformed into feature representations via a shared encoder. Captions are then generated from these differential features to describe their differences. Furthermore, a unique technique is proposed that involves mixing the input audio with additional audio, and using the additional audio as a reference. This results in the difference between the mixed audio and the reference audio reverting back to the original input audio. This allows the original input's caption to be used as the caption for their difference, eliminating the need for additional annotations for the difference",
    "link": "http://arxiv.org/abs/2309.08141",
    "context": "Title: Audio Difference Learning for Audio Captioning. (arXiv:2309.08141v1 [eess.AS])\nAbstract: This study introduces a novel training paradigm, audio difference learning, for improving audio captioning. The fundamental concept of the proposed learning method is to create a feature representation space that preserves the relationship between audio, enabling the generation of captions that detail intricate audio information. This method employs a reference audio along with the input audio, both of which are transformed into feature representations via a shared encoder. Captions are then generated from these differential features to describe their differences. Furthermore, a unique technique is proposed that involves mixing the input audio with additional audio, and using the additional audio as a reference. This results in the difference between the mixed audio and the reference audio reverting back to the original input audio. This allows the original input's caption to be used as the caption for their difference, eliminating the need for additional annotations for the difference",
    "path": "papers/23/09/2309.08141.json",
    "total_tokens": 853,
    "translated_title": "音频差异学习用于音频字幕生成",
    "translated_abstract": "本研究引入了一种新的训练范式，即音频差异学习，用于改进音频字幕生成。所提出的学习方法的基本概念是创建一个保留音频之间关系的特征表示空间，从而能够生成详细描述复杂音频信息的字幕。该方法使用参考音频和输入音频，通过共享编码器将它们转换为特征表示。然后，从这些差异特征生成字幕描述它们的差异。此外，提出了一种独特的技术，涉及将输入音频与额外音频混合，并使用额外音频作为参考。这样，混合音频与参考音频之间的差异回到原始输入音频。这允许将原始输入的字幕作为其差异的字幕使用，消除了为差异添加额外注释的需求。",
    "tldr": "本研究引入了音频差异学习方法，通过创建特征表示空间来改进音频字幕生成。该方法使用参考音频和输入音频，生成描述它们差异的字幕，同时提出了一种独特的混合技术来消除差异和原始输入之间的需求。",
    "en_tdlr": "This study introduces audio difference learning for improving audio captioning by creating a feature space that preserves the relationship between audio. It generates captions detailing intricate audio information by utilizing reference and input audio. Additionally, a unique mixing technique is proposed to eliminate the need for additional annotations."
}