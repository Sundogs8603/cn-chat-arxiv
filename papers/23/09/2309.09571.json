{
    "title": "Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)",
    "abstract": "Small CNN-based models usually require transferring knowledge from a large model before they are deployed in computationally resource-limited edge devices. Masked image modeling (MIM) methods achieve great success in various visual tasks but remain largely unexplored in knowledge distillation for heterogeneous deep models. The reason is mainly due to the significant discrepancy between the Transformer-based large model and the CNN-based small network. In this paper, we develop the first Heterogeneous Generative Knowledge Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge from large Transformer models to small CNN-based models in a generative self-supervised fashion. Our method builds a bridge between Transformer-based models and CNNs by training a UNet-style student with sparse convolution, which can effectively mimic the visual representation inferred by a teacher over masked modeling. Our method is a simple yet effective learning paradigm to learn the visual ",
    "link": "http://arxiv.org/abs/2309.09571",
    "context": "Title: Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)\nAbstract: Small CNN-based models usually require transferring knowledge from a large model before they are deployed in computationally resource-limited edge devices. Masked image modeling (MIM) methods achieve great success in various visual tasks but remain largely unexplored in knowledge distillation for heterogeneous deep models. The reason is mainly due to the significant discrepancy between the Transformer-based large model and the CNN-based small network. In this paper, we develop the first Heterogeneous Generative Knowledge Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge from large Transformer models to small CNN-based models in a generative self-supervised fashion. Our method builds a bridge between Transformer-based models and CNNs by training a UNet-style student with sparse convolution, which can effectively mimic the visual representation inferred by a teacher over masked modeling. Our method is a simple yet effective learning paradigm to learn the visual ",
    "path": "papers/23/09/2309.09571.json",
    "total_tokens": 983,
    "translated_title": "使用遮掩图像建模的异构生成知识蒸馏",
    "translated_abstract": "在计算资源有限的边缘设备上部署时，小型基于CNN的模型通常需要从大型模型中转移知识。遮掩图像建模（MIM）方法在各种视觉任务中取得了很大的成功，但在异构深度模型的知识蒸馏中仍然很少被探索。主要原因是基于Transformer的大型模型与基于CNN的小型网络之间存在显著差异。在本文中，我们基于MIM开发了第一个使用异构生成知识蒸馏（H-GKD）的方法，可以以生成的自监督方式高效地从大型Transformer模型传递知识给小型基于CNN的模型。我们的方法通过训练一个带有稀疏卷积的UNet风格的学生网络来构建Transformer模型和CNN之间的桥梁，这个学生网络可以有效地模仿教师网络在遮掩建模下推断的视觉表示。我们的方法是一种简单而有效的学习范式，用于学习视觉提示和知识在异构模型之间的转移。",
    "tldr": "本文提出了一种基于遮掩图像建模的异构生成知识蒸馏（H-GKD）方法，可以高效地从大型Transformer模型传递知识给小型CNN模型。通过训练一个带有稀疏卷积的UNet风格学生网络，桥接了Transformer模型和CNN模型之间的差距，实现了视觉表示的有效模拟。",
    "en_tdlr": "This paper presents a Heterogeneous Generative Knowledge Distillation (H-GKD) method based on Masked Image Modeling (MIM), which efficiently transfers knowledge from large Transformer models to small CNN models. By training a UNet-style student network with sparse convolution, it bridges the gap between Transformer models and CNNs and effectively mimics the visual representation."
}