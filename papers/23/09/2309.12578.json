{
    "title": "SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. (arXiv:2309.12578v1 [cs.LG])",
    "abstract": "Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer duri",
    "link": "http://arxiv.org/abs/2309.12578",
    "context": "Title: SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. (arXiv:2309.12578v1 [cs.LG])\nAbstract: Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer duri",
    "path": "papers/23/09/2309.12578.json",
    "total_tokens": 834,
    "translated_title": "SPION：通过卷积泛滥填充实现Transformer的逐层稀疏训练",
    "translated_abstract": "稀疏化Transformer已经引起了相当大的兴趣，因为训练Transformer需要很大的计算量。先前稀疏化Transformer的方法要么使用固定的模式，要么使用数据驱动的方法来减少涉及计算多头注意力的操作数量，这是Transformer的主要瓶颈。然而，现有方法存在不可避免的问题，如由于在所有层中应用统一的固定模式而导致的潜在序列关键特征损失，以及由于使用额外参数学习注意力操作中的稀疏模式而导致模型大小的增加。在本文中，我们提出了一种新颖的Transformer稀疏化方案，该方案将卷积滤波器和泛滥填充方法相结合，以高效地捕捉注意力操作的逐层稀疏模式。我们的稀疏化方法在Transformer的计算复杂度和内存占用方面表现出色。",
    "tldr": "本文提出了一种新颖的Transformer稀疏化方案，通过集成卷积滤波器和泛滥填充方法，高效地实现了注意力操作的逐层稀疏模式，减少了计算复杂度和内存占用。"
}