{
    "title": "RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])",
    "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa",
    "link": "http://arxiv.org/abs/2309.17182",
    "context": "Title: RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])\nAbstract: COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa",
    "path": "papers/23/09/2309.17182.json",
    "total_tokens": 886,
    "translated_title": "RECOMBINER：鲁棒性和增强的贝叶斯隐式神经表示的压缩方法",
    "translated_abstract": "COMBINER是一种最近提出的数据压缩方法，它解决了先前基于隐式神经表示的方法存在的关键效率问题：避免了量化，并实现了对速率-失真性能的直接优化。然而，COMBINER仍然存在明显的局限性：1）使用因式化的先验和后验逼近，缺乏灵活性；2）不能有效地适应数据中的局部偏离全局模式；3）其性能易受建模选择和变分参数初始化的影响。我们提出的方法，鲁棒和增强的COMBINER(RECOMBINER)，通过以下方式解决了这些问题：1）通过对INR权重进行线性参数化，丰富变分逼近，并保持计算成本；2）通过增加可学习的位置编码来增强我们的INR，使其适应局部细节；3）将高分辨率数据分割成...",
    "tldr": "RECOMBINER是一种鲁棒性和增强的贝叶斯隐式神经表示的压缩方法，通过丰富变分逼近、增加位置编码和分割高分辨率数据来解决COMBINER存在的局限性。"
}