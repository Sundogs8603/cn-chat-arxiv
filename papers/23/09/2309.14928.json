{
    "title": "Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])",
    "abstract": "Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag",
    "link": "http://arxiv.org/abs/2309.14928",
    "context": "Title: Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])\nAbstract: Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag",
    "path": "papers/23/09/2309.14928.json",
    "total_tokens": 939,
    "translated_title": "噪声容忍的无监督视觉语言模型适配器",
    "translated_abstract": "最近在大规模的视觉语言模型中取得了非常显著的表现，在各种零样本图像分类任务中获得了良好的性能。然而，先前的研究通过引入少样本有标签目标样本已经取得了显著的改进，但仍需要目标样本的标注，这在处理各种视觉识别任务时大大降低了可扩展性。我们设计了一种噪声容忍的无监督适配器(NtUA)，它允许使用少样本无标签目标样本来学习优秀的目标模型。NtUA作为一个键值缓存，将少样本无标签目标样本的视觉特征和预测的伪标签作为键值对进行建模。它由两个互补的设计组成。第一个是自适应缓存形成，通过根据其预测置信度对键值对进行加权，以对抗伪标签的噪声。第二个是伪标签修正，它通过利用键值对的权重来修正伪标签以及缓存权重。",
    "tldr": "这篇论文介绍了一种噪声容忍的无监督适配器(NtUA)，它可以使用少样本无标签目标样本来学习优秀的视觉语言模型。NtUA通过自适应缓存形成和伪标签修正来对抗伪标签噪声。",
    "en_tdlr": "This paper presents a noise-tolerant unsupervised adapter (NtUA) that allows learning superior vision-language models with few-shot unlabelled target samples. NtUA combats pseudo-label noise through adaptive cache formation and pseudo-label rectification."
}