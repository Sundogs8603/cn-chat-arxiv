{
    "title": "Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])",
    "abstract": "Large language models (LLMs) like ChatGPT have excited scientists across fields; in medicine, one source of excitement is the potential applications of LLMs trained on electronic health record (EHR) data. But there are tough questions we must first answer if health care institutions are interested in having LLMs trained on their own data; should they train an LLM from scratch or fine-tune it from an open-source model? For healthcare institutions with a predefined budget, what are the biggest LLMs they can afford? In this study, we take steps towards answering these questions with an analysis on dataset sizes, model sizes, and costs for LLM training using EHR data. This analysis provides a framework for thinking about these questions in terms of data scale, compute scale, and training budgets.",
    "link": "http://arxiv.org/abs/2309.12339",
    "context": "Title: Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])\nAbstract: Large language models (LLMs) like ChatGPT have excited scientists across fields; in medicine, one source of excitement is the potential applications of LLMs trained on electronic health record (EHR) data. But there are tough questions we must first answer if health care institutions are interested in having LLMs trained on their own data; should they train an LLM from scratch or fine-tune it from an open-source model? For healthcare institutions with a predefined budget, what are the biggest LLMs they can afford? In this study, we take steps towards answering these questions with an analysis on dataset sizes, model sizes, and costs for LLM training using EHR data. This analysis provides a framework for thinking about these questions in terms of data scale, compute scale, and training budgets.",
    "path": "papers/23/09/2309.12339.json",
    "total_tokens": 876,
    "translated_title": "对医疗机构在电子病历上使用大型语言模型进行训练的考虑",
    "translated_abstract": "大型语言模型（LLM）（如ChatGPT）引起了跨学科科学家的兴趣；在医学领域中，对LLM在电子病历数据上进行训练的潜在应用也引发了关注。然而，如果医疗机构有意让LLM在自己的数据上进行训练，我们首先必须面对一些棘手的问题：他们应该从头开始训练LLM，还是从开源模型进行微调？对于预先确定预算的医疗机构来说，他们可以负担得起的最大LLM是什么？在本研究中，我们通过分析数据集大小、模型大小和使用电子病历数据进行LLM训练的成本来对这些问题进行了初步探讨。这个分析为从数据规模、计算规模和训练预算的角度思考这些问题提供了一个框架。",
    "tldr": "本研究通过分析数据集大小、模型大小和使用电子病历数据进行大型语言模型（LLM）训练的成本，提供了一个思考医疗机构是否应该训练LLM以及如何在预算限制下选择合适LLM的框架。",
    "en_tdlr": "This study provides a framework for healthcare institutions to consider whether to train large language models (LLMs) and how to select suitable LLMs within budget constraints, by analyzing dataset sizes, model sizes, and costs for LLM training using electronic health record (EHR) data."
}