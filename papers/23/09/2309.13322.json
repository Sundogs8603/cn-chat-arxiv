{
    "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
    "abstract": "arXiv:2309.13322v2 Announce Type: replace  Abstract: The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \"Cross-Model Detection,\" by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a",
    "link": "https://arxiv.org/abs/2309.13322",
    "context": "Title: From Text to Source: Results in Detecting Large Language Model-Generated Content\nAbstract: arXiv:2309.13322v2 Announce Type: replace  Abstract: The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \"Cross-Model Detection,\" by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a",
    "path": "papers/23/09/2309.13322.json",
    "total_tokens": 791,
    "translated_title": "从文本到来源: 在检测大型语言模型生成内容方面的结果",
    "translated_abstract": "大型语言模型（LLMs）的广泛应用引起了对信息错误和道德影响的担忧。为了解决这些问题，需要开发稳健的方法来检测和归因LLMs生成的文本。本文研究了“跨模型检测”，评估了一个经过训练以区分源LLM生成的文本和人工编写文本的分类器是否也能检测目标LLM生成的文本，而无需进一步训练。该研究全面探讨了不同大小和家族的LLMs，评估了对分类器泛化的对话微调技术、量化和水印的影响。研究还探讨了模型归因，包括源模型识别、模型家族和模型大小分类，以及量化和水印检测。我们的结果揭示了几个关键发现：",
    "tldr": "本研究探讨了跨模型检测方法，在评估分类器是否能检测来自目标LLM的文本方面取得了关键发现",
    "en_tdlr": "This study investigates cross-model detection methods and identifies key findings in evaluating whether a classifier can detect text from a target LLM."
}