{
    "title": "Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])",
    "abstract": "As recently demonstrated, Deep Neural Networks (DNN), usually trained using single precision IEEE 754 floating point numbers (binary32), can also work using lower precision. Therefore, 16-bit and 8-bit compressed format have attracted considerable attention. In this paper, we focused on two families of formats that have already achieved interesting results in compressing binary32 numbers in machine learning applications, without sensible degradation of the accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely used for reducing the storage of the weights/biases of trained DNNs, the inference still often happens on the 32-bit FPU of the CPU (especially if GPUs are not available). In this paper we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers of a vector capable CPU, in order to save bandwidth usage and increase cache efficiency. Finally, we show the",
    "link": "http://arxiv.org/abs/2309.07158",
    "context": "Title: Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])\nAbstract: As recently demonstrated, Deep Neural Networks (DNN), usually trained using single precision IEEE 754 floating point numbers (binary32), can also work using lower precision. Therefore, 16-bit and 8-bit compressed format have attracted considerable attention. In this paper, we focused on two families of formats that have already achieved interesting results in compressing binary32 numbers in machine learning applications, without sensible degradation of the accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely used for reducing the storage of the weights/biases of trained DNNs, the inference still often happens on the 32-bit FPU of the CPU (especially if GPUs are not available). In this paper we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers of a vector capable CPU, in order to save bandwidth usage and increase cache efficiency. Finally, we show the",
    "path": "papers/23/09/2309.07158.json",
    "total_tokens": 896,
    "translated_title": "AI中的压缩实数：使用RISC-V CPU的案例研究",
    "translated_abstract": "正如最近的研究所示，通常使用单精度IEEE 754浮点数（二进制32位）训练的深度神经网络（DNN）也可以使用较低精度。因此，16位和8位的压缩格式引起了相当大的关注。本文侧重于两个已经在机器学习应用中压缩二进制32位数取得有趣结果的格式族：bfloat和posit。尽管16位和8位的bfloat/posit常用于减少已经训练好的DNN的权重/偏差的存储，但推理仍然经常在CPU的32位FPU上进行（尤其是如果没有GPU可用）。本文提出了一种在计算之前解压bfloat/posit张量的方法，即在压缩操作数加载到支持向量的向量寄存器之后，以节省带宽使用和提高缓存效率。最后，我们展示了",
    "tldr": "本文针对AI中的深度神经网络（DNN）使用较低精度进行训练，探讨了16位和8位压缩格式的应用。通过在计算之前解压压缩操作数，可以提高带宽使用和缓存效率。",
    "en_tdlr": "This paper focuses on the application of 16-bit and 8-bit compressed formats for training deep neural networks (DNN) in AI. By decompressing the operands before computation, it aims to improve bandwidth usage and cache efficiency."
}