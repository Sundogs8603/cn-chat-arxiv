{
    "title": "Guide Your Agent with Adaptive Multimodal Rewards. (arXiv:2309.10790v2 [cs.LG] UPDATED)",
    "abstract": "Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-traine",
    "link": "http://arxiv.org/abs/2309.10790",
    "context": "Title: Guide Your Agent with Adaptive Multimodal Rewards. (arXiv:2309.10790v2 [cs.LG] UPDATED)\nAbstract: Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-traine",
    "path": "papers/23/09/2309.10790.json",
    "total_tokens": 1010,
    "translated_title": "用自适应多模态奖励引导你的智能体",
    "translated_abstract": "在模仿学习中，开发一个能够适应未知环境的智能体仍然是一个具有挑战性的问题。本文提出了一种名为自适应返回条件策略(ARP)的高效框架，用于通过自然语言任务描述和预训练的多模态编码器来提升智能体的泛化能力。我们的关键思想是在预训练的多模态嵌入空间(例如CLIP)中计算视觉观测和自然语言指令之间的相似度，并将其作为奖励信号。然后，我们使用用多模态奖励标记的专家演示来训练一个返回条件策略。由于多模态奖励在每个时间步提供自适应信号，我们的ARP有效地缓解了目标误泛化问题。与现有的文本条件策略相比，即使面对未知的文本指令，我们的ARP在泛化性能方面也表现出众。为了提高奖励的质量，我们还引入了一种预训练微调方法。",
    "tldr": "本文提出了一种自适应返回条件策略（ARP）框架，通过使用自然语言任务描述和预训练的多模态编码器来提升智能体的泛化能力。通过在预训练的多模态嵌入空间中计算视觉观察和自然语言指令之间的相似度，并将其用作奖励信号，ARP有效缓解了目标误泛化问题，并在面对未知的文本指令时展现出了出色的泛化性能。",
    "en_tdlr": "This paper presents Adaptive Return-conditioned Policy (ARP), a framework that enhances the generalization ability of an agent by using natural language task descriptions and pre-trained multimodal encoders. By calculating the similarity between visual observations and language instructions in a pre-trained multimodal embedding space, ARP effectively mitigates goal misgeneralization and achieves superior generalization performance even when faced with unknown text instructions."
}