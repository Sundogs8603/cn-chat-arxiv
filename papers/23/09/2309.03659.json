{
    "title": "Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])",
    "abstract": "Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter",
    "link": "http://arxiv.org/abs/2309.03659",
    "context": "Title: Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])\nAbstract: Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter",
    "path": "papers/23/09/2309.03659.json",
    "total_tokens": 960,
    "translated_title": "在语义图像分割中实现可比较的知识蒸馏",
    "translated_abstract": "知识蒸馏（KD）是解决语义分割中大模型尺寸和慢推理速度的一种提出的解决方案。在我们的研究中，我们从过去4年的14个出版物中鉴定出了25个提出的蒸馏损失项。不幸的是，基于已发布结果的术语比较通常是不可能的，因为训练配置的差异。这个问题的一个很好的例子是对比2022年的两个出版物。使用相同的模型和数据集，结构和统计纹理蒸馏（SSTKD）报告了学生mIoU增加了4.54个百分点，最终性能达到了29.19，而自适应透视蒸馏（APD）仅仅提高了学生性能2.06个百分点，但实现了39.25的最终性能。这种极端差异的原因通常是超参数的次优选择以及作为参考点的学生模型性能不佳。在我们的工作中，我们揭示了超参数不足的问题。",
    "tldr": "在这项研究中，我们探索了语义图像分割中知识蒸馏的问题。我们发现了25种蒸馏损失项，并指出由于训练配置的差异导致术语比较困难。此外，我们发现超参数选择不当会导致极端的性能差异。"
}