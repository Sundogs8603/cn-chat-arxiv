{
    "title": "Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])",
    "abstract": "FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.",
    "link": "http://arxiv.org/abs/2309.17224",
    "context": "Title: Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])\nAbstract: FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.",
    "path": "papers/23/09/2309.17224.json",
    "total_tokens": 906,
    "translated_title": "使用8位浮点数训练和推断大型语言模型",
    "translated_abstract": "FP8格式正在受到青睐，以提高训练和推断大型深度学习模型的计算效率。它们的主要挑战是需要谨慎选择缩放以防止由于较高精度格式的动态范围的减少而导致性能下降。尽管关于选择INT格式的这些缩放因子的文献很多，但对于FP8来说，这一关键方面尚未得到解决。本文提出了一种基于动态更新权重、梯度和激活的每个张量尺度的FP8线性层缩放选择方法。我们将这种方法应用于使用FP8训练和验证GPT和Llama 2等类型的大型语言模型，模型大小范围从111M到70B不等。为了便于理解FP8的动态特性，在训练和推断过程中，我们的结果附带了权重、激活和梯度的每个张量尺度分布的图示。",
    "tldr": "本文介绍了一种用于选择FP8线性层的缩放方法，该方法基于动态更新每个张量的权重、梯度和激活的尺度。通过将这种方法应用于大型语言模型的训练和验证，我们证明了其在提高计算效率方面的有效性。",
    "en_tdlr": "This paper presents a methodology for selecting scaling factors for FP8 linear layers, based on dynamically updating the per-tensor scales for weights, gradients, and activations. By applying this methodology to training and validating large language models, it demonstrates its effectiveness in improving computational efficiency."
}