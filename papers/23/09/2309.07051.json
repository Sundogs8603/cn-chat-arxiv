{
    "title": "UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons. (arXiv:2309.07051v1 [cs.HC])",
    "abstract": "The automatic co-speech gesture generation draws much attention in computer animation. Previous works designed network structures on individual datasets, which resulted in a lack of data volume and generalizability across different motion capture standards. In addition, it is a challenging task due to the weak correlation between speech and gestures. To address these problems, we present UnifiedGesture, a novel diffusion model-based speech-driven gesture synthesis approach, trained on multiple gesture datasets with different skeletons. Specifically, we first present a retargeting network to learn latent homeomorphic graphs for different motion capture standards, unifying the representations of various gestures while extending the dataset. We then capture the correlation between speech and gestures based on a diffusion model architecture using cross-local attention and self-attention to generate better speech-matched and realistic gestures. To further align speech and gesture and increa",
    "link": "http://arxiv.org/abs/2309.07051",
    "context": "Title: UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons. (arXiv:2309.07051v1 [cs.HC])\nAbstract: The automatic co-speech gesture generation draws much attention in computer animation. Previous works designed network structures on individual datasets, which resulted in a lack of data volume and generalizability across different motion capture standards. In addition, it is a challenging task due to the weak correlation between speech and gestures. To address these problems, we present UnifiedGesture, a novel diffusion model-based speech-driven gesture synthesis approach, trained on multiple gesture datasets with different skeletons. Specifically, we first present a retargeting network to learn latent homeomorphic graphs for different motion capture standards, unifying the representations of various gestures while extending the dataset. We then capture the correlation between speech and gestures based on a diffusion model architecture using cross-local attention and self-attention to generate better speech-matched and realistic gestures. To further align speech and gesture and increa",
    "path": "papers/23/09/2309.07051.json",
    "total_tokens": 924,
    "translated_title": "UnifiedGesture: 多个骨架的统一手势合成模型",
    "translated_abstract": "自动共语手势生成在计算机动画中引起了很大关注。先前的工作设计了个别数据集上的网络结构，导致了数据量的不足和在不同动作捕捉标准之间的泛化能力不强。此外，由于语言和手势之间的弱相关性，这是一个具有挑战性的任务。为了解决这些问题，我们提出了UnifiedGesture，一种基于扩散模型的语音驱动手势合成方法，该方法在具有不同骨架的多个手势数据集上进行训练。具体来说，我们首先提出一个重新定位网络，学习不同动作捕捉标准的潜在同胚图，统一各种手势的表示并扩展数据集。然后，我们基于扩散模型架构捕捉语音和手势之间的相关性，使用跨局部注意力和自注意力生成更好的与语音匹配和更加逼真的手势。为了进一步对齐语音和手势并增加两者的一致性，我们引入了一个新的多层注意力机制以及一个姿势稳定化模块。",
    "tldr": "UnifiedGesture 是一种训练在多个具有不同骨架的手势数据集上的基于扩散模型的语音驱动手势合成方法，通过重新定位网络和扩散模型架构来统一手势表示并捕捉语音和手势之间的相关性。",
    "en_tdlr": "UnifiedGesture is a speech-driven gesture synthesis approach based on diffusion model, trained on multiple gesture datasets with different skeletons, aiming to unify gesture representation and capture the correlation between speech and gestures."
}