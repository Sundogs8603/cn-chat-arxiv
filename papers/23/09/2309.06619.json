{
    "title": "RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])",
    "abstract": "Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM",
    "link": "http://arxiv.org/abs/2309.06619",
    "context": "Title: RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])\nAbstract: Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM",
    "path": "papers/23/09/2309.06619.json",
    "total_tokens": 904,
    "translated_title": "RT-LM: 面向实时推理的语言模型不确定性感知资源管理",
    "translated_abstract": "最近语言模型（LMs）的进展引起了人们的广泛关注，因为它们具备生成类似人类回应的能力。尽管在对话AI等各种应用中展示了良好的前景，但由于计算成本极高且推理延迟无法预测，这些LMs在各种设备上的部署面临挑战。由于语言的本质导致的不确定性引发的不同推理延迟可能导致计算效率不高，从而降低LMs的整体性能，特别是在高流量的工作负载下。不幸的是，这些不确定性源的带宽非常广泛，给延迟的预测和由此产生的效果带来了复杂性。为了了解和减轻不确定性对实时响应需求系统的影响，我们首先要理解、量化和优化LMs中这些不确定性导致的延迟性能变化。具体而言，我们提出了RT-LM",
    "tldr": "这篇论文介绍了RT-LM，它是一种针对语言模型的实时推理进行优化的不确定性感知资源管理方法。该方法能够理解、量化和优化由于语言的不确定性而导致的推理延迟性能变化，以提高LMs的整体性能。",
    "en_tdlr": "This paper presents RT-LM, an uncertainty-aware resource management method for optimizing real-time inference of language models. It aims to understand, quantify, and optimize the performance variations in latency induced by uncertainty in language, to improve the overall performance of LMs."
}