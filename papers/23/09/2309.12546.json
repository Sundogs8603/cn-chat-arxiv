{
    "title": "Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])",
    "abstract": "Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves stat",
    "link": "http://arxiv.org/abs/2309.12546",
    "context": "Title: Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])\nAbstract: Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves stat",
    "path": "papers/23/09/2309.12546.json",
    "total_tokens": 904,
    "translated_title": "问句生成的自动回答可行性评估",
    "translated_abstract": "传统的自动评估指标，如BLEU和ROUGE，是为自然语言生成（NLG）任务开发的，它们基于生成文本与参考文本之间的n-gram重叠度来衡量。这些简单的评估指标可能对于更复杂的任务，比如问句生成（QG），是不足够的，因为QG需要生成可以由参考答案回答的问题。因此，开发更复杂的自动评估指标仍然是QG研究中的一个紧迫问题。本文提出了一种基于提示的可回答性度量（PMAN），一种新颖的自动评估指标，用于评估问句生成任务中生成的问题是否可以由参考答案回答。大量实验证明，该评估结果可靠，并与人工评价一致。我们进一步应用我们的指标来评估QG模型的性能，结果显示我们的指标补充了传统的指标。我们基于ChatGPT的QG模型的实现取得了令人满意的成绩。",
    "tldr": "这项工作提出了一种新颖的自动评估指标，用于评估问句生成任务中生成的问题是否可以由参考答案回答。实验证明该指标结果可靠，并与人工评价一致。并且这个指标可以补充传统的指标。",
    "en_tdlr": "This work proposes a novel automatic evaluation metric to assess whether the generated questions in question generation tasks are answerable by the reference answers. Experimental results show that the metric is reliable and aligns with human evaluations. Furthermore, the metric complements conventional metrics."
}