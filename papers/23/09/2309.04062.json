{
    "title": "3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])",
    "abstract": "Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica",
    "link": "http://arxiv.org/abs/2309.04062",
    "context": "Title: 3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])\nAbstract: Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica",
    "path": "papers/23/09/2309.04062.json",
    "total_tokens": 961,
    "translated_title": "3D去噪器是好的2D教师：通过去噪和跨模态蒸馏进行分子预训练",
    "translated_abstract": "从大量无标签数据中预训练分子表示对于分子属性预测至关重要，因为获取地面真实标签的成本很高。虽然存在各种基于2D图形的分子预训练方法，但这些方法难以在预测性能上显示出统计学上的显著提升。因此，最近的工作提出了在去噪任务下基于3D构象的预训练方法，取得了有希望的结果。然而，在下游微调过程中，使用3D构象训练的模型需要准确的先前未见过的分子原子坐标，这在大规模情况下计算成本很高。基于这一限制，我们提出了D&D，一种自监督的分子表示学习框架，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练。通过去噪和跨模态知识蒸馏，我们的方法不仅利用了从去噪中获得的知识，而且应用起来很容易。",
    "tldr": "本论文提出了一种自监督的分子表示学习框架D&D，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练，以解决从大规模无标签数据中预训练分子表示的问题。",
    "en_tdlr": "This paper proposes a self-supervised molecular representation learning framework, called D&D, which pretrains a 2D graph encoder by distilling representations from a 3D denoiser to solve the problem of pretraining molecular representations from large unlabeled data."
}