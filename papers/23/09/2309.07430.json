{
    "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])",
    "abstract": "Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and",
    "link": "http://arxiv.org/abs/2309.07430",
    "context": "Title: Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])\nAbstract: Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and",
    "path": "papers/23/09/2309.07430.json",
    "total_tokens": 944,
    "translated_title": "临床文本摘要: 大型语言模型的应用优于人类专家",
    "translated_abstract": "在临床工作中，浏览大量的文本数据并总结关键信息对临床医生的时间分配造成了很大的负担。尽管大型语言模型（LLMs）在自然语言处理（NLP）任务中展现了巨大的潜力，但它们在各种临床摘要任务中的效果尚未得到严格的检验。在本研究中，我们对八个LLMs进行了领域适应方法的实验，涵盖了六个数据集和四个不同的摘要任务：放射学报告、患者问题、病历记录和医患对话。我们进行了全面的定量评估，发现模型和适应方法之间存在权衡，并且在某些情况下，LLMs的最新进展可能不会带来改进的结果。此外，通过与六名医生进行的临床阅读者研究，我们发现最佳适应的LLM的摘要在完整性和正确性方面优于人类摘要。我们的进一步定性分析揭示了LLMs和人类在面对的共同挑战。",
    "tldr": "本研究通过对八个大型语言模型在临床摘要任务上的领域适应方法实验进行了全面的定量评估，发现最佳适应的模型的摘要在完整性和正确性方面优于人类摘要。"
}