{
    "title": "Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])",
    "abstract": "Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m",
    "link": "http://arxiv.org/abs/2309.13202",
    "context": "Title: Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])\nAbstract: Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m",
    "path": "papers/23/09/2309.13202.json",
    "total_tokens": 960,
    "translated_title": "大型语言模型和控制机制提高了生物医学摘要的文本可读性",
    "translated_abstract": "生物医学文献通常使用复杂的语言和难以理解的专业术语。因此，简化在提高公共健康素养方面起着重要作用。将自然语言处理（NLP）模型应用于自动化此类任务可以使非专业读者快速直接地获取信息。在本研究中，我们使用公开可用的用于生物医学摘要简化的数据集（PLABA）来调查最先进大型语言模型（LLMs）在生物医学摘要简化任务上的能力。应用的方法包括领域微调和基于提示的学习（PBL）在：1）编码器-解码器模型（T5、SciFive和BART）上，2）仅解码器的GPT模型（GPT-3.5和GPT-4）来自OpenAI和BioGPT，以及3）基于控制令牌机制的基于BART的模型。我们使用了一系列自动评估指标，包括BLEU、ROUGE、SARI和BERTscore，并进行了人工评估。",
    "tldr": "本研究使用大型语言模型和控制机制改善了生物医学摘要的文本可读性，具体包括领域微调和基于提示的学习方法，以及应用于编码器-解码器模型和GPT模型的控制令牌机制。"
}