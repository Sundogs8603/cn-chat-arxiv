{
    "title": "Early-Exit with Class Exclusion for Efficient Inference of Neural Networks",
    "abstract": "arXiv:2309.13443v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been successfully applied in various fields. In DNNs, a large number of multiply-accumulate (MAC) operations are required to be performed, posing critical challenges in applying them in resource-constrained platforms, e.g., edge devices. To address this challenge, in this paper, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantage of the learned features in these layers to exclude as many irrelevant classes as possible, so that later layers only have to determine the target class among the remaining classes. When only one class remains at a layer, this class is the corresponding classification result. Experimental results demonstrate the computational cost of DNNs in inference can be reduced significantly with the proposed early-exit technique. The codes can be found at https://github.com/HWAI-TUDa/Early",
    "link": "https://arxiv.org/abs/2309.13443",
    "context": "Title: Early-Exit with Class Exclusion for Efficient Inference of Neural Networks\nAbstract: arXiv:2309.13443v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been successfully applied in various fields. In DNNs, a large number of multiply-accumulate (MAC) operations are required to be performed, posing critical challenges in applying them in resource-constrained platforms, e.g., edge devices. To address this challenge, in this paper, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantage of the learned features in these layers to exclude as many irrelevant classes as possible, so that later layers only have to determine the target class among the remaining classes. When only one class remains at a layer, this class is the corresponding classification result. Experimental results demonstrate the computational cost of DNNs in inference can be reduced significantly with the proposed early-exit technique. The codes can be found at https://github.com/HWAI-TUDa/Early",
    "path": "papers/23/09/2309.13443.json",
    "total_tokens": 809,
    "translated_title": "使用类别排除的早期退出实现神经网络有效推理",
    "translated_abstract": "深度神经网络在各个领域已经被成功应用。在神经网络中，需要执行大量的乘法累加（MAC）运算，这在资源受限的平台（如边缘设备）中应用它们时会带来挑战。为了解决这个挑战，本文提出了一种基于类别的动态推理早期退出方法。我们利用这些层中学到的特征来排除尽可能多的不相关类别，使得后续层只需要在剩余类别中确定目标类别。当在某一层中仅剩下一个类别时，这个类别就是相应的分类结果。实验结果表明，提出的早期退出技术可以显著降低DNN在推理中的计算成本。",
    "tldr": "提出了一种基于类别排除的动态推理早期退出方法，通过在中间层利用学到的特征排除大量不相关类别，从而显著降低神经网络推理的计算成本。",
    "en_tdlr": "Introduced a class-based early-exit method for dynamic inference, which significantly reduces the computational cost of neural network inference by excluding a large number of irrelevant classes using learned features in intermediate layers."
}