{
    "title": "Towards Tuning-Free Minimum-Volume Nonnegative Matrix Factorization. (arXiv:2309.13733v1 [stat.ML])",
    "abstract": "Nonnegative Matrix Factorization (NMF) is a versatile and powerful tool for discovering latent structures in data matrices, with many variations proposed in the literature. Recently, Leplat et al.\\@ (2019) introduced a minimum-volume NMF for the identifiable recovery of rank-deficient matrices in the presence of noise. The performance of their formulation, however, requires the selection of a tuning parameter whose optimal value depends on the unknown noise level. In this work, we propose an alternative formulation of minimum-volume NMF inspired by the square-root lasso and its tuning-free properties. Our formulation also requires the selection of a tuning parameter, but its optimal value does not depend on the noise level. To fit our NMF model, we propose a majorization-minimization (MM) algorithm that comes with global convergence guarantees. We show empirically that the optimal choice of our tuning parameter is insensitive to the noise level in the data.",
    "link": "http://arxiv.org/abs/2309.13733",
    "context": "Title: Towards Tuning-Free Minimum-Volume Nonnegative Matrix Factorization. (arXiv:2309.13733v1 [stat.ML])\nAbstract: Nonnegative Matrix Factorization (NMF) is a versatile and powerful tool for discovering latent structures in data matrices, with many variations proposed in the literature. Recently, Leplat et al.\\@ (2019) introduced a minimum-volume NMF for the identifiable recovery of rank-deficient matrices in the presence of noise. The performance of their formulation, however, requires the selection of a tuning parameter whose optimal value depends on the unknown noise level. In this work, we propose an alternative formulation of minimum-volume NMF inspired by the square-root lasso and its tuning-free properties. Our formulation also requires the selection of a tuning parameter, but its optimal value does not depend on the noise level. To fit our NMF model, we propose a majorization-minimization (MM) algorithm that comes with global convergence guarantees. We show empirically that the optimal choice of our tuning parameter is insensitive to the noise level in the data.",
    "path": "papers/23/09/2309.13733.json",
    "total_tokens": 927,
    "translated_title": "无需调参的最小体积非负矩阵分解",
    "translated_abstract": "非负矩阵分解（NMF）是一种多功能且强大的工具，用于发现数据矩阵中的潜在结构，在文献中提出了许多变体。最近，Leplat等人（2019年）引入了最小体积NMF，用于在存在噪声的情况下可识别地恢复秩缺失矩阵。然而，他们的模型性能需要选择一个调节参数，其最优值依赖于未知的噪声水平。在这项工作中，我们提出了一种受平方根套索启发并具有无需调参特性的最小体积NMF替代式。我们的替代式也需要选择一个调节参数，但其最优值不取决于噪声水平。为了拟合我们的NMF模型，我们提出了一种带有全局收敛性保证的主导最小化（MM）算法。我们通过实验证明，我们调节参数的最优选择对数据中的噪声水平不敏感。",
    "tldr": "本论文提出了一个无需调参的最小体积非负矩阵分解方法，通过引入平方根套索的启发和无需调参的特性，解决了既有方法在选择调节参数方面的依赖性问题。通过实验证明，该方法对数据中的噪声水平不敏感。",
    "en_tdlr": "This paper presents a tuning-free minimum-volume nonnegative matrix factorization method, which addresses the dependency issue of existing approaches in selecting tuning parameters by introducing the inspiration of square-root lasso and its tuning-free properties. Empirical results demonstrate the insensitivity of this method to the noise level in the data."
}