{
    "title": "Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])",
    "abstract": "Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in",
    "link": "http://arxiv.org/abs/2309.03157",
    "context": "Title: Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])\nAbstract: Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in",
    "path": "papers/23/09/2309.03157.json",
    "total_tokens": 986,
    "translated_title": "通过深度强化学习学习充电：无人机覆盖路径规划",
    "translated_abstract": "覆盖路径规划（CPP）是机器人学中一个关键问题，其目标是找到一个有效的路径，覆盖兴趣区域中的每一个点。本文解决了充电有限的无人机（UAV）的电力限制CPP问题。在这个问题中，将充电旅程整合到整体覆盖策略中带来了一个显著的挑战，突出了制定战略性、长期性决策的复杂任务。我们提出了一种基于近端策略优化（PPO）的深度强化学习（DRL）方法，利用基于地图的观测信息，运用动作屏蔽和折扣因子调度来优化整个任务周期内的覆盖轨迹。我们还提供了一个位置历史记录给智能体，以处理充电能力引起的新出现的状态循环。我们的方法优于基准启发式方法，在不同目标区域和地图上具有泛化性能，但对未知地图的泛化性能有限。",
    "tldr": "本论文提出了一种通过深度强化学习学习充电来解决无人机覆盖路径规划问题的方法。该方法利用基于地图的观测信息，在整个任务周期内优化覆盖轨迹，并采用动作屏蔽和折扣因子调度等技术。实验结果表明，该方法优于基准启发式方法，在不同目标区域和地图上具有一定的泛化性能。",
    "en_tdlr": "This paper proposes a method for UAV coverage path planning using deep reinforcement learning to learn the recharging process. The method utilizes map-based observations and optimizes coverage trajectories over the entire mission horizon. Experimental results show that the proposed method outperforms baseline heuristics and has limited generalization to unseen maps."
}