{
    "title": "HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])",
    "abstract": "Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting.",
    "link": "http://arxiv.org/abs/2309.12669",
    "context": "Title: HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])\nAbstract: Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting.",
    "path": "papers/23/09/2309.12669.json",
    "total_tokens": 834,
    "translated_title": "HRoT: 混合提示策略和检索思路用于表-文本混合问答",
    "translated_abstract": "在给定的表格和文本数据(TextTableQA)上回答数值问题是一个具有挑战性的任务。最近，大型语言模型在自然语言处理领域引起了重大关注。随着大型语言模型的出现，上下文学习和思路链提示成为了这个领域中特别流行的两个研究主题。在本文中，我们引入了一种称为混合提示策略和检索思路的新提示策略，用于文本和表格混合问答。通过上下文学习，我们促使模型在处理混合数据时能够具备检索思考的能力。我们的方法在少样本设置下，在MultiHiertt数据集上实现了优越的性能，超过了全部监督SOTA方法。",
    "tldr": "本文介绍了一种新的混合提示策略和检索思路，用于解决表-文本混合问答任务。该方法通过上下文学习，使模型具备处理混合数据的检索思考能力，并在MultiHiertt数据集上的少样本设置下取得了优越性能。",
    "en_tdlr": "This paper introduces a new hybrid prompt strategy and retrieval of thought for table-text hybrid question answering. By using in-context learning, the model is prompted to develop the ability of retrieval thinking when dealing with hybrid data. The method achieves superior performance in the few-shot setting on the MultiHiertt dataset compared to the fully-supervised SOTA method."
}