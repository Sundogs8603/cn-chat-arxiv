{
    "title": "Maximum Weight Entropy. (arXiv:2309.15704v1 [cs.LG])",
    "abstract": "This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in \"over-restricted\" regions of the weight space due to the use of \"over-regularization\" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks \"consistent\" with the training observations. Considering stochastic neural networks, a practical optimizati",
    "link": "http://arxiv.org/abs/2309.15704",
    "context": "Title: Maximum Weight Entropy. (arXiv:2309.15704v1 [cs.LG])\nAbstract: This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in \"over-restricted\" regions of the weight space due to the use of \"over-regularization\" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks \"consistent\" with the training observations. Considering stochastic neural networks, a practical optimizati",
    "path": "papers/23/09/2309.15704.json",
    "total_tokens": 781,
    "translated_title": "最大权重熵",
    "translated_abstract": "本文研究了在深度学习中使用贝叶斯和集成方法进行不确定性量化和超出分布检测的问题。当在超出分布的情况下使用标准方法时，我们观察到预测多样性的缺乏。针对这个问题，本文提出了一个实用的解决方案，认为标准方法在权重空间中采样的“过度约束”导致了权重多样性的缺乏。本文建议采用最大熵原理来解决这个问题，通过最大化权重多样性，描述最大熵权重分布来表示认知不确定性，从而产生与训练观察一致的神经网络。",
    "tldr": "本文提出了在深度学习中使用最大熵原理的最大权重熵方法，通过最大化权重多样性来解决标准方法在超出分布情况下预测多样性缺乏的问题。",
    "en_tdlr": "This paper proposes the Maximum Weight Entropy method using the maximum entropy principle in deep learning to address the lack of prediction diversity in standard methods when dealing with out-of-distribution scenarios. By maximizing weight diversity, the proposed method tackles the issue and produces neural networks consistent with training observations."
}