{
    "title": "VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])",
    "abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond \"tabular\" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled ima",
    "link": "http://arxiv.org/abs/2309.06703",
    "context": "Title: VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])\nAbstract: Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond \"tabular\" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled ima",
    "path": "papers/23/09/2309.06703.json",
    "total_tokens": 834,
    "translated_title": "VLSlice：交互式视觉和语言切片发现",
    "translated_abstract": "最近的视觉和语言研究表明，大规模预训练可以学习出具有通用性的模型，可以有效地迁移到下游任务。尽管这可能改善数据集规模的聚合指标，但通过分析针对特定偏差维度的手工子组时，发现了系统性的不良行为。然而，这种子组分析通常会因为注释工作而停滞，而收集所需数据需要大量的时间和资源。先前的方法尝试自动发现子组以规避这些限制，但通常利用现有任务特定注释上的模型行为，并在超出“表格”数据的更复杂输入上快速降级，其中没有研究视觉和语言模型。本文介绍了VLSlice，一种交互式系统，可以通过用户引导发现一致的表示级子组，具有一致的视觉语言行为，被称为视觉和语言切片，从未标记的图像中获取。",
    "tldr": "这项工作提出了一种交互式系统VLSlice，可以通过用户引导发现一致的视觉和语言行为的表示级子组，以解决自动发现子组时的困难。",
    "en_tdlr": "This work presents VLSlice, an interactive system that enables user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, addressing the challenges of automating subgroup discovery."
}