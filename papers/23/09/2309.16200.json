{
    "title": "Max-Sliced Mutual Information. (arXiv:2309.16200v1 [cs.LG])",
    "abstract": "Quantifying the dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast comput",
    "link": "http://arxiv.org/abs/2309.16200",
    "context": "Title: Max-Sliced Mutual Information. (arXiv:2309.16200v1 [cs.LG])\nAbstract: Quantifying the dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast comput",
    "path": "papers/23/09/2309.16200.json",
    "total_tokens": 850,
    "translated_title": "最大切片互信息",
    "translated_abstract": "量化高维随机变量之间的依赖关系对于统计学习和推断至关重要。两种传统方法是标准相关分析（CCA），其识别出原始变量的最大相关投影版本，以及香农的互信息，它是一种通用的依赖度量，也能捕捉高阶依赖关系。然而，CCA仅考虑线性依赖关系，这在某些应用中可能不足够，而互信息在高维情况下通常难以计算/估计。本研究提出了一种折衷方案，即基于信息论的可扩展方式，称为最大切片互信息（mSMI）。mSMI等于高维变量的低维投影之间的最大互信息，而在高斯情况下减少到CCA。它兼具两者的优点：捕捉数据中的复杂依赖关系，同时适用于快速计算。",
    "tldr": "本论文提出了一种新的方法，最大切片互信息（mSMI），来量化高维随机变量之间的依赖关系。mSMI在捕捉复杂依赖关系的同时也适用于快速计算。"
}