{
    "title": "The fine print on tempered posteriors. (arXiv:2309.05292v1 [cs.LG])",
    "abstract": "We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.",
    "link": "http://arxiv.org/abs/2309.05292",
    "context": "Title: The fine print on tempered posteriors. (arXiv:2309.05292v1 [cs.LG])\nAbstract: We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.",
    "path": "papers/23/09/2309.05292.json",
    "total_tokens": 968,
    "translated_title": "深入研究混杂后验参数问题",
    "translated_abstract": "我们对混杂后验参数进行了详细调查，发现了一些关键而以前未被讨论的问题。与以往的结果相反，我们首先证明，在实际模型和数据集以及对后验的紧密控制的Laplace近似情况下，随机性通常并不能提高测试准确性。最低的温度通常是最优的。人们可能会认为，具有一定随机性的贝叶斯模型至少能在校准方面取得改进。然而，我们通过实验证明，当获得增益时，这是以测试准确性的降低为代价的。然后，我们讨论了使用贝叶斯模型以目标频率主义指标提供对优化目标中温度参数λ的简单解释的问题。与之前的研究相反，我们最终通过PAC-Bayesian分析表明，温度λ不能简单地被视为修正了先验或似然的错误设定。",
    "tldr": "这项研究深入探讨了混杂后验参数的问题，发现了一些重要的新观点。研究结果表明，在实际情况下，随机性通常不能提高测试准确性，并且对于某些贝叶斯模型，增加随机性反而会降低测试准确性。此外，研究还解释了优化目标中温度参数的重要性，并指出温度不能简单地被视为修正了先验或似然的错误设定。",
    "en_tdlr": "This study provides a detailed investigation of tempered posteriors and reveals important findings. The research shows that stochasticity usually does not improve test accuracy and can even degrade it for certain Bayesian models. Additionally, it explains the significance of the temperature parameter in the optimization objective and highlights that it cannot simply be seen as fixing misspecified prior or likelihood functions."
}