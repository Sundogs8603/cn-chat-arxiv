{
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])",
    "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W",
    "link": "http://arxiv.org/abs/2309.15505",
    "context": "Title: Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])\nAbstract: We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W",
    "path": "papers/23/09/2309.15505.json",
    "total_tokens": 990,
    "translated_title": "有限标量量化: 简化 VQ-VAE 方法",
    "translated_abstract": "我们提出用有限标量量化 (FSQ) 替代 VQ-VAE 潜在表示中的向量量化 (VQ)。在 FSQ 中，我们将 VAE 表示投影到几个维度 (通常少于10个)，每个维度被量化为一组固定的值，从而形成一个（隐式的）码本，由这些值的乘积组成。通过合适地选择维度和每个维度可以取的值的数量，我们获得与 VQ 中相同的码本大小。在这样的离散表示上，我们可以训练已经在 VQ-VAE 表示上训练过的相同模型，例如用于图像生成、多模态生成和密集预测计算机视觉任务的自回归和掩码变换器模型。具体而言，我们在图像生成中使用 FSQ 和 MaskGIT，在深度估计、着色和全景分割中使用 FSQ 和 UViM。尽管 FSQ 的设计要简单得多，我们在所有这些任务中获得了有竞争力的性能。",
    "tldr": "该论文提出了有限标量量化 (FSQ) 方法，用来简化 VQ-VAE 方法中的向量量化 (VQ)。通过投影和量化 VAE 表示，我们得到与 VQ 相同大小的码本。在这种离散表示上，我们可以训练相同的模型，并在图像生成、多模态生成和计算机视觉任务中取得竞争性能。",
    "en_tdlr": "This paper proposes a method called Finite Scalar Quantization (FSQ) to simplify the vector quantization (VQ) in VQ-VAE. By projecting and quantizing the VAE representation, we obtain the same codebook size as VQ. With this discrete representation, we can train the same models and achieve competitive performance in tasks such as image generation and multimodal generation in computer vision."
}