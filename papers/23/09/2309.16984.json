{
    "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])",
    "abstract": "Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.",
    "link": "http://arxiv.org/abs/2309.16984",
    "context": "Title: Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])\nAbstract: Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.",
    "path": "papers/23/09/2309.16984.json",
    "total_tokens": 873,
    "translated_title": "一种作为丰富高效的策略类别的一致性模型在强化学习中的应用",
    "translated_abstract": "基于分数的生成模型（如扩散模型）在建模多模态数据方面被证明是有效的，从图像生成到强化学习（RL）。然而，扩散模型的推理过程可能会很慢，这阻碍了它在具有迭代采样的RL中的使用。我们提出使用一致性模型作为一种高效且表达力强的策略表示，即一致性策略，并结合演员-评论家风格的算法将其应用于三种典型的RL设置：离线、离线到在线和在线。对于离线RL，我们展示了生成模型作为多模态数据中的策略的表达能力。对于离线到在线RL，一致性策略显示出比扩散策略更高的计算效率，并且性能可比。对于在线RL，一致性策略显示出显著的加速效果，甚至比扩散策略具有更高的平均性能。",
    "tldr": "这篇论文介绍了一种基于一致性模型的策略表示方法，在强化学习中具有高效且表达力强的特点。实验表明，一致性策略在各种RL设置中都具有良好的性能表现。"
}