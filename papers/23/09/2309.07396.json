{
    "title": "DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])",
    "abstract": "Several prior studies have suggested that word frequency biases can cause the Bert model to learn indistinguishable sentence embeddings. Contrastive learning schemes such as SimCSE and ConSERT have already been adopted successfully in unsupervised sentence embedding to improve the quality of embeddings by reducing this bias. However, these methods still introduce new biases such as sentence length bias and false negative sample bias, that hinders model's ability to learn more fine-grained semantics. In this paper, we reexamine the challenges of contrastive sentence embedding learning from a debiasing perspective and argue that effectively eliminating the influence of various biases is crucial for learning high-quality sentence embeddings. We think all those biases are introduced by simple rules for constructing training data in contrastive learning and the key for contrastive learning sentence embedding is to mimic the distribution of training data in supervised machine learning in uns",
    "link": "http://arxiv.org/abs/2309.07396",
    "context": "Title: DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])\nAbstract: Several prior studies have suggested that word frequency biases can cause the Bert model to learn indistinguishable sentence embeddings. Contrastive learning schemes such as SimCSE and ConSERT have already been adopted successfully in unsupervised sentence embedding to improve the quality of embeddings by reducing this bias. However, these methods still introduce new biases such as sentence length bias and false negative sample bias, that hinders model's ability to learn more fine-grained semantics. In this paper, we reexamine the challenges of contrastive sentence embedding learning from a debiasing perspective and argue that effectively eliminating the influence of various biases is crucial for learning high-quality sentence embeddings. We think all those biases are introduced by simple rules for constructing training data in contrastive learning and the key for contrastive learning sentence embedding is to mimic the distribution of training data in supervised machine learning in uns",
    "path": "papers/23/09/2309.07396.json",
    "total_tokens": 934,
    "translated_title": "DebCSE: 以去偏见的角度重新思考无监督对比句子嵌入学习",
    "translated_abstract": "先前的研究指出，词频偏差会导致Bert模型学习到无法区分的句子嵌入。一些对比学习方案，如SimCSE和ConSERT，已成功用于改善无监督句子嵌入的质量，减少偏差。然而，这些方法仍会引入新的偏差，如句子长度偏差和假负样本偏差，这些偏差阻碍了模型学习更精细的语义。本文从去偏见的角度重新审视对比句子嵌入学习的挑战，并认为有效消除各种偏差对于学习高质量句子嵌入至关重要。我们认为，所有这些偏差都是由于对比学习中构建训练数据的简单规则引入的，对比学习句子嵌入的关键是在无监督机器学习中模拟训练数据的分布。",
    "tldr": "本文重新思考了无监督对比句子嵌入学习，并从去偏见的角度提出了DebCSE方法。通过消除各种偏差，包括词频偏差、句子长度偏差和假负样本偏差，DebCSE旨在学习高质量的句子嵌入。"
}