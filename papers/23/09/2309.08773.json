{
    "title": "Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])",
    "abstract": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that",
    "link": "http://arxiv.org/abs/2309.08773",
    "context": "Title: Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])\nAbstract: This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that",
    "path": "papers/23/09/2309.08773.json",
    "total_tokens": 824,
    "translated_title": "增强音频生成可控性的方法：通过表示相似性正则化",
    "translated_abstract": "本文提出了一种创新的方法，通过在模型训练期间强调音频和文本表示之间的对齐，增强对音频生成的控制能力。在基于语言模型的音频生成中，模型利用来自文本和音频标记表示的输入来预测后续的音频标记。然而，当前的配置缺乏明确的正则化来确保所选择的文本表示与语言模型的预测之间的对齐。我们的提议涉及音频和文本表示正则化的整合，特别是在无分类器引导（CFG）阶段，在语言模型训练过程中，文本条件被排除在跨注意力之外。我们提出的表示正则化的目的是最小化与同一训练批次中其他样本的音频和文本相似性差异。在音乐和音频生成任务上的实验结果表明，",
    "tldr": "这篇论文介绍了一种创新的方法，通过在训练过程中强调音频和文本表示之间的对齐来增强音频生成的可控性。实验结果表明，在音乐和音频生成任务中，这种方法取得了良好的效果。",
    "en_tdlr": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. Experimental results demonstrate that this method achieves good performance in music and audio generation tasks."
}