{
    "title": "Neural Koopman prior for data assimilation",
    "abstract": "arXiv:2309.05317v2 Announce Type: replace  Abstract: With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show",
    "link": "https://arxiv.org/abs/2309.05317",
    "context": "Title: Neural Koopman prior for data assimilation\nAbstract: arXiv:2309.05317v2 Announce Type: replace  Abstract: With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show",
    "path": "papers/23/09/2309.05317.json",
    "total_tokens": 856,
    "translated_title": "数据同化的神经Koopman先验",
    "translated_abstract": "随着大规模数据集、计算能力和诸如自动微分和表达能力强的神经网络架构等工具的日益普及，序贯数据现在经常以数据驱动的方式处理，通过从观测数据训练动力学模型。尽管神经网络经常被视为不可解释的黑盒架构，但它们仍然可以受益于数据的物理先验和数学知识。在本文中，我们使用了一种神经网络架构，该架构利用长期以来已知的Koopman算子理论，将动力系统嵌入到潜在空间中，其中它们的动态可以被线性描述，从而呈现出许多吸引人的特性。我们引入了使该模型能够进行长期连续重构的方法，即使在数据呈不规则采样时间序列的困难情境中也可以顺利进行。我们还展示了自监督学习的潜力，因为我们展示",
    "tldr": "本文介绍了一种利用神经Koopman先验进行数据同化的方法，将动态系统嵌入到潜在空间，使得对其动态进行线性描述，并展示了长期连续重构和自监督学习的潜力。",
    "en_tdlr": "This paper presents a method for data assimilation using neural Koopman prior to embed dynamical systems in latent spaces for linear dynamics description, showcasing the potential for long-term continuous reconstruction and self-supervised learning."
}