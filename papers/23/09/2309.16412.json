{
    "title": "Selective Nonparametric Regression via Testing. (arXiv:2309.16412v1 [stat.ML])",
    "abstract": "Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.",
    "link": "http://arxiv.org/abs/2309.16412",
    "context": "Title: Selective Nonparametric Regression via Testing. (arXiv:2309.16412v1 [stat.ML])\nAbstract: Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.",
    "path": "papers/23/09/2309.16412.json",
    "total_tokens": 793,
    "translated_title": "通过检验进行选择性非参数回归",
    "translated_abstract": "在具有误差敏感的机器学习应用中，预测中的放弃可能性（或选择性预测）是一个重要问题。虽然分类设置中得到了广泛研究，但对于回归问题的选择性方法发展较少。在这项工作中，我们考虑非参数异方差回归问题，并通过检验给定点处条件方差的假设来开发一个放弃程序。与现有方法不同，提出的方法不仅允许考虑方差本身的值，还允许考虑对应方差预测器的不确定性。我们证明了所得估计器的风险的非渐近界，并展示了几种不同收敛模式的存在。理论分析与一系列在模拟和真实数据上的实验一起进行。",
    "tldr": "通过检验给定条件方差的假设，我们开发了一种选择性非参数回归方法，允许考虑方差本身的值以及对应方差预测器的不确定性，并证明了估计器的风险的非渐近界。",
    "en_tdlr": "We develop a selective nonparametric regression method by testing the hypothesis on the conditional variance, allowing for consideration of the value of the variance itself as well as the uncertainty of the variance predictor, and provide non-asymptotic bounds on the risk of the resulting estimator."
}