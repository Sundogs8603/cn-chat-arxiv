{
    "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])",
    "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying mod",
    "link": "http://arxiv.org/abs/2309.11674",
    "context": "Title: A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])\nAbstract: Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying mod",
    "path": "papers/23/09/2309.11674.json",
    "total_tokens": 897,
    "translated_title": "机器翻译中的范式转变：提升大型语言模型的翻译性能",
    "translated_abstract": "生成式大型语言模型(LLMs)在各种NLP任务中取得了显著进展。然而，在翻译任务中，尤其是那些具有适度模型大小（即7B或13B参数）的任务，这些进展尚未得到反映，仍然落后于传统的有监督编码器-解码器翻译模型。先前的研究试图改善这些适度LLMs的翻译能力，但其增益受到限制。在本研究中，我们提出了一种新颖的LLMs微调方法，专为翻译任务而设计，消除了传统翻译模型通常依赖大量平行数据的需求。我们的方法包括两个微调阶段：在单语数据上进行初始微调，然后在一小部分高质量平行数据上进行后续微调。我们通过这种策略开发的LLM被称为基于先进语言模型的翻译器(ALMA)。",
    "tldr": "本论文提出了一种基于先进语言模型的翻译器(ALMA)的微调方法，可以提升大型语言模型在翻译任务上的性能，消除了对大量平行数据的依赖。",
    "en_tdlr": "This paper proposes a fine-tuning approach for advanced language models (ALMA) in machine translation, which improves the translation performance of large language models and eliminates the need for abundant parallel data."
}