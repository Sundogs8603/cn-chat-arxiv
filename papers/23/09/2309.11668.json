{
    "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])",
    "abstract": "Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effec",
    "link": "http://arxiv.org/abs/2309.11668",
    "context": "Title: Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])\nAbstract: Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effec",
    "path": "papers/23/09/2309.11668.json",
    "total_tokens": 981,
    "translated_title": "面向大型语言模型的机器翻译消歧效果研究",
    "translated_abstract": "在机器翻译领域，解决语义歧义一直被认为是一个核心挑战。最近在歧义句子的翻译性能基准测试中，传统神经机器翻译系统的局限性暴露出来，无法捕捉到其中许多情况。大型语言模型(LLMs)已经成为一个有希望的替代方案，表现出与传统NMT模型相当的性能，并引入了控制目标输出的新范式。本文研究了LLMs在翻译包含多义词和稀有词义的歧义句子方面的能力，并提出了两种通过上下文学习和精心策划的歧义数据集微调来改进处理此类歧义的方法。实验证明，我们的方法在五个语言方向中有四个方向能够与DeepL和NLLB等最先进系统匹敌甚至超越。我们的研究为 effective disambiguation for machine translation 提供了有价值的见解。",
    "tldr": "本文研究了大型语言模型(LLMs)在翻译歧义句子方面的能力，并通过上下文学习和歧义数据集微调提出了改进处理歧义的方法。实验证明，这些方法在多个语言方向上有着与最先进系统相当甚至超越的表现。这些研究为机器翻译的有效消歧提供了宝贵的见解。",
    "en_tdlr": "This paper investigates the capability of large language models (LLMs) in translating ambiguous sentences and proposes methods for improving the handling of ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. The experiments show that our methods can match or outperform state-of-the-art systems in multiple language directions, providing valuable insights into effective disambiguation for machine translation."
}