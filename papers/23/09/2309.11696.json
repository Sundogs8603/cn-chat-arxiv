{
    "title": "Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su",
    "link": "http://arxiv.org/abs/2309.11696",
    "context": "Title: Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])\nAbstract: Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su",
    "path": "papers/23/09/2309.11696.json",
    "total_tokens": 868,
    "translated_title": "带有短期和长期记忆协作的增强记忆型LLM个性化",
    "translated_abstract": "大型语言模型（LLMs），如GPT3.5，在理解和生成自然语言方面表现出卓越的能力。然而，它们的非个性化生成方式可能导致用户特定结果的亚优化。通常，用户根据自己的知识和偏好以不同的方式进行对话。这就需要增强面向用户的LLM的任务，但这方面的研究尚未深入探索。之前的研究已经探索了基于记忆的方法来存储和检索知识，以增强生成而无需为新的查询进行重新训练。然而，我们认为仅仅使用记忆模块无法理解用户的偏好，并且完全训练一个LLM可能成本过高。在本研究中，我们提出了一种新颖的计算仿生记忆机制，配备了一个参数高效的微调模式，用于个性化LLMs。我们广泛的实验结果证明了该方法的有效性和可行性。",
    "tldr": "本研究提出了一种计算仿生记忆机制，配备了参数高效的微调模式，用于个性化LLMs。实验证明了该方法的有效性和可行性。"
}