{
    "title": "On the training and generalization of deep operator networks. (arXiv:2309.01020v1 [math.NA])",
    "abstract": "We present a novel training method for deep operator networks (DeepONets), one of the most popular neural network models for operators. DeepONets are constructed by two sub-networks, namely the branch and trunk networks. Typically, the two sub-networks are trained simultaneously, which amounts to solving a complex optimization problem in a high dimensional space. In addition, the nonconvex and nonlinear nature makes training very challenging. To tackle such a challenge, we propose a two-step training method that trains the trunk network first and then sequentially trains the branch network. The core mechanism is motivated by the divide-and-conquer paradigm and is the decomposition of the entire complex training task into two subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization process is introduced which significantly improves stability and generalization ability. On the theoretical side, we establish a generalization error estimate in terms of the number of tr",
    "link": "http://arxiv.org/abs/2309.01020",
    "context": "Title: On the training and generalization of deep operator networks. (arXiv:2309.01020v1 [math.NA])\nAbstract: We present a novel training method for deep operator networks (DeepONets), one of the most popular neural network models for operators. DeepONets are constructed by two sub-networks, namely the branch and trunk networks. Typically, the two sub-networks are trained simultaneously, which amounts to solving a complex optimization problem in a high dimensional space. In addition, the nonconvex and nonlinear nature makes training very challenging. To tackle such a challenge, we propose a two-step training method that trains the trunk network first and then sequentially trains the branch network. The core mechanism is motivated by the divide-and-conquer paradigm and is the decomposition of the entire complex training task into two subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization process is introduced which significantly improves stability and generalization ability. On the theoretical side, we establish a generalization error estimate in terms of the number of tr",
    "path": "papers/23/09/2309.01020.json",
    "total_tokens": 888,
    "translated_title": "关于深度运算符网络的训练和泛化性",
    "translated_abstract": "我们提出了一种新的训练方法，用于深度运算符网络（DeepONets），这是一种最流行的用于运算符的神经网络模型。DeepONets由两个子网络构成，分别是分支网络和主干网络。通常情况下，这两个子网络同时进行训练，这相当于在高维空间中解决一个复杂的优化问题。此外，非凸和非线性性质使得训练非常具有挑战性。为了克服这种挑战，我们提出了一种两步训练方法，首先训练主干网络，然后顺序训练分支网络。核心机制受到分而治之的启发，将整个复杂训练任务分解为两个具有降低复杂性的子任务。其中引入了格拉姆-施密特正交化过程，显著提高了稳定性和泛化能力。在理论方面，我们建立了一个关于训练样本数的泛化误差估计。",
    "tldr": "我们提出了一种用于深度运算符网络的新的训练方法，通过将训练任务分解为两个降低复杂性的子任务，并引入正交化过程来提高网络的稳定性和泛化能力。",
    "en_tdlr": "We propose a novel training method for deep operator networks (DeepONets) that improves stability and generalization ability by decomposing the training task into two subtasks of reduced complexity and introducing an orthonormalization process."
}