{
    "title": "Can I Trust Your Answer? Visually Grounded Video Question Answering",
    "abstract": "arXiv:2309.01327v2 Announce Type: replace-cross  Abstract: We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a g",
    "link": "https://arxiv.org/abs/2309.01327",
    "context": "Title: Can I Trust Your Answer? Visually Grounded Video Question Answering\nAbstract: arXiv:2309.01327v2 Announce Type: replace-cross  Abstract: We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a g",
    "path": "papers/23/09/2309.01327.json",
    "total_tokens": 853,
    "translated_title": "我能相信你的回答吗？基于视觉的视频问答",
    "translated_abstract": "我们研究了基于视觉的视频问答，以应对利用预训练技术进行视频语言理解的新趋势。具体来说，通过迫使视觉语言模型（VLMs）回答问题并同时提供视觉证据，我们旨在确定这些技术的预测在多大程度上真正基于相关视频内容，而不是来自语言或无关视觉上下文的虚假相关性。为此，我们构建了NExT-GQA--一个带有10.5K时间定位（或位置）标签与原始QA对相关联的NExT-QA扩展。通过NExT-GQA，我们审查了一系列最先进的VLMs。通过事后注意力分析，我们发现这些模型在证实答案方面非常薄弱，尽管它们的QA性能强劲。这暴露了当前VLM在做出可靠预测方面的局限性。为此，我们进一步探讨并提出了一个解决方案",
    "tldr": "通过研究基于视觉的视频问答，发现当前视觉语言模型在做出可靠预测方面存在局限性，并提出了一种改进方法。",
    "en_tdlr": "The study on visually grounded video question answering reveals limitations in current vision-language models in making reliable predictions and proposes an improvement approach."
}