{
    "title": "Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation. (arXiv:2309.07624v1 [cs.CL])",
    "abstract": "Despite the success of neural models in solving reasoning tasks, their compositional generalization capabilities remain unclear. In this work, we propose a new setting of the structured explanation generation task to facilitate compositional reasoning research. Previous works found that symbolic methods achieve superior compositionality by using pre-defined inference rules for iterative reasoning. But these approaches rely on brittle symbolic transfers and are restricted to well-defined tasks. Hence, we propose a dynamic modularized reasoning model, MORSE, to improve the compositional generalization of neural models. MORSE factorizes the inference process into a combination of modules, where each module represents a functional unit. Specifically, we adopt modularized self-attention to dynamically select and route inputs to dedicated heads, which specializes them to specific functions. We conduct experiments for increasing lengths and shapes of reasoning trees on two benchmarks to test ",
    "link": "http://arxiv.org/abs/2309.07624",
    "context": "Title: Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation. (arXiv:2309.07624v1 [cs.CL])\nAbstract: Despite the success of neural models in solving reasoning tasks, their compositional generalization capabilities remain unclear. In this work, we propose a new setting of the structured explanation generation task to facilitate compositional reasoning research. Previous works found that symbolic methods achieve superior compositionality by using pre-defined inference rules for iterative reasoning. But these approaches rely on brittle symbolic transfers and are restricted to well-defined tasks. Hence, we propose a dynamic modularized reasoning model, MORSE, to improve the compositional generalization of neural models. MORSE factorizes the inference process into a combination of modules, where each module represents a functional unit. Specifically, we adopt modularized self-attention to dynamically select and route inputs to dedicated heads, which specializes them to specific functions. We conduct experiments for increasing lengths and shapes of reasoning trees on two benchmarks to test ",
    "path": "papers/23/09/2309.07624.json",
    "total_tokens": 879,
    "translated_title": "动态模块化推理用于组合结构化解释生成",
    "translated_abstract": "尽管神经模型在解决推理任务方面取得了成功，但其组合泛化能力仍不清楚。本文提出了一种新的结构化解释生成任务设置，以促进组合推理研究。以往的研究发现，符号方法通过使用预定义的推理规则进行反复推理实现了优秀的组合性。但这些方法依赖于脆弱的符号传递，并且局限于定义明确的任务。因此，我们提出了一种动态模块化推理模型MORSE，以提高神经模型的组合推理能力。MORSE将推理过程分解为多个模块的组合，其中每个模块代表一个功能单元。具体而言，我们采用模块化的自注意力机制来动态选择和路由输入到专门的头部，使其专门针对特定函数。我们在两个基准数据集上进行了实验，测试了推理树的长度和形状的提高。",
    "tldr": "本文提出了一种动态模块化推理模型MORSE，用于改善神经模型的组合推理能力，通过采用模块化的自注意力机制，实现了对输入的动态选择和路由，提高了神经模型的组合泛化能力。",
    "en_tdlr": "This paper proposes a dynamic modularized reasoning model, MORSE, to improve the compositional generalization of neural models. By adopting modularized self-attention, MORSE achieves dynamic selection and routing of inputs, thereby enhancing the compositional generalization capability of neural models."
}