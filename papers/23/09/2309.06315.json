{
    "title": "Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning. (arXiv:2309.06315v1 [cs.LG])",
    "abstract": "A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific I",
    "link": "http://arxiv.org/abs/2309.06315",
    "context": "Title: Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning. (arXiv:2309.06315v1 [cs.LG])\nAbstract: A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific I",
    "path": "papers/23/09/2309.06315.json",
    "total_tokens": 914,
    "translated_title": "使用马尔科夫边界引导修剪学习最简化Tsetlin机器子句",
    "translated_abstract": "如果包含了预测变量所需的所有信息，那么一组变量就是随机变量的马尔科夫盖被。如果盖被无法减少而不丢失有用信息，则被称为马尔科夫边界。识别随机变量的马尔科夫边界是有优势的，因为边界外的所有变量都是多余的。因此，马尔科夫边界提供了最佳的特征集。然而，从数据中学习马尔科夫边界具有两个挑战。如果从马尔科夫边界中移除一个或多个变量，边界外的变量可能开始提供信息。相反，边界内的变量可能停止提供信息。每个候选变量的真正作用只有在识别了马尔科夫边界后才会显现。在本文中，我们提出了一种新的Tsetlin机器（TM）反馈方案，以补充类型I和类型II的反馈。该方案引入了一种新颖的有限状态自动机 - 一种上下文特定的机器。",
    "tldr": "本论文提出了一种新的Tsetlin机器（TM）反馈方案，通过引入马尔科夫边界来学习最简化的Tsetlin机器子句，以提供最佳的特征集。",
    "en_tdlr": "This paper proposes a new feedback scheme for Tsetlin Machines (TM) that learns minimalistic Tsetlin Machine clauses by introducing the Markov boundary to provide an optimal feature set."
}