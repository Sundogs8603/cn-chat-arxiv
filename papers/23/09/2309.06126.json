{
    "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])",
    "abstract": "Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.",
    "link": "http://arxiv.org/abs/2309.06126",
    "context": "Title: AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])\nAbstract: Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.",
    "path": "papers/23/09/2309.06126.json",
    "total_tokens": 867,
    "translated_title": "AstroLLaMA: 面向天文学的专业基础模型",
    "translated_abstract": "大型语言模型在许多人类语言任务中表现出色，但在学术天文学等高度专业化领域往往难以胜任。为了弥合这个差距，我们介绍了AstroLLaMA，这是一个从arXiv上的超过300,000个天文学摘要中使用LLaMA-2 fine-tuned得到的70亿参数模型。AstroLLaMA针对传统因果语言建模进行了优化，其困惑度比Llama-2低30％，表现出明显的领域适应性。尽管参数明显较少，但我们的模型生成的文本完成和嵌入提取比最先进的基础模型更具洞察力和科学相关性。AstroLLaMA作为一个强大的领域特定模型，具有广泛的fine-tuning潜力。其公开发布旨在推动围绕天文学的研究，包括自动论文摘要和对话代理的开发。",
    "tldr": "AstroLLaMA是一个专门用于天文学的模型，通过从arXiv中的天文学摘要fine-tuned得到，其在因果语言建模中表现出色，生成的文本完成和嵌入提取比其他基础模型更具洞察力和科学相关性。",
    "en_tdlr": "AstroLLaMA is a specialized model for astronomy, fine-tuned from astronomy abstracts, which performs well in causal language modeling and generates text completions and embedding extraction with more insight and scientific relevance compared to other foundation models."
}