{
    "title": "Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])",
    "abstract": "Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown to achieve remarkable performance across a variety of natural language tasks. Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses. However, the high costs associated with training and implementing LLMs pose challenges to academic research. Furthermore, the availability of pretrained LLMs and instruction-tune datasets for Vietnamese language is limited. To tackle these concerns, we leverage large-scale instruction-following datasets from open-source projects, namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and specific medical domain. To the best of our knowledge, these are the first instructional dataset for Vietnamese. Subsequently, we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs: Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models: Bloomz-Chat, Blo",
    "link": "http://arxiv.org/abs/2309.04646",
    "context": "Title: Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])\nAbstract: Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown to achieve remarkable performance across a variety of natural language tasks. Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses. However, the high costs associated with training and implementing LLMs pose challenges to academic research. Furthermore, the availability of pretrained LLMs and instruction-tune datasets for Vietnamese language is limited. To tackle these concerns, we leverage large-scale instruction-following datasets from open-source projects, namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and specific medical domain. To the best of our knowledge, these are the first instructional dataset for Vietnamese. Subsequently, we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs: Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models: Bloomz-Chat, Blo",
    "path": "papers/23/09/2309.04646.json",
    "total_tokens": 928,
    "translated_title": "高效调优用于越南聊天机器人的大型语言模型",
    "translated_abstract": "大型语言模型（LLMs），如GPT-4、PaLM和LLaMa，在各种自然语言任务中表现出色。最近的指令调优进展使得LLMs能够按照用户指令并产生类似人类回复的能力。然而，训练和实现LLMs所需的高成本对学术研究提出了挑战。此外，越南语言的预训练LLMs和指令调谐数据集的可用性有限。为了解决这些问题，我们利用来自开源项目（Alpaca、GPT4All和Chat-Doctor）的大规模指令跟随数据集，涵盖了通用和特定的医学领域。据我们所知，这是第一个用于越南语的指令数据集。随后，我们利用参数高效调优，通过低秩适应（LoRA）在两个开放的LLMs上：Bloomz（多语言）和GPTJ-6B（越南语），得到四个模型：Bloomz-Chat，Blo",
    "tldr": "本研究针对越南语聊天机器人的开发，通过利用来自Alpaca、GPT4All和Chat-Doctor等开源项目的大规模指令跟随数据集，成功训练了四个模型，此为越南语的首个指令数据集。",
    "en_tdlr": "This research focuses on the development of Vietnamese chatbots. By leveraging large-scale instruction-following datasets from open-source projects such as Alpaca, GPT4All, and Chat-Doctor, four models were trained, marking the first instructional dataset for Vietnamese."
}