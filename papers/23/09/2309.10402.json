{
    "title": "Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])",
    "abstract": "The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$ is exactly $\\max\\{d_x,d_y,2\\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\\min}=\\max\\{d_x+1,d_y\\}$ when the domain is ${\\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\\min}$ for uniform approximation using general activation functions including ReLU: $w_{\\min}\\ge d_y+1$ if $d_x<d_y\\le2d_x$. Together with our first result, this shows a dichotomy be",
    "link": "http://arxiv.org/abs/2309.10402",
    "context": "Title: Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])\nAbstract: The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$ is exactly $\\max\\{d_x,d_y,2\\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\\min}=\\max\\{d_x+1,d_y\\}$ when the domain is ${\\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\\min}$ for uniform approximation using general activation functions including ReLU: $w_{\\min}\\ge d_y+1$ if $d_x<d_y\\le2d_x$. Together with our first result, this shows a dichotomy be",
    "path": "papers/23/09/2309.10402.json",
    "total_tokens": 1236,
    "translated_title": "使用ReLU网络在紧致域上进行通用逼近的最小宽度",
    "translated_abstract": "经过研究，限制宽度网络的通用逼近性质已经作为深度限制网络的经典通用逼近定理的对偶进行研究。已经有几次尝试来表征使得通用逼近性质成立的最小宽度$w_{\\min}$，但只有很少几个找到了确切的值。在这项工作中，我们证明了对于从$[0,1]^{d_x}$到$\\mathbb R^{d_y}$的$L^p$函数的通用逼近的最小宽度，如果激活函数是ReLU-Like（例如ReLU，GELU，Softplus），那么它的确切值是$\\max\\{d_x,d_y,2\\}$。与已知的结果$w_{\\min}=\\max\\{d_x+1,d_y\\}$相比，当域为${\\mathbb R^{d_x}}$时，我们的结果首次表明，在紧致域上的逼近要求比在${\\mathbb R^{d_x}}$上的要求更小。我们接下来利用包括ReLU在内的一般激活函数进行一致逼近的最小宽度$w_{\\min}$证明了一个下界：如果$d_x<d_y\\le2d_x$，则$w_{\\min}\\ge d_y+1$。结合我们的第一个结果，这表明了一个二分法。",
    "tldr": "本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\\mathbb R^{d_y}$所需的最小宽度为$\\max\\{d_x,d_y,2\\}$，从而表明在紧致域上的逼近比在${\\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\\min}\\ge d_y+1$（当$d_x<d_y\\le2d_x$）。",
    "en_tdlr": "This study shows that the minimum width for approximating $L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$ using ReLU-like activation functions is $\\max\\{d_x,d_y,2\\}$, highlighting that approximation on a compact domain requires smaller width than on ${\\mathbb R^{d_x}}$. Additionally, a lower bound of $w_{\\min}\\ge d_y+1$ (for $d_x<d_y\\le2d_x$) is proven when using general activation functions including ReLU, showcasing a dichotomy."
}