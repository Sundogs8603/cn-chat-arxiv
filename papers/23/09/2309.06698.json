{
    "title": "Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])",
    "abstract": "Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most pr",
    "link": "http://arxiv.org/abs/2309.06698",
    "context": "Title: Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])\nAbstract: Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most pr",
    "path": "papers/23/09/2309.06698.json",
    "total_tokens": 942,
    "translated_title": "用于低资源语言的程序性语言理解基准测试：以土耳其语为例的案例研究",
    "translated_abstract": "理解程序性自然语言（例如，逐步说明）是执行和规划的关键步骤。然而，虽然英语中存在丰富的语料库和下游任务，但大多数语言缺乏这样的资源。为了解决这个问题，我们对土耳其程序文本进行了案例研究。我们首先使用自动翻译工具将土耳其wikiHow中的教程数量从2,000个扩展到52,000个，翻译质量和对原始含义的忠实性由专家团队在一个随机集上进行验证。然后，我们在语料库上生成了多个下游任务，例如链接操作、目标推理和摘要。为了解决这些任务，我们通过微调大型语言特定模型（如TR-BART和BERTurk）以及多语言模型（如mBART、mT5和XLM）实现了强基线模型。我们发现，在大多数任务中，语言特定模型始终以显著的优势胜过多语言模型。",
    "tldr": "本研究通过对土耳其语的程序文本进行案例研究，扩展了土耳其wikiHow的教程数量，并研究了几个下游任务。研究发现，在大多数任务中，语言特定模型相对于多语言模型具有明显优势。"
}