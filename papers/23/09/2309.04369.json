{
    "title": "Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments o",
    "link": "http://arxiv.org/abs/2309.04369",
    "context": "Title: Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments o",
    "path": "papers/23/09/2309.04369.json",
    "total_tokens": 903,
    "translated_title": "超越静态数据集：深度交互方法用于LLM评估",
    "translated_abstract": "大型语言模型（LLM）在各种实际任务中取得了进展，这激发了对LLM评估的需求。现有的LLM评估方法主要依赖于静态数据集的监督信号，无法评估LLM在动态实际场景中的能力，而这些场景中深度交互广泛存在。其他的LLM评估方法基于人工评估，成本高且耗时，并且无法进行大规模的LLM评估。为了解决上述问题，我们提出了一种新颖的基于深度交互的LLM评估框架。在我们提出的框架中，通过精心设计的评估任务，可以评估LLM在实际领域中与其他LLM的深度交互中的性能。此外，我们提出的框架是一种通用的评估方法，可应用于诸多实际任务，如机器翻译和代码生成。通过广泛的实验证明了我们提出的方法的有效性。",
    "tldr": "该论文提出了一种基于深度交互的LLM评估框架，突破了静态数据集的限制，能够评估LLM在动态实际场景中的能力，并适用于多种实际任务。",
    "en_tdlr": "This paper proposes a deep interaction-based framework for evaluating Large Language Models (LLMs), overcoming the limitations of static datasets and enabling evaluation of LLMs' abilities in dynamic real-world scenarios. The framework is applicable to various real-world tasks and has been shown to be effective through extensive experiments."
}