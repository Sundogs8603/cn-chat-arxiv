{
    "title": "Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting",
    "link": "http://arxiv.org/abs/2309.01289",
    "context": "Title: Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting",
    "path": "papers/23/09/2309.01289.json",
    "total_tokens": 968,
    "translated_title": "联邦正交训练：减轻连续联邦学习中的全局灾难性遗忘",
    "translated_abstract": "联邦学习（FL）因其能够实现分散数据上的隐私保护训练而受到了极大的关注。当前联邦学习领域的文献主要集中在单任务学习上。然而，随着时间的推移，客户端可能会出现新的任务，全局模型应该在不遗忘之前任务的情况下学习这些任务。这种真实场景被称为连续联邦学习（CFL）。CFL面临的主要挑战是全局灾难性遗忘，即当全局模型在新任务上训练时，其在旧任务上的性能下降。近期有一些关于CFL的研究提出了解决全局灾难性遗忘问题的方法。然而，这些方法要么对过去数据样本的可用性做出了不切实际的假设，要么违反了FL的隐私原则。我们提出了一种新方法，联邦正交训练（FOT），以克服这些缺点并解决全局灾难性遗忘问题。",
    "tldr": "本研究提出了一种名为联邦正交训练（FOT）的方法，用于解决连续联邦学习中的全局灾难性遗忘问题，该方法克服了现有方法对过去数据的不切实际假设和隐私原则的违反。",
    "en_tdlr": "This paper proposes a method called Federated Orthogonal Training (FOT) to address the problem of global catastrophic forgetting in Continual Federated Learning (CFL), overcoming the unrealistic assumptions on past data availability and violation of privacy principles in existing methods."
}