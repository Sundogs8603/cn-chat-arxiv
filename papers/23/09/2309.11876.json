{
    "title": "Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])",
    "abstract": "Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp",
    "link": "http://arxiv.org/abs/2309.11876",
    "context": "Title: Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])\nAbstract: Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp",
    "path": "papers/23/09/2309.11876.json",
    "total_tokens": 896,
    "translated_title": "多级非对称对比学习在医学图像分割预训练中的应用",
    "translated_abstract": "对比学习是一种从无标签数据中学习图像级表示的强大技术，为解决大规模预训练和有限标注数据之间的困境提供了一种有前途的方法。然而，大多数现有的对比学习策略主要针对自然图像的下游任务设计，因此当直接应用于医学图像（其下游任务通常是分割）时，它们往往是次优的甚至不如从头开始训练。在这项工作中，我们提出了一种名为JCL的新型非对称对比学习框架，用于医学图像分割的自我监督预训练。具体来说，（1）我们提出了一种新颖的非对称对比学习策略，同时在一阶段内对编码器和解码器进行预训练，以提供更好的分割模型初始化。 （2）我们设计了一个多级对比损失，用于考虑特征级别、图像级别和像素级别投影的对应关系。",
    "tldr": "本论文提出了一种针对医学图像分割的自我监督预训练方法，通过多级非对称对比学习的框架，在编码器和解码器同时进行预训练，提供更好的分割模型初始化。",
    "en_tdlr": "This paper presents a self-supervised pre-training method for medical image segmentation, which utilizes a multi-level asymmetric contrastive learning framework to simultaneously pre-train the encoder and the decoder, providing better initialization for segmentation models."
}