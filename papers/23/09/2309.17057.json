{
    "title": "Tell Me a Story! Narrative-Driven XAI with Large Language Models. (arXiv:2309.17057v1 [cs.AI])",
    "abstract": "In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in under",
    "link": "http://arxiv.org/abs/2309.17057",
    "context": "Title: Tell Me a Story! Narrative-Driven XAI with Large Language Models. (arXiv:2309.17057v1 [cs.AI])\nAbstract: In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in under",
    "path": "papers/23/09/2309.17057.json",
    "total_tokens": 1004,
    "translated_title": "讲给我听！基于大型语言模型的叙事驱动可解释人工智能",
    "translated_abstract": "在当今重要领域中，黑盒机器学习模型的盛行加大了对可解释人工智能（XAI）的需求。广泛使用的SHAP值虽然量化了特征重要性，但往往过于复杂，缺乏人性化的解释。此外，反事实（CF）解释展示了“如果”但没有解释“为什么”。为了弥合这一差距，我们引入了XAIstories。利用大型语言模型，XAIstories提供了叙事来阐明AI预测：基于SHAP解释的SHAPstories解释预测得分，而基于CF解释的CFstories解释决策。我们的结果令人震惊：超过90%的调查普通读者认为SHAPstories生成的叙事是有说服力的。数据科学家主要认为SHAPstories在向普通读者传达解释方面具有价值，92%的数据科学家表示这将有助于非专业人士的易用性和信心。",
    "tldr": "这项研究提出了基于大型语言模型的XAIstories框架，通过叙事方式解释AI预测，其中SHAPstories基于SHAP解释解释预测得分，CFstories基于CF解释解释决策。研究结果表明，超过90%的普通读者认可SHAPstories生成的叙事的说服力，92%的数据科学家认为SHAPstories能够提高非专业人士的易用性和信心。"
}