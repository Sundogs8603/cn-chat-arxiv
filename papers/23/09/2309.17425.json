{
    "title": "Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)",
    "abstract": "Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train ",
    "link": "http://arxiv.org/abs/2309.17425",
    "context": "Title: Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)\nAbstract: Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train ",
    "path": "papers/23/09/2309.17425.json",
    "total_tokens": 903,
    "translated_title": "数据过滤网络",
    "translated_abstract": "大型训练集已成为机器学习的基石，并为语言建模和多模态学习的最新进展奠定了基础。虽然对于预训练的数据采集仍然是一种常见的范式，但数据策划往往仍然是临时的。一种常见的方法是首先从网络上收集大量数据，然后通过各种启发式方法将此候选池筛选到实际的训练集中。在这项工作中，我们研究了学习数据过滤网络（DFN）用于筛选大型未策划数据集的问题。我们的主要发现是，用于筛选的网络的质量与其在下游任务上的表现是不同的：例如，一个在ImageNet上表现良好的模型可能会产生比一个在ImageNet上准确率较低但在一小部分高质量数据上进行训练的模型更差的训练集。基于我们的洞察力，我们构建了新的数据过滤网络，从而产生了最先进的图像-文本数据集。具体而言，我们表现最佳的数据集DFN-5B使我们能够进行训练。",
    "tldr": "本文研究了学习数据过滤网络用于筛选大型未策划数据集的问题，并构建了新的数据过滤网络，从而产生最先进的图像-文本数据集。"
}