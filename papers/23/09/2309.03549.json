{
    "title": "Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])",
    "abstract": "Inspired by the remarkable success of Latent Diffusion Models (LDMs) for image synthesis, we study LDM for text-to-video generation, which is a formidable challenge due to the computational and memory constraints during both model training and inference. A single LDM is usually only capable of generating a very limited number of video frames. Some existing works focus on separate prediction models for generating more video frames, which suffer from additional training cost and frame-level jittering, however. In this paper, we propose a framework called \"Reuse and Diffuse\" dubbed $\\textit{VidRD}$ to produce more frames following the frames already generated by an LDM. Conditioned on an initial video clip with a small number of frames, additional frames are iteratively generated by reusing the original latent features and following the previous diffusion process. Besides, for the autoencoder used for translation between pixel space and latent space, we inject temporal layers into its dec",
    "link": "http://arxiv.org/abs/2309.03549",
    "context": "Title: Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])\nAbstract: Inspired by the remarkable success of Latent Diffusion Models (LDMs) for image synthesis, we study LDM for text-to-video generation, which is a formidable challenge due to the computational and memory constraints during both model training and inference. A single LDM is usually only capable of generating a very limited number of video frames. Some existing works focus on separate prediction models for generating more video frames, which suffer from additional training cost and frame-level jittering, however. In this paper, we propose a framework called \"Reuse and Diffuse\" dubbed $\\textit{VidRD}$ to produce more frames following the frames already generated by an LDM. Conditioned on an initial video clip with a small number of frames, additional frames are iteratively generated by reusing the original latent features and following the previous diffusion process. Besides, for the autoencoder used for translation between pixel space and latent space, we inject temporal layers into its dec",
    "path": "papers/23/09/2309.03549.json",
    "total_tokens": 857,
    "translated_title": "重用与扩散：用于文本到视频生成的迭代去噪方法",
    "translated_abstract": "受潜在扩散模型（LDM）在图像合成方面的显著成功启发，我们研究了用于文本到视频生成的LDM，这是由于模型训练和推断过程中的计算和内存限制而面临的重大挑战。单个LDM通常只能生成有限数量的视频帧。一些现有的工作专注于为生成更多的视频帧而使用独立的预测模型，但这会导致额外的训练成本和帧级抖动。在本文中，我们提出了一个名为“重用与扩散”的框架，称为$\\textit{VidRD}$，以生成更多的帧，并且这些帧是在由LDM生成的先前帧之后产生的。此外，对于用于像素空间和潜在空间之间转换的自编码器，我们将时间层注入其解码器中。",
    "tldr": "这篇论文研究了在文本到视频生成中，使用重用和迭代扩散的方法可以生成更多视频帧，相比其他方法，该方法避免了额外的训练成本和帧级抖动。",
    "en_tdlr": "This paper investigates the use of reuse and iterative diffusion in text-to-video generation, allowing for the generation of more video frames while avoiding additional training cost and frame-level jittering compared to other methods."
}