{
    "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning. (arXiv:2309.06599v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-spe",
    "link": "http://arxiv.org/abs/2309.06599",
    "context": "Title: Reasoning with Latent Diffusion in Offline Reinforcement Learning. (arXiv:2309.06599v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-spe",
    "path": "papers/23/09/2309.06599.json",
    "total_tokens": 935,
    "translated_title": "离线强化学习中的潜在扩散推理",
    "translated_abstract": "离线强化学习（RL）有望通过静态数据集学习高奖励策略，而无需进一步的环境交互。然而，离线RL中的一个关键挑战在于有效地将静态数据集中的子优化轨迹片段连接起来，同时避免由于数据集中的支持不足而产生的外推误差。现有方法使用保守的方法进行调整，这些方法难以调节，并且在多模态数据上存在困难（如我们所示），或者依赖于嘈杂的蒙特卡洛回报样本进行奖励条件。在这项工作中，我们提出了一种新颖的方法，利用潜在扩散的表达能力，将支持内的轨迹序列建模为压缩的潜在技能。这有助于通过批处理约束学习Q函数，同时避免外推误差。潜在空间也具有表达能力，并且能够优雅地处理多模态数据。我们展示了学习到的时间抽象潜在空间编码更丰富的任务特定特征。",
    "tldr": "本研究提出了一种新颖的离线强化学习方法，利用潜在扩散建模轨迹序列，并通过批处理约束避免外推误差。该方法能够处理多模态数据并编码更丰富的任务特定特征。",
    "en_tdlr": "This paper proposes a novel offline reinforcement learning method that leverages latent diffusion to model trajectory sequences and avoids extrapolation errors through batch-constraining. The approach can handle multi-modal data and encode richer task-specific features."
}