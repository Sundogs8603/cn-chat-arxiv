{
    "title": "Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])",
    "abstract": "Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution is trained. This meta predictive learning is successfully validated on classifying handwritten digi",
    "link": "http://arxiv.org/abs/2309.04106",
    "context": "Title: Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])\nAbstract: Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution is trained. This meta predictive learning is successfully validated on classifying handwritten digi",
    "path": "papers/23/09/2309.04106.json",
    "total_tokens": 859,
    "translated_title": "自然语言的元预测学习模型",
    "translated_abstract": "基于自注意机制的大型语言模型不仅在自然语言本身上取得了令人惊讶的表现，而且在各种不同性质的任务中也表现出色。然而，关于语言处理，我们的人脑可能不是按照同样的原理运作。因此，关于大型语言模型采用的人工自我监督与脑计算之间的联系引起了一场辩论。在脑计算中最有影响力的假设之一是预测编码框架，该框架提出通过局部学习来最小化预测误差。然而，预测编码和相关的学分分配在语言处理中的作用仍然未知。在这里，我们提出了一个基于预测编码框架的均场学习模型，假设每个连接的突触权重遵循脉冲和斑点分布，只对分布进行训练。这种元预测学习在手写数字分类上得到了成功验证。",
    "tldr": "该论文提出了一种基于预测编码框架的均场学习模型，通过假设突触权重遵循脉冲和斑点分布并只对分布进行训练，成功地应用于手写数字分类。",
    "en_tdlr": "This paper proposes a mean-field learning model based on the predictive coding framework, which assumes that the synaptic weight follows a spike and slab distribution and only the distribution is trained. It is successfully validated in classifying handwritten digits."
}