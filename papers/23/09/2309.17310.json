{
    "title": "Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])",
    "abstract": "We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim",
    "link": "http://arxiv.org/abs/2309.17310",
    "context": "Title: Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])\nAbstract: We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim",
    "path": "papers/23/09/2309.17310.json",
    "total_tokens": 866,
    "translated_title": "机器学习中的一次出训练数据的可辨识性分析",
    "translated_abstract": "我们引入了一个新的分析框架，用于量化机器学习算法在训练集中包含少量数据点后输出分布的变化，我们将这个概念定义为一次出训练数据的可辨识性(LOOD)。这个问题对于衡量机器学习中的数据记忆和信息泄漏以及训练数据对模型预测的影响至关重要。我们使用高斯过程模型来建模机器学习算法的随机性，并通过对成员推断攻击使用广泛的经验分析验证了LOOD。我们的理论框架使我们能够研究信息泄漏的原因以及泄漏程度高的位置。例如，我们分析了激活函数对数据记忆的影响。此外，我们的方法允许我们优化...",
    "tldr": "这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。"
}