{
    "title": "MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models. (arXiv:2309.13567v2 [cs.CL] UPDATED)",
    "abstract": "With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw socia",
    "link": "http://arxiv.org/abs/2309.13567",
    "context": "Title: MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models. (arXiv:2309.13567v2 [cs.CL] UPDATED)\nAbstract: With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw socia",
    "path": "papers/23/09/2309.13567.json",
    "total_tokens": 1024,
    "translated_title": "MentaLLaMA：利用大型语言模型在社交媒体上进行可解释的心理健康分析",
    "translated_abstract": "随着网络技术的发展，社交媒体文本正在成为自动心理健康分析的丰富数据源。由于传统的判别方法存在解释性不足的问题，最近开始探索利用大型语言模型进行社交媒体上可解释的心理健康分析，旨在提供详细的解释和预测。结果表明，ChatGPT能够生成接近人类解释的正确分类。然而，LLMs在零 shot/few-shot 方式下仍然实现了不令人满意的分类性能。领域特定的微调是一个有效的解决方案，但面临两个挑战：1）缺乏高质量的训练数据。2）没有发布用于可解释的心理健康分析的开源 LLMs 以降低微调成本。为了缓解这些问题，我们在社交媒体上构建了第一个多任务和多源可解释的心理健康指导 (IMHI) 数据集，包含105K个数据样本。",
    "tldr": "本研究利用大型语言模型在社交媒体上进行可解释的心理健康分析。针对解释性不足的问题，研究发现ChatGPT能够生成接近人类解释的分类结果。然而，LLMs在零 shot/few-shot 方式下的分类性能仍不理想。为了解决缺乏训练数据和开源LLMs的问题，研究建立了第一个多任务和多源的解释性心理健康指导数据集。",
    "en_tdlr": "This study explores interpretable mental health analysis on social media using large language models. It is found that ChatGPT can generate explanations close to human interpretations, but LLMs still struggle with satisfactory classification performance. To address the lack of training data and open-source LLMs, the researchers built the first multi-task and multi-source interpretable mental health instruction dataset."
}