{
    "title": "ELRA: Exponential learning rate adaption gradient descent optimization method. (arXiv:2309.06274v1 [cs.LG])",
    "abstract": "We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential ",
    "link": "http://arxiv.org/abs/2309.06274",
    "context": "Title: ELRA: Exponential learning rate adaption gradient descent optimization method. (arXiv:2309.06274v1 [cs.LG])\nAbstract: We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential ",
    "path": "papers/23/09/2309.06274.json",
    "total_tokens": 930,
    "translated_title": "ELRA: 指数学习率自适应梯度下降优化方法",
    "translated_abstract": "我们提出了一种新颖的，快速（指数学习率自适应），从基础原理出发（无超参数依赖）的基于梯度的优化算法。该方法的主要思想是通过情境感知来调整学习率α，主要是追求正交邻近梯度。该方法具有高成功率和快速收敛速度，并且不依赖于手动调节的参数，具有更大的通用性。它可以应用于任意维度n的问题，并且与问题的维度线性（O(n)阶）扩展。它优化凸和非凸连续景观，并提供一定程度的梯度。与Ada系列（AdaGrad，AdaMax，AdaDelta，Adam等）相比，该方法具有旋转不变性：优化路径和性能与坐标选择无关。通过在MNIST基准数据集上与最先进的优化器进行广泛实验，展示了其出色的性能。我们根据其核心思想将这种新类优化器命名为指数学习率自适应梯度下降优化方法（ELRA）。",
    "tldr": "提出了一种快速、从基础原理出发且无超参数依赖的基于梯度的优化算法，通过情境感知调整学习率，具有高成功率和快速收敛速度，并且具备旋转不变性。",
    "en_tdlr": "Presented a fast and hyper-parameter-free optimizer algorithm based on gradients, which adapts the learning rate through situational awareness, achieves high success rate and fast convergence, and is rotation invariant."
}