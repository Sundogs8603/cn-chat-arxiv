{
    "title": "FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems. (arXiv:2309.08635v1 [cs.IR])",
    "abstract": "Federated Learning (FL) has emerged as a key approach for distributed machine learning, enhancing online personalization while ensuring user data privacy. Instead of sending private data to a central server as in traditional approaches, FL decentralizes computations: devices train locally and share updates with a global server. A primary challenge in this setting is achieving fast and accurate model training - vital for recommendation systems where delays can compromise user engagement. This paper introduces FedFNN, an algorithm that accelerates decentralized model training. In FL, only a subset of users are involved in each training epoch. FedFNN employs supervised learning to predict weight updates from unsampled users, using updates from the sampled set. Our evaluations, using real and synthetic data, show: 1. FedFNN achieves training speeds 5x faster than leading methods, maintaining or improving accuracy; 2. the algorithm's performance is consistent regardless of client cluster va",
    "link": "http://arxiv.org/abs/2309.08635",
    "context": "Title: FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems. (arXiv:2309.08635v1 [cs.IR])\nAbstract: Federated Learning (FL) has emerged as a key approach for distributed machine learning, enhancing online personalization while ensuring user data privacy. Instead of sending private data to a central server as in traditional approaches, FL decentralizes computations: devices train locally and share updates with a global server. A primary challenge in this setting is achieving fast and accurate model training - vital for recommendation systems where delays can compromise user engagement. This paper introduces FedFNN, an algorithm that accelerates decentralized model training. In FL, only a subset of users are involved in each training epoch. FedFNN employs supervised learning to predict weight updates from unsampled users, using updates from the sampled set. Our evaluations, using real and synthetic data, show: 1. FedFNN achieves training speeds 5x faster than leading methods, maintaining or improving accuracy; 2. the algorithm's performance is consistent regardless of client cluster va",
    "path": "papers/23/09/2309.08635.json",
    "total_tokens": 864,
    "translated_title": "FedFNN: 在联邦推荐系统中通过更新预测实现更快的训练收敛",
    "translated_abstract": "联邦学习（FL）已成为分布式机器学习的关键方法，增强了在线个性化的同时确保用户数据的隐私。与传统方法将私有数据发送到中央服务器不同，FL将计算分散：设备在本地训练并与全局服务器共享更新。在这种情况下，主要挑战是实现快速和准确的模型训练，这对于推荐系统来说至关重要，因为延迟可能会损害用户参与度。本文介绍了FedFNN，一种加速分散式模型训练的算法。在FL中，每个训练周期仅涉及用户子集。FedFNN利用监督学习从未抽样的用户中预测权重更新，使用来自抽样集的更新。我们使用真实和合成数据进行了评估，结果显示：1. FedFNN的训练速度比领先方法快5倍，保持或提高准确性；2. 该算法的性能与客户端集群的变化无关。",
    "tldr": "FedFNN是一种在联邦推荐系统中加速训练的算法。通过预测未抽样用户的权重更新，使用已抽样集的更新，FedFNN实现了比其他方法快5倍的训练速度，同时保持或提高了准确性。"
}