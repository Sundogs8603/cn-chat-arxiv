{
    "title": "Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity. (arXiv:2309.13591v2 [cs.LG] UPDATED)",
    "abstract": "The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysi",
    "link": "http://arxiv.org/abs/2309.13591",
    "context": "Title: Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity. (arXiv:2309.13591v2 [cs.LG] UPDATED)\nAbstract: The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysi",
    "path": "papers/23/09/2309.13591.json",
    "total_tokens": 820,
    "translated_title": "鲁棒性分布式学习：在数据异构性下的严格误差界和断点分析",
    "translated_abstract": "鲁棒性分布式学习算法在数据均匀性的情况下与实际观测吻合，但在实际场景中的数据异构性下，现有的学习误差下界基本上是空洞的，并且与实际观测明显不匹配。本文考虑了更加实际的数据异构性模型，称为（G，B）-梯度差异度，并证明它涵盖了比现有理论更多的学习问题。特别地，我们展示了在数据异构性下的断点分析低于经典的1/2。我们还证明了任何分布式学习算法的学习误差的新下界。我们导出了分布式梯度下降的鲁棒变种的匹配上界，并通过实验证明了我们的分析结果。",
    "tldr": "在数据异构性下，鲁棒性分布式学习算法在更实际的数据异构性模型下得到了改进，在学习误差下界和断点分析方面取得了新的结果。",
    "en_tdlr": "Improved robust distributed learning algorithms under data heterogeneity by considering a more realistic heterogeneity model, achieving new results in lower bounds on learning error and breakdown point analysis."
}