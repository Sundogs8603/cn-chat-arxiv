{
    "title": "Improving CLIP Robustness with Knowledge Distillation and Self-Training. (arXiv:2309.10361v1 [cs.CV])",
    "abstract": "This paper examines the robustness of a multi-modal computer vision model, CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised learning. The main objective is twofold: first, to evaluate the robustness of CLIP, and second, to explore strategies for augmenting its robustness. To achieve this, we introduce a novel approach named LP-CLIP. This technique involves the distillation of CLIP features through the incorporation of a linear probing layer positioned atop its encoding structure. This newly added layer is trained utilizing pseudo-labels produced by CLIP, coupled with a self-training strategy. The LP-CLIP technique offers a promising approach to enhance the robustness of CLIP without the need for annotations. By leveraging a simple linear probing layer, we aim to improve the model's ability to withstand various uncertainties and challenges commonly encountered in real-world scenarios. Importantly, our approach does not rely on annotated data, which makes it ",
    "link": "http://arxiv.org/abs/2309.10361",
    "context": "Title: Improving CLIP Robustness with Knowledge Distillation and Self-Training. (arXiv:2309.10361v1 [cs.CV])\nAbstract: This paper examines the robustness of a multi-modal computer vision model, CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised learning. The main objective is twofold: first, to evaluate the robustness of CLIP, and second, to explore strategies for augmenting its robustness. To achieve this, we introduce a novel approach named LP-CLIP. This technique involves the distillation of CLIP features through the incorporation of a linear probing layer positioned atop its encoding structure. This newly added layer is trained utilizing pseudo-labels produced by CLIP, coupled with a self-training strategy. The LP-CLIP technique offers a promising approach to enhance the robustness of CLIP without the need for annotations. By leveraging a simple linear probing layer, we aim to improve the model's ability to withstand various uncertainties and challenges commonly encountered in real-world scenarios. Importantly, our approach does not rely on annotated data, which makes it ",
    "path": "papers/23/09/2309.10361.json",
    "total_tokens": 1018,
    "translated_title": "用知识蒸馏和自训练提高CLIP鲁棒性的研究",
    "translated_abstract": "本文研究了一种多模态计算机视觉模型CLIP（对比语言-图像预训练）在无监督学习中的鲁棒性。主要目标是评估CLIP的鲁棒性，并探索增强其鲁棒性的策略。为此，我们引入了一种名为LP-CLIP的新方法。该技术通过在其编码结构顶部加入一个线性探测层，将CLIP特征蒸馏出来。这个新增的层使用CLIP生成的伪标签进行训练，并结合自训练策略。LP-CLIP技术提供了一种在不需要注释数据的情况下增强CLIP鲁棒性的有希望方法。通过利用简单的线性探测层，我们旨在提高该模型在现实场景中面临各种不确定性和挑战的能力。重要的是，我们的方法不依赖于注释数据。",
    "tldr": "本文提出了一种名为LP-CLIP的方法，通过知识蒸馏和自训练来提高CLIP多模态计算机视觉模型的鲁棒性，这种方法不需要注释数据。 LP-CLIP通过在CLIP编码结构顶部添加线性探测层来蒸馏CLIP特征，并使用由CLIP生成的伪标签进行训练，具有增强模型在现实场景中应对各种挑战和不确定性的能力。"
}