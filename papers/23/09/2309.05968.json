{
    "title": "Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity. (arXiv:2309.05968v1 [cs.LG])",
    "abstract": "We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advanc",
    "link": "http://arxiv.org/abs/2309.05968",
    "context": "Title: Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity. (arXiv:2309.05968v1 [cs.LG])\nAbstract: We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advanc",
    "path": "papers/23/09/2309.05968.json",
    "total_tokens": 899,
    "translated_title": "神经网络层矩阵分解揭示潜在流形编码和存储容量",
    "translated_abstract": "我们证明了通用逼近定理的逆定理，即神经网络(NN)编码定理，它表明对于每个稳定收敛的NN和连续激活函数，其权重矩阵实际上编码了一个连续函数，该函数在有界域内近似于训练数据集，并且误差在有限范围内。我们进一步展示了使用特征值分解和奇异值分解来对每个NN层的权重矩阵进行矩阵分解，可以揭示训练数据集所编码和表示的潜在空间流形的性质，以及每个NN层执行的几何操作的性质。我们的结果对于理解NN如何通过利用存储容量来突破维度诅咒具有意义，并且这两者是互补的。这种层矩阵分解(LMD)进一步揭示了NN层的特征值分解与最新的研究有密切关联。",
    "tldr": "通过神经网络层矩阵分解，我们揭示了神经网络层编码训练数据集的潜在流形和数学运算的几何性质，这对于理解神经网络如何突破维度诅咒具有重要意义。",
    "en_tdlr": "Through neural network layer matrix decomposition, we reveal the latent manifold encoding of training datasets and the geometric nature of mathematical operations, shedding light on how neural networks overcome the curse of dimensionality."
}