{
    "title": "USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])",
    "abstract": "We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib",
    "link": "http://arxiv.org/abs/2309.08023",
    "context": "Title: USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])\nAbstract: We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib",
    "path": "papers/23/09/2309.08023.json",
    "total_tokens": 968,
    "translated_title": "基于大型预训练基础模型的多语种演讲者转换检测模型（USM-SCD）",
    "translated_abstract": "我们提出了一种多语种演讲者转换检测模型（USM-SCD），可以同时检测演讲者转换并为96种语言执行自动语音识别。该模型是从一个经过大量受监督和无监督数据训练的语音基础模型进行调整而来的，展示了从大型通用基础模型到下游任务的微调的实用性。通过一系列消融研究，我们分析了这个多语种演讲者转换检测模型的性能。我们展示了USM-SCD模型可以在由来自96种语言的数据构成的测试集上实现超过75%的平均演讲者转换检测F1得分。在美式英语上，USM-SCD模型可以在各种公共和内部测试集上实现85.8%的演讲者转换检测F1得分，相对于之前的单语言基准模型提高了21%。我们还展示了只需要微调可训练模型参数的四分之一就可以实现最佳模型性能。",
    "tldr": "USM-SCD是一种基于大型预训练基础模型的多语种演讲者转换检测模型，通过微调模型参数，可以同时检测演讲者转换并为96种语言执行自动语音识别。在实验中表现出了优异的性能。",
    "en_tdlr": "USM-SCD is a multilingual speaker change detection model based on large pretrained foundation models. By fine-tuning the model parameters, it can simultaneously detect speaker turns and perform ASR for 96 languages, achieving excellent performance in experiments."
}