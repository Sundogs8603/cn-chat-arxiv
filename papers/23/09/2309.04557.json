{
    "title": "Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])",
    "abstract": "We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\\cal D}_1,\\dots,{\\cal D}_N$ for the same learning model $f_{\\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\\{\\theta_i(t)\\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\\theta^\\star_{1},\\ldots,\\theta^\\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\\mathcal{O}(Np^2)$ fewer elementary operati",
    "link": "http://arxiv.org/abs/2309.04557",
    "context": "Title: Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])\nAbstract: We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\\cal D}_1,\\dots,{\\cal D}_N$ for the same learning model $f_{\\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\\{\\theta_i(t)\\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\\theta^\\star_{1},\\ldots,\\theta^\\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\\mathcal{O}(Np^2)$ fewer elementary operati",
    "path": "papers/23/09/2309.04557.json",
    "total_tokens": 997,
    "translated_title": "用于核回归的遗憾最优联邦迁移学习及其在美式期权定价中的应用",
    "translated_abstract": "我们提出了一种最优的迭代方案，用于联邦迁移学习，其中中心计划者可以访问同一学习模型 $f_{\\theta}$ 的数据集 ${\\cal D}_1,\\dots,{\\cal D}_N$。我们的目标是在尊重模型 $f_{\\theta(T)}$ 的损失函数的情况下，尽量减小生成参数 $\\{\\theta_i(t)\\}_{t=0}^T$ 在所有 $T$ 次迭代中与每个数据集得到的专门参数$\\theta^\\star_{1},\\ldots,\\theta^\\star_N$ 的累积偏差。我们仅允许每个专门模型（节点/代理）和中心计划者（服务器）在每次迭代（轮）之间进行持续通信。对于模型 $f_{\\theta}$ 是有限秩核回归的情况，我们得出了遗憾最优算法的显式更新公式。通过利用遗憾最优算法中的对称性，我们进一步开发了一种几乎遗憾最优的启发式算法，其运行需要较少的 $\\mathcal{O}(Np^2)$ 个基本运算。",
    "tldr": "本论文提出了一种遗憾最优算法的迭代方案，用于联邦迁移学习，在核回归模型中具体化，并提出了一个几乎遗憾最优的启发式算法，可以减小生成参数与专门参数之间的累积偏差。"
}