{
    "title": "Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])",
    "abstract": "Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac",
    "link": "http://arxiv.org/abs/2309.10302",
    "context": "Title: Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])\nAbstract: Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac",
    "path": "papers/23/09/2309.10302.json",
    "total_tokens": 1030,
    "translated_title": "解耦训练：令人沮丧的简单多领域学习的回归",
    "translated_abstract": "多领域学习（MDL）旨在训练一个模型，在多个重叠但非相同的领域中具有最小的平均风险。为了解决数据集偏差和领域优势的挑战，从对齐分布减少领域差距的角度或通过实施领域特定的塔、门甚至专家来保留差异，已经提出了许多MDL方法。MDL模型变得越来越复杂，具有复杂的网络架构或损失函数，引入额外的参数并增加计算成本。在本文中，我们提出了一种令人沮丧的、无超参数的多领域学习方法，命名为解耦训练（D-Train）。D-Train是一种三阶段的从一般到特殊的训练策略，首先在所有领域上进行预训练以热身一个根模型，然后通过将其拆分为多个头部在每个领域上进行后训练，最后通过固定骨干进行头部微调，实现解耦训练以获得更好的性能。",
    "tldr": "这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。",
    "en_tdlr": "This paper proposes a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training (D-Train). The method involves a three-phase training strategy, including pre-training, post-training on each domain, and fine-tuning the heads. The decoupled training approach aims to achieve better performance by decoupling the training process."
}