{
    "title": "When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])",
    "abstract": "With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a",
    "link": "http://arxiv.org/abs/2309.00007",
    "context": "Title: When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])\nAbstract: With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a",
    "path": "papers/23/09/2309.00007.json",
    "total_tokens": 915,
    "translated_title": "当度量不可靠时：朝着Top-k多标签学习的不可察觉的对抗性扰动",
    "translated_abstract": "随着深度神经网络的巨大成功，对抗性学习在各种研究中得到了广泛关注，涵盖了从多类别学习到多标签学习的范围。然而，现有的对抗性攻击方法只关注传统的视觉不可察觉性，而忽视了来自度量的新的可感知问题，例如Precision@k和mAP@k。具体而言，当一个训练良好的多标签分类器在某些样本上的表现远低于预期时，受害者可以很容易地意识到这种性能退化源于攻击，而不是模型本身。因此，理想的多标签对抗性攻击应该能够欺骗视觉感知，并且避开度量的监测。为此，本文首先引入了度量不可察觉性的概念。然后，设计了一种新的损失函数来生成这种既能够实现视觉不可察觉性又能够避开度量监测的对抗性扰动。",
    "tldr": "本文提出了一种面向Top-k多标签学习的不可察觉的对抗性扰动方法，既能够欺骗人眼的视觉感知，又能够避开度量监测。",
    "en_tdlr": "This paper proposes an imperceptible adversarial perturbation method for Top-k multi-label learning, which can deceive visual perception while evading metric monitoring."
}