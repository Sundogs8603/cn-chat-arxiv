{
    "title": "Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding. (arXiv:2309.06195v1 [cs.LG])",
    "abstract": "Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gra",
    "link": "http://arxiv.org/abs/2309.06195",
    "context": "Title: Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding. (arXiv:2309.06195v1 [cs.LG])\nAbstract: Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gra",
    "path": "papers/23/09/2309.06195.json",
    "total_tokens": 827,
    "translated_title": "通过平滑软阈值函数的展开ISTA和ADMM网络的优化保证",
    "translated_abstract": "解决线性反问题在许多应用中起着关键作用。基于算法展开的，模型感知的数据驱动方法在有效解决这些问题方面引起了广泛关注。学习的迭代软阈值算法（LISTA）和交替方向乘子压缩感知网络（ADMM-CSNet）是两种广泛使用的这种方法，分别基于ISTA和ADMM算法。本文研究了具有平滑软阈值在过参数化（OP）区域的有限层展开网络（如LISTA和ADMM-CSNet）的优化保证，即通过学习迭代次数的增加实现接近零训练损失。我们通过利用Polyak-Lojasiewicz的一个修改版本，即PL$^*$条件，实现了这一目标。在损失函数图景的特定区域内满足PL$^*$条件可以确保全局最小值的存在，并实现从初始化时的指数收敛。",
    "tldr": "本文研究了通过平滑软阈值函数的展开ISTA和ADMM网络在过参数化区域中的优化保证，通过满足特定区域的PL$^*$条件实现接近零训练损失。"
}