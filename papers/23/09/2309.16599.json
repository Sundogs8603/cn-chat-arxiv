{
    "title": "Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation. (arXiv:2309.16599v1 [cs.CL])",
    "abstract": "Zero-shot translation (ZST), which is generally based on a multilingual neural machine translation model, aims to translate between unseen language pairs in training data. The common practice to guide the zero-shot language mapping during inference is to deliberately insert the source and target language IDs, e.g., <EN> for English and <DE> for German. Recent studies have shown that language IDs sometimes fail to navigate the ZST task, making them suffer from the off-target problem (non-target language words exist in the generated translation) and, therefore, difficult to apply the current multilingual translation model to a broad range of zero-shot language scenarios. To understand when and why the navigation capabilities of language IDs are weakened, we compare two extreme decoder input cases in the ZST directions: Off-Target (OFF) and On-Target (ON) cases. By contrastively visualizing the contextual word representations (CWRs) of these cases with teacher forcing, we show that 1) the",
    "link": "http://arxiv.org/abs/2309.16599",
    "context": "Title: Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation. (arXiv:2309.16599v1 [cs.CL])\nAbstract: Zero-shot translation (ZST), which is generally based on a multilingual neural machine translation model, aims to translate between unseen language pairs in training data. The common practice to guide the zero-shot language mapping during inference is to deliberately insert the source and target language IDs, e.g., <EN> for English and <DE> for German. Recent studies have shown that language IDs sometimes fail to navigate the ZST task, making them suffer from the off-target problem (non-target language words exist in the generated translation) and, therefore, difficult to apply the current multilingual translation model to a broad range of zero-shot language scenarios. To understand when and why the navigation capabilities of language IDs are weakened, we compare two extreme decoder input cases in the ZST directions: Off-Target (OFF) and On-Target (ON) cases. By contrastively visualizing the contextual word representations (CWRs) of these cases with teacher forcing, we show that 1) the",
    "path": "papers/23/09/2309.16599.json",
    "total_tokens": 966,
    "translated_title": "在负样本上进行不太可能的调整惊人地改善了零样本翻译",
    "translated_abstract": "零样本翻译(ZST)通常基于多语言神经机器翻译模型，旨在在训练数据中翻译未见过的语言对。在推理过程中，常见的指导零样本语言映射的做法是故意插入源语言和目标语言的ID，例如英语的<EN>和德语的<DE>。最近的研究表明，语言ID有时无法正确引导ZST任务，导致生成的翻译中存在非目标语言词汇，从而使当前的多语言翻译模型难以应用于广泛的零样本语言场景。为了了解语言ID的导航能力何时以及为何减弱，我们比较了ZST方向上两种极端的解码器输入情况：非目标(Off-Target，OFF)和目标(On-Target，ON)情况。通过对这些情况下使用教师强制的上下文词表示(CWRs)进行对比可视化，我们展示了：1）the",
    "tldr": "在零样本翻译中，语言ID有时无法正确引导任务，并导致生成的翻译中存在非目标语言词汇。本论文通过比较不同解码器输入情况下的上下文词表示，提出了在负样本上进行调整的方法，显著改善了零样本翻译的效果。",
    "en_tdlr": "Language IDs sometimes fail to properly guide zero-shot translation tasks, resulting in non-target language words in the generated translation. This paper proposes a method of tuning on negative samples by comparing contextual word representations under different decoder input conditions, which remarkably improves the effectiveness of zero-shot translation."
}