{
    "title": "Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling. (arXiv:2309.16882v1 [cs.LG])",
    "abstract": "Time series modeling, a crucial area in science, often encounters challenges when training Machine Learning (ML) models like Recurrent Neural Networks (RNNs) using the conventional mini-batch training strategy that assumes independent and identically distributed (IID) samples and initializes RNNs with zero hidden states. The IID assumption ignores temporal dependencies among samples, resulting in poor performance. This paper proposes the Message Propagation Through Time (MPTT) algorithm to effectively incorporate long temporal dependencies while preserving faster training times relative to the stateful solutions. MPTT utilizes two memory modules to asynchronously manage initial hidden states for RNNs, fostering seamless information exchange between samples and allowing diverse mini-batches throughout epochs. MPTT further implements three policies to filter outdated and preserve essential information in the hidden states to generate informative initial hidden states for RNNs, facilitati",
    "link": "http://arxiv.org/abs/2309.16882",
    "context": "Title: Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling. (arXiv:2309.16882v1 [cs.LG])\nAbstract: Time series modeling, a crucial area in science, often encounters challenges when training Machine Learning (ML) models like Recurrent Neural Networks (RNNs) using the conventional mini-batch training strategy that assumes independent and identically distributed (IID) samples and initializes RNNs with zero hidden states. The IID assumption ignores temporal dependencies among samples, resulting in poor performance. This paper proposes the Message Propagation Through Time (MPTT) algorithm to effectively incorporate long temporal dependencies while preserving faster training times relative to the stateful solutions. MPTT utilizes two memory modules to asynchronously manage initial hidden states for RNNs, fostering seamless information exchange between samples and allowing diverse mini-batches throughout epochs. MPTT further implements three policies to filter outdated and preserve essential information in the hidden states to generate informative initial hidden states for RNNs, facilitati",
    "path": "papers/23/09/2309.16882.json",
    "total_tokens": 925,
    "translated_title": "消息在时间中的传播：一种用于保留时序建模中序列依赖性的算法",
    "translated_abstract": "时间序列建模是科学中一个关键领域，然而在使用传统的小批量训练策略（假设样本独立同分布且RNN的隐藏状态初始化为零）训练机器学习模型（如RNN）时，经常会遇到挑战。传统的假设忽视了样本间的时间依赖关系，导致性能较差。本文提出了一种名为消息传播通过时间（MPTT）的算法，能够有效地融入长时间依赖关系，同时保持相对于有状态解决方案更快的训练时间。MPTT利用两个内存模块异步管理RNN的初始隐藏状态，促进样本之间的无缝信息交换，并允许在epochs期间使用多样的小批量。MPTT进一步实施了三种策略来过滤过期的信息并保留隐藏状态中的关键信息，以生成信息丰富的初始隐藏状态，从而方便RNN的训练。",
    "tldr": "本文提出了一种名为消息传播通过时间（MPTT）的算法，用于在时间序列建模中保留序列依赖性。MPTT通过异步管理初始隐藏状态，并采用三种策略过滤过期信息，以生成信息丰富的隐藏状态，从而提高RNN的训练效果。"
}