{
    "title": "Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors. (arXiv:2309.06349v1 [stat.ML])",
    "abstract": "Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\\alpha$-TS, where we use a fractional or $\\alpha$-posterior ($\\alpha\\in(0,1)$) instead of the standard posterior distribution. To compute an $\\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\\alpha$. For $\\alpha$-TS we obtain both instance-dependent $\\mathcal{O}\\left(\\sum_{k \\neq i^*} \\Delta_k\\left(\\frac{\\log(T)}{C(\\alpha)\\Delta_k^2} + \\frac{1}{2} \\right)\\right)$ and instance-independent $\\mathcal{O}(\\sqrt{KT\\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior di",
    "link": "http://arxiv.org/abs/2309.06349",
    "context": "Title: Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors. (arXiv:2309.06349v1 [stat.ML])\nAbstract: Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\\alpha$-TS, where we use a fractional or $\\alpha$-posterior ($\\alpha\\in(0,1)$) instead of the standard posterior distribution. To compute an $\\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\\alpha$. For $\\alpha$-TS we obtain both instance-dependent $\\mathcal{O}\\left(\\sum_{k \\neq i^*} \\Delta_k\\left(\\frac{\\log(T)}{C(\\alpha)\\Delta_k^2} + \\frac{1}{2} \\right)\\right)$ and instance-independent $\\mathcal{O}(\\sqrt{KT\\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior di",
    "path": "papers/23/09/2309.06349.json",
    "total_tokens": 991,
    "translated_title": "使用分数后验概率对汤普森抽样进行广义遗憾分析",
    "translated_abstract": "汤普森抽样（TS）是解决随机多臂赌博问题的最流行和最早的算法之一。我们考虑了TS的一个变种，称为α-TS，其中我们使用分数或α-后验（α∈（0,1））代替标准后验分布。为了计算α-后验，标准后验的定义中的似然函数被一个因子α搅拌。对于α-TS，我们在非常温和的先验和奖励分布条件下获得了既依赖于实例的Ο（∑_{k≠i^*}Δ_k（\\frac{\\log(T)}{C(α)Δ_k^2}+\\frac{1}{2}））也依赖于实例独立的Ο（\\sqrt{KT\\log K}）频率遗憾界，其中Δ_k是第k个和最好的臂的真实均值奖励之间的差，而C(α)是已知的常数。子高斯和指数族模型都满足我们对奖励分布的一般条件。我们对先验的条件是...",
    "tldr": "这项研究对使用分数后验概率的汤普森抽样算法进行了广义遗憾分析，获得了依赖于实例和实例独立的频率遗憾界。这对多臂赌博问题的解决有重要意义。",
    "en_tdlr": "This study presents a generalized regret analysis of Thompson sampling using fractional posteriors, obtaining instance-dependent and instance-independent frequentist regret bounds. This has significant implications for solving the stochastic multi-armed bandit problem."
}