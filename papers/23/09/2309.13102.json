{
    "title": "Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR. (arXiv:2309.13102v1 [eess.AS])",
    "abstract": "In this paper, we start by training End-to-End Automatic Speech Recognition (ASR) models using Federated Learning (FL) and examining the fundamental considerations that can be pivotal in minimizing the performance gap in terms of word error rate between models trained using FL versus their centralized counterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii) loss characteristics via altering Connectionist Temporal Classification (CTC) weight, (iii) model initialization through seed start, (iv) carrying over modeling setup from experiences in centralized training to FL, e.g., pre-layer or post-layer normalization, and (v) FL-specific hyperparameters, such as number of local epochs, client sampling size, and learning rate scheduler, specifically for ASR under heterogeneous data distribution. We shed light on how some optimizers work better than others via inducing smoothness. We also summarize the applicability of algorithms, trends, and propose best practices from ",
    "link": "http://arxiv.org/abs/2309.13102",
    "context": "Title: Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR. (arXiv:2309.13102v1 [eess.AS])\nAbstract: In this paper, we start by training End-to-End Automatic Speech Recognition (ASR) models using Federated Learning (FL) and examining the fundamental considerations that can be pivotal in minimizing the performance gap in terms of word error rate between models trained using FL versus their centralized counterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii) loss characteristics via altering Connectionist Temporal Classification (CTC) weight, (iii) model initialization through seed start, (iv) carrying over modeling setup from experiences in centralized training to FL, e.g., pre-layer or post-layer normalization, and (v) FL-specific hyperparameters, such as number of local epochs, client sampling size, and learning rate scheduler, specifically for ASR under heterogeneous data distribution. We shed light on how some optimizers work better than others via inducing smoothness. We also summarize the applicability of algorithms, trends, and propose best practices from ",
    "path": "papers/23/09/2309.13102.json",
    "total_tokens": 906,
    "translated_title": "FL4ASR中优化器引起的平滑性的重要性：理解端到端ASR的联邦学习",
    "translated_abstract": "本文首先使用联邦学习（FL）训练端到端自动语音识别（ASR）模型，并研究了在最小化FL模型与中心化模型之间的单词错误率性能差距方面，可以起到关键作用的基本考虑因素。具体而言，我们研究了自适应优化器的影响，通过改变连接主义时序分类（CTC）权重来研究损失特征，通过种子起始值对模型进行初始化，通过从中心化训练的经验中延续建模设置到FL中，例如预层或后层归一化，以及针对异质数据分布的ASR的FL专用超参数，如本地epoch数量、客户端采样大小和学习率调度器。我们阐明了一些优化器通过引导平滑性而比其他优化器更好的工作原理。我们还总结了算法的适用性、趋势，并提出了最佳实践。",
    "tldr": "本论文通过使用联邦学习训练端到端ASR模型，研究了优化器对平滑性的重要性，并总结了适用的算法和最佳实践。",
    "en_tdlr": "This paper explores the importance of optimizers in inducing smoothness in FL4ASR, presents the findings of training End-to-End ASR models using Federated Learning, and suggests best practices for minimizing the performance gap between FL models and their centralized counterpart."
}