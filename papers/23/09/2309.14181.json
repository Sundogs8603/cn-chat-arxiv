{
    "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)",
    "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions o",
    "link": "http://arxiv.org/abs/2309.14181",
    "context": "Title: Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)\nAbstract: The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions o",
    "path": "papers/23/09/2309.14181.json",
    "total_tokens": 737,
    "translated_title": "Q-Bench: 一种用于低级别视觉通用基础模型的基准测试",
    "translated_abstract": "多模态大型语言模型（MLLMs）的快速发展引发了计算机视觉从专门模型向通用基础模型的转变。然而，评估MLLMs在低级别视觉感知和理解方面的能力仍然不足。为了填补这一差距，我们提出了Q-Bench，这是一个综合性的基准测试，用于系统评估MLLMs在三个领域的潜在能力：低级别视觉感知、低级别视觉描述和整体视觉质量评估。",
    "tldr": "Q-Bench是一个综合性的基准测试，用于评估多模态大型语言模型在低级别视觉感知和理解方面的能力。",
    "en_tdlr": "Q-Bench is a comprehensive benchmark to evaluate the abilities of Multi-modality Large Language Models (MLLMs) on low-level visual perception and understanding."
}