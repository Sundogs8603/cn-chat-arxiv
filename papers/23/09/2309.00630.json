{
    "title": "Commodities Trading through Deep Policy Gradient Methods. (arXiv:2309.00630v1 [q-fin.TR])",
    "abstract": "Algorithmic trading has gained attention due to its potential for generating superior returns. This paper investigates the effectiveness of deep reinforcement learning (DRL) methods in algorithmic commodities trading. It formulates the commodities trading problem as a continuous, discrete-time stochastic dynamical system. The proposed system employs a novel time-discretization scheme that adapts to market volatility, enhancing the statistical properties of subsampled financial time series. To optimize transaction-cost- and risk-sensitive trading agents, two policy gradient algorithms, namely actor-based and actor-critic-based approaches, are introduced. These agents utilize CNNs and LSTMs as parametric function approximators to map historical price observations to market positions.Backtesting on front-month natural gas futures demonstrates that DRL models increase the Sharpe ratio by $83\\%$ compared to the buy-and-hold baseline. Additionally, the risk profile of the agents can be custo",
    "link": "http://arxiv.org/abs/2309.00630",
    "context": "Title: Commodities Trading through Deep Policy Gradient Methods. (arXiv:2309.00630v1 [q-fin.TR])\nAbstract: Algorithmic trading has gained attention due to its potential for generating superior returns. This paper investigates the effectiveness of deep reinforcement learning (DRL) methods in algorithmic commodities trading. It formulates the commodities trading problem as a continuous, discrete-time stochastic dynamical system. The proposed system employs a novel time-discretization scheme that adapts to market volatility, enhancing the statistical properties of subsampled financial time series. To optimize transaction-cost- and risk-sensitive trading agents, two policy gradient algorithms, namely actor-based and actor-critic-based approaches, are introduced. These agents utilize CNNs and LSTMs as parametric function approximators to map historical price observations to market positions.Backtesting on front-month natural gas futures demonstrates that DRL models increase the Sharpe ratio by $83\\%$ compared to the buy-and-hold baseline. Additionally, the risk profile of the agents can be custo",
    "path": "papers/23/09/2309.00630.json",
    "total_tokens": 901,
    "translated_title": "通过深度策略梯度方法进行大宗商品交易",
    "translated_abstract": "由于其潜在的较好收益能力，算法交易受到了广泛关注。本文研究了深度强化学习（DRL）方法在算法性大宗商品交易中的有效性。它将大宗商品交易问题建模为连续的离散时间随机动态系统。所提出的系统采用了一种新颖的时间离散化方案，可以根据市场波动性调整，提高了子采样金融时间序列的统计特性。为了优化对交易成本和风险敏感的交易代理，引入了两种策略梯度算法，即基于演员的方法和基于演员-评论者的方法。这些代理利用CNN和LSTM作为参数化函数逼近器将历史价格观测映射到市场位置。对最近月份的天然气期货进行回测的结果表明，与买入并持有基线相比，DRL模型将夏普比率提高了83%。此外，代理的风险特征可以定制。",
    "tldr": "本文研究了深度强化学习在算法性大宗商品交易中的有效性，通过引入新颖的时间离散化方案和两种策略梯度算法，DRL模型将夏普比率提高了83%。",
    "en_tdlr": "This paper investigates the effectiveness of deep reinforcement learning in algorithmic commodities trading, and the DRL models increase the Sharpe ratio by 83% through the introduction of a novel time-discretization scheme and two policy gradient algorithms."
}