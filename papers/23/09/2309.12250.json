{
    "title": "SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])",
    "abstract": "Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.",
    "link": "http://arxiv.org/abs/2309.12250",
    "context": "Title: SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])\nAbstract: Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.",
    "path": "papers/23/09/2309.12250.json",
    "total_tokens": 775,
    "translated_title": "SQUARE: 使用多个正负参考答案自动评估问答系统",
    "translated_abstract": "问答系统的评估是非常具有挑战性和昂贵的，最可靠的方法是通过人工标注问题的答案的正确性。最近的研究表明，基于Transformer LM编码器的相似性度量在问答评估中具有良好的迁移性，但它们的使用受限于单个正确参考答案。我们提出了一种新的评估指标：SQuArE（句子级问答评估），使用多个参考答案（组合多个正确和错误的参考答案）进行句子形式的问答评估。我们在句子级提取式（答案选择）和生成式（GenQA）问答系统上评估了SQuArE，在多个学术和工业数据集上，结果表明它优于先前的基准，并与人工标注具有最高的相关性。",
    "tldr": "提出了一种新的问答系统评估指标SQuArE，使用多个参考答案进行句子级问答评估，实现了高度相关性的评估结果。",
    "en_tdlr": "A new evaluation metric called SQuArE is proposed for QA systems, which uses multiple reference answers for sentence-level QA evaluation and achieves highly correlated evaluation results."
}