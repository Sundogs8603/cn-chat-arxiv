{
    "title": "Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])",
    "abstract": "In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.",
    "link": "http://arxiv.org/abs/2309.07038",
    "context": "Title: Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])\nAbstract: In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.",
    "path": "papers/23/09/2309.07038.json",
    "total_tokens": 870,
    "translated_title": "高效强化学习用于跳跃式单脚机器人",
    "translated_abstract": "在这项工作中，我们考虑了一个复杂的控制问题，即使单脚机器人能够跳到任何方向，其脚下的地形可能是不平的，我们要使它达到目标位置。这是一个更大类别问题的模板，使用标准的基于优化的技术解决这些问题非常具有挑战性和计算开销。强化学习 (RL) 可能是一个有趣的替代方案，但完全从零开始学习的端到端方法是不切实际的。本文提出的解决方案是在 RL 框架中注入物理知识来指导学习过程。这种方法带来了广泛的好处，如大幅减少学习时间，并且能够学习和修正执行运动的低级控制器可能出现的错误。我们通过与基于优化和端到端 RL 方法的比较，证明了我们方法的优势。",
    "tldr": "本论文研究了如何通过在强化学习框架中注入物理知识来解决跳跃式单脚机器人的控制问题，这样可以大幅减少学习时间并且能够学习和修正可能出现的错误。"
}