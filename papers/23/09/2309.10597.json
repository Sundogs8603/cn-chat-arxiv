{
    "title": "Motif-Centric Representation Learning for Symbolic Music. (arXiv:2309.10597v1 [cs.SD])",
    "abstract": "Music motif, as a conceptual building block of composition, is crucial for music structure analysis and automatic composition. While human listeners can identify motifs easily, existing computational models fall short in representing motifs and their developments. The reason is that the nature of motifs is implicit, and the diversity of motif variations extends beyond simple repetitions and modulations. In this study, we aim to learn the implicit relationship between motifs and their variations via representation learning, using the Siamese network architecture and a pretraining and fine-tuning pipeline. A regularization-based method, VICReg, is adopted for pretraining, while contrastive learning is used for fine-tuning. Experimental results on a retrieval-based task show that these two methods complement each other, yielding an improvement of 12.6% in the area under the precision-recall curve. Lastly, we visualize the acquired motif representations, offering an intuitive comprehension",
    "link": "http://arxiv.org/abs/2309.10597",
    "context": "Title: Motif-Centric Representation Learning for Symbolic Music. (arXiv:2309.10597v1 [cs.SD])\nAbstract: Music motif, as a conceptual building block of composition, is crucial for music structure analysis and automatic composition. While human listeners can identify motifs easily, existing computational models fall short in representing motifs and their developments. The reason is that the nature of motifs is implicit, and the diversity of motif variations extends beyond simple repetitions and modulations. In this study, we aim to learn the implicit relationship between motifs and their variations via representation learning, using the Siamese network architecture and a pretraining and fine-tuning pipeline. A regularization-based method, VICReg, is adopted for pretraining, while contrastive learning is used for fine-tuning. Experimental results on a retrieval-based task show that these two methods complement each other, yielding an improvement of 12.6% in the area under the precision-recall curve. Lastly, we visualize the acquired motif representations, offering an intuitive comprehension",
    "path": "papers/23/09/2309.10597.json",
    "total_tokens": 937,
    "translated_title": "面向符号音乐的主题中心表示学习",
    "translated_abstract": "音乐主题作为音乐组合的概念构建块，在音乐结构分析和自动作曲中至关重要。尽管人类听众可以轻松识别主题，但现有的计算模型在表示主题及其发展方面存在不足。这是因为主题的特性是隐含的，而主题变化的多样性超越了简单的重复和调制。在这项研究中，我们旨在通过表示学习学习主题与其变化之间的隐含关系，使用Siamese网络架构和预训练与微调流程。采用基于正则化的VICReg方法进行预训练，而对比学习用于微调。对基于检索的任务的实验结果表明，这两种方法互补，使在精确率-召回率曲线下的面积提升了12.6%。最后，我们可视化获取的主题表示，提供直观的理解。",
    "tldr": "该论文提出了一种面向符号音乐的主题中心表示学习方法，通过Siamese网络架构和预训练与微调流程学习主题和其变化之间的隐含关系。实验证明，这两种方法相互补充，使精确率-召回率曲线下的面积提升了12.6%。最后，通过可视化获得的主题表示，提供了直观的理解。",
    "en_tdlr": "This paper proposes a motif-centric representation learning method for symbolic music, using Siamese network architecture and a pretraining and fine-tuning pipeline. Experimental results show that these methods complement each other, resulting in a 12.6% improvement in the precision-recall curve. Lastly, the acquired motif representations are visualized, providing an intuitive comprehension."
}