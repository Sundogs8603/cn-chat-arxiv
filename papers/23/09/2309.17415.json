{
    "title": "Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)",
    "abstract": "This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive \"right\" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in",
    "link": "http://arxiv.org/abs/2309.17415",
    "context": "Title: Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)\nAbstract: This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive \"right\" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in",
    "path": "papers/23/09/2309.17415.json",
    "total_tokens": 902,
    "translated_title": "直觉还是依赖？研究LLMs对冲突提示的鲁棒性",
    "translated_abstract": "本文探讨了LLMs对其内部记忆或给定提示的偏好的鲁棒性，由于噪声或任务设置，在真实应用中可能存在对立信息。为此，我们建立了一个定量的基准框架，并进行角色扮演干预来控制LLMs的偏好。具体而言，我们定义了两种鲁棒性，即事实鲁棒性和决策风格，事实鲁棒性是指从提示或记忆中识别正确事实的能力，而决策风格是基于认知理论对LLMs在进行一致选择过程中行为的分类 - 直觉型、依赖型或理性型，这里假设没有明确的“正确”答案。我们对七个开源和闭源LLMs进行了大量实验，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。尽管详细的说明可以减轻选择误导性答案的情况，但也会增加出现不明确答案的情况。",
    "tldr": "本文研究了LLMs对冲突提示的鲁棒性，发现这些模型对于误导性提示特别容易受到影响，尤其是在指导常识知识方面。",
    "en_tdlr": "This paper investigates the robustness of LLMs to conflicting prompts and finds that these models are highly susceptible to misleading prompts, especially when instructing commonsense knowledge."
}