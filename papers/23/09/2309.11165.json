{
    "title": "Assessment of Pre-Trained Models Across Languages and Grammars. (arXiv:2309.11165v1 [cs.CL])",
    "abstract": "We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.",
    "link": "http://arxiv.org/abs/2309.11165",
    "context": "Title: Assessment of Pre-Trained Models Across Languages and Grammars. (arXiv:2309.11165v1 [cs.CL])\nAbstract: We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.",
    "path": "papers/23/09/2309.11165.json",
    "total_tokens": 812,
    "translated_title": "跨语言和语法对预训练模型的评估",
    "translated_abstract": "我们提出了一种评估多语言大型语言模型（LLMs）在多形式的句法结构方面学习句法的方法。我们通过将解析视为序列标记来恢复组成和依赖结构。为此，我们选择了几个LLMs，并在13个不同的UD树库用于依赖解析和10个树库用于组成解析进行研究。我们的结果显示：（i）该框架在不同编码之间具有一致性，（ii）预训练词向量不偏好依赖语法而非组成语法表示，（iii）子词标记化是表示句法的必要条件，与基于字符的模型相反，（iv）从词向量中恢复句法时，语言在预训练数据中的出现频率比任务数据的数量更重要。",
    "tldr": "本研究通过评估预训练模型在多语言和语法上的表现，发现预训练词向量对句法表示没有偏好，而语言在预训练数据中的出现频率比任务数据的数量更重要。",
    "en_tdlr": "This study evaluates the performance of pre-trained models across languages and grammars, finding that pre-trained word vectors do not favor a specific syntactic representation and that the occurrence of a language in the pretraining data is more important than the amount of task data."
}