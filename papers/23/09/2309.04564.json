{
    "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])",
    "abstract": "Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring metho",
    "link": "http://arxiv.org/abs/2309.04564",
    "context": "Title: When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])\nAbstract: Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring metho",
    "path": "papers/23/09/2309.04564.json",
    "total_tokens": 987,
    "translated_title": "当少就意味着更多：探究数据修剪对大规模预训练语言模型( LLMS )的影响",
    "translated_abstract": "最近几年，大量的文本数据对于大型语言模型( LLMS )的发展做出了显著贡献。这些数据通常通过从互联网上抓取获取，导致预训练数据集由嘈杂的网络文本构成。过去，为了减小数据集并使其更高质量，采用了以规则为基础的手工启发式过滤器。在这项工作中，我们采取更广泛的视角，探讨了可估算的数据质量，以系统性地衡量预训练数据的质量。我们在大规模上进行了严格比较，包括使用困惑度的简单数据质量评估器，以及更复杂和计算密集的错误L2-范数和记忆化评估。这些指标用于对预训练语料库进行排序和修剪，并随后比较在这些修剪后数据集上训练的LLMs。令人惊讶的是，我们发现困惑度作为一种简单的技术优于更加计算密集的评分方法。",
    "tldr": "在这项工作中，研究人员探究了数据修剪对大规模预训练语言模型(LLMs)的影响。通过比较数据质量评估器和修剪预训练语料库后训练的LLMs，他们发现困惑度作为一种简单的技术优于更加计算密集的评分方法。"
}