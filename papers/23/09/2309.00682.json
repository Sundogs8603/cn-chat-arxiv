{
    "title": "Randomized Polar Codes for Anytime Distributed Machine Learning. (arXiv:2309.00682v1 [cs.DC])",
    "abstract": "We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.",
    "link": "http://arxiv.org/abs/2309.00682",
    "context": "Title: Randomized Polar Codes for Anytime Distributed Machine Learning. (arXiv:2309.00682v1 [cs.DC])\nAbstract: We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.",
    "path": "papers/23/09/2309.00682.json",
    "total_tokens": 846,
    "translated_title": "任意分布式机器学习的随机极化码",
    "translated_abstract": "我们提出了一个新颖的分布式计算框架，该框架能够抵抗慢计算节点，并能够进行线性操作的近似和精确计算。所提出的机制在编码计算的背景下，结合了随机摘要和极化码的概念。我们提出了一种用于处理实值数据的串行译码算法，以维持低计算复杂度用于恢复。此外，我们提供了一种即时估计器，即使可用节点输出无法译码，也能够生成可证明准确的估计结果。我们展示了该框架在各种情境下的潜在应用，如大规模矩阵乘法和黑盒优化。我们在无服务器云计算系统上实现了这些方法，并提供了数值结果来展示它们在实践中的可扩展性，包括ImageNet规模计算。",
    "tldr": "本论文提出了一个用于任意分布式机器学习的随机极化码框架，通过结合随机摘要和极化码的概念，实现了对慢计算节点的鲁棒性，并提供了适用于实值数据的低计算复杂度的串行译码算法和即时估计器，该框架在大规模矩阵乘法和黑盒优化等多个应用场景中具有潜在的应用价值。"
}