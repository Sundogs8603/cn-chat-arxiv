{
    "title": "Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])",
    "abstract": "This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope",
    "link": "http://arxiv.org/abs/2309.11089",
    "context": "Title: Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])\nAbstract: This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope",
    "path": "papers/23/09/2309.11089.json",
    "total_tokens": 991,
    "translated_title": "结合Dropout不确定性与轨迹采样的实用概率模型基于深度强化学习",
    "translated_abstract": "本文针对当前建立在神经网络上的概率模型基于强化学习（MBRL）的预测稳定性、预测准确性和控制能力进行了研究。提出了一种新颖的方法，即基于dropout的概率集成和轨迹采样（DPETS），在一个框架中通过Monte-Carlo dropout和轨迹采样来稳定地预测系统不确定性。其损失函数设计用于纠正神经网络的拟合误差，以更准确地预测概率模型。其策略中的状态传播被扩展用于滤除aleatoric不确定性，从而实现了更优越的控制能力。通过在几个Mujoco基准控制任务和一个实际机械臂操作任务下的评估，DPETS在平均回报和收敛速度上优于相关MBRL方法，同时在显著的样本效率方面表现出优异性能，胜过知名的无模型基线模型。",
    "tldr": "本论文提出了一种实用的概率模型基于深度强化学习方法，通过结合dropout不确定性和轨迹采样，稳定地预测系统不确定性，纠正神经网络的拟合误差，过滤aleatoric不确定性，从而实现了优越的控制能力。在实验评估中，该方法在多个控制任务和实际机械臂操作任务中表现出优于其他方法和模型的性能。"
}