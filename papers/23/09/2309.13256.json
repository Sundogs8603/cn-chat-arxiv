{
    "title": "Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. (arXiv:2309.13256v1 [cs.LG])",
    "abstract": "Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using be",
    "link": "http://arxiv.org/abs/2309.13256",
    "context": "Title: Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. (arXiv:2309.13256v1 [cs.LG])\nAbstract: Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using be",
    "path": "papers/23/09/2309.13256.json",
    "total_tokens": 1099,
    "translated_title": "针对预训练语言模型的少样本学习者的反向攻击防御",
    "translated_abstract": "预训练语言模型（PLM）作为少样本学习者展示出了卓越的性能。然而，在这种设置下，它们的安全风险尚未得到广泛探究。在这项工作中，我们进行了一项初步研究，表明少样本学习者的PLMs极易受到反向攻击，而现有的防御措施由于少样本情境的独特挑战而不足。为了应对这些挑战，我们提出了MDP，一种新颖、轻量级、可插拔且有效的预训练语言模型少样本学习者的防御方法。具体而言，MDP利用了被污染样本和清洁样本之间的掩码敏感性差距：参考有限的少样本数据作为分布锚点，它比较不同掩码下给定样本的表示，并识别出具有显著变化的被污染样本。我们通过分析表明，MDP对于攻击者在攻击效果和检测逃避性之间产生了有趣的进退两难。实证评估使用be",
    "tldr": "本研究针对预训练语言模型作为少样本学习者的安全风险进行了初步研究，发现其极易受到反向攻击。为了应对这个问题，我们提出了一种名为MDP的轻量级、可插拔且有效的防御方法，通过利用被污染样本和清洁样本之间的掩码敏感性差距来识别污染样本。实证评估结果表明，MDP在攻击效果和检测逃避性之间形成了进退两难。"
}