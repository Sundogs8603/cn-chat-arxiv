{
    "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.",
    "link": "http://arxiv.org/abs/2309.04175",
    "context": "Title: Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.",
    "path": "papers/23/09/2309.04175.json",
    "total_tokens": 885,
    "translated_title": "使用结构化医学知识库对大型语言模型进行知识调整，实现中文可靠响应生成",
    "translated_abstract": "大型语言模型（LLMs）在一般领域的自然语言处理（NLP）任务中取得了显著的成功。然而，由于领域知识的限制，LLMs有时会生成具有关于医学事实的幻觉的响应。这些缺点在医学背景下使用LLMs时存在潜在的风险。为了解决这个挑战，我们提出了知识调整，通过利用结构化医学知识库来使LLMs能够高效掌握领域知识并实现可靠的响应生成。我们还发布了cMedKnowQA，一个从医学知识库构建的中文医学知识问答数据集，以评估LLMs的医学知识水平。实验结果表明，经过cMedKnowQA的知识调整的LLMs，在响应生成方面可以表现出比原始指导调整更高水平的准确性，并为LLMs的领域适应提供了一种可靠的新方式。",
    "tldr": "本研究提出了一种使用结构化医学知识库进行知识调整的方法，以提高大型语言模型在中文可靠响应生成方面的准确性和领域适应能力。"
}