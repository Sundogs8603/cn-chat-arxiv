{
    "title": "PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])",
    "abstract": "In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: \"prompt-",
    "link": "http://arxiv.org/abs/2309.12109",
    "context": "Title: PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])\nAbstract: In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: \"prompt-",
    "path": "papers/23/09/2309.12109.json",
    "total_tokens": 789,
    "translated_title": "PEFTT: 多参数效率的低资源藏语预训练语言模型微调",
    "translated_abstract": "在大语言模型时代，传统模型训练对于普通用户和机构而言越来越难以想象。在这些模型上，对于高资源语言的高效微调的探索是一个不可否认的趋势，而这种趋势正逐渐流行起来。然而，对于低资源语言，如藏语，目前的研究非常有限。藏语自然语言处理的研究本就稀缺而受限。尽管目前由于其低资源性质，还没有现有的大语言模型用于藏语，但这一天毫无疑问会到来。因此，对于像藏语这样的低资源语言模型的高效微调研究非常必要。我们的研究可以作为填补这一重要空白的参考。",
    "tldr": "本研究在低资源语言模型如藏语中进行了高效微调策略的研究，填补了这一重要空白。"
}