{
    "title": "XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection. (arXiv:2309.14677v1 [cs.CR])",
    "abstract": "With the advancement of deep learning (DL) in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Nonetheless, such existing works lack the effective representation that can retain the non-sequential semantic characteristics and contextual relationship of source code attributes. Hence, in this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and Graph Neural Network (GCN) to detect software vulnerabilities. By jointly training the CodeBERT and GCN modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-s",
    "link": "http://arxiv.org/abs/2309.14677",
    "context": "Title: XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection. (arXiv:2309.14677v1 [cs.CR])\nAbstract: With the advancement of deep learning (DL) in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Nonetheless, such existing works lack the effective representation that can retain the non-sequential semantic characteristics and contextual relationship of source code attributes. Hence, in this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and Graph Neural Network (GCN) to detect software vulnerabilities. By jointly training the CodeBERT and GCN modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-s",
    "path": "papers/23/09/2309.14677.json",
    "total_tokens": 894,
    "translated_title": "XGV-BERT:利用上下文化语言模型和图神经网络进行高效的软件漏洞检测",
    "translated_abstract": "随着深度学习在各个领域的发展，通过数据驱动方法揭示软件漏洞的尝试越来越多。然而，现有的工作缺乏能够保留源代码属性的非顺序语义特征和上下文关系的有效表示。因此，在这项工作中，我们提出了XGV-BERT，这是一种结合了预训练的CodeBERT模型和图神经网络（GCN）用于检测软件漏洞的框架。通过在XGV-BERT中联合训练CodeBERT和GCN模块，所提出的模型利用了大规模预训练、利用庞大原始数据和通过图卷积学习训练数据的迁移学习的优势。研究结果表明，与VulDeePecker和SySeVR等两种现有方法相比，XGV-BERT方法显著提高了漏洞检测的准确性。对于VulDeePecker数据集，XGV-BERT取得了令人印象深刻的F1-s",
    "tldr": "XGV-BERT提出了一种结合了预训练的CodeBERT模型和图神经网络（GCN）的框架，用于高效的软件漏洞检测。研究结果表明，XGV-BERT相比其他方法显著提高了漏洞检测的准确性。"
}