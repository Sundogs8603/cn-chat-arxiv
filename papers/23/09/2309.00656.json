{
    "title": "Local and adaptive mirror descents in extensive-form games. (arXiv:2309.00656v1 [cs.GT])",
    "abstract": "We study how to learn $\\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\\tilde{\\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best ",
    "link": "http://arxiv.org/abs/2309.00656",
    "context": "Title: Local and adaptive mirror descents in extensive-form games. (arXiv:2309.00656v1 [cs.GT])\nAbstract: We study how to learn $\\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\\tilde{\\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best ",
    "path": "papers/23/09/2309.00656.json",
    "total_tokens": 1230,
    "translated_title": "本文研究在带有轨迹反馈的零和不完全信息博弈中如何学习ε-最优策略。在这种情况下，玩家根据他们在固定数量的回合中观察到的信息依次更新策略。现有的方法由于使用了重要性采样来对动作序列进行估计，导致方差较高。为了减小这种方差，我们考虑使用一种固定采样方法，即玩家在随时间变化的情况下仍然更新策略，但观察到的信息是通过给定的固定采样策略获得的。我们的方法基于一种自适应的在线镜像下降（OMD）算法，该算法将OMD应用于每个信息集，使用逐渐减小的学习率和正则化损失。我们证明了这种方法在高概率下具有收敛速度为$\\tilde{\\mathcal{O}}(T^{-1/2})$，并在应用最佳策略时对游戏参数具有近乎最优的依赖关系。",
    "translated_abstract": "本文研究在带有轨迹反馈的零和不完全信息博弈中学习ε-最优策略的问题。现有方法由于使用了重要性采样，存在方差较高的问题。为了减小方差，我们提出了一种固定采样方法，使用固定采样策略来观察信息。我们的方法基于自适应的在线镜像下降算法，对每个信息集进行局部更新，并使用逐渐减小的学习率和正则化损失。我们证明了该方法在高概率下具有收敛速度为$\\tilde{\\mathcal{O}}(T^{-1/2})$，并在最佳策略下对游戏参数具有近乎最优的依赖关系。",
    "tldr": "本文研究了在零和不完全信息博弈中学习ε-最优策略的问题。通过提出一种固定采样方法，并使用自适应的在线镜像下降算法进行局部更新，我们取得了收敛速度为$\\tilde{\\mathcal{O}}(T^{-1/2})$的结果，并在最佳策略下对游戏参数具有近乎最优的依赖关系。",
    "en_tdlr": "This paper studies how to learn ε-optimal strategies in zero-sum imperfect information games. By proposing a fixed sampling approach and using an adaptive online mirror descent algorithm for local updates, a convergence rate of $\\tilde{\\mathcal{O}}(T^{-1/2})$ is achieved, with near-optimal dependence on the game parameters when applied with the best strategy."
}