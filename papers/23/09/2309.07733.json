{
    "title": "Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features. (arXiv:2309.07733v1 [cs.CL])",
    "abstract": "Recent advances in eXplainable AI (XAI) have provided new insights into how models for vision, language, and tabular data operate. However, few approaches exist for understanding speech models. Existing work focuses on a few spoken language understanding (SLU) tasks, and explanations are difficult to interpret for most users. We introduce a new approach to explain speech classification models. We generate easy-to-interpret explanations via input perturbation on two information levels. 1) Word-level explanations reveal how each word-related audio segment impacts the outcome. 2) Paralinguistic features (e.g., prosody and background noise) answer the counterfactual: ``What would the model prediction be if we edited the audio signal in this way?'' We validate our approach by explaining two state-of-the-art SLU models on two speech classification tasks in English and Italian. Our findings demonstrate that the explanations are faithful to the model's inner workings and plausible to humans. O",
    "link": "http://arxiv.org/abs/2309.07733",
    "context": "Title: Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features. (arXiv:2309.07733v1 [cs.CL])\nAbstract: Recent advances in eXplainable AI (XAI) have provided new insights into how models for vision, language, and tabular data operate. However, few approaches exist for understanding speech models. Existing work focuses on a few spoken language understanding (SLU) tasks, and explanations are difficult to interpret for most users. We introduce a new approach to explain speech classification models. We generate easy-to-interpret explanations via input perturbation on two information levels. 1) Word-level explanations reveal how each word-related audio segment impacts the outcome. 2) Paralinguistic features (e.g., prosody and background noise) answer the counterfactual: ``What would the model prediction be if we edited the audio signal in this way?'' We validate our approach by explaining two state-of-the-art SLU models on two speech classification tasks in English and Italian. Our findings demonstrate that the explanations are faithful to the model's inner workings and plausible to humans. O",
    "path": "papers/23/09/2309.07733.json",
    "total_tokens": 971,
    "translated_title": "通过单词级音频片段和语音特征解释语音分类模型",
    "translated_abstract": "最近可解释的人工智能（XAI）的进展为我们对视觉、语言和表格数据模型的运行方式提供了新的见解。然而，很少有方法用于理解语音模型。现有工作专注于一些口语理解任务，并且对大多数用户来说，解释难以解释。我们提出了一种解释语音分类模型的新方法。我们通过在两个信息级别上对输入进行扰动来生成易于解释的解释。1) 单词级解释显示每个与单词相关的音频片段如何影响结果。2) 语音特征（例如韵律和背景噪音）回答了反事实问题：“如果我们以这种方式编辑音频信号，模型的预测会是什么？”我们通过解释两个最先进的英语和意大利语口语理解模型的两个口语分类任务来验证我们的方法。我们的发现表明这些解释符合模型的内部工作并且对人类而言是可信的。",
    "tldr": "该论文提出了一种解释语音分类模型的新方法，通过在单词级别上解释音频段落和语音特征。该方法通过对输入进行扰动，生成易于理解的解释，并回答了如果修改了音频信号会对模型预测产生怎样影响的问题。通过验证，发现这些解释与模型内部工作相符，且对人类而言是可信的。",
    "en_tdlr": "This paper introduces a new approach to explain speech classification models, by explaining word-level audio segments and paralinguistic features. The approach generates easy-to-interpret explanations by perturbing the input and answers the question of how modifying the audio signal would affect the model's prediction. The findings validate that the explanations align with the model's inner workings and are plausible to humans."
}