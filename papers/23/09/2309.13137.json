{
    "title": "Trading-off Mutual Information on Feature Aggregation for Face Recognition. (arXiv:2309.13137v1 [cs.CV])",
    "abstract": "Despite the advances in the field of Face Recognition (FR), the precision of these methods is not yet sufficient. To improve the FR performance, this paper proposes a technique to aggregate the outputs of two state-of-the-art (SOTA) deep FR models, namely ArcFace and AdaFace. In our approach, we leverage the transformer attention mechanism to exploit the relationship between different parts of two feature maps. By doing so, we aim to enhance the overall discriminative power of the FR system. One of the challenges in feature aggregation is the effective modeling of both local and global dependencies. Conventional transformers are known for their ability to capture long-range dependencies, but they often struggle with modeling local dependencies accurately. To address this limitation, we augment the self-attention mechanism to capture both local and global dependencies effectively. This allows our model to take advantage of the overlapping receptive fields present in corresponding locati",
    "link": "http://arxiv.org/abs/2309.13137",
    "context": "Title: Trading-off Mutual Information on Feature Aggregation for Face Recognition. (arXiv:2309.13137v1 [cs.CV])\nAbstract: Despite the advances in the field of Face Recognition (FR), the precision of these methods is not yet sufficient. To improve the FR performance, this paper proposes a technique to aggregate the outputs of two state-of-the-art (SOTA) deep FR models, namely ArcFace and AdaFace. In our approach, we leverage the transformer attention mechanism to exploit the relationship between different parts of two feature maps. By doing so, we aim to enhance the overall discriminative power of the FR system. One of the challenges in feature aggregation is the effective modeling of both local and global dependencies. Conventional transformers are known for their ability to capture long-range dependencies, but they often struggle with modeling local dependencies accurately. To address this limitation, we augment the self-attention mechanism to capture both local and global dependencies effectively. This allows our model to take advantage of the overlapping receptive fields present in corresponding locati",
    "path": "papers/23/09/2309.13137.json",
    "total_tokens": 910,
    "translated_title": "牺牲特征聚合中的互信息用于人脸识别",
    "translated_abstract": "尽管人脸识别技术取得了很大进展，但这些方法的准确度还不够。为了提高识别性能，本文提出了一种技术，可以聚合两个最先进的人脸识别模型ArcFace和AdaFace的输出。在我们的方法中，我们利用变换器注意机制来利用两个特征图的不同部分之间的关系。通过这样做，我们的目标是增强整个人脸识别系统的判别能力。特征聚合中的一个挑战是有效建模局部和全局依赖关系。传统的变换器以捕捉长程依赖性而闻名，但却往往难以准确建模局部依赖关系。为了解决这个问题，我们改进了自注意机制，以有效捕捉局部和全局依赖关系。这使得我们的模型能够利用对应位置中的重叠接受域。",
    "tldr": "本文提出了一种技术，用于聚合ArcFace和AdaFace两个最先进的人脸识别模型的输出，通过利用变换器注意机制来增强整体人脸识别系统的判别能力。同时，改进了自注意机制以有效捕捉局部和全局依赖关系。",
    "en_tdlr": "This paper proposes a technique for aggregating the outputs of two state-of-the-art face recognition models, ArcFace and AdaFace, using the transformer attention mechanism to enhance the overall discriminative power of the face recognition system. Furthermore, an improved self-attention mechanism is introduced to effectively capture both local and global dependencies."
}