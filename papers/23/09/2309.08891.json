{
    "title": "V2CE: Video to Continuous Events Simulator. (arXiv:2309.08891v1 [cs.CV])",
    "abstract": "Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuo",
    "link": "http://arxiv.org/abs/2309.08891",
    "context": "Title: V2CE: Video to Continuous Events Simulator. (arXiv:2309.08891v1 [cs.CV])\nAbstract: Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuo",
    "path": "papers/23/09/2309.08891.json",
    "total_tokens": 912,
    "translated_title": "V2CE: 视频到连续事件模拟器",
    "translated_abstract": "基于动态视觉传感器（DVS）的解决方案近年来在各种计算机视觉任务中受到了广泛关注，其在动态范围、时间分辨率和推理速度方面具有显著优势。然而，与RGB相机等主动像素传感器（APS）设备相比，DVS作为一个相对新兴的视觉传感器，缺乏充足的标记数据集。之前将APS数据转换为事件的努力往往要面对许多问题，如与真实事件之间的领域转移、缺乏定量验证和时间轴内的分层问题。在本文中，我们提出了一种新颖的方法来从多个角度将视频转换为事件流，考虑到DVS的特定特征。一系列精心设计的损失函数显著提高了生成的事件体素的质量。我们还提出了一种新颖的局部动态感知时间戳推断策略，以准确地从事件体素中恢复事件时间戳。",
    "tldr": "本文介绍了一种视频到事件流的转换方法，考虑到DVS的特定特征，通过精心设计的损失函数提高生成的事件体素的质量。同时，提出了一种新颖的局部动态感知时间戳推断策略，以准确地恢复事件时间戳。",
    "en_tdlr": "This paper presents a novel method for converting video to event streams, taking into account the specific characteristics of DVS. The quality of generated event voxels is significantly improved through carefully designed loss functions. Additionally, a novel local dynamic-aware timestamp inference strategy is proposed to accurately recover event timestamps."
}