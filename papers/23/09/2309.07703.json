{
    "title": "Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])",
    "abstract": "Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau",
    "link": "http://arxiv.org/abs/2309.07703",
    "context": "Title: Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])\nAbstract: Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau",
    "path": "papers/23/09/2309.07703.json",
    "total_tokens": 786,
    "translated_title": "测量因果控制的因果熵和信息增益",
    "translated_abstract": "人工智能模型和方法通常缺乏因果可解释性。尽管解释性机器学习（IML）方法取得了进展，但它们经常将重要性赋予那些对结果变量没有因果影响的特征。在模型训练之前或之后，选择因果相关的特征将提供一种解决方案。利用信息论量进行特征选择的方法在识别统计相关特征方面非常成功。然而，它们所基于的信息论量不包含因果关系，因此在这种情况下不适用。为了解决这个挑战，本文提出了能够考虑系统因果结构的信息论量，可以用于评估某个给定结果变量的因果重要性。具体来说，我们引入了因果熵和因果互信息的因果版本。",
    "tldr": "本文提出了一种考虑因果结构的信息论量，用于评估某个特定结果变量的因果重要性，解决了因果可解释性的挑战。",
    "en_tdlr": "This article proposes causal versions of entropy and mutual information that incorporate the causal structure of the system, offering a solution to the challenge of causal interpretability in evaluating the causal importance of features for a given outcome variable."
}