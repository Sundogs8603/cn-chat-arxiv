{
    "title": "Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration. (arXiv:2309.09408v2 [cs.RO] UPDATED)",
    "abstract": "Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), a",
    "link": "http://arxiv.org/abs/2309.09408",
    "context": "Title: Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration. (arXiv:2309.09408v2 [cs.RO] UPDATED)\nAbstract: Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), a",
    "path": "papers/23/09/2309.09408.json",
    "total_tokens": 939,
    "translated_title": "引导在线蒸馏：通过离线演示来促进安全强化学习",
    "translated_abstract": "安全强化学习旨在找到满足成本约束的高奖励策略。从零开始进行学习时，安全强化学习代理往往过于保守，这阻碍了探索并限制了整体性能。在许多现实任务中，例如自动驾驶，可以获得大规模的专家演示数据。我们认为从离线数据中提取专家策略来引导在线探索是解决保守性问题的一种有希望的解决方案。大容量模型，例如决策转换器（DT），已被证明在离线策略学习中表现出色。然而，在真实场景中收集的数据很少包含危险情况（例如碰撞），这使得策略很难学习安全概念。此外，这些大规模策略网络无法满足像自动驾驶这样的真实任务推理时间的计算速度要求。为此，我们提出了引导在线蒸馏（GOLD）的方法。",
    "tldr": "本研究提出了Guided Online Distillation (GOLD)方法，通过从离线演示数据中提取专家策略来引导在线探索，解决了安全强化学习中保守性问题，并通过决策转换器模型对离线策略学习进行有效的高容量建模。",
    "en_tdlr": "This paper presents Guided Online Distillation (GOLD) method which addresses the conservatism issue in safe reinforcement learning by extracting expert policy from offline demonstration data to guide online exploration, and effectively models offline policy learning using decision transformers."
}