{
    "title": "Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v1 [cs.LG])",
    "abstract": "We formulate a data independent latent space regularisation constraint for general unsupervised autoencoders. The regularisation rests on sampling the autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre quadrature. Revisiting this classic enables to prove that regularised autoencoders ensure a one-to-one re-embedding of the initial data manifold to its latent representation. Demonstrations show that prior proposed regularisation strategies, such as contractive autoencoding, cause topological defects already for simple examples, and so do convolutional based (variational) autoencoders. In contrast, topological preservation is ensured already by standard multilayer perceptron neural networks when being regularised due to our contribution. This observation extends through the classic FashionMNIST dataset up to real world encoding problems for MRI brain scans, suggesting that, across disciplines, reliable low dimensional representations of complex high-dimensiona",
    "link": "http://arxiv.org/abs/2309.08228",
    "context": "Title: Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v1 [cs.LG])\nAbstract: We formulate a data independent latent space regularisation constraint for general unsupervised autoencoders. The regularisation rests on sampling the autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre quadrature. Revisiting this classic enables to prove that regularised autoencoders ensure a one-to-one re-embedding of the initial data manifold to its latent representation. Demonstrations show that prior proposed regularisation strategies, such as contractive autoencoding, cause topological defects already for simple examples, and so do convolutional based (variational) autoencoders. In contrast, topological preservation is ensured already by standard multilayer perceptron neural networks when being regularised due to our contribution. This observation extends through the classic FashionMNIST dataset up to real world encoding problems for MRI brain scans, suggesting that, across disciplines, reliable low dimensional representations of complex high-dimensiona",
    "path": "papers/23/09/2309.08228.json",
    "total_tokens": 954,
    "translated_title": "在高斯－勒让德节点上由于潜在空间正则化而压缩中保持拓扑数据结构完整性的确保",
    "translated_abstract": "我们为一般的无监督自编码器制定了一个数据无关的潜在空间正则化约束。该正则化基于在勒让德节点上对自编码器的雅可比矩阵进行采样，这些节点是高斯-勒让德积分的中心。重新审视这个经典问题能够证明，经过正则化的自编码器能够将初始数据流形一对一地重新嵌入到其潜在表示中。实验证明，之前提出的正则化策略（如收缩自编码）在简单示例中已经导致了拓扑缺陷，而基于卷积的（变分）自编码器也是如此。相比之下，通过我们的贡献，标准的多层感知器神经网络在正则化的情况下已经确保了拓扑完整性。这个观察结果适用于经典的FashionMNIST数据集以及MRI脑部扫描的真实世界编码问题，这表明在各个领域中，对于复杂的高维数据，可靠的低维表示已得以确保。",
    "tldr": "通过在高斯-勒让德节点上进行潜在空间正则化，我们的研究提出了一种新的无监督自编码器，能够确保在压缩过程中保持拓扑数据结构的完整性。",
    "en_tdlr": "By regularizing the latent space using Gauss-Legendre nodes, our research introduces a novel unsupervised autoencoder that ensures the preservation of topological data structures during compression."
}