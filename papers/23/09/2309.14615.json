{
    "title": "Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents. (arXiv:2309.14615v1 [cs.LG])",
    "abstract": "In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a \"gray-box\" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Netwo",
    "link": "http://arxiv.org/abs/2309.14615",
    "context": "Title: Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents. (arXiv:2309.14615v1 [cs.LG])\nAbstract: In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a \"gray-box\" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Netwo",
    "path": "papers/23/09/2309.14615.json",
    "total_tokens": 934,
    "translated_title": "深度强化学习交易代理的灰盒对抗攻击",
    "translated_abstract": "近年来，深度强化学习（Deep RL）已成功应用于诸如复杂游戏、自动驾驶汽车和聊天机器人等许多系统中，其中一个有趣的应用案例是将其作为自动化股票交易代理。一般来说，任何自动化交易代理都容易受到交易环境中的对手的操纵，因此研究其鲁棒性对于其实践成功至关重要。然而，用于研究RL鲁棒性的典型机制，即基于白盒梯度基础的对抗样本生成技术（如FGSM），对于这种用例来说已经过时，因为模型受到安全的国际交易所API的保护，如纳斯达克。在这项研究中，我们证明了一种“灰盒”方法可以攻击基于Deep RL的交易代理，仅通过在同一股票市场进行交易，而无需额外接触交易代理。在我们提出的方法中，对手代理使用了一个混合的深度神经网络",
    "tldr": "本研究展示了一种通过在同一股票市场进行交易的方式，利用灰盒方法对基于深度强化学习的交易代理进行攻击的可能性。这种方法可以应对交易代理受到对手操纵的问题。",
    "en_tdlr": "This research demonstrates the possibility of attacking deep reinforcement learning-based trading agents using a gray-box approach, by trading in the same stock market, providing a solution to the problem of adversaries manipulating trading agents."
}