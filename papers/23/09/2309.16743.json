{
    "title": "High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])",
    "abstract": "Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced",
    "link": "http://arxiv.org/abs/2309.16743",
    "context": "Title: High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])\nAbstract: Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced",
    "path": "papers/23/09/2309.16743.json",
    "total_tokens": 1019,
    "translated_title": "从大型集合运行中高通量训练深度代理",
    "translated_abstract": "近年来，深度学习方法在加速数值解算器方面取得了突破，这些方法可以提供对物理世界的忠实但计算密集型的模拟。这些深度代理通常通过同一解算器生成的有限量的数据有监督地进行训练，而我们提出了一个开源框架，可以从大量集合运行的模拟中在线训练这些模型。它利用多个级别的并行性来生成丰富的数据集，并通过直接流式传输生成的数据来避免I/O瓶颈和存储问题。训练储备池在最大化GPU吞吐量的同时，减轻了流式传输的固有偏差。实验结果表明，通过提出的方法，在2小时内能够在8TB的数据上训练完全连接网络作为热方程的代理，相比传统的离线过程，准确率提升了47％，批量吞吐量提高了13倍。",
    "tldr": "该论文提出了一个用于加速数值解算器的深度代理的高通量训练方法，通过从大量集合运行的模拟中在线训练模型，利用多级并行性生成丰富的数据集，直接流式传输数据以避免I/O瓶颈和存储问题，同时使用训练储备池来减轻流式传输的固有偏差。实验结果显示，使用该方法能够在2小时内在8TB的数据上训练出准确率提升了47%、批量吞吐量提高了13倍的热方程代理网络。",
    "en_tdlr": "This paper proposes a high throughput training method for deep surrogates in accelerating numerical solvers. It enables online training from a large ensemble run of simulations, leveraging parallelism and streaming to avoid I/O bottlenecks and storage issues. Experimental results show that the proposed approach achieves a 47% improvement in accuracy and a 13-fold increase in batch throughput compared to traditional offline procedures."
}