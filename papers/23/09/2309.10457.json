{
    "title": "Diffusion-based speech enhancement with a weighted generative-supervised learning loss. (arXiv:2309.10457v1 [cs.CV])",
    "abstract": "Diffusion-based generative models have recently gained attention in speech enhancement (SE), providing an alternative to conventional supervised methods. These models transform clean speech training samples into Gaussian noise centered at noisy speech, and subsequently learn a parameterized model to reverse this process, conditionally on noisy speech. Unlike supervised methods, generative-based SE approaches usually rely solely on an unsupervised loss, which may result in less efficient incorporation of conditioned noisy speech. To address this issue, we propose augmenting the original diffusion training objective with a mean squared error (MSE) loss, measuring the discrepancy between estimated enhanced speech and ground-truth clean speech at each reverse process iteration. Experimental results demonstrate the effectiveness of our proposed methodology.",
    "link": "http://arxiv.org/abs/2309.10457",
    "context": "Title: Diffusion-based speech enhancement with a weighted generative-supervised learning loss. (arXiv:2309.10457v1 [cs.CV])\nAbstract: Diffusion-based generative models have recently gained attention in speech enhancement (SE), providing an alternative to conventional supervised methods. These models transform clean speech training samples into Gaussian noise centered at noisy speech, and subsequently learn a parameterized model to reverse this process, conditionally on noisy speech. Unlike supervised methods, generative-based SE approaches usually rely solely on an unsupervised loss, which may result in less efficient incorporation of conditioned noisy speech. To address this issue, we propose augmenting the original diffusion training objective with a mean squared error (MSE) loss, measuring the discrepancy between estimated enhanced speech and ground-truth clean speech at each reverse process iteration. Experimental results demonstrate the effectiveness of our proposed methodology.",
    "path": "papers/23/09/2309.10457.json",
    "total_tokens": 804,
    "translated_title": "基于扩散的语音增强方法与加权生成-监督学习损失",
    "translated_abstract": "最近，基于扩散的生成模型在语音增强中受到关注，提供了一种替代传统监督方法的方法。这些模型将干净语音训练样本转化为以噪声语音为中心的高斯噪声，并在此基础上学习一个参数化模型来逆转这个过程，有条件地根据噪声语音进行预测。与监督方法不同，基于生成的语音增强方法通常仅依赖于无监督损失，这可能导致对有条件噪声语音的融合不够高效。为了解决这个问题，我们提出使用均方差（MSE）损失来增强原始的扩散训练目标，在每次逆转过程迭代中，测量估计增强语音与真实干净语音之间的差异。实验结果证明了我们所提出方法的有效性。",
    "tldr": "本文通过在扩散训练目标中增加均方差（MSE）损失，改进了基于扩散的语音增强方法。实验证明了该方法的有效性。",
    "en_tdlr": "This paper improves the diffusion-based speech enhancement method by adding mean squared error (MSE) loss to the training objective. Experimental results demonstrate the effectiveness of this method."
}