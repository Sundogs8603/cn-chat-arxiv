{
    "title": "Neural Machine Translation Models Can Learn to be Few-shot Learners. (arXiv:2309.08590v1 [cs.CL])",
    "abstract": "The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
    "link": "http://arxiv.org/abs/2309.08590",
    "context": "Title: Neural Machine Translation Models Can Learn to be Few-shot Learners. (arXiv:2309.08590v1 [cs.CL])\nAbstract: The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
    "path": "papers/23/09/2309.08590.json",
    "total_tokens": 888,
    "translated_title": "神经机器翻译模型可以学会成为少样本学习器",
    "translated_abstract": "大型语言模型的新兴能力是使用少量示例来学习在新领域和任务中的表现，也称为上下文学习（ICL）。在这项工作中，我们展示了一个更小的模型可以通过微调向专门的训练目标进行ICL的训练，在神经机器翻译的领域自适应任务上进行演示。通过ICL的能力，模型可以利用相关的少样本示例调整其输出以适应该领域。我们比较了这种领域自适应方法与传统的监督技术以及具有40B参数的大型语言模型的ICL的质量。我们的方法可以在多个领域中进行高效的批量推理，并在翻译质量和即时适应率方面优于最先进的基准方法，即在展示单个示例后能够重现特定术语的能力。",
    "tldr": "本研究展示了只需进行微调就可以训练一种更小的模型，使其具备上下文学习的能力，即使用少样本示例进行自适应，从而提高神经机器翻译的领域自适应任务的效果，并超过了传统监督技术和大型语言模型的表现。"
}