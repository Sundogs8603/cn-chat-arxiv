{
    "title": "Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts. (arXiv:2309.13850v1 [stat.ML])",
    "abstract": "Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\\ast}$ becomes unknown and the true mode",
    "link": "http://arxiv.org/abs/2309.13850",
    "context": "Title: Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts. (arXiv:2309.13850v1 [stat.ML])\nAbstract: Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\\ast}$ becomes unknown and the true mode",
    "path": "papers/23/09/2309.13850.json",
    "total_tokens": 859,
    "translated_title": "统计角度下的前K稀疏Softmax门控混合专家",
    "translated_abstract": "前K稀疏softmax门控混合专家被广泛用于在不增加计算成本的情况下扩展大规模深度学习架构。尽管在现实应用中非常受欢迎，但对该门控函数的理论理解仍然是一个未解决的问题。主要挑战来自于前K稀疏softmax门控函数的结构，它将输入空间划分为具有不同行为的多个区域。通过专注于高斯混合专家，我们对前K稀疏softmax门控函数对密度和参数估计的影响建立了理论结果。我们的结果依赖于定义参数之间的新损失函数，以捕捉输入区域的不同行为。当真实专家数量$k_{\\ast}$已知时，我们证明了密度和参数估计的收敛速度都与样本量成正比。然而，当$k_{\\ast}$变为未知且真实模式时",
    "tldr": "该论文研究前K稀疏softmax门控混合专家在密度和参数估计方面的作用，通过定义新的损失函数，探讨了输入区域的不同行为。研究发现，在真实专家数量已知的情况下，密度和参数估计的收敛速度与样本量成正比，但当真实模式未知时"
}