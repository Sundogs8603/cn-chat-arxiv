{
    "title": "Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])",
    "abstract": "One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multiling",
    "link": "http://arxiv.org/abs/2309.10272",
    "context": "Title: Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])\nAbstract: One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multiling",
    "path": "papers/23/09/2309.10272.json",
    "total_tokens": 1027,
    "translated_title": "混合Distil-BERT: 用于孟加拉语、英语和印地语的混合语言建模",
    "translated_abstract": "在自然语言处理领域中，文本分类是最受欢迎的下游任务之一。当文本是混合编码时，文本分类任务变得更加困难。虽然在预训练过程中没有接触到这种文本，但不同的BERT模型已经成功地解决了混合编码的自然语言处理挑战。再次，为了提高性能，混合编码自然语言处理模型已经依赖于将合成数据与真实数据相结合。当BERT模型使用相应的混合编码语言进行预训练时，了解它们的性能受到了怎样的影响是至关重要的。在本文中，我们介绍了Tri-Distil-BERT，一个在孟加拉语、英语和印地语上预训练的多语言模型，以及Mixed-Distil-BERT，一个在混合编码数据上微调的模型。这两个模型在多个自然语言处理任务上进行了评估，并展示出与更大的模型如mBERT和XLM-R相竞争的性能。我们的两层预训练方法为多语言任务提供了高效的替代选择。",
    "tldr": "本文介绍了Tri-Distil-BERT和Mixed-Distil-BERT两个模型，Tri-Distil-BERT是一个在孟加拉语、英语和印地语上预训练的多语言模型，Mixed-Distil-BERT是一个在混合编码数据上微调的模型。这两个模型在多个自然语言处理任务上表现出与更大的模型相竞争的性能。"
}