{
    "title": "XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])",
    "abstract": "Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark",
    "link": "http://arxiv.org/abs/2309.11063",
    "context": "Title: XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])\nAbstract: Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark",
    "path": "papers/23/09/2309.11063.json",
    "total_tokens": 881,
    "translated_title": "XATU: 面向可解释性文本更新的细粒度基于指令的基准测试",
    "translated_abstract": "文本编辑是一个关键的任务，涉及修改文本以更好地与用户意图对齐。然而，现有的文本编辑基准数据集在提供粗粒度指令方面存在局限性。因此，尽管编辑后的输出似乎合理，但往往偏离了黄金参考中列出的预期更改，导致评估分数较低。为了全面调查大型语言模型的文本编辑能力，本文引入了XATU，这是第一个专门为细粒度基于指令的可解释性文本编辑而设计的基准测试。XATU涵盖了广泛的主题和文本类型，包括词汇、句法、语义和知识密集型的编辑。为了增强可解释性，我们利用高质量的数据源和人工注释，生成了一个包含细粒度指令和黄金标准编辑说明的基准测试。通过评估现有的开放和封闭的大型语言模型对我们的基准测试进行对比",
    "tldr": "XATU是第一个细粒度基于指令的可解释性文本编辑基准测试，涵盖广泛的编辑类型，并通过引入细粒度指令和黄金标准编辑说明来提高可解释性。"
}