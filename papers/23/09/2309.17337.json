{
    "title": "Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])",
    "abstract": "While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \\emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled",
    "link": "http://arxiv.org/abs/2309.17337",
    "context": "Title: Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])\nAbstract: While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \\emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled",
    "path": "papers/23/09/2309.17337.json",
    "total_tokens": 893,
    "translated_title": "向实现管道意识的机器学习公平性迈进：开发实际指南和工具的研究议程。",
    "translated_abstract": "虽然算法公平性是一个蓬勃发展的研究领域，但在实践中，减少偏见问题常常被简化为通过强制执行任意选择的公平性度量标准来实现，要么是在优化阶段强制执行公平性约束，要么是在后处理模型输出时，或者通过操纵训练数据。最近的工作呼吁机器学习社区采取更全面的方法来解决公平性问题，通过系统地研究通过机器学习流程中所做的许多设计选择，并确定针对问题根本原因而不是其症状的干预措施。尽管我们赞同这种基于管道的方法是在实际场景中解决算法不公平性最合适的方法，但我们认为目前几乎没有方法在实践中\"实施\"这种方法。根据我们作为教育者和实践者的经验，我们首先证明了没有清晰的指南和工具包，即使拥有专业的机器学习知识的个人也会遇到困难。",
    "tldr": "该论文提出了一种实现机器学习公平性的管道意识方法，并强调需要指南和工具来在实践中应用这种方法。",
    "en_tdlr": "This paper presents a pipeline-aware approach to operationalize fairness in machine learning, emphasizing the need for guidelines and tools to apply this approach in practice."
}