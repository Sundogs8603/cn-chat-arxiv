{
    "title": "Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)",
    "abstract": "Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,7",
    "link": "http://arxiv.org/abs/2309.09507",
    "context": "Title: Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)\nAbstract: Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,7",
    "path": "papers/23/09/2309.09507.json",
    "total_tokens": 879,
    "translated_title": "通过准确性预测器修剪大型语言模型",
    "translated_abstract": "大型语言模型(LLMs)具有数百亿个参数（甚至更多），在各种自然语言处理任务中展示出了令人印象深刻的能力。然而，庞大的模型规模给训练、推理和部署带来了挑战，因此有必要对模型进行压缩。目前，大部分LLMs模型压缩需要手动设计修剪特性，存在诸如复杂的优化流程和难以保留模型部分能力的问题。因此，我们提出了一种新的修剪方法：首先，建立一组特定数量的架构-准确性对的训练集，然后训练一个非神经模型作为准确性预测器。通过使用准确性预测器进一步优化搜索空间和搜索，可以自动选择最优模型。实验结果显示我们提出的方法是有效且高效的。与基准模型相比，在Wikitext2和PTB上的困惑度（PPL）分别下降了9.48%和5.7%。",
    "tldr": "本文提出了一种通过准确性预测器来修剪大型语言模型的新方法，实验证明该方法是有效且高效的，可以自动选择最优模型并降低困惑度。"
}