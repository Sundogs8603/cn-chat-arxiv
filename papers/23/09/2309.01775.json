{
    "title": "Gated recurrent neural networks discover attention",
    "abstract": "Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention und",
    "link": "https://arxiv.org/abs/2309.01775",
    "context": "Title: Gated recurrent neural networks discover attention\nAbstract: Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention und",
    "path": "papers/23/09/2309.01775.json",
    "total_tokens": 903,
    "translated_title": "门控循环神经网络发现注意力",
    "translated_abstract": "最近的架构发展使得循环神经网络（RNN）在某些序列建模任务上达到甚至超越Transformer的性能。这些现代RNN具有一个重要的设计模式：线性循环层通过带有乘法门控的前馈路径相互连接。在这里，我们展示了具备这两个设计元素的RNN可以精确地实现（线性）自注意力，这是Transformer的主要构建模块。通过逆向工程一组经过训练的RNN，我们发现梯度下降在实践中发现了我们的构造。特别是，我们考察了训练有素解决简单的上下文学习任务的RNN，发现梯度下降给我们的RNN注入了与Transformer使用的基于注意力的上下文学习算法相同的特性。我们的发现强调了神经网络中乘法相互作用的重要性，并暗示某些RNN可能意外地实现了注意力。",
    "tldr": "本研究发现，具备线性循环层和带有乘法门控的前馈路径的RNN可以精确地实现注意力，这是Transformer的主要构建模块。逆向工程的结果显示，梯度下降在实践中发现了该构造，并使RNN具备与Transformer相同的基于注意力的上下文学习算法。"
}