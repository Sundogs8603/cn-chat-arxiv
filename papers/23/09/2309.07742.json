{
    "title": "Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])",
    "abstract": "Focus in Explainable AI is shifting from explanations defined in terms of low-level elements, such as input features, to explanations encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post-hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in Human-interpretable Representation Learning (HRL) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post-hoc explainers and concept-based neural networks. Our formalization of HRL builds ",
    "link": "http://arxiv.org/abs/2309.07742",
    "context": "Title: Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])\nAbstract: Focus in Explainable AI is shifting from explanations defined in terms of low-level elements, such as input features, to explanations encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post-hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in Human-interpretable Representation Learning (HRL) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post-hoc explainers and concept-based neural networks. Our formalization of HRL builds ",
    "path": "papers/23/09/2309.07742.json",
    "total_tokens": 908,
    "translated_title": "解读存在于观察者的心中：用于可解释性表示学习的因果框架",
    "translated_abstract": "可解释的人工智能的重点正在从以输入特征为基础定义的解释，转向以从数据中学习的可解释概念为基础的解释。然而，如何可靠地获得这些概念仍然基本不清楚。缺少对概念可解释性的一致性概念，导致事后解释者和基于概念的神经网络使用的概念通过多种相互不兼容的策略获得。至关重要的是，这些策略中大多数忽视了人类问题面：一个表示只有在被接收的人类能够理解的程度上才能理解。在人类可解释表示学习（HRL）中，关键挑战是如何对这个人类因素进行建模和操作。在这项工作中，我们提出了一种数学框架，用于获取适用于事后解释者和基于概念的神经网络的可解释表示。我们对HRL的形式化建模...",
    "tldr": "可解释性表示学习的关键挑战是如何在人类因素中进行建模和操作。本论文提出了一种数学框架，用于获取可解释的表示，适用于事后解释者和基于概念的神经网络。",
    "en_tdlr": "The key challenge in interpretable representation learning is how to model and operationalize the human element. This paper proposes a mathematical framework for acquiring interpretable representations suitable for both post-hoc explainers and concept-based neural networks."
}