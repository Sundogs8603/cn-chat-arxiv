{
    "title": "Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness. (arXiv:2309.16973v1 [cs.LG])",
    "abstract": "To obtain a near-optimal policy with fewer interactions in Reinforcement Learning (RL), a promising approach involves the combination of offline RL, which enhances sample efficiency by leveraging offline datasets, and online RL, which explores informative transitions by interacting with the environment. Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained agent within limited online interactions. However, due to the significant distribution shift between online experiences and offline data, most offline RL algorithms suffer from performance drops and fail to achieve stable policy improvement in O2O adaptation. To address this problem, we propose the Robust Offline-to-Online (RO2O) algorithm, designed to enhance offline policies through uncertainty and smoothness, and to mitigate the performance drop in online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty penalty and adversarial samples for policy and value smoothness, which enable RO2",
    "link": "http://arxiv.org/abs/2309.16973",
    "context": "Title: Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness. (arXiv:2309.16973v1 [cs.LG])\nAbstract: To obtain a near-optimal policy with fewer interactions in Reinforcement Learning (RL), a promising approach involves the combination of offline RL, which enhances sample efficiency by leveraging offline datasets, and online RL, which explores informative transitions by interacting with the environment. Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained agent within limited online interactions. However, due to the significant distribution shift between online experiences and offline data, most offline RL algorithms suffer from performance drops and fail to achieve stable policy improvement in O2O adaptation. To address this problem, we propose the Robust Offline-to-Online (RO2O) algorithm, designed to enhance offline policies through uncertainty and smoothness, and to mitigate the performance drop in online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty penalty and adversarial samples for policy and value smoothness, which enable RO2",
    "path": "papers/23/09/2309.16973.json",
    "total_tokens": 903,
    "translated_title": "通过不确定性和平滑性实现强化学习的鲁棒离线到在线学习",
    "translated_abstract": "为了在强化学习中以较少的互动次数获得接近最优策略，一种有前途的方法是将离线强化学习（通过利用离线数据集提高样本效率）和在线强化学习（通过与环境互动探索信息丰富的转换）相结合。离线到在线（O2O）强化学习提供了一种改进离线训练代理的范例，但由于在线经验与离线数据之间存在显著的分布偏差，大多数离线强化学习算法在O2O适应中性能下降并无法实现稳定的策略改进。为了解决这个问题，我们提出了Robust Offline-to-Online（RO2O）算法，旨在通过不确定性和平滑性增强离线策略，并减少在线适应中的性能下降。具体而言，RO2O算法通过Q-ensemble实现不确定性惩罚，并通过对抗样本实现策略和价值的平滑性，从而实现RO2O的目标。",
    "tldr": "该论文提出了一种名为RO2O的算法，通过不确定性和平滑性增强离线训练的强化学习代理，在离线到在线学习中缓解性能下降问题。",
    "en_tdlr": "The paper proposes a algorithm called RO2O, which enhances offline-trained reinforcement learning agents through uncertainty and smoothness, mitigating performance drops in online adaptation."
}