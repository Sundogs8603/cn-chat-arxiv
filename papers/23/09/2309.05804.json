{
    "title": "Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])",
    "abstract": "Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation",
    "link": "http://arxiv.org/abs/2309.05804",
    "context": "Title: Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])\nAbstract: Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation",
    "path": "papers/23/09/2309.05804.json",
    "total_tokens": 903,
    "translated_title": "生成“nice”而不是生成“good”不像生成“rice”那么糟糕！朝着上下文和语义融合的对话生成损失函数和评估指标。",
    "translated_abstract": "在过去的二十年中，对话建模取得了重大进展，从简单的基于规则的回答发展到个性化和有说服力的回答生成。然而，尽管这些进展，对话生成的目标函数和评估指标仍然停滞不前，分别是交叉熵和BLEU。这些基于词汇的指标存在以下主要限制：(a)没有语义考虑的词对词匹配：它将生成“nice”和生成“rice”作为“good”的失败统一考虑。(b)缺少为评估生成的回答提供上下文属性：即使生成的回答与正在进行的对话上下文相关，如果不与语料库中提供的黄金话语匹配，仍然可能受到惩罚。在本文中，我们首先全面调查这些限制，并提出了一种名为上下文语义融合对话(SemTextualLogue)损失函数的新损失函数。此外，我们还制定了一套新的评估指标，",
    "tldr": "提出了一种新的对话生成损失函数和评估指标，解决了以往方法中词汇匹配和缺少上下文考虑的限制。",
    "en_tdlr": "A new dialogue generation loss function and evaluation metric are proposed to address the limitations of word-to-word matching and lack of context consideration in previous methods."
}