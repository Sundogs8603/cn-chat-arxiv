{
    "title": "Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])",
    "abstract": "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We als",
    "link": "http://arxiv.org/abs/2309.11669",
    "context": "Title: Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])\nAbstract: Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We als",
    "path": "papers/23/09/2309.11669.json",
    "total_tokens": 926,
    "translated_title": "基于循环评估的配对知识图谱-文本数据集构建",
    "translated_abstract": "将知识图谱（KG）和文本配对的数据集（KG-T）可用于训练生成KG文本和相反的神经模型。然而，在KG和文本配对不等效的数据集上训练的模型可能导致更多的虚构和更差的召回。本文通过生成具有不同噪声级别的数据集来经验证实了这一点，并发现更嘈杂的数据集确实会导致更多的虚构。我们认为，训练在数据集上能够循环生成源KG或文本的正向和反向模型的能力是衡量KG和文本之间等价性的代理。通过循环评估，我们发现手动创建的WebNLG要比自动创建的TeKGen和T-REx好得多。在这些观察的指导下，我们使用旨在提高KG和文本等价性的启发式方法构建了一个新的、改进的数据集LAGRANGE，并展示了每个启发式方法对循环评估的影响。",
    "tldr": "本文通过循环评估验证了训练在等价性较差的数据集上的模型可能导致更多虚构和更差召回的问题。在此基础上，提出了一个改进的数据集LAGRANGE，使用启发式方法提高KG和文本之间的等价性。",
    "en_tdlr": "This paper empirically verifies that models trained on datasets with poor equivalence can result in more hallucination and poorer recall. Based on this, a new and improved dataset called LAGRANGE is proposed, which uses heuristics to enhance the equivalence between knowledge graph and text."
}