{
    "title": "Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])",
    "abstract": "In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.",
    "link": "http://arxiv.org/abs/2309.08339",
    "context": "Title: Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])\nAbstract: In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.",
    "path": "papers/23/09/2309.08339.json",
    "total_tokens": 816,
    "translated_title": "ADAM在非凸设置中具有恒定步长的收敛性：一个简单的证明",
    "translated_abstract": "在神经网络训练中，RMSProp和ADAM仍然是广泛使用的优化算法。它们的性能关键之一在于选择适当的步长，这会显著影响它们的有效性。值得注意的是，这些算法的性能可以因选择的步长而变化很大。此外，关于它们的理论收敛性问题仍然是一个感兴趣的话题。在本文中，我们在非凸设置中对ADAM的恒定步长版本进行了理论分析。我们证明了步长达到几乎肯定渐近收敛到零的充分条件，而只需最小的假设。我们还给出了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。",
    "tldr": "本文分析了ADAM在非凸设置中具有恒定步长的收敛性，给出了步长达到几乎肯定渐近收敛的充分条件，并提供了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。"
}