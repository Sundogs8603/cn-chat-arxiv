{
    "title": "Unsupervised speech enhancement with diffusion-based generative models. (arXiv:2309.10450v1 [cs.CV])",
    "abstract": "Recently, conditional score-based diffusion models have gained significant attention in the field of supervised speech enhancement, yielding state-of-the-art performance. However, these methods may face challenges when generalising to unseen conditions. To address this issue, we introduce an alternative approach that operates in an unsupervised manner, leveraging the generative power of diffusion models. Specifically, in a training phase, a clean speech prior distribution is learnt in the short-time Fourier transform (STFT) domain using score-based diffusion models, allowing it to unconditionally generate clean speech from Gaussian noise. Then, we develop a posterior sampling methodology for speech enhancement by combining the learnt clean speech prior with a noise model for speech signal inference. The noise parameters are simultaneously learnt along with clean speech estimation through an iterative expectationmaximisation (EM) approach. To the best of our knowledge, this is the first",
    "link": "http://arxiv.org/abs/2309.10450",
    "context": "Title: Unsupervised speech enhancement with diffusion-based generative models. (arXiv:2309.10450v1 [cs.CV])\nAbstract: Recently, conditional score-based diffusion models have gained significant attention in the field of supervised speech enhancement, yielding state-of-the-art performance. However, these methods may face challenges when generalising to unseen conditions. To address this issue, we introduce an alternative approach that operates in an unsupervised manner, leveraging the generative power of diffusion models. Specifically, in a training phase, a clean speech prior distribution is learnt in the short-time Fourier transform (STFT) domain using score-based diffusion models, allowing it to unconditionally generate clean speech from Gaussian noise. Then, we develop a posterior sampling methodology for speech enhancement by combining the learnt clean speech prior with a noise model for speech signal inference. The noise parameters are simultaneously learnt along with clean speech estimation through an iterative expectationmaximisation (EM) approach. To the best of our knowledge, this is the first",
    "path": "papers/23/09/2309.10450.json",
    "total_tokens": 923,
    "translated_title": "基于扩散式生成模型的无监督语音增强",
    "translated_abstract": "最近，在监督语音增强领域，条件得分型扩散模型引起了广泛关注，其表现出了最先进的性能。然而，这些方法在推广到未知条件时可能面临挑战。为了解决这个问题，我们引入了一种在无监督方式下运作的替代方法，利用扩散模型的生成能力。具体而言，在训练阶段，利用基于得分的扩散模型在短时傅里叶变换（STFT）域中学习了一个干净语音先验分布，使其能无条件地从高斯噪声生成干净语音。然后，我们通过将学到的干净语音先验与噪声模型相结合，开发了一种对语音增强进行后验采样的方法来进行语音信号推断。噪声参数是通过迭代的期望最大化（EM）方法与干净语音估计同时学习的。据我们所知，这是第一次开发这种无监督的方法来处理语音增强问题。",
    "tldr": "这篇论文介绍了一种无监督的语音增强方法，利用扩散模型的生成能力，在训练阶段通过学习干净语音先验分布和噪声模型，实现对语音信号的推断。这是目前尚无的处理语音增强问题的方法。",
    "en_tdlr": "This paper introduces an unsupervised speech enhancement method that leverages the generative power of diffusion models. By learning clean speech prior distribution and noise model in the training phase, the method can infer speech signals effectively. This is a novel approach in the field of speech enhancement."
}