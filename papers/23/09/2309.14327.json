{
    "title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v2 [cs.CV] UPDATED)",
    "abstract": "Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B para",
    "link": "http://arxiv.org/abs/2309.14327",
    "context": "Title: DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v2 [cs.CV] UPDATED)\nAbstract: Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B para",
    "path": "papers/23/09/2309.14327.json",
    "total_tokens": 878,
    "translated_title": "DeepSpeed-VisualChat：通过多模态因果关注实现的多轮多图交错聊天",
    "translated_abstract": "现有的大部分多模态模型由于无法熟练地处理多图、多回合对话中交错的图像和文本输入，面临着在训练资源分配和数据可访问性方面的重要限制，这影响了它们在不同交互领域中的适应性和可扩展性。为了解决这个问题，我们提出了 DeepSpeed-VisualChat 框架，旨在通过融合多模态功能，集中提高大型视觉和语言模型处理交错输入的能力。我们的框架的显著特点在于：(1) 提供对多轮多图对话的开源支持，(2) 引入创新的多模态因果关注机制，以及 (3) 在现有数据集上使用数据融合技术，以确保多轮多图对话中的无缝交互。与现有框架相比，DeepSpeed-VisualChat 在可扩展性方面展现出卓越的表现，可达到 70B 参数。",
    "tldr": "DeepSpeed-VisualChat是一个用于多轮多图交错聊天的框架，通过引入创新的多模态因果关注机制和数据融合技术，具有优越的可扩展性。"
}