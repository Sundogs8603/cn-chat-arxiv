{
    "title": "ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)",
    "abstract": "Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl",
    "link": "http://arxiv.org/abs/2309.14078",
    "context": "Title: ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)\nAbstract: Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl",
    "path": "papers/23/09/2309.14078.json",
    "total_tokens": 969,
    "translated_title": "基于ODE的无模型反馈强化学习用于POMDPs",
    "translated_abstract": "神经常微分方程（ODEs）被广泛认为是建模物理机制的标准，它们有助于在未知的物理或生物环境中进行近似推断。在部分可观测（PO）环境中，如何从原始观测中推断看不见的信息困扰着代理人。通过使用具有紧凑上下文的循环策略，基于上下文的强化学习提供了一种灵活的方法来从历史转换中提取不可观察的信息。为了帮助代理人提取更多与动态相关的信息，我们提出了一种新颖的基于ODE的循环模型，并结合了无模型的强化学习框架来解决部分可观察的马尔可夫决策过程（POMDPs）。我们通过各种PO连续控制和元强化学习任务实验验证了我们方法的有效性。此外，我们的实验表明，由于ODEs具有建模不规则性的能力，我们的方法对于不规则观察具有鲁棒性。",
    "tldr": "本论文提出了一种基于ODE的循环模型结合无模型强化学习来解决部分可观察的马尔可夫决策过程（POMDPs）。实验结果表明该方法在PO连续控制和元强化学习任务中表现出了良好的效果，并且对于不规则观察具有鲁棒性。"
}