{
    "title": "Bregman Graph Neural Network. (arXiv:2309.06645v1 [cs.LG])",
    "abstract": "Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the \"skip connection\". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers",
    "link": "http://arxiv.org/abs/2309.06645",
    "context": "Title: Bregman Graph Neural Network. (arXiv:2309.06645v1 [cs.LG])\nAbstract: Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the \"skip connection\". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers",
    "path": "papers/23/09/2309.06645.json",
    "total_tokens": 862,
    "translated_title": "Bregman图神经网络",
    "translated_abstract": "近期关于图神经网络（GNNs）的众多研究主要集中在将GNN架构建模为具有平滑假设的优化问题上。然而，在节点分类任务中，GNNs引起的平滑效果往往会使连接节点的表示和标签过于同质化，导致过度平滑和错误分类等不利影响。在本文中，我们提出了一种受Bregman距离概念启发的GNNs双层优化框架。我们展示了相应提出的GNNs层可以通过引入类似“跳跃连接”的机制有效缓解过度平滑问题。通过全面的实证研究，我们验证了我们的理论结果，在同构图和异构图中，Bregman增强的GNNs在性能上优于原始的GNNs。此外，我们的实验还显示出，即使层数较多，Bregman GNNs也能产生更稳健的学习准确度。",
    "tldr": "Bregman GNNs提出了一种受Bregman距离概念启发的双层优化框架，能够有效缓解过度平滑问题并在同构图和异构图中性能优于原始的GNNs。",
    "en_tdlr": "Bregman GNNs propose a novel bilevel optimization framework inspired by the notion of Bregman distance, effectively mitigating the over-smoothing issue and outperforming the original GNNs in both homophilic and heterophilic graphs."
}