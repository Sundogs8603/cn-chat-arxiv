{
    "title": "Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition. (arXiv:2309.12714v1 [eess.AS])",
    "abstract": "Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output featuremaps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose meth",
    "link": "http://arxiv.org/abs/2309.12714",
    "context": "Title: Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition. (arXiv:2309.12714v1 [eess.AS])\nAbstract: Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output featuremaps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose meth",
    "path": "papers/23/09/2309.12714.json",
    "total_tokens": 876,
    "translated_title": "无监督表示改进了监督学习中的语音情感识别",
    "translated_abstract": "语音情感识别（SER）在增强人机交互方面起着重要作用，通过对情感状态进行更深入的理解，为更具共情力和有效沟通做出贡献。本研究提出了一种创新方法，将自监督特征提取与监督分类相结合，用于从小音频片段中识别情感。在预处理步骤中，为消除手工制作音频特征的需要，我们采用了基于Wav2Vec模型的自监督特征提取器，从音频数据中捕捉声学特征。然后，将预处理步骤的输出特征图输入到基于卷积神经网络（CNN）的自定义模型中进行情感分类。在ShEMO数据集上测试，所提出的方法超过了两种基准方法：支持向量机分类器和预训练CNN的迁移学习。",
    "tldr": "这项研究提出了一种新颖的方法，利用无监督特征提取和监督分类相结合的方式，在小样本音频片段中进行情感识别。通过使用自监督特征提取器和基于CNN的模型，该方法在情感识别上表现出优于传统方法的结果。"
}