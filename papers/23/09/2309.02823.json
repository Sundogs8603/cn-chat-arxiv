{
    "title": "Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses. (arXiv:2309.02823v1 [cs.CL])",
    "abstract": "Recently, utilizing deep neural networks to build the opendomain dialogue models has become a hot topic. However, the responses generated by these models suffer from many problems such as responses not being contextualized and tend to generate generic responses that lack information content, damaging the user's experience seriously. Therefore, many studies try introducing more information into the dialogue models to make the generated responses more vivid and informative. Unlike them, this paper improves the quality of generated responses by learning the implicit pattern information between contexts and responses in the training samples. In this paper, we first build an open-domain dialogue model based on the pre-trained language model (i.e., GPT-2). And then, an improved scheduled sampling method is proposed for pre-trained models, by which the responses can be used to guide the response generation in the training phase while avoiding the exposure bias problem. More importantly, we de",
    "link": "http://arxiv.org/abs/2309.02823",
    "context": "Title: Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses. (arXiv:2309.02823v1 [cs.CL])\nAbstract: Recently, utilizing deep neural networks to build the opendomain dialogue models has become a hot topic. However, the responses generated by these models suffer from many problems such as responses not being contextualized and tend to generate generic responses that lack information content, damaging the user's experience seriously. Therefore, many studies try introducing more information into the dialogue models to make the generated responses more vivid and informative. Unlike them, this paper improves the quality of generated responses by learning the implicit pattern information between contexts and responses in the training samples. In this paper, we first build an open-domain dialogue model based on the pre-trained language model (i.e., GPT-2). And then, an improved scheduled sampling method is proposed for pre-trained models, by which the responses can be used to guide the response generation in the training phase while avoiding the exposure bias problem. More importantly, we de",
    "path": "papers/23/09/2309.02823.json",
    "total_tokens": 914,
    "translated_title": "通过学习上下文和回复之间的模式信息促进开放领域对话生成",
    "translated_abstract": "近年来，利用深度神经网络构建开放领域对话模型已成为热门话题。然而，这些模型生成的回复存在许多问题，如缺乏上下文化和容易生成缺乏信息内容的通用回复，严重影响用户体验。因此，许多研究试图引入更多信息到对话模型中，使生成的回复更加生动和信息丰富。与它们不同，本文通过学习训练样本中上下文和回复之间的隐式模式信息来提高生成的回复质量。首先，我们基于预训练语言模型（即GPT-2）构建了一个开放领域对话模型。然后，提出了一种改进的预训练模型的计划采样方法，通过该方法，在训练阶段可以利用生成的回复来指导回复生成，同时避免暴露偏差问题。更重要的是，我们将这种方法与传统的基于最大似然估计的方法进行了比较，并表明了我们方法的优势。",
    "tldr": "本文通过学习上下文和回复之间的隐式模式信息提高了开放领域对话生成模型的质量，采用了改进的预训练模型的计划采样方法，使生成的回复更加生动和信息丰富。",
    "en_tdlr": "This paper improves the quality of open-domain dialogue generation models by learning the implicit pattern information between contexts and responses. It proposes an improved scheduled sampling method for pre-trained models to make the generated responses more vivid and informative."
}