{
    "title": "Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels",
    "abstract": "arXiv:2309.09697v2 Announce Type: replace  Abstract: Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of a specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs that considers all the three labels of NLI task. We create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a m",
    "link": "https://arxiv.org/abs/2309.09697",
    "context": "Title: Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels\nAbstract: arXiv:2309.09697v2 Announce Type: replace  Abstract: Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of a specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs that considers all the three labels of NLI task. We create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a m",
    "path": "papers/23/09/2309.09697.json",
    "total_tokens": 917,
    "translated_title": "在自然语言推理中评估预训练语言模型的性别偏见，考虑所有标签",
    "translated_abstract": "在多种语言的预训练语言模型（PLMs）中发现了歧视性的性别偏见。在自然语言推理（NLI）中，现有的偏见评估方法专注于三个标签中的一个特定标签的预测结果，例如中性。然而，这种评估方法可能不准确，因为独特的偏见推理与独特的预测标签相关联。为了解决这一限制，我们提出了一种考虑NLI任务的三个标签的PLMs偏见评估方法。我们创建了三个代表不同类型偏见的评估数据组。然后，我们基于每个数据组的相应标签输出定义了一种偏见度量。在实验中，我们引入了一种用于NLI偏见度量的元评估技术，并用它来确认我们的偏见度量可以更好地区分有偏见的，不正确的推理与非偏见的不正确推理，胜过基线，从而导致了m",
    "tldr": "提出了一种考虑自然语言推理任务三个标签的预训练语言模型偏见评估方法，通过创造代表不同类型偏见的评估数据组，并实验证明该方法能更好地区分有偏见的、不正确的推理和非有偏见的不正确推理。",
    "en_tdlr": "Proposed a bias evaluation method for pre-trained language models in natural language inference that considers all three labels of NLI task, by creating evaluation data groups representing different types of biases and experimentally demonstrating its ability to better distinguish biased, incorrect inferences from non-biased incorrect inferences."
}