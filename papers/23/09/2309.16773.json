{
    "title": "Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])",
    "abstract": "Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs",
    "link": "http://arxiv.org/abs/2309.16773",
    "context": "Title: Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])\nAbstract: Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs",
    "path": "papers/23/09/2309.16773.json",
    "total_tokens": 1040,
    "translated_title": "神经规模定律在表型药物发现中的应用",
    "translated_abstract": "近期深度神经网络在自然语言处理和计算机视觉领域的突破是通过模型和数据规模的提升而非新的计算范例的发现。本研究探讨了规模是否也能对用于辅助小分子药物发现的模型产生类似的影响。我们通过大规模和系统化的分析深度神经网络的规模、数据训练集和学习方法如何相互影响，并对我们的表型化学竞技场（Pheno-CA）基准进行精确度测试：这是一个基于图像高内涵筛选数据的多样化药物开发任务集。令人惊讶的是，我们发现在Pheno-CA任务中明确被监督的深度神经网络在数据和模型规模扩大时并没有持续改进。为了解决这个问题，我们引入了一项新的前体任务，即逆生物过程（IBP），它设计成类似于在自然语言处理中取得成功的因果目标函数。我们确实发现深度神经网络的性能取决于引入IBP任务的方式，当使用IBP预训练时，模型在Pheno-CA任务上的性能得到了显著改善。我们的结果揭示了在表型药物发现中，与深度神经网络的规模和学习方式相关的因素对模型的性能产生重要影响。",
    "tldr": "本研究调查了规模对于辅助小分子药物发现模型的影响，并发现DNN在表型药物发现任务中并没有持续改进，通过引入逆生物过程预训练可以显著提升模型性能。",
    "en_tdlr": "This study investigates the impact of scale on models for small molecule drug discovery and finds that DNNs do not continuously improve in phenotypic drug discovery tasks. However, the performance of the models is significantly improved by introducing pre-training with an inverse biological process."
}