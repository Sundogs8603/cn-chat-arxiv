{
    "title": "Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API Names?. (arXiv:2309.07804v1 [cs.SE])",
    "abstract": "Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex, have shown their superior performance in various downstream tasks. The correctness and unambiguity of API usage among these code models are crucial for achieving desirable program functionalities, requiring them to learn various API fully qualified names structurally and semantically. Recent studies reveal that even state-of-the-art pre-trained code models struggle with suggesting the correct APIs during code generation. However, the reasons for such poor API usage performance are barely investigated. To address this challenge, we propose using knowledge probing as a means of interpreting code models, which uses cloze-style tests to measure the knowledge stored in models. Our comprehensive study examines a code model's capability of understanding API fully qualified names from two different perspectives: API call and API import. Specifically, we reveal that current code models struggle with understanding API n",
    "link": "http://arxiv.org/abs/2309.07804",
    "context": "Title: Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API Names?. (arXiv:2309.07804v1 [cs.SE])\nAbstract: Recent breakthroughs in pre-trained code models, such as CodeBERT and Codex, have shown their superior performance in various downstream tasks. The correctness and unambiguity of API usage among these code models are crucial for achieving desirable program functionalities, requiring them to learn various API fully qualified names structurally and semantically. Recent studies reveal that even state-of-the-art pre-trained code models struggle with suggesting the correct APIs during code generation. However, the reasons for such poor API usage performance are barely investigated. To address this challenge, we propose using knowledge probing as a means of interpreting code models, which uses cloze-style tests to measure the knowledge stored in models. Our comprehensive study examines a code model's capability of understanding API fully qualified names from two different perspectives: API call and API import. Specifically, we reveal that current code models struggle with understanding API n",
    "path": "papers/23/09/2309.07804.json",
    "total_tokens": 874,
    "translated_title": "弹出测验！预训练代码模型是否具有正确API名称的知识？",
    "translated_abstract": "最近在预训练代码模型（如CodeBERT和Codex）方面取得了重大突破，展现了其在各种下游任务中的卓越性能。这些代码模型对于实现期望的程序功能的API的正确性和明确性非常关键，要求它们在结构和语义上学习各种API的完全限定名。最近的研究表明，即使是最先进的预训练代码模型在代码生成过程中也很难正确建议API。然而，关于API使用性能差的原因几乎没有进行研究。为了解决这个挑战，我们提出使用知识探测作为解释代码模型的手段，通过填空式测试来衡量模型中存储的知识。我们全面地研究了代码模型理解API完全限定名的能力，从API调用和API导入两个不同的角度来进行。具体而言，我们发现当前的代码模型在理解API方面存在困难。",
    "tldr": "我们研究了预训练代码模型在理解API名称方面的困难，并提出使用知识探测的方法来解释模型。我们发现当前的代码模型在理解API的完全限定名方面遇到了困难。",
    "en_tdlr": "We investigated the challenges of pre-trained code models in understanding API names and proposed the use of knowledge probing to interpret the models. We found that current code models struggle with understanding fully qualified API names."
}