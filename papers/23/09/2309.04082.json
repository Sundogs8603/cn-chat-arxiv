{
    "title": "Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])",
    "abstract": "Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i",
    "link": "http://arxiv.org/abs/2309.04082",
    "context": "Title: Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])\nAbstract: Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i",
    "path": "papers/23/09/2309.04082.json",
    "total_tokens": 897,
    "translated_title": "对于图表示学习的混合曲率Transformer: 拐弯你的注意力",
    "translated_abstract": "现实世界中的图往往具有不适合典型欧几里得空间的分层或循环结构。虽然存在能够利用双曲或球面空间学习更准确嵌入这些结构的图神经网络，但这些方法局限于消息传递范式，使得模型容易受到过度平滑和过度压缩等副作用的影响。最近的研究提出了基于全局注意力的图Transformer，可以轻松建模长程交互，但对非欧几里得几何的拓展尚未得到探索。为了弥合这一差距，我们提出了完全乘积立体变换器，这是一种对常曲率空间的变换器的推广。当与标记化的图Transformer结合使用时，我们的模型可以以端到端的方式学习适合输入图的曲率，无需在不同曲率上进行额外调整。",
    "tldr": "本文提出了一种混合曲率Transformer，用于图表示学习。通过将完全乘积立体变换器与标记化的图Transformer相结合，模型能够以端到端的方式学习适应输入图的曲率，从而更好地嵌入图中的分层或循环结构。",
    "en_tdlr": "This paper introduces mixed-curvature Transformers for graph representation learning. By combining Fully Product-Stereographic Transformer with tokenized graph Transformers, the model can learn the appropriate curvature for the input graph in an end-to-end manner, resulting in better embedding of hierarchical or cyclical structures in the graph."
}