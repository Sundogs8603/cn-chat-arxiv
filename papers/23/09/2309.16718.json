{
    "title": "A Real-World Quadrupedal Locomotion Benchmark for Offline Reinforcement Learning. (arXiv:2309.16718v1 [cs.RO])",
    "abstract": "Online reinforcement learning (RL) methods are often data-inefficient or unreliable, making them difficult to train on real robotic hardware, especially quadruped robots. Learning robotic tasks from pre-collected data is a promising direction. Meanwhile, agile and stable legged robotic locomotion remains an open question in their general form. Offline reinforcement learning (ORL) has the potential to make breakthroughs in this challenging field, but its current bottleneck lies in the lack of diverse datasets for challenging realistic tasks. To facilitate the development of ORL, we benchmarked 11 ORL algorithms in the realistic quadrupedal locomotion dataset. Such dataset is collected by the classic model predictive control (MPC) method, rather than the model-free online RL method commonly used by previous benchmarks. Extensive experimental results show that the best-performing ORL algorithms can achieve competitive performance compared with the model-free RL, and even surpass it in som",
    "link": "http://arxiv.org/abs/2309.16718",
    "context": "Title: A Real-World Quadrupedal Locomotion Benchmark for Offline Reinforcement Learning. (arXiv:2309.16718v1 [cs.RO])\nAbstract: Online reinforcement learning (RL) methods are often data-inefficient or unreliable, making them difficult to train on real robotic hardware, especially quadruped robots. Learning robotic tasks from pre-collected data is a promising direction. Meanwhile, agile and stable legged robotic locomotion remains an open question in their general form. Offline reinforcement learning (ORL) has the potential to make breakthroughs in this challenging field, but its current bottleneck lies in the lack of diverse datasets for challenging realistic tasks. To facilitate the development of ORL, we benchmarked 11 ORL algorithms in the realistic quadrupedal locomotion dataset. Such dataset is collected by the classic model predictive control (MPC) method, rather than the model-free online RL method commonly used by previous benchmarks. Extensive experimental results show that the best-performing ORL algorithms can achieve competitive performance compared with the model-free RL, and even surpass it in som",
    "path": "papers/23/09/2309.16718.json",
    "total_tokens": 963,
    "translated_title": "为线下强化学习提供一个现实世界的四足步态基准测试",
    "translated_abstract": "在真实的机器人硬件上训练在线强化学习方法往往效率低下或不可靠，尤其对于四足机器人更是如此。从预收集的数据中学习机器人任务是一个有前途的方向。同时，灵活稳定的四足步态机器人行走仍然是一个尚未解决的问题。线下强化学习有潜力在这个具有挑战性的领域取得突破，但目前的瓶颈在于缺乏多样化的用于挑战性现实任务的数据集。为了促进线下强化学习的发展，我们在现实的四足步态数据集上对11种线下强化学习算法进行了基准测试。这个数据集是通过经典的模型预测控制方法(MPC)收集的，而不是之前基准测试常用的无模型在线强化学习方法。广泛的实验结果表明，表现最好的线下强化学习算法与无模型强化学习相比具有竞争力的性能，甚至在某些方面超越它。",
    "tldr": "该论文提出了一个现实世界的四足步态基准测试，用于评估线下强化学习算法在挑战性任务中的性能。实验结果表明，最好的算法能够与无模型强化学习相媲美甚至超越其性能。"
}