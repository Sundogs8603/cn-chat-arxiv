{
    "title": "Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning. (arXiv:2309.02476v1 [stat.ML])",
    "abstract": "Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\\bx$) and output ($\\by$), active learning focuses solely on the input data ($\\bx$).  In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampl",
    "link": "http://arxiv.org/abs/2309.02476",
    "context": "Title: Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning. (arXiv:2309.02476v1 [stat.ML])\nAbstract: Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\\bx$) and output ($\\by$), active learning focuses solely on the input data ($\\bx$).  In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampl",
    "path": "papers/23/09/2309.02476.json",
    "total_tokens": 879,
    "translated_title": "通过不确定性估计实现优化样本选择及其在深度学习中的应用",
    "translated_abstract": "现代深度学习在很大程度上依赖于大型标记数据集，但在手动标注和计算资源方面往往会带来高昂的成本。为了应对这些挑战，研究人员已经探索了一些信息性子集选择技术，包括核心集选择和主动学习。具体而言，核心集选择涉及到同时采样输入（$\\bx$）和输出（$\\by$）的数据，而主动学习仅关注输入数据（$\\bx$）。在本研究中，我们提出了针对线性softmax回归背景下同时解决核心集选择和主动学习的理论最优解。我们提出的方法，COPS（基于不确定性的最优子采样），旨在最小化基于子采样数据训练的模型的期望损失。与现有的依赖于显式计算逆协方差矩阵的方法不同，这种方法在深度学习场景中不容易应用。COPS利用模型的逻辑回归值来估计采样的...",
    "tldr": "该论文提出了一种理论最优解——COPS（基于不确定性的最优子采样），用于解决深度学习中的核心集选择和主动学习问题，在减少标记数据集成本的同时最小化模型的期望损失。"
}