{
    "title": "A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])",
    "abstract": "Large language models (LLMs) have played a pivotal role in revolutionizing various facets of our daily existence. Solving attention regression is a fundamental task in optimizing LLMs. In this work, we focus on giving a provable guarantee for the one-layer attention network objective function $L(X,Y) = \\sum_{j_0 = 1}^n \\sum_{i_0 = 1}^d ( \\langle \\langle \\exp( \\mathsf{A}_{j_0} x ) , {\\bf 1}_n \\rangle^{-1} \\exp( \\mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \\rangle - b_{j_0,i_0} )^2$. Here $\\mathsf{A} \\in \\mathbb{R}^{n^2 \\times d^2}$ is Kronecker product between $A_1 \\in \\mathbb{R}^{n \\times d}$ and $A_2 \\in \\mathbb{R}^{n \\times d}$. $A_3$ is a matrix in $\\mathbb{R}^{n \\times d}$, $\\mathsf{A}_{j_0} \\in \\mathbb{R}^{n \\times d^2}$ is the $j_0$-th block of $\\mathsf{A}$. The $X, Y \\in \\mathbb{R}^{d \\times d}$ are variables we want to learn. $B \\in \\mathbb{R}^{n \\times d}$ and $b_{j_0,i_0} \\in \\mathbb{R}$ is one entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \\in \\mathbb{R}^d$ is the $i_",
    "link": "http://arxiv.org/abs/2309.07418",
    "context": "Title: A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])\nAbstract: Large language models (LLMs) have played a pivotal role in revolutionizing various facets of our daily existence. Solving attention regression is a fundamental task in optimizing LLMs. In this work, we focus on giving a provable guarantee for the one-layer attention network objective function $L(X,Y) = \\sum_{j_0 = 1}^n \\sum_{i_0 = 1}^d ( \\langle \\langle \\exp( \\mathsf{A}_{j_0} x ) , {\\bf 1}_n \\rangle^{-1} \\exp( \\mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \\rangle - b_{j_0,i_0} )^2$. Here $\\mathsf{A} \\in \\mathbb{R}^{n^2 \\times d^2}$ is Kronecker product between $A_1 \\in \\mathbb{R}^{n \\times d}$ and $A_2 \\in \\mathbb{R}^{n \\times d}$. $A_3$ is a matrix in $\\mathbb{R}^{n \\times d}$, $\\mathsf{A}_{j_0} \\in \\mathbb{R}^{n \\times d^2}$ is the $j_0$-th block of $\\mathsf{A}$. The $X, Y \\in \\mathbb{R}^{d \\times d}$ are variables we want to learn. $B \\in \\mathbb{R}^{n \\times d}$ and $b_{j_0,i_0} \\in \\mathbb{R}$ is one entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \\in \\mathbb{R}^d$ is the $i_",
    "path": "papers/23/09/2309.07418.json",
    "total_tokens": 1278,
    "translated_title": "在张量和SVM Trick基础上重新构建单层LLM注意力机制，并在矩阵乘法时间内解决它的快速优化视角",
    "translated_abstract": "大型语言模型(LLMs)在我们日常生活的各个方面都发挥着重要作用。解决LLMs中的注意力回归是优化LLMs的基本任务。本文致力于为一层注意力网络目标函数 $L(X,Y) = \\sum_{j_0 = 1}^n \\sum_{i_0 = 1}^d ( \\langle \\langle \\exp( \\mathsf{A}_{j_0} x ) , {\\bf 1}_n \\rangle^{-1} \\exp( \\mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \\rangle - b_{j_0,i_0} )^2$ 提供可证明的保证。这里 $\\mathsf{A} \\in \\mathbb{R}^{n^2 \\times d^2}$ 是$A_1 \\in \\mathbb{R}^{n \\times d}$ 和 $A_2 \\in \\mathbb{R}^{n \\times d}$ 的Kronecker积。$A_3$ 是$\\mathbb{R}^{n \\times d}$ 中的一个矩阵，$\\mathsf{A}_{j_0} \\in \\mathbb{R}^{n \\times d^2}$ 是$\\mathsf{A}$ 的第$j_0$个块。$X, Y \\in \\mathbb{R}^{d \\times d}$ 是我们要学习的变量。$B \\in \\mathbb{R}^{n \\times d}$ 和 $b_{j_0,i_0} \\in \\mathbb{R}$ 是$B$ 的第$j_0$行和第$i_0$列的一个元素，$Y_{*,i_0} \\in \\mathbb{R}^d$ 是$Y$ 的第$i_0$列。",
    "tldr": "在本文中，我们使用张量和SVM Trick重新构建了单层LLM注意力机制，并提供了对应的优化视角。我们的工作可以在矩阵乘法时间内解决注意力回归问题。"
}