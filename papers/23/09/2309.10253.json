{
    "title": "GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])",
    "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial \"jailbreak\" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \\fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \\fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \\fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \\fuzzer: a seed selection strategy for balancing efficiency ",
    "link": "http://arxiv.org/abs/2309.10253",
    "context": "Title: GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])\nAbstract: Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial \"jailbreak\" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \\fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \\fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \\fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \\fuzzer: a seed selection strategy for balancing efficiency ",
    "path": "papers/23/09/2309.10253.json",
    "total_tokens": 948,
    "translated_title": "GPTFUZZER : 使用自动生成的越狱提示对大型语言模型进行红队测试",
    "translated_abstract": "大型语言模型（LLMs）最近非常受欢迎，广泛用于日常对话到基于人工智能的编程。然而，尽管取得了巨大的成功，LLMs并不完全可靠，可能会提供有关进行有害或非法活动的详细指导。虽然安全措施可以减少这些输出的风险，但对抗性的\"越狱\"攻击仍然可以利用LLMs生成有害内容。这些越狱模板通常是手工精心制作的，使大规模测试具有挑战性。在本文中，我们介绍了一种新颖的黑盒越狱模糊测试框架\\fuzzer，受AFL模糊测试框架的启发。与手工工程不同，\\fuzzer自动化生成用于红队测试LLMs的越狱模板。在核心部分，\\fuzzer从人工编写的模板作为种子开始，然后使用变异操作对其进行变异以生成新的模板。我们详细介绍了\\fuzzer的三个关键组成部分：用于平衡效率的种子选择策略",
    "tldr": "GPTFUZZER是一种黑盒越狱模糊测试框架，自动生成用于红队测试大型语言模型的越狱模板。这种自动化方法避免了手工工程，并通过种子选择策略提高了效率。"
}