{
    "title": "Bias Amplification Enhances Minority Group Performance. (arXiv:2309.06717v1 [cs.LG])",
    "abstract": "Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared w",
    "link": "http://arxiv.org/abs/2309.06717",
    "context": "Title: Bias Amplification Enhances Minority Group Performance. (arXiv:2309.06717v1 [cs.LG])\nAbstract: Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared w",
    "path": "papers/23/09/2309.06717.json",
    "total_tokens": 889,
    "translated_title": "偏见放大增强了少数群体的表现",
    "translated_abstract": "由标准训练产生的神经网络在罕见的子群上的准确性较差，尽管在平均水平上取得了较高准确性，这是由于某些虚假特征与标签之间的关联。之前基于最差群体损失最小化的方法（例如Group-DRO）在改善最差群体准确性方面是有效的，但需要为所有训练样本提供昂贵的群体注释。在本文中，我们关注更具挑战性和现实性的情景，即群体注释仅在一个小的验证集上可用，或者根本不可用。我们提出了BAM，一种新的两阶段训练算法：在第一阶段，通过引入可学习的辅助变量为每个训练样本训练一个偏见放大方案的模型；在第二阶段，我们对偏见放大的模型误分类的样本进行加权，然后在重新加权的数据集上继续训练同一模型。实验证明，BAM相对于其他方法在性能上取得了竞争性的表现。",
    "tldr": "本论文提出了一种名为BAM的两阶段训练算法，通过引入可学习的辅助变量来放大偏见，提高了少数群体的表现。",
    "en_tdlr": "This paper proposes a two-stage training algorithm called BAM, which amplifies bias by introducing a learnable auxiliary variable, and enhances the performance of minority groups."
}