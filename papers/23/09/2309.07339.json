{
    "title": "Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])",
    "abstract": "Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating ",
    "link": "http://arxiv.org/abs/2309.07339",
    "context": "Title: Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])\nAbstract: Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating ",
    "path": "papers/23/09/2309.07339.json",
    "total_tokens": 932,
    "translated_title": "高效的量子循环强化学习：基于量子储水池计算",
    "translated_abstract": "量子强化学习（QRL）已经成为解决顺序决策任务的框架，并展示了量子优势的实证结果。一个值得注意的发展是通过量子循环神经网络（QRNN）来处理部分可观察环境等内存密集任务。然而，QRL模型结合QRNN面临着挑战，包括QRL与QRNN的低效训练，因为QRNN中的梯度计算既耗费计算资源又耗时。本研究通过构建利用基于QRNN的储水池的QRL代理来解决这一挑战，具体采用量子长短时记忆（QLSTM）。QLSTM参数是随机初始化并固定不变的。该模型使用异步优势演员-评论家（A3C）算法进行训练。通过数值模拟，我们验证了QLSTM-Reservoir RL框架的有效性。其性能在标准基准测试上得到了评估，证明了其潜力。",
    "tldr": "本研究提出了一种高效的量子循环强化学习方法，通过构建利用基于量子循环神经网络的储水池的QRL代理，解决了QRL与QRNN的低效训练问题。通过数值模拟验证了这种方法的有效性，并在标准基准测试中展示了其潜力。",
    "en_tdlr": "This study proposes an efficient quantum recurrent reinforcement learning approach, which addresses the issue of inefficient training of QRL with QRNN by constructing QRL agents utilizing QRNN-based reservoirs. The efficacy of this approach is validated through numerical simulations, and its potential is demonstrated on standard benchmarks."
}