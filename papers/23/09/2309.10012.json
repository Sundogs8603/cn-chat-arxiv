{
    "title": "Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])",
    "abstract": "In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained",
    "link": "http://arxiv.org/abs/2309.10012",
    "context": "Title: Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])\nAbstract: In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained",
    "path": "papers/23/09/2309.10012.json",
    "total_tokens": 883,
    "translated_title": "窥探过去：改进不断学习中生成回放的知识保留",
    "translated_abstract": "在这项工作中，我们改进了不断学习环境中的生成回放，以在具有挑战性的场景中表现良好。当前的生成回放方法通常在小型和简单的数据集上进行基准测试，因为它们不足以生成更复杂的数据和更多的类别。我们注意到，在基于VAE的生成回放中，这可能归因于当生成的特征映射到潜在空间时与原始特征之间存在较大差异。因此，我们提出了三种修改方法，使模型能够学习和生成复杂的数据。具体而言，我们将当前模型与先前模型之间的潜在空间进行蒸馏，以减少特征漂移。此外，我们提出了一种用于改善生成特征对齐的重建和原始数据的潜在匹配。进一步地，基于对保存知识的重建效果更好的观察，我们通过先前训练过的模型周期性生成的方式进一步增强了知识保留。",
    "tldr": "本文改进了不断学习环境中的生成回放方法，以在复杂场景下表现更好。通过蒸馏潜在空间、改善生成特征对齐和周期性生成以增强知识保留。",
    "en_tdlr": "This paper improves generative replay in a continual learning setting to perform better in complex scenarios. It proposes modifications in distillation, feature alignment, and cyclic generation to enhance knowledge retention."
}