{
    "title": "Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])",
    "abstract": "Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im",
    "link": "http://arxiv.org/abs/2309.15123",
    "context": "Title: Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])\nAbstract: Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im",
    "path": "papers/23/09/2309.15123.json",
    "total_tokens": 912,
    "translated_title": "揭示分子表示学习中的神经缩放定律",
    "translated_abstract": "分子表示学习（MRL）已经成为药物和材料发现的强大工具，在虚拟筛选和反向设计等各种任务中发挥着重要作用。虽然对于分子表示的数据数量和质量对MRL的影响的模型中心技术的推进引起了大量关注，但在这个领域内，对于这两个因素的影响还不完全清楚。在本文中，我们从数据中心的角度深入研究了MRL的神经缩放行为，考察了四个关键维度：（1）数据模态，（2）数据集划分，（3）预训练的作用，和（4）模型容量。我们的实证研究证实了数据量和MRL性能之间的一致的幂律关系。此外，通过详细分析，我们确定了提高学习效率的潜在途径。为了挑战这些缩放定律，我们将七种常见的数据修剪策略应用于分子数据并进行了性能评估。我们的研究结果强调了改善学习效率的潜在途径。",
    "tldr": "从数据中心的角度研究了分子表示学习的神经缩放行为，发现了数据量和性能之间的一致幂律关系，并提出了潜在的提高学习效率的方法。"
}