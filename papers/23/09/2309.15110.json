{
    "title": "Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow. (arXiv:2309.15110v1 [cs.CV])",
    "abstract": "Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal",
    "link": "http://arxiv.org/abs/2309.15110",
    "context": "Title: Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow. (arXiv:2309.15110v1 [cs.CV])\nAbstract: Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal",
    "path": "papers/23/09/2309.15110.json",
    "total_tokens": 970,
    "translated_title": "Doduo: 从无监督语义感知流中学习密集视觉对应关系",
    "translated_abstract": "密集视觉对应关系在机器人感知中起着至关重要的作用。本工作致力于建立捕捉动态场景经历重大变化的一对图像之间的密集对应关系。我们介绍了Doduo，它可以从野外图像和视频中学习通用的密集视觉对应关系，无需地面真实监督。给定一对图像，Doduo估计了密集的流场，编码了一个图像中每个像素到另一个图像中相应像素的位移。Doduo使用基于流场的扭曲来获得训练的监督信号。结合自监督流训练的语义先验，Doduo产生了对场景动态变化具有鲁棒性的准确密集对应关系。在野外视频数据集上训练后，Doduo在点级对应估计上显示出优于现有的自监督对应关系学习基线的性能。我们还将Doduo应用于关节估计和零样本目标.",
    "tldr": "Doduo是一个从野外图像和视频中学习通用密集视觉对应关系的无监督方法。它使用流场扭曲来获得训练监督信号，并结合语义先验进行自监督流训练，可产生鲁棒准确的密集对应关系。在测试中，Doduo在点级对应估计上表现优于现有的自监督对应关系学习基线。"
}