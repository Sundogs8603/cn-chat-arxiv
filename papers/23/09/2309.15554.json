{
    "title": "Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023. (arXiv:2309.15554v1 [cs.CL])",
    "abstract": "This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.",
    "link": "http://arxiv.org/abs/2309.15554",
    "context": "Title: Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023. (arXiv:2309.15554v1 [cs.CL])\nAbstract: This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.",
    "path": "papers/23/09/2309.15554.json",
    "total_tokens": 908,
    "translated_title": "直接模型用于同时翻译和自动字幕生成：FBK在IWSLT2023中的参与",
    "translated_abstract": "本文描述了FBK在IWSLT 2023评估活动的同时翻译和自动字幕生成任务中的参与。我们的提交关注于使用直接模型来执行这两个任务：对于同时翻译任务，我们利用已经训练好的离线模型的知识，并直接应用策略来进行实时推理；对于字幕生成任务，我们将直接ST模型调整为生成符合规范的字幕，并利用相同的架构生成与音视频内容同步所需的时间戳。我们的英德SimulST系统在计算感知延迟方面比2021年和2022年的任务中排名靠前的系统有所减少，并获得了高达3.5 BLEU的增益。我们的自动字幕生成系统在英德和英西文对中优于基于直接系统的唯一现有解决方案，分别获得了3.7和1.7的SubER增益。",
    "tldr": "本文描述了FBK的研究成果，他们使用直接模型来实现同时翻译和自动字幕生成任务，并在计算感知延迟方面取得了突破性的进展，同时在自动字幕生成任务中也优于现有解决方案。",
    "en_tdlr": "This paper describes the use of direct models to achieve simultaneous translation and automatic subtitling, with a focus on reduced computational latency. The results show significant improvements compared to previous systems, both in terms of real-time translation and subtitle synchronization."
}