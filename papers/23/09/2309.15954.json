{
    "title": "The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering. (arXiv:2309.15954v1 [cs.CV])",
    "abstract": "The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss",
    "link": "http://arxiv.org/abs/2309.15954",
    "context": "Title: The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering. (arXiv:2309.15954v1 [cs.CV])\nAbstract: The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss",
    "path": "papers/23/09/2309.15954.json",
    "total_tokens": 889,
    "translated_title": "细节决定成败：深入探索数据过滤的兔子洞",
    "translated_abstract": "预训练数据的质量对基础模型的性能起着关键作用。流行的基础模型通常设计自己的数据过滤方法，这使得分析和比较不同的数据过滤方法变得困难。DataComp是一个新的基准，专门用于评估不同的数据过滤方法。本文描述了我们参与DataComp挑战时的学习和解决方案。我们的过滤策略包括三个阶段：单模态过滤、跨模态过滤和数据分布对齐。我们整合了现有的方法并提出了新的解决方案，例如在水平翻转图像上计算CLIP分数以减少场景文字的干扰，使用视觉和语言模型检索目标下游任务的训练样本，重新平衡数据分布以提高分配计算预算的效率等。我们对设计选择进行了详尽的分析和讨论。",
    "tldr": "本文介绍了一种针对数据过滤的新的基准，包括单模态过滤、跨模态过滤和数据分布对齐等策略。通过综合现有方法和提出新的解决方案，改善了基础模型的性能，并提供了深入的分析和讨论。"
}