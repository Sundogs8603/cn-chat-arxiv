{
    "title": "Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])",
    "abstract": "Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including ",
    "link": "http://arxiv.org/abs/2309.03999",
    "context": "Title: Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])\nAbstract: Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including ",
    "path": "papers/23/09/2309.03999.json",
    "total_tokens": 911,
    "translated_title": "自适应自监督表示到多领域设置中",
    "translated_abstract": "当在单个领域上训练时，当前最先进的自监督方法非常有效，但在未知领域上的泛化能力有限。我们观察到，即使在混合领域上进行训练，这些模型的泛化能力也很差，因此不适合在多样化的真实世界环境中部署。因此，我们提出了一个通用、轻量级的领域解缠模块（DDM），可以插入到任何自监督编码器中，在具有或不具有共享类的多个不同领域上有效地进行表示学习。在自监督损失的预训练过程中，DDM通过分割表示空间为领域变体和领域不变部分来强制实现表示空间中的解缠。当没有领域标签可用时，DDM使用强健的聚类方法来发现伪领域。我们展示了使用DDM进行预训练可以在最先进的自监督模型上显示出高达3.5%的线性探测准确率的提升。",
    "tldr": "该论文提出了一种通用的、轻量级的领域解缠模块（DDM），可以在多个不同领域上进行有效的自监督表示学习，并显示出较高的线性探测准确率提升。",
    "en_tdlr": "This paper proposes a general-purpose, lightweight Domain Disentanglement Module (DDM) for effective self-supervised representation learning across multiple domains, leading to significant improvements in linear probing accuracy."
}