{
    "title": "Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty. (arXiv:2309.03433v1 [cs.CL])",
    "abstract": "Open Information Extraction (OIE) task aims at extracting structured facts from unstructured text, typically in the form of (subject, relation, object) triples. Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate structured output due to the restrictions on fine-tuning the model. Second, LLMs generates responses autoregressively based on probability, which makes the predicted relations lack confidence. In this paper, we assess the capabilities of LLMs in improving the OIE task. Particularly, we propose various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations. Our experiments on three OIE benchmark datasets show that our approach hold",
    "link": "http://arxiv.org/abs/2309.03433",
    "context": "Title: Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty. (arXiv:2309.03433v1 [cs.CL])\nAbstract: Open Information Extraction (OIE) task aims at extracting structured facts from unstructured text, typically in the form of (subject, relation, object) triples. Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate structured output due to the restrictions on fine-tuning the model. Second, LLMs generates responses autoregressively based on probability, which makes the predicted relations lack confidence. In this paper, we assess the capabilities of LLMs in improving the OIE task. Particularly, we propose various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations. Our experiments on three OIE benchmark datasets show that our approach hold",
    "path": "papers/23/09/2309.03433.json",
    "total_tokens": 905,
    "translated_title": "使用大型语言模型改进开放信息抽取：对演示不确定性进行研究",
    "translated_abstract": "开放信息抽取（OIE）任务旨在从非结构化文本中提取结构化事实，通常以（主体，关系，客体）三元组的形式存在。尽管像ChatGPT这样的大型语言模型具有作为通用任务解决器的潜力，但由于两个关键问题，它们在OIE任务中落后于最先进的（监督）方法。首先，由于对模型微调的限制，LLMs很难区分无关的上下文和相关关系，并生成结构化输出。其次，LLMs基于概率自回归生成响应，导致预测的关系缺乏自信。在本文中，我们评估了LLMs在改进OIE任务中的能力。特别是，我们提出了各种上下文学习策略来增强LLM的指令跟随能力，并提出了演示不确定性量化模块来增强生成关系的自信度。我们对三个OIE基准数据集进行的实验证明了我们的方法的有效性。",
    "tldr": "本研究使用大型语言模型改进了开放信息抽取任务。通过提出上下文学习策略和演示不确定性量化模块，增强了模型的指令跟随能力和生成关系的自信度。"
}