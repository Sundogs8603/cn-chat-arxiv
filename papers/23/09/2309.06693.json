{
    "title": "Kernel-Based Stochastic Learning of Large-Scale Semiparametric Monotone Index Models with an Application to Aging and Household Risk Preference. (arXiv:2309.06693v1 [econ.EM])",
    "abstract": "This paper studies semiparametric estimation of monotone index models in a data-rich environment, where the number of covariates ($p$) and sample size ($n$) can both be large. Motivated by the mini-batch gradient descent algorithm (MBGD) that is widely used as a stochastic optimization tool in the machine learning field, this paper proposes a novel subsample- and iteration-based semiparametric estimation procedure. Starting from any initial guess of the parameter, in each round of iteration we draw a random subsample from the data set, and use such subsample to update the parameter based on the gradient of some well-chosen loss function, where the nonparametric component is replaced with its kernel estimator. Our proposed algorithm essentially generalizes MBGD algorithm to the semiparametric setup. Compared with the KBGD algorithm proposed by Khan et al. (2023) whose computational complexity is of order $O(n^2)$ in each update, the computational burden of our new estimator can be made ",
    "link": "http://arxiv.org/abs/2309.06693",
    "context": "Title: Kernel-Based Stochastic Learning of Large-Scale Semiparametric Monotone Index Models with an Application to Aging and Household Risk Preference. (arXiv:2309.06693v1 [econ.EM])\nAbstract: This paper studies semiparametric estimation of monotone index models in a data-rich environment, where the number of covariates ($p$) and sample size ($n$) can both be large. Motivated by the mini-batch gradient descent algorithm (MBGD) that is widely used as a stochastic optimization tool in the machine learning field, this paper proposes a novel subsample- and iteration-based semiparametric estimation procedure. Starting from any initial guess of the parameter, in each round of iteration we draw a random subsample from the data set, and use such subsample to update the parameter based on the gradient of some well-chosen loss function, where the nonparametric component is replaced with its kernel estimator. Our proposed algorithm essentially generalizes MBGD algorithm to the semiparametric setup. Compared with the KBGD algorithm proposed by Khan et al. (2023) whose computational complexity is of order $O(n^2)$ in each update, the computational burden of our new estimator can be made ",
    "path": "papers/23/09/2309.06693.json",
    "total_tokens": 957,
    "translated_title": "基于核的大规模半参数单调指数模型的随机学习与应用于老龄化和家庭风险偏好的研究",
    "translated_abstract": "本文研究了在数据丰富的环境中，协变量数量（p）和样本量（n）都可能很大的情况下，单调指数模型的半参数估计。受到在机器学习领域广泛用作随机优化工具的小批量梯度下降算法（MBGD）的启发，本文提出了一种新颖的基于子样本和迭代的半参数估计方法。在每一轮迭代中，从数据集中随机抽样一个子样本，并使用该子样本根据一些经过精心选择的损失函数的梯度来更新参数，其中非参数部分被其核估计器替代。我们提出的算法实质上将MBGD算法推广到了半参数设置。与Khan等人（2023年）提出的KBGD算法相比，其每次更新的计算复杂度为O(n^2)，我们的新估计器的计算负担可以被降低。",
    "tldr": "本文提出了一种基于核的随机学习算法，用于在数据丰富的环境中估计大规模半参数单调指数模型。相比现有算法，该方法具有更低的计算复杂度，并能够有效处理高维数据。"
}