{
    "title": "Computational Approaches for App-to-App Retrieval and Design Consistency Check. (arXiv:2309.10328v1 [cs.HC])",
    "abstract": "Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show tha",
    "link": "http://arxiv.org/abs/2309.10328",
    "context": "Title: Computational Approaches for App-to-App Retrieval and Design Consistency Check. (arXiv:2309.10328v1 [cs.HC])\nAbstract: Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show tha",
    "path": "papers/23/09/2309.10328.json",
    "total_tokens": 847,
    "translated_title": "应用于应用检索和设计一致性检查的计算方法",
    "translated_abstract": "从移动用户界面（UI）中提取语义表示，并将这些表示用于设计师的决策过程，已经显示出作为有效的计算设计支持工具的潜力。目前的方法依赖于在小规模移动UI数据集上训练的机器学习模型来提取语义向量，并使用屏幕截图进行对比来检索给定查询截图的相似UI。然而，这些方法的可用性有限，因为它们通常不是开源的，并且对从业人员来说，训练流程复杂，并且无法进行应用程序到应用程序的检索。为此，我们（1）使用大规模网络图片训练的视觉模型，并测试它们是否能够以零-shot方式提取UI表示，并超越现有的专用模型，以及（2）使用数学方法实现应用到应用的检索和设计一致性分析。我们的实验表明...",
    "tldr": "通过使用大规模网络图片训练的视觉模型，零-shot提取UI表示，并使用数学方法实现应用到应用的检索和设计一致性分析，我们提出了一种计算方法以解决现有方法的局限性。",
    "en_tdlr": "We propose computational approaches to overcome the limitations of existing methods by utilizing visual models trained with large-scale web images to extract UI representations in a zero-shot manner, and employing mathematical methods for app-to-app retrieval and design consistency analysis."
}