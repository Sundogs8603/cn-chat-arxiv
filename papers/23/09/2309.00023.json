{
    "title": "Continual Learning From a Stream of APIs. (arXiv:2309.00023v1 [cs.LG])",
    "abstract": "Continual learning (CL) aims to learn new tasks without forgetting previous tasks. However, existing CL methods require a large amount of raw data, which is often unavailable due to copyright considerations and privacy risks. Instead, stakeholders usually release pre-trained machine learning models as a service (MLaaS), which users can access via APIs. This paper considers two practical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL (DFCL-APIs), which achieve CL from a stream of APIs with partial or no raw data. Performing CL under these two new settings faces several challenges: unavailable full raw data, unknown model parameters, heterogeneous models of arbitrary architecture and scale, and catastrophic forgetting of previous APIs. To overcome these issues, we propose a novel data-free cooperative continual distillation learning framework that distills knowledge from a stream of APIs into a CL model by generating pseudo data, just by querying APIs. Specifically",
    "link": "http://arxiv.org/abs/2309.00023",
    "context": "Title: Continual Learning From a Stream of APIs. (arXiv:2309.00023v1 [cs.LG])\nAbstract: Continual learning (CL) aims to learn new tasks without forgetting previous tasks. However, existing CL methods require a large amount of raw data, which is often unavailable due to copyright considerations and privacy risks. Instead, stakeholders usually release pre-trained machine learning models as a service (MLaaS), which users can access via APIs. This paper considers two practical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL (DFCL-APIs), which achieve CL from a stream of APIs with partial or no raw data. Performing CL under these two new settings faces several challenges: unavailable full raw data, unknown model parameters, heterogeneous models of arbitrary architecture and scale, and catastrophic forgetting of previous APIs. To overcome these issues, we propose a novel data-free cooperative continual distillation learning framework that distills knowledge from a stream of APIs into a CL model by generating pseudo data, just by querying APIs. Specifically",
    "path": "papers/23/09/2309.00023.json",
    "total_tokens": 970,
    "translated_title": "从API流中进行持续学习",
    "translated_abstract": "持续学习旨在学习新任务而不忘记以前的任务。然而，现有的持续学习方法需要大量原始数据，由于版权考虑和隐私风险，这些数据通常不可用。相反，利益相关者通常通过API释放预训练的机器学习模型作为服务（MLaaS），用户可以通过API访问。本文考虑了两种实用但新颖的持续学习设置：数据有效的持续学习（DECL-APIs）和无数据的持续学习（DFCL-APIs），通过部分或无原始数据从API流中实现持续学习。在这两种新设置下进行持续学习面临几个挑战：无法获取完整的原始数据，未知的模型参数，任意架构和规模的异构模型以及对以前API的灾难性遗忘。为了克服这些问题，我们提出了一种新颖的无数据合作持续蒸馏学习框架，通过查询API生成伪数据，将API流中的知识蒸馏到持续学习模型中。",
    "tldr": "本文介绍了基于API流的持续学习方法，包括数据有效的持续学习和无数据的持续学习。通过查询API生成伪数据，将API流中的知识蒸馏到持续学习模型中，从而解决了无法获取完整的原始数据、未知的模型参数、异构模型和灾难性遗忘等挑战。",
    "en_tdlr": "This paper presents a continual learning approach based on a stream of APIs, including data-efficient continual learning and data-free continual learning. By generating pseudo data through querying APIs, knowledge from the API stream is distilled into the continual learning model, addressing challenges such as unavailable full raw data, unknown model parameters, heterogeneous models, and catastrophic forgetting."
}