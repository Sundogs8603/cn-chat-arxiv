{
    "title": "Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])",
    "abstract": "Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-",
    "link": "http://arxiv.org/abs/2309.06917",
    "context": "Title: Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])\nAbstract: Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-",
    "path": "papers/23/09/2309.06917.json",
    "total_tokens": 891,
    "translated_title": "使用狄利克雷生成基础的回顾的连续学习",
    "translated_abstract": "最近在面向任务的数据驱动对话系统（ToDs）方面的进展由于计算约束和耗时问题而困扰着增量学习。连续学习（CL）试图通过避免密集的预训练来解决这个问题，但它面临着灾难性遗忘（CF）的问题。虽然基于生成的回顾CL方法取得了显著进展，但生成能准确反映底层任务特定分布的伪样本仍然是一个挑战。在本文中，我们提出了狄利克雷连续学习（DCL），这是一种新颖的基于生成的回顾策略用于CL。与传统上在条件变分自动编码器（CVAE）中使用的高斯潜变量不同，DCL利用狄利克雷分布的灵活性和多样性来建模潜变量先验。这使得它能够高效地捕捉先前任务的句级特征，并有效地指导伪样本的生成。此外还引入了Jensen-苏彻利散度作为训练目标，以进一步提高DCL的性能。",
    "tldr": "该论文提出了一种新颖的基于狄利克雷生成的回顾策略，用于解决连续学习中伪样本生成的挑战。",
    "en_tdlr": "This paper proposes a novel generative-based rehearsal strategy using the Dirichlet distribution to address the challenge of generating pseudo samples in continual learning."
}