{
    "title": "CaT: Balanced Continual Graph Learning with Graph Condensation. (arXiv:2309.09455v2 [cs.LG] UPDATED)",
    "abstract": "Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lig",
    "link": "http://arxiv.org/abs/2309.09455",
    "context": "Title: CaT: Balanced Continual Graph Learning with Graph Condensation. (arXiv:2309.09455v2 [cs.LG] UPDATED)\nAbstract: Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lig",
    "path": "papers/23/09/2309.09455.json",
    "total_tokens": 830,
    "translated_title": "CaT: 带有图压缩的平衡持续图学习",
    "translated_abstract": "持续图学习（CGL）的目标是以流式方式不断更新图模型。由于模型在训练新数据时容易忘记之前学到的知识，灾难性遗忘问题已成为CGL的主要关注点。最近的基于回放的方法通过使用（1）整个新数据和（2）存储回放图以近似历史数据分布的基于采样的记忆库来更新模型，试图解决这个问题。在更新模型后，从输入图中采样的新回放图将添加到现有的记忆库中。尽管这些方法对CGL来说直观且有效，但本文发现其中存在两个问题。首先，绝大多数基于采样的方法在存储预算紧张时难以完全捕捉历史分布。其次，在复杂新数据的规模和现有数据之间存在显著的数据不平衡。",
    "tldr": "本文提出了一种名为CaT的方法，通过引入图压缩技术，解决持续图学习中存储预算紧张和数据不平衡的问题。"
}