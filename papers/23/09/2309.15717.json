{
    "title": "Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription. (arXiv:2309.15717v1 [eess.AS])",
    "abstract": "In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to perf",
    "link": "http://arxiv.org/abs/2309.15717",
    "context": "Title: Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription. (arXiv:2309.15717v1 [eess.AS])\nAbstract: In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to perf",
    "path": "papers/23/09/2309.15717.json",
    "total_tokens": 891,
    "translated_title": "Timbre-Trap:一种低资源框架用于与乐器无关的音乐转录",
    "translated_abstract": "近年来，音乐转录的研究主要集中在架构设计和乐器特定数据采集上。由于多样化数据集的不足，进展通常仅限于钢琴转录等单乐器任务。一些研究探索了多乐器转录作为一种增强模型在低资源任务上性能的手段，但这些方法面临着同样的数据可用性问题。我们提出了一种新颖的框架Timbre-Trap，它通过利用音高和音色之间的强分离性将音乐转录和音频重构统一起来。我们训练一个单独的U-Net来同时估计音高显著度和重构复杂的频谱系数，通过一个简单的切换机制在解码阶段选择两者之一的输出。这样，模型学会了产生对应于没有音色的音频的系数，可以被解释为音高显著度。我们证明了该框架能够取得优越的性能。",
    "tldr": "Timbre-Trap是一个低资源框架，将音乐转录和音频重构统一起来，通过利用音高和音色的强分离性，同时估计音高显著度和重构频谱系数，取得优越性能。"
}