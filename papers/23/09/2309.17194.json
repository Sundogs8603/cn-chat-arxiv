{
    "title": "Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])",
    "abstract": "Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture",
    "link": "http://arxiv.org/abs/2309.17194",
    "context": "Title: Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])\nAbstract: Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture",
    "path": "papers/23/09/2309.17194.json",
    "total_tokens": 875,
    "translated_title": "通过多变量投影进行广义激活",
    "translated_abstract": "激活函数对于引入神经网络的非线性起着至关重要的作用，Rectified Linear Unit (ReLU)常因其简单和有效而受青睐。受浅层前向神经网络 (FNN) 和单次投影梯度下降 (PGD) 算法之间结构相似性的启发，我们将ReLU视为从R投影到非负半线R+的操作。在这个解释基础上，我们通过用凸锥的广义投影算子替代ReLU，如二阶锥 (SOC) 投影，从而将其自然地扩展为多变量投影单元 (MPU)，这是具有多个输入和多个输出的激活函数。我们进一步提供了数学证明，证明了使用SOC投影激活的FNN在表达能力方面优于使用ReLU的FNN。通过对广泛采用的架构进行实验评估",
    "tldr": "通过将ReLU视为从R投影到非负半线R+的操作，我们将其通过用凸锥的广义投影算子替代，扩展为具有多个输入和多个输出的多变量投影单元 (MPU)激活函数，并证明其在表达能力方面优于ReLU激活的FNN。",
    "en_tdlr": "By considering ReLU as a projection from R onto the nonnegative half-line R+ and extending it with a generalized projection operator onto a convex cone, we introduce a Multivariate Projection Unit (MPU) activation function with multiple inputs and multiple outputs, and prove its superior expressive power over ReLU-activated FNNs."
}