{
    "title": "End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining. (arXiv:2309.04516v1 [eess.AS])",
    "abstract": "The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and fu",
    "link": "http://arxiv.org/abs/2309.04516",
    "context": "Title: End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining. (arXiv:2309.04516v1 [eess.AS])\nAbstract: The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and fu",
    "path": "papers/23/09/2309.04516.json",
    "total_tokens": 897,
    "translated_title": "利用声学语言模型预训练的端到端语音识别和消除语言迟滞",
    "translated_abstract": "在处理不连贯和对话语音的转录中，近年来，分为两个阶段的模型，在转录和清理阶段之间进行分离。我们认为之前的端到端消除语言迟滞的尝试不够成功，是因为大规模语言模型预训练给予了词汇模型表示优势。直到最近，大规模自监督预训练的目标对于学习有效音频表示的高维度和有限可用的大型音频数据集的发展受到限制，从而给两阶段方法带来了相对优势，该方法利用预训练表示进行词汇标记。基于最近大规模音频预训练的成功，我们重新考虑了两阶段和端到端模型之间的性能比较，并发现基于音频的语言模型使用弱自监督目标进行预训练的性能与相似训练过的两阶段模型的性能相匹配甚至超越。",
    "tldr": "该论文研究了利用声学语言模型预训练的端到端语音识别和消除语言迟滞的方法，发现基于音频的语言模型使用自监督目标进行预训练可以与两阶段模型的性能相匹配甚至超越。",
    "en_tdlr": "This paper investigates end-to-end speech recognition and disfluency removal using acoustic language model pretraining. It finds that audio-based language models pretrained with self-supervised objectives can match or exceed the performance of two-stage models."
}