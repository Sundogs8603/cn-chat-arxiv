{
    "title": "Adapting Double Q-Learning for Continuous Reinforcement Learning. (arXiv:2309.14471v1 [cs.LG])",
    "abstract": "Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.",
    "link": "http://arxiv.org/abs/2309.14471",
    "context": "Title: Adapting Double Q-Learning for Continuous Reinforcement Learning. (arXiv:2309.14471v1 [cs.LG])\nAbstract: Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.",
    "path": "papers/23/09/2309.14471.json",
    "total_tokens": 796,
    "translated_title": "为连续强化学习适应双Q-Learning",
    "translated_abstract": "大部分离线策略强化学习算法使用过高估计偏差控制技术。大多数这些技术基于启发式方法，主要解决的是过高估计的结果，而非其根本原因。本文提出了一种新颖的校正偏差的方法，类似于双Q-Learning。我们提出使用一种由两个组成成分构成的混合策略。每个策略成分由分别最大化和评估的网络处理，从而消除了过高估计偏差的基础。我们的方法在一小组MuJoCo环境上展示了令人期待的接近SOTA的结果。",
    "tldr": "本文介绍了一种新颖的校正偏差方法，通过使用两个组件的混合策略并由分开的网络进行评估，消除了离线策略强化学习算法中的过高估计偏差。在一小组MuJoCo环境中，该方法显示出了有希望接近SOTA的结果。 (校正偏差方法，混合策略，分开的网络评估)",
    "en_tdlr": "This work introduces a novel bias correction method by using a mixture strategy with two components assessed by separate networks, which eliminates the overestimation bias in off-policy reinforcement learning algorithms. The approach demonstrates promising near-SOTA results on a small set of MuJoCo environments. (bias correction method, mixture strategy, separate network evaluation)"
}