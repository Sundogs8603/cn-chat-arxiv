{
    "title": "Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition. (arXiv:2309.08436v1 [eess.AS])",
    "abstract": "We study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. This modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where EOC corresponds to the blank symbol. We further explore the remaining differences between a standard transducer and our model. Additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. Through experiments on Librispeech and TED-LIUM-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.",
    "link": "http://arxiv.org/abs/2309.08436",
    "context": "Title: Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition. (arXiv:2309.08436v1 [eess.AS])\nAbstract: We study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. This modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where EOC corresponds to the blank symbol. We further explore the remaining differences between a standard transducer and our model. Additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. Through experiments on Librispeech and TED-LIUM-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.",
    "path": "papers/23/09/2309.08436.json",
    "total_tokens": 926,
    "translated_title": "基于分块注意力编码器-解码器模型的流式语音识别研究",
    "translated_abstract": "我们研究了一种可流式运行的基于注意力的编码器-解码器模型，其中解码器或编码器和解码器都可以在预定义的固定大小的窗口（称为块）上操作。一种特殊的块结束符（EOC）符号从一个块进入到下一个块，有效地替代了传统的序列结束符。这个修改将我们的模型置于一个操作块而不是帧的转换模型，其中EOC对应空白符号。我们进一步探索了标准转换器模型和我们模型之间的其他差异。此外，我们还研究了长篇演讲的泛化能力、束搜索大小和长度规范化等相关方面。通过在Librispeech和TED-LIUM-v2上的实验，并通过连接连续的序列进行长篇试验，我们发现我们的流式模型相比于非流式变种具有竞争性的性能，并且对于长篇演讲非常泛化。",
    "tldr": "本研究提出了一种基于分块注意力编码器-解码器模型的流式语音识别方法，通过在预定义的固定大小窗口上操作，实现了模型的流式运行。实验结果表明，该模型相比非流式变种具有相当的性能，并且在长篇演讲中具有很好的泛化能力。",
    "en_tdlr": "This paper proposes a streamable attention-based encoder-decoder model for speech recognition, which operates on predefined fixed-size windows and achieves competitive performance compared to non-streamable variants, as well as good generalization in long-form speeches."
}