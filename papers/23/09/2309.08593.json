{
    "title": "Attention-Only Transformers and Implementing MLPs with Attention Heads. (arXiv:2309.08593v1 [cs.LG])",
    "abstract": "The transformer architecture is widely used in machine learning models and consists of two alternating sublayers: attention heads and MLPs. We prove that an MLP neuron can be implemented by a masked attention head with internal dimension 1 so long as the MLP's activation function comes from a restricted class including SiLU and close approximations of ReLU and GeLU. This allows one to convert an MLP-and-attention transformer into an attention-only transformer at the cost of greatly increasing the number of attention heads. We also prove that attention heads can perform the components of an MLP (linear transformations and activation functions) separately. Finally, we prove that attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error.",
    "link": "http://arxiv.org/abs/2309.08593",
    "context": "Title: Attention-Only Transformers and Implementing MLPs with Attention Heads. (arXiv:2309.08593v1 [cs.LG])\nAbstract: The transformer architecture is widely used in machine learning models and consists of two alternating sublayers: attention heads and MLPs. We prove that an MLP neuron can be implemented by a masked attention head with internal dimension 1 so long as the MLP's activation function comes from a restricted class including SiLU and close approximations of ReLU and GeLU. This allows one to convert an MLP-and-attention transformer into an attention-only transformer at the cost of greatly increasing the number of attention heads. We also prove that attention heads can perform the components of an MLP (linear transformations and activation functions) separately. Finally, we prove that attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error.",
    "path": "papers/23/09/2309.08593.json",
    "total_tokens": 801,
    "translated_title": "仅使用注意力的Transformer和使用注意力头实现MLPs",
    "translated_abstract": "Transformer架构被广泛应用于机器学习模型，由注意力头和多层感知器（MLPs）交替组成。我们证明了只要MLP的激活函数来自限制类（包括SiLU和接近的ReLU和GeLU），就可以通过带有内部维度为1的掩模注意力头来实现MLP神经元。这样就可以将MLP和注意力Transformer转换为仅注意力的Transformer，但代价是大大增加了注意力头的数量。我们还证明了注意力头可以分别执行MLP的组成部分（线性变换和激活函数）。最后，我们证明了注意力头可以在其权重矩阵中编码任意的掩码模式，并且这个近似误差可以任意小。",
    "tldr": "该论文证明了通过使用带有内部维度为1的掩模注意力头实现MLP神经元，可以将MLP和注意力Transformer转换为仅注意力的Transformer。同时，该论文还证明了注意力头可以分别执行MLP的组成部分，并且可以在其权重矩阵中编码任意的掩码模式。",
    "en_tdlr": "This paper demonstrates that MLP neurons can be implemented using masked attention heads with internal dimension 1, enabling the transformation of MLP-and-attention transformers into attention-only transformers. Additionally, attention heads are shown to be capable of performing the components of an MLP separately and encoding arbitrary masking patterns in their weight matrices."
}