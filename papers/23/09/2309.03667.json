{
    "title": "Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])",
    "abstract": "Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chain-of-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether fine-tuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-of-thought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the c",
    "link": "http://arxiv.org/abs/2309.03667",
    "context": "Title: Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])\nAbstract: Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chain-of-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether fine-tuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-of-thought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the c",
    "path": "papers/23/09/2309.03667.json",
    "total_tokens": 903,
    "translated_title": "从数学问题中生成Prolog谓词的语言模型的探索",
    "translated_abstract": "最近，由ChatGPT驱动的自然语言处理（NLP）的兴趣急剧增加。ChatGPT是一个基于Transformer的大规模生成语言模型，展示了在各种基于自然语言的任务上的多样性。然而，大型语言模型在需要推理的数学问题解决中通常表现较差。先前的研究已经证明了通过思维链激励在增强推理能力方面的有效性。现在，我们的目标是研究是否通过微调模型来生成逻辑语言（Prolog）代码，并将这些代码传递给编译器可以进一步提高准确性。因此，我们使用思维链来微调LLaMA7B作为基准模型，并开发其他用于生成Prolog代码、Prolog代码+思维链、思维链+Prolog代码的微调LLaMA7B模型。结果显示，Prolog生成模型超过了基准模型的性能，而还依然思维链。",
    "tldr": "该论文调查了将语言模型用于从数学问题中生成Prolog谓词的潜力，并展示了通过微调模型和使用思维链的方法来提高模型的准确性。结果表明，生成Prolog代码的模型在性能上超过了基准模型。",
    "en_tdlr": "This paper explores the potential of using language models to generate Prolog predicates from mathematics questions, and demonstrates the improvement in accuracy by fine-tuning the model and incorporating chain-of-thought. The results show that the Prolog generation model outperforms the baseline in performance."
}