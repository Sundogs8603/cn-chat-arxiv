{
    "title": "A Simple Text to Video Model via Transformer. (arXiv:2309.14683v1 [cs.CV])",
    "abstract": "We present a general and simple text to video model based on Transformer. Since both text and video are sequential data, we encode both texts and images into the same hidden space, which are further fed into Transformer to capture the temporal consistency and then decoder to generate either text or images. Considering the image signal may become weak in the long sequence, we introduce the U-Net to reconstruct image from its noised version. Specifically, we increase the noise level to the original image in the long sequence, then use the $down$ module from U-Net to encode noised images, which are further input to transformer to predict next clear images. We also add a constraint to promote motion between any generated image pair in the video. We use GPT2 and test our approach on UCF101 dataset and show it can generate promising videos.",
    "link": "http://arxiv.org/abs/2309.14683",
    "context": "Title: A Simple Text to Video Model via Transformer. (arXiv:2309.14683v1 [cs.CV])\nAbstract: We present a general and simple text to video model based on Transformer. Since both text and video are sequential data, we encode both texts and images into the same hidden space, which are further fed into Transformer to capture the temporal consistency and then decoder to generate either text or images. Considering the image signal may become weak in the long sequence, we introduce the U-Net to reconstruct image from its noised version. Specifically, we increase the noise level to the original image in the long sequence, then use the $down$ module from U-Net to encode noised images, which are further input to transformer to predict next clear images. We also add a constraint to promote motion between any generated image pair in the video. We use GPT2 and test our approach on UCF101 dataset and show it can generate promising videos.",
    "path": "papers/23/09/2309.14683.json",
    "total_tokens": 792,
    "translated_title": "通过Transformer的简单文本到视频模型",
    "translated_abstract": "我们提出了一种基于Transformer的通用简单文本到视频模型。由于文本和视频都是序列数据，我们将文本和图像编码为相同的隐藏空间，然后将其输入到Transformer中以捕捉时间一致性，再通过解码器生成文本或图像。考虑到长序列中图像信号可能变弱，我们引入U-Net来从图像的噪声版本中重建图像。具体来说，在长序列中，我们增加原始图像的噪声级别，然后使用U-Net中的下采样模块来编码带噪声的图像，进而输入到Transformer中以预测下一个清晰的图像。我们还添加了一个约束条件，以促进视频中任意生成的图像对之间的运动。我们使用了GPT2，并在UCF101数据集上测试了我们的方法，结果显示它可以生成有希望的视频。",
    "tldr": "该研究提出了一种基于Transformer的简单文本到视频模型，通过同时编码文本和图像到隐藏空间，并使用U-Net进行图像重建，能够生成有希望的视频。"
}