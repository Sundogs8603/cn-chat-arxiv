{
    "title": "Scaling Laws for Sparsely-Connected Foundation Models. (arXiv:2309.08520v1 [cs.LG])",
    "abstract": "We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., \"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the \"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and c",
    "link": "http://arxiv.org/abs/2309.08520",
    "context": "Title: Scaling Laws for Sparsely-Connected Foundation Models. (arXiv:2309.08520v1 [cs.LG])\nAbstract: We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., \"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the \"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and c",
    "path": "papers/23/09/2309.08520.json",
    "total_tokens": 1103,
    "translated_title": "针对稀疏连接模型的尺度定律研究",
    "translated_abstract": "本文研究了参数稀疏性对基于大规模数据集训练的Transformer（即“基础模型”）在视觉和语言领域中的尺度行为的影响。在这个设定下，我们首次确定了描述权重稀疏性、非零参数数量和训练数据量之间关系的尺度定律，并在ViT/JFT-4B和T5/C4上通过实验证明了该定律在模型和数据规模上的适用性。这些结果使我们能够确定“最佳稀疏性”，即对于给定的有效模型大小和训练预算，能够获得最佳性能的稀疏水平。对于固定数量的非零参数，我们发现最佳稀疏性随着用于训练的数据量的增加而增加。我们还扩展了研究范围，包括不同的稀疏结构（如硬件友好的n:m模式）和策略（如从预训练稠密模型开始）。本研究结果对于理解权重稀疏性在不同参数和条件下的能力和限制具有重要意义。",
    "tldr": "本研究探讨了参数稀疏性对基于大规模数据集训练的Transformer模型在视觉和语言领域中的尺度行为影响，并通过实验证明了权重稀疏性、非零参数数量和训练数据量之间的尺度定律。研究结果可以帮助确定对于给定的有效模型大小和训练预算，所需的最佳稀疏水平。同时，研究还拓展了对不同稀疏结构和策略的探究，揭示了权重稀疏性的能力和限制。",
    "en_tdlr": "This study explores the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets in vision and language domains. The study identifies the scaling law that describes the relationship between weight sparsity, number of non-zero parameters, and amount of training data. The findings provide insight into determining the optimal sparsity level for a given effective model size and training budget, and also extend the study to different sparsity structures and strategies."
}