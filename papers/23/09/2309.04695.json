{
    "title": "Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])",
    "abstract": "Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for",
    "link": "http://arxiv.org/abs/2309.04695",
    "context": "Title: Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])\nAbstract: Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for",
    "path": "papers/23/09/2309.04695.json",
    "total_tokens": 864,
    "translated_title": "在上下文中学习编程风格以解决基于知识的问答中的问题",
    "translated_abstract": "目前，针对基于知识的问答(KBQA)的方法通常依赖复杂的训练技术和模型框架，导致在实际应用中存在许多限制。最近，大型语言模型(LLMs)中的上下文学习(ICL)能力的出现为KBQA提供了一种简单且无需训练的语义解析范式：给定少量问题及其标记的逻辑表达式作为演示示例，LLMs能够理解任务意图并为新问题生成逻辑表达式。然而，当前强大的LLMs在预训练过程中对逻辑表达式的了解很少，导致格式错误率较高。为了解决这个问题，我们提出了一种针对KBQA的代码风格上下文学习方法，将陌生逻辑表达式的生成过程转换为更为熟悉的代码生成过程。对三个主流数据集的实验结果表明，我们的方法显著减轻了生成逻辑表达式中的格式错误问题。",
    "tldr": "本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。",
    "en_tdlr": "This paper proposes a code-style in-context learning method to address the formatting error problem in generating logical forms in knowledge-based question answering."
}