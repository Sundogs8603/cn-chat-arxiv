{
    "title": "Mitigating Word Bias in Zero-shot Prompt-based Classifiers. (arXiv:2309.04992v1 [cs.CL])",
    "abstract": "Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with th",
    "link": "http://arxiv.org/abs/2309.04992",
    "context": "Title: Mitigating Word Bias in Zero-shot Prompt-based Classifiers. (arXiv:2309.04992v1 [cs.CL])\nAbstract: Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with th",
    "path": "papers/23/09/2309.04992.json",
    "total_tokens": 841,
    "translated_title": "减轻零样本基于提示的分类器中的词偏差",
    "translated_abstract": "基于提示的分类器是一种吸引人的零样本分类方法。然而，提示模板和标签词的精确选择可以在很大程度上影响性能，即使在语义上等效的设置经常显示出显著的性能差异。这种差异部分可以归因于词偏差，其中分类器可能对某些类别有偏见。为了解决这个问题，可以在有标签数据集上优化分类阈值，但这也减弱了基于提示的分类器的某些优势。本文通过研究类别的期望边际概率来解决这个问题。在这里，概率被重新加权，以实现类别之间的统一先验，在无监督的方式下进行。此外，我们建立了类别先验和语言模型字先验之间的理论联系，并提供了以零资源方式设置阈值的能力。我们展示了匹配的类别先验与实际分类概率之间的强相关性。",
    "tldr": "这项研究关注减轻基于提示的分类器中的词偏差问题，并提出了一种无监督的方法来优化类别先验概率，从而提高分类性能。",
    "en_tdlr": "This study focuses on mitigating word bias in prompt-based classifiers and proposes an unsupervised approach to optimize class prior probabilities, leading to improved classification performance."
}