{
    "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study. (arXiv:2309.15800v1 [cs.CL])",
    "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good resul",
    "link": "http://arxiv.org/abs/2309.15800",
    "context": "Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study. (arXiv:2309.15800v1 [cs.CL])\nAbstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good resul",
    "path": "papers/23/09/2309.15800.json",
    "total_tokens": 857,
    "translated_title": "探索离散语音单元在语音识别、翻译和理解中的应用：一项比较研究",
    "translated_abstract": "语音信号通常以每秒数万次的速率进行采样，包含冗余信息，导致序列建模的低效性。高维度的语音特征，如频谱图，通常被用作随后模型的输入。然而，它们仍然具有冗余性。最近的研究提出了使用从自监督学习表示中得到的离散语音单元，可以显著压缩语音数据的大小。应用各种方法，如去重和子词建模，可以进一步压缩语音序列长度。因此，训练时间显著缩短，同时仍然保持不错的性能。在这项研究中，我们对离散单元在端到端语音处理模型中的应用进行了全面系统的探索。对12个自动语音识别、3个语音翻译和1个口语理解语料库的实验表明，离散单元可以取得相当好的结果。",
    "tldr": "本文就离散语音单元在语音处理模型中的应用进行了全面系统的探索，并在多个任务中进行了实验验证，结果表明离散单元表现良好。",
    "en_tdlr": "This paper comprehensively explores the application of discrete speech units in speech processing models, and experimental results on multiple tasks demonstrate their notable performance."
}