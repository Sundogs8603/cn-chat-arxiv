{
    "title": "Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])",
    "abstract": "Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the opt",
    "link": "http://arxiv.org/abs/2309.16286",
    "context": "Title: Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])\nAbstract: Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the opt",
    "path": "papers/23/09/2309.16286.json",
    "total_tokens": 898,
    "translated_title": "通用的异构联邦交叉相关和实例相似性学习",
    "translated_abstract": "联邦学习是一种重要的保护隐私的多方学习范式，涉及与他人的合作学习和对私有数据的本地更新。模型异质性和灾难性遗忘是两个重要的挑战，极大地限制了应用和泛化性能。本文提出了一种新颖的FCCL+方法，即联邦相关性和相似性学习与非目标蒸馏，促进了域内区分能力和域间泛化能力。对于异质性问题，我们利用无关的未标记公共数据来进行异构参与者之间的通信。我们构建交叉相关矩阵，并在标志和特征水平上对实例相似性分布进行对齐，有效地克服了通信障碍并提高了广泛的能力。对于本地更新阶段的灾难性遗忘，FCCL+引入了联邦非目标蒸馏，既保留了域间知识又避免了优化问题。",
    "tldr": "本文提出了一种通用的异构联邦交叉相关和实例相似性学习的方法，利用非目标蒸馏来解决模型异质性和灾难性遗忘问题，提高了联邦学习的泛化能力和应用性能。",
    "en_tdlr": "This paper presents a generalizable approach for heterogeneous federated cross-correlation and instance similarity learning, which addresses the issues of model heterogeneity and catastrophic forgetting. The proposed method utilizes non-target distillation to improve the generalization ability and applicability of federated learning."
}