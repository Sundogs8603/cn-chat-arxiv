{
    "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])",
    "abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, whic",
    "link": "http://arxiv.org/abs/2309.11235",
    "context": "Title: OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])\nAbstract: Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, whic",
    "path": "papers/23/09/2309.11235.json",
    "total_tokens": 884,
    "translated_title": "OpenChat: 用混合质量数据推进开源语言模型",
    "translated_abstract": "如今，像LLaMA这样的开源大型语言模型已经出现。最近的发展中使用了监督微调（SFT）和强化学习微调（RLFT）来使这些模型与人类目标保持一致。然而，SFT方法将所有训练数据的混合质量等同对待，而RLFT方法则需要高质量的成对或基于排名的偏好数据。在这项研究中，我们提出了一种新的框架，名为OpenChat，用于利用混合质量数据推进开源语言模型。具体而言，我们考虑了一般的SFT训练数据，其中包含了少量的专家数据和大量的次优数据，没有任何优先级标签。我们提出了C(onditioned)-RLFT，将不同的数据源视为粗粒度的奖励标签，并学习一个条件化策略，以利用互补的数据质量信息。有趣的是，在C-RLFT中，最优策略可以通过单阶段无强化学习的监督学习轻松求解，使得该问题得到了简化。",
    "tldr": "OpenChat是一种用于推进开源语言模型的新框架，能够利用混合质量数据中的信息并简化RLFT方法的求解过程。",
    "en_tdlr": "OpenChat is a novel framework that advances open-source language models by leveraging mixed-quality data and simplifying the RLFT method's solving process."
}