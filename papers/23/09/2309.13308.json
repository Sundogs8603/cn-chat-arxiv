{
    "title": "Calibrating LLM-Based Evaluator. (arXiv:2309.13308v1 [cs.CL])",
    "abstract": "Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multip",
    "link": "http://arxiv.org/abs/2309.13308",
    "context": "Title: Calibrating LLM-Based Evaluator. (arXiv:2309.13308v1 [cs.CL])\nAbstract: Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multip",
    "path": "papers/23/09/2309.13308.json",
    "total_tokens": 961,
    "translated_title": "校准基于LLM的评估器",
    "translated_abstract": "大型语言模型（LLMs）的最新进展在语言建模方面和出色的能力使它们成为有前景的无参考自然语言生成质量评估器，并且是人工评估的有竞争力替代品。然而，由于闭源或高计算消耗来托管和调节，缺乏进一步校准现成LLM评估器以实现更好的与人类一致性。在这项工作中，我们提出了AutoCalibrate，一个多阶段，无梯度的方法，用于自动校准和调整基于LLM的评估器以符合人类偏好。我们不是直接建模人类偏好，而是在一组人员标签中隐含地包括它们。然后，通过在不同的少样本示例上进行上下文学习，语言模型本身起草了一组初步的评分标准。为了进一步校准此一组标准，我们选择了表现最好的演员，并进行自我完善的再起草。我们的实验证明，多任务学习在多任务学习和评价中的有效性。",
    "tldr": "这篇论文提出了AutoCalibrate，一种多阶段、无梯度的方法，用于自动校准和调整基于LLM的评估器以符合人类偏好。通过在少样本示例上进行上下文学习，该方法隐含地包括人类偏好，并通过选择最佳表现者和自我完善来进一步校准评分标准。",
    "en_tdlr": "This paper proposes AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator towards human preference. It implicitly encompasses human preferences by leveraging in-context learning on few-shot examples and further calibrates the scoring criteria by selecting the best performers and self-refinement."
}