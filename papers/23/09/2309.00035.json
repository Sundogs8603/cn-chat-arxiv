{
    "title": "FACET: Fairness in Computer Vision Evaluation Benchmark. (arXiv:2309.00035v1 [cs.CV])",
    "abstract": "Computer vision models have known performance disparities across attributes such as gender and skin tone. This means during tasks such as classification and detection, model performance differs for certain classes based on the demographics of the people in the image. These disparities have been shown to exist, but until now there has not been a unified approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vision EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks - image classification, object detection and segmentation. For every image in FACET, we hired expert reviewers to manually annotate person-related attributes such as perceived skin tone and hair type, manually draw bounding boxes and label fine-grained person-related classes such as disk jockey or guitarist. In addition, we use FACET to benchmark state-of-the-art vision models ",
    "link": "http://arxiv.org/abs/2309.00035",
    "context": "Title: FACET: Fairness in Computer Vision Evaluation Benchmark. (arXiv:2309.00035v1 [cs.CV])\nAbstract: Computer vision models have known performance disparities across attributes such as gender and skin tone. This means during tasks such as classification and detection, model performance differs for certain classes based on the demographics of the people in the image. These disparities have been shown to exist, but until now there has not been a unified approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vision EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks - image classification, object detection and segmentation. For every image in FACET, we hired expert reviewers to manually annotate person-related attributes such as perceived skin tone and hair type, manually draw bounding boxes and label fine-grained person-related classes such as disk jockey or guitarist. In addition, we use FACET to benchmark state-of-the-art vision models ",
    "path": "papers/23/09/2309.00035.json",
    "total_tokens": 901,
    "translated_title": "FACET:计算机视觉评估基准中的公平性",
    "translated_abstract": "计算机视觉模型在性别和肤色等属性上存在已知的性能差异。这意味着在分类和检测等任务中，模型对于特定类别的性能会根据图像中人的人口统计学而有所不同。这些差异已被证明存在，但直到现在还没有一个统一的方法来衡量计算机视觉模型在常见的用例中的这些差异。我们提出了一个名为FACET（FAirness in Computer Vision EvaluaTion）的新基准，这是一个公开可用的大型评估集，包含了三种最常见的视觉任务-图像分类、物体检测和分割的32k张图片。对于FACET中的每一张图片，我们雇用专家评审员手动标注与人相关的属性，如感知的肤色和发型类型，手动绘制边界框，并标记细粒度的与人相关的类别，如碟艺人或吉他手。此外，我们使用FACET来评估最先进的视觉模型。",
    "tldr": "FACET是一个用于评估计算机视觉模型公平性的大型基准，提供了可公开访问的图像集，对于常见的视觉任务进行了标注，同时使用专家评审员手动标注人类属性和类别信息，并用于对最先进的视觉模型进行基准测试。",
    "en_tdlr": "FACET is a large benchmark for evaluating fairness in computer vision models, providing a publicly available dataset annotated with person-related attributes and categories, and used for benchmarking state-of-the-art vision models."
}