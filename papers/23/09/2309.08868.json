{
    "title": "MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])",
    "abstract": "International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.",
    "link": "http://arxiv.org/abs/2309.08868",
    "context": "Title: MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])\nAbstract: International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.",
    "path": "papers/23/09/2309.08868.json",
    "total_tokens": 900,
    "translated_title": "MHLAT: 自动ICD编码的多跳标签关注模型",
    "translated_abstract": "国际疾病分类（ICD）编码是将ICD诊断代码分配给临床笔记的任务。鉴于大量的标签（近9000个）和庞大的文本（多达8000个标记），这可能是具有挑战性的。然而，与以前的研究中的单通读过程不同，人们倾向于再次阅读文本和标签定义以获得更自信的答案。此外，尽管预训练语言模型已被用于解决这些问题，但它们的内存使用量很大。为了解决上述问题，我们提出了一个简单但有效的模型，称为多跳标签关注（MHLAT），利用多跳标签关注来获取更准确和丰富的表示。对三个基准MIMIC数据集的广泛实验表明，我们的方法在所有七个度量标准上实现了明显更好或有竞争力的性能，并且需要优化的参数更少。",
    "tldr": "我们提出了一种名为MHLAT的简单但有效的模型，利用多跳标签关注来提供更准确和丰富的表示。实验结果表明，我们的方法在所有七个度量标准上都实现了明显更好或有竞争力的性能，并且需要优化的参数更少。"
}