{
    "title": "Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization. (arXiv:2309.15603v1 [cs.LG])",
    "abstract": "In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperform",
    "link": "http://arxiv.org/abs/2309.15603",
    "context": "Title: Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization. (arXiv:2309.15603v1 [cs.LG])\nAbstract: In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperform",
    "path": "papers/23/09/2309.15603.json",
    "total_tokens": 892,
    "translated_title": "在多任务强化学习中，通过最优输运正则化来提取知识",
    "translated_abstract": "在多任务强化学习中，通过从其他不同但相关的任务中转移知识，可以提高训练代理的数据效率。传统方法依赖于Kullback-Leibler正则化来稳定从一个任务到其他任务的知识转移。本文中，我们探索了用一种新颖的基于最优输运的正则化来替代Kullback-Leibler散度的方向。通过使用Sinkhorn映射，我们可以近似计算任务的状态分布之间的最优输运距离。然后将该距离作为分摊奖励，用来规范信息共享的量。我们在几个基于栅格的导航多目标环境上进行了实验，验证了这种方法的有效性。结果表明，我们添加的基于最优输运的奖励能够加速代理学习过程并超过了传统方法。",
    "tldr": "本论文提出了一种在多任务强化学习中使用最优输运正则化来提取知识的方法，通过近似计算任务状态分布之间的最优输运距离，并将其作为奖励来规范信息分享的量。实验证明这种方法能够显著加速代理的学习过程并超越传统方法。"
}