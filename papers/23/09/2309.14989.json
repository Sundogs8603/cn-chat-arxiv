{
    "title": "Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])",
    "abstract": "We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\\mathfrak{t} \\in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\\mathfrak{t}_k$ allocates $\\Delta \\mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \\textit{interaction times} ($\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfra",
    "link": "http://arxiv.org/abs/2309.14989",
    "context": "Title: Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])\nAbstract: We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\\mathfrak{t} \\in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\\mathfrak{t}_k$ allocates $\\Delta \\mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \\textit{interaction times} ($\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfra",
    "path": "papers/23/09/2309.14989.json",
    "total_tokens": 924,
    "translated_title": "非平稳强化学习中的节奏适应",
    "translated_abstract": "首先我们提出并解决了非平稳强化学习中的“时间同步”问题，这是阻碍其在真实世界应用中的一个关键因素。现实中，环境的变化是按照墙钟时间（$\\mathfrak{t}$）而不是按照情节进展（$k$）发生的，其中墙钟时间表示固定持续时间$\\mathfrak{t} \\in [0, T]$内实际流逝的时间。在现有的工作中，在情节$k$时，智能体生成一个轨迹并训练一个策略，然后转入情节$k+1$。然而，在时间不同步的环境下，智能体在时间$\\mathfrak{t}_k$分配$\\Delta \\mathfrak{t}$用于轨迹生成和训练，然后在$\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$时刻转入下一个情节。尽管情节总数固定（$K$），智能体根据相互作用时间的选择（$\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K$）积累不同的轨迹。",
    "tldr": "该论文解决了非平稳强化学习中\"时间同步\"问题，通过考虑墙钟时间而不是情节进展来实现对环境变化的适应。"
}