{
    "title": "Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])",
    "abstract": "Artificial Intelligence makes great advances today and starts to bridge the gap between vision and language. However, we are still far from understanding, explaining and controlling explicitly the visual content from a linguistic perspective, because we still lack a common explainable representation between the two domains. In this work we come to address this limitation and propose the Graph of Events in Space and Time (GEST), by which we can represent, create and explain, both visual and linguistic stories. We provide a theoretical justification of our model and an experimental validation, which proves that GEST can bring a solid complementary value along powerful deep learning models. In particular, GEST can help improve at the content-level the generation of videos from text, by being easily incorporated into our novel video generation engine. Additionally, by using efficient graph matching techniques, the GEST graphs can also improve the comparisons between texts at the semantic l",
    "link": "http://arxiv.org/abs/2309.08612",
    "context": "Title: Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])\nAbstract: Artificial Intelligence makes great advances today and starts to bridge the gap between vision and language. However, we are still far from understanding, explaining and controlling explicitly the visual content from a linguistic perspective, because we still lack a common explainable representation between the two domains. In this work we come to address this limitation and propose the Graph of Events in Space and Time (GEST), by which we can represent, create and explain, both visual and linguistic stories. We provide a theoretical justification of our model and an experimental validation, which proves that GEST can bring a solid complementary value along powerful deep learning models. In particular, GEST can help improve at the content-level the generation of videos from text, by being easily incorporated into our novel video generation engine. Additionally, by using efficient graph matching techniques, the GEST graphs can also improve the comparisons between texts at the semantic l",
    "path": "papers/23/09/2309.08612.json",
    "total_tokens": 869,
    "translated_title": "通过时空事件图解释视觉与语言",
    "translated_abstract": "人工智能在今天取得了巨大的进展，并开始弥合视觉与语言之间的鸿沟。然而，从语言的角度来理解、解释和明确控制视觉内容仍然存在很大困难，因为我们在两个领域之间仍然缺乏一个共同的可解释性表示。在这项工作中，我们找到了解决这个限制的方法，并提出了时空事件图（GEST），通过它我们可以表示、创建和解释视觉和语言故事。我们提供了对我们模型的理论验证和实验验证，证明了GEST在强大的深度学习模型之外能够带来坚实的互补价值。特别地，GEST可以通过容易地被整合到我们的新型视频生成引擎中，帮助在内容层面改进从文本到视频的生成。此外，通过使用有效的图匹配技术，GEST图还可以改进语义上的文本比较。",
    "tldr": "本论文提出了一种称为时空事件图（GEST）的方法，能够解释、表示和生成视觉和语言故事。通过将GEST图与深度学习模型相结合，可以改善从文本到视频的生成，并提高语义上的文本比较。",
    "en_tdlr": "This paper proposes an approach called Graph of Events in Space and Time (GEST) to explain, represent, and generate visual and linguistic stories. By combining GEST graphs with deep learning models, it can improve video generation from text and enhance semantic text comparison."
}