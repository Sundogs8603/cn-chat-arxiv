{
    "title": "EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets. (arXiv:2309.04725v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown promising performance on various NLP tasks via task prompting. And their performance can be further improved by appending task demonstrations to the head of the prompt. And usually, a better performance can be achieved with more demonstrations. However, asking the users to write the demonstrations can be cumbersome. As a simple yet cost-effective workaround, this paper proposes a novel method called EPA (\\textbf{E}asy \\textbf{P}rompt \\textbf{A}ugmentation)\\footnote{While this paper considers augmenting prompts via demonstrations, we name it EPA as the name EDA is already taken by a well-known NLP method \\citep{wei-zou-2019-eda}.} that effectively minimizes user efforts in writing demonstrations while improving the model performance at the same time. EPA achieves these goals by automatically augmenting the demonstrations with multiple sources/targets, where each of them paraphrases each other. This is well motivated as augmenting data via paraphra",
    "link": "http://arxiv.org/abs/2309.04725",
    "context": "Title: EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets. (arXiv:2309.04725v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown promising performance on various NLP tasks via task prompting. And their performance can be further improved by appending task demonstrations to the head of the prompt. And usually, a better performance can be achieved with more demonstrations. However, asking the users to write the demonstrations can be cumbersome. As a simple yet cost-effective workaround, this paper proposes a novel method called EPA (\\textbf{E}asy \\textbf{P}rompt \\textbf{A}ugmentation)\\footnote{While this paper considers augmenting prompts via demonstrations, we name it EPA as the name EDA is already taken by a well-known NLP method \\citep{wei-zou-2019-eda}.} that effectively minimizes user efforts in writing demonstrations while improving the model performance at the same time. EPA achieves these goals by automatically augmenting the demonstrations with multiple sources/targets, where each of them paraphrases each other. This is well motivated as augmenting data via paraphra",
    "path": "papers/23/09/2309.04725.json",
    "total_tokens": 826,
    "translated_title": "EPA: 通过多个来源和多个目标实现大型语言模型上的简易提示增强",
    "translated_abstract": "大型语言模型（LLM）通过任务提示已经在各种自然语言处理任务上展现出了有希望的性能。通过在提示头部添加任务演示可以进一步提高性能，并且通常情况下，使用更多的演示可以达到更好的性能。然而，要求用户编写演示可能会很麻烦。作为一种简单而具有成本效益的解决方法，本文提出了一种名为EPA (Easy Prompt Augmentation)的新方法，旨在在提高模型性能的同时，有效减少用户编写演示的工作量。EPA通过自动使用多个来源/目标来扩充演示，其中每个来源/目标互为释义，从而实现这些目标。",
    "tldr": "本论文提出了一种名为EPA的简易提示增强方法，通过自动使用多个来源/目标来扩充演示，从而提高了大型语言模型的性能，减少了用户编写演示的工作量。",
    "en_tdlr": "This paper proposes a method called EPA (Easy Prompt Augmentation) that effectively enhances large language models by automatically augmenting demonstrations with multiple sources/targets, improving performance while reducing user effort in writing demonstrations."
}