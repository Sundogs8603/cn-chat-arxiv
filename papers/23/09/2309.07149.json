{
    "title": "Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])",
    "abstract": "Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. \"brain decoding\"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba",
    "link": "http://arxiv.org/abs/2309.07149",
    "context": "Title: Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])\nAbstract: Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. \"brain decoding\"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba",
    "path": "papers/23/09/2309.07149.json",
    "total_tokens": 946,
    "translated_title": "通过知识蒸馏和潜在扩散模型从脑电图中解码视觉脑表示",
    "translated_abstract": "从人脑活动中解码视觉表示已成为一个蓬勃发展的研究领域，特别是在脑机接口的背景下。我们的研究提出了一种创新方法，利用来自图像Net数据集的脑电图（EEG）数据来分类和重构图像（即“脑解码”）。我们分析了来自6名参与者的EEG记录，每名参与者观看了覆盖40个独特语义类别的50个图像。这些EEG读数被转换为频谱图，然后用于训练一个卷积神经网络（CNN），集成了基于预训练的对比语言-图像预训练（CLIP）图像分类教师网络的知识蒸馏过程。这种策略使我们的模型达到了80%的前五位准确率，显著优于标准CNN和各种基于RNN的基准。此外，我们还引入了一种图像重构机制。",
    "tldr": "本研究提出了一种创新方法，利用脑电图数据解码人脑中的视觉表示。通过将EEG数据转换为频谱图并使用卷积神经网络进行训练，结合基于知识蒸馏的图像分类教师网络，我们的模型在图像分类和重建任务上表现出色。"
}