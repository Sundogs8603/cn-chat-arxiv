{
    "title": "FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)",
    "abstract": "Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b",
    "link": "http://arxiv.org/abs/2309.14062",
    "context": "Title: FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)\nAbstract: Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b",
    "path": "papers/23/09/2309.14062.json",
    "total_tokens": 964,
    "translated_title": "FeCAM：在免去样本的连续学习中利用类别分布的异质性",
    "translated_abstract": "免去样本的增量式学习（CIL）面临着许多挑战，因为它禁止了来自先前任务的数据回顾，从而导致了灾难性遗忘。最近的增量学习方法通过在第一个任务之后冻结特征提取器来学习分类器，已经引起了广泛关注。在本文中，我们探索了用于CIL的原型网络，该网络使用冻结的特征提取器生成新的类别原型，并根据到原型的欧氏距离对特征进行分类。通过对类别特征分布进行分析，我们发现基于欧氏度量的分类对于联合训练的特征是成功的。然而，当从非恒定数据中学习时，我们观察到欧氏度量是次优的，并且特征分布是异质的。为了解决这个挑战，我们重新审视了用于CIL的各向异性马哈拉诺比斯距离。此外，我们通过实验证明了建模特征协方差关系的重要性。",
    "tldr": "本文针对免去样本的增量式学习（CIL）中的异质性类别分布问题，使用原型网络和改进的各向异性马哈拉诺比斯距离进行特征分类和建模，有效解决了非恒定数据学习中的特征分布异质性挑战。"
}