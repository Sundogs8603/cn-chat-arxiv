{
    "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying. (arXiv:2309.08351v1 [cs.CL])",
    "abstract": "Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.",
    "link": "http://arxiv.org/abs/2309.08351",
    "context": "Title: Headless Language Models: Learning without Predicting with Contrastive Weight Tying. (arXiv:2309.08351v1 [cs.CL])\nAbstract: Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.",
    "path": "papers/23/09/2309.08351.json",
    "total_tokens": 744,
    "translated_title": "无头语言模型：通过对比权重绑定学习而非预测",
    "translated_abstract": "自监督的语言模型预训练通常涉及对大量单词进行概率预测。在本研究中，我们提出了一种创新的方法，将注意力从概率预测转移到通过对比权重绑定的方式对输入嵌入进行重构。我们将这种方法应用于在单语和多语境下预训练无头语言模型。我们的方法具有实际优势，可以将训练计算要求减少高达20倍，同时增强下游性能和数据效率。与在相似计算预算下的传统语言模型相比，我们观察到显著的+1.6 GLUE分数增加和显著的+2.7 LAMBADA准确性提高。",
    "tldr": "该论文提出了一种无头语言模型的创新方法，通过对比权重绑定的方式对输入嵌入进行重构，从而提高了下游性能和数据效率，并且在计算预算相似的情况下，获得了显著的GLUE分数增加和LAMBADA准确性提高。"
}