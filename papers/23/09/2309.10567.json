{
    "title": "Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])",
    "abstract": "Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.",
    "link": "http://arxiv.org/abs/2309.10567",
    "context": "Title: Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])\nAbstract: Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.",
    "path": "papers/23/09/2309.10567.json",
    "total_tokens": 833,
    "translated_title": "多模态建模用于口语识别",
    "translated_abstract": "口语识别是指在给定的话语中自动预测口语的任务。传统上，它被建模为一种基于语音的语言识别任务。以往的技术都局限于单一模态；然而，在视频数据中，存在许多其他元数据，这些元数据对于这个任务可能会有益处。在这项工作中，我们提出了MuSeLI，一种多模态口语识别方法，它深入研究了使用各种元数据源来增强语言识别。我们的研究发现，诸如视频标题、描述和地理位置等元数据提供了大量信息，能够识别多媒体录制的口语。我们利用两个不同的YouTube视频的公共数据集进行实验，并在语言识别任务中获得了最新的结果。我们还进行了一项消融研究，描述了每种模态对语言识别的独特贡献。",
    "tldr": "该论文提出了一种多模态口语识别方法MuSeLI，利用视频标题、描述和地理位置等元数据来增强语言识别任务，并在两个YouTube视频数据集上获得了最新的结果。",
    "en_tdlr": "The paper proposes a multimodal approach, MuSeLI, for spoken language identification, which utilizes metadata like video title, description, and geographic location to enhance the language identification task. It achieves state-of-the-art results on two diverse YouTube video datasets."
}