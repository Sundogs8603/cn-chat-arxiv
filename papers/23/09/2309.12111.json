{
    "title": "Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])",
    "abstract": "Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet m",
    "link": "http://arxiv.org/abs/2309.12111",
    "context": "Title: Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])\nAbstract: Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet m",
    "path": "papers/23/09/2309.12111.json",
    "total_tokens": 935,
    "translated_title": "使用循环模型的音频-乐谱检索的段落摘要",
    "translated_abstract": "跨模态音乐检索的许多应用与将乐谱图像与音频录音连接在一起有关。目前的典型方法是通过深度神经网络学习一个关联短固定大小的音频和乐谱片段的联合嵌入空间，通过适当的相似性结构。然而，这种策略带来的两个挑战是训练网络需要强对齐的数据，并且由于局部和全局速度差异而造成的音频和乐谱片段之间的音乐内容差异。在本文中，我们通过设计一个跨模态循环网络来解决这两个缺点，该网络学习可以摘要对应音频和乐谱的更长段落的联合嵌入。我们方法的好处是它只需要弱对齐的音频-乐谱对，以及循环网络能够处理音频和乐谱之间的节奏变化导致的非线性。",
    "tldr": "本文提出了一个使用循环模型的音频-乐谱检索方法，通过学习跨模态循环网络生成可以摘要对应音频和乐谱的更长段落的联合嵌入。相比于传统方法，该方法只需要弱对齐的音频-乐谱对，并且能够处理音频和乐谱之间的节奏变化导致的非线性。",
    "en_tdlr": "This paper proposes a passage summarization method for audio-sheet music retrieval using recurrent models. The method learns a cross-modal recurrent network that can generate joint embeddings to summarize longer passages of corresponding audio and sheet music. It only requires weakly aligned audio-sheet music pairs and handles the non-linearities caused by tempo variations between audio and sheet music."
}