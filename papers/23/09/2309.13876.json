{
    "title": "Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. (arXiv:2309.13876v2 [cs.CL] UPDATED)",
    "abstract": "Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.",
    "link": "http://arxiv.org/abs/2309.13876",
    "context": "Title: Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. (arXiv:2309.13876v2 [cs.CL] UPDATED)\nAbstract: Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.",
    "path": "papers/23/09/2309.13876.json",
    "total_tokens": 909,
    "translated_title": "使用开源工具和公开可用数据复现Whisper风格训练",
    "translated_abstract": "在大量数据上预训练语音模型取得了显著的成功。OpenAI的Whisper是一个多语言多任务模型，经过了680k小时的监督式语音数据训练。它在各种语音识别和翻译基准测试中表现出良好的泛化能力，甚至在零样本设置中也能够发挥良好的作用。然而，开发这种模型的完整流程（从数据收集到训练）并不公开可访问，这使得研究人员难以进一步改进其性能并解决训练相关的问题，如效率、健壮性、公平性和偏见。本文介绍了一个名为Open Whisper-style Speech Model（OWSM）的模型，使用开源工具和公开可用数据复现了Whisper风格的训练。OWSM甚至支持更多的翻译方向，并且可以更高效地训练。我们将公开发布用于数据准备、训练、推理和评分的所有脚本，以及预训练模型和训练日志，以促进开放科学。",
    "tldr": "本研究复现了Whisper风格的训练，使用开源工具和公开可用数据开发了一个名为OWSM的模型，支持更多的翻译方向并且更高效地训练。"
}