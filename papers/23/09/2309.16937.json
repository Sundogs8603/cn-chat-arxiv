{
    "title": "SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])",
    "abstract": "Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C",
    "link": "http://arxiv.org/abs/2309.16937",
    "context": "Title: SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])\nAbstract: Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C",
    "path": "papers/23/09/2309.16937.json",
    "total_tokens": 915,
    "translated_title": "SSHR: 利用自监督层次表示提高多语种自动语音识别的能力",
    "translated_abstract": "多语种自动语音识别（ASR）系统因其在全球范围内扩展语言覆盖的潜力而受到关注。虽然自监督学习（SSL）在多语种ASR方面表现出了其有效性，但值得注意的是，SSL模型的不同层次表示可能包含尚未充分利用的不同信息。在本研究中，我们提出了一种利用自监督层次表示（SSHR）来优化多语种ASR的新方法。我们首先分析SSL模型的不同层次对于语言相关和内容相关信息的表达情况，发现具有更强相关性的层次。然后，我们从相关中间层中提取语言相关帧，并通过自注意机制引导特定内容的提取。此外，我们通过提出的交叉CTC方法，引导模型在最终层次获得更多内容相关信息。我们在两个多语种数据集（C）上评估了SSHR的性能。",
    "tldr": "本研究提出了一种利用自监督层次表示（SSHR）优化多语种自动语音识别的方法。通过分析自监督学习模型的不同层次表示，提取出语言相关帧和特定内容，并引导模型在最终层次获取更多内容相关信息。",
    "en_tdlr": "This study proposes a method called SSHR that leverages self-supervised hierarchical representations to optimize multilingual automatic speech recognition. By analyzing different layers of the self-supervised learning model, it extracts language-related frames, guides specific content extraction, and steers the model to acquire more content-related information in the final layers."
}