{
    "title": "Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])",
    "abstract": "This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original",
    "link": "http://arxiv.org/abs/2310.11104",
    "context": "Title: Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])\nAbstract: This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original",
    "path": "papers/23/10/2310.11104.json",
    "total_tokens": 944,
    "translated_title": "ReLU-FNNs的本地Lipschitz常数计算：使用精确性验证计算上界",
    "translated_abstract": "本文关注使用修正线性单元（ReLUs）作为激活函数的前馈神经网络（FNNs）的本地Lipschitz常数计算。对于目标输入，FNN的本地Lipschitz常数是衡量其可靠性的合理指标。通过使用捕捉ReLUs行为的乘法器的标准过程，我们首先将本地Lipschitz常数的上界计算问题简化为一个半定规划问题（SDP）。在这里，我们引入了新的共正乘法器来准确捕捉ReLU的行为。然后，通过考虑用于上界计算的SDP的对偶问题，我们进一步得出了一个可行的测试来确定计算上界的精确性。然而，对于具有数百个ReLU的实际FNNs，这些SDP是无法解决的。为了解决这个问题，我们进一步提出了一种构造减序模型的方法，其输入输出属性与原始模型相同。",
    "tldr": "本文研究了使用ReLUs作为激活函数的前馈神经网络的本地Lipschitz常数计算，并介绍了一种通过精确性验证计算上界的方法。",
    "en_tdlr": "This paper focuses on the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with rectified linear units (ReLUs) as activation functions. It introduces copositive multipliers to accurately capture the behavior of ReLUs and proposes a method to conclude the exactness of the computed upper bound. Additionally, it addresses the intractability issue for practical FNNs with hundreds of ReLUs by constructing a reduced order model with identical input-output properties."
}