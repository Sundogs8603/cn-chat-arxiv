{
    "title": "Towards Example-Based NMT with Multi-Levenshtein Transformers. (arXiv:2310.08967v1 [cs.CL])",
    "abstract": "Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions.  For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multi-way alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances.",
    "link": "http://arxiv.org/abs/2310.08967",
    "context": "Title: Towards Example-Based NMT with Multi-Levenshtein Transformers. (arXiv:2310.08967v1 [cs.CL])\nAbstract: Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions.  For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multi-way alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances.",
    "path": "papers/23/10/2310.08967.json",
    "total_tokens": 888,
    "translated_title": "运用多Levenshtein Transformer进行基于示例的NMT研究",
    "translated_abstract": "检索增强的机器翻译（RAMT）引起了越来越多的关注。这是因为RAMT不仅可以提高翻译度量，而且还被认为可以实现某种形式的领域适应。在这项研究中，我们研究了RAMT的另一个显著特点，即允许用户返回对翻译决策有贡献的示例，从而使翻译决策更加透明。为此，我们提出了一种新的架构，旨在增加这种透明度。该模型采用了一个检索增强的Levenshtein Transformer的版本，并使其能够同时编辑内存中找到的多个模糊匹配。我们讨论了如何在该模型中进行训练和推理，基于多路径对齐算法和模仿学习。我们的实验证明，编辑多个示例对翻译得分有积极的影响，特别是增加了从现有实例中复制的目标跨度的数量。",
    "tldr": "本论文研究了检索增强的机器翻译的透明度特性，提出了一个新的架构，旨在增加用户对翻译决策的透明度，并通过编辑多个示例来提高翻译效果。",
    "en_tdlr": "This paper investigates the transparency feature of retrieval-augmented machine translation (RAMT) and proposes a novel architecture to increase user transparency in translation decisions by editing multiple examples, which positively affects translation scores."
}