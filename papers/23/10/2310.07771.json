{
    "title": "DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])",
    "abstract": "With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of",
    "link": "http://arxiv.org/abs/2310.07771",
    "context": "Title: DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])\nAbstract: With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of",
    "path": "papers/23/10/2310.07771.json",
    "total_tokens": 925,
    "translated_title": "DrivingDiffusion：基于布局引导的多视角驾驶场景视频生成与潜在扩散模型",
    "translated_abstract": "随着基于强大和统一的鸟瞰图（BEV）表示的自动驾驶的普及，对具有准确注释的高质量和大规模多视角视频数据的需求迫切存在。然而，由于昂贵的采集和注释成本，获得此类大规模多视角数据并非易事。为了缓解这个问题，我们提出了一种基于空间-时间一致性的扩散框架DrivingDiffusion，以生成由3D布局控制的逼真多视角视频。在给定3D布局的情况下合成多视角视频时，存在三个挑战：如何保持1）跨视角的一致性和2）跨帧的一致性？3）如何保证生成实例的质量？我们的DrivingDiffusion通过级联多视角单帧图像生成步骤、被多个摄像机共享的单视角视频生成步骤以及能够处理长视频生成的后处理来解决这个问题。在多视角模型中，生成实例的一致性能够保证。",
    "tldr": "DrivingDiffusion是一个基于3D布局的多视角驾驶场景视频生成框架，通过解决跨视角和跨帧的一致性问题，以及保证生成实例质量，实现了逼真的多视角视频生成。",
    "en_tdlr": "DrivingDiffusion is a multi-view driving scene video generation framework based on 3D layout, which achieves realistic multi-view video generation by addressing the challenges of cross-view and cross-frame consistency, as well as ensuring the quality of generated instances."
}