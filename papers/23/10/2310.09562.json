{
    "title": "Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?",
    "abstract": "arXiv:2310.09562v2 Announce Type: replace-cross  Abstract: Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to ",
    "link": "https://arxiv.org/abs/2310.09562",
    "context": "Title: Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?\nAbstract: arXiv:2310.09562v2 Announce Type: replace-cross  Abstract: Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to ",
    "path": "papers/23/10/2310.09562.json",
    "total_tokens": 908,
    "translated_title": "CLIP的泛化性能主要源于训练-测试之间的高相似性吗？",
    "translated_abstract": "基于 CLIP 等基础模型被训练在数亿样本上，能够轻松泛化到新任务和输入。CLIP 出色地展示了在广泛的超出分布（OOD）基准上的零样本和少样本能力，而先前的研究主要将其归因于当今的大规模和全面的训练数据集（如 LAION）。然而，对于 CLIP 来说，像超出分布泛化这样的术语是否具有意义是值得怀疑的，因为像 LAION 这样的网页规模数据集可能只是包含许多与最初为 ImageNet 设计的常见 OOD 基准相似的样本。为了测试这一假设，我们在复制 ImageNet 的训练-测试相似性相对于常见 OOD 基准的剪枝 LAION 分割上重新训练 CLIP。虽然我们观察到在一些基准上的性能下降，但令人惊讶的是，CLIP 的整体性能仍然很高。这表明高训练-测试相似性是不足以...",
    "tldr": "CLIP在经过重现ImageNet训练-测试相似性的剪枝LAION分割重新训练后，虽然在某些基准上表现有所下降，但整体性能仍然很高"
}