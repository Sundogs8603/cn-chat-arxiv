{
    "title": "VeCLIP: Improving CLIP Training via Visual-enriched Captions",
    "abstract": "arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our",
    "link": "https://arxiv.org/abs/2310.07699",
    "context": "Title: VeCLIP: Improving CLIP Training via Visual-enriched Captions\nAbstract: arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our",
    "path": "papers/23/10/2310.07699.json",
    "total_tokens": 889,
    "translated_title": "VeCLIP：通过富含视觉信息的标题改进CLIP训练",
    "translated_abstract": "大规模网络爬取数据集对于预训练视觉-语言模型（如CLIP）的成功至关重要。然而，网络爬取的AltTexts存在固有的噪音和潜在的不相关性，造成了精确的图像-文字对齐方面的挑战。本研究引入了一种适用于嘈杂标题重写的可扩展流程。与利用大型语言模型（LLMs）进行标题重写的现有方法在小型策划数据集（如CC3M和CC12M）上已经显示出了希望。我们强调将视觉概念融入标题中，称为富含视觉信息的标题（VeCap），以确保数据多样性。为了优化AltTexts与新生成的VeCap的利用，我们提出了一种新颖的混合训练方案。我们展示了该方法在大规模网络爬取数据集上训练CLIP的适应性，称为VeCLIP。通过使用这种经济有效的流程，我们轻松扩展了我们的实验。",
    "tldr": "本研究提出了一种通过将视觉概念融入标题中的方式来改进CLIP训练的方法，名为VeCLIP，该方法在大规模网络爬取数据集上展示了良好的性能。"
}