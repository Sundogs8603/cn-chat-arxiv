{
    "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
    "abstract": "arXiv:2310.08491v2 Announce Type: replace  Abstract: Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric",
    "link": "https://arxiv.org/abs/2310.08491",
    "context": "Title: Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\nAbstract: arXiv:2310.08491v2 Announce Type: replace  Abstract: Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric",
    "path": "papers/23/10/2310.08491.json",
    "total_tokens": 898,
    "translated_title": "Prometheus: 在语言模型中引入细粒度评估能力",
    "translated_abstract": "最近，使用强大的专有大型语言模型（例如GPT-4）作为长篇回答的评估者已经成为事实上的标准。然而，对于具有大规模评估任务和考虑自定义标准（例如儿童可读性）的从业者来说，使用专有LLMs作为评估者是不可靠的，这是由于其闭源性质、无控制的版本和高昂的成本。在这项工作中，我们提出了Prometheus，这是一个完全开源的LLM，只要携带适当的参考材料（参考答案、评分标准），就可以与GPT-4的评估能力相媲美。我们首先构建了反馈收集（Feedback Collection），这是一个由1K个细粒度评分标准、20K条指令和GPT-4生成的100K条响应和语言反馈组成的新数据集。使用这个反馈收集，我们训练了Prometheus，一个13B的评估者LLM，可以根据定制的评分标准评估任何给定的长篇文本。",
    "tldr": "Prometheus是一个开源的LLM，通过使用自定义评分标准和适当的参考材料，可以在与GPT-4相媲美的评估能力上进行评估。",
    "en_tdlr": "Prometheus is an open-source LLM that can evaluate any given long-form text on par with GPT-4's evaluation capabilities by using customized score rubric and appropriate reference materials."
}