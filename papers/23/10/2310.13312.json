{
    "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models. (arXiv:2310.13312v1 [cs.CL])",
    "abstract": "Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.",
    "link": "http://arxiv.org/abs/2310.13312",
    "context": "Title: Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models. (arXiv:2310.13312v1 [cs.CL])\nAbstract: Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.",
    "path": "papers/23/10/2310.13312.json",
    "total_tokens": 913,
    "translated_title": "探索语料库多样性对金融预训练语言模型的影响",
    "translated_abstract": "在过去的几年里，已经提出了各种特定领域的预训练语言模型（PLMs），在生物医学、科学和临床等专业领域表现出色。此外，由于金融数据分析的重大经济影响，金融PLM也得到了研究。然而，我们发现金融PLMs在预训练中没有充分多样化的金融数据。这种缺乏多样性的训练数据导致了较差的泛化性能，在许多下游任务上，通用的PLMs，包括BERT，往往优于金融PLMs。为了解决这个问题，我们收集了各种广泛的金融语料库，并在这些不同的数据集上训练金融语言模型（FiLM）。我们的实验证据证实，FiLM不仅优于现有的金融PLMs，而且优于通用领域的PLMs。此外，我们还提供了实证证据，即这种改进即使对于未见过的语料组也可以实现。",
    "tldr": "本研究探索了金融预训练语言模型的语料库多样性对其性能的影响，并通过在多样化的金融语料库上训练的新模型FiLM，取得了比现有模型更好的结果。",
    "en_tdlr": "This study explores the impact of corpus diversity on financial pretrained language models (PLMs), and achieves better results than existing models by training a new model called FiLM on diverse financial corpora."
}