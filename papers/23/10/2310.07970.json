{
    "title": "Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])",
    "abstract": "Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demo",
    "link": "http://arxiv.org/abs/2310.07970",
    "context": "Title: Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])\nAbstract: Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demo",
    "path": "papers/23/10/2310.07970.json",
    "total_tokens": 905,
    "translated_title": "超参数自适应搜索用于代理优化：一种自调整方法",
    "translated_abstract": "代理优化算法在优化昂贵的黑盒函数方面表现出了潜力。然而，它们的性能受与采样和代理拟合相关的超参数的影响很大，这对于它们的广泛应用构成挑战。我们研究了各种代理优化算法中超参数的影响，并提出了一种超参数自适应搜索的代理优化方法（HASSO）。HASSO不是一个超参数调整算法，而是一种通用的自调整代理优化算法，它在同时优化主要目标函数的过程中动态调整自己的超参数，而不需要额外的评估。其目标是提高代理优化算法对于从业者的可访问性、效果和收敛速度。我们的方法识别并修改了每个问题和代理优化方法特定的最有影响力的超参数，减少了手动调整的需求，同时不显著增加计算负担。实验结果演示了我们方法的有效性。",
    "tldr": "这个论文研究了代理优化算法中超参数的影响，并提出了一种自适应搜索的代理优化方法（HASSO），该方法可以动态调整超参数而不需要额外的评估。该方法旨在提高代理优化算法的可访问性、效果和收敛速度。",
    "en_tdlr": "This paper investigates the impact of hyperparameters on surrogate optimization algorithms and proposes a self-adjusting approach called Hyperparameter Adaptive Search for Surrogate Optimization (HASSO), which dynamically tunes its own hyperparameters without additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of surrogate optimization algorithms."
}