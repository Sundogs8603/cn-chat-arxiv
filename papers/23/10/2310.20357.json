{
    "title": "Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model. (arXiv:2310.20357v1 [cs.AI])",
    "abstract": "The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric",
    "link": "http://arxiv.org/abs/2310.20357",
    "context": "Title: Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model. (arXiv:2310.20357v1 [cs.AI])\nAbstract: The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric",
    "path": "papers/23/10/2310.20357.json",
    "total_tokens": 827,
    "translated_title": "提升多模态大语言模型的空间意识能力",
    "translated_abstract": "多模态大语言模型（MLLM）是指扩展了大语言模型（LLM）的能力，可以接收和推断多模态数据。空间意识是MLLM的关键能力之一，包括了理解物体之间以及物体与场景之间的空间关系的多种技能。自动驾驶、智能医疗、机器人技术、虚拟现实和增强现实等行业对MLLM的空间意识能力有很大需求。然而，当前MLLM的空间意识能力与人类需求之间存在明显差距。为了解决这个问题，本文提出使用更精确的物体之间的空间位置信息来引导MLLM，从而提供更准确的用户查询响应。具体来说，针对特定的多模态任务，我们利用算法获取几何空间信息和场景图来获取相关的几何特征。",
    "tldr": "本论文提出了一种改进多模态大语言模型空间意识能力的方法，通过利用精确的物体空间位置信息指导模型，在用户查询中提供更准确的响应。",
    "en_tdlr": "This paper proposes a method to enhance the spatial awareness capability of multi-modal large language models (MLLM) by using precise spatial position information between objects to guide the model, resulting in more accurate responses to user queries."
}