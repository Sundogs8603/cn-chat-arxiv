{
    "title": "MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks. (arXiv:2310.19677v2 [cs.CL] UPDATED)",
    "abstract": "Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human partic",
    "link": "http://arxiv.org/abs/2310.19677",
    "context": "Title: MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks. (arXiv:2310.19677v2 [cs.CL] UPDATED)\nAbstract: Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human partic",
    "path": "papers/23/10/2310.19677.json",
    "total_tokens": 931,
    "translated_title": "Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks - MoCa",
    "translated_abstract": "人类对物理和社会世界的常识理解是基于直觉理论的。这些理论支持进行因果和道德判断。当发生不好的事情时，我们自然会问：谁做了什么，为什么？认知科学的丰富文献研究了人们的因果和道德直觉。这项工作揭示了一些系统地影响人们判断的因素，如规范违反以及伤害是否可避免或不可避免。我们收集了24篇认知科学论文的故事数据集，并开发了一个系统来注释每个故事所研究的因素。利用这个数据集，我们测试了大型语言模型（LLM）在基于文本场景上是否与人类参与者的因果和道德判断相一致。在总体水平上，最近的LLM的一致性有所改善。然而，通过统计分析，我们发现LLM在考虑这些不同因素时与人类参与者存在较大差异。",
    "tldr": "本研究旨在衡量大型语言模型（LLM）在因果和道德判断任务上与人类的一致性。通过收集24篇认知科学论文的故事数据集，并使用统计分析方法，发现LLMs在考虑因果和道德因素时与人类参与者存在差异。"
}