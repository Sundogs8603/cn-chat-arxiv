{
    "title": "Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity. (arXiv:2310.17998v1 [cs.LG])",
    "abstract": "Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma t",
    "link": "http://arxiv.org/abs/2310.17998",
    "context": "Title: Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity. (arXiv:2310.17998v1 [cs.LG])\nAbstract: Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma t",
    "path": "papers/23/10/2310.17998.json",
    "total_tokens": 972,
    "translated_title": "关于Adam迭代复杂度上下界的缩小",
    "translated_abstract": "最近，Arjevani等人[1]在$L$-平滑条件和有界噪声方差的假设下，为一阶优化问题建立了迭代复杂度的下界。然而，对Adam收敛性的现有文献进行彻底回顾发现存在明显的差距：它们都未达到上述的下界。在本文中，我们通过推导出Adam的新收敛保证，仅需要$L$-平滑条件和有界噪声方差的假设来弥合这一差距。我们的结果适用于广泛的超参数范围。特别是在合理选择的超参数下，我们得到了Adam的迭代复杂度的上界，并且证明它满足一阶优化器的下界。据我们所知，这是第一个为Adam的收敛性建立如此紧致的上界。我们的证明利用了新颖的技巧来处理动量和自适应学习率之间的交织问题，并将降解引理中的一阶项转化为较简单的形式。",
    "tldr": "本文通过推导出一种新的收敛保证，仅需满足$L$-平滑条件和有界噪声方差的假设，来弥合Adam收敛性的差距。特别是在合理选择的超参数下，我们得到了Adam的迭代复杂度的上界，并且证明它满足一阶优化器的下界。"
}