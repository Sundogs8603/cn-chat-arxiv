{
    "title": "NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])",
    "abstract": "Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W",
    "link": "http://arxiv.org/abs/2310.07184",
    "context": "Title: NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])\nAbstract: Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W",
    "path": "papers/23/10/2310.07184.json",
    "total_tokens": 922,
    "translated_title": "NeuroInspect：通过类条件可视化实现的可解释的基于神经元的调试框架",
    "translated_abstract": "尽管深度学习在各个领域取得了显著进展，但深度学习模型仍然容易出错。这个问题需要深度学习从业者使用有效的调试工具来解释网络中的决策过程。然而，现有的调试方法常常需要额外的数据或调整决策过程，限制了它们的适用性。为了解决这个问题，我们提出了NeuroInspect，这是一个基于神经元的可解释的调试框架，包括三个关键阶段：反事实解释、特征可视化和虚假相关性削减。我们的调试框架首先确定网络中导致错误的神经元，然后可视化嵌入在这些神经元中的特征，以便人类解释。为了提供这些解释，我们引入了CLIP-Illusion，一种新颖的特征可视化方法，它生成代表特征的图像，并以类为条件来考察神经元与决策层之间的联系。",
    "tldr": "NeuroInspect是一个基于神经元的可解释的调试框架，通过确定网络中导致错误的神经元并可视化嵌入其中的特征，提供了人类可解释的解释。引入了CLIP-Illusion来生成特征图像，并以类为条件来考察神经元与决策层之间的联系。",
    "en_tdlr": "NeuroInspect is an interpretable neuron-based debugging framework that identifies neurons responsible for errors in the network, visualizes embedded features, and introduces CLIP-Illusion to examine the connection between neurons and the decision layer through conditional visualization."
}