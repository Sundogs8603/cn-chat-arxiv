{
    "title": "PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])",
    "abstract": "Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 ",
    "link": "http://arxiv.org/abs/2310.17415",
    "context": "Title: PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])\nAbstract: Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 ",
    "path": "papers/23/10/2310.17415.json",
    "total_tokens": 905,
    "translated_title": "PETA: 评估亚词切分对蛋白质迁移学习在下游应用中的影响",
    "translated_abstract": "大规模的蛋白质语言模型擅长捕捉原始结构中的进化信息，对蛋白质工程具有重要实用价值。与自然语言模型相比，蛋白质氨基酸序列的数据量较小，组合空间有限。选择合适的词汇表大小来优化预训练模型是一个关键问题。此外，尽管自然语言领域拥有大量的基准测试和研究，但目前还缺乏一个全面评估蛋白质语言模型质量的基准。鉴于这些挑战，PETA使用了三种标记化方法，在14种不同的词汇表大小下训练语言模型。它在33个不同的下游数据集上进行了数千次测试，评估了模型的迁移学习能力，并结合了两个分类头和三个随机种子以减轻潜在偏见。大量实验表明，词汇表大小在50以上大约能够获得最佳的性能。",
    "tldr": "PETA通过评估亚词切分在蛋白质迁移学习中的影响，提出了关于蛋白质语言模型的综合评估方法，并发现词汇表大小在50以上能够获得最佳性能。",
    "en_tdlr": "PETA proposes a comprehensive evaluation method for protein language models by evaluating the impact of sub-word tokenization on protein transfer learning and finds that vocabulary sizes above 50 achieve optimal performance."
}