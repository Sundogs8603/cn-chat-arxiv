{
    "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning. (arXiv:2310.12490v1 [cs.CL])",
    "abstract": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
    "link": "http://arxiv.org/abs/2310.12490",
    "context": "Title: Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning. (arXiv:2310.12490v1 [cs.CL])\nAbstract: Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
    "path": "papers/23/10/2310.12490.json",
    "total_tokens": 829,
    "translated_title": "Co$^2$PT：通过反事实对比提示调整来缓解预训练语言模型中的偏见",
    "translated_abstract": "预训练语言模型被广泛应用于许多重要的实际应用中。然而，最近的研究表明这些模型可能会从大规模的预训练语料库中编码社会偏见，甚至在后续应用中放大偏见。为了解决这个挑战，我们提出了Co$^2$PT，一种通过反事实对比提示调整在下游任务中减轻偏见的高效有效的去偏调整方法。我们在三个外部偏见基准上进行的实验表明，Co$^2$PT在提示调整过程中减轻偏见的有效性以及对现有上游去偏语言模型的适应性。这些发现表明了Co$^2$PT的优势，并为进一步改进下游任务中的偏见减轻提供了有希望的途径。",
    "tldr": "Co$^2$PT是一种通过反事实对比提示调整方法，可以在下游任务中减轻预训练语言模型中的偏见，并适应现有的去偏语言模型。",
    "en_tdlr": "Co$^2$PT is an efficient and effective method for mitigating biases in pre-trained language models by using counterfactual contrastive prompt tuning in downstream tasks. It adapts to existing debiased language models and shows promising avenues for further enhancement in bias mitigation."
}