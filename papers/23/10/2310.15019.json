{
    "title": "Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)",
    "abstract": "Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.",
    "link": "http://arxiv.org/abs/2310.15019",
    "context": "Title: Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)\nAbstract: Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.",
    "path": "papers/23/10/2310.15019.json",
    "total_tokens": 867,
    "translated_title": "语言模型在不平衡文本分类中的元学习: 挑战与机遇",
    "translated_abstract": "检测违规言论内容是重要但困难的。虽然机器学习是应对这一挑战性任务的强大工具，但由于训练数据的数量和质量限制以及违规定义和数据标注的不一致性等因素，难以突破性能瓶颈。为了充分发挥有限资源的潜力，我们提出了一种元学习技术(MLT)，它将使用不同文本表示构建的个体模型进行组合。我们通过分析证明，所得到的技术在数值上是稳定的，并产生合理的组合权重。我们将MLT与阈值移动(TM)技术相结合，进一步提高组合预测器在高度不平衡的分布和超出分布数据集上的性能。我们还提供了计算结果，展示了所提出的MLT方法的统计优势。所有作者对这项工作贡献相同。",
    "tldr": "本文提出了一种元学习技术(MLT)，通过将不同文本表示构建的个体模型进行组合，在不平衡的文本分类中提高了性能，并通过阈值移动技术进一步改善了预测器的性能。",
    "en_tdlr": "This paper proposes a meta learning technique (MLT) that combines individual models built with different text representations to improve the performance of imbalanced text classification, and further improves the performance of the predictor through the threshold-moving technique."
}