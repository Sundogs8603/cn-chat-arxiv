{
    "title": "A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])",
    "abstract": "This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.",
    "link": "http://arxiv.org/abs/2310.02654",
    "context": "Title: A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])\nAbstract: This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.",
    "path": "papers/23/10/2310.02654.json",
    "total_tokens": 835,
    "translated_title": "对资源受限的FPGA的时间序列Transformer模型进行量化感知训练的研究",
    "translated_abstract": "本研究探讨了对时间序列Transformer模型进行量化感知训练（QAT）。我们提出了一种新颖的自适应量化方案，在QAT阶段动态选择对称和非对称方案。我们的方法表明，将量化方案与真实数据分布匹配可以减少计算开销，同时保持可接受的精度。此外，我们的方法在应用于现实世界数据和混合精度量化时表现出鲁棒性，其中大多数对象被量化为4位。我们的发现为模型量化和部署决策提供了参考，并为推进量化技术奠定了基础。",
    "tldr": "本研究探索了对时间序列Transformer模型进行量化感知训练的方法，并提出了一种新颖的自适应量化方案，通过匹配量化方案与实际数据分布，可以降低计算开销并保持可接受的精度。此外，该方法在应用于现实世界数据和混合精度量化时表现出鲁棒性。这些发现为模型量化和部署决策提供了参考，并推进了量化技术的发展。",
    "en_tdlr": "This study explores quantisation-aware training on time series Transformer models and proposes a novel adaptive quantisation scheme. Matching the quantisation scheme to the real data distribution reduces computational overhead while maintaining acceptable precision. The approach demonstrates robustness when applied to real-world data and mixed-precision quantisation. These findings inform model quantisation and deployment decisions and advance quantisation techniques."
}