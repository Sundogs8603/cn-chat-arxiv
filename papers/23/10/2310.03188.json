{
    "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication. (arXiv:2310.03188v1 [cs.AI])",
    "abstract": "Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students ",
    "link": "http://arxiv.org/abs/2310.03188",
    "context": "Title: Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication. (arXiv:2310.03188v1 [cs.AI])\nAbstract: Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students ",
    "path": "papers/23/10/2310.03188.json",
    "total_tokens": 945,
    "translated_title": "论文标题:通过交互式沟通将预训练的知识提取到下游模型中的对话模型",
    "translated_abstract": "最近机器学习的许多突破都是由预训练的基础模型实现的。通过扩大模型参数、训练数据和计算资源，基础模型在许多应用领域显著提高了最先进技术的水平。然而，如何有效地使用这些模型来执行下游任务仍然是一个开放的问题。知识蒸馏（KD）已经被用来解决这个挑战。KD将知识从一个大型教师模型传递给一个较小的学生模型。虽然KD在提高学生模型性能方面取得了成功，但最近的研究发现，强大的教师并不一定会导致强大的学生，因为它们之间存在巨大的能力差距。此外，预训练数据和下游任务之间的潜在的分布偏移可能使KD中的知识传递对于提高下游任务性能不够优化。在本文中，我们通过一个交互式沟通过程扩展了KD，以帮助学生模型更好地利用预训练知识。",
    "tldr": "通过交互式沟通将预训练的知识提取到下游模型中的对话模型，解决了强大教师无法保证强大学生的问题，同时利用交互式沟通提高了知识蒸馏在提高下游任务性能方面的优化。"
}