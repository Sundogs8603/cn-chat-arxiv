{
    "title": "NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation. (arXiv:2310.19820v1 [cs.LG])",
    "abstract": "Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via (1) gradient surgery to tackle the gradient conflicts between them and (2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller's effectiveness in boosting TNNs' achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.",
    "link": "http://arxiv.org/abs/2310.19820",
    "context": "Title: NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation. (arXiv:2310.19820v1 [cs.LG])\nAbstract: Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN model is jointly trained with the weight-sharing teacher model via (1) gradient surgery to tackle the gradient conflicts between them and (2) uncertainty-aware distillation to mitigate the overfitting of the teacher model. Extensive experiments across diverse tasks validate NetDistiller's effectiveness in boosting TNNs' achievable accuracy over state-of-the-art methods. Our code is available at https://github.com/GATECH-EIC/NetDistiller.",
    "path": "papers/23/10/2310.19820.json",
    "total_tokens": 890,
    "translated_title": "NetDistiller: 通过原地蒸馏增强微型深度学习",
    "translated_abstract": "在边缘设备上部署微型神经网络 (TNNs) 面临的一个基本挑战是提高任务准确性，这些设备在内存、计算、带宽和电源方面都有严格的限制。为了解决这个问题，我们提出了一个名为NetDistiller的框架，通过将TNNs看作是通过扩展TNN通道数而构造的权重共享教师模型的子网络，提高TNNs的可实现准确性。具体来说，目标TNN模型与权重共享教师模型一起进行联合训练，方法包括(1)通过梯度手术解决它们之间的梯度冲突，以及(2)通过考虑不确定性的蒸馏来减轻教师模型的过拟合。广泛的实验验证了NetDistiller在提高TNNs可实现准确性方面的有效性，超过了现有方法的表现。我们的代码可在https://github.com/GATECH-EIC/NetDistiller找到。",
    "tldr": "NetDistiller提出了一个框架，通过将微型神经网络作为权重共享教师模型的子网络，通过联合训练和不确定性-aware的蒸馏来提高微型神经网络的准确性。"
}