{
    "title": "Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention. (arXiv:2310.15258v1 [cs.CL])",
    "abstract": "In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across languages), and (2) where the language of the context and the question is different (which we term code-switched reasoning). On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate that although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting. Following this observation, we propose a novel attention mechanism that uses a dedicated set of parameters to encourage cross-lingual at",
    "link": "http://arxiv.org/abs/2310.15258",
    "context": "Title: Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention. (arXiv:2310.15258v1 [cs.CL])\nAbstract: In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across languages), and (2) where the language of the context and the question is different (which we term code-switched reasoning). On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate that although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting. Following this observation, we propose a novel attention mechanism that uses a dedicated set of parameters to encourage cross-lingual at",
    "path": "papers/23/10/2310.15258.json",
    "total_tokens": 965,
    "translated_title": "打破语言障碍：通过结构化自注意力提高跨语言推理能力",
    "translated_abstract": "在这项工作中，我们研究了当多语言语言模型（MultiLMs）在不同语言中进行推理的细调时，它们是否能够将逻辑推理能力转移到其他语言中。我们评估了MultiLMs在两种方案下的跨语言推理能力：（1）在测试的新语言中，上下文和问题的语言保持不变（即推理仍然是单语言的，但模型必须在语言间传递学习到的推理能力），以及（2）上下文和问题的语言不同（我们称之为代码切换推理）。在两个逻辑推理数据集RuleTaker和LeapOfThought上，我们证明虽然MultiLMs在单语言环境中可以跨语言传递推理能力，但在代码切换环境中难以实现推理能力的传递。基于此观察，我们提出了一种使用专门的一组参数来鼓励跨语言推理的新型注意机制。",
    "tldr": "本研究通过研究多语言语言模型（MultiLMs）在不同语言中进行推理时的细调，发现在单语言环境下它们可以传递推理能力，但在代码切换环境下难以实现推理能力的传递。基于此观察，我们提出了一种新的注意机制来鼓励跨语言推理。",
    "en_tdlr": "This study investigates the fine-tuning of multilingual language models (MultiLMs) for reasoning in different languages and finds that while they can transfer reasoning abilities in a monolingual setting, they struggle to do so in a code-switched setting. As a result, a novel attention mechanism is proposed to encourage cross-lingual reasoning."
}