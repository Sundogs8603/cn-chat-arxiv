{
    "title": "Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])",
    "abstract": "Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are",
    "link": "http://arxiv.org/abs/2310.01424",
    "context": "Title: Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])\nAbstract: Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are",
    "path": "papers/23/10/2310.01424.json",
    "total_tokens": 992,
    "translated_title": "从语言模型中识别和减轻隐私风险：一项调查",
    "translated_abstract": "语言模型（LMs）的快速发展使其被广泛采用于许多领域。除了潜在的好处外，这些模型还带来了一系列风险，包括隐私风险。尤其是随着LMs规模的增加，它们对训练数据的记忆潜力增加，从而导致泄露私人信息的风险。随着LMs的日益普及，我们必须了解这些隐私风险以及如何减轻它们。为了帮助研究人员和决策者了解LM隐私攻击和减轻措施的知识状况，包括需要更多工作的领域，我们介绍了第一份关于LM隐私的技术调查。我们（i）确定了攻击在LMs上存在的显著维度的分类法，（ii）调查现有攻击并使用我们的分类法来突出主要趋势，（iii）讨论现有的减轻策略，突出其优势和局限性，识别关键差距，展示开放问题和建议未来工作方向。",
    "tldr": "这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。",
    "en_tdlr": "This survey explores privacy risks and mitigation measures in language models (LMs), presenting a taxonomy of dimensions, surveying existing attacks, discussing their trends, and evaluating current mitigation strategies' strengths and limitations, while identifying key gaps and suggesting future research directions."
}