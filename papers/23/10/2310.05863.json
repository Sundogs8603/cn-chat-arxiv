{
    "title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models. (arXiv:2310.05863v2 [eess.AS] UPDATED)",
    "abstract": "Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-v",
    "link": "http://arxiv.org/abs/2310.05863",
    "context": "Title: Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models. (arXiv:2310.05863v2 [eess.AS] UPDATED)\nAbstract: Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-v",
    "path": "papers/23/10/2310.05863.json",
    "total_tokens": 1009,
    "translated_title": "面向多模型大语言模型的细粒度音频-视觉联合表示",
    "translated_abstract": "音频-视觉大型语言模型(LLM)引起了重要关注，但对于输入流的细粒度组合却未得到充分探讨，这对于LLM理解一般视频输入是具有挑战性但必要的。为此，本文提出了一种面向多模型LLM的细粒度音频-视觉联合表示(FAVOR)学习框架，该框架将基于文本的LLM扩展到能够同时感知音频输入流中的语音和音频事件以及视觉输入流中的图像或视频，帧级别地。我们提出了一种因果Q-Former结构，配合因果关注模块，将音频和视觉特征流融合到联合表示中，并将联合空间与LLM输入嵌入空间对齐，以增强对音频-视觉帧在时间上的因果关系捕获。还提出了一种音频-视觉评估基准(AVEB)，其中包括六个代表性的单模态任务和五个反映音频-视觉的跨模态任务。",
    "tldr": "本文提出了一种面向多模型LLM的细粒度音频-视觉联合表示学习框架(FAVOR)，该框架能够同时感知音频和视觉输入流，并通过因果关注模块捕获音频-视觉帧的因果关系。还提出了一个音频-视觉评估基准(AVEB)用于评估该模型的性能。",
    "en_tdlr": "This paper proposes a fine-grained audio-visual joint representation learning framework (FAVOR) for multimodal large language models (LLM), which can simultaneously perceive audio and visual input streams and capture the causal relations of audio-visual frames using a causal attention module. It also introduces an audio-visual evaluation benchmark (AVEB) to assess the performance of the model."
}