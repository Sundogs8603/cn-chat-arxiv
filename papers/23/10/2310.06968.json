{
    "title": "ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning. (arXiv:2310.06968v1 [cs.CV])",
    "abstract": "Recent text-to-image generative models can generate high-fidelity images from text prompts. However, these models struggle to consistently generate the same objects in different contexts with the same appearance. Consistent object generation is important to many downstream tasks like generating comic book illustrations with consistent characters and setting. Numerous approaches attempt to solve this problem by extending the vocabulary of diffusion models through fine-tuning. However, even lightweight fine-tuning approaches can be prohibitively expensive to run at scale and in real-time. We introduce a method called ObjectComposer for generating compositions of multiple objects that resemble user-specified images. Our approach is training-free, leveraging the abilities of preexisting models. We build upon the recent BLIP-Diffusion model, which can generate images of single objects specified by reference images. ObjectComposer enables the consistent generation of compositions containing ",
    "link": "http://arxiv.org/abs/2310.06968",
    "context": "Title: ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning. (arXiv:2310.06968v1 [cs.CV])\nAbstract: Recent text-to-image generative models can generate high-fidelity images from text prompts. However, these models struggle to consistently generate the same objects in different contexts with the same appearance. Consistent object generation is important to many downstream tasks like generating comic book illustrations with consistent characters and setting. Numerous approaches attempt to solve this problem by extending the vocabulary of diffusion models through fine-tuning. However, even lightweight fine-tuning approaches can be prohibitively expensive to run at scale and in real-time. We introduce a method called ObjectComposer for generating compositions of multiple objects that resemble user-specified images. Our approach is training-free, leveraging the abilities of preexisting models. We build upon the recent BLIP-Diffusion model, which can generate images of single objects specified by reference images. ObjectComposer enables the consistent generation of compositions containing ",
    "path": "papers/23/10/2310.06968.json",
    "total_tokens": 790,
    "translated_title": "ObjectComposer: 不需要微调的一致生成多个对象的方法",
    "translated_abstract": "最近的文本到图像生成模型可以从文本提示中生成高保真度的图像。然而，这些模型在不同上下文中一致地生成相同外观的对象方面存在困难。一致的对象生成对于许多下游任务如生成具有一致角色和场景的漫画书插图非常重要。许多方法通过对扩展扩散模型的词汇表进行微调来解决这个问题。然而，即使是轻量级微调方法在大规模和实时运行时也可能代价过高。我们引入了一种称为ObjectComposer的方法，用于生成与用户指定的图像相似的多个对象的组合。我们的方法不需要训练，利用现有模型的能力。我们建立在最近的BLIP-Diffusion模型之上，该模型可以根据参考图像生成单个对象的图像。ObjectComposer可以实现生成包含 。。。",
    "tldr": "ObjectComposer是一种不需要微调的方法，可以一致地生成多个对象的组合。这解决了现有模型在不同上下文中生成对象外观不一致的问题。",
    "en_tdlr": "ObjectComposer is a fine-tuning-free method that enables consistent generation of compositions containing multiple objects, addressing the issue of inconsistent object appearance in different contexts."
}