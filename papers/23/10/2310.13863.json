{
    "title": "Distributionally Robust Optimization with Bias and Variance Reduction. (arXiv:2310.13863v1 [stat.ML])",
    "abstract": "We consider the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3$\\times$ faster than baselines such as stochastic gradient and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.",
    "link": "http://arxiv.org/abs/2310.13863",
    "context": "Title: Distributionally Robust Optimization with Bias and Variance Reduction. (arXiv:2310.13863v1 [stat.ML])\nAbstract: We consider the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3$\\times$ faster than baselines such as stochastic gradient and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.",
    "path": "papers/23/10/2310.13863.json",
    "total_tokens": 974,
    "translated_title": "具有偏差和方差减小的分布鲁棒优化",
    "translated_abstract": "我们考虑使用谱风险基于不确定性集和$f$-散度罚项的分布鲁棒优化(DRO)问题。这个问题包括常见的风险敏感学习目标，如正则化条件风险价值(Conditional Value-at-Risk, CVaR)和平均top-$k$损失。我们提出了Prospect，一种仅需调节单个学习率超参数的随机梯度算法，并证明在平滑正则化损失情况下，它具有线性收敛性。这与之前的算法形成了对比，这些算法要么需要调节多个超参数，要么由于梯度估计存在偏差或不适当的正则化而导致失败。在实证方面，我们展示了Prospect在跨表格、视觉和语言领域的分布偏移和公平性基准上能以2-3倍的速度收敛，比基线方法如随机梯度和随机鞍点方法快。",
    "tldr": "本论文提出了一种可以减小偏差和方差的分布鲁棒优化方法，通过谱风险和$f$-散度罚项来解决风险敏感学习问题。提出的Prospect算法只需调节一个学习率超参数，并证明在平滑正则化损失情况下具有线性收敛性。实验证明，Prospect算法在分布偏移和公平性基准上能以2-3倍的速度收敛。",
    "en_tdlr": "This paper proposes a distributionally robust optimization method with bias and variance reduction, tackling risk-sensitive learning objectives using spectral risk and f-divergence penalty. The Prospect algorithm is introduced, which only requires tuning a single learning rate hyperparameter and demonstrates linear convergence for smooth regularized losses. Empirical results show that Prospect algorithm converges 2-3 times faster than baselines on distribution shift and fairness benchmarks in various domains."
}