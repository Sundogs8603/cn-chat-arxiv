{
    "title": "Quantized Transformer Language Model Implementations on Edge Devices. (arXiv:2310.03971v1 [cs.CL])",
    "abstract": "Large-scale transformer-based models like the Bidirectional Encoder Representations from Transformers (BERT) are widely used for Natural Language Processing (NLP) applications, wherein these models are initially pre-trained with a large corpus with millions of parameters and then fine-tuned for a downstream NLP task. One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency. In order to overcome these limitations, such large-scale models can be converted to an optimized FlatBuffer format, tailored for deployment on resource-constrained edge devices. Herein, we evaluate the performance of such FlatBuffer transformed MobileBERT models on three different edge devices, fine-tuned for Reputation analysis of English language tweets in the RepLab 2013 dataset. In addition, this study encompassed an evaluation of the deployed models, wherein their latency, performance, a",
    "link": "http://arxiv.org/abs/2310.03971",
    "context": "Title: Quantized Transformer Language Model Implementations on Edge Devices. (arXiv:2310.03971v1 [cs.CL])\nAbstract: Large-scale transformer-based models like the Bidirectional Encoder Representations from Transformers (BERT) are widely used for Natural Language Processing (NLP) applications, wherein these models are initially pre-trained with a large corpus with millions of parameters and then fine-tuned for a downstream NLP task. One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency. In order to overcome these limitations, such large-scale models can be converted to an optimized FlatBuffer format, tailored for deployment on resource-constrained edge devices. Herein, we evaluate the performance of such FlatBuffer transformed MobileBERT models on three different edge devices, fine-tuned for Reputation analysis of English language tweets in the RepLab 2013 dataset. In addition, this study encompassed an evaluation of the deployed models, wherein their latency, performance, a",
    "path": "papers/23/10/2310.03971.json",
    "total_tokens": 829,
    "translated_title": "边缘设备上的量化Transformer语言模型实现",
    "translated_abstract": "大规模基于Transformer的模型，如双向编码器Transformer（BERT），被广泛用于自然语言处理（NLP）应用中。这些模型首先通过大规模语料库进行预训练，然后对下游NLP任务进行微调。这些大规模模型的一个主要限制是它们不能在资源受限的设备上部署，因为其模型大小较大且推理延迟增加。为了克服这些限制，可以将这些大规模模型转换为优化的FlatBuffer格式，以便在资源受限的边缘设备上部署。在本研究中，我们评估了转换为FlatBuffer的MobileBERT模型在三种不同边缘设备上的性能，并对其进行了针对RepLab 2013数据集中英语推文的声誉分析进行微调。此外，该研究还评估了部署模型的延迟，性能等指标。",
    "tldr": "在这项研究中，我们评估了将大规模Transformer模型转换为FlatBuffer格式在资源受限的边缘设备上的性能表现，并对其进行了声誉分析任务的微调。"
}