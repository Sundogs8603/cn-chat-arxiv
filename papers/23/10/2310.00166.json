{
    "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback. (arXiv:2310.00166v1 [cs.AI])",
    "abstract": "Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made with",
    "link": "http://arxiv.org/abs/2310.00166",
    "context": "Title: Motif: Intrinsic Motivation from Artificial Intelligence Feedback. (arXiv:2310.00166v1 [cs.AI])\nAbstract: Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made with",
    "path": "papers/23/10/2310.00166.json",
    "total_tokens": 970,
    "translated_title": "Motif: 来自人工智能反馈的内在动机",
    "translated_abstract": "在没有先验知识的情况下，探索丰富的环境并评估自己的行动是非常具有挑战性的。在本文中，我们提出了Motif，一种用大型语言模型（LLM）将先验知识与代理程序接口的通用方法。Motif的基本思想是将LLMs用于决策，而无需与环境进行交互：它通过从LLM中产生对配对标题的偏好来构建内在奖励，然后使用该奖励对代理程序进行强化学习训练。我们在具有挑战性、开放性和程序生成的NetHack游戏上评估了Motif的性能和行为。令人惊讶的是，仅通过学习最大化其内在奖励，Motif的游戏得分比直接训练以最大化得分的算法更高。当将Motif的内在奖励与环境奖励相结合时，我们的方法明显优于现有方法，并在以前从未取得进展的任务上取得进展。",
    "tldr": "本文提出了一种名为Motif的方法，通过与大型语言模型（LLM）交互来获得先验知识，并将其用于代理程序的强化学习训练。实验证明，Motif的内在奖励相比直接最大化得分的算法在挑战性游戏中获得了更高的游戏得分，并在之前没有取得进展的任务上取得了显著的进展。",
    "en_tdlr": "This paper proposes a method called Motif, which interfaces prior knowledge from a Large Language Model (LLM) with an agent to train it using reinforcement learning. The results show that Motif achieves higher game scores compared to algorithms directly maximizing scores, and makes significant progress on tasks where no advancements have ever been made before."
}