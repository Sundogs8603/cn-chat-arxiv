{
    "title": "Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])",
    "abstract": "A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the dataset",
    "link": "http://arxiv.org/abs/2310.03916",
    "context": "Title: Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])\nAbstract: A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the dataset",
    "path": "papers/23/10/2310.03916.json",
    "total_tokens": 791,
    "translated_title": "朝着时间序列数据的基础模型迈进",
    "translated_abstract": "基础模型是一个基于大规模和多样化的数据集进行训练的机器学习模型，通常使用基于自监督学习的预训练技术，可以适应各种下游任务。然而，当前关于时间序列预训练的研究主要集中在仅使用单一领域数据进行预训练的模型上，导致对其他类型时间序列的知识缺乏。本文旨在通过利用多领域的无标签样本来开发一种有效的时间序列基础模型。为实现这一目标，我们重新利用了公开可用的UCR存档，并评估了四种现有的基于自监督学习的预训练方法以及一种新方法。",
    "tldr": "本文旨在通过利用多领域的无标签样本来开发一种有效的时间序列基础模型，以解决当前关于时间序列预训练的研究集中在单一领域数据上的问题。",
    "en_tdlr": "This paper aims to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains, addressing the limitation of current research on time series pre-training focused on data from a single domain."
}