{
    "title": "Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])",
    "abstract": "Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \\textbf{E}mergent $\\textbf{M}$ixture-$\\textbf{o}$f-$\\textbf{E}$xperts (E",
    "link": "http://arxiv.org/abs/2310.10908",
    "context": "Title: Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])\nAbstract: Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \\textbf{E}mergent $\\textbf{M}$ixture-$\\textbf{o}$f-$\\textbf{E}$xperts (E",
    "path": "papers/23/10/2310.10908.json",
    "total_tokens": 829,
    "translated_title": "自发性模块化结构：密集预训练Transformer能否从自发模块化结构中获益？",
    "translated_abstract": "将模块化设计引入神经网络能够展示出较好的泛化能力和学习效率等优点。现有的模块化神经网络通常是“显式”的，因为它们的模块化架构是预先定义的，每个模块都被期望实现不同的功能。相反，最近的研究表明在标准的预训练Transformer中存在“隐式”的模块化结构，即“自发模块化”。他们表明这样的模块化结构在早期预训练阶段就会出现，并且完全是自发的。然而，大多数Transformer模型仍然被视为单体模型，没有充分利用其模块化的特性。因此，鉴于显式模块化架构的优良特性，我们探索了密集预训练Transformer是否以及如何从自发模块化结构中获益的问题。",
    "tldr": "该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。",
    "en_tdlr": "This paper investigates whether and how dense pre-trained Transformers can benefit from emergent modular structures."
}