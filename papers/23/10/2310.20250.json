{
    "title": "Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning. (arXiv:2310.20250v1 [cs.AI])",
    "abstract": "Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both globa",
    "link": "http://arxiv.org/abs/2310.20250",
    "context": "Title: Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning. (arXiv:2310.20250v1 [cs.AI])\nAbstract: Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both globa",
    "path": "papers/23/10/2310.20250.json",
    "total_tokens": 936,
    "translated_title": "基于多样化节点抽样的层次化Transformer池化的图表示学习",
    "translated_abstract": "图池化方法广泛用于降采样图形，在图级任务如图分类和图生成中取得了令人印象深刻的结果。一种重要的方法称为节点丢弃池化，旨在利用可学习的评分函数丢弃具有相对较低显著性得分的节点。然而，现有的节点丢弃方法存在两个限制：（1）对于每个池化节点，这些模型在主要以GNN为骨架的情况下难以捕捉长距离依赖关系；（2）仅汇集得分最高的节点往往会保留相似的节点，从而丢弃低得分节点的丰富信息。为了解决这些问题，我们提出了一种名为GTPool的图变换池化方法，将Transformer引入到节点丢弃池化中，以便高效地捕捉长距离的成对交互，并同时多样化采样节点。具体来说，我们设计了一个基于自我注意机制的评分模块，同时考虑了全局和局部信息，以指导节点的丢弃。",
    "tldr": "本论文提出了一种基于多样化节点抽样的层次化Transformer池化方法，通过引入Transformer机制，能够有效捕捉图中节点之间的长距离依赖关系，并同时多样化采样节点，以解决现有节点丢弃方法中的限制。"
}