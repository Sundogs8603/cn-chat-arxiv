{
    "title": "Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information. (arXiv:2310.03968v1 [cs.LG])",
    "abstract": "We address the fundamental limits of learning unknown parameters of any stochastic process from time-series data, and discover exact closed-form expressions for how optimal inference scales with observation length. Given a parametrized class of candidate models, the Fisher information of observed sequence probabilities lower-bounds the variance in model estimation from finite data. As sequence-length increases, the minimal variance scales as the square inverse of the length -- with constant coefficient given by the information rate. We discover a simple closed-form expression for this information rate, even in the case of infinite Markov order. We furthermore obtain the exact analytic lower bound on model variance from the observation-induced metadynamic among belief states. We discover ephemeral, exponential, and more general modes of convergence to the asymptotic information rate. Surprisingly, this myopic information rate converges to the asymptotic Fisher information rate with exac",
    "link": "http://arxiv.org/abs/2310.03968",
    "context": "Title: Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information. (arXiv:2310.03968v1 [cs.LG])\nAbstract: We address the fundamental limits of learning unknown parameters of any stochastic process from time-series data, and discover exact closed-form expressions for how optimal inference scales with observation length. Given a parametrized class of candidate models, the Fisher information of observed sequence probabilities lower-bounds the variance in model estimation from finite data. As sequence-length increases, the minimal variance scales as the square inverse of the length -- with constant coefficient given by the information rate. We discover a simple closed-form expression for this information rate, even in the case of infinite Markov order. We furthermore obtain the exact analytic lower bound on model variance from the observation-induced metadynamic among belief states. We discover ephemeral, exponential, and more general modes of convergence to the asymptotic information rate. Surprisingly, this myopic information rate converges to the asymptotic Fisher information rate with exac",
    "path": "papers/23/10/2310.03968.json",
    "total_tokens": 1068,
    "translated_title": "学习非Markov行为的最终极限：费舍尔信息率和超量信息",
    "translated_abstract": "我们研究了从时间序列数据中学习任何随机过程的未知参数的基本限制，并发现了如何随着观测长度的增加，最佳推断的封闭形式表达式。给定一个参数化的候选模型类，观测序列概率的费舍尔信息下界了来自有限数据的模型估计方差。随着序列长度的增加，最小方差的缩放率与长度的平方倒数成反比 -- 其中常数系数由信息率给出。我们发现了这个信息率的简单闭式表达式，即使在无限Markov阶的情况下也是如此。此外，我们通过观察引起的信念状态间元动力学获得了模型方差的精确分析下界。我们还发现了瞬时、指数和更一般的收敛模式，以达到渐近信息率。令人惊讶的是，这个短视的信息率收敛到渐近的费舍尔信息率。",
    "tldr": "该论文研究了从时间序列数据中学习任何随机过程的未知参数的极限，发现了最佳推断的封闭形式表达式。对于给定的参数化候选模型类，观测序列概率的费舍尔信息下界了来自有限数据的模型估计方差，最小方差随序列长度的增加而减小，缩放率由信息率给出。通过分析观察引起的信念状态间的元动力学，得到了模型方差的精确下界，并发现了不同的收敛模式，最终短视的信息率收敛到渐近的费舍尔信息率。",
    "en_tdlr": "This paper investigates the ultimate limits of learning unknown parameters of any stochastic process from time-series data, and provides closed-form expressions for optimal inference with increasing observation length. The Fisher information of observed sequence probabilities sets a lower bound on model estimation variance from finite data, and the minimal variance decreases as the square inverse of sequence length, with a constant coefficient determined by the information rate. The paper also explores different convergence modes and discovers that the myopic information rate converges to the asymptotic Fisher information rate."
}