{
    "title": "InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])",
    "abstract": "Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the \"easy-first\" text generation process of current diffusion models and the \"keyword-first\" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a \"keyinfo-first\" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.",
    "link": "http://arxiv.org/abs/2310.11976",
    "context": "Title: InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])\nAbstract: Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the \"easy-first\" text generation process of current diffusion models and the \"keyword-first\" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a \"keyinfo-first\" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.",
    "path": "papers/23/10/2310.11976.json",
    "total_tokens": 889,
    "translated_title": "InfoDiffusion: 针对非自回归文本生成的信息熵感知扩散过程",
    "translated_abstract": "扩散模型在文本生成领域引起了广泛关注。一些研究已经探索了具有不同结构的文本扩散模型，并将它们应用于各种任务，包括命名实体识别和摘要生成。然而，当前扩散模型的“easy-first”文本生成过程与人类的“keyword-first”自然文本生成过程存在明显差异，这引起了有限的关注。为了弥合这一差距，我们提出了一种非自回归文本扩散模型InfoDiffusion。我们的方法引入了“keyinfo-first”生成策略，并结合了基于文本信息量的噪声调度。此外，InfoDiffusion结合了自我条件和一个新提出的部分加噪模型结构。实验结果表明，InfoDiffusion在生成质量和多样性方面优于基线模型，并展示出更高的采样效率。",
    "tldr": "InfoDiffusion是一种非自回归文本扩散模型，通过引入“keyinfo-first”生成策略和基于文本信息量的噪声调度，以及结合自我条件和部分加噪模型结构的方法，提高了生成质量和多样性，并展示出更高的采样效率。"
}