{
    "title": "Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi",
    "link": "http://arxiv.org/abs/2310.00648",
    "context": "Title: Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)\nAbstract: Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi",
    "path": "papers/23/10/2310.00648.json",
    "total_tokens": 960,
    "translated_title": "更少就意味着更多: 对参数高效微调的特洛伊攻击",
    "translated_abstract": "参数高效微调（PEFT）可以将预训练语言模型（PLM）高效地适应到特定任务中。通过仅微调一小部分（额外的）参数，PEFT实现了与全面微调相当的性能。然而，尽管它被广泛使用，但PEFT的安全性影响仍然鲜为人知。在本文中，我们进行了一项初步研究，揭示了PEFT对特洛伊攻击的独特易受攻击性。具体而言，我们提出了PETA，一种通过双层优化考虑下游适应的新型攻击方式：上层目标将后门嵌入PLM中，而下层目标模拟PEFT以保留PLM的任务特定性能。通过对多种下游任务和触发器设计的广泛评估，我们证明了PETA在攻击成功率和未受影响的干净准确性方面的有效性，即使受害用户在使用纯净数据对带有后门的PLM进行PEFT后。",
    "tldr": "本文研究了参数高效微调（PEFT）的安全性问题，发现PEFT易受特洛伊攻击。通过提出一种新的攻击方式PETA，并在各种下游任务和触发器设计中进行广泛测试，发现PETA能够在攻击成功率和未受影响的准确性方面取得有效结果。",
    "en_tdlr": "This paper investigates the security implications of parameter-efficient fine-tuning (PEFT) and reveals its vulnerability to Trojan attacks. A novel attack, PETA, is proposed and extensively evaluated in various downstream tasks and trigger designs, demonstrating its effectiveness in terms of attack success rate and unaffected accuracy."
}