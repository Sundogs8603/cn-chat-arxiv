{
    "title": "Comparing Comparators in Generalization Bounds. (arXiv:2310.10534v1 [cs.LG])",
    "abstract": "We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.",
    "link": "http://arxiv.org/abs/2310.10534",
    "context": "Title: Comparing Comparators in Generalization Bounds. (arXiv:2310.10534v1 [cs.LG])\nAbstract: We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.",
    "path": "papers/23/10/2310.10534.json",
    "total_tokens": 805,
    "translated_title": "对比分类器在泛化界限中的比较",
    "translated_abstract": "我们推导了涉及任意凸比较函数的通用信息理论和PAC-Bayesian泛化界限，该函数测量训练误差和样本误差之间的差异。该界限在比较函数的累积生成函数(CG), 被界定在一族限制分布函数的CGF上限的假设下成立。我们证明了当比较函数是CGF的凸共轭，也被称为Cram\\'er函数时，得到的界限是最紧的。这个结论更广泛地适用于具有类似结构的泛化界限。这证实了已知界限在有界和次高斯损失情况下的近最优性，并且在其他限制分布下得到了新的界限。",
    "tldr": "本文推导了涉及任意凸比较函数的通用信息理论和PAC-Bayesian泛化界限，证明了最紧界限是由凸共轭的累积生成函数(CGF)构成的，使得这些界限广泛适用于不同结构的泛化界限。",
    "en_tdlr": "This paper derives generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function. It shows that the tightest possible bound is obtained with the comparator being the convex conjugate of the cumulant-generating function (CGF), and confirms the near-optimality of known bounds for bounded and sub-Gaussian losses, leading to novel bounds under other bounding distributions."
}