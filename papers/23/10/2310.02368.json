{
    "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])",
    "abstract": "Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca",
    "link": "http://arxiv.org/abs/2310.02368",
    "context": "Title: Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])\nAbstract: Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca",
    "path": "papers/23/10/2310.02368.json",
    "total_tokens": 987,
    "translated_title": "从自动反馈中进行强化学习以生成高质量的单元测试",
    "translated_abstract": "软件测试是软件开发的关键方面，创建符合最佳实践的高质量测试对于有效的维护至关重要。最近，大型语言模型（LLM）在代码生成方面越来越受欢迎，包括自动创建测试用例。然而，这些LLM通常在大量公开可用的代码上进行训练，其中可能包含不符合最佳实践甚至包含测试代码异味（反模式）的测试用例。为了解决这个问题，我们提出了一种称为静态质量指标强化学习（RLSQM）的新技术。首先，我们分析了LLM生成的反模式，并展示了LLM可以生成不良的测试代码异味。因此，我们为每个静态质量指标训练了专门的奖励模型，然后利用Proximal Policy Optimization （PPO）来训练逐个优化单个质量指标的模型。此外，我们将这些奖励融合到一个统一的奖励模型中，以实现对整体质量的优化。",
    "tldr": "本论文提出了一种名为静态质量指标强化学习（RLSQM）的新技术，用于解决大型语言模型（LLM）在自动生成测试用例时可能生成不良代码异味的问题。通过训练特定的奖励模型和利用PPO算法进行优化，我们实现了对单个质量指标和整体质量的优化。",
    "en_tdlr": "This paper proposes a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM) to address the issue of undesirable test smells generated by Large Language Models (LLMs) in the automated creation of test cases. By training specific reward models and utilizing PPO algorithm for optimization, the paper achieves optimization for individual quality metrics and overall quality."
}