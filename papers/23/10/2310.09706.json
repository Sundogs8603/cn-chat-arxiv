{
    "title": "AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking. (arXiv:2310.09706v2 [cs.IR] UPDATED)",
    "abstract": "User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive SelfSup",
    "link": "http://arxiv.org/abs/2310.09706",
    "context": "Title: AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking. (arXiv:2310.09706v2 [cs.IR] UPDATED)\nAbstract: User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive SelfSup",
    "path": "papers/23/10/2310.09706.json",
    "total_tokens": 853,
    "translated_title": "AdaptSSR: 使用自适应增强自监督排序方法预训练用户模型",
    "translated_abstract": "用户建模旨在捕捉用户的特征或兴趣，但受到数据稀疏性问题的影响，往往需要依赖特定任务的标注数据。最近的几项研究通过在大量用户行为序列上进行对比学习的预训练来解决这个问题。一般而言，这些方法假设通过数据增强构建的同一行为序列的不同视图在语义上是一致的，即反映用户的相似特征或兴趣，并在特征空间中最大化它们的一致性。然而，由于用户行为的多样兴趣和大量噪音，现有的增强方法往往会丢失某些用户特征或引入噪声行为。因此，直接最大化增强视图之间的相似性可能导致负面迁移。为此，我们提出用新的预训练任务替代对比学习任务：自适应增强自监督排序方法。",
    "tldr": "在用户建模中，通过自适应增强自监督排序方法预训练用户模型，解决了数据稀疏性问题和现有增强方法引入的噪音问题。",
    "en_tdlr": ""
}