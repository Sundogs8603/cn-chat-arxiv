{
    "title": "HarmonyDream: Task Harmonization Inside World Models",
    "abstract": "Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a s",
    "link": "https://arxiv.org/abs/2310.00344",
    "context": "Title: HarmonyDream: Task Harmonization Inside World Models\nAbstract: Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a s",
    "path": "papers/23/10/2310.00344.json",
    "total_tokens": 843,
    "translated_title": "HarmonyDream: 世界模型中的任务协调化",
    "translated_abstract": "基于模型的强化学习（MBRL）通过利用世界模型来实现高效学习的目标，世界模型通常包含观测建模和奖励建模两个任务。本文通过实证研究深入理解了每个任务在世界模型中的作用，并发现通过缓解观测建模或奖励建模的占用优势来实现高效学习的潜力。我们的关键观点是，虽然当前的显式MBRL方法试图通过观测模型恢复环境的丰富细节，但由于环境的复杂性和模型容量的限制，这是困难的。另一方面，隐式MBRL中奖励模型占主导地位，擅长学习紧凑的任务导向动态，但在没有更丰富的学习信号的情况下不适合高效学习。在这些观点和发现的基础上，我们提出了一个...",
    "tldr": "在基于模型的强化学习中，通过深入研究观测建模和奖励建模的作用，发现通过缓解观测建模或奖励建模的占用优势来实现高效学习的潜力。",
    "en_tdlr": "Potential for achieving efficient learning in model-based reinforcement learning by mitigating the domination of either observation or reward modeling is uncovered through a deeper understanding of the role each task plays in world models."
}