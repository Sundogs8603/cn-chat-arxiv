{
    "title": "Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication. (arXiv:2310.09394v1 [cs.LG])",
    "abstract": "Recent studies on semantic communication commonly rely on neural network (NN) based transceivers such as deep joint source and channel coding (DeepJSCC). Unlike traditional transceivers, these neural transceivers are trainable using actual source data and channels, enabling them to extract and communicate semantics. On the flip side, each neural transceiver is inherently biased towards specific source data and channels, making different transceivers difficult to understand intended semantics, particularly upon their initial encounter. To align semantics over multiple neural transceivers, we propose a distributed learning based solution, which leverages split learning (SL) and partial NN fine-tuning techniques. In this method, referred to as SL with layer freezing (SLF), each encoder downloads a misaligned decoder, and locally fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this fraction, SLF controls computing and communication costs. Simulation results confirm t",
    "link": "http://arxiv.org/abs/2310.09394",
    "context": "Title: Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication. (arXiv:2310.09394v1 [cs.LG])\nAbstract: Recent studies on semantic communication commonly rely on neural network (NN) based transceivers such as deep joint source and channel coding (DeepJSCC). Unlike traditional transceivers, these neural transceivers are trainable using actual source data and channels, enabling them to extract and communicate semantics. On the flip side, each neural transceiver is inherently biased towards specific source data and channels, making different transceivers difficult to understand intended semantics, particularly upon their initial encounter. To align semantics over multiple neural transceivers, we propose a distributed learning based solution, which leverages split learning (SL) and partial NN fine-tuning techniques. In this method, referred to as SL with layer freezing (SLF), each encoder downloads a misaligned decoder, and locally fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this fraction, SLF controls computing and communication costs. Simulation results confirm t",
    "path": "papers/23/10/2310.09394.json",
    "total_tokens": 898,
    "translated_title": "基于分割学习的语义对齐用于弹性多用户语义通信",
    "translated_abstract": "最近对语义通信的研究常常依赖于基于神经网络的收发器，如深度联合源和信道编码（DeepJSCC）。与传统的收发器不同，这些神经收发器可以使用实际的源数据和信道进行训练，使其能够提取和传递语义信息。然而，每个神经收发器固有地偏向于特定的源数据和信道，使得不同的收发器很难理解预期的语义，尤其是在初始遇到时。为了在多个神经收发器之间对齐语义，我们提出了一种基于分布式学习的解决方案，利用分割学习（SL）和局部神经网络微调技术。在这种称为具有层冻结的SL（SLF）的方法中，每个编码器下载一个不对齐的解码器，并在本地微调这些编码器-解码器神经网络层的一部分。通过调整这个比例，SLF可以控制计算和通信成本。仿真结果证实了该方法的有效性。",
    "tldr": "本文提出了一种基于分割学习的方法，用于在多个神经收发器之间对齐语义。该方法使用局部微调技术，可以有效地控制计算和通信成本。",
    "en_tdlr": "This paper proposes a split learning method for aligning semantics among multiple neural transceivers. The method utilizes partial fine-tuning techniques to effectively control computation and communication costs."
}