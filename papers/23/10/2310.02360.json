{
    "title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])",
    "abstract": "Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai",
    "link": "http://arxiv.org/abs/2310.02360",
    "context": "Title: Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])\nAbstract: Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai",
    "path": "papers/23/10/2310.02360.json",
    "total_tokens": 884,
    "translated_title": "优先级软Q分解用于字典型强化学习",
    "translated_abstract": "复杂任务的强化学习仍然存在挑战，主要是由于设计标量奖励函数的困难以及从头开发模型的固有低效性。相反，最好是将复杂任务以基本子任务的形式指定，并在可能的情况下重复使用子任务的解决方案。在这项工作中，我们解决了连续空间的字典型多目标强化学习问题，其中包含了优先级子任务，这些问题通常很难解决。我们展示了这些问题可以通过子任务转换进行标量化，并使用价值分解逐步解决。利用这一洞察力，我们提出了优先级软Q分解（PSQD），一种在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案的新算法。PSQD能够在零-shot组成之后重复使用先前学习的子任务解决方案，并进行适应步骤。它具备保留子任务训练信息并在复合任务中适应的能力。",
    "tldr": "本论文提出了一种用于字典型强化学习的优先级软Q分解算法（PSQD），能够在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案，实现了先前学习的子任务解决方案的零-shot组成和适应。"
}