{
    "title": "Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])",
    "abstract": "Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles",
    "link": "http://arxiv.org/abs/2310.11986",
    "context": "Title: Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])\nAbstract: Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles",
    "path": "papers/23/10/2310.11986.json",
    "total_tokens": 859,
    "translated_title": "生成型AI系统的社会技术安全评估",
    "translated_abstract": "生成型AI系统会产生一系列风险。为了确保生成型AI系统的安全，需要对这些风险进行评估。本文提出了一个三层框架，采用结构化的社会技术方法来评估这些风险。该框架包括能力评估，这是目前主要的安全评估方法。在此基础上，我们进一步建立在系统安全原则的基础上，特别是认识到上下文决定了特定能力是否会造成伤害。为了考虑相关的上下文，我们的框架增加了人机互动和系统影响作为额外的评估层面。其次，我们调查了生成型AI系统安全评估的现状，并创建了现有评估的库。从分析中得出了三个显著的评估差距。我们提出了解决这些差距的前进方式，概述了实际步骤和角色。",
    "tldr": "本文提出了一个三层框架，采用社会技术方法对生成型AI系统的安全风险进行评估。同时，评估现状调查发现了三个显著的评估差距，并提出了解决这些差距的方法。",
    "en_tdlr": "This paper proposes a three-layered framework for the safety evaluation of generative AI systems using a sociotechnical approach. It also identifies three significant evaluation gaps and suggests ways to address them."
}