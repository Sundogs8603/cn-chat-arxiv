{
    "title": "Subtractive Mixture Models via Squaring: Representation and Learning",
    "abstract": "arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.",
    "link": "https://arxiv.org/abs/2310.00724",
    "context": "Title: Subtractive Mixture Models via Squaring: Representation and Learning\nAbstract: arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.",
    "path": "papers/23/10/2310.00724.json",
    "total_tokens": 779,
    "translated_title": "通过平方的消减混合模型:表示和学习",
    "translated_abstract": "混合模型传统上是通过将几个分布作为组件相加来表示和学习的。允许混合减去概率质量或密度可以大大减少建模复杂分布所需的组件数量。然而，学习这种减法混合模型并确保它们仍然编码非负函数是具有挑战性的。我们探讨了如何通过平方来学习和执行深度减法混合模型。我们在概率电路框架中进行这些研究，这使我们能够表示张量化的混合模型并泛化其他减法模型。我们在理论上证明了允许减法的平方电路类可以比传统的加法混合模型具有指数级更具表达力；我们在一系列真实世界分布估计任务上实证展示了这种增加的表达力。",
    "tldr": "通过平方操作实现的消减混合模型在表达能力上优于传统加法混合模型，并在真实世界分布估计任务中得到了实验证明。",
    "en_tdlr": "Subtractive mixture models achieved through squaring outperform traditional additive mixture models in expressiveness and are empirically demonstrated on real-world distribution estimation tasks."
}