{
    "title": "Making Scalable Meta Learning Practical. (arXiv:2310.05674v2 [cs.LG] UPDATED)",
    "abstract": "Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU ",
    "link": "http://arxiv.org/abs/2310.05674",
    "context": "Title: Making Scalable Meta Learning Practical. (arXiv:2310.05674v2 [cs.LG] UPDATED)\nAbstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU ",
    "path": "papers/23/10/2310.05674.json",
    "total_tokens": 1039,
    "translated_title": "实现可扩展的元学习的可行性",
    "translated_abstract": "尽管元学习（即学会学习）在机器学习程序中学习多样的归纳偏置方面非常灵活，但由于计算/内存开销巨大、训练不稳定和缺乏有效的分布式训练支持，它长期以来一直被认为不具有良好的可扩展性。在这项工作中，我们专注于通过引入SAMA，将隐式微分算法和系统的进展相结合，从而使可扩展的元学习具有实用性。具体来说，SAMA旨在灵活支持元学习程序的基本级别中适应性优化器的广泛范围，同时通过避免显式计算二阶梯度信息和利用为一阶梯度实现的高效分布式训练技术来降低计算负担。在多个大规模元学习基准测试中进行评估时，SAMA在单个/多个GPU上分别展示了高达1.7 / 4.8倍的吞吐量增加和2.0 / 3.8倍的内存消耗减少。",
    "tldr": "本文介绍了一种名为SAMA的方法，通过结合隐式微分算法和系统进展，使得元学习具有可扩展性和实用性。SAMA在基于元学习的程序中灵活支持各种自适应优化器，同时通过避免显式计算二阶梯度信息和利用高效的分布式训练技术降低计算负担。实验结果表明，SAMA在多个大规模元学习基准测试中展示了显著的吞吐量提升和内存消耗减少。",
    "en_tdlr": "This paper introduces a method called SAMA, which combines advances in implicit differentiation algorithms and systems, to make meta learning scalable and practical. SAMA flexibly supports various adaptive optimizers in meta learning programs while reducing computational burden by avoiding explicit computation of second-order gradients and using efficient distributed training techniques. Experimental results demonstrate significant throughput improvement and memory consumption reduction in multiple large-scale meta learning benchmarks."
}