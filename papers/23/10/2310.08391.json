{
    "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?. (arXiv:2310.08391v1 [stat.ML])",
    "abstract": "Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.",
    "link": "http://arxiv.org/abs/2310.08391",
    "context": "Title: How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?. (arXiv:2310.08391v1 [stat.ML])\nAbstract: Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.",
    "path": "papers/23/10/2310.08391.json",
    "total_tokens": 837,
    "translated_title": "多少个预训练任务需要用于线性回归的上下文学习？",
    "translated_abstract": "在多样任务上进行预训练的Transformer展现了非凡的上下文学习（ICL）能力，使其能够仅基于输入上下文解决未见任务，而无需调整模型参数。本文研究了其中最简单设置的ICL：预训练线性参数化的单层线性注意力模型，用于具有高斯先验的线性回归。我们为注意力模型预训练建立了一个统计任务复杂度界，表明有效的预训练只需要少量独立任务。此外，我们证明了预训练模型与贝叶斯最优算法非常接近，即几乎实现了固定上下文长度下未见任务的贝叶斯最优风险。这些理论发现对之前的实验研究进行了补充，并为ICL的统计基础提供了启示。",
    "tldr": "本文研究了在线性回归中的上下文学习，并发现有效的预训练只需要少量独立任务，预训练模型与贝叶斯最优算法接近。这些理论发现对ICL的统计基础提供了启示。"
}