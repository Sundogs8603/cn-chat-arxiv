{
    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)",
    "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,",
    "link": "http://arxiv.org/abs/2310.01352",
    "context": "Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)\nAbstract: Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,",
    "path": "papers/23/10/2310.01352.json",
    "total_tokens": 980,
    "translated_title": "RA-DIT: 检索增强的双重指令调优",
    "translated_abstract": "检索增强语言模型（RALMs）通过访问外部数据存储中的长尾和最新知识来提高性能，但构建起来具有挑战性。现有的方法要么需要昂贵的检索特定修改来进行语言模型预训练，要么使用事后集成数据存储的方法，导致性能不理想。我们引入了一种轻量级的微调方法——检索增强的双重指令调优（RA-DIT），通过为任何语言模型添加检索能力来实现。我们的方法分为两个不同的微调步骤：（1）一个更新预训练的语言模型以更好地利用检索到的信息，（2）另一个更新检索器以返回更相关的结果，符合语言模型的偏好。通过在需要知识利用和上下文意识的任务上进行微调，我们证明了每个阶段都能显著提高性能，并且同时使用两个阶段可以获得额外的收益。我们的最佳模型是RA-DIT 65B。",
    "tldr": "本论文介绍了一种轻量级的微调方法RA-DIT，通过为任何语言模型添加检索能力来提高性能。该方法分为两个步骤：一是更新语言模型以更好地利用检索到的信息，二是更新检索器以返回更相关的结果。实验证明每个步骤都能显著提高性能，同时使用两个步骤可以获得额外的收益。",
    "en_tdlr": "This paper introduces a lightweight fine-tuning method, RA-DIT, which enhances the performance of language models by adding retrieval capabilities. The method consists of two steps: updating the language model to better utilize retrieved information and updating the retriever to return more relevant results. Experimental results show significant performance improvements for each step, with additional gains when both steps are used."
}