{
    "title": "Learning A Disentangling Representation For PU Learning. (arXiv:2310.03833v1 [cs.LG])",
    "abstract": "In this paper, we address the problem of learning a binary (positive vs. negative) classifier given Positive and Unlabeled data commonly referred to as PU learning. Although rudimentary techniques like clustering, out-of-distribution detection, or positive density estimation can be used to solve the problem in low-dimensional settings, their efficacy progressively deteriorates with higher dimensions due to the increasing complexities in the data distribution. In this paper we propose to learn a neural network-based data representation using a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified using simple clustering techniques, effectively emulating the phenomenon observed in low-dimensional settings. We adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters. We conduct experiments on simulated PU data that demonstrate th",
    "link": "http://arxiv.org/abs/2310.03833",
    "context": "Title: Learning A Disentangling Representation For PU Learning. (arXiv:2310.03833v1 [cs.LG])\nAbstract: In this paper, we address the problem of learning a binary (positive vs. negative) classifier given Positive and Unlabeled data commonly referred to as PU learning. Although rudimentary techniques like clustering, out-of-distribution detection, or positive density estimation can be used to solve the problem in low-dimensional settings, their efficacy progressively deteriorates with higher dimensions due to the increasing complexities in the data distribution. In this paper we propose to learn a neural network-based data representation using a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified using simple clustering techniques, effectively emulating the phenomenon observed in low-dimensional settings. We adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters. We conduct experiments on simulated PU data that demonstrate th",
    "path": "papers/23/10/2310.03833.json",
    "total_tokens": 874,
    "translated_title": "学习PU学习的解缠表示",
    "translated_abstract": "在本文中，我们解决了学习一个二元（正 vs. 负）分类器的问题，给定了常常被称为PU学习的正样本和未标记样本数据。虽然在低维度情况下可以使用基本的技术（如聚类、超出分布检测或正样本密度估计）来解决该问题，但是随着数据分布的复杂性逐渐增加，这些方法的效果逐渐下降。在本文中，我们提出了一种使用损失函数来学习基于神经网络的数据表示的方法，该方法可以将未标记数据投影到两个（正和负）簇中，可以使用简单的聚类技术轻松识别，从而有效模拟低维度情况下观察到的现象。我们采用了一种向量量化技术来放大学习到的未标记数据簇之间的分离。我们在模拟的PU数据上进行了实验证明…",
    "tldr": "本文提出了一种基于神经网络的数据表示方法，通过学习一个损失函数来将未标记数据投影为两个簇，以模拟低维情况下的PU学习问题。实验证明学习到的表示可以增强未标记数据簇之间的分离效果。"
}