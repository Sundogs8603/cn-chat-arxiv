{
    "title": "Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])",
    "abstract": "Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a \"target\" state, while avoiding a set of \"unsafe\" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide saf",
    "link": "http://arxiv.org/abs/2310.01951",
    "context": "Title: Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])\nAbstract: Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a \"target\" state, while avoiding a set of \"unsafe\" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide saf",
    "path": "papers/23/10/2310.01951.json",
    "total_tokens": 893,
    "translated_title": "贝叶斯神经网络的概率性到达-避免问题",
    "translated_abstract": "基于模型的强化学习旨在同时学习未知随机环境的动力学并综合出适用于其中的最佳策略。在这样的环境中，确保通过策略作出的序列决策的安全性和鲁棒性是安全关键场景下的一个关键挑战。本文研究了两个互补的问题：第一，对由贝叶斯神经网络（BNN）描述的动力学模型进行迭代预测的到达-避免概率的计算；第二，合成最优控制策略以满足给定的到达-避免规范（达到“目标”状态，同时避免一组“不安全”状态）和学习的BNN模型。我们的解决方案利用区间传播和向后递归技术来计算策略动作序列满足到达-避免规范的概率的下界。这样计算出的下界提供了安全性保证。",
    "tldr": "本文讨论了贝叶斯神经网络在计算到达-避免概率和合成最优控制策略中的应用，通过利用区间传播和向后递归技术，计算出了概率的下界作为安全性保证。",
    "en_tdlr": "This paper discusses the application of Bayesian neural networks in computing reach-avoid probabilities and synthesizing optimal control policies. By using interval propagation and backward recursion techniques, lower bounds for the probabilities are computed as safety guarantees."
}