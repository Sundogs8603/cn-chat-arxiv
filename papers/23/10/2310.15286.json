{
    "title": "A Doubly Robust Approach to Sparse Reinforcement Learning. (arXiv:2310.15286v1 [stat.ML])",
    "abstract": "We propose a new regret minimization algorithm for episodic sparse linear Markov decision process (SMDP) where the state-transition distribution is a linear function of observed features. The only previously known algorithm for SMDP requires the knowledge of the sparsity parameter and oracle access to an unknown policy. We overcome these limitations by combining the doubly robust method that allows one to use feature vectors of \\emph{all} actions with a novel analysis technique that enables the algorithm to use data from all periods in all episodes. The regret of the proposed algorithm is $\\tilde{O}(\\sigma^{-1}_{\\min} s_{\\star} H \\sqrt{N})$, where $\\sigma_{\\min}$ denotes the restrictive the minimum eigenvalue of the average Gram matrix of feature vectors, $s_\\star$ is the sparsity parameter, $H$ is the length of an episode, and $N$ is the number of rounds. We provide a lower regret bound that matches the upper bound up to logarithmic factors on a newly identified subclass of SMDPs. Our",
    "link": "http://arxiv.org/abs/2310.15286",
    "context": "Title: A Doubly Robust Approach to Sparse Reinforcement Learning. (arXiv:2310.15286v1 [stat.ML])\nAbstract: We propose a new regret minimization algorithm for episodic sparse linear Markov decision process (SMDP) where the state-transition distribution is a linear function of observed features. The only previously known algorithm for SMDP requires the knowledge of the sparsity parameter and oracle access to an unknown policy. We overcome these limitations by combining the doubly robust method that allows one to use feature vectors of \\emph{all} actions with a novel analysis technique that enables the algorithm to use data from all periods in all episodes. The regret of the proposed algorithm is $\\tilde{O}(\\sigma^{-1}_{\\min} s_{\\star} H \\sqrt{N})$, where $\\sigma_{\\min}$ denotes the restrictive the minimum eigenvalue of the average Gram matrix of feature vectors, $s_\\star$ is the sparsity parameter, $H$ is the length of an episode, and $N$ is the number of rounds. We provide a lower regret bound that matches the upper bound up to logarithmic factors on a newly identified subclass of SMDPs. Our",
    "path": "papers/23/10/2310.15286.json",
    "total_tokens": 922,
    "translated_title": "一种用于稀疏强化学习的双重稳健方法",
    "translated_abstract": "我们提出了一种新的遗憾最小化算法，用于状态-转移分布为观测特征的线性函数的周期性稀疏线性马尔可夫决策过程（SMDP）。之前已知的SMDP算法需要知道稀疏参数并能够访问未知策略的oracle。我们通过结合双重稳健方法允许使用\\emph{所有}动作的特征向量和一种新颖的分析技术来克服这些限制。该算法的遗憾是$\\tilde{O}(\\sigma^{-1}_{\\min} s_{\\star} H \\sqrt{N})$，其中$\\sigma_{\\min}$表示特征向量的平均Gram矩阵的最小特征值，$s_\\star$是稀疏参数，$H$是一个周期的长度，$N$是回合数。我们提供了一个匹配上界的遗憾下界，适用于新识别出的SMDP子类，对数因子除外。",
    "tldr": "我们提出了一种新的遗憾最小化算法，用于稀疏强化学习问题，通过结合双重稳健方法和新颖的分析技术，克服了之前算法对稀疏参数和未知策略的限制。",
    "en_tdlr": "We propose a new regret minimization algorithm for sparse reinforcement learning by combining the doubly robust method and a novel analysis technique, overcoming the limitations of the previous algorithm regarding sparsity parameter and unknown policy."
}