{
    "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models. (arXiv:2310.12936v2 [cs.CL] UPDATED)",
    "abstract": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
    "link": "http://arxiv.org/abs/2310.12936",
    "context": "Title: A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models. (arXiv:2310.12936v2 [cs.CL] UPDATED)\nAbstract: Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
    "path": "papers/23/10/2310.12936.json",
    "total_tokens": 891,
    "translated_title": "预训练遮蔽语言模型中社会偏见与任务表现的预测因素分析",
    "translated_abstract": "先前的研究中报告了各种类型的预训练遮蔽语言模型(MLMs)存在社会偏见。然而，MLM与许多潜在因素相关，如模型大小、训练数据大小、训练目标、预训练数据的领域、分词和预训练语料中包含的语言等。目前尚不清楚这些因素中哪些影响了MLMs学习的社会偏见。为了研究模型因素与MLMs学习的社会偏见以及模型的下游任务表现之间的关系，我们对39个不同模型大小、训练目标、分词方法、训练数据领域和语言的预训练MLMs进行了全面研究。我们的结果揭示了在先前的文献中经常被忽视的重要因素，如分词或模型目标。",
    "tldr": "本研究对39个不同模型大小、训练目标、分词方法、训练数据领域和语言的预训练语言模型进行了全面研究，发现分词和模型目标等因素对MLMs学习的社会偏见具有重要影响。",
    "en_tdlr": "This study comprehensively investigates 39 pretrained language models with different model sizes, training objectives, tokenization methods, training data domains, and languages, revealing the important influences of factors such as tokenization and model objectives on the social biases learned by MLMs."
}