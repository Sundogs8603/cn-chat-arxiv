{
    "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace",
    "abstract": "arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a",
    "link": "https://arxiv.org/abs/2310.19651",
    "context": "Title: Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace\nAbstract: arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a",
    "path": "papers/23/10/2310.19651.json",
    "total_tokens": 838,
    "translated_title": "指令调整的动态：大型语言模型的每个能力都有其自己的增长速率",
    "translated_abstract": "指令调整是一种新兴方法，用于激发大型语言模型（LLMs）的普遍智能。然而，指令数据的创建仍然主要是启发式的，导致现有数据集在数量和质量上存在显着差异。我们的研究通过对数据量、参数大小和数据构建方法如何影响LLM的每个基本能力（如创意写作、代码生成和逻辑推理）的发展进行细致分析，以更好地理解数据构建准则。我们提供了一个经过精心策划的数据集，涵盖了十种能力的超过40k个实例，并研究了具有70亿至330亿参数的经过指令调整的模型。我们的研究揭示了三个主要发现：",
    "tldr": "本研究通过对指令调整对每个大型语言模型的各项能力（如创意写作、代码生成和逻辑推理）的发展影响进行细致分析，得出了关于数据集规模、参数规模和数据构建方法的指导原则。",
    "en_tdlr": "This study provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of Large Language Models (LLMs) and offers data construction guidelines for instruction-tuned models."
}