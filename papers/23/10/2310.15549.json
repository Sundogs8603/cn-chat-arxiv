{
    "title": "Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing. (arXiv:2310.15549v1 [math.OC])",
    "abstract": "Gradient descent (GD) is crucial for generalization in machine learning models, as it induces implicit regularization, promoting compact representations. In this work, we examine the role of GD in inducing implicit regularization for tensor optimization, particularly within the context of the lifted matrix sensing framework. This framework has been recently proposed to address the non-convex matrix sensing problem by transforming spurious solutions into strict saddles when optimizing over symmetric, rank-1 tensors. We show that, with sufficiently small initialization scale, GD applied to this lifted problem results in approximate rank-1 tensors and critical points with escape directions. Our findings underscore the significance of the tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.",
    "link": "http://arxiv.org/abs/2310.15549",
    "context": "Title: Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing. (arXiv:2310.15549v1 [math.OC])\nAbstract: Gradient descent (GD) is crucial for generalization in machine learning models, as it induces implicit regularization, promoting compact representations. In this work, we examine the role of GD in inducing implicit regularization for tensor optimization, particularly within the context of the lifted matrix sensing framework. This framework has been recently proposed to address the non-convex matrix sensing problem by transforming spurious solutions into strict saddles when optimizing over symmetric, rank-1 tensors. We show that, with sufficiently small initialization scale, GD applied to this lifted problem results in approximate rank-1 tensors and critical points with escape directions. Our findings underscore the significance of the tensor parametrization of matrix sensing, in combination with first-order methods, in achieving global optimality in such problems.",
    "path": "papers/23/10/2310.15549.json",
    "total_tokens": 861,
    "translated_title": "张量优化中的算法正则化: 迈向矩阵感知中的升维方法",
    "translated_abstract": "梯度下降（GD）在机器学习模型中对于泛化至关重要，因为它引入了隐式正则化，促进了紧凑的表示。在这项工作中，我们研究了GD在张量优化中引导隐式正则化的作用，尤其是在升维矩阵感知框架中的应用。最近提出的这个框架通过将对称的、秩1的张量上的优化问题转化成严格鞍点，从而解决了非凸的矩阵感知问题。我们发现，在足够小的初始化尺度下，将GD应用于这个升维问题，可以得到近似秩1的张量和具有逃逸方向的临界点。我们的研究结果强调了张量参数化与一阶方法在这类问题中实现全局最优性的重要性。",
    "tldr": "在矩阵感知问题中，通过对称、秩1张量进行优化求解时，将梯度下降应用于升维问题可以得到近似的秩1张量和具有逃逸方向的临界点，这结果强调了张量参数化和一阶方法在实现全局最优性方面的重要性。",
    "en_tdlr": "Applying gradient descent to the lifted problem of optimizing symmetric, rank-1 tensors in matrix sensing can yield approximate rank-1 tensors and critical points with escape directions, highlighting the importance of tensor parametrization and first-order methods in achieving global optimality."
}