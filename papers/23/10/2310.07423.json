{
    "title": "Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])",
    "abstract": "Recently, large pre-trained multilingual speech models have shown potential in scaling Automatic Speech Recognition (ASR) to many low-resource languages. Some of these models employ language adapters in their formulation, which helps to improve monolingual performance and avoids some of the drawbacks of multi-lingual modeling on resource-rich languages. However, this formulation restricts the usability of these models on code-switched speech, where two languages are mixed together in the same utterance. In this work, we propose ways to effectively fine-tune such models on code-switched speech, by assimilating information from both language adapters at each language adaptation point in the network. We also model code-switching as a sequence of latent binary sequences that can be used to guide the flow of information from each language adapter at the frame level. The proposed approaches are evaluated on three code-switched datasets encompassing Arabic, Mandarin, and Hindi languages paire",
    "link": "http://arxiv.org/abs/2310.07423",
    "context": "Title: Adapting the adapters for code-switching in multilingual ASR. (arXiv:2310.07423v1 [cs.CL])\nAbstract: Recently, large pre-trained multilingual speech models have shown potential in scaling Automatic Speech Recognition (ASR) to many low-resource languages. Some of these models employ language adapters in their formulation, which helps to improve monolingual performance and avoids some of the drawbacks of multi-lingual modeling on resource-rich languages. However, this formulation restricts the usability of these models on code-switched speech, where two languages are mixed together in the same utterance. In this work, we propose ways to effectively fine-tune such models on code-switched speech, by assimilating information from both language adapters at each language adaptation point in the network. We also model code-switching as a sequence of latent binary sequences that can be used to guide the flow of information from each language adapter at the frame level. The proposed approaches are evaluated on three code-switched datasets encompassing Arabic, Mandarin, and Hindi languages paire",
    "path": "papers/23/10/2310.07423.json",
    "total_tokens": 884,
    "translated_title": "为多语言自动语音识别中的代码切换调整适配器",
    "translated_abstract": "最近，大规模预训练的多语言语音模型展示了在许多资源匮乏语言中扩展自动语音识别（ASR）的潜力。其中一些模型在其组成中使用语言适配器，这有助于提高单语性能，并避免多语言建模在资源丰富的语言中的一些缺点。然而，这种组成限制了这些模型在代码切换语音上的可用性，即在同一个话语中混合了两种语言。在这项工作中，我们提出了一种有效地在代码切换语音上微调这些模型的方法，通过在网络中的每个语言适配点同化来自两个语言适配器的信息。我们还将代码切换建模为一系列可以用于指导每个帧级别的语言适配器信息流的潜在二进制序列。我们对包括阿拉伯语、普通话和印地语在内的三个代码切换数据集进行了评估。",
    "tldr": "本论文研究了如何在多语言自动语音识别中有效地调整适配器来处理代码切换语音，提出了将两个语言适配器的信息融合并将代码切换建模为潜在二进制序列的方法。",
    "en_tdlr": "This paper investigates adapting adapters in multilingual ASR to effectively handle code-switching speech, proposing methods to assimilate information from both language adapters and model code-switching as latent binary sequences."
}