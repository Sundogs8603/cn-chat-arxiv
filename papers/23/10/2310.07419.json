{
    "title": "Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else. (arXiv:2310.07419v1 [cs.CV])",
    "abstract": "Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic m",
    "link": "http://arxiv.org/abs/2310.07419",
    "context": "Title: Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else. (arXiv:2310.07419v1 [cs.CV])\nAbstract: Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic m",
    "path": "papers/23/10/2310.07419.json",
    "total_tokens": 992,
    "translated_title": "多概念 T2I-Zero: 仅调整文本嵌入，不做其他改变",
    "translated_abstract": "最近的文本到图像扩散模型的进展使得从文本提示生成逼真的图像成为可能。尽管取得了很大进步，现有模型仍然难以自然地生成组合的多概念图像，限制了它们可视化人类想象力的能力。虽然最近的一些研究尝试解决这个问题，但要么引入额外的训练，要么在推理时采用指导。在这项工作中，我们考虑了一个更具雄心的目标：使用预训练的扩散模型实现自然多概念生成，并且几乎没有额外的成本。为了实现这个目标，我们确定了预训练的文本到图像扩散模型中使用的文本嵌入的局限性。具体而言，我们观察到概念支配和非定位贡献严重降低了多概念生成性能。我们进一步设计了一个最小廉价的解决方案，通过调整（而不是重新训练）文本嵌入来解决上述问题，以获得更真实的生成效果。",
    "tldr": "本研究提出了一种新的方法，通过仅仅调整文本嵌入，而不需重新训练模型，实现了自然的多概念生成。通过解决预训练模型中文本嵌入的局限性，如概念支配和非定位贡献，我们设计了一个廉价的解决方案，提高了多概念生成的性能。",
    "en_tdlr": "This paper proposes a novel approach to achieve natural multi-concept generation by tweaking text embeddings without retraining the model. By addressing the limitations of text embeddings in pre-trained models, such as concept dominance and non-localized contribution, a low-cost solution is designed to improve multi-concept generation performance."
}