{
    "title": "Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules. (arXiv:2310.14753v2 [cs.LG] UPDATED)",
    "abstract": "Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (i.e., subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in M",
    "link": "http://arxiv.org/abs/2310.14753",
    "context": "Title: Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules. (arXiv:2310.14753v2 [cs.LG] UPDATED)\nAbstract: Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (i.e., subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in M",
    "path": "papers/23/10/2310.14753.json",
    "total_tokens": 894,
    "translated_title": "重新思考分子掩码图模型中的分词器和解码器",
    "translated_abstract": "掩码图模型在自监督表示学习中表现出众，特别是对于分子图。通过对之前的研究进行仔细审查，我们可以揭示一个常见的方案，包括三个关键组成部分：（1）图分词器，它将分子图分解为较小的片段（即子图），并将其转换为标记；（2）图掩码，用掩码破坏图；（3）图自编码器，它首先在掩码图上应用编码器生成表示，然后在表示上使用解码器恢复原始图的标记。然而，之前的MGM研究主要关注图掩码和编码器，对于分词器和解码器的理解较有限。为了弥补这个差距，我们首先总结了节点、边、主题和图神经网络（GNN）的常用分子分词器，然后考察它们作为MGM重构目标的作用。此外，我们还探索了在MGM中采用高表达解码器的潜力。",
    "tldr": "重新思考分子掩码图模型中的分词器和解码器，通过对分子分词器的总结和解码器的探索，填补了分词器和解码器的研究空白。",
    "en_tdlr": "Rethinking tokenizer and decoder in masked graph modeling for molecules fills the research gap by summarizing molecule tokenizers and exploring the potential of expressive decoders."
}