{
    "title": "Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])",
    "abstract": "This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call \"Language-World,\" which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https",
    "link": "http://arxiv.org/abs/2310.17019",
    "context": "Title: Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])\nAbstract: This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call \"Language-World,\" which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https",
    "path": "papers/23/10/2310.17019.json",
    "total_tokens": 990,
    "translated_title": "使用大语言模型有条件地组合机器人技能",
    "translated_abstract": "这篇论文结合了两个贡献。首先，我们介绍了Meta-World基准的一个扩展，称为“语言世界”，它允许一个大型语言模型在一个模拟机器人环境中使用半结构化自然语言查询和用自然语言描述的脚本技能。通过使用与Meta-World相同的任务集，可以轻松比较语言世界和Meta-World的结果，从而对最近使用大型语言模型（LLMs）和使用深度强化学习的方法进行比较。其次，我们介绍了一种称为计划条件行为克隆（PCBC）的方法，它允许对高级计划的行为进行微调，使用端到端演示。使用语言世界，我们展示了PCBC能够在各种少样本情况下取得卓越的性能，通常只需一个演示即可实现任务的泛化。我们将语言世界作为开源软件提供，网址是https://...",
    "tldr": "这篇论文介绍了一个扩展的Meta-World基准，称为“语言世界”，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本技能。同时，引入了计划条件行为克隆（PCBC）的方法，通过端到端演示对高级计划进行微调。实验结果表明，在少样本情况下，PCBC在语言世界中能够实现强大的性能。",
    "en_tdlr": "This paper introduces an extension of the Meta-World benchmark, called \"Language-World,\" that enables large language models to operate in a simulated robotic environment using natural language queries and scripted skills. It also presents a method called Plan Conditioned Behavioral Cloning (PCBC) that fine-tunes high-level plans using end-to-end demonstrations. Experimental results show that PCBC achieves strong performance in few-shot scenarios in the Language-World environment."
}