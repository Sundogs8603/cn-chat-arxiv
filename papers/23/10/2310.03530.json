{
    "title": "Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])",
    "abstract": "The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.",
    "link": "http://arxiv.org/abs/2310.03530",
    "context": "Title: Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])\nAbstract: The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.",
    "path": "papers/23/10/2310.03530.json",
    "total_tokens": 891,
    "translated_title": "在数据-参数域上，联合群不变函数引导了通用神经网络",
    "translated_abstract": "将输入数据的对称性和几何性考虑为编码在神经网络内部数据表示中，但是具体的编码规则还没有得到深入研究。通过关注数据-参数域上的联合群不变函数，我们提出了一种系统的规则，从数据域上的群作用中找到参数域上的双重群作用。此外，我们引入了由联合不变函数导出的广义神经网络，并利用Schur引理给出了它们的普遍性定理的新的群论证明。由于传统的普遍性定理是基于函数分析方法进行证明的，这项研究揭示了逼近理论的群论方面，将几何深度学习与抽象调和分析相连接。",
    "tldr": "本研究通过探索联合群不变函数在数据-参数域上的作用，提出了一种系统的规则来解码神经网络内部数据表示中的对称性和几何性。利用这一规则，我们引入了由联合不变函数导出的通用神经网络，并利用群论证明了其普适性。这一研究揭示了逼近理论和深度学习中的群论方面，并将几何深度学习与抽象调和分析相连接。",
    "en_tdlr": "This study proposes a systematic rule to decode the symmetry and geometry encoded in the internal data representation of a neural network by exploring the joint group invariant functions on the data-parameter domain. By introducing generalized neural networks derived from these joint invariant functions, the universality theorems are proved using group theory, shedding light on the connection between geometric deep learning and abstract harmonic analysis in approximation theory."
}