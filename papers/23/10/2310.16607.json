{
    "title": "On the Interplay between Fairness and Explainability. (arXiv:2310.16607v1 [cs.CL])",
    "abstract": "In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both. In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. We find that bias mitigation algorithms do not always lead to fairer models. Moreover, we discover that ",
    "link": "http://arxiv.org/abs/2310.16607",
    "context": "Title: On the Interplay between Fairness and Explainability. (arXiv:2310.16607v1 [cs.CL])\nAbstract: In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both. In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. We find that bias mitigation algorithms do not always lead to fairer models. Moreover, we discover that ",
    "path": "papers/23/10/2310.16607.json",
    "total_tokens": 861,
    "translated_title": "公平性与可解释性之间的相互作用",
    "translated_abstract": "为了构建可靠和值得信赖的自然语言处理(NLP)应用，模型需要在不同的人口统计数据中既具有公平性又可解释。通常，这两个目标，即公平性和可解释性，会被独立地进行优化和/或研究。相反，我们认为未来可信的NLP系统应该同时考虑两者。在这项工作中，我们进行了首次研究，以了解它们如何相互影响：更公平的模型是否依赖于更合理的解释？反之亦然。为此，我们在两个英语多类文本分类数据集BIOS和ECtHR上进行实验，这些数据集分别提供了有关性别和国籍的信息，以及人工标注的解释。我们使用多种方法对预训练语言模型进行微调，包括(i)偏见缓解，旨在提高公平性；(ii)解释提取，旨在产生合理的解释。我们发现，偏见缓解算法并不总是导致更公平的模型。此外，我们还发现",
    "tldr": "公平的NLP模型并不总是依赖于更合理的解释，偏见缓解算法并不总是导致更公平的模型。"
}