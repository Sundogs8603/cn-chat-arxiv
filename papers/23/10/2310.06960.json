{
    "title": "Jaynes Machine: The universal microstructure of deep neural networks. (arXiv:2310.06960v1 [cond-mat.stat-mech] CROSS LISTED)",
    "abstract": "We present a novel theory of the microstructure of deep neural networks. Using a theoretical framework called statistical teleodynamics, which is a conceptual synthesis of statistical thermodynamics and potential game theory, we predict that all highly connected layers of deep neural networks have a universal microstructure of connection strengths that is distributed lognormally ($LN({\\mu}, {\\sigma})$). Furthermore, under ideal conditions, the theory predicts that ${\\mu}$ and ${\\sigma}$ are the same for all layers in all networks. This is shown to be the result of an arbitrage equilibrium where all connections compete and contribute the same effective utility towards the minimization of the overall loss function. These surprising predictions are shown to be supported by empirical data from six large-scale deep neural networks in real life. We also discuss how these results can be exploited to reduce the amount of data, time, and computational resources needed to train large deep neural",
    "link": "http://arxiv.org/abs/2310.06960",
    "context": "Title: Jaynes Machine: The universal microstructure of deep neural networks. (arXiv:2310.06960v1 [cond-mat.stat-mech] CROSS LISTED)\nAbstract: We present a novel theory of the microstructure of deep neural networks. Using a theoretical framework called statistical teleodynamics, which is a conceptual synthesis of statistical thermodynamics and potential game theory, we predict that all highly connected layers of deep neural networks have a universal microstructure of connection strengths that is distributed lognormally ($LN({\\mu}, {\\sigma})$). Furthermore, under ideal conditions, the theory predicts that ${\\mu}$ and ${\\sigma}$ are the same for all layers in all networks. This is shown to be the result of an arbitrage equilibrium where all connections compete and contribute the same effective utility towards the minimization of the overall loss function. These surprising predictions are shown to be supported by empirical data from six large-scale deep neural networks in real life. We also discuss how these results can be exploited to reduce the amount of data, time, and computational resources needed to train large deep neural",
    "path": "papers/23/10/2310.06960.json",
    "total_tokens": 967,
    "translated_title": "Jaynes Machine: 深度神经网络的通用微结构",
    "translated_abstract": "我们提出了关于深度神经网络微结构的新理论。使用名为统计远动力学的理论框架，它是统计热力学和潜在博弈理论的概念综合，我们预测深度神经网络所有高连接层具有分布为对数正态分布的通用连接强度微结构（$LN({\\mu}, {\\sigma})$)。此外，在理想条件下，理论预测对于所有网络的所有层，${\\mu}$和${\\sigma}$是相同的。这是由所有连接竞争并对整体损失函数最小化做出相同有效效用的套利均衡的结果。这些令人惊讶的预测得到了来自六个大规模深度神经网络的实证数据的支持。我们还讨论了如何利用这些结果来减少训练大规模深度神经网络所需的数据量，时间和计算资源。",
    "tldr": "Jaynes Machine提出了一种关于深度神经网络微结构的新理论，预测了所有高连接层具有分布为对数正态分布的通用连接强度微结构，并在理想条件下预测了${\\mu}$和${\\sigma}$在所有网络的所有层中是相同的。实证数据支持这些预测，并讨论了如何利用这些结果来减少训练大规模深度神经网络所需的资源。",
    "en_tdlr": "Jaynes Machine presents a new theory on the microstructure of deep neural networks, predicting a universal microstructure of connection strengths with a lognormal distribution for all highly connected layers. The theory further suggests that this microstructure is the same across all networks and under ideal conditions. Empirical data from six large-scale deep neural networks supports these predictions, and the potential applications of these findings in reducing resource requirements for training such networks are discussed."
}