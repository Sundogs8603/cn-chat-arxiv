{
    "title": "LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction. (arXiv:2310.03414v1 [cs.CL])",
    "abstract": "Multi-document summarization is a challenging task due to its inherent subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4 among DUC-2004 reference summaries. In this work, we aim to enhance the objectivity of news summarization by focusing on the main event of a group of related news documents and presenting it coherently with sufficient context. Our primary objective is to succinctly report the main event, ensuring that the summary remains objective and informative. To achieve this, we employ an extract-rewrite approach that incorporates a main-event biased monotone-submodular function for content selection. This enables us to extract the most crucial information related to the main event from the document cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for rewriting the extracted content into a coherent text. The evaluation using objective metrics and human evaluators confirms the effectiveness of our approach, as it surpasses pote",
    "link": "http://arxiv.org/abs/2310.03414",
    "context": "Title: LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction. (arXiv:2310.03414v1 [cs.CL])\nAbstract: Multi-document summarization is a challenging task due to its inherent subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4 among DUC-2004 reference summaries. In this work, we aim to enhance the objectivity of news summarization by focusing on the main event of a group of related news documents and presenting it coherently with sufficient context. Our primary objective is to succinctly report the main event, ensuring that the summary remains objective and informative. To achieve this, we employ an extract-rewrite approach that incorporates a main-event biased monotone-submodular function for content selection. This enables us to extract the most crucial information related to the main event from the document cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for rewriting the extracted content into a coherent text. The evaluation using objective metrics and human evaluators confirms the effectiveness of our approach, as it surpasses pote",
    "path": "papers/23/10/2310.03414.json",
    "total_tokens": 922,
    "translated_title": "基于LLM的基于主事件偏向的单调次模内容提取的多文档摘要",
    "translated_abstract": "多文档摘要是一项具有挑战性的任务，由于其固有的主观偏见，DUC-2004参考摘要的互评ROUGE-1分数仅为0.4。在本研究中，我们旨在通过聚焦相关新闻文档的主要事件并以充分的上下文一致地呈现它来增强新闻摘要的客观性。我们的主要目标是简洁地报道主要事件，以确保摘要保持客观且信息丰富。为实现这一目标，我们采用提取-重写方法，其中包括使用主事件偏向的单调次模函数进行内容选择。这使我们能够从文档群中提取与主要事件相关的最关键信息。为确保连贯性，我们利用经过微调的语言模型（LLM）将提取的内容重写为连贯的文本。使用客观评估指标和人工评估者的评估确认了我们方法的有效性，因为它超越了潜在的方法。",
    "tldr": "该论文提出了一种基于主事件偏向的单调次模内容提取的多文档摘要方法，通过使用提取-重写方法和微调的语言模型，确保摘要客观性和信息丰富性。",
    "en_tdlr": "This paper proposes a multi-document summarization method based on main-event biased monotone-submodular content extraction, which ensures the objectivity and informativeness of the summary by using an extract-rewrite approach and a fine-tuned language model."
}