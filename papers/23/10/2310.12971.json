{
    "title": "CLAIR: Evaluating Image Captions with Large Language Models. (arXiv:2310.12971v1 [cs.CV] CROSS LISTED)",
    "abstract": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the u",
    "link": "http://arxiv.org/abs/2310.12971",
    "context": "Title: CLAIR: Evaluating Image Captions with Large Language Models. (arXiv:2310.12971v1 [cs.CV] CROSS LISTED)\nAbstract: The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the u",
    "path": "papers/23/10/2310.12971.json",
    "total_tokens": 888,
    "translated_title": "CLAIR: 用大型语言模型评估图像标题",
    "translated_abstract": "机器生成图像标题的评估是一个有趣但持久存在的挑战。有效的评估指标必须考虑多个相似性维度，包括语义相关性、视觉结构、物体交互、标题多样性和特定性。现有的高度工程化的评估方法试图捕捉特定方面，但在提供与人类判断密切一致的整体评分方面仍有不足之处。在这里，我们提出了CLAIR，一种利用大型语言模型（LLM）的零射语言建模能力来评估候选标题的新方法。在我们的评估中，CLAIR相对于现有指标更能与人类对标题质量的判断相关。值得注意的是，在Flickr8K-Expert上，CLAIR在与SPICE相比的相关改进方面提高了39.6％，在与RefCLIP-S等图像增强方法相比的相关改进方面提高了18.3％。此外，CLAIR提供了可解释性结果，允许语言模型识别u",
    "tldr": "CLAIR是一种基于大型语言模型的新方法，用于评估机器生成的图像标题。相对于现有的评估方法，CLAIR在与人类判断的相关性方面表现更好，并针对具体数据集取得了较大改进。",
    "en_tdlr": "CLAIR is a novel method that uses large language models to evaluate machine-generated image captions. It outperforms existing measures in terms of correlation with human judgments and achieves significant improvements on specific datasets."
}