{
    "title": "How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])",
    "abstract": "A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U",
    "link": "http://arxiv.org/abs/2310.03494",
    "context": "Title: How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])\nAbstract: A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U",
    "path": "papers/23/10/2310.03494.json",
    "total_tokens": 951,
    "translated_title": "如何水平采样过程影响深度强化学习中的零样本泛化",
    "translated_abstract": "阻止广泛采用通过深度强化学习（RL）训练的自主代理代理的关键局限是它们有限的适应新环境能力，即使这些环境与训练中遇到的环境具有相似特征。本研究探讨了个体环境实例或层级的非均匀采样策略如何影响RL代理的零样本泛化（ZSG）能力，并考虑了两种失效模式：过拟合和过度泛化。首先，我们测量了代理的内部表示与训练层级集之间的相互信息（MI），发现MI与实例的过拟合相关性很强。与均匀采样相比，基于值损失优先级的自适应采样策略更能有效地保持较低的MI，这为这类技术提供了一种新的理论解释。接下来，我们将注意力转向无监督环境设计（U）",
    "tldr": "这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。"
}