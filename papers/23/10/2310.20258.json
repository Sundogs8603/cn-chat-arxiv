{
    "title": "Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])",
    "abstract": "Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc",
    "link": "http://arxiv.org/abs/2310.20258",
    "context": "Title: Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])\nAbstract: Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc",
    "path": "papers/23/10/2310.20258.json",
    "total_tokens": 848,
    "translated_title": "通过学习相关的潜在空间推进贝叶斯优化",
    "translated_abstract": "贝叶斯优化是一种通过有限的函数评估来优化黑盒函数的强大方法。最近的研究表明，通过深度生成模型（如变分自动编码器）在潜在空间中进行优化，可以对结构化或离散数据进行有效和高效的贝叶斯优化。然而，由于优化不是在输入空间中进行，这导致了潜在的差距，可能导致次优解。为了减轻这种差距，我们提出了相关潜在空间贝叶斯优化（CoBO），它专注于学习相关的潜在空间，其特点是潜在空间中的距离和目标函数内的距离之间存在强相关性。具体来说，我们的方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以最小化有希望区域周围的潜在差距。我们在几个优化任务中展示了我们方法的有效性。",
    "tldr": "本文提出了一种通过学习相关的潜在空间来推进贝叶斯优化的方法。该方法引入了Lipschitz正则化、损失加权和信任区域重新协调，以减小在潜在空间和目标函数之间的差距，并在多个优化任务中展示了其有效性。"
}