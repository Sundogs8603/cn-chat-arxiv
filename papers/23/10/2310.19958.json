{
    "title": "PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh",
    "link": "http://arxiv.org/abs/2310.19958",
    "context": "Title: PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])\nAbstract: Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh",
    "path": "papers/23/10/2310.19958.json",
    "total_tokens": 862,
    "translated_title": "PriPrune: 在剪枝联邦学习中量化和保护隐私",
    "translated_abstract": "联邦学习（FL）是一种允许多个客户设备和服务器通过仅交换模型更新而共同训练全局模型的范例，而不需要设备共享他们的局部训练数据的方法。这些设备在通信和计算资源方面往往受到限制，并且可以进一步从模型剪枝中受益 - 这是一种广泛用于减小模型大小和复杂度的范例。直观地说，通过使局部模型更粗糙，剪枝预计在FL环境中也会提供一定的隐私攻击保护。然而，目前尚未对此保护进行正式或实验性的特征化，并且不清楚它是否足以抵御最先进的攻击。在这篇论文中，我们对剪枝FL模型的隐私保障进行了首次调查。我们推导出剪枝FL模型泄露的信息论上界。我们通过理论发现进行了补充和验证",
    "tldr": "本文针对剪枝联邦学习中的隐私问题进行了调查，推导出了泄露信息的上限，并进行了理论验证和实验验证。",
    "en_tdlr": "This paper investigates the privacy concerns of model pruning in federated learning and provides information-theoretic upper bounds on leaked information, which are complemented and validated theoretically and experimentally."
}