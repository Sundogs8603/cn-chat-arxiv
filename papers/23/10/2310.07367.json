{
    "title": "Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model. (arXiv:2310.07367v1 [cs.LG])",
    "abstract": "In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\\epsilon$ non-interactive LDP model and provide a lower bound of $\\Omega(\\frac{\\sqrt{dk\\log d}}{\\sqrt{n}\\epsilon})$ on the $\\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an ",
    "link": "http://arxiv.org/abs/2310.07367",
    "context": "Title: Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model. (arXiv:2310.07367v1 [cs.LG])\nAbstract: In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\\epsilon$ non-interactive LDP model and provide a lower bound of $\\Omega(\\frac{\\sqrt{dk\\log d}}{\\sqrt{n}\\epsilon})$ on the $\\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an ",
    "path": "papers/23/10/2310.07367.json",
    "total_tokens": 943,
    "translated_title": "在本地差分隐私模型中改进稀疏线性回归的分析",
    "translated_abstract": "本文重新审视了在本地差分隐私模型中稀疏线性回归的问题。现有的非交互式和顺序本地模型的研究主要集中在获取基础参数为1稀疏的下界上，将这些下界扩展到更一般的k稀疏情况证明非常具有挑战性。此外，目前还不清楚是否存在高效的非交互式本地差分隐私算法。为了解决这些问题，我们首先考虑了在ε非交互式本地差分隐私模型中的问题，并对子高斯数据的L2范数估计误差提出了一个下界Ω(sqrt(dklogd)/(sqrtnε))，其中n是样本量，d是空间的维数。我们提出了一种创新的非交互式本地差分隐私算法，这是该问题的首个非交互式本地差分隐私算法。这个算法可以产生一种新颖且高效的估计器作为有价值的副产品。",
    "tldr": "本文在本地差分隐私模型中改进了稀疏线性回归的分析。我们提出的创新算法不仅是首个非交互式本地差分隐私算法，而且产生了一种新颖且高效的估计器。"
}