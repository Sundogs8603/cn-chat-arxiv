{
    "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])",
    "abstract": "Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. ",
    "link": "http://arxiv.org/abs/2310.03675",
    "context": "Title: Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])\nAbstract: Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. ",
    "path": "papers/23/10/2310.03675.json",
    "total_tokens": 873,
    "translated_title": "使用整数进行Hadamard域训练的类增量量化学习",
    "translated_abstract": "连续学习是许多现代机器学习应用中的一个理想特性，它允许现场适应和更新，包括适应分布变化、微调和学习新任务。对于具有隐私和低延迟要求的应用，连续学习所带来的计算和内存需求可能会对资源受限的边缘平台造成成本限制。通过完全量化训练（FQT）减少计算精度可以同时减少内存占用并增加训练和推断的计算效率。然而，激进的量化，特别是整数FQT，往往会将模型的准确性降低到无法接受的水平。在本文中，我们提出了一种利用廉价的Hadamard变换实现仅通过整数矩阵乘法进行低精度训练的技术。我们进一步确定哪些张量需要随机舍入，并提出了分块矩阵乘法以实现低位宽累加器。",
    "tldr": "本文提出了一种使用廉价的Hadamard变换来实现整数矩阵乘法的低精度训练技术，以在资源受限的边缘平台上实现计算和内存的高效性。",
    "en_tdlr": "This paper proposes a technique that uses inexpensive Hadamard transforms to enable low-precision training with integer matrix multiplications, achieving high computational and memory efficiency on resource-constrained edge platforms."
}