{
    "title": "Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear",
    "link": "http://arxiv.org/abs/2310.01798",
    "context": "Title: Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear",
    "path": "papers/23/10/2310.01798.json",
    "total_tokens": 838,
    "translated_title": "大型语言模型尚不能自我纠正推理错误",
    "translated_abstract": "大型语言模型(LLMs)凭借其在各种应用中无可比拟的文本生成能力而成为突破性的技术。然而，对于其生成内容的准确性和适当性仍存在疑虑。自我纠正方法被提出作为解决这些问题的一种方法。本文在此基础上对LLMs内部的自我纠正的作用和效果进行了批判性的考察，揭示了其真正的潜力和限制。我们的研究主要关注内在自我纠正的概念，即LLMs尝试仅仅依靠其固有能力来纠正其初始响应，而不依赖于外部反馈的支持。在推理的背景下，我们的研究表明LLMs在没有外部反馈的情况下很难自我纠正其响应，甚至有时候其表现可能在自我纠正后下降。基于这些洞见，我们对未来的研究提出了建议。",
    "tldr": "大型语言模型(LLMs)的自我纠正能力在推理方面存在困难，甚至可能在自我纠正后性能下降。",
    "en_tdlr": "Large Language Models (LLMs) struggle with self-correction in reasoning and their performance may even degrade after self-correction."
}