{
    "title": "Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage. (arXiv:2310.18434v1 [cs.LG])",
    "abstract": "The goal of an offline reinforcement learning (RL) algorithm is to learn optimal polices using historical (offline) data, without access to the environment for online exploration. One of the main challenges in offline RL is the distribution shift which refers to the difference between the state-action visitation distribution of the data generating policy and the learning policy. Many recent works have used the idea of pessimism for developing offline RL algorithms and characterizing their sample complexity under a relatively weak assumption of single policy concentrability. Different from the offline RL literature, the area of distributionally robust learning (DRL) offers a principled framework that uses a minimax formulation to tackle model mismatch between training and testing environments. In this work, we aim to bridge these two areas by showing that the DRL approach can be used to tackle the distributional shift problem in offline RL. In particular, we propose two offline RL algor",
    "link": "http://arxiv.org/abs/2310.18434",
    "context": "Title: Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage. (arXiv:2310.18434v1 [cs.LG])\nAbstract: The goal of an offline reinforcement learning (RL) algorithm is to learn optimal polices using historical (offline) data, without access to the environment for online exploration. One of the main challenges in offline RL is the distribution shift which refers to the difference between the state-action visitation distribution of the data generating policy and the learning policy. Many recent works have used the idea of pessimism for developing offline RL algorithms and characterizing their sample complexity under a relatively weak assumption of single policy concentrability. Different from the offline RL literature, the area of distributionally robust learning (DRL) offers a principled framework that uses a minimax formulation to tackle model mismatch between training and testing environments. In this work, we aim to bridge these two areas by showing that the DRL approach can be used to tackle the distributional shift problem in offline RL. In particular, we propose two offline RL algor",
    "path": "papers/23/10/2310.18434.json",
    "total_tokens": 991,
    "translated_title": "分布鲁棒学习和离线强化学习的桥梁：缓解分布偏移和部分数据覆盖的方法",
    "translated_abstract": "离线强化学习算法的目标是使用历史（离线）数据学习最优策略，而无需访问环境进行在线探索。离线强化学习的主要挑战之一是分布偏移，即数据生成策略的状态-动作访问分布与学习策略的差异。许多最新的研究利用悲观主义的思想开发离线强化学习算法，并在相对较弱的单一策略集中性假设下表征其样本复杂性。与离线强化学习文献不同，分布鲁棒学习（DRL）的领域提供了一个原则性框架，采用极小极大形式来解决训练和测试环境之间的模型不匹配问题。在这项工作中，我们旨在通过展示DRL方法可以用来解决离线强化学习中的分布偏移问题。特别地，我们提出了两种离线强化学习算法",
    "tldr": "本论文介绍了一种将分布鲁棒学习（DRL）与离线强化学习（RL）相结合的方法，用于解决离线强化学习中的分布偏移问题。通过使用DRL方法，可以有效地缓解训练和测试环境之间的模型不匹配，并提出了两种离线强化学习算法。",
    "en_tdlr": "This paper presents an approach that combines distributionally robust learning (DRL) with offline reinforcement learning (RL) to address the distribution shift problem in offline RL. By using DRL, it effectively mitigates the model mismatch between training and testing environments, and proposes two offline RL algorithms."
}