{
    "title": "Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])",
    "abstract": "Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that",
    "link": "http://arxiv.org/abs/2310.02699",
    "context": "Title: Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])\nAbstract: Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that",
    "path": "papers/23/10/2310.02699.json",
    "total_tokens": 854,
    "translated_title": "持续对比式语音理解",
    "translated_abstract": "最近，神经网络在各个领域取得了令人印象深刻的进展，其中包括语音处理。然而，这个领域的最新突破通常需要使用大规模数据集和庞大的计算资源进行离线训练。不幸的是，这些模型在持续学习新任务时往往难以保持之前获得的知识，并且重新训练几乎总是不可行的。在本文中，我们研究了一种在类别增量学习（CIL）设置下学习序列到序列模型用于语音理解的问题，并提出了一种称为COCONUT的CIL方法，该方法依赖于经验重播和对比式学习的组合。通过对仅对回放样本应用改进版本的标准有监督对比损失，COCONUT通过将同一类别的样本拉近并将其他样本推开，保留了学习到的表示。此外，我们利用了一种多模态对比损失。",
    "tldr": "COCONUT是一种类别增量学习方法，结合了经验重播和对比式学习，在语音理解领域中解决了模型在持续学习新任务时难以保持之前知识的问题。",
    "en_tdlr": "COCONUT is a class-incremental learning method that combines experience replay and contrastive learning, addressing the issue of models struggling to retain previously acquired knowledge when learning new tasks in spoken language understanding."
}