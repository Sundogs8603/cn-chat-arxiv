{
    "title": "PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])",
    "abstract": "Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.",
    "link": "http://arxiv.org/abs/2310.07170",
    "context": "Title: PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model. (arXiv:2310.07170v1 [cs.CL])\nAbstract: Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.",
    "path": "papers/23/10/2310.07170.json",
    "total_tokens": 863,
    "translated_title": "PHALM:通过引导人类和语言模型从零开始构建知识图谱",
    "translated_abstract": "尽管预训练的Transformer在自然语言理解方面取得了显著进展，但神经语言模型往往无法良好处理常识知识。为了创造常识感知的模型，已经尝试了从自动获取到众包获取知识的方法。然而，从零开始以较低成本获得高质量的知识库是困难的。在本文中，我们提出了PHALM，一种通过引导众包工作者和大型语言模型(LLM)来从零开始构建知识图谱的方法。我们使用这种方法构建了一个日语事件知识图谱，并训练了日语常识生成模型。实验结果显示了构建的图谱和训练模型生成的推理的可接受性。我们还报告了引导人类和LLM的差异。我们的代码、数据和模型可以在github.com/nlp-waseda/comet-atomic-ja上获得。",
    "tldr": "本文提出了一种从零开始构建知识图谱的方法，通过引导众包工作者和大型语言模型，该方法在构建日语事件知识图谱和训练常识生成模型方面取得了良好的结果。",
    "en_tdlr": "This paper proposes a method to build a knowledge graph from scratch by prompting crowdworkers and a large language model, achieving good results in constructing a Japanese event knowledge graph and training commonsense generation models."
}