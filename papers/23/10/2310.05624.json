{
    "title": "Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)",
    "abstract": "Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token ag",
    "link": "http://arxiv.org/abs/2310.05624",
    "context": "Title: Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)\nAbstract: Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token ag",
    "path": "papers/23/10/2310.05624.json",
    "total_tokens": 840,
    "translated_title": "可适应本地性感知的泛化隐式神经表示",
    "translated_abstract": "泛化的隐式神经表示（INR）通过使用潜在编码来调节其权重或中间特征，使单个连续函数（即基于坐标的神经网络）能够表示多个数据实例。然而，最先进的调制方法的表达能力有限，因为它无法定位和捕获数据实体（如特定像素和光线）的细粒度细节。为了解决这个问题，我们提出了一个新的框架，将Transformer编码器与具有本地感知INR解码器相结合。Transformer编码器从数据实例中预测一组潜在令牌，将本地信息编码到每个潜在令牌中。具有本地感知的INR解码器通过交叉注意力有选择地聚合潜在令牌以获取坐标输入的调制向量，随后通过多个频带逐步解码以实现粗细调制。",
    "tldr": "本文提出了一种结合Transformer编码器和具有本地感知的INR解码器的框架，用于解决泛化的隐式神经表示中无法定位和捕获细粒度细节的问题。",
    "en_tdlr": "This paper proposes a framework that combines a transformer encoder with a locality-aware INR decoder to address the issue of the inability to localize and capture fine-grained details in generalizable implicit neural representations."
}