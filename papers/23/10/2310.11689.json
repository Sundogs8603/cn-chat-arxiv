{
    "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO",
    "link": "http://arxiv.org/abs/2310.11689",
    "context": "Title: Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])\nAbstract: Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO",
    "path": "papers/23/10/2310.11689.json",
    "total_tokens": 905,
    "translated_title": "自我评估的自适应改进LLMs中的选择性预测",
    "translated_abstract": "大型语言模型（LLMs）在自然语言理解和生成等多种任务中取得了巨大进展。然而，在高风险决策场景中仍然限于其潜在的错误。选择性预测是一种可以通过在LLMs不确定时使其避免预测而提高其可靠性的技术。在本文中，我们提出了一种新颖的自我评估的自适应框架，以提高LLMs的选择性预测性能。我们的框架基于使用参数效率调整来适应特定任务并改进其自我评估能力的思想。我们在各种问答（QA）数据集上评估了我们的方法，并展示其优于最先进的选择性预测方法。例如，在CoQA基准测试中，我们的方法将AUACC从91.23%提高到92.63%，并将AURO",
    "tldr": "本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。"
}