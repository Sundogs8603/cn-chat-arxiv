{
    "title": "Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization. (arXiv:2310.06159v1 [cs.LG])",
    "abstract": "Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal c",
    "link": "http://arxiv.org/abs/2310.06159",
    "context": "Title: Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization. (arXiv:2310.06159v1 [cs.LG])\nAbstract: Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal c",
    "path": "papers/23/10/2310.06159.json",
    "total_tokens": 927,
    "translated_title": "经过缩放梯度下降法的，甚至过参数化的可证明加速的病态低秩估计",
    "translated_abstract": "科学和工程中遇到的许多问题可以归纳为从不完整且可能损坏的线性测量中估计低秩对象（例如矩阵和张量）。通过矩阵和张量分解的视角，其中一种最流行的方法是使用简单的迭代算法，如梯度下降（GD）直接恢复低秩因子，这样可以实现小内存和计算开销。然而，GD的收敛速率线性地依赖于低秩对象的条件数，有时甚至是二次的，因此当问题病态时，GD的收敛非常缓慢。本章介绍了一种新的算法方法，称为缩放梯度下降法（ScaledGD），它能够以恒定速率线性地收敛，而不依赖于低秩对象的条件数，同时保持了梯度下降在各种任务中的每次迭代成本较低，包括感知、鲁棒主成分估计等任务。",
    "tldr": "本章提出了一种名为缩放梯度下降（ScaledGD）的新算法，能够在恒定速率下收敛，而不受低秩对象条件数的影响，并且在各种任务中具有低迭代成本。"
}