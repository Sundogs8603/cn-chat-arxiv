{
    "title": "On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics",
    "abstract": "arXiv:2310.19527v2 Announce Type: replace  Abstract: Risk-aware Reinforcement Learning (RL) algorithms like SAC and TD3 were shown empirically to outperform their risk-neutral counterparts in a variety of continuous-action tasks. However, the theoretical basis for the pessimistic objectives these algorithms employ remains unestablished, raising questions about the specific class of policies they are implementing. In this work, we apply the expected utility hypothesis, a fundamental concept in economics, to illustrate that both risk-neutral and risk-aware RL goals can be interpreted through expected utility maximization using an exponential utility function. This approach reveals that risk-aware policies effectively maximize value certainty equivalent, aligning them with conventional decision theory principles. Furthermore, we propose Dual Actor-Critic (DAC). DAC is a risk-aware, model-free algorithm that features two distinct actor networks: a pessimistic actor for temporal-difference ",
    "link": "https://arxiv.org/abs/2310.19527",
    "context": "Title: On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics\nAbstract: arXiv:2310.19527v2 Announce Type: replace  Abstract: Risk-aware Reinforcement Learning (RL) algorithms like SAC and TD3 were shown empirically to outperform their risk-neutral counterparts in a variety of continuous-action tasks. However, the theoretical basis for the pessimistic objectives these algorithms employ remains unestablished, raising questions about the specific class of policies they are implementing. In this work, we apply the expected utility hypothesis, a fundamental concept in economics, to illustrate that both risk-neutral and risk-aware RL goals can be interpreted through expected utility maximization using an exponential utility function. This approach reveals that risk-aware policies effectively maximize value certainty equivalent, aligning them with conventional decision theory principles. Furthermore, we propose Dual Actor-Critic (DAC). DAC is a risk-aware, model-free algorithm that features two distinct actor networks: a pessimistic actor for temporal-difference ",
    "path": "papers/23/10/2310.19527.json",
    "total_tokens": 936,
    "translated_title": "关于风险感知代理理论：桥接演员-评论家和经济学",
    "translated_abstract": "arXiv:2310.19527v2 公告类型：替换 摘要：风险感知强化学习（RL）算法如SAC和TD3在各种连续动作任务中的实证表现优于其风险中性对应物。然而，这些算法采用的悲观目标的理论基础尚未建立，这引发了关于它们实施的具体政策类别的问题。 在本研究中，我们应用了期望效用假设，这是经济学中的一个基本概念，以阐明风险中性和风险感知RL目标可以通过使用指数效用函数的期望效用最大化来解释。 这种方法揭示了风险感知政策有效地最大化了价值确定性等价物，使其与传统决策理论原则保持一致。此外，我们提出了双演员-评论家（Dual Actor-Critic，DAC）。 DAC是一种风险感知的无模型算法，具有两个不同的演员网络：一个用于时序差分的悲观演员。",
    "tldr": "通过应用期望效用假设，本文揭示了风险中性和风险感知RL目标实际上可以通过使用指数效用函数的期望效用最大化来解释，提出了双演员-评论家（DAC）算法，为风险感知的RL算法贡献了框架。"
}