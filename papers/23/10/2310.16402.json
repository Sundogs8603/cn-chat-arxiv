{
    "title": "Video Referring Expression Comprehension via Transformer with Content-conditioned Query. (arXiv:2310.16402v1 [cs.CV])",
    "abstract": "Video Referring Expression Comprehension (REC) aims to localize a target object in videos based on the queried natural language. Recent improvements in video REC have been made using Transformer-based methods with learnable queries. However, we contend that this naive query design is not ideal given the open-world nature of video REC brought by text supervision. With numerous potential semantic categories, relying on only a few slow-updated queries is insufficient to characterize them. Our solution to this problem is to create dynamic queries that are conditioned on both the input video and language to model the diverse objects referred to. Specifically, we place a fixed number of learnable bounding boxes throughout the frame and use corresponding region features to provide prior information. Also, we noticed that current query features overlook the importance of cross-modal alignment. To address this, we align specific phrases in the sentence with semantically relevant visual areas, a",
    "link": "http://arxiv.org/abs/2310.16402",
    "context": "Title: Video Referring Expression Comprehension via Transformer with Content-conditioned Query. (arXiv:2310.16402v1 [cs.CV])\nAbstract: Video Referring Expression Comprehension (REC) aims to localize a target object in videos based on the queried natural language. Recent improvements in video REC have been made using Transformer-based methods with learnable queries. However, we contend that this naive query design is not ideal given the open-world nature of video REC brought by text supervision. With numerous potential semantic categories, relying on only a few slow-updated queries is insufficient to characterize them. Our solution to this problem is to create dynamic queries that are conditioned on both the input video and language to model the diverse objects referred to. Specifically, we place a fixed number of learnable bounding boxes throughout the frame and use corresponding region features to provide prior information. Also, we noticed that current query features overlook the importance of cross-modal alignment. To address this, we align specific phrases in the sentence with semantically relevant visual areas, a",
    "path": "papers/23/10/2310.16402.json",
    "total_tokens": 912,
    "translated_title": "基于内容条件查询的Transformer视频指代表达理解",
    "translated_abstract": "视频指代表达理解（REC）旨在根据查询的自然语言在视频中定位目标对象。最近，使用具有可学习查询的Transformer方法在视频REC方面取得了进展。然而，我们认为，考虑到文本监督带来的视频REC的开放世界性质，这种简单的查询设计并不理想。由于存在众多潜在的语义类别，仅依靠一些更新缓慢的查询无法充分描述它们。我们解决这个问题的方法是创建动态查询，这些查询受输入视频和语言的影响，以建模所指的多样化对象。具体而言，我们在帧中放置了固定数量的可学习边界框，并使用相应的区域特征提供先验信息。此外，我们注意到当前的查询特征忽视了跨模态对齐的重要性。为了解决这个问题，我们将句子中的特定短语与语义相关的视觉区域进行对齐。",
    "tldr": "本论文提出了一种基于内容条件查询的Transformer方法，用于视频指代表达理解。通过创建动态查询，考虑输入视频和语言的影响，来建模多样化的指代对象。同时，对句子中的特定短语进行对齐，以解决当前查询特征忽视跨模态对齐的问题。",
    "en_tdlr": "This paper proposes a Transformer-based method with content-conditioned queries for video referring expression comprehension. By creating dynamic queries that consider the input video and language, it models diverse referred objects. It also addresses the issue of current query features overlooking cross-modal alignment by aligning specific phrases in the sentence."
}