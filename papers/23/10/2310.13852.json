{
    "title": "Gradual Domain Adaptation: Theory and Algorithms. (arXiv:2310.13852v1 [cs.LG])",
    "abstract": "Unsupervised domain adaptation (UDA) adapts a model from a labeled source domain to an unlabeled target domain in a one-off way. Though widely applied, UDA faces a great challenge whenever the distribution shift between the source and the target is large. Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains to gradually adapt from the source to the target domain. In this work, we first theoretically analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads to an interesting insight: to minimize the generalization error on the target domain, the sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic between the source and target domains. The insight is particularly useful under the situation where intermediate domains are missing or scarce, which is often the case in real-world applications. Based",
    "link": "http://arxiv.org/abs/2310.13852",
    "context": "Title: Gradual Domain Adaptation: Theory and Algorithms. (arXiv:2310.13852v1 [cs.LG])\nAbstract: Unsupervised domain adaptation (UDA) adapts a model from a labeled source domain to an unlabeled target domain in a one-off way. Though widely applied, UDA faces a great challenge whenever the distribution shift between the source and the target is large. Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains to gradually adapt from the source to the target domain. In this work, we first theoretically analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads to an interesting insight: to minimize the generalization error on the target domain, the sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic between the source and target domains. The insight is particularly useful under the situation where intermediate domains are missing or scarce, which is often the case in real-world applications. Based",
    "path": "papers/23/10/2310.13852.json",
    "total_tokens": 885,
    "translated_title": "渐进域自适应：理论与算法",
    "translated_abstract": "无监督域自适应（UDA）是将模型从有标记的源域适应到无标记的目标域的一种一次性方法。尽管被广泛应用，但当源域和目标域之间的分布偏移较大时，UDA面临巨大挑战。渐进域自适应（GDA）通过使用中间域逐渐从源域适应到目标域来缓解这个限制。在这项工作中，我们首先从理论上分析了一种常见的GDA算法——渐进自训练，并提供了与Kumar等人（2020）相比显著改进的泛化界限。我们的理论分析得出一个有趣的观点：为了最小化目标域上的泛化误差，中间域的顺序应该均匀地放置在源域和目标域之间的Wasserstein测地线上。这个观点在中间域缺失或稀缺的情况下尤其有用，而这在现实世界的应用中经常出现。",
    "tldr": "本文研究了渐进域自适应中的渐进自训练算法，提出了一个改进的泛化界限，并指出了中间域在源域和目标域之间均匀放置的重要性。"
}