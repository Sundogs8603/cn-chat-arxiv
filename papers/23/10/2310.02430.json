{
    "title": "Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks. (arXiv:2310.02430v1 [cs.NE])",
    "abstract": "Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. ",
    "link": "http://arxiv.org/abs/2310.02430",
    "context": "Title: Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks. (arXiv:2310.02430v1 [cs.NE])\nAbstract: Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. ",
    "path": "papers/23/10/2310.02430.json",
    "total_tokens": 987,
    "translated_title": "反复神经网络(RNN)机制解释的片段记忆理论",
    "translated_abstract": "了解反复神经网络(RNN)的复杂操作对于推动其能力和应用至关重要。在这项追求中，我们提出了片段记忆理论(EMT)，说明了RNN可以被概念化为最近提出的通用序列片段记忆模型的离散时间类比。为了证实EMT，我们引入了一系列新颖的算法任务，旨在探索RNN的变量绑定行为。利用EMT，我们制定了一个数学严谨的电路，用于促进这些任务中的变量绑定。我们的实证研究发现，经过训练的RNN始终会收敛到变量绑定电路，从而表明了RNN动力学的普遍性。基于这些发现，我们设计了一个算法来定义一个特权基础，揭示了在时间储存和组合变量中起重要作用的隐藏神经元，这是成功推广这些任务的关键机制。",
    "tldr": "提出了片段记忆理论(EMT)，将反复神经网络(RNN)概念化为通用序列片段记忆模型的离散时间类比，并且通过实验证实了EMT的有效性。通过引入新的算法任务，发现受训练的RNN始终会收敛到变量绑定电路，揭示了RNN动力学的普遍性，并且设计了一个算法来揭示变量的时间存储和组合中起重要作用的隐藏神经元。"
}