{
    "title": "Deceptive Fairness Attacks on Graphs via Meta Learning. (arXiv:2310.15653v1 [cs.LG])",
    "abstract": "We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust an",
    "link": "http://arxiv.org/abs/2310.15653",
    "context": "Title: Deceptive Fairness Attacks on Graphs via Meta Learning. (arXiv:2310.15653v1 [cs.LG])\nAbstract: We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust an",
    "path": "papers/23/10/2310.15653.json",
    "total_tokens": 910,
    "translated_title": "通过元学习对图进行欺骗性公平攻击",
    "translated_abstract": "我们研究了对图进行欺骗性公平攻击，以回答以下问题：如何通过污染攻击图学习模型来欺骗性地加剧偏见？我们通过一个双层优化问题回答了这个问题，并提出了一个基于元学习的框架FATE。FATE广泛适用于各种公平定义和图学习模型，以及任意选择的操作方法。我们进一步将FATE实例化为攻击图神经网络上的统计平衡和个体公平。我们在半监督节点分类任务的真实世界数据集上进行了大量实验评估。实验结果表明，FATE能够在维持下游任务的效用的同时放大图神经网络的偏见，无论是否考虑公平性。我们希望本文能够对公平图学习的对抗性鲁棒性提供一些见解，并能为设计鲁棒和公平的图学习模型提供启示。",
    "tldr": "本文研究了对图进行欺骗性公平攻击的方法，通过元学习的框架FATE，在保持下游任务效用的前提下，能够放大图神经网络的偏见，无论是否考虑公平性。该研究可以为设计鲁棒和公平的图学习模型提供启示。"
}