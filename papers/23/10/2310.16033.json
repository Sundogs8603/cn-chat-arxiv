{
    "title": "ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)",
    "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive details as well as larger components in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject related to the question, declining up to $45.91\\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. To scale up the usefulness of human cropping, we propose ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs. We construct five variant",
    "link": "http://arxiv.org/abs/2310.16033",
    "context": "Title: ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)\nAbstract: Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive details as well as larger components in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject related to the question, declining up to $45.91\\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. To scale up the usefulness of human cropping, we propose ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs. We construct five variant",
    "path": "papers/23/10/2310.16033.json",
    "total_tokens": 1001,
    "translated_title": "ViCrop: 利用多模态大型语言模型在零样本视觉问答中感知细小视觉细节",
    "translated_abstract": "多模态大型语言模型(MLLMs)在视觉问答(VQA)上取得了令人期待的零样本准确性，这是一个影响各种下游应用和领域的基本任务。鉴于这些模型的广泛使用潜力，研究它们在处理不同的图像和问题属性方面的限制非常重要。在这项工作中，我们研究了MLLMs是否能够像较大的组件一样感知图像中的细节。特别是，我们发现它们在回答视觉问题时对与问题相关的视觉主题的尺寸非常敏感，并且随着尺寸的减小，零样本准确性下降多达45.91%。此外，通过观察到人类可视剪裁可以显著减轻其对尺寸的敏感性，我们证明了这种效应是因果的。为了扩大人类可视剪裁的实用性，我们提出了ViCrop，这是一个利用自动可视剪裁来增强MLLMs零样本VQA的通用框架。",
    "tldr": "本研究探讨了多模态大型语言模型在零样本视觉问答中感知细小视觉细节的能力。实验表明，这些模型对于与问题相关的视觉主题的尺寸非常敏感，通过引入人类可视剪裁可以显著提升其准确性。",
    "en_tdlr": "This study investigates the ability of multimodal large language models (MLLMs) to perceive small visual details in zero-shot visual question answering. The experiments show that these models are sensitive to the size of the visual subject related to the question, and the introduction of human visual cropping significantly improves their accuracy."
}