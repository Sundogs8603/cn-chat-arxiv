{
    "title": "Semi-Federated Learning: Convergence Analysis and Optimization of A Hybrid Learning Framework. (arXiv:2310.02559v1 [cs.IT])",
    "abstract": "Under the organization of the base station (BS), wireless federated learning (FL) enables collaborative model training among multiple devices. However, the BS is merely responsible for aggregating local updates during the training process, which incurs a waste of the computational resource at the BS. To tackle this issue, we propose a semi-federated learning (SemiFL) paradigm to leverage the computing capabilities of both the BS and devices for a hybrid implementation of centralized learning (CL) and FL. Specifically, each device sends both local gradients and data samples to the BS for training a shared global model. To improve communication efficiency over the same time-frequency resources, we integrate over-the-air computation for aggregation and non-orthogonal multiple access for transmission by designing a novel transceiver structure. To gain deep insights, we conduct convergence analysis by deriving a closed-form optimality gap for SemiFL and extend the result to two extra cases.",
    "link": "http://arxiv.org/abs/2310.02559",
    "context": "Title: Semi-Federated Learning: Convergence Analysis and Optimization of A Hybrid Learning Framework. (arXiv:2310.02559v1 [cs.IT])\nAbstract: Under the organization of the base station (BS), wireless federated learning (FL) enables collaborative model training among multiple devices. However, the BS is merely responsible for aggregating local updates during the training process, which incurs a waste of the computational resource at the BS. To tackle this issue, we propose a semi-federated learning (SemiFL) paradigm to leverage the computing capabilities of both the BS and devices for a hybrid implementation of centralized learning (CL) and FL. Specifically, each device sends both local gradients and data samples to the BS for training a shared global model. To improve communication efficiency over the same time-frequency resources, we integrate over-the-air computation for aggregation and non-orthogonal multiple access for transmission by designing a novel transceiver structure. To gain deep insights, we conduct convergence analysis by deriving a closed-form optimality gap for SemiFL and extend the result to two extra cases.",
    "path": "papers/23/10/2310.02559.json",
    "total_tokens": 929,
    "translated_title": "半联邦学习：混合学习框架的收敛分析与优化",
    "translated_abstract": "在基站（BS）的组织下，无线联邦学习（FL）实现了多个设备之间的协同模型训练。然而，BS仅负责在训练过程中聚合本地更新，这导致了BS上计算资源的浪费。为了解决这个问题，我们提出了一种半联邦学习（SemiFL）范式，以充分利用BS和设备的计算能力，进行集中式学习（CL）和FL的混合实现。具体来说，每个设备将本地梯度和数据样本发送给BS，以训练共享的全局模型。为了提高通信效率，我们设计了一种新的收发器结构，将空中计算集成到聚合和非正交多址接入传输中。为了深入了解，我们通过推导SemiFL的闭式最优性差距进行了收敛分析，并将结果扩展到了两种额外的情况。",
    "tldr": "该论文提出了一种半联邦学习（SemiFL）范式，以在基站和设备之间进行集中式学习（CL）和无线联邦学习（FL）的混合实现。通过集成空中计算和非正交多址接入传输，提高了通信效率，并进行了收敛分析和优化。",
    "en_tdlr": "This paper proposes a semi-federated learning (SemiFL) paradigm that combines centralized learning (CL) and wireless federated learning (FL) to leverage the computing capabilities of both the base station and devices. By integrating over-the-air computation and non-orthogonal multiple access, communication efficiency is improved. The paper also includes convergence analysis and optimization."
}