{
    "title": "An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])",
    "abstract": "Textural Inversion, a prompt learning method, learns a singular embedding for a new \"word\" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new \"words\" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new \"words\" with known words. We evaluate via image generation",
    "link": "http://arxiv.org/abs/2310.12274",
    "context": "Title: An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])\nAbstract: Textural Inversion, a prompt learning method, learns a singular embedding for a new \"word\" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new \"words\" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new \"words\" with known words. We evaluate via image generation",
    "path": "papers/23/10/2310.12274.json",
    "total_tokens": 1039,
    "translated_title": "一图抵千言：使用多概念提示学习来学习对象级概念",
    "translated_abstract": "文字反转是一种提示学习方法，它学习一种新的“单词”的嵌入表示图像风格和外观，使其能够整合到自然语言句子中生成新的合成图像。然而，即使对于可获得个别概念的嵌入，识别和整合一个场景中的多个对象级概念仍然面临着显著的挑战，这也得到了我们的实证测试的进一步证实。为了解决这个挑战，我们引入了一个多概念提示学习（MCPL）的框架，可以从一个句子-图像对中同时学习多个新的“词”。为了增强词概念相关性的准确性，我们提出了三种正则化技术：注意力掩码（AttnMask）将学习集中在相关区域；提示对比损失（PromptCL）将不同概念的嵌入分离开来；以及绑定形容词（Bind adj.）将新的“词”与已知词相关联。我们通过图像生成进行评估",
    "tldr": "提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。",
    "en_tdlr": "A framework called Multi-Concept Prompt Learning (MCPL) is proposed to address the challenge of identifying and integrating multiple object-level concepts within a single scene by simultaneously learning multiple new \"words\". To enhance the accuracy of word-concept correlation, three regularization techniques, including Attention Masking, Prompts Contrastive Loss, and Bind adjective, are introduced. The framework is evaluated through image generation, demonstrating its ability to generate diverse and synthesized images."
}