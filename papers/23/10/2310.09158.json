{
    "title": "Learning To Teach Large Language Models Logical Reasoning. (arXiv:2310.09158v1 [cs.AI])",
    "abstract": "Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across",
    "link": "http://arxiv.org/abs/2310.09158",
    "context": "Title: Learning To Teach Large Language Models Logical Reasoning. (arXiv:2310.09158v1 [cs.AI])\nAbstract: Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across",
    "path": "papers/23/10/2310.09158.json",
    "total_tokens": 887,
    "translated_title": "学习如何教大型语言模型逻辑推理",
    "translated_abstract": "大型语言模型（LLMs）因其在语言生成和强大的泛化能力方面的出色表现而受到学术界和工业界的广泛关注。然而，由于其固有问题（如幻觉），目前的LLMs在实际推理任务中仍然输出不可靠的内容。为了更好地解决这个问题，在本文中，我们进行了深入研究，系统地探索了LLMs在逻辑推理中的能力。更具体地说，我们首先研究了LLMs在不同任务（包括事件关系提取和演绎推理）中在逻辑推理方面的不足之处。我们的研究表明，LLMs在解决需要严格推理的任务时并不是很好的推理者，会产生反事实的答案，这要求我们不断改进。因此，我们全面探索了不同的策略，赋予LLMs逻辑推理能力，从而使其能够生成更具逻辑一致性的答案。",
    "tldr": "本文通过深入调查和研究，探索了大型语言模型（LLMs）在逻辑推理中的能力，并提出了多种策略来赋予LLMs逻辑推理能力，使其能够生成更具逻辑一致性的答案。"
}