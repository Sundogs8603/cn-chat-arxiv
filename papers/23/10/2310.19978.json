{
    "title": "Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations. (arXiv:2310.19978v1 [cs.LG])",
    "abstract": "To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\\mathcal{O}( T D S + T N S)$ to $\\mathcal{O}(N S + T \\sqrt{D} \\log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\\times$, depending on the value of the privacy parameter $\\epsilon$ and the sparsity of the dataset.",
    "link": "http://arxiv.org/abs/2310.19978",
    "context": "Title: Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations. (arXiv:2310.19978v1 [cs.LG])\nAbstract: To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\\mathcal{O}( T D S + T N S)$ to $\\mathcal{O}(N S + T \\sqrt{D} \\log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\\times$, depending on the value of the privacy parameter $\\epsilon$ and the sparsity of the dataset.",
    "path": "papers/23/10/2310.19978.json",
    "total_tokens": 882,
    "translated_title": "通过更快的Frank-Wolfe迭代方法提高差分隐私LASSO正则化逻辑回归的扩展能力",
    "translated_abstract": "据我们所知，目前没有方法可以在稀疏输入数据上训练差分隐私回归模型。为了解决这个问题，我们将$ L_1 $惩罚线性回归的Frank-Wolfe算法适应于稀疏输入，并有效地利用它们。通过这样做，我们将算法的训练时间从$ \\mathcal {O}（TDS + TNS）$减少到$ \\mathcal {O}（NS + T \\sqrt {D} \\log {D} + TS ^ 2）$，其中$ T $是迭代次数，而$ N $是数据集的行数，$ D $是特征数，$ S $是稀疏率。我们的结果表明，这个过程可以将运行时间缩短多达$ 2,200 \\times $，这取决于隐私参数$ \\epsilon $的值和数据集的稀疏程度。",
    "tldr": "本论文提出了一种通过改进Frank-Wolfe算法来训练差分隐私回归模型的方法，并在稀疏输入数据上有效。通过这种方法，可以显著减少算法的训练时间，并在实验中取得了显著的性能提升。",
    "en_tdlr": "This paper proposes a method to train differentially private regression models using an improved Frank-Wolfe algorithm, which is effective on sparse input data. The approach significantly reduces training time and achieves notable performance improvements in experiments."
}