{
    "title": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity",
    "abstract": "arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars",
    "link": "https://arxiv.org/abs/2310.15929",
    "context": "Title: E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity\nAbstract: arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars",
    "path": "papers/23/10/2310.15929.json",
    "total_tokens": 828,
    "translated_title": "E-Sparse: 通过基于信息熵的 N:M 稀疏性提升大型语言模型推理能力",
    "translated_abstract": "传统的剪枝方法在大型语言模型（LLMs）中很难实现，因为它们训练过程昂贵，计算需求大。本文首次将隐藏状态特征的信息熵引入到剪枝度量设计中，即 E-Sparse，以提高LLM中 N:M 稀疏性的准确性。E-Sparse利用信息丰富性来提升通道的重要性，并进一步结合几种新颖技术来实现：(1)引入信息熵来增强参数权重和输入特征范数的重要性作为一种新颖的剪枝度量，并在不修改剩余权重的情况下执行N:M稀疏性。(2)设计全局朴素洗牌和局部块洗牌，快速优化信息分布，充分应对 N:M 稀疏性对LLMs准确性的影响。E-Sparse 被实现为一种 Spars",
    "tldr": "首次将信息熵引入剪枝度量设计，提高在大型语言模型中 N:M 稀疏性的准确性。",
    "en_tdlr": "First time introducing information entropy into pruning metric design to enhance N:M sparsity accuracy in Large Language Models."
}