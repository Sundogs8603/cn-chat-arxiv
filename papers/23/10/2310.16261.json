{
    "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining. (arXiv:2310.16261v1 [cs.CL])",
    "abstract": "We analyze the masked language modeling pretraining objective function from the perspective of the distributional hypothesis. We investigate whether better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data's distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct analyses over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.",
    "link": "http://arxiv.org/abs/2310.16261",
    "context": "Title: The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining. (arXiv:2310.16261v1 [cs.CL])\nAbstract: We analyze the masked language modeling pretraining objective function from the perspective of the distributional hypothesis. We investigate whether better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data's distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct analyses over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.",
    "path": "papers/23/10/2310.16261.json",
    "total_tokens": 891,
    "translated_title": "分布假设不能完全解释蒙面语言模型预训练的好处",
    "translated_abstract": "我们从分布假设的角度分析了蒙面语言模型预训练的目标函数。我们研究了使用蒙面语言模型预训练的模型是否能够归因于预训练数据的语义相似性所编码的分布特性而获得更好的样本效率和更好的泛化能力。通过一个合成数据集，我们的分析表明，分布特性确实导致了预训练蒙面语言模型的更好样本效率，但不能完全解释其泛化能力。我们还在两个真实数据集上进行了分析，并证明分布特性也不能解释预训练自然语言模型的泛化能力。我们的结果揭示了我们对模型预训练的理解有限，并提供了未来的研究方向。",
    "tldr": "本研究分析了蒙面语言模型预训练的目标函数，并探讨了预训练数据的分布特性对于预训练模型具有更好的样本效率的影响。结果发现，分布特性能够提高样本效率，但不能完全解释模型的泛化能力。此外，通过对真实数据的分析发现分布特性也无法解释自然语言模型的泛化能力。"
}