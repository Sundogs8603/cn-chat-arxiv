{
    "title": "Generative quantum machine learning via denoising diffusion probabilistic models",
    "abstract": "Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \\emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and ",
    "link": "https://arxiv.org/abs/2310.05866",
    "context": "Title: Generative quantum machine learning via denoising diffusion probabilistic models\nAbstract: Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \\emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and ",
    "path": "papers/23/10/2310.05866.json",
    "total_tokens": 919,
    "translated_title": "通过去噪扩散概率模型进行生成性量子机器学习",
    "translated_abstract": "深度生成模型是计算机视觉、文本生成和大型语言模型的关键技术。最近，由于其能够生成多样化和高质量的样本，以及结构灵活、训练简单的特点，去噪扩散概率模型（DDPMs）在许多计算机视觉任务中受到了广泛关注。量子生成模型利用纠缠和叠加的能力为学习经典和量子数据带来了新的见解。受经典模型的启发，我们提出了“量子去噪扩散概率模型”（QuDDPM），以实现对量子数据的高效可训练的生成学习。QuDDPM采用足够层数的电路来保证表达能力，同时引入多个中间训练任务，将目标分布与噪声之间的插值作为训练过程，以避免贫瘠平原并保证高效的训练。我们给出了学习误差的上界和...（未完待续）",
    "tldr": "通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。"
}