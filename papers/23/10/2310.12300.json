{
    "title": "Measuring Pointwise $\\mathcal{V}$-Usable Information In-Context-ly. (arXiv:2310.12300v1 [cs.CL])",
    "abstract": "In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challen",
    "link": "http://arxiv.org/abs/2310.12300",
    "context": "Title: Measuring Pointwise $\\mathcal{V}$-Usable Information In-Context-ly. (arXiv:2310.12300v1 [cs.CL])\nAbstract: In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challen",
    "path": "papers/23/10/2310.12300.json",
    "total_tokens": 909,
    "translated_title": "在上下文中测量逐点可用信息",
    "translated_abstract": "在上下文学习（ICL）是一种新的学习范式，随着大型语言模型的发展而受到青睐。本文将最近提出的难度度量指标逐点可用信息（PVI）调整为适用于上下文的版本（上下文PVI）。与原始PVI相比，上下文PVI更高效，因为它只需少量示例并且不需要微调。我们进行了全面的实证分析以评估上下文PVI的可靠性。我们的研究结果表明，上下文PVI的估计值表现出类似于原始PVI的特征。具体针对上下文环境，我们展示了上下文PVI的估计值在不同示例选取和拍摄次数下保持一致。在不同的示例选取中，上下文PVI的估计值的方差是微不足道的，这表明上下文PVI是稳定的。此外，我们演示了如何利用上下文PVI来识别挑战。",
    "tldr": "这项研究适用于上下文学习范式，通过调整逐点可用信息度量指标为适用于上下文的版本，将其命名为上下文PVI，并证明了上下文PVI的可靠性和稳定性。",
    "en_tdlr": "This research is applicable to the in-context learning paradigm, adapting the pointwise $\\mathcal{V}$-usable information metric to an in-context version called in-context PVI. The study demonstrates the reliability and stability of in-context PVI."
}