{
    "title": "Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data. (arXiv:2310.01975v1 [cs.LG])",
    "abstract": "Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high prediction accuracy. To study this \"benign overfitting\" phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optima",
    "link": "http://arxiv.org/abs/2310.01975",
    "context": "Title: Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data. (arXiv:2310.01975v1 [cs.LG])\nAbstract: Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high prediction accuracy. To study this \"benign overfitting\" phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optima",
    "path": "papers/23/10/2310.01975.json",
    "total_tokens": 991,
    "translated_title": "两层ReLU卷积神经网络在XOR数据中的良性过拟合",
    "translated_abstract": "现代深度学习模型通常是高度过度参数化的，以便能够过拟合训练数据。令人惊讶的是，这种过拟合的神经网络通常仍然能够达到很高的预测准确率。为了研究这种“良性过拟合”现象，最近一系列的工作从理论上研究了线性模型和两层神经网络的学习。然而，这些分析大多仍限于贝叶斯最优分类器为线性的非常简单的学习问题。在本研究中，我们研究了一类XOR类型的分类任务，并考虑了标签翻转的噪声。我们证明，在样本复杂度和信噪比满足一定条件的情况下，通过梯度下降训练的过度参数化的ReLU卷积神经网络可以达到近乎贝叶斯最优准确率。此外，我们还建立了一个匹配的下界结果，表明当满足前述条件不成立时，所获得卷积神经网络的预测准确率与贝叶斯最优相差一个绝对常数。",
    "tldr": "本文研究了两层ReLU卷积神经网络在XOR数据中的良性过拟合现象。实验证明，在一定条件下，通过梯度下降训练的过度参数化的ReLU卷积神经网络可以达到近乎贝叶斯最优准确率。",
    "en_tdlr": "This paper investigates the phenomenon of benign overfitting in two-layer ReLU convolutional neural networks for XOR data. The experimental results show that, under certain conditions, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy."
}