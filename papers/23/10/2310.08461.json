{
    "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
    "abstract": "arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr",
    "link": "https://arxiv.org/abs/2310.08461",
    "context": "Title: DistillSpec: Improving Speculative Decoding via Knowledge Distillation\nAbstract: arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr",
    "path": "papers/23/10/2310.08461.json",
    "total_tokens": 832,
    "translated_title": "DistillSpec：通过知识蒸馏改进投机性解码",
    "translated_abstract": "投机性解码（SD）通过使用更快的草稿模型生成多个标记，然后由更大的目标模型并行验证这些标记，从而生成符合目标模型分布的文本，加速大型语言模型推理。然而，找到与目标模型良好对齐的紧凑草稿模型具有挑战性。为了解决这个问题，我们提出了DistillSpec，它使用知识蒸馏来更好地将草稿模型与目标模型对齐，然后应用SD。DistillSpec做出了两个关键设计选择，我们通过系统研究证明这对改进草稿和目标对齐至关重要：利用来自草稿模型的on-policy数据生成，以及将发散函数定制到任务和解码策略。值得注意的是，DistillSpec在一系列标准基准测试上比标准SD获得了令人印象深刻的10-45%的加速，使用贪婪和非贪婪方法。",
    "tldr": "DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。",
    "en_tdlr": "DistillSpec improves speculative decoding using knowledge distillation, achieving 10-45% speedups on standard benchmarks."
}