{
    "title": "Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)",
    "abstract": "We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla",
    "link": "http://arxiv.org/abs/2310.12692",
    "context": "Title: Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)\nAbstract: We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla",
    "path": "papers/23/10/2310.12692.json",
    "total_tokens": 961,
    "translated_title": "通过对随机分区的视图进行一致分配的表示学习",
    "translated_abstract": "我们提出了一种自监督聚类方法，即对随机分区的视图进行一致分配（CARP），用于学习视觉特征的表示学习。CARP以端到端在线的方式使用梯度下降来学习原型，而无需额外的非可微模块来解决聚类分配问题。CARP通过基于原型的随机分区优化新的预训练任务，从而对模型进行正则化并强制视图之间的分配一致性。此外，我们的方法改善了训练的稳定性，并防止了联合嵌入训练中的崩溃解决方案。通过广泛的评估，我们证明CARP的表示适用于学习下游任务。我们在17个数据集上评估了CARP的表示能力，包括线性评估、少样本分类、k-NN、k-means、图像检索和复制检测等许多标准协议。我们将CARP的性能与11种现有的自监督方法进行了比较。",
    "tldr": "本论文提出了一种称为CARP的自监督聚类方法，通过对随机分区的视图进行一致分配，实现了可靠的表示学习，同时提高了训练稳定性和防止了崩溃解决方案的出现。在广泛的评估中，证明CARP的表示适用于多种下游任务，并与11种现有的自监督方法进行了比较。",
    "en_tdlr": "This paper presents CARP, a self-supervised clustering method that achieves reliable representation learning through consistent assignment of views over random partitions. CARP improves training stability and prevents collapsed solutions, and its representations are suitable for various downstream tasks, as demonstrated in extensive evaluations and comparisons with 11 existing self-supervised methods."
}