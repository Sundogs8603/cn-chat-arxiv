{
    "title": "Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])",
    "abstract": "The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti",
    "link": "http://arxiv.org/abs/2310.06234",
    "context": "Title: Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])\nAbstract: The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti",
    "path": "papers/23/10/2310.06234.json",
    "total_tokens": 945,
    "translated_title": "大型Vision Transformer的高效适应性通过Adapter重组",
    "translated_abstract": "高容量预训练模型的出现彻底改变了计算机视觉问题解决的方式，将焦点从训练特定任务模型转向了适应预训练模型。因此，以高效的方式适应大型预训练模型到下游任务已成为一个重要的研究领域。现有的解决方案主要集中在设计轻量级的适配器及其与预训练模型的交互，目标是最小化需要更新的参数数量。本研究提出了一种新颖的Adapter重组（ARC）策略，从一个新的角度解决了高效预训练模型适应的问题。我们的方法考虑了适应参数的可重用性，并引入了参数共享方案。具体而言，我们利用对称的向下/向上投影来构建瓶颈操作，这些操作在不同层之间共享。通过学习低维度的重新缩放系数，我们可以有效地重新组合层适应参数。",
    "tldr": "本研究提出了一种名为Adapter重组（ARC）的策略，旨在从一种新的角度解决高效预训练模型适应的问题。该方法通过考虑适应参数的可重用性和引入参数共享方案，利用对称的投影操作来构建共享的瓶颈操作，并通过学习低维度的重新缩放系数来有效重新组合层适应参数。"
}