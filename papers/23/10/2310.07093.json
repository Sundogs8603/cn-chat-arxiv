{
    "title": "Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])",
    "abstract": "To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.",
    "link": "http://arxiv.org/abs/2310.07093",
    "context": "Title: Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning. (arXiv:2310.07093v1 [cs.CL])\nAbstract: To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.",
    "path": "papers/23/10/2310.07093.json",
    "total_tokens": 935,
    "translated_title": "论辩立场预测：多模态和少样本学习的探索性研究",
    "translated_abstract": "为了推进作为多模态问题的论辩立场预测，多模态论证挖掘的首个共享任务主持了关于枪支控制和堕胎等关键社会议题的立场预测。我们的探索性研究旨在评估图像在推文中用于立场预测的必要性，并比较开箱即用的基于文本的大型语言模型（LLM）在少样本环境下与微调的单模态和多模态模型的性能。我们的工作表明，微调的文本语言模型集合（0.817 F1分数）优于多模态模型（0.677 F1分数）和基于文本的少样本预测使用最新的LLM（0.550 F1分数）。除了性能差异，我们的研究结果显示，多模态模型在将图像内容进行自然语言摘要时表现更好，而不是使用原始像素结构，并且在上下文示例下使用可以提高LLM的少样本预测性能。",
    "tldr": "该研究通过探索性研究发现，在多模态立场预测中，微调的文本语言模型集合表现更好，并且多模态模型更适合将图像内容进行自然语言摘要，使用上下文示例可以提高LLM的少样本预测性能。",
    "en_tdlr": "This study finds through exploratory research that ensemble of fine-tuned text-based language models performs better in multimodal stance prediction, and multimodal models are more suitable for summarizing image content as natural language, and the use of in-context examples improves few-shot prediction performance of LLMs."
}