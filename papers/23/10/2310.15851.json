{
    "title": "Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])",
    "abstract": "The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen",
    "link": "http://arxiv.org/abs/2310.15851",
    "context": "Title: Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])\nAbstract: The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen",
    "path": "papers/23/10/2310.15851.json",
    "total_tokens": 955,
    "translated_title": "自我防御：增强LLM的自我保护能力",
    "translated_abstract": "盗破攻击可以绕过大型语言模型（LLM）的安全措施，生成有害内容。这种滥用LLM的行为导致了负面的社会后果。目前，解决盗破攻击的主要方法有两种：安全训练和保护措施。安全训练侧重于进一步训练LLM以增强其安全性。而保护措施则是通过实施外部模型或过滤器来防止有害输出。然而，安全训练在适应新的攻击类型方面具有局限性，并且往往会导致模型性能下降。保护措施在帮助方面也被证明有限。为了解决这些问题，我们提出了一种称为自我防御的新方法，结合了安全方法的优势。自我防御包括两个阶段。在第一阶段，我们增强了模型评估有害内容的能力，在第二阶段，我们指导模型在自己的回应上始终执行有害内容检测。实验结果表明，我们的方法能够显著减少有害内容的生成，提高模型的安全性。",
    "tldr": "这篇论文提出了一种称为自我防御的新方法，通过结合安全训练和保护措施的优势，提升大型语言模型（LLM）的安全性，从而减少有害内容的生成。",
    "en_tdlr": "This paper proposes a new approach called Self-Guard, combining the strengths of safety training and safeguards, to enhance the security of Large Language Models (LLMs) and reduce the generation of harmful content."
}