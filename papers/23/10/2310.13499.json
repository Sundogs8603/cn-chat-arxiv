{
    "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings. (arXiv:2310.13499v1 [cs.CL])",
    "abstract": "This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be learned through knowledge distillation. However, the vanilla DistillCSE through the standard implementation of knowledge distillation only achieves marginal improvements due to severe overfitting. The further quantitative analyses demonstrate the reason that the standard knowledge distillation exhibits a relatively large variance of the teacher model's logits due to the essence of contrastive learning. To mitigate the issue induced by high variance, this paper accordingly proposed two simple yet effective solutions for knowledge distillation: a Group-P shuffling strategy as an implicit regularization and the averaging logits from multiple teacher components. Experiments on standard benchmarks",
    "link": "http://arxiv.org/abs/2310.13499",
    "context": "Title: DistillCSE: Distilled Contrastive Learning for Sentence Embeddings. (arXiv:2310.13499v1 [cs.CL])\nAbstract: This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be learned through knowledge distillation. However, the vanilla DistillCSE through the standard implementation of knowledge distillation only achieves marginal improvements due to severe overfitting. The further quantitative analyses demonstrate the reason that the standard knowledge distillation exhibits a relatively large variance of the teacher model's logits due to the essence of contrastive learning. To mitigate the issue induced by high variance, this paper accordingly proposed two simple yet effective solutions for knowledge distillation: a Group-P shuffling strategy as an implicit regularization and the averaging logits from multiple teacher components. Experiments on standard benchmarks",
    "path": "papers/23/10/2310.13499.json",
    "total_tokens": 885,
    "translated_title": "DistillCSE: 蒸馏对比学习用于句子嵌入",
    "translated_abstract": "本文提出了DistillCSE框架，它在自我训练范式下进行对比学习并使用知识蒸馏。DistillCSE的潜在优势是其自我增强特性：通过使用基模型提供额外的监督信号，可以通过知识蒸馏学习到更强的模型。然而，由于严重过拟合，通过标准的知识蒸馏实现的普通DistillCSE只能取得边际的改进。进一步的定量分析显示，标准知识蒸馏由于对比学习的本质，导致了教师模型的标签的相对大的方差。为了缓解高方差引起的问题，本文提出了两个简单而有效的知识蒸馏解决方案：一种Group-P随机策略作为隐式正则化和从多个教师组件平均标签的方法。在标准基准测试上进行了实验证明。",
    "tldr": "本文提出了DistillCSE框架，使用知识蒸馏和对比学习来提升句子嵌入的性能。通过引入隐式正则化和平均教师模型的标签，改进了标准知识蒸馏的问题。",
    "en_tdlr": "This paper proposes the DistillCSE framework, which improves sentence embeddings using knowledge distillation and contrastive learning. By introducing implicit regularization and averaging the labels of multiple teacher components, it addresses the issues of standard knowledge distillation."
}