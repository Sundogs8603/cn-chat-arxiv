{
    "title": "Data Pruning via Moving-one-Sample-out. (arXiv:2310.14664v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies",
    "link": "http://arxiv.org/abs/2310.14664",
    "context": "Title: Data Pruning via Moving-one-Sample-out. (arXiv:2310.14664v2 [cs.LG] UPDATED)\nAbstract: In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies",
    "path": "papers/23/10/2310.14664.json",
    "total_tokens": 872,
    "translated_title": "通过移除单个样本进行数据修剪",
    "translated_abstract": "本文提出了一种新颖的数据修剪方法称为移除单个样本(MoSo)，旨在从训练集中识别并移除最不相关的样本。MoSo的核心思想是通过评估样本对最优经验风险的影响来确定每个样本的重要性。这通过衡量从训练集中排除一个特定样本时，经验风险的变化程度来实现。我们提出了一种高效的一阶近似器，它仅需要来自不同训练阶段的梯度信息，而不是使用计算上昂贵的逐个样本重新训练的过程。我们近似的关键思想是，梯度与训练集的平均梯度一致的样本更具信息量，并且应该获得更高的分数，可以直观地理解为：如果来自特定样本的梯度与平均梯度向量一致，则意味着",
    "tldr": "本文提出了一种新颖的数据修剪方法MoSo，它通过评估样本对最优经验风险的影响来确定每个样本的重要性，并提出了一种高效的一阶近似器来计算样本的重要性，该近似器只需要梯度信息。"
}