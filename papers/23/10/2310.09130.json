{
    "title": "Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])",
    "abstract": "Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a",
    "link": "http://arxiv.org/abs/2310.09130",
    "context": "Title: Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])\nAbstract: Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a",
    "path": "papers/23/10/2310.09130.json",
    "total_tokens": 929,
    "translated_title": "使用本地差分隐私保护大型语言模型推断：拆分与去噪",
    "translated_abstract": "大型语言模型（LLMs）通过捕捉向量空间中的隐藏语义，展示了在自然语言理解方面的强大能力。这一过程丰富了文本嵌入的价值，从而促进了作为服务（EaaS）的嵌入模型商业模式。然而，直接将文本传输到服务器面临着较大的隐私泄露风险，这是一个尚未得到有效解决的问题。为了缓解这个问题，我们引入了Split-N-Denoise（SnD），一种创新的框架，通过在客户端上以最小的计算成本执行令牌嵌入层来拆分模型。这使得客户端能够在将嵌入传输到服务器之前引入噪声，并随后接收和去噪后的扰动输出嵌入用于下游任务。我们的方法专为LLMs的推断阶段设计，不需要修改模型参数。广泛的实验证明了SnD在各种LLM中优化隐私-效用权衡方面的有效性。",
    "tldr": "本文提出了一种名为SnD的创新框架，用于保护大型语言模型推断阶段的隐私。该方法通过在客户端上执行令牌嵌入层和引入噪声来优化隐私-效用权衡，无需修改模型参数。",
    "en_tdlr": "This paper introduces an innovative framework called SnD to protect the privacy during the inference stage of large language models. The approach optimizes the privacy-utility tradeoff by executing the token embedding layer on the client side and introducing noise, without modifying the model parameters."
}