{
    "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens. (arXiv:2310.08152v1 [cs.CL])",
    "abstract": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
    "link": "http://arxiv.org/abs/2310.08152",
    "context": "Title: Context Compression for Auto-regressive Transformers with Sentinel Tokens. (arXiv:2310.08152v1 [cs.CL])\nAbstract: The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
    "path": "papers/23/10/2310.08152.json",
    "total_tokens": 915,
    "translated_title": "带有标记符号的自回归Transformer的上下文压缩",
    "translated_abstract": "注意力模块的二次复杂性使其在生成过程中逐渐成为基于Transformer的LLM的主要计算部分。此外，处理长输入时产生的过多的键值缓存也会在内存占用和推理延迟方面带来严重问题。在这项工作中，我们提出了一种即插即用的方法，能够将指定范围内的中间激活逐步压缩为紧凑形式，从而在处理后续上下文时减少内存和计算成本。在领域内语言建模和零样本开放文档生成的实验中，我们的方法在流畅度、N-gram匹配和语义相似性方面优于稀疏注意力基线。最后，我们全面评估了上下文压缩对系统改进的益处。代码可在https://github.com/DRSY/KV_Compression获得。",
    "tldr": "本论文提出了一种带有标记符号的自回归Transformer的上下文压缩方法，该方法通过将指定范围内的中间激活逐步压缩为紧凑形式，从而减少内存和计算成本。实验证明，在语言建模和文档生成方面，该方法相比稀疏注意力基线具有更好的流畅度、N-gram匹配和语义相似性。",
    "en_tdlr": "This paper proposes a context compression approach for auto-regressive Transformers with sentinel tokens, which compresses the intermediate activation of a specified span of tokens into compact ones, reducing memory and computational cost. Experiments show that it outperforms sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity in language modeling and document generation tasks."
}