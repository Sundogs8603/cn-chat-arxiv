{
    "title": "Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation",
    "abstract": "arXiv:2310.00796v2 Announce Type: replace  Abstract: Strong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be efficiently injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. Our analysis shows t",
    "link": "https://arxiv.org/abs/2310.00796",
    "context": "Title: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation\nAbstract: arXiv:2310.00796v2 Announce Type: replace  Abstract: Strong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be efficiently injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. Our analysis shows t",
    "path": "papers/23/10/2310.00796.json",
    "total_tokens": 819,
    "translated_title": "通过模拟将结构归纳偏差注入Seq2Seq模型",
    "translated_abstract": "强烈的归纳偏差有助于从少量数据中学习，并帮助在训练分布之外进行泛化。流行的神经架构如Transformers本身缺乏seq2seq NLP任务的强结构归纳偏差。因此，即使在大量文本上进行了预训练，它们也在系统泛化方面遇到困难，例如在外推到更长的输入时。我们展示了如何通过预训练来有效地将结构归纳偏差注入seq2seq模型，以在合成数据上模拟结构转换。具体地，我们通过预训练模拟FST描述来将结构归纳偏差注入到Transformer中。我们的实验表明，我们的方法给予了所需的归纳偏差，从而提高了系统泛化能力和FST类似任务的少样本学习。我们的分析显示",
    "tldr": "通过模拟结构转换在Seq2Seq模型中注入结构归纳偏差，提高了系统泛化和FST类似任务的少样本学习。",
    "en_tdlr": "Injecting structural inductive bias into a Seq2Seq model through simulating structural transformations improves systematic generalization and few-shot learning for FST-like tasks."
}