{
    "title": "Online Sensitivity Optimization in Differentially Private Learning. (arXiv:2310.00829v2 [cs.LG] UPDATED)",
    "abstract": "Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former wi",
    "link": "http://arxiv.org/abs/2310.00829",
    "context": "Title: Online Sensitivity Optimization in Differentially Private Learning. (arXiv:2310.00829v2 [cs.LG] UPDATED)\nAbstract: Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former wi",
    "path": "papers/23/10/2310.00829.json",
    "total_tokens": 901,
    "translated_title": "在差分隐私学习中的在线敏感度优化",
    "translated_abstract": "训练差分隐私机器学习模型需要限制个体对优化过程的贡献。为了实现这一目标，在平均和批次消毒之前，将个体的梯度的2-范数剪切到预定的阈值。这种选择在两个相对的方面对优化产生不利影响：在较低值时过度剪切加剧了偏差，而在较高值时增加了消毒噪声。这个选择在很大程度上取决于数据集、模型架构，甚至在同一个优化过程中也有所不同，需要通过网格搜索来进行细致调整。为了避免在超参数调优中产生的隐私开销，我们提出了一种新的方法来动态优化剪切阈值。我们将这个阈值视为额外的可学习参数，建立了阈值和成本函数之间的清晰关系。这使我们能够优化前者，自动获得一个调整好的剪切阈值，从而在差分隐私学习中降低了调优的隐私开销。",
    "tldr": "本文提出了一种在线敏感度优化方法，通过建立剪切阈值和成本函数之间的关系来避免在差分隐私学习中产生的隐私开销，并实现了动态调整剪切阈值的功能。"
}