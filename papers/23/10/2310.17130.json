{
    "title": "M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])",
    "abstract": "Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensi",
    "link": "http://arxiv.org/abs/2310.17130",
    "context": "Title: M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])\nAbstract: Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensi",
    "path": "papers/23/10/2310.17130.json",
    "total_tokens": 1008,
    "translated_title": "M2C：自动多模态漫画补充的研究",
    "translated_abstract": "多模态漫画分析旨在通过视觉和文本特征增强对漫画的理解，受到自然语言处理和计算机视觉社区的广泛关注。当前，大部分漫画都是手绘的，容易遇到问题，如缺页、文本污染和老化，从而导致遗漏了漫画文字内容，严重阻碍了人类的理解。换句话说，多模态漫画补充（M2C）任务尚未得到研究，该任务旨在通过为视觉和语言理解提供共享语义空间来处理上述问题。为此，我们首先通过建立一个涵盖两种语言的新M2C基准数据集，提出了多模态漫画补充任务。首先，我们设计了一种名为MCoT的漫画争议方法，通过使用大型语言模型在漫画中挖掘事件知识。然后，提出了一种有效的基准方法FVP-M$^{2}$，使用精细的视觉提示来支持漫画补充。",
    "tldr": "该论文提出了一个新的研究任务——多模态漫画补充（M2C），旨在通过共享语义空间来解决手绘漫画中遗漏文字内容的问题。研究首先建立了一个涵盖两种语言的M2C基准数据集，并设计了一种名为MCoT的漫画争议方法和一种基于精细视觉提示的补充模型FVP-M$^{2}$。",
    "en_tdlr": "This paper introduces a new research task - Multimodal Manga Complement (M2C), which aims to address the issue of missing text content in hand-drawn manga by providing a shared semantic space. The study first establishes an M2C benchmark dataset covering two languages, and proposes a manga argumentation method called MCoT and an effective complement model FVP-M$^{2}$ using fine-grained visual prompts."
}