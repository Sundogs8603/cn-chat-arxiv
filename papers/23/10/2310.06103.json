{
    "title": "Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding. (arXiv:2310.06103v1 [cs.CL])",
    "abstract": "A number of methods have been proposed for End-to-End Spoken Language Understanding (E2E-SLU) using pretrained models, however their evaluation often lacks multilingual setup and tasks that require prediction of lexical fillers, such as slot filling. In this work, we propose a unified method that integrates multilingual pretrained speech and text models and performs E2E-SLU on six datasets in four languages in a generative manner, including the prediction of lexical fillers. We investigate how the proposed method can be improved by pretraining on widely available speech recognition data using several training objectives. Pretraining on 7000 hours of multilingual data allows us to outperform the state-of-the-art ultimately on two SLU datasets and partly on two more SLU datasets. Finally, we examine the cross-lingual capabilities of the proposed model and improve on the best known result on the PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate of 23.65%.",
    "link": "http://arxiv.org/abs/2310.06103",
    "context": "Title: Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding. (arXiv:2310.06103v1 [cs.CL])\nAbstract: A number of methods have been proposed for End-to-End Spoken Language Understanding (E2E-SLU) using pretrained models, however their evaluation often lacks multilingual setup and tasks that require prediction of lexical fillers, such as slot filling. In this work, we propose a unified method that integrates multilingual pretrained speech and text models and performs E2E-SLU on six datasets in four languages in a generative manner, including the prediction of lexical fillers. We investigate how the proposed method can be improved by pretraining on widely available speech recognition data using several training objectives. Pretraining on 7000 hours of multilingual data allows us to outperform the state-of-the-art ultimately on two SLU datasets and partly on two more SLU datasets. Finally, we examine the cross-lingual capabilities of the proposed model and improve on the best known result on the PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate of 23.65%.",
    "path": "papers/23/10/2310.06103.json",
    "total_tokens": 957,
    "translated_title": "利用多语言自监督预训练模型实现序列到序列的端到端口语理解",
    "translated_abstract": "已经提出了许多使用预训练模型的端到端口语理解（E2E-SLU）方法，但它们的评估通常缺乏多语言设置和需要预测词汇填充器（例如槽填充）等任务。在这项工作中，我们提出了一种统一的方法，集成了多语言预训练语音和文本模型，并以生成方式在四种语言的六个数据集上进行E2E-SLU，包括对词汇填充器的预测。我们研究了如何通过在广泛可用的语音识别数据上进行预训练来改进所提出的方法，使用了几种训练目标。在7000小时的多语言数据上进行预训练使我们能够在两个SLU数据集上最终超过最先进的方法，而在另外两个SLU数据集上则部分超过。最后，我们检查了所提出模型的跨语言能力，并将在PortMEDIA-Language数据集上的最佳已知结果提高了近一半，达到了23.65%的概念/值错误率。",
    "tldr": "这项工作提出了一种利用多语言自监督预训练模型的统一方法，用于多语言的序列到序列的端到端口语理解，成功地预测了词汇填充器，并在多个数据集上取得了优于其他方法的结果。",
    "en_tdlr": "This work proposes a unified method that leverages multilingual self-supervised pretrained models for sequence-to-sequence end-to-end spoken language understanding (E2E-SLU), successfully predicting lexical fillers and achieving better results than other methods on multiple datasets."
}