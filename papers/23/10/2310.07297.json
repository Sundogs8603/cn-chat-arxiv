{
    "title": "Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])",
    "abstract": "Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta",
    "link": "http://arxiv.org/abs/2310.07297",
    "context": "Title: Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])\nAbstract: Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta",
    "path": "papers/23/10/2310.07297.json",
    "total_tokens": 903,
    "translated_title": "通过扩散行为实现得分正则化策略优化",
    "translated_abstract": "最近的离线强化学习研究展示了扩散建模的巨大潜力，这充分展现了其在表达异质行为策略方面的优越性。然而，从扩散策略中采样非常缓慢，因为需要数十到数百次迭代推理步骤来进行一次动作采样。为了解决这个问题，我们提出了一种从评论家模型和预训练的扩散行为模型中提取高效确定性推理策略的方法，在优化过程中利用后者直接对策略梯度进行正则化，使用行为分布的得分函数。我们的方法在训练和评估过程中充分发挥了扩散建模的强大生成能力，同时完全绕过了计算密集和耗时的扩散采样方案。在D4RL任务上的广泛结果显示，我们的方法将动作采样速度提高了超过25倍，相比于各种领先的基于扩散的方法在运动任务中。",
    "tldr": "通过利用扩散行为模型，我们提出了一种在离线强化学习中用于优化策略的得分正则化方法，从而避免了耗时且计算密集的扩散采样方案，并在D4RL任务上实现了超过25倍的动作采样速度提升。",
    "en_tdlr": "By leveraging diffusion behavior models, we propose a score regularized method for optimizing policies in offline reinforcement learning, which avoids time-consuming and computationally intensive diffusion sampling schemes and achieves more than 25 times improvement in action sampling speed on D4RL tasks."
}