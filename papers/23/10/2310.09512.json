{
    "title": "Attentive Multi-Layer Perceptron for Non-autoregressive Generation. (arXiv:2310.09512v1 [cs.CL])",
    "abstract": "Autoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \\textbf{A}ttentive \\textbf{M}ulti-\\textbf{L}ayer \\textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architectu",
    "link": "http://arxiv.org/abs/2310.09512",
    "context": "Title: Attentive Multi-Layer Perceptron for Non-autoregressive Generation. (arXiv:2310.09512v1 [cs.CL])\nAbstract: Autoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \\textbf{A}ttentive \\textbf{M}ulti-\\textbf{L}ayer \\textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architectu",
    "path": "papers/23/10/2310.09512.json",
    "total_tokens": 886,
    "translated_title": "注重多层感知机的非自回归生成",
    "translated_abstract": "自回归（AR）生成因其高效性而几乎主宰了序列生成。最近，非自回归（NAR）生成因其效率和日益增长的功效而越来越受欢迎。然而，其效率仍然被序列长度的二次复杂度限制，这对于长序列生成的扩展来说是不可接受的，而且很少有工作来缓解这个问题。在本文中，我们提出了一种新颖的MLP变体，即注重多层感知机（AMLP），以产生具有线性时间和空间复杂度的生成模型。AMLP与经典的MLP不同，它利用注意机制计算输入中的自适应投影。样本感知的自适应投影使得序列中的标记之间可以进行通信，并模拟查询和键空间之间的度量。此外，我们将AMLP与流行的NAR模型结合起来，得到了一种高效的NAR-AMLP体系结构。",
    "tldr": "本文提出了一种注重多层感知机（AMLP）的非自回归生成模型，具有线性时间和空间复杂度，并通过样本感知的自适应投影实现了序列中标记之间的通信。"
}