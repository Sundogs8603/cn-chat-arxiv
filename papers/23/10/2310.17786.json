{
    "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])",
    "abstract": "Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c",
    "link": "http://arxiv.org/abs/2310.17786",
    "context": "Title: Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])\nAbstract: Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c",
    "path": "papers/23/10/2310.17786.json",
    "total_tokens": 916,
    "translated_title": "理解何时动力学不变的数据增强对模型无关的强化学习更新有益",
    "translated_abstract": "最近，数据增强（DA）已经成为一种利用领域知识以低成本产生额外数据的强化学习（RL）任务的方法，往往能够显著提高数据效率。虽然之前的研究已经证明将增强数据直接纳入模型无关的RL更新中的效用，但目前还不太清楚特定的DA策略何时会提高数据效率。本文旨在找出DA的一般方面，以确定导致观察到的学习改进的因素。我们的研究集中在具有动力学不变的数据增强函数的稀疏奖励任务上，这是理解DA及其与RL训练整合的更一般的理解的一个初始步骤。实验上，我们分离了三个与DA相关的方面：状态-动作覆盖率，奖励密度和每次更新生成的增强转换的数量（增强回放率）。根据我们的实验，我们得出两个结论：(1) 增加状态-动作覆盖率可改进学习效果；",
    "tldr": "本文研究了在稀疏奖励任务中，动力学不变的数据增强函数对模型无关的强化学习更新的影响。实验结果表明，增加状态-动作覆盖率可以提高学习效果。",
    "en_tdlr": "This paper investigates the impact of dynamics-invariant data augmentation functions on model-free reinforcement learning updates in sparse-reward tasks. The experimental results show that increasing state-action coverage improves learning performance."
}