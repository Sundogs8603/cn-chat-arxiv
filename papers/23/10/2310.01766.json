{
    "title": "Exploring Counterfactual Alignment Loss towards Human-centered AI. (arXiv:2310.01766v1 [cs.LG])",
    "abstract": "Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human ",
    "link": "http://arxiv.org/abs/2310.01766",
    "context": "Title: Exploring Counterfactual Alignment Loss towards Human-centered AI. (arXiv:2310.01766v1 [cs.LG])\nAbstract: Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human ",
    "path": "papers/23/10/2310.01766.json",
    "total_tokens": 890,
    "translated_title": "探索针对以人为中心的人工智能的反事实对齐损失",
    "translated_abstract": "深度神经网络在监督学习任务中具有令人印象深刻的准确性。然而，它们缺乏透明度，使得人们难以信任它们的结果，特别是在安全-批评领域如医疗保健中。为了解决这个问题，最近的解释引导学习方法提出了将基于梯度的注意力映射与人类专家标注的图像区域对齐的方法，从而获得一个本质上以人为中心的模型。然而，这些方法所基于的注意力映射可能无法因果地归因于模型预测，从而损害了对其对齐的有效性。为了解决这个问题，我们提出了一个基于反事实生成的新型以人为中心的框架。具体而言，我们利用反事实生成的因果归因能力引入了一种新的损失，称为反事实对齐损失（CF-Align）。这个损失保证了分类器由反事实生成归因的特征与人类专家对齐。",
    "tldr": "该论文提出了一个基于反事实生成的以人为中心的框架，并引入了一种新的损失函数，用于保证反事实生成归因的特征与人类专家对齐。"
}