{
    "title": "Preventing Language Models From Hiding Their Reasoning. (arXiv:2310.18512v2 [cs.LG] UPDATED)",
    "abstract": "Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditio",
    "link": "http://arxiv.org/abs/2310.18512",
    "context": "Title: Preventing Language Models From Hiding Their Reasoning. (arXiv:2310.18512v2 [cs.LG] UPDATED)\nAbstract: Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditio",
    "path": "papers/23/10/2310.18512.json",
    "total_tokens": 843,
    "translated_title": "防止语言模型隐藏其推理过程",
    "translated_abstract": "大型语言模型（LLM）通常通过推理的中间步骤来生成复杂问题的答案。当这些推理的中间步骤被用来监控模型的活动时，关键是这种明确的推理是可信的，即反映出模型实际上在推理什么。本文关注一种可能导致推理的中间步骤不可信的方式：编码推理，即LLM可能以人类读者无法理解的方式将推理的中间步骤编码在生成的文本中。我们展示了语言模型可以被训练成利用编码推理以获得更高的性能，而用户并不需要理解中间推理步骤。我们认为，随着语言模型的增强，这种行为更可能自然出现。最后，我们描述了一种评估针对编码推理的防御方法的方法，并表明在合适的条件下可以实现。",
    "tldr": "本论文研究了防止语言模型隐藏推理过程的问题。我们发现语言模型可以通过编码推理来提高性能，而无需用户理解中间推理步骤。随着语言模型变得越来越强大，这种行为可能会越来越普遍。我们还提出了一种方法来评估防御编码推理的方法。"
}