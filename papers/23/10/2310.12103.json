{
    "title": "Quality Diversity through Human Feedback. (arXiv:2310.12103v1 [cs.AI])",
    "abstract": "Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with",
    "link": "http://arxiv.org/abs/2310.12103",
    "context": "Title: Quality Diversity through Human Feedback. (arXiv:2310.12103v1 [cs.AI])\nAbstract: Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with",
    "path": "papers/23/10/2310.12103.json",
    "total_tokens": 940,
    "translated_title": "通过人类反馈实现质量多样性",
    "translated_abstract": "从人类反馈中进行强化学习（RLHF）在提高定性任务的基础模型性能方面显示出潜力。尽管如此，当仅将其概念化为最大化平均人类偏好的学习奖励模型的机制时，特别是在要求多样化模型响应的图像生成等领域，其效果往往受到限制。与此同时，致力于寻找多样化的高质量解决方案的质量多样性（QD）算法通常受到对手动定义多样性指标的依赖约束。有趣的是，通过融合来自两者的见解，可以克服RLHF和QD的这些局限性。本文介绍了通过人类反馈实现质量多样性（QDHF），该方法利用人类反馈推断多样性指标，扩展了QD算法的适用性。实证结果表明，与现有的QD方法相比，QDHF在自动多样性发现方面表现出色，并且与QD的搜索能力相匹配。",
    "tldr": "本文提出了一种通过人类反馈实现质量多样性（Quality Diversity through Human Feedback，QDHF）的方法，该方法利用人类反馈推断多样性指标，扩展了质量多样性（Quality Diversity，QD）算法的适用性。实验证明，QDHF在自动多样性发现方面表现出色，并且具有与QD相匹配的搜索能力。",
    "en_tdlr": "This paper introduces Quality Diversity through Human Feedback (QDHF), a method that utilizes human feedback to infer diversity metrics, expanding the applicability of Quality Diversity (QD) algorithms. Empirical results demonstrate that QDHF excels in automatic diversity discovery and matches the search capabilities of QD."
}