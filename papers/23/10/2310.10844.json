{
    "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as",
    "link": "http://arxiv.org/abs/2310.10844",
    "context": "Title: Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as",
    "path": "papers/23/10/2310.10844.json",
    "total_tokens": 926,
    "translated_title": "对大型语言模型的恶意攻击所揭示的漏洞调查",
    "translated_abstract": "大型语言模型（LLMs）在体系结构和能力方面迅速发展，随着它们在复杂系统中的深入整合，审查其安全性变得更加紧迫。本文调查了对LLMs进行恶意攻击的新兴跨学科领域的研究，该领域是可信任的机器学习的一个子领域，结合自然语言处理和安全性的观点。先前的研究表明，即使是经过安全调整的LLMs（通过指导调整和通过人类反馈进行加强学习）也可能受到恶意攻击的影响，这些攻击利用弱点并误导人工智能系统，如ChatGPT和Bard等模型的“越狱”攻击的普遍存在证明了这一点。在本调查中，我们首先概述了大型语言模型，描述了它们的安全对齐，并根据各种学习结构对现有研究进行分类：仅文本攻击、多模态攻击以及专门针对复杂系统的其他攻击方法。",
    "tldr": "本论文调查了对大型语言模型进行恶意攻击的研究，发现即使经过安全调整的模型也容易受到攻击。这些攻击利用弱点并误导AI系统，对于复杂系统的攻击尤为明显。",
    "en_tdlr": "This paper surveys the research on adversarial attacks on large language models and finds that even safety-aligned models are vulnerable. These attacks exploit weaknesses and mislead AI systems, especially in complex systems."
}