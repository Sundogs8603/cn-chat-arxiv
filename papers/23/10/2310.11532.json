{
    "title": "Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])",
    "abstract": "In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across m",
    "link": "http://arxiv.org/abs/2310.11532",
    "context": "Title: Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])\nAbstract: In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across m",
    "path": "papers/23/10/2310.11532.json",
    "total_tokens": 844,
    "translated_title": "多阶段大型语言模型纠错用于语音识别",
    "translated_abstract": "本文研究了使用大型语言模型（LLMs）来改进竞争性语音识别系统性能的方法。与传统语言模型专注于单一数据领域不同，LLMs的崛起为我们提供了机会，既能推动最先进的ASR性能的极限，又能在多个领域中实现更高的鲁棒性和有效的泛化能力。基于此，我们提出了一种新颖的多阶段方法，将传统语言模型重新评分和LLM提示相结合。具体而言，该方法有两个阶段：第一阶段使用语言模型对ASR假设的N个最佳列表进行重新评分，并进行置信度检查；第二阶段使用提示对第一阶段置信度较低的结果进行ASR错误修正。我们的实验结果表明了该方法的有效性，相对于竞争性ASR系统，在WER上取得了10%~20%的相对改善。",
    "tldr": "本文提出了一种多阶段的方法，通过结合传统语言模型重新评分和大型语言模型提示，在语音识别中获得了显著的性能提升。",
    "en_tdlr": "This paper proposes a multi-stage approach that combines traditional language model re-scoring and large language model prompting, achieving a significant performance improvement in speech recognition."
}