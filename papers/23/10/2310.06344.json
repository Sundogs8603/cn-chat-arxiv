{
    "title": "Filter Pruning For CNN With Enhanced Linear Representation Redundancy. (arXiv:2310.06344v1 [cs.CV])",
    "abstract": "Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new s",
    "link": "http://arxiv.org/abs/2310.06344",
    "context": "Title: Filter Pruning For CNN With Enhanced Linear Representation Redundancy. (arXiv:2310.06344v1 [cs.CV])\nAbstract: Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new s",
    "path": "papers/23/10/2310.06344.json",
    "total_tokens": 945,
    "translated_title": "增强线性表示冗余的卷积神经网络滤波剪枝",
    "translated_abstract": "结构化网络剪枝优于非结构化方法，因为它们可以利用发展中的并行计算技术。在本文中，我们提出了一种新的结构化剪枝方法。首先，为了创建更多的结构化冗余，我们提出了一种从同一层的不同特征图的相关系数矩阵计算得到的数据驱动损失函数项，称为相关系数矩阵损失（CCM-loss）。这个损失项可以在训练过程中鼓励神经网络学习更强的特征图之间的线性表示关系，从而在剪枝过程中更容易去除同质化的部分。CCM-loss为我们提供了另一种通用的超越数学工具，除了集中生成零的L*-norm正则化之外，还可以为不同类型的冗余生成更多的冗余。此外，我们设计了一个基于主成分分析的匹配通道选择策略，以充分利用CCM-loss的最大潜能。",
    "tldr": "本文提出了一种增强线性表示冗余的卷积神经网络滤波剪枝方法，通过引入相关系数矩阵损失（CCM-loss）和匹配通道选择策略，可以在训练过程中加强特征图之间的线性表示关系，在剪枝过程中更好地去除同质化的部分。"
}