{
    "title": "Validating Synthetic Usage Data in Living Lab Environments. (arXiv:2310.07142v1 [cs.IR])",
    "abstract": "Evaluating retrieval performance without editorial relevance judgments is challenging, but instead, user interactions can be used as relevance signals. Living labs offer a way for small-scale platforms to validate information retrieval systems with real users. If enough user interaction data are available, click models can be parameterized from historical sessions to evaluate systems before exposing users to experimental rankings. However, interaction data are sparse in living labs, and little is studied about how click models can be validated for reliable user simulations when click data are available in moderate amounts.  This work introduces an evaluation approach for validating synthetic usage data generated by click models in data-sparse human-in-the-loop environments like living labs. We ground our methodology on the click model's estimates about a system ranking compared to a reference ranking for which the relative performance is known. Our experiments compare different click m",
    "link": "http://arxiv.org/abs/2310.07142",
    "context": "Title: Validating Synthetic Usage Data in Living Lab Environments. (arXiv:2310.07142v1 [cs.IR])\nAbstract: Evaluating retrieval performance without editorial relevance judgments is challenging, but instead, user interactions can be used as relevance signals. Living labs offer a way for small-scale platforms to validate information retrieval systems with real users. If enough user interaction data are available, click models can be parameterized from historical sessions to evaluate systems before exposing users to experimental rankings. However, interaction data are sparse in living labs, and little is studied about how click models can be validated for reliable user simulations when click data are available in moderate amounts.  This work introduces an evaluation approach for validating synthetic usage data generated by click models in data-sparse human-in-the-loop environments like living labs. We ground our methodology on the click model's estimates about a system ranking compared to a reference ranking for which the relative performance is known. Our experiments compare different click m",
    "path": "papers/23/10/2310.07142.json",
    "total_tokens": 803,
    "translated_title": "在生活实验室环境中验证合成使用数据",
    "translated_abstract": "在没有编辑相关性判断的情况下评估检索性能是具有挑战性的，但是可以使用用户交互作为相关信号。生活实验室为小规模平台提供了一种使用真实用户验证信息检索系统的方式。如果有足够多的用户交互数据，可以从历史会话中对点击模型进行参数化，以在将用户暴露于实验排名之前评估系统。然而，在生活实验室中，交互数据很稀疏，关于当点击数据数量较少时如何验证可靠的用户模拟方面的研究很少。本研究介绍了一种验证在数据稀疏的人在循环环境（如生活实验室）中由点击模型产生的合成使用数据的评估方法。我们的方法基于点击模型对系统排名与已知相对性能的参考排名之间的估计。我们的实验比较了不同的点击模型。",
    "tldr": "本研究介绍了一种在数据稀缺的人在循环环境（如生活实验室）中验证由点击模型产生的合成使用数据的评估方法。"
}