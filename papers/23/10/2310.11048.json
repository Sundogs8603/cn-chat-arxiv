{
    "title": "Understanding Contrastive Learning via Distributionally Robust Optimization. (arXiv:2310.11048v1 [cs.LG])",
    "abstract": "This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\\phi$-divergence-based generalized mutual information. We also identify CL'",
    "link": "http://arxiv.org/abs/2310.11048",
    "context": "Title: Understanding Contrastive Learning via Distributionally Robust Optimization. (arXiv:2310.11048v1 [cs.LG])\nAbstract: This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\\phi$-divergence-based generalized mutual information. We also identify CL'",
    "path": "papers/23/10/2310.11048.json",
    "total_tokens": 838,
    "translated_title": "通过分布鲁棒优化理解对比学习",
    "translated_abstract": "该研究揭示了对比学习（CL）对采样偏差的内在容忍度，其中负样本可能包含类似的语义（例如标签）。然而，现有理论在解释这一现象方面存在不足。我们通过分布鲁棒优化（DRO）的视角分析CL，得出了几个关键见解：（1）CL本质上是在负采样分布上进行DRO，从而实现对各种潜在分布的强大性能和对采样偏差的鲁棒性；（2）温度$\\tau$的设计不仅仅是一种启发式方法，而是作为一个拉格朗日系数，调节潜在分布集合的大小；（3）在DRO和互信息之间建立了一个理论连接，从而为“InfoNCE作为MI估计”的提供了新证据，以及基于$\\phi$-散度的广义互信息的新估计方法。我们还确定了CL的创新点。",
    "tldr": "通过分布鲁棒优化的视角，该研究揭示了对比学习对采样偏差的内在容忍度，并提供了几个关键见解。"
}