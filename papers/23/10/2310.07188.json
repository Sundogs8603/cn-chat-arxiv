{
    "title": "Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])",
    "abstract": "Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning ",
    "link": "http://arxiv.org/abs/2310.07188",
    "context": "Title: Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])\nAbstract: Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning ",
    "path": "papers/23/10/2310.07188.json",
    "total_tokens": 931,
    "translated_title": "自适应门控在基于混合专家语言模型中的应用",
    "translated_abstract": "大型语言模型（如OpenAI的ChatGPT）在各种NLP任务中展现出了出色的语言理解能力。稀疏激活的混合专家（MoE）已经成为一种有前途的解决方案，可以在保持计算操作数量恒定的同时扩大模型规模。现有的MoE模型采用了固定的门控网络，每个标记都由相同数量的专家计算。然而，这种方法与我们的直觉相矛盾，因为每个序列中的标记在语言复杂性方面有所不同，因此需要不同的计算成本。先前的研究中很少讨论每个标记的计算和模型性能之间的权衡。本文介绍了在MoE中使用自适应门控的灵活训练策略，该策略可以根据专家概率分布将标记处理为可变数量的专家。所提出的框架在改进训练效率的同时保持稀疏性。此外，课程学习也在该框架中应用以进一步提高模型性能。",
    "tldr": "本文介绍了一种自适应门控的混合专家语言模型训练策略，通过根据标记的专家概率分布将标记分配给变量数量的专家，同时保持模型的稀疏性和提高训练效率。",
    "en_tdlr": "This paper introduces adaptive gating in mixture-of-experts (MoE) based language models, which allows tokens to be processed by a variable number of experts based on expert probability distribution, preserving sparsity while improving training efficiency. The proposed framework also applies curriculum learning to further enhance model performance."
}