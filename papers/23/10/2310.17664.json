{
    "title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search. (arXiv:2310.17664v1 [cs.LG])",
    "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.",
    "link": "http://arxiv.org/abs/2310.17664",
    "context": "Title: Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search. (arXiv:2310.17664v1 [cs.LG])\nAbstract: Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.",
    "path": "papers/23/10/2310.17664.json",
    "total_tokens": 992,
    "translated_title": "基于神经架构搜索的级联多任务自适应学习",
    "translated_abstract": "级联多个预训练模型是构建端到端系统的有效方法。然而，对整个级联模型进行微调在参数和内存效率上并不高，并且我们的观察结果表明，仅在级联模型上应用适配器模块无法达到与微调相当的性能。我们提出了一种基于神经架构搜索（NAS）框架的自适应学习方法，以优化端到端级联多任务模型。每个特定模型上的候选自适应操作包括冻结、插入适配器和微调。我们还在损失函数上添加了一个惩罚项，以限制学习到的结构，并考虑可训练参数的数量。惩罚项成功限制了搜索到的架构，并且所提出的方法能够搜索出与手工设计类似的调整方案，将优化参数压缩到了全微调的8.7%，且性能更好。",
    "tldr": "该论文提出了一种基于神经架构搜索的自适应学习方法，用于优化端到端级联多任务模型。该方法通过在特定模块上应用冻结、插入适配器和微调等自适应操作，并在损失函数上添加惩罚项，成功限制了搜索到的架构。结果表明，该方法能够搜索出与手工设计类似的调整方案，并将优化参数压缩到了全微调的8.7%，同时获得了更好的性能。",
    "en_tdlr": "This paper proposes an adaptive learning method based on Neural Architecture Search (NAS) to optimize end-to-end cascaded multi-task models. The method applies adaptive operations such as freezing, inserting adapters, and fine-tuning on specific modules, and adds a penalty item on the loss function to restrict the learned structure. The results show that the method can search for tuning schemes similar to hand-crafting and compress the optimizing parameters to 8.7% of full fine-tuning, while achieving better performance."
}