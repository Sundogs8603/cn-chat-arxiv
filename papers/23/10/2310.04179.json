{
    "title": "Entropic Score metric: Decoupling Topology and Size in Training-free NAS. (arXiv:2310.04179v1 [cs.CV])",
    "abstract": "Neural Networks design is a complex and often daunting task, particularly for resource-constrained scenarios typical of mobile-sized models. Neural Architecture Search is a promising approach to automate this process, but existing competitive methods require large training time and computational resources to generate accurate models. To overcome these limits, this paper contributes with: i) a novel training-free metric, named Entropic Score, to estimate model expressivity through the aggregated element-wise entropy of its activations; ii) a cyclic search algorithm to separately yet synergistically search model size and topology. Entropic Score shows remarkable ability in searching for the topology of the network, and a proper combination with LogSynflow, to search for model size, yields superior capability to completely design high-performance Hybrid Transformers for edge applications in less than 1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet classifica",
    "link": "http://arxiv.org/abs/2310.04179",
    "context": "Title: Entropic Score metric: Decoupling Topology and Size in Training-free NAS. (arXiv:2310.04179v1 [cs.CV])\nAbstract: Neural Networks design is a complex and often daunting task, particularly for resource-constrained scenarios typical of mobile-sized models. Neural Architecture Search is a promising approach to automate this process, but existing competitive methods require large training time and computational resources to generate accurate models. To overcome these limits, this paper contributes with: i) a novel training-free metric, named Entropic Score, to estimate model expressivity through the aggregated element-wise entropy of its activations; ii) a cyclic search algorithm to separately yet synergistically search model size and topology. Entropic Score shows remarkable ability in searching for the topology of the network, and a proper combination with LogSynflow, to search for model size, yields superior capability to completely design high-performance Hybrid Transformers for edge applications in less than 1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet classifica",
    "path": "papers/23/10/2310.04179.json",
    "total_tokens": 877,
    "translated_title": "Entropic Score度量：在无需训练的NAS中解耦拓扑和大小",
    "translated_abstract": "神经网络设计是一项复杂且常常令人望而生畏的任务，特别是对于移动大小模型的资源受限场景。神经网络架构搜索是一种自动化这一过程的有希望的方法，但是现有的竞争方法需要大量的训练时间和计算资源来生成准确的模型。为了克服这些限制，本文做出了以下贡献：i）一种名为Entropy Score的新型无需训练的度量方法，通过聚合的逐元素熵估计模型的表达能力；ii）一种循环搜索算法，分别但协同地搜索模型大小和拓扑。Entropy Score在搜索网络的拓扑方面显示出显著能力，与LogSynflow的适当组合用于搜索模型大小，可以在不到1个GPU小时内完全设计用于边缘应用的高性能混合变压器，从而实现了ImageNet分类上最快且最准确的NAS方法。",
    "tldr": "本文提出了一种名为Entropy Score的新型无需训练的度量方法，用于估计模型的表达能力，并通过与LogSynflow的组合搜索模型的大小，从而在较短时间内设计出用于边缘应用的高性能网络。"
}