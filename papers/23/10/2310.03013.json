{
    "title": "SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])",
    "abstract": "Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig",
    "link": "http://arxiv.org/abs/2310.03013",
    "context": "Title: SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])\nAbstract: Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig",
    "path": "papers/23/10/2310.03013.json",
    "total_tokens": 872,
    "translated_title": "SemiReward: 半监督学习的通用奖励模型",
    "translated_abstract": "半监督学习在自训练框架和伪标签上取得了显著进展。主要挑战是如何区分高质量的伪标签，避免确证偏见。然而，现有的伪标签选择策略限制于预定义的方案或复杂的手工制作策略，无法同时实现高质量标签、快速收敛和任务多样性。为此，我们提出了一种半监督奖励框架（SemiReward），用于预测奖励分数以评估和过滤高质量的伪标签，可以在各种任务类型和场景下与主流的半监督学习方法相结合使用。为了减少确证偏见，在两个阶段通过生成模型和子抽样策略进行在线训练。通过在三种模态的13个标准半监督学习基准上进行分类和回归任务的广泛实验验证，表明SemiReward取得了显著的成果。",
    "tldr": "SemiReward是一个通用奖励模型，通过预测奖励分数来评估和过滤高质量的伪标签，可以应用于各种半监督学习任务，并在实验中取得了显著的成果。",
    "en_tdlr": "SemiReward is a general reward model that predicts reward scores to evaluate and filter out high-quality pseudo labels, which can be applied to various semi-supervised learning tasks, and has achieved significant results in experiments."
}