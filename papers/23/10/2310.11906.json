{
    "title": "Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])",
    "abstract": "Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations compute",
    "link": "http://arxiv.org/abs/2310.11906",
    "context": "Title: Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])\nAbstract: Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations compute",
    "path": "papers/23/10/2310.11906.json",
    "total_tokens": 845,
    "translated_title": "宁愿是护士也不愿是医生 -- 对比性解释的研究",
    "translated_abstract": "对比性解释是将一个决策与另一个进行对比解释，它比非对比性解释更接近于人类解释决策的方式。这一观点尚未经过实证验证。我们对四个英文文本分类数据集（SST2、DynaSent、BIOS和DBpedia-Animals）进行了分析。我们通过微调和提取来自三种不同模型（RoBERTa、GPT-2和T5）的解释，并应用三种后期可解释性方法（LRP、GradientxInput和GradNorm）。我们还针对BIOS数据集中的100个样本的对比性和非对比性设置进行了人类理性注释的收集和发布。模型基础理性与人类标注之间在对比性和非对比性设置下的交叉比较显示，无论是对于模型还是人类，两个设置之间存在高度一致性。此外，模型基础解释计算…",
    "tldr": "这项研究通过分析四个英文文本分类数据集和人类理性注释，验证了对比性解释与非对比性解释在模型和人类之间的高度一致性。",
    "en_tdlr": "This study validates the high agreement between contrastive explanations and non-contrastive explanations in both models and humans through analyzing four English text-classification datasets and human rationale annotations."
}