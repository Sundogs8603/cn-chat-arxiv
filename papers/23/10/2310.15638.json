{
    "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])",
    "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementa",
    "link": "http://arxiv.org/abs/2310.15638",
    "context": "Title: CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])\nAbstract: Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementa",
    "path": "papers/23/10/2310.15638.json",
    "total_tokens": 922,
    "translated_title": "CoAnnotating：人类和大型语言模型在数据注释中的不确定性引导工作分配",
    "translated_abstract": "在自然语言处理（NLP）中，标注数据在训练模型和评估性能方面起着关键作用。鉴于大型语言模型（LLMs）的最新发展，像ChatGPT这样的模型在许多文本注释任务上展现出了零 shot 能力，与人类注释者相比甚至超过了人类。由于成本较低且可扩展性较高，这样的LLMs可以作为手动标注的替代品。然而，目前尚未有工作利用LLMs作为补充注释者，也没有探索如何最佳分配人类和LLMs的注释工作以实现质量和成本的目标。我们提出了CoAnnotating，一种新颖的人类-LLM联合注释范式，用于大规模非结构化文本的注释。在这个框架下，我们利用不确定性来估计LLMs的注释能力。我们的实证研究表明，CoAnnotating是一种有效的工作分配方式，能够在不同数据集上提高高达21%的性能，相比随机基线。",
    "tldr": "CoAnnotating是一种新颖的人类-LLM联合注释框架，利用不确定性来估计LLMs的能力，并能在不同数据集上获得高达21%的性能提升。",
    "en_tdlr": "CoAnnotating is a novel framework for human-LLM co-annotation that utilizes uncertainty to estimate LLMs' capability, resulting in up to 21% performance improvement across different datasets."
}