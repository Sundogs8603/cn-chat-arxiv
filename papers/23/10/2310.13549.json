{
    "title": "The Perils & Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])",
    "abstract": "Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper compr",
    "link": "http://arxiv.org/abs/2310.13549",
    "context": "Title: The Perils & Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])\nAbstract: Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper compr",
    "path": "papers/23/10/2310.13549.json",
    "total_tokens": 951,
    "translated_title": "大语言模型在事实检查中的危险与潜力",
    "translated_abstract": "自主事实检查利用机器学习来验证论断，在虚假信息超出人工事实检查能力的情况下变得至关重要。像GPT-4这样的大语言模型越来越被信任，可以验证信息、撰写学术论文、法律诉讼和新闻文章，强调了它们在区分真实与虚假以及验证其输出的重要性。本文通过让大语言模型代理人提出查询、检索上下文数据和做出决策来评估大语言模型在事实检查中的使用。重要的是，在我们的框架中，代理人解释其推理过程并引用检索到的相关来源。我们的结果显示，当配备了上下文信息时，大语言模型的能力得到了增强。GPT-4优于GPT-3，但准确性因查询语言和论断真实性而异。虽然大语言模型在事实检查中显示出潜力，但由于准确性不一致，必须谨慎使用。我们的研究呼吁进一步的研究，促进更深入的理解。",
    "tldr": "本文评估了大语言模型在事实检查中的应用，发现配备上下文信息后，大语言模型表现出更强的能力。然而，准确性存在一定差异，因此在使用中需要谨慎。进一步研究仍然需要进行。",
    "en_tdlr": "This paper evaluates the application of large language models in fact-checking and finds that with contextual information, the models demonstrate enhanced capabilities. However, there are variations in accuracy, highlighting the need for caution in their usage. Further research is called for."
}