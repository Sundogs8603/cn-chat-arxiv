{
    "title": "One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition. (arXiv:2310.01688v1 [eess.AS])",
    "abstract": "This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when'' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style\" prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.",
    "link": "http://arxiv.org/abs/2310.01688",
    "context": "Title: One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition. (arXiv:2310.01688v1 [eess.AS])\nAbstract: This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when'' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style\" prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.",
    "path": "papers/23/10/2310.01688.json",
    "total_tokens": 942,
    "translated_title": "一种模型统治全球？朝着端到端联合说话人先行护理和语音识别迈进。",
    "translated_abstract": "本文提出了一种名为SLIDAR（滑动窗口先行护理增强识别）的联合说话人先行护理（SD）和自动语音识别（ASR）的新框架。SLIDAR可以处理任意长度的输入，可以处理任意数量的说话人，同时有效地解决了“谁在什么时候说了什么”的问题。SLIDAR采用滑动窗口方法，包括了一个端到端的先行护理增强语音转录（E2E DAST）模型，该模型为每个窗口提供本地的转录、先行护理和说话人嵌入。E2E DAST模型基于编码器-解码器架构，利用了最近的技术，如序列化输出训练和“Whisper-style”提示。然后，通过对说话人嵌入进行聚类来合并本地输出，得到最终的SD+ASR结果。",
    "tldr": "本文提出了一种名为SLIDAR的新框架，实现了联合说话人先行护理和自动语音识别，能够处理任意长度的输入和任意数量的说话人，并通过将本地输出合并来获得最终的结果。实验证实了该方法在近距离和远距离语音场景中的有效性。",
    "en_tdlr": "This paper proposes a new framework called SLIDAR for joint speaker diarization and automatic speech recognition. The framework can handle variable-length inputs and any number of speakers, and achieves effective results by combining local outputs. Experimental results confirm its effectiveness in close-talk and far-field speech scenarios."
}