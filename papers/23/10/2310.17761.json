{
    "title": "Distributed Personalized Empirical Risk Minimization. (arXiv:2310.17761v1 [cs.LG])",
    "abstract": "This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. W",
    "link": "http://arxiv.org/abs/2310.17761",
    "context": "Title: Distributed Personalized Empirical Risk Minimization. (arXiv:2310.17761v1 [cs.LG])\nAbstract: This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. W",
    "path": "papers/23/10/2310.17761.json",
    "total_tokens": 882,
    "translated_title": "分布式个性化经验风险最小化",
    "translated_abstract": "本文倡导了一种新的范式：个性化经验风险最小化（PERM），以便在不对参与设备共享的计算资源施加严格限制的情况下从异构数据源中进行学习。在PERM中，我们的目标是通过学习如何与谁学习，并通过有效估计数据分布之间的统计差异来个性化聚合本地经验损失，从而获得所有本地分布的最佳统计准确性并克服数据异构性问题。为了学习规模化的个性化模型，我们提出了一种分布式算法，它通过模型重排取代了标准的模型平均化，以同时优化所有设备的PERM目标。这还允许我们为不同的客户学习不同的模型架构（例如具有不同参数数量的神经网络），从而限制了各个客户的潜在内存和计算资源。",
    "tldr": "本文引入个性化经验风险最小化（PERM）的新范式，旨在实现不对参与设备共享的计算资源施加限制的情况下从异构数据源中进行学习，并通过估计数据分布之间的统计差异来个性化聚合本地经验损失，从而克服数据异构性问题。",
    "en_tdlr": "This paper proposes a new paradigm called Personalized Empirical Risk Minimization (PERM) for learning from heterogeneous data sources without imposing constraints on computational resources shared by participating devices. By estimating the statistical discrepancy among data distributions, PERM personalizes the aggregation of local empirical losses and overcomes the issue of data heterogeneity."
}