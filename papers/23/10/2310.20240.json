{
    "title": "Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape. (arXiv:2310.20240v1 [cs.CV])",
    "abstract": "The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement ",
    "link": "http://arxiv.org/abs/2310.20240",
    "context": "Title: Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape. (arXiv:2310.20240v1 [cs.CV])\nAbstract: The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement ",
    "path": "papers/23/10/2310.20240.json",
    "total_tokens": 946,
    "translated_title": "为面部注入生命：具有自然头部姿态和详细形状的语音驱动的3D面部动画",
    "translated_abstract": "创建逼真的语音驱动的3D面部动画需要音频输入和面部表情之间的自然和精确的同步。然而，现有作品仍无法渲染具有灵活头部姿态和自然面部细节（如皱纹）的形状。这种限制主要由两个方面导致：1）收集具有详细3D面部形状的训练集非常昂贵。详细形状注释的稀缺性阻碍了具有表情丰富的面部动画的模型训练。2）与嘴部运动相比，头部姿态与语音内容的相关性较小。因此，同时对嘴部运动和头部姿态进行建模导致了面部运动的可控性不足。为了解决这些挑战，我们引入了VividTalker框架，该框架旨在实现具有灵活头部姿态和自然面部细节的语音驱动的3D面部动画。",
    "tldr": "本论文提出了一个新的框架VividTalker，用于实现具有灵活头部姿态和自然面部细节的语音驱动的3D面部动画。该框架通过明确将面部动画分为头部姿态和嘴部运动来解决目前现有作品中的限制和挑战。",
    "en_tdlr": "This paper introduces a new framework called VividTalker for achieving speech-driven 3D facial animation with flexible head pose and natural facial details. The framework explicitly disentangles facial animation into head pose and mouth movement, addressing the limitations and challenges in existing works."
}