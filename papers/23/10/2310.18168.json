{
    "title": "Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])",
    "abstract": "Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent \"Wikipedia\" will behave truthfully on topics that were only generated by \"Science\" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod",
    "link": "http://arxiv.org/abs/2310.18168",
    "context": "Title: Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])\nAbstract: Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent \"Wikipedia\" will behave truthfully on topics that were only generated by \"Science\" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod",
    "path": "papers/23/10/2310.18168.json",
    "total_tokens": 949,
    "translated_title": "使用人设来建模语言模型中的真实性",
    "translated_abstract": "大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。",
    "tldr": "本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。",
    "en_tdlr": "This study explores the possibility of using personas to model truthfulness in large language models. By modeling a truthful persona, language models can generalize truthfulness beyond specific contexts and discern the truthfulness of text generated by different agents based on shared features."
}