{
    "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)",
    "abstract": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model",
    "link": "http://arxiv.org/abs/2310.05057",
    "context": "Title: BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)\nAbstract: The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model",
    "path": "papers/23/10/2310.05057.json",
    "total_tokens": 948,
    "translated_title": "BRAINTEASER：大型语言模型的横向思维难题",
    "translated_abstract": "语言模型的成功激励了自然语言处理社区关注需要隐含和复杂推理的任务，依赖于类人的常识机制。虽然这些垂直思维任务相对较受欢迎，但横向思维难题却鲜有关注。为了弥合这一差距，我们设计了BRAINTEASER：一个多项选择问题回答任务，旨在测试模型表现出横向思维和违反默认常识联系的能力。我们设计了一个三步骤的程序来创建第一个横向思维基准，包括数据收集、干扰项生成和对抗性样本生成，共有1,100个具有高质量注释的难题。为了评估模型的横向推理一致性，我们基于问题的语义和上下文重建来丰富BRAINTEASER。我们对最先进的指导性和常识语言模型进行的实验揭示了人类和模型之间的显著差距。",
    "tldr": "本文介绍了一项名为BRAINTEASER的多项选择问题回答任务，旨在测试大型语言模型表现出横向思维和违反默认常识联系的能力。通过创建一个横向思维基准，丰富问题的语义和上下文重建，实验证明模型与人类之间存在显著差距。",
    "en_tdlr": "This paper introduces a multiple-choice Question Answering task called BRAINTEASER, aimed at testing the ability of large language models to exhibit lateral thinking and defy default commonsense associations. By creating a lateral thinking benchmark and enriching the questions with semantic and contextual reconstruction, experiments reveal a significant gap between models and human performance."
}