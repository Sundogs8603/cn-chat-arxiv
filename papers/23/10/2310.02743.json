{
    "title": "Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \"true\" reward, these learned reward models are susceptible to \\textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \"gold\" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sa",
    "link": "http://arxiv.org/abs/2310.02743",
    "context": "Title: Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])\nAbstract: Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \"true\" reward, these learned reward models are susceptible to \\textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \"gold\" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sa",
    "path": "papers/23/10/2310.02743.json",
    "total_tokens": 850,
    "translated_title": "奖励模型集成有助于减轻过度优化问题",
    "translated_abstract": "人类反馈强化学习（RLHF）是一种将大型语言模型微调以遵循指令的标准方法。在这个过程中，学习到的奖励模型被用来近似人类偏好。然而，作为“真实”奖励的不完美表示，这些学习到的奖励模型容易受到过度优化的影响。Gao等人在一个人工反馈实验中研究了这个现象，使用一个较大的“金标准”奖励模型作为真实奖励（而不是人类），并显示过度优化仍然是一个持续存在的问题，无论代理奖励模型和训练数据的大小如何。使用类似的设置，我们进行了一项系统研究，评估了在使用两种优化方法时，使用基于集合的保守优化目标（最坏情况优化和权重不确定性优化）来减轻奖励模型过度优化的有效性。",
    "tldr": "本研究通过探究奖励模型集成和保守优化目标的效果，对减轻奖励模型过度优化进行了系统研究。",
    "en_tdlr": "This paper investigates the effectiveness of using reward model ensembles and conservative optimization objectives for mitigating reward model overoptimization."
}