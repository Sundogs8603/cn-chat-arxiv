{
    "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
    "abstract": "arXiv:2310.01381v3 Announce Type: replace-cross  Abstract: Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation",
    "link": "https://arxiv.org/abs/2310.01381",
    "context": "Title: DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation\nAbstract: arXiv:2310.01381v3 Announce Type: replace-cross  Abstract: Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation",
    "path": "papers/23/10/2310.01381.json",
    "total_tokens": 826,
    "translated_title": "DiffAR：去噪扩散自回归模型用于原始语音波形生成",
    "translated_abstract": "近期，扩散模型已被证明与高质量语音生成相关。大多数工作集中在生成频谱图，因此进一步需要后续模型将频谱图转换为波形（即声码器）。本文提出了一个扩散概率端到端模型，用于生成原始语音波形。所提出的模型是自回归的，按顺序生成重叠帧，其中每个帧依赖于先前生成的部分。因此，我们的模型可以有效地合成无限语音持续时间，同时保持高保真合成和时间一致性。我们为无条件和有条件语音生成实现了所提出的模型，后者可以由输入的音素序列、振幅和音调值驱动。直接处理波形具有一些经验优势。具体来说，它允许创建",
    "tldr": "本研究提出了一个扩散概率端到端模型，用于生成原始语音波形，实现了高保真合成和时间一致性，可实现无限语音持续时间，并支持无条件和有条件语音生成。",
    "en_tdlr": "This work proposes a diffusion probabilistic end-to-end model for generating raw speech waveforms, achieving high-fidelity synthesis and temporal coherence, enabling infinite speech duration, supporting both unconditional and conditional speech generation."
}