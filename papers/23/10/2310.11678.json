{
    "title": "Using Experience Classification for Training Non-Markovian Tasks. (arXiv:2310.11678v1 [cs.LG])",
    "abstract": "Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.",
    "link": "http://arxiv.org/abs/2310.11678",
    "context": "Title: Using Experience Classification for Training Non-Markovian Tasks. (arXiv:2310.11678v1 [cs.LG])\nAbstract: Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.",
    "path": "papers/23/10/2310.11678.json",
    "total_tokens": 981,
    "translated_title": "使用经验分类训练非马尔可夫任务",
    "translated_abstract": "不同于标准的强化学习模型，许多实际任务是非马尔可夫的，其奖励是基于状态历史而不仅仅是当前状态。解决非马尔可夫任务在实际应用中（如自动驾驶、金融交易和医学诊断）中经常面临挑战。我们提出了一种新颖的强化学习方法，以实现在有限轨迹上表达的非马尔可夫奖励的目标逻辑LTL$_f$（线性时态逻辑）。为此，我们引入了一种从LTL$_f$到MDPs（马尔可夫决策过程）的线性复杂度的编码，以利用先进的强化学习算法。然后，我们利用基于自动机结构（语义等价于LTL$_f$规范）的优先化经验回放技术来改善训练过程。我们通过对几个带有非马尔可夫任务的基准问题进行实证评估，以展示我们方法的可行性和有效性。",
    "tldr": "该论文提出了一种使用经验分类的方法来训练非马尔可夫任务。通过将非马尔可夫任务转化为有限轨迹上的线性时态逻辑表达，并利用优先化经验回放技术改善训练过程，以实现非马尔可夫奖励的目标逻辑。实验证明了该方法的可行性和有效性。",
    "en_tdlr": "This paper proposes a method using experience classification to train non-Markovian tasks. By transforming non-Markovian tasks into linear temporal logic expressions on finite traces and improving the training process with prioritized experience replay technique, the target logic of non-Markovian rewards is achieved. Empirical evaluations demonstrate the feasibility and effectiveness of this method."
}