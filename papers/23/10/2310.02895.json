{
    "title": "CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])",
    "abstract": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\\textbf{Co}$ncomitant $\\textbf{Li}$n",
    "link": "http://arxiv.org/abs/2310.02895",
    "context": "Title: CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])\nAbstract: We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\\textbf{Co}$ncomitant $\\textbf{Li}$n",
    "path": "papers/23/10/2310.02895.json",
    "total_tokens": 906,
    "translated_title": "CoLiDE: 共同线性有向无环图估计",
    "translated_abstract": "我们处理从遵循线性结构方程模型 (SEM) 的观测数据中学习有向无环图 (DAG) 结构的组合问题。利用不可微分、非凸的有效性特征，最近的研究提出了一种连续受限优化范式，以便高效地探索DAG空间。大多数现有方法使用套索类型的评分函数来引导这个搜索过程，这些函数在$\\textit{未知}$SEM噪声方差在问题实例之间发生变化时需进行昂贵的惩罚参数重新调整，并且隐含地依赖于有界同方差假设。在这项工作中，我们提出了一个新的凸评分函数，用于稀疏感知线性DAG的学习，该函数结合了标度的共同估计，从而有效地将稀疏参数与外生噪声水平分离。通过平滑的、非凸的无环惩罚项进行正则化，可以得到CoLiDE （共同线性DAG估计）。",
    "tldr": "本论文提出了CoLiDE算法用于学习线性DAG，该算法使用了一个新的凸评分函数，结合了标度的共同估计，从而有效地将稀疏参数与外生噪声水平分离。",
    "en_tdlr": "This paper proposes the CoLiDE algorithm for learning linear DAGs, which uses a new convex score function that incorporates concomitant estimation of scale and effectively separates the sparsity parameter from the exogenous noise levels."
}