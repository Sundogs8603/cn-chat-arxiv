{
    "title": "Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation. (arXiv:2310.01612v1 [cs.IR])",
    "abstract": "In recent years, with large language models (LLMs) achieving state-of-the-art performance in context understanding, increasing efforts have been dedicated to developing LLM-enhanced sequential recommendation (SR) methods. Considering that most existing LLMs are not specifically optimized for recommendation tasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods. Though numerous adaptation methods have been developed, it still remains a significant challenge to adapt LLMs for SR both efficiently and effectively. To address this challenge, in this paper, we introduce a novel side sequential network adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features three key designs to allow both efficient and effective LLM adaptation. First, SSNA learns adapters separate from LLMs, while fixing all the pre-trained parameters within LLMs to allow efficient adaptation. In addition, SSNA adapts the top-a layers of LLMs jointly, and integrates adapters sequentially",
    "link": "http://arxiv.org/abs/2310.01612",
    "context": "Title: Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation. (arXiv:2310.01612v1 [cs.IR])\nAbstract: In recent years, with large language models (LLMs) achieving state-of-the-art performance in context understanding, increasing efforts have been dedicated to developing LLM-enhanced sequential recommendation (SR) methods. Considering that most existing LLMs are not specifically optimized for recommendation tasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods. Though numerous adaptation methods have been developed, it still remains a significant challenge to adapt LLMs for SR both efficiently and effectively. To address this challenge, in this paper, we introduce a novel side sequential network adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features three key designs to allow both efficient and effective LLM adaptation. First, SSNA learns adapters separate from LLMs, while fixing all the pre-trained parameters within LLMs to allow efficient adaptation. In addition, SSNA adapts the top-a layers of LLMs jointly, and integrates adapters sequentially",
    "path": "papers/23/10/2310.01612.json",
    "total_tokens": 923,
    "translated_title": "面向大型语言模型的顺序推荐的高效有效适应研究",
    "translated_abstract": "近年来，随着大型语言模型在上下文理解方面取得了最先进的性能，越来越多的工作致力于开发增强顺序推荐的大型语言模型方法。考虑到大多数现有的语言模型并没有专门针对推荐任务进行优化，将它们适应到顺序推荐中成为了增强顺序推荐的关键一步。虽然已经提出了许多适应方法，但是如何高效且有效地适应语言模型仍然是一个重要的挑战。为了解决这个挑战，本文提出了一种新颖的侧序列网络适应方法，称为SSNA，用于增强大型语言模型的顺序推荐。SSNA具有三个关键设计，可以实现高效和有效的语言模型适应。首先，SSNA学习与语言模型分离的适配器，同时固定所有预训练参数以实现高效的适应。此外，SSNA联合适应语言模型的顶层，按顺序集成适配器。",
    "tldr": "本文介绍了一种用于大型语言模型增强顺序推荐的侧序列网络适应方法（SSNA），它通过学习适配器来实现高效且有效的语言模型适应。",
    "en_tdlr": "This paper presents a novel side sequential network adaptation method (SSNA) for enhancing sequential recommendation with large language models (LLMs), enabling efficient and effective adaptation of LLMs through learning separate adapters."
}