{
    "title": "Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v1 [cs.CL])",
    "abstract": "For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.",
    "link": "http://arxiv.org/abs/2310.13620",
    "context": "Title: Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v1 [cs.CL])\nAbstract: For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.",
    "path": "papers/23/10/2310.13620.json",
    "total_tokens": 780,
    "translated_title": "在语言模型中连接信息论和几何压缩",
    "translated_abstract": "为了真实地模拟人类语言，语言模型（LM）必须将大量的、潜在无限的信息压缩到相对较少的维度中。我们提出从几何和信息论的两个角度分析（预训练的）LM的压缩。我们证明了这两个视角高度相关，语言数据的内在几何维度可以预测它们在LM下的编码长度。然后我们展示了，进一步，语言数据集的高压缩预测了对该数据集的快速适应，确认了能够压缩语言信息是成功LM性能的重要部分。作为我们分析的实际副产品，我们首次评估了一系列在语言数据上的内在维度估计器，表明只有一部分封装了信息论压缩、几何压缩和适应度之间的关系。",
    "tldr": "本文提出了从几何和信息论的角度分析语言模型（LM）中的压缩问题，并展示了这两个视角高度相关的关系。此外，我们还发现语言数据的高压缩预测了对该数据集的快速适应。"
}