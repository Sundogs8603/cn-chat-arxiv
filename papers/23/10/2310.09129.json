{
    "title": "Computing Marginal and Conditional Divergences between Decomposable Models with Applications. (arXiv:2310.09129v1 [cs.LG])",
    "abstract": "The ability to compute the exact divergence between two high-dimensional distributions is useful in many applications but doing so naively is intractable. Computing the alpha-beta divergence -- a family of divergences that includes the Kullback-Leibler divergence and Hellinger distance -- between the joint distribution of two decomposable models, i.e chordal Markov networks, can be done in time exponential in the treewidth of these models. However, reducing the dissimilarity between two high-dimensional objects to a single scalar value can be uninformative. Furthermore, in applications such as supervised learning, the divergence over a conditional distribution might be of more interest. Therefore, we propose an approach to compute the exact alpha-beta divergence between any marginal or conditional distribution of two decomposable models. Doing so tractably is non-trivial as we need to decompose the divergence between these distributions and therefore, require a decomposition over the m",
    "link": "http://arxiv.org/abs/2310.09129",
    "context": "Title: Computing Marginal and Conditional Divergences between Decomposable Models with Applications. (arXiv:2310.09129v1 [cs.LG])\nAbstract: The ability to compute the exact divergence between two high-dimensional distributions is useful in many applications but doing so naively is intractable. Computing the alpha-beta divergence -- a family of divergences that includes the Kullback-Leibler divergence and Hellinger distance -- between the joint distribution of two decomposable models, i.e chordal Markov networks, can be done in time exponential in the treewidth of these models. However, reducing the dissimilarity between two high-dimensional objects to a single scalar value can be uninformative. Furthermore, in applications such as supervised learning, the divergence over a conditional distribution might be of more interest. Therefore, we propose an approach to compute the exact alpha-beta divergence between any marginal or conditional distribution of two decomposable models. Doing so tractably is non-trivial as we need to decompose the divergence between these distributions and therefore, require a decomposition over the m",
    "path": "papers/23/10/2310.09129.json",
    "total_tokens": 837,
    "translated_title": "计算可分解模型之间的边际和条件差异及其应用",
    "translated_abstract": "在许多应用中，计算两个高维分布之间的精确差异是有用的，但直接计算是不可行的。对于两个可分解模型（即弦图马尔可夫网络）的联合分布计算α-β差异（包括Kullback-Leibler差异和Hellinger距离）可以在指数时间内成为这些模型的树宽度。然而，将两个高维对象之间的不相似性减少为单个标量值可能是不具有信息性的。此外，在诸如监督学习的应用中，对于条件分布的差异可能更有兴趣。因此，我们提出了一种方法来计算两个可分解模型的任何边际或条件分布之间的精确α-β差异。以可行的方式进行此计算是非平凡的，因为我们需要对这些分布之间的差异进行分解，因此需要对条件分布进行分解。",
    "tldr": "提出了一种计算可分解模型之间边际和条件差异的方法，能够在高维分布中精确计算差异，具有广泛的应用价值。",
    "en_tdlr": "Proposed a method for computing marginal and conditional divergences between decomposable models, enabling accurate computation of divergences in high-dimensional distributions, with broad applications."
}