{
    "title": "Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])",
    "abstract": "Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired",
    "link": "http://arxiv.org/abs/2310.04323",
    "context": "Title: Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])\nAbstract: Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired",
    "path": "papers/23/10/2310.04323.json",
    "total_tokens": 918,
    "translated_title": "可调节的鲁棒性强化学习在在线3D装箱问题中的应用",
    "translated_abstract": "设计有效的策略来解决在线3D装箱问题一直是一个长期的挑战，主要是由于传入箱子序列的不可预测性和严格的物理约束。尽管当前用于在线3D装箱问题的深度强化学习（DRL）方法在优化潜在的箱子序列分布的平均性能方面取得了令人期待的结果，但它们在现实世界中往往无法处理一些最坏情况。标准的鲁棒性DRL算法往往过分优化最坏情况下的性能，从而牺牲了在正常问题实例分布下的性能。为了解决这些问题，我们首先引入了一种基于排列的攻击者来研究解决在线3D装箱问题的DRL方法和启发式方法的实际鲁棒性。然后，我们提出了一种可调节的鲁棒性强化学习（Adjustable Robust Reinforcement Learning，AR2L）框架，该框架允许有效地调节鲁棒性权重以实现所需的性能平衡。",
    "tldr": "我们提出了一种可调节的鲁棒性强化学习（AR2L）框架来解决在线3D装箱问题，该框架允许有效地调节鲁棒性权重以实现所需的性能平衡。"
}