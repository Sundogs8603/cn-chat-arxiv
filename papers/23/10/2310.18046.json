{
    "title": "ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese. (arXiv:2310.18046v1 [cs.CL])",
    "abstract": "In recent years, Visual Question Answering (VQA) has gained significant attention for its diverse applications, including intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore",
    "link": "http://arxiv.org/abs/2310.18046",
    "context": "Title: ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese. (arXiv:2310.18046v1 [cs.CL])\nAbstract: In recent years, Visual Question Answering (VQA) has gained significant attention for its diverse applications, including intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore",
    "path": "papers/23/10/2310.18046.json",
    "total_tokens": 893,
    "translated_title": "ViCLEVR：一种用于越南语视觉问答的视觉推理数据集和混合多模态融合模型",
    "translated_abstract": "近年来，视觉问答（VQA）因其包括智能汽车辅助、帮助视障人士以及使用自然语言查询的文档图像信息检索等多种应用而受到重视。VQA需要有效地将问题和图像的信息进行集成以生成准确的答案。VQA的神经模型在大规模数据集上取得了显著的进展，主要关注资源丰富的语言，如英语。为了解决这个问题，我们引入了ViCLEVR数据集，这是一组开创性的用于评估越南语中各种视觉推理能力并减小偏差的集合。该数据集包括超过26,000张图像和30,000个问题-答案对（QAs），每个问题都有注释指定所涉及的推理类型。利用这个数据集，我们对当代视觉推理系统进行了全面的分析，提供了有价值的见解，包括它们的优点和局限性。",
    "tldr": "本论文介绍了ViCLEVR数据集，它是一种用于评估越南语视觉推理能力的集合。通过分析该数据集，对当代视觉推理系统进行了全面的分析，揭示了它们的优点和局限性。",
    "en_tdlr": "This paper introduces the ViCLEVR dataset, a collection for evaluating visual reasoning capabilities in Vietnamese. Through analysis of this dataset, a comprehensive examination of contemporary visual reasoning systems is conducted, revealing their strengths and limitations."
}