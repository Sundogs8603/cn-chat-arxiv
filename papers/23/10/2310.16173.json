{
    "title": "On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\\epsilon$-Greedy Exploration. (arXiv:2310.16173v1 [cs.LG])",
    "abstract": "This paper provides a theoretical understanding of Deep Q-Network (DQN) with the $\\varepsilon$-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\\epsilon$-greedy policy. We prove an iterative procedure with decaying $\\epsilon$ ",
    "link": "http://arxiv.org/abs/2310.16173",
    "context": "Title: On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\\epsilon$-Greedy Exploration. (arXiv:2310.16173v1 [cs.LG])\nAbstract: This paper provides a theoretical understanding of Deep Q-Network (DQN) with the $\\varepsilon$-greedy exploration in deep reinforcement learning. Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored. First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However, the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\\epsilon$-greedy policy. We prove an iterative procedure with decaying $\\epsilon$ ",
    "path": "papers/23/10/2310.16173.json",
    "total_tokens": 858,
    "translated_title": "关于具有epsilon-greedy探索的Deep Q网络的收敛性与样本复杂度分析",
    "translated_abstract": "本文在深度强化学习中提供了对Deep Q网络（DQN）具有epsilon-greedy探索的理论理解。尽管DQN取得了巨大的实证成就，但其理论描述仍然不完善。首先，现有分析中的探索策略要么不切实际，要么被忽略。其次，与传统的Q学习算法相比，DQN采用目标网络和经验回放来获得训练Q网络所使用的均方贝尔曼误差（MSBE）的无偏估计。然而，现有的DQN理论分析缺乏收敛性分析，或者通过使用计算量非常大的神经网络来规避技术挑战，这在计算效率上并不高效。本文提供了第一个关于DQN实际设置的收敛性和样本复杂度分析，其中包括epsilon-greedy策略。我们证明了一个具有递减epsilon的迭代过程。",
    "tldr": "本文首次提供了关于实际设置下具有epsilon-greedy策略的Deep Q网络（DQN）的收敛性和样本复杂度分析。",
    "en_tdlr": "This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of Deep Q-Networks (DQN) with epsilon-greedy strategy."
}