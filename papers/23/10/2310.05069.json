{
    "title": "Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration. (arXiv:2310.05069v2 [cs.CL] UPDATED)",
    "abstract": "Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model's bias toward predicting label words which frequently occurred during the pretraining. These words typically receive high probabilities. To address this issue, we combine the models with calibration techniques which modify the probabilities of label words predicted by the models. We first validate the effectiveness of a proposed simple calibration method together with other existing techniques on monolingual encoders in both zero- and few-shot scenarios. We subsequently employ these calibration techniques on multilingual encoders, resulting in substantial performance improvements across a wide range",
    "link": "http://arxiv.org/abs/2310.05069",
    "context": "Title: Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration. (arXiv:2310.05069v2 [cs.CL] UPDATED)\nAbstract: Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model's bias toward predicting label words which frequently occurred during the pretraining. These words typically receive high probabilities. To address this issue, we combine the models with calibration techniques which modify the probabilities of label words predicted by the models. We first validate the effectiveness of a proposed simple calibration method together with other existing techniques on monolingual encoders in both zero- and few-shot scenarios. We subsequently employ these calibration techniques on multilingual encoders, resulting in substantial performance improvements across a wide range",
    "path": "papers/23/10/2310.05069.json",
    "total_tokens": 850,
    "translated_title": "揭示多语言编码器的潜力：通过概率校准增强零-shot性能",
    "translated_abstract": "预训练的多语言编码器模型可以通过将输入示例重新转化为填空式提示，直接执行零-shot多语言任务或语言探测。这通过预测屏蔽标记位置的标签词的概率来完成，无需对模型参数进行任何更新。然而，这种方法的性能受到模型对于在预训练期间频繁出现的标签词预测偏好的限制。这些词通常获得较高的概率。为了解决这个问题，我们将模型与校准技术结合，修改模型预测的标签词的概率。首先，我们验证了一个提出的简单校准方法与其他已有技术在单语言编码器的零-shot和少样本情况下的有效性。随后，我们将这些校准技术应用于多语言编码器中，显著提高了性能。",
    "tldr": "通过概率校准方法，本研究通过结合校准技术和多语言编码器，解决了预训练模型对于频繁出现的标签词预测偏好的问题，并显著提高了零-shot性能。"
}