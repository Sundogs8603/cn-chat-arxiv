{
    "title": "Deep Neural Networks Tend To Extrapolate Predictably",
    "abstract": "arXiv:2310.00873v2 Announce Type: replace  Abstract: Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate e",
    "link": "https://arxiv.org/abs/2310.00873",
    "context": "Title: Deep Neural Networks Tend To Extrapolate Predictably\nAbstract: arXiv:2310.00873v2 Announce Type: replace  Abstract: Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate e",
    "path": "papers/23/10/2310.00873.json",
    "total_tokens": 883,
    "translated_title": "深度神经网络倾向于可预测的外推",
    "translated_abstract": "传统观点认为，神经网络在面对超出分布范围（OOD）的输入时，预测往往是不可预测和过于自信的。我们的工作重新评估了具有高维输入的神经网络对这一假设的情况。我们观察到，与以往认为的任意外推不同，神经网络的预测往往在输入数据变得越来越OOD时趋向于一个恒定值。此外，我们发现这个值往往能够紧密地逼近最优恒定解（OCS），即在不观察输入的情况下最小化训练数据上的平均损失的预测。我们展示了在包括CIFAR10-C、ImageNet-R、S在内的8个数据集上以及不同分布转移（包括交叉熵、MSE和高斯NLL）和不同架构（CNN和变压器）下出现这种现象的结果。此外，我们对这种行为提出了一个解释，并首次验证了它。",
    "tldr": "我们的工作表明，与传统观点不同，在面对超出分布范围的输入时，高维输入的神经网络预测往往会趋向于一个恒定值，且这个值通常能够接近最优恒定解。",
    "en_tdlr": "Our work shows that contrary to conventional wisdom, neural networks with high-dimensional inputs tend to predict towards a constant value when faced with out-of-distribution inputs, and this value often closely approximates the optimal constant solution."
}