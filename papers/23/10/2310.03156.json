{
    "title": "FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])",
    "abstract": "The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate ",
    "link": "http://arxiv.org/abs/2310.03156",
    "context": "Title: FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])\nAbstract: The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate ",
    "path": "papers/23/10/2310.03156.json",
    "total_tokens": 895,
    "translated_title": "FedHyper:一种用于联邦学习的通用和稳健学习率调度器与超梯度下降",
    "translated_abstract": "联邦学习（FL）的理论框架正在迅速发展，但其实际应用面临一系列复杂挑战，其中超参数优化是其中一个关键挑战。在众多超参数调整中，学习率的适应性成为一个重要组成部分，有望显著提高FL系统的效果。为了满足这一关键需求，本文提出了FedHyper，一种专为FL设计的基于超梯度的学习率调整算法。FedHyper作为一种通用的学习率调度器，可以随着训练的进行调整全局和局部的学习率。此外，FedHyper不仅展示了对各种初始学习率配置的无与伦比的稳健性，还极大地减少了繁琐的经验性学习率调整的必要性。我们提供了FedHyper收敛速度的全面理论分析。",
    "tldr": "FedHyper是一种适用于联邦学习的通用和稳健学习率调度器，通过超梯度下降算法实现对全局和局部学习率的自适应调整，能够显著提高联邦学习系统的效果，同时减少了对经验调整的需求。"
}