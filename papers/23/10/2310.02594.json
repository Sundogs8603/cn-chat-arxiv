{
    "title": "I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding. (arXiv:2310.02594v1 [cs.CL])",
    "abstract": "Spoken language understanding (SLU) typically includes two subtasks: intent detection and slot filling. Currently, it has achieved great success in high-resource languages, but it still remains challenging in low-resource languages due to the scarcity of labeled training data. Hence, there is a growing interest in zero-shot cross-lingual SLU. Despite of the success of existing zero-shot cross-lingual SLU models, most of them neglect to achieve the mutual guidance between intent and slots. To address this issue, we propose an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance. Specifically, we not only apply intra-knowledge distillation between intent predictions or slot predictions of the same utterance in different languages, but also apply inter-knowledge distillation between intent predictions and slot predictions of the same utterance. Our experimental results demonstrate that our propose",
    "link": "http://arxiv.org/abs/2310.02594",
    "context": "Title: I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding. (arXiv:2310.02594v1 [cs.CL])\nAbstract: Spoken language understanding (SLU) typically includes two subtasks: intent detection and slot filling. Currently, it has achieved great success in high-resource languages, but it still remains challenging in low-resource languages due to the scarcity of labeled training data. Hence, there is a growing interest in zero-shot cross-lingual SLU. Despite of the success of existing zero-shot cross-lingual SLU models, most of them neglect to achieve the mutual guidance between intent and slots. To address this issue, we propose an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance. Specifically, we not only apply intra-knowledge distillation between intent predictions or slot predictions of the same utterance in different languages, but also apply inter-knowledge distillation between intent predictions and slot predictions of the same utterance. Our experimental results demonstrate that our propose",
    "path": "papers/23/10/2310.02594.json",
    "total_tokens": 946,
    "translated_title": "I$^2$KD-SLU: 一种面向零样本跨语言口语理解的内外知识蒸馏框架",
    "translated_abstract": "口语理解通常包括两个子任务：意图检测和词槽填充。目前，在高资源语言中已经取得了很大的成功，但在低资源语言中仍然存在挑战，原因是标记的训练数据稀缺。因此，对于零样本跨语言口语理解越来越受关注。尽管现有的零样本跨语言口语理解模型取得了成功，但大多数忽略了意图和词槽之间的相互指导。为了解决这个问题，我们提出了一种内外知识蒸馏框架用于零样本跨语言口语理解（I$^2$KD-SLU），以建模相互指导。具体而言，我们不仅在不同语言中的相同话语的意图预测或词槽预测之间应用内部知识蒸馏，而且还在相同话语的意图预测和词槽预测之间应用间接知识蒸馏。我们的实验结果表明我们的提议可以有效地建模相互指导。",
    "tldr": "I$^2$KD-SLU是一种内外知识蒸馏框架，用于解决零样本跨语言口语理解中意图和词槽之间的相互指导问题。",
    "en_tdlr": "I$^2$KD-SLU is an intra-inter knowledge distillation framework that addresses the issue of mutual guidance between intent and slots in zero-shot cross-lingual spoken language understanding."
}