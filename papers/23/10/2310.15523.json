{
    "title": "Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])",
    "abstract": "For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture glob",
    "link": "http://arxiv.org/abs/2310.15523",
    "context": "Title: Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])\nAbstract: For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture glob",
    "path": "papers/23/10/2310.15523.json",
    "total_tokens": 920,
    "translated_title": "生成式和对比式范式在图自监督学习中互补",
    "translated_abstract": "对于图自监督学习（GSSL），掩码自编码器（MAE）遵循生成式范式，并学习重构掩码图的边缘或节点特征。对比学习（CL）通过最大化同一图的增强视图之间的相似性来广泛用于GSSL。然而，MAE和CL在现有的GSSL工作中被单独考虑。我们观察到MAE和CL的范式是互补的，并提出了图对比掩码自编码器（GCMAE）框架来统一它们。具体而言，通过专注于局部边缘或节点特征，MAE不能捕捉到图的全局信息，并对特定的边缘和特征敏感。相反，在提取全局信息方面，CL表现出色，因为它考虑了图之间的关系。因此，我们将GCMAE装备了一个MAE分支和一个CL分支，并且这两个分支共享一个通用的编码器，这使得MAE分支能够利用CL分支提取的全局信息。为了强制GCMAE捕捉全局信息...",
    "tldr": "生成式和对比式范式在图自监督学习中是互补的，我们提出了图对比掩码自编码器（GCMAE）框架来统一它们，GCMAE通过利用对比学习的全局信息来弥补掩码自编码器在捕捉全局信息方面的不足。"
}