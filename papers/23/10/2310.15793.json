{
    "title": "Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])",
    "abstract": "This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the \"few-shot\" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that \"Parameter Efficient Fine-Tuning\" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on ",
    "link": "http://arxiv.org/abs/2310.15793",
    "context": "Title: Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])\nAbstract: This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the \"few-shot\" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that \"Parameter Efficient Fine-Tuning\" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on ",
    "path": "papers/23/10/2310.15793.json",
    "total_tokens": 943,
    "translated_title": "通过学习前缀子空间改进大型语言模型的泛化能力",
    "translated_abstract": "本文关注于大型语言模型（LLMs）在稀缺数据环境中的微调（也被称为“少样本”学习设置）。我们提出了一种基于神经网络子空间的方法来增加LLMs的泛化能力。这种优化方法最近在计算机视觉领域中引入，旨在通过在参数空间中的整个单纯形模型的联合优化，识别更广的局部最优解，从而提高模型的泛化能力。然而，将其适应于大规模预训练变换器模型则面临一些挑战。首先，它们大量的参数使得联合训练多个模型变得困难，其次，它们的确定性参数初始化方案使其不适用于最初的子空间方法。我们在本文中展示，“参数高效微调”（PEFT）方法与最初的方法完全兼容，并提出学习整个连续前缀的单纯形。我们在实验证明了这个方法在大型语言模型的泛化上的有效性。",
    "tldr": "本文提出了一种通过学习前缀子空间来改进大型语言模型的泛化能力的方法。我们通过联合优化模型参数空间中的整个单纯形模型，在稀缺数据环境中实现了更广的局部最优解。这种方法在预训练变换器模型中表现出了很好的兼容性和有效性。",
    "en_tdlr": "This paper proposes a method to improve the generalization capabilities of large language models by learning prefix subspaces. It achieves wider local optima by jointly optimizing an entire simplex of models in the parameter space, showing compatibility and effectiveness in the context of pre-trained transformer models."
}