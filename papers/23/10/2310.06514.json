{
    "title": "AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments",
    "abstract": "arXiv:2310.06514v2 Announce Type: replace Abstract: Feature attribution explains neural network outputs by identifying relevant input features. The attribution has to be faithful, meaning that the attributed features must mirror the input features that influence the output. One recent trend to test faithfulness is to fit a model on designed data with known relevant features and then compare attributions with ground truth input features.This idea assumes that the model learns to use all and only these designed features, for which there is no guarantee. In this paper, we solve this issue by designing the network and manually setting its weights, along with designing data. The setup, AttributionLab, serves as a sanity check for faithfulness: If an attribution method is not faithful in a controlled environment, it can be unreliable in the wild. The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements.",
    "link": "https://arxiv.org/abs/2310.06514",
    "context": "Title: AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments\nAbstract: arXiv:2310.06514v2 Announce Type: replace Abstract: Feature attribution explains neural network outputs by identifying relevant input features. The attribution has to be faithful, meaning that the attributed features must mirror the input features that influence the output. One recent trend to test faithfulness is to fit a model on designed data with known relevant features and then compare attributions with ground truth input features.This idea assumes that the model learns to use all and only these designed features, for which there is no guarantee. In this paper, we solve this issue by designing the network and manually setting its weights, along with designing data. The setup, AttributionLab, serves as a sanity check for faithfulness: If an attribution method is not faithful in a controlled environment, it can be unreliable in the wild. The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements.",
    "path": "papers/23/10/2310.06514.json",
    "total_tokens": 862,
    "translated_title": "AttributionLab:在可控环境下的特征归因忠实性",
    "translated_abstract": "特征归因通过识别相关的输入特征来解释神经网络的输出。这种归因必须是忠实的，意味着被归因的特征必须反映影响输出的输入特征。最近的一个趋势是通过在设计数据上拟合模型并将归因与真实的输入特征进行比较来测试忠实性。这个想法假设模型学会了仅使用这些设计特征，但并没有保证。本文通过设计网络并手动设置其权重以及设计数据来解决这个问题。这个设定称为“AttributionLab”，它可以作为忠实性的合理性检查：如果一个归因方法在一个可控环境中并不忠实，那么它在实际应用中可能是不可靠的。这个环境也是一个用于控制实验的实验室，我们可以通过这些实验来分析归因方法并提出改进的建议。",
    "tldr": "本文提出了AttributionLab，一个可控环境下测试特征归因忠实性的实验室。通过在设计数据上拟合模型并与真实输入特征进行比较，该实验室可以用于分析归因方法的性能，并提出改进建议。",
    "en_tdlr": "This paper presents AttributionLab, a laboratory for testing the faithfulness of feature attribution in a controlled environment. By fitting models on designed data and comparing them with ground truth input features, AttributionLab enables the analysis of attribution methods and the suggestion of improvements."
}