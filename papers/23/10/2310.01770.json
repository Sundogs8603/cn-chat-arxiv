{
    "title": "A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])",
    "abstract": "Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \\citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t",
    "link": "http://arxiv.org/abs/2310.01770",
    "context": "Title: A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])\nAbstract: Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \\citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t",
    "path": "papers/23/10/2310.01770.json",
    "total_tokens": 884,
    "translated_title": "损失平坦性与神经网络中压缩表示的简单联系",
    "translated_abstract": "对深度神经网络的泛化能力进行研究的方法有很多种，包括至少两种不同的方法：一种基于参数空间中损失景观的形状，另一种基于特征空间中表示流形的结构（即单位活动的空间）。这两种方法相关但很少同时进行研究和明确关联。在这里，我们提出了一种简单的分析方法来建立这种联系。我们展示了在深度神经网络学习的最后阶段，神经表示流形的体积压缩与正在进行的参数优化所探索的最小值周围的损失平坦性相关。我们证明了这可以由一个相对简单的数学关系来预测：损失平坦性意味着神经表示的压缩。我们的结果与\\citet{ma_linear_2021}的先前研究密切相关，该研究展示了平坦性（即小特征值）与表示流形的体积压缩之间的关系。",
    "tldr": "该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。",
    "en_tdlr": "This paper investigates the connection between loss flatness and compressed representations in deep neural networks, demonstrating that loss flatness implies compression of neural representations through a simple mathematical relationship."
}