{
    "title": "Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])",
    "abstract": "Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\\epsilon,\\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine",
    "link": "http://arxiv.org/abs/2310.19973",
    "context": "Title: Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])\nAbstract: Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\\epsilon,\\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine",
    "path": "papers/23/10/2310.19973.json",
    "total_tokens": 865,
    "translated_title": "通过$f$-差分隐私统一增强混合机制的隐私边界",
    "translated_abstract": "差分隐私（DP）机器学习算法会产生许多随机性，如随机初始化、随机批次抽样和洗牌。然而，由于这些随机性会导致难以分析的混合分布，所以在证明差分隐私边界时很难将其纳入考虑。本文旨在改进洗牌模型和一次迭代的差分隐私梯度下降（DP-GD）中用于随机初始化的隐私边界，采用$f$-DP方法。我们导出了洗牌模型的折衷函数的闭式表达式，优于基于$(\\epsilon,\\delta)$-DP的最新结果。此外，我们还对随机初始化对一次迭代的DP-GD的隐私性进行了研究。我们对折衷函数的数值计算表明，随机初始化可以增强DP-GD的隐私性。我们对这些混合机制的$f$-DP保证的分析依赖于一种不等式。",
    "tldr": "本文通过$f$-差分隐私方法改进了洗牌模型和DP-GD中随机初始化的隐私边界，折衷函数的闭式表达式优于$(\\epsilon,\\delta)$-DP的结果，并且随机初始化可以增强DP-GD的隐私性。"
}