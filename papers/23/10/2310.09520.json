{
    "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)",
    "abstract": "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w",
    "link": "http://arxiv.org/abs/2310.09520",
    "context": "Title: Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)\nAbstract: While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w",
    "path": "papers/23/10/2310.09520.json",
    "total_tokens": 908,
    "translated_title": "Reward-Augmented Decoding: 使用单向奖励模型实现高效的受控文本生成",
    "translated_abstract": "尽管大型语言模型已经在许多应用中证明了其有效性，但它们通常生成的文本存在问题或者缺乏所需的属性。本文提出了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，它利用一个小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。具体而言，RAD利用奖励模型对生成的文本进行评分，并通过重新调整采样概率来更倾向于高奖励的标记。通过使用单向奖励模型，RAD能够缓存先前生成步骤的激活值，降低计算开销。通过在生成非有害和情感受控文本方面的实验，我们证明RAD在仅改变生成过程的方法中表现最佳，并且与涉及重新训练语言模型的最先进方法相当。我们进一步验证了RAD在非常大的语言模型上的有效性。",
    "tldr": "该论文介绍了一种名为Reward-Augmented Decoding (RAD)的文本生成方法，使用小型的单向奖励模型来鼓励语言模型生成具有特定属性的文本。RAD在生成非有害和情感受控文本方面表现最佳，并且在非常大的语言模型上也很有效。",
    "en_tdlr": "This paper introduces Reward-Augmented Decoding (RAD), a text generation method that uses a small unidirectional reward model to encourage a language model to generate text with specific properties. RAD performs best in generating non-toxic and sentiment-controlled text, and is effective on very large language models."
}