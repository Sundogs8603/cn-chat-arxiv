{
    "title": "Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])",
    "abstract": "Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D",
    "link": "http://arxiv.org/abs/2310.16655",
    "context": "Title: Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])\nAbstract: Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D",
    "path": "papers/23/10/2310.16655.json",
    "total_tokens": 975,
    "translated_title": "从图像中实现强化学习的控制中心表示的研究",
    "translated_abstract": "基于图像的强化学习是一项实际但具有挑战性的任务。其中一个主要障碍在于提取控制中心的表示，同时忽略不相关的信息。虽然遵循等仿函式原则的方法展示了学习状态表示来解决这个问题的潜力，但它们仍然面临着潜在动力学的有限表达能力和适应稀疏奖励环境的困难。为了解决这些限制，我们引入了ReBis，旨在通过将免奖励控制信息与奖励特定知识集成来捕捉控制中心信息。ReBis利用变形器架构隐式建模动力学，并结合分块掩码消除时空冗余。此外，ReBis将等仿函式损失与非对称重建损失相结合，以防止在稀疏奖励环境中的特征崩溃。在Atari游戏和...",
    "tldr": "该论文提出了一个名为ReBis的方法，通过整合无奖励控制信息和奖励特定知识，来捕捉图像中的控制中心信息。ReBis利用变形器架构来建模动力学，并通过分块掩码消除时空冗余。此外，ReBis还结合了等仿函式损失和非对称重建损失，以防止在稀疏奖励环境中的特征崩溃。"
}