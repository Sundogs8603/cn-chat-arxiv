{
    "title": "Understanding writing style in social media with a supervised contrastively pre-trained transformer. (arXiv:2310.11081v1 [cs.CL])",
    "abstract": "Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 auth",
    "link": "http://arxiv.org/abs/2310.11081",
    "context": "Title: Understanding writing style in social media with a supervised contrastively pre-trained transformer. (arXiv:2310.11081v1 [cs.CL])\nAbstract: Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 auth",
    "path": "papers/23/10/2310.11081.json",
    "total_tokens": 896,
    "translated_title": "使用有监督的对比预训练变换器理解社交媒体中的写作风格",
    "translated_abstract": "在线社交网络成为有害行为的肥沃土壤，从仇恨言论到虚假信息的传播。恶意行为者现在有了前所未有的自由来行使不当行为，导致严重的社会动荡和严重后果，如美国总统选举期间的国会山袭击事件和COVID-19大流行期间的反疫苗运动。理解在线语言比以往任何时候都更加迫切。虽然现有的研究主要关注内容分析，但我们的目标是将重点转移到通过将内容与其各自的作者相关联来理解有害行为。许多新颖的方法尝试学习文本中作者的风格特征，但其中许多方法受到小数据集或次优训练损失的限制。为了克服这些限制，我们引入了基于来自公共来源的大型语料库的Style Transformer for Authorship Representations (STAR)进行训练，其语料库规模达到4.5 x 10^6 auth。",
    "tldr": "该论文介绍了一种使用有监督对比预训练变换器(Supervised Contrastively Pre-trained Transformer)来理解社交媒体中的写作风格的方法，该方法能够有效地将内容与其作者相关联，从而更好地理解有害行为。"
}