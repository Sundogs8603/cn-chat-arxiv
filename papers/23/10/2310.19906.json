{
    "title": "Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])",
    "abstract": "The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph",
    "link": "http://arxiv.org/abs/2310.19906",
    "context": "Title: Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])\nAbstract: The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph",
    "path": "papers/23/10/2310.19906.json",
    "total_tokens": 826,
    "translated_title": "可解释的基于原型的图信息瓶颈",
    "translated_abstract": "图神经网络（GNN）的成功导致了对其决策过程的理解和对其预测的解释的需求，这催生了可解释的人工智能（XAI），为黑盒模型提供透明的解释。最近，原型的使用成功提高了模型的可解释性，通过学习原型来暗示影响预测的训练图。然而，这些方法往往会给原型提供来自整个图的过多信息，导致关键子结构的排除或无关子结构的包含，这可以限制模型在下游任务中的解释能力和性能。在这项工作中，我们提出了一种新颖的可解释的GNN框架，称为解释性的基于原型的图信息瓶颈 (PGIB)，将原型学习纳入信息瓶颈框架，为原型提供输入图的关键子图。",
    "tldr": "这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。"
}