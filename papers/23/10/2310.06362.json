{
    "title": "InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective. (arXiv:2310.06362v1 [cs.CL])",
    "abstract": "Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck leads to confusion on analogous classes. To enable the model learn more sufficient representations, we propose a novel replay-based continual text classification method, InfoCL. Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization and better recover the previously learned representations. In addition, InfoCL incorporates an adversarial memory augmentation strategy to alleviate the overfitting problem of replay. Experimental results demo",
    "link": "http://arxiv.org/abs/2310.06362",
    "context": "Title: InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective. (arXiv:2310.06362v1 [cs.CL])\nAbstract: Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck leads to confusion on analogous classes. To enable the model learn more sufficient representations, we propose a novel replay-based continual text classification method, InfoCL. Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization and better recover the previously learned representations. In addition, InfoCL incorporates an adversarial memory augmentation strategy to alleviate the overfitting problem of replay. Experimental results demo",
    "path": "papers/23/10/2310.06362.json",
    "total_tokens": 925,
    "translated_title": "InfoCL：从信息论的角度缓解连续文本分类中的灾难性遗忘",
    "translated_abstract": "连续学习（CL）旨在不断学习新知识的同时避免对旧任务的灾难性遗忘。我们集中于在类增量设置下的连续文本分类。最近的CL研究已经确定了类比类上的性能严重降低是灾难性遗忘的一个关键因素。通过对CL中表示学习过程的深入探索，我们发现信息瓶颈的压缩效应导致了类比类的混淆。为了使模型学习更充分的表示，我们提出了一种新的基于回放的连续文本分类方法InfoCL。我们的方法利用快慢和当前过去对比学习来进行互信息最大化，并更好地恢复之前学到的表示。此外，InfoCL还采用对抗性记忆增强策略来缓解回放的过拟合问题。实验结果表明…",
    "tldr": "在连续文本分类中，通过深入探索表示学习过程，我们发现信息瓶颈的压缩效应导致类比类的混淆。为了解决这个问题，我们提出了一种新的回放式连续文本分类方法InfoCL，利用快速学习和对比学习来最大化互信息，并通过对抗性记忆增强来缓解过拟合问题。",
    "en_tdlr": "In continual text classification, we discovered that the compression effect of the information bottleneck leads to confusion on analogous classes. To address this, we propose a novel replay-based method called InfoCL, which utilizes fast-slow and contrastive learning to maximize mutual information and incorporates adversarial memory augmentation to alleviate overfitting."
}