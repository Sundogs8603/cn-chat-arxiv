{
    "title": "Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)",
    "abstract": "Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper",
    "link": "http://arxiv.org/abs/2310.04742",
    "context": "Title: Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)\nAbstract: Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper",
    "path": "papers/23/10/2310.04742.json",
    "total_tokens": 889,
    "translated_title": "参数高效的多任务模型融合与部分线性化",
    "translated_abstract": "大型预训练模型在机器学习中取得了重大进展，并成为基础组件。模型融合方法，如任务算法，已被证明具有强大的可扩展性，可以将来自不同任务的微调权重合并到一个多任务模型中。然而，对于在多个下游任务上高效微调大型预训练模型仍然具有挑战性，导致多任务模型融合效率低下。在这项工作中，我们提出了一种改进多任务融合的新方法，适用于像LoRA微调这样的参数高效微调技术。具体而言，我们的方法仅部分线性化适配器模块，并在线性化的适配器上应用任务算法。这样一来，我们可以利用模型融合优势来进行线性化微调，同时保持微调和推理的效率。我们证明，我们的部分线性化技术使多个任务更有效地融合到单个模型中，表现更好。",
    "tldr": "本文提出了一种参数高效的多任务模型融合方法，通过部分线性化适配器模块，并应用任务算法，实现了对大型预训练模型在多个下游任务上的高效微调，从而提高了多任务模型融合的效果和效率。",
    "en_tdlr": "This paper proposes a parameter efficient multi-task model fusion method by partially linearizing adapter modules and applying task arithmetic, which achieves efficient fine-tuning of large pre-trained models on multiple downstream tasks and improves the effectiveness and efficiency of multi-task model fusion."
}