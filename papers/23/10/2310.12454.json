{
    "title": "Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models. (arXiv:2310.12454v1 [cs.CL])",
    "abstract": "Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the e",
    "link": "http://arxiv.org/abs/2310.12454",
    "context": "Title: Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models. (arXiv:2310.12454v1 [cs.CL])\nAbstract: Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the e",
    "path": "papers/23/10/2310.12454.json",
    "total_tokens": 835,
    "translated_title": "重新思考构建用于理解预训练语言模型机制的有效度量方法",
    "translated_abstract": "预训练语言模型被期望能够有效地将输入文本映射到一组向量，同时保留文本内在的关系。因此，设计一个白盒模型来计算反映这些向量内部关系存在的度量成为分析预训练语言模型后续可解释性的常见方法。然而，在源模型缺乏内在可解释性时，在白盒模型中实现可解释性并保证度量计算的严谨性变得具有挑战性。因此，本文讨论了在这种权衡中寻找平衡的方法，并提出了一种新颖的方法来构建理解预训练语言模型机制的度量方法。我们特别设计了一系列沿这一研究方向的度量方法，并使用这些方法中的树拓扑探针模型对BERT-large进行了测量。",
    "tldr": "本研究重新思考了构建用于理解预训练语言模型机制的有效度量方法。通过设计一系列度量方法，并使用树拓扑探针模型对BERT-large进行了实证研究。",
    "en_tdlr": "This study rethinks the construction of effective metrics for understanding the mechanisms of pretrained language models. By designing a series of metrics and conducting empirical research using the tree topological probe model on BERT-large."
}