{
    "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately. (arXiv:2310.03046v1 [cs.SE])",
    "abstract": "Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before ",
    "link": "http://arxiv.org/abs/2310.03046",
    "context": "Title: EcoAssistant: Using LLM Assistant More Affordably and Accurately. (arXiv:2310.03046v1 [cs.SE])\nAbstract: Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before ",
    "path": "papers/23/10/2310.03046.json",
    "total_tokens": 875,
    "translated_title": "EcoAssistant: 更经济、更准确地使用LLM助手",
    "translated_abstract": "如今，用户使用大型语言模型（LLM）作为助手来回答需要外部知识的查询；他们询问特定城市的天气情况，股票价格，甚至询问自己附近的特定地点在哪里。这些查询要求LLM生成调用外部API的代码来回答用户的问题，但LLM很少能一次性生成正确的代码，因此需要在执行结果上进行迭代的代码优化。此外，使用LLM助手支持高查询量可能会很昂贵。在本工作中，我们提出了一个框架，EcoAssistant，使LLM能够更经济、更准确地回答代码驱动的查询。EcoAssistant包含三个组件。首先，它允许LLM助手与自动代码执行器对话，以迭代优化代码或根据执行结果生成答案。其次，我们使用了LLM助手的层次结构，在尝试用较弱、更便宜的LLM回答查询之前。",
    "tldr": "EcoAssistant是一个框架，旨在使LLM助手更经济、更准确地回答代码驱动的查询，并且包含了与自动代码执行器对话和使用层次结构的LLM助手的功能。"
}