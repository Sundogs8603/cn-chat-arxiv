{
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
    "abstract": "arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-",
    "link": "https://arxiv.org/abs/2310.06625",
    "context": "Title: iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\nAbstract: arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-",
    "path": "papers/23/10/2310.06625.json",
    "total_tokens": 809,
    "translated_title": "iTransformer: 反转Transformer在时间序列预测中是有效的",
    "translated_abstract": "最近线性预测模型的兴起对基于Transformer的预测器的架构修改的持续热情提出了质疑。这些预测器利用Transformer来模拟对时间序列的时间标记的全局依赖关系，每个时间标记由相同时间戳的多个变量组成。然而，由于性能下降和计算爆炸，Transformer在预测具有更大回溯窗口的系列时受到挑战。此外，每个时间标记的嵌入融合了代表潜在延迟事件和不同物理测量的多个变量，这可能会导致无法学习变量-centric表示并导致无意义的注意力映射。在这项工作中，我们反思了Transformer组件的能力，并重新利用了Transformer架构，而没有修改基本组件。我们提出了iTransformer，它简单地应用了注意力和馈送-",
    "tldr": "iTransformer通过重新利用Transformer架构，在时间序列预测中简单应用注意力和馈送，提高了性能并克服了其他模型在处理具有更大回溯窗口的系列时面临的挑战",
    "en_tdlr": "iTransformer improves performance and overcomes challenges faced by other models in handling series with larger lookback windows by repurposing the Transformer architecture to simply apply attention and feed-forward mechanisms for time series forecasting."
}