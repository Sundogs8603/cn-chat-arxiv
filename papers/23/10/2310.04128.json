{
    "title": "Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])",
    "abstract": "Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementatio",
    "link": "http://arxiv.org/abs/2310.04128",
    "context": "Title: Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])\nAbstract: Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementatio",
    "path": "papers/23/10/2310.04128.json",
    "total_tokens": 915,
    "translated_title": "以快速遗忘记忆为特点的强化学习",
    "translated_abstract": "几乎所有的现实世界任务都是部分可观测的，必须在强化学习（RL）中使用记忆。大多数无模型方法使用从监督学习（SL）借用的记忆模型将轨迹汇总为潜在的马尔可夫状态，尽管RL往往表现出不同的训练和效率特性。为了解决这个差异，我们引入了快速遗忘记忆，这是一种针对RL特别设计的算法无关的记忆模型。我们的方法通过受计算心理学启发的强结构先验来限制模型搜索空间。它在递归RL算法中可以替代循环神经网络（RNN），在各种递归基准和算法中实现了比RNN更高的奖励，而不需要改变任何超参数。此外，快速遗忘记忆的训练速度比RNN快两个数量级，这归因于它的对数时间和线性空间复杂度。",
    "tldr": "以快速遗忘记忆为特点的算法无关的记忆模型针对强化学习中的部分可观测任务，通过强结构先验限制模型搜索空间，实现了比循环神经网络更高的奖励并具有更快的训练速度。"
}