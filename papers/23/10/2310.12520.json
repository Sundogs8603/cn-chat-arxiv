{
    "title": "Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond. (arXiv:2310.12520v1 [cs.CL])",
    "abstract": "Recent advancements in multimodal techniques open exciting possibilities for models excelling in diverse tasks involving text, audio, and image processing. Models like GPT-4V, blending computer vision and language modeling, excel in complex text and image tasks. Numerous prior research endeavors have diligently examined the performance of these Vision Large Language Models (VLLMs) across tasks like object detection, image captioning and others. However, these analyses often focus on evaluating the performance of each modality in isolation, lacking insights into their cross-modal interactions. Specifically, questions concerning whether these vision-language models execute vision and language tasks consistently or independently have remained unanswered. In this study, we draw inspiration from recent investigations into multilingualism and conduct a comprehensive analysis of model's cross-modal interactions. We introduce a systematic framework that quantifies the capability disparities be",
    "link": "http://arxiv.org/abs/2310.12520",
    "context": "Title: Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond. (arXiv:2310.12520v1 [cs.CL])\nAbstract: Recent advancements in multimodal techniques open exciting possibilities for models excelling in diverse tasks involving text, audio, and image processing. Models like GPT-4V, blending computer vision and language modeling, excel in complex text and image tasks. Numerous prior research endeavors have diligently examined the performance of these Vision Large Language Models (VLLMs) across tasks like object detection, image captioning and others. However, these analyses often focus on evaluating the performance of each modality in isolation, lacking insights into their cross-modal interactions. Specifically, questions concerning whether these vision-language models execute vision and language tasks consistently or independently have remained unanswered. In this study, we draw inspiration from recent investigations into multilingualism and conduct a comprehensive analysis of model's cross-modal interactions. We introduce a systematic framework that quantifies the capability disparities be",
    "path": "papers/23/10/2310.12520.json",
    "total_tokens": 902,
    "translated_title": "迷失在翻译中：当GPT-4V(ision)无法与文本一致时。VLLM和更多的视觉语言一致性分析。 (arXiv:2310.12520v1 [cs.CL])",
    "translated_abstract": "最近多模态技术的进展为在涉及文本、音频和图像处理的多样任务中表现出色的模型开辟了令人兴奋的可能性。像将计算机视觉和语言建模结合在一起的GPT-4V这样的模型，在复杂的文本和图像任务上表现出色。以往的研究努力已经认真地考察了这些视觉大语言模型(VLLMs)在对象检测、图像字幕等任务中的性能。然而，这些分析往往集中在评估每种模态在单独任务中的性能，缺乏对它们跨模态交互的洞察。具体问题关于这些视觉语言模型是否一致地执行视觉和语言任务，还是独立地执行，仍然没有答案。在本研究中，我们借鉴了对多语言的最新研究，并对模型的跨模态交互进行了全面分析。我们引入了一个系统框架，量化了能力差异。",
    "tldr": "本研究通过对视觉语言一致性进行全面分析，揭示了视觉大语言模型在跨模态任务中的能力差异。",
    "en_tdlr": "This study reveals the capability disparities of Vision Large Language Models in cross-modal tasks through a comprehensive analysis of vision-language consistency."
}