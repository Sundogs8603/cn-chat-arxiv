{
    "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])",
    "abstract": "Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models",
    "link": "http://arxiv.org/abs/2310.02410",
    "context": "Title: Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])\nAbstract: Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models",
    "path": "papers/23/10/2310.02410.json",
    "total_tokens": 965,
    "translated_title": "量化专家混合（MoQE）：低位量化和鲁棒性的互补效应",
    "translated_abstract": "大规模的专家混合（MoE）模型在各种语言任务中，包括机器翻译任务，通过专家并行性的高效模型扩展能力，实现了最先进的质量。然而，在部署时，这也带来了更大的内存消耗和增加的内存带宽瓶颈的基本问题。在本文中，我们提出了量化专家混合（MoQE），这是一种简单的仅将专家权重应用于超低位2位量化的量化方法，以减轻MoE模型的增大内存和延迟问题。我们展示了低位量化与MoE架构结合，即使在大多数情况下不需要任何额外的训练，也能提供可靠的模型性能，并且显著减小内存大小。特别地，MoE模型中的专家层对量化比传统的前馈网络（FFN）层更具鲁棒性。在我们的综合分析中，我们展示了MoE模型...",
    "tldr": "本文提出了一种量化专家混合（MoQE）方法，通过将专家权重应用2位低位量化，减轻了大规模专家混合（MoE）模型在内存消耗和延迟问题上的压力，同时在大多数情况下不需要额外训练也能保持可靠的模型性能。",
    "en_tdlr": "This paper proposes a method called Mixture of Quantized Experts (MoQE), which mitigates the memory and latency issues of large Mixture of Experts (MoE) models by applying 2-bit low-bit quantization to expert weights. It shows that this approach can significantly reduce memory consumption without additional training and maintain reliable model performance."
}