{
    "title": "Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language. (arXiv:2310.13540v1 [cs.IR])",
    "abstract": "With the thriving of pre-trained language model (PLM) widely verified in various of NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel Unified pre-trained language model enhanced sequential recommendation (UPSR), aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise & ambiguity, and text length, to guide the text->item adaptation and behavior sequence->text sequence adaptation differently for pre-training and fine-tuning ",
    "link": "http://arxiv.org/abs/2310.13540",
    "context": "Title: Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language. (arXiv:2310.13540v1 [cs.IR])\nAbstract: With the thriving of pre-trained language model (PLM) widely verified in various of NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel Unified pre-trained language model enhanced sequential recommendation (UPSR), aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise & ambiguity, and text length, to guide the text->item adaptation and behavior sequence->text sequence adaptation differently for pre-training and fine-tuning ",
    "path": "papers/23/10/2310.13540.json",
    "total_tokens": 947,
    "translated_title": "全面将多领域预训练推荐建模为语言",
    "translated_abstract": "随着预训练语言模型（PLM）在各种自然语言处理任务中的广泛应用，先驱性工作试图探索将PLM中的通用文本信息与用户历史行为序列中的个性化行为信息相结合，以增强顺序推荐（SR）。然而，尽管输入格式和任务目标存在共性，行为和文本信息之间存在巨大差距，这阻碍了将SR作为语言建模完全建模。为了填补这一差距，我们提出了一种新颖的统一预训练语言模型增强顺序推荐（UPSR）方法，旨在构建一个统一的预训练推荐模型用于多领域推荐任务。我们正式设计了自然性、领域一致性、信息性、噪声和模糊性以及文本长度等五个关键指标，分别用于指导预训练和微调过程中的文本->物品适应和行为序列->文本序列适应。",
    "tldr": "本研究提出了一种新颖的统一预训练语言模型增强顺序推荐方法（UPSR），旨在构建一个统一的预训练推荐模型用于多领域推荐任务。研究者设计了五个关键指标来指导预训练和微调过程中的文本->物品适应和行为序列->文本序列适应。"
}