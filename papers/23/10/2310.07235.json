{
    "title": "Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])",
    "abstract": "While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O",
    "link": "http://arxiv.org/abs/2310.07235",
    "context": "Title: Are GATs Out of Balance?. (arXiv:2310.07235v1 [cs.LG])\nAbstract: While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. O",
    "path": "papers/23/10/2310.07235.json",
    "total_tokens": 938,
    "translated_title": "GATs是否失衡？",
    "translated_abstract": "虽然图神经网络（GNNs）的表达能力和计算能力已经在理论上得到了研究，但它们的优化和学习动态在大多数情况下仍然未被探索。我们的研究针对图注意力网络（GAT），这是一种流行的GNN架构，其中节点的邻域聚合由参数化的注意力系数加权。我们推导出GAT梯度流动力学的守恒定律，这解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。这种效应在深层的GAT中被放大，它们的性能要明显差于浅层的GAT。为了缓解这个问题，我们设计了一种平衡GAT网络的初始化方案。我们的方法 i) 可以更有效地传播梯度，从而使深层网络可训练，ii) 相比于标准初始化，可以实现训练和收敛时间的显著加速。",
    "tldr": "本研究揭示了Graph Attention Network (GAT)梯度流动力学的守恒定律，解释了为什么标准初始化下的GAT中高比例的参数在训练过程中很难改变。我们还提出了一种平衡GAT网络的初始化方案，使得深层网络更容易进行训练，并且相比标准初始化，具有更快的收敛速度。"
}