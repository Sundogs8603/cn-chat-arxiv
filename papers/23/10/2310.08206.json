{
    "title": "Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])",
    "abstract": "Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \\textbf{\\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig",
    "link": "http://arxiv.org/abs/2310.08206",
    "context": "Title: Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])\nAbstract: Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \\textbf{\\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig",
    "path": "papers/23/10/2310.08206.json",
    "total_tokens": 963,
    "translated_title": "基于粗粒度引导森林和多中心损失的长尾分类",
    "translated_abstract": "长尾分类是现实世界中不可避免且具有挑战性的问题。大部分现有的长尾分类方法仅关注解决类间不平衡，即头部类别的样本比尾部类别的样本多，而忽略了类内不平衡，即同一类别中头部属性样本数量远大于尾部属性样本数量。模型的偏差是由这两个因素引起的，由于大多数数据集中的属性是隐含的且属性组合非常复杂，处理类内不平衡更加困难。为此，我们提出了一种基于粗粒度引导森林（CLF）和多中心损失（MCL）的长尾分类框架，名为Cognisance，旨在通过不变特征学习构建多粒度联合解决模型。在这个方法中，我们设计了一种新颖的样本选择策略和损失函数，以平衡不同类别和属性之间的样本分布。",
    "tldr": "本论文提出了一种基于粗粒度引导森林和多中心损失的长尾分类框架，名为Cognisance。该框架致力于解决长尾分类问题中的类间和类内不平衡，并通过不变特征学习构建多粒度联合解决模型。"
}