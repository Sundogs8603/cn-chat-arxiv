{
    "title": "Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])",
    "abstract": "Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiment",
    "link": "http://arxiv.org/abs/2310.16400",
    "context": "Title: Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])\nAbstract: Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiment",
    "path": "papers/23/10/2310.16400.json",
    "total_tokens": 981,
    "translated_title": "融合潜变扩散模型的视频编辑：多源潜变扩散模型",
    "translated_abstract": "潜变扩散模型（LDM）以其在图像和视频合成方面的强大能力而闻名。然而，视频编辑方法存在着预训练数据不足或视频逐帧重新训练成本高的问题。为了解决这个问题，我们提出了FLDM（融合潜变扩散模型），这是一个无需训练的框架，通过在视频LDM中应用现成的图像编辑方法来实现基于文本的视频编辑。具体而言，FLDM在去噪过程中融合了图像LDM和视频LDM的潜变。这样，可以保持视频LDM的时间一致性，同时也可以利用图像LDM的高保真度。同时，由于图像LDM和视频LDM都可以替换，所以FLDM具有很高的灵活性，可以利用高级图像编辑方法，如InstructPix2Pix和ControlNet。据我们所知，FLDM是第一种将现成的图像编辑方法应用于视频LDM进行视频编辑的方法。进行了广泛的定量和定性实验。",
    "tldr": "本论文提出了一种名为FLDM的无需训练的框架，通过融合图像 Latent Diffusion Model（LDM）和视频 LDM，在视频编辑过程中实现了文本引导的视频编辑。这一方法既保持了视频的时间一致性，又利用了图像 LDM 的高保真度，并且具有灵活性与可替换性。",
    "en_tdlr": "This paper introduces a training-free framework called FLDM, which achieves text-guided video editing by fusing image Latent Diffusion Model (LDM) and video LDM. The proposed method preserves temporal consistency and utilizes the high fidelity of image LDM, while also offering flexibility and replaceability."
}