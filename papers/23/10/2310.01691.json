{
    "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])",
    "abstract": "Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.",
    "link": "http://arxiv.org/abs/2310.01691",
    "context": "Title: Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])\nAbstract: Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.",
    "path": "papers/23/10/2310.01691.json",
    "total_tokens": 767,
    "translated_title": "零样本连续提示传递：在语言模型之间泛化任务语义",
    "translated_abstract": "在自然语言处理（NLP）中，通过调整提示已经成为一种越来越受欢迎的方法，用于将大型语言模型适应特定任务。然而，这些提示，特别是连续提示，在不同模型之间的可传递性仍然是一个挑战。在这项工作中，我们提出了一种零样本连续提示传递方法，其中源提示被编码到相对空间中，并搜索相应的目标提示以将其传递到目标模型。实验结果证实了我们方法的有效性，表明连续提示中的“任务语义”可以在各种语言模型之间泛化。此外，我们发现将来自多个源模型的“任务语义”结合可以进一步增强传递的泛化能力。",
    "tldr": "这项工作提出了一种零样本连续提示传递方法，通过将源提示编码到相对空间中，并搜索相应的目标提示，在不同的语言模型之间实现了任务语义的泛化，实验证实了该方法的有效性。"
}