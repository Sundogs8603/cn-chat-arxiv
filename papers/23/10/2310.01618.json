{
    "title": "Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods. (arXiv:2310.01618v1 [cs.LG])",
    "abstract": "Deep neural networks, despite their success in numerous applications, often function without established theoretical foundations. In this paper, we bridge this gap by drawing parallels between deep learning and classical numerical analysis. By framing neural networks as operators with fixed points representing desired solutions, we develop a theoretical framework grounded in iterative methods for operator equations. Under defined conditions, we present convergence proofs based on fixed point theory. We demonstrate that popular architectures, such as diffusion models and AlphaFold, inherently employ iterative operator learning. Empirical assessments highlight that performing iterations through network operators improves performance. We also introduce an iterative graph neural network, PIGN, that further demonstrates benefits of iterations. Our work aims to enhance the understanding of deep learning by merging insights from numerical analysis, potentially guiding the design of future net",
    "link": "http://arxiv.org/abs/2310.01618",
    "context": "Title: Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods. (arXiv:2310.01618v1 [cs.LG])\nAbstract: Deep neural networks, despite their success in numerous applications, often function without established theoretical foundations. In this paper, we bridge this gap by drawing parallels between deep learning and classical numerical analysis. By framing neural networks as operators with fixed points representing desired solutions, we develop a theoretical framework grounded in iterative methods for operator equations. Under defined conditions, we present convergence proofs based on fixed point theory. We demonstrate that popular architectures, such as diffusion models and AlphaFold, inherently employ iterative operator learning. Empirical assessments highlight that performing iterations through network operators improves performance. We also introduce an iterative graph neural network, PIGN, that further demonstrates benefits of iterations. Our work aims to enhance the understanding of deep learning by merging insights from numerical analysis, potentially guiding the design of future net",
    "path": "papers/23/10/2310.01618.json",
    "total_tokens": 907,
    "translated_title": "操作学习与数值分析相遇：通过迭代方法改进神经网络",
    "translated_abstract": "深度神经网络尽管在许多应用中取得成功，但往往缺乏确立的理论基础。本文通过将深度学习与经典数值分析进行对比，弥合了这一差距。我们将神经网络视为具有固定点的算子，这些固定点代表着期望的解，从而建立了基于算子方程的迭代方法的理论框架。在定义的条件下，我们提出了基于固定点理论的收敛证明。我们证明了流行的架构，如扩散模型和AlphaFold，本质上采用了迭代算子学习。经验证明，通过网络算子进行迭代可以提高性能。我们还介绍了一种迭代图神经网络，PIGN，进一步展示了迭代的好处。我们的工作旨在通过将数值分析的见解与深度学习相结合，增加对深度学习的理解，有可能指导未来网络设计。",
    "tldr": "本文通过将深度学习与经典数值分析进行比较，将神经网络视为具有固定点的算子，并通过迭代方法的收敛证明，改进了神经网络的性能。通过实证评估，我们证明了通过网络算子进行迭代可以提高性能，并通过引入迭代图神经网络PIGN进一步展示了迭代的好处。",
    "en_tdlr": "This paper improves the performance of neural networks by comparing them to operators and using iterative methods based on fixed point theory. The study shows that performing iterations through network operators enhances performance and introduces an iterative graph neural network, PIGN, to further demonstrate the benefits of iterations."
}