{
    "title": "Language Models Hallucinate, but May Excel at Fact Verification",
    "abstract": "arXiv:2310.14564v2 Announce Type: replace  Abstract: Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently \"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insigh",
    "link": "https://arxiv.org/abs/2310.14564",
    "context": "Title: Language Models Hallucinate, but May Excel at Fact Verification\nAbstract: arXiv:2310.14564v2 Announce Type: replace  Abstract: Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently \"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insigh",
    "path": "papers/23/10/2310.14564.json",
    "total_tokens": 887,
    "translated_title": "语言模型存在幻觉，但在事实验证方面可能表现出色",
    "translated_abstract": "自然语言处理(NLP)的最新进展很大程度上要归功于大型语言模型(LLMs)的显著进步。然而，LLMs经常会“幻觉”，导致产生与事实不符的输出。我们精心设计的人类评估证实了严重的幻觉问题，发现即使GPT-3.5的事实输出不到25%。这凸显了事实验证器的重要性，以衡量和激励进展。我们的系统调查确认了LLMs可以被重新用作有效的事实验证器，与人类判断具有很强的相关性。令人惊讶的是，在我们的研究中，表现最差的生成器FLAN-T5-11B作为事实验证器表现最佳，甚至优于GPT3.5和ChatGPT等更优秀的LLMs。深入研究，我们分析了这些LLMs对高质量证据的依赖，以及它们在鲁棒性和泛化能力方面的不足。我们的研究提出了一些见解。",
    "tldr": "语言模型存在幻觉现象，但研究表明它们可以作为有效的事实验证器，甚至在某些情况下胜过更强大的语言模型。",
    "en_tdlr": "Language models exhibit hallucination but can be repurposed as effective fact verifiers, sometimes outperforming more capable models."
}