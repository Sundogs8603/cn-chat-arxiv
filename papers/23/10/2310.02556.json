{
    "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we intr",
    "link": "http://arxiv.org/abs/2310.02556",
    "context": "Title: NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we intr",
    "path": "papers/23/10/2310.02556.json",
    "total_tokens": 921,
    "translated_title": "NOLA: 网络作为低秩随机基向量的线性组合",
    "translated_abstract": "最近，由于大型语言模型（LLMs）在各种下游任务上表现出的惊人少样本性能，它们受到了广泛关注。然而，由于检查点的庞大大小（例如GPT-3的350GB），对所有参数进行微调并为每个下游任务或领域存储一个唯一模型变得不切实际。当前的文献，例如LoRA，展示了对LLM的原始权重进行低秩修改的潜力，从而实现了针对特定任务的模型的高效适应和存储。这些方法可以将微调LLM所需的参数数量减少几个数量级。然而，这些方法面临两个主要限制：1）参数减少受到秩一分解的下界限制，2）减少的程度受到模型架构和选择的秩的严重影响。例如，在更大模型中，即使是秩一分解，参数的数量也可能超过真正需要进行适应的参数数量。在这篇论文中，我们介绍了NOLA，它通过将网络表示为低秩随机基向量的线性组合，解决了这些限制。",
    "tldr": "该论文介绍了一种名为NOLA的方法，该方法通过将网络表示为低秩随机基向量的线性组合来减少语言模型的参数数量，从而实现高效的适应和存储。"
}