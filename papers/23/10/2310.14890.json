{
    "title": "Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])",
    "abstract": "This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\\%, 10\\%, and 40\\% has a worst-class error rate of 40\\%, whereas the average is 20\\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\\% error rate, while the benign and healthy classes have 10\\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.",
    "link": "http://arxiv.org/abs/2310.14890",
    "context": "Title: Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])\nAbstract: This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\\%, 10\\%, and 40\\% has a worst-class error rate of 40\\%, whereas the average is 20\\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\\% error rate, while the benign and healthy classes have 10\\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.",
    "path": "papers/23/10/2310.14890.json",
    "total_tokens": 797,
    "translated_title": "Boosting用于界定最差分类误差",
    "translated_abstract": "本文解决了最差类别误差率的问题，而不是针对所有类别的标准误差率的平均。例如，一个三类别分类任务，其中各类别的误差率分别为10％，10％和40％，其最差类别误差率为40％，而在类别平衡条件下的平均误差率为20％。最差类别错误在许多应用中很重要。例如，在医学图像分类任务中，对于恶性肿瘤类别具有40％的错误率而良性和健康类别具有10％的错误率是不能被接受的。我们提出了一种保证最差类别训练误差上界的提升算法，并推导出其泛化界。实验结果表明，该算法降低了最差类别的测试误差率，同时避免了对训练集的过拟合。",
    "tldr": "该论文提出了一种基于Boosting的算法，可以保证最差类别训练误差的上界，并降低了最差类别的测试误差率。",
    "en_tdlr": "This paper proposes a Boosting algorithm that guarantees an upper bound for the worst-class training error and reduces the worst-class test error rates."
}