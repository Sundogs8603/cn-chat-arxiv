{
    "title": "Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)",
    "abstract": "This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an ",
    "link": "http://arxiv.org/abs/2310.14037",
    "context": "Title: Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)\nAbstract: This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an ",
    "path": "papers/23/10/2310.14037.json",
    "total_tokens": 901,
    "translated_title": "通过视觉模块插件解锁密集检索的多模态能力",
    "translated_abstract": "本文提出了通过视觉模块插件（MARVEL）学习查询和多模态文档的嵌入空间以进行检索的多模态检索模型。MARVEL使用统一的编码器模型对查询和多模态文档进行编码，有助于减小图像和文本之间的模态差距。具体而言，我们通过将视觉模块编码的图像特征作为其输入，使得经过训练的密集检索器T5-ANCE具有图像理解能力。为了促进多模态检索任务，我们基于ClueWeb22数据集构建了ClueWeb22-MM数据集，将锚文本作为查询，并从锚链接的网页中提取相关文本和图像文档。实验证明，MARVEL在多模态检索数据集WebQA和ClueWeb22-MM上明显优于最先进的方法。进一步的分析表明，视觉模块插件方法为实现图像理解能力量身定制。",
    "tldr": "本文介绍了一种名为MARVEL的多模态检索模型，通过视觉模块插件为密集检索器添加图像理解能力，并且在多模态检索任务中取得了显著优于最先进方法的结果。",
    "en_tdlr": "This paper presents a multi-modal retrieval model called MARVEL, which enhances the image understanding ability of a dense retriever by incorporating a visual module plugin. The proposed model outperforms state-of-the-art methods in multi-modal retrieval tasks, achieving significant improvements in the WebQA and ClueWeb22-MM datasets."
}