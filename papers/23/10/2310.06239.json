{
    "title": "Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])",
    "abstract": "Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% a",
    "link": "http://arxiv.org/abs/2310.06239",
    "context": "Title: Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])\nAbstract: Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% a",
    "path": "papers/23/10/2310.06239.json",
    "total_tokens": 881,
    "translated_title": "模型调优还是提示调优？对于临床概念和关系提取的大型语言模型的研究",
    "translated_abstract": "本研究旨在开发基于软提示的大型语言模型（LLMs）学习算法，研究提示的形状、使用冻结/解冻LLMs进行提示调优、迁移学习和少样本学习能力。通过开发了基于软提示的LLM模型，并通过比较四种训练策略（1.无提示微调；2.解冻LLMs的硬提示；3.解冻LLMs的软提示；4.冻结LLMs的软提示）来评估了七个预训练的LLMs在两个基准数据集上的临床概念和关系提取能力。在跨机构环境中评估了基于提示学习算法的迁移学习能力，并评估了少样本学习能力。",
    "tldr": "本研究探索了大型语言模型在临床概念和关系提取任务中的应用，通过软提示调优取得了最佳性能，并观察了迁移学习和少样本学习的能力。",
    "en_tdlr": "This study explores the application of large language models in clinical concept and relation extraction, achieving the best performance through soft prompt tuning, and observing the abilities of transfer learning and few-shot learning."
}