{
    "title": "Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])",
    "abstract": "Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight",
    "link": "http://arxiv.org/abs/2310.03182",
    "context": "Title: Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])\nAbstract: Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight",
    "path": "papers/23/10/2310.03182.json",
    "total_tokens": 997,
    "translated_title": "基于概念瓶颈模型的鲁棒和可解释的医学图像分类器",
    "translated_abstract": "医学图像分类是医疗保健中一个关键问题，有望减轻医生的工作负担并促进对患者的诊断。然而，当将深度学习模型应用于现实世界的医疗保健应用时，存在两个挑战。首先，神经模型往往会学习到不相关的相关性，而不是所期望的特征，这在推广到新领域时可能会有所不足（例如，具有不同年龄的患者）。其次，这些黑盒模型缺乏解释性。在进行诊断预测时，了解模型为何做出决策对于可信度和安全性至关重要。为了解决这两个限制，本文提出了一种新的范例，用自然语言概念构建鲁棒和可解释的医学图像分类器。具体来说，我们首先从GPT-4中查询临床概念，然后利用视觉-语言模型将潜在的图像特征转化为明确的概念。我们对我们的方法在八个场景下进行了系统评估",
    "tldr": "本文提出了一种新的方法来构建鲁棒和可解释的医学图像分类器，通过从GPT-4中查询临床概念，并利用视觉-语言模型将潜在的图像特征转化为明确的概念，以解决在真实世界医疗应用中的两个挑战：学习不想关的相关性和缺乏解释性。"
}