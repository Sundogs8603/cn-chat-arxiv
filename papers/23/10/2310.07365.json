{
    "title": "GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])",
    "abstract": "Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as ",
    "link": "http://arxiv.org/abs/2310.07365",
    "context": "Title: GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])\nAbstract: Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as ",
    "path": "papers/23/10/2310.07365.json",
    "total_tokens": 860,
    "translated_title": "GraphControl:为图领域迁移学习中的通用图预训练模型添加条件控制",
    "translated_abstract": "图结构化数据在世界中无处不在，这种数据模型了对象之间的复杂关系，为各种Web应用提供了可能。Web上每天涌现的无标签图数据为这些应用提供了巨大的潜力。图自监督算法在从丰富的无标签图数据中获得通用知识方面取得了显著成功。这些预训练模型可以应用于各种下游Web应用，节省训练时间，提高下游（目标）性能。然而，即使在表面上看起来相似的领域中，不同的图在属性语义方面也可能存在显着差异，这给将预训练模型迁移到下游任务中带来了困难，甚至是不可行性。具体而言，例如，下游任务中的附加特定任务节点信息（特异性）通常会被有意省略，以便利用预训练表示（可迁移性）。这种权衡被称为",
    "tldr": "在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。",
    "en_tdlr": "GraphControl enables effective transfer of universal graph pre-trained models in graph domain transfer learning by adding conditional control, overcoming the issue of attribute semantic differences across different graph domains."
}