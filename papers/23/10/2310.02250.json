{
    "title": "Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$ into $\\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\\mathbb{R}^k$ back into $\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne",
    "link": "http://arxiv.org/abs/2310.02250",
    "context": "Title: Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)\nAbstract: Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$ into $\\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\\mathbb{R}^k$ back into $\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne",
    "path": "papers/23/10/2310.02250.json",
    "total_tokens": 912,
    "translated_title": "为什么自编码器起作用？",
    "translated_abstract": "深度神经网络自编码器被广泛用于模型压缩。它们可以识别数据在输入的欧几里德空间R^n中，位于k维子集K中的内在维度。其基本思想是获得一个将R^n映射为R^k的编码层（称为瓶颈层或潜变量空间），以及一个将R^k映射回R^n的解码层，使得在组合这两个映射时可以恢复来自集合K的输入数据。这通过调整网络中的参数（权重）来最小化输入和重构输出之间的差异来实现。由于神经网络（具有连续激活函数）计算连续映射，实现完美重构的网络的存在将意味着K在R^k中是一个k维子集的同胚，因此明显存在拓扑障碍来寻找这样的网络。",
    "tldr": "自编码器是一种深度神经网络模型，通过调整参数实现输入数据和重构输出之间的最小差异，用于识别数据在高维空间中的内在维度，并且对于某些拓扑结构，存在难以找到完美重构网络的限制。",
    "en_tdlr": "Autoencoders are deep neural network models that use parameter adjustment to minimize the difference between input data and reconstructed output. They are used to identify the intrinsic dimensions of data in high-dimensional spaces and face limitations in finding perfect reconstruction networks for certain topological structures."
}