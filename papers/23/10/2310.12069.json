{
    "title": "Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])",
    "abstract": "The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.",
    "link": "http://arxiv.org/abs/2310.12069",
    "context": "Title: Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])\nAbstract: The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.",
    "path": "papers/23/10/2310.12069.json",
    "total_tokens": 742,
    "translated_title": "用于科学数据的Transformer：天文学家的教学综述",
    "translated_abstract": "与ChatGPT和相关生成型人工智能产品相关的深度学习架构被称为Transformer。最初应用于自然语言处理，Transformer和它们利用的自注意机制在自然科学领域引起了广泛关注。本教学和非正式综述的目标是向科学家介绍Transformer。我们的教学和非正式综述包括自注意机制的数学基础，对原始Transformer架构的描述，以及在天文学中应用于时间序列和成像数据的一节。我们还包括了一个常见问题解答部分，供那些对生成型人工智能感兴趣并希望开始使用Transformer进行研究的读者参考。",
    "tldr": "本综述旨在向科学家介绍Transformer的应用，包括自然语言处理和自注意机制。此外，还介绍了在天文学中应用于时间序列和成像数据的具体情况，并提供了常见问题解答部分。",
    "en_tdlr": "This pedagogical review introduces scientists to the application of Transformers, including natural language processing and the self-attention mechanism. Additionally, it discusses the specific applications in astronomy for time series and imaging data, and provides a section for frequently asked questions."
}