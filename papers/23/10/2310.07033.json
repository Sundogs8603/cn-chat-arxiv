{
    "title": "Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])",
    "abstract": "Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D",
    "link": "http://arxiv.org/abs/2310.07033",
    "context": "Title: Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])\nAbstract: Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D",
    "path": "papers/23/10/2310.07033.json",
    "total_tokens": 928,
    "translated_title": "健康系统规模的计算病理学——来自30亿图像的自监督基础模型",
    "translated_abstract": "最近的自监督学习突破使得可以使用大型无标签数据集来训练视觉基础模型，从而可以推广到各种下游任务中。虽然这种训练范式非常适合医学领域，因为标注往往稀缺，但是在医学领域，特别是病理学领域，大规模预训练的研究还不够充分。以往的病理学自监督学习工作利用较小的数据集进行预训练和评估下游性能。该项目的目标是训练最大的学术基础模型，并通过在大规模临床病理学数据集上进行预训练和评估下游性能来评估最显著的自监督学习算法。我们收集了迄今为止最大的病理学数据集，包括来自超过423,000个显微镜幻灯片的30亿张图像。我们比较了使用掩蔽自编码器（MAE）和D来预训练视觉变换器模型。",
    "tldr": "该论文通过使用大规模无标签数据集训练自监督基础模型并在大型临床病理学数据集上进行评估，旨在训练最大的学术基础模型并评估最显著的自监督学习算法。",
    "en_tdlr": "This paper aims to train the largest academic foundation model and evaluate the most prominent self-supervised learning algorithms by using large unlabeled datasets, specifically in the field of pathology."
}