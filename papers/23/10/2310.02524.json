{
    "title": "Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])",
    "abstract": "Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of",
    "link": "http://arxiv.org/abs/2310.02524",
    "context": "Title: Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])\nAbstract: Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of",
    "path": "papers/23/10/2310.02524.json",
    "total_tokens": 776,
    "translated_title": "联邦条件随机优化",
    "translated_abstract": "条件随机优化在机器学习任务中广泛应用，例如不变学习、AUPRC最大化和元学习。随着这些应用中对使用大规模分布式数据进行模型训练的需求增加，对于高效通信的分布式优化算法，例如联邦学习算法的需求也越来越大。本文考虑联邦学习中的非凸条件随机优化，并提出了第一个联邦条件随机优化算法(FCSG)，其中包括条件随机梯度估计器和基于动量的算法(FCSG-M)。为了达到单机设定下的下界复杂度，我们通过方差减少设计了加速算法(Acc-FCSG-M)以实现最佳的样本和通信复杂度。与现有的FL中MAML的优化分析相比，联邦条件随机优化考虑了样本的。",
    "tldr": "本文提出了一种新的联邦条件随机优化算法(FCSG)，针对联邦学习中的非凸条件随机优化问题，通过设计加速算法(Acc-FCSG-M)来实现最佳的样本和通信复杂度。"
}