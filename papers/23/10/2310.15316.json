{
    "title": "Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])",
    "abstract": "The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
    "link": "http://arxiv.org/abs/2310.15316",
    "context": "Title: Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])\nAbstract: The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
    "path": "papers/23/10/2310.15316.json",
    "total_tokens": 882,
    "translated_title": "探测用于文档级事件抽取的表示",
    "translated_abstract": "探测分类器框架已被应用于解释深度神经网络模型在各种自然语言处理（NLP）应用中的表现。然而，研究主要集中在句子级NLP任务上。本研究首次将探测范式应用于学习用于文档级信息抽取（IE）的表示。我们设计了八个嵌入式探针，用于分析与文档级事件抽取相关的表面、语义和事件理解能力。我们将它们应用于从三种不同的基于LLM的文档级IE方法中学习的模型得到的表示上。我们发现，这些模型训练的编码器得到的嵌入可以适度地提高参数检测和标注，但只能微弱地增强事件级任务，尽管在有助于连贯性和事件类型预测的信息上存在一些权衡。我们进一步发现，编码器模型在处理文档长度和跨句子的话语方面存在困难。",
    "tldr": "这项研究首次将探测范式应用于文档级信息抽取表示，发现训练得到的编码器嵌入可以提高参数检测和标注，但对于事件级任务的改进有限，且存在在连贯性和事件类型预测方面的权衡。"
}