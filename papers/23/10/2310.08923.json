{
    "title": "Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning. (arXiv:2310.08923v1 [cs.CL])",
    "abstract": "Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illust",
    "link": "http://arxiv.org/abs/2310.08923",
    "context": "Title: Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning. (arXiv:2310.08923v1 [cs.CL])\nAbstract: Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illust",
    "path": "papers/23/10/2310.08923.json",
    "total_tokens": 873,
    "translated_title": "朝着最大信息增益的信息丰富的少样本提示实现上下文学习",
    "translated_abstract": "大型语言模型具有利用少量与新的下游任务有关的示例进行上下文学习的能力。然而，这种学习范式饱受输入分布、顺序和提示格式等因素引起的高度不稳定性的困扰。在本文中，我们证明，即使当所有这些因素保持不变时，随机选择示例仍然会导致高方差。因此，我们旨在通过量化观察给定示例候选人后预测所获得的信息增益（IG）来探索数据示例的信息能力，然后我们提出从中采样那些具有最大IG的示例。此外，我们发现模板偏差的存在，在采样过程中可能会导致对IG的不公平评估。为了缓解这种偏差，我们引入了采样之前的校准策略。实验结果证明了我们方法的有效性。",
    "tldr": "本文旨在解决大型语言模型在上下文学习中的不稳定性问题，通过量化信息增益来探索数据示例的信息能力，并采用校准策略来缓解模板偏差。实验证明了该方法的有效性。",
    "en_tdlr": "This paper aims to address the instability issue in in-context learning of large language models. It explores the information capability of data examples by quantifying information gain and mitigates template bias with a calibration strategy. Experimental results demonstrate the effectiveness of the proposed approach."
}