{
    "title": "Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])",
    "abstract": "Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge",
    "link": "http://arxiv.org/abs/2310.18285",
    "context": "Title: Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])\nAbstract: Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge",
    "path": "papers/23/10/2310.18285.json",
    "total_tokens": 853,
    "translated_title": "异构联邦学习与群体感知提示调整",
    "translated_abstract": "Transformer在各种机器学习任务中取得了显著的成功，促使它们被广泛采用。本文探索了它们在联邦学习（FL）领域的应用，特别关注具有不同本地数据集的异构场景。为了满足FL的计算和通信需求，我们利用预训练的Transformer，并使用高效的提示调整策略。我们的策略引入了同时学习共享和群体提示的概念，能够同时获取通用知识和群体特定知识。此外，提示选择模块为每个输入分配个性化的群体提示，使全局模型与每个客户端数据分布对齐。这种方法使我们能够训练一个单一的全局模型，能够自动适应不同的本地客户端数据分布，而无需进行本地微调。通过这种方式，我们提出的方法有效地搭建了链接",
    "tldr": "本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。"
}