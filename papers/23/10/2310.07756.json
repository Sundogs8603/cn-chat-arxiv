{
    "title": "Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])",
    "abstract": "Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue t",
    "link": "http://arxiv.org/abs/2310.07756",
    "context": "Title: Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])\nAbstract: Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue t",
    "path": "papers/23/10/2310.07756.json",
    "total_tokens": 873,
    "translated_title": "从随机数据投影器进行无监督表示学习",
    "translated_abstract": "通过利用人工设计的数据增强方法下的变换不变性假设，自监督表示学习（SSRL）已经取得了显著的进展。虽然基于增强的SSRL算法在计算机视觉和自然语言处理中推动了性能的提升，但它们通常不适用于其他数据模态，并且可能与应用特定的数据增强约束冲突。本文提出了一种SSRL方法，可以应用于任何数据模态和网络架构，因为它不依赖于增强或掩蔽。具体而言，我们通过重建随机数据投影来学习高质量的数据表示。我们对跨多种模态和实际应用的表示学习任务进行了评估，结果表明它优于多个最先进的SSRL基线模型。由于其广泛适用性和强大的实证结果，我们认为...",
    "tldr": "本文提出了一种无监督表示学习（SSRL）方法，通过重建随机数据投影来学习高质量的数据表示，不依赖于增强或掩蔽技术，可以应用于任何数据模态和网络架构。实验结果表明该方法在各种任务中优于其他SSRL算法。",
    "en_tdlr": "This paper presents a self-supervised representation learning (SSRL) approach that learns high-quality data representations by reconstructing random data projections, without relying on augmentation or masking techniques. It can be applied to any data modality and network architecture, and outperforms other SSRL algorithms in various tasks."
}