{
    "title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention. (arXiv:2310.07911v1 [cs.CL])",
    "abstract": "Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.",
    "link": "http://arxiv.org/abs/2310.07911",
    "context": "Title: Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention. (arXiv:2310.07911v1 [cs.CL])\nAbstract: Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.",
    "path": "papers/23/10/2310.07911.json",
    "total_tokens": 876,
    "translated_title": "一个对多个部分进行对比的方法：利用注意力头嵌入以实现参数有效的多头注意力机制",
    "translated_abstract": "在各种自然语言处理任务中，扩展预训练语言模型已经在性能上取得了巨大的提升，但这也带来了大量的内存需求。受Transformer中位置嵌入的启发，我们旨在简化和减少多头注意力机制的内存占用。我们提出了一种替代模块，只使用一个共享的投影矩阵和多个头部嵌入（MHE），即每个头部一个。我们经验证明，相比于其他注意力机制，我们的MHE注意力在内存使用效率上更高，同时在多项下游任务中实现了与传统的多头注意力机制相当的预测性能保持比。与单头注意力相比，MHE注意力仅需要额外的参数（$3nd$，其中$n$是注意力头的数量，$d$是头嵌入的大小）的微不足道的部分，而多头注意力则需要$(3n^2-3n)d^2-3nd$个额外参数。",
    "tldr": "本论文提出一种利用注意力头嵌入来实现参数有效的多头注意力机制，该机制在多项下游任务中取得了与传统多头注意力机制相当的预测性能，同时显著减少内存占用。",
    "en_tdlr": "This paper proposes a parameter-efficient multi-head attention mechanism that leverages attention-head embeddings, achieving comparable predictive performance to traditional multi-head attention while significantly reducing memory requirements."
}