{
    "title": "Splitting the Difference on Adversarial Training. (arXiv:2310.02480v1 [cs.LG])",
    "abstract": "The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes: \"clean\" and \"adversarial.\" This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust",
    "link": "http://arxiv.org/abs/2310.02480",
    "context": "Title: Splitting the Difference on Adversarial Training. (arXiv:2310.02480v1 [cs.LG])\nAbstract: The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes: \"clean\" and \"adversarial.\" This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust",
    "path": "papers/23/10/2310.02480.json",
    "total_tokens": 887,
    "translated_title": "对抗训练中的差异性分割",
    "translated_abstract": "对抗性样本的存在指出了深度神经网络的一个基本弱点。对抗训练作为针对此类样本最有效的防御方法，需要在某种程度上训练模型以提高鲁棒性，通常以自然准确性的降低为代价。大多数对抗训练方法的目标是学习模型，为每个类别找到一个共同的决策边界，涵盖了干净和扰动的样本。在这项工作中，我们采取了一个根本不同的方法，将每个类别的扰动样本视为一个单独的需要学习的类别，有效地将每个类别分为两个类别：\"干净\"和\"对抗性\"。这种分割使得需要学习的类别数量翻倍，但同时大大简化了决策边界。我们提供了一种理论上的合理性论证，以阐明我们的方法有望在何种条件下有益。同样，我们通过实验证明了我们的方法学习到了鲁棒的模型。",
    "tldr": "本文提出了一种基于分割类别的对抗训练方法，将每个类别的扰动样本视为单独的类别进行学习，从而简化了决策边界，提高了模型的鲁棒性。",
    "en_tdlr": "This paper proposes an adversarial training method that splits the perturbed examples of each class into separate classes to simplify decision boundaries and improve model robustness."
}