{
    "title": "Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])",
    "abstract": "Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.",
    "link": "http://arxiv.org/abs/2310.12408",
    "context": "Title: Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])\nAbstract: Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.",
    "path": "papers/23/10/2310.12408.json",
    "total_tokens": 761,
    "translated_title": "通过梯度特征学习证明神经网络的可靠性",
    "translated_abstract": "神经网络在实践中取得了显著的表现，但目前的理论分析不足以理解其成功，例如神经切线核方法无法捕捉到其关键的特征学习能力，而最近对特征学习的分析通常是问题特定的。本研究提出了一个统一的分析框架，针对由梯度下降训练的双层网络。该框架以梯度特征学习原理为核心，并通过在几个典型问题中的应用来证明其有效性，例如高斯混合和奇偶函数。该框架还揭示了有趣的网络学习现象，如超越核的特征学习和彩票票据假设。",
    "tldr": "本研究提出了一个针对梯度特征学习的统一分析框架，证明了双层神经网络在训练过程中的可靠性，并在多个典型问题上展示了其有效性和有趣的学习现象。",
    "en_tdlr": "This paper proposes a unified analysis framework for gradient feature learning in two-layer neural networks, demonstrating their reliability during the training process and showcasing their effectiveness and interesting learning phenomena in various typical problems."
}