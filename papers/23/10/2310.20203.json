{
    "title": "Importance Estimation with Random Gradient for Neural Network Pruning. (arXiv:2310.20203v1 [cs.LG])",
    "abstract": "Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. F",
    "link": "http://arxiv.org/abs/2310.20203",
    "context": "Title: Importance Estimation with Random Gradient for Neural Network Pruning. (arXiv:2310.20203v1 [cs.LG])\nAbstract: Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. F",
    "path": "papers/23/10/2310.20203.json",
    "total_tokens": 984,
    "translated_title": "使用随机梯度进行神经网络剪枝的重要性估计",
    "translated_abstract": "全局神经元重要性估计被用于剪枝神经网络以提高效率。大多数现有方法要么使用激活或梯度信息，要么两者兼有，以确定每个神经元或卷积核的全局重要性，这要求大量有标签的样本。在这项工作中，我们使用启发式方法推导出类似于Taylor一阶（TaylorFO）近似方法的重要性估计。我们将我们的方法命名为TaylorFO-abs和TaylorFO-sq。我们提出了两种额外的方法来改进这些重要性估计方法。首先，我们从网络的最后一层传播随机梯度，从而避免了对有标签的样本的需求。其次，我们对最后一层的梯度幅度进行归一化，使所有样本对重要性得分的贡献相似。我们的方法结合额外的技术在CIFAR-100和STL-10数据集上测试时表现优于先前的方法。",
    "tldr": "本文提出了一种使用随机梯度进行神经网络剪枝的重要性估计方法，该方法通过启发式推导出Taylor一阶近似方法。此外，通过在传播过程中使用随机梯度和归一化处理，可以避免对有标签样本的需求，并使得所有样本对重要性得分的贡献相似。在实验中，该方法相比于先前方法在不同数据集和架构上表现更好。",
    "en_tdlr": "This paper proposes a method for estimating the importance of neurons in neural network pruning using random gradients, which is derived heuristically based on a first-order Taylor approximation. Additionally, by propagating random gradients and normalizing the gradient magnitude, the method eliminates the need for labeled examples and ensures that all examples contribute similarly to the importance score. Experimental results show that the proposed method outperforms previous methods on different datasets and architectures."
}