{
    "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])",
    "abstract": "Recent research suggests that the feed-forward module within Transformers can be viewed as a collection of key-value memories, where the keys learn to capture specific patterns from the input based on the training examples. The values then combine the output from the 'memories' of the keys to generate predictions about the next token. This leads to an incremental process of prediction that gradually converges towards the final token choice near the output layers. This interesting perspective raises questions about how multilingual models might leverage this mechanism. Specifically, for autoregressive models trained on two or more languages, do all neurons (across layers) respond equally to all languages? No! Our hypothesis centers around the notion that during pretraining, certain model parameters learn strong language-specific features, while others learn more language-agnostic (shared across languages) features. To validate this, we conduct experiments utilizing parallel corpora of t",
    "link": "http://arxiv.org/abs/2310.15552",
    "context": "Title: Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])\nAbstract: Recent research suggests that the feed-forward module within Transformers can be viewed as a collection of key-value memories, where the keys learn to capture specific patterns from the input based on the training examples. The values then combine the output from the 'memories' of the keys to generate predictions about the next token. This leads to an incremental process of prediction that gradually converges towards the final token choice near the output layers. This interesting perspective raises questions about how multilingual models might leverage this mechanism. Specifically, for autoregressive models trained on two or more languages, do all neurons (across layers) respond equally to all languages? No! Our hypothesis centers around the notion that during pretraining, certain model parameters learn strong language-specific features, while others learn more language-agnostic (shared across languages) features. To validate this, we conduct experiments utilizing parallel corpora of t",
    "path": "papers/23/10/2310.15552.json",
    "total_tokens": 888,
    "translated_title": "揭示Transformer模型中的多语言性：探索前馈网络中的语言特异性",
    "translated_abstract": "最近的研究表明，Transformer模型中的前馈模块可以被视为一组键值记忆，其中键通过训练样本学习捕捉输入中的特定模式。值将键的“记忆”的输出进行组合，生成关于下一个标记的预测。这导致一种递增的预测过程，逐渐收敛于靠近输出层的最终标记选择。这个有趣的观点引发了对多语言模型如何利用这种机制的问题。具体来说，对于在两种或更多语言上训练的自回归模型，所有神经元（在不同层上）是否都对所有语言作出相同的响应？不是！我们的假设集中在这样一个观念上：在预训练期间，某些模型参数学习了强烈的语言特定特征，而其他参数学习了更多的语言无关特征（共享于多种语言）。为了验证这一点，我们利用平行语料进行实验证明。",
    "tldr": "这项研究揭示了在Transformer模型中，前馈模块可以被视为键值记忆的集合，通过学习特定语言的模式，并结合共享特征，实现多语言模型的预测过程。"
}