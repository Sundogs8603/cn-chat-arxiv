{
    "title": "Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])",
    "abstract": "Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t",
    "link": "http://arxiv.org/abs/2310.17162",
    "context": "Title: Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])\nAbstract: Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t",
    "path": "papers/23/10/2310.17162.json",
    "total_tokens": 918,
    "translated_title": "基于内容的音乐大语言建模的控制",
    "translated_abstract": "近年来，在音乐音频领域出现了大规模语言模型的迅速增长。这些模型使得能够进行高质量音乐的端到端生成，并且一些模型可以使用文本描述进行条件生成。然而，文本在音乐上的控制能力本质上是有限的，因为它们只能通过元数据（如歌手和乐器）或高级表示（如流派和情感）间接地描述音乐。我们的目标是进一步提供对音高、和弦和鼓乐等固有音乐语言的直接和基于内容的控制能力。为此，我们提出了Coco-Mulla，这是一种用于音乐大语言建模的基于内容的控制方法。它使用了针对基于Transformer的音频模型量身定制的参数高效微调（PEFT）方法。实验表明，我们的方法在低资源半监督学习中实现了高质量的音乐生成，相比原始模型，参数调优的比例不到4%。",
    "tldr": "该论文提出了一种基于内容的控制方法，用于音乐大语言建模。通过对音高、和弦和鼓乐等固有音乐语言的直接控制，实现了高质量的音乐生成，并且使用了参数高效微调的方法，比原始模型的参数数量少于4%。",
    "en_tdlr": "This paper proposes a content-based control method for music large language modeling, enabling direct control over intrinsic music languages such as pitch, chords, and drum track. The approach achieves high-quality music generation and employs a parameter-efficient fine-tuning method, with less than 4% of the original model’s parameters."
}