{
    "title": "FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy",
    "abstract": "arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener",
    "link": "https://arxiv.org/abs/2310.09789",
    "context": "Title: FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy\nAbstract: arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener",
    "path": "papers/23/10/2310.09789.json",
    "total_tokens": 662,
    "translated_title": "FLrce: 具有提前停止策略的资源高效联邦学习",
    "translated_abstract": "针对边缘设备资源短缺和不平衡的训练贡献，提出了FLrce方法，通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，在不泄露本地数据的情况下，有效解决了传统联邦学习中存在的安全和资源利用不均衡问题。",
    "tldr": "FLrce方法通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，解决了安全和资源利用不均衡问题。",
    "en_tdlr": "FLrce proposes a resource-efficient federated learning approach by introducing an early-stopping strategy and binary pruning mechanism to address the issues of security and resource utilization imbalance in traditional federated learning."
}