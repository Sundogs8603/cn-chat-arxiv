{
    "title": "A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])",
    "abstract": "The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \\emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.",
    "link": "http://arxiv.org/abs/2310.17813",
    "context": "Title: A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])\nAbstract: The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \\emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.",
    "path": "papers/23/10/2310.17813.json",
    "total_tokens": 842,
    "translated_title": "一个特征学习的光谱条件",
    "translated_abstract": "针对训练规模越来越大的神经网络的推动，本文研究了在大型网络宽度上的初始化和训练。一个关键挑战是对网络的内部表示进行非平凡的演变，即特征学习。研究表明，通过缩放权重矩阵和更新的谱范数，我们可以实现特征学习，缩放系数为$\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$，与基于Frobenius范数和元素大小的启发式缩放方法有所不同。我们的光谱缩放分析还导出了最大更新参数化的基本推导。总之，我们旨在为读者提供对神经网络中特征学习的坚实概念理解。",
    "tldr": "本文研究了在大型神经网络中特征学习的光谱条件，并提出了将权重矩阵和更新的谱范数缩放为$\\sqrt{\\texttt{fan-out}/\\texttt{fan-in}}$的方法，以实现特征学习。同时，作者还导出了最大更新参数化的推导，旨在帮助读者对神经网络中的特征学习理解更加深入。",
    "en_tdlr": "This paper investigates the spectral condition for feature learning in large-scale neural networks, proposing a scaling method for weight matrices and updates based on the spectral norm, aiming to provide a solid conceptual understanding for readers. Additionally, the authors derive the maximal update parametrization based on the spectral scaling analysis."
}