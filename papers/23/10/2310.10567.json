{
    "title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling. (arXiv:2310.10567v2 [cs.CL] UPDATED)",
    "abstract": "Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the ",
    "link": "http://arxiv.org/abs/2310.10567",
    "context": "Title: RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling. (arXiv:2310.10567v2 [cs.CL] UPDATED)\nAbstract: Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the ",
    "path": "papers/23/10/2310.10567.json",
    "total_tokens": 910,
    "translated_title": "RegaVAE：一种检索增强的高斯混合变分自编码器用于语言建模",
    "translated_abstract": "检索增强的语言模型在解决语言模型中的过时信息和虚构现象等问题上表现出了潜力。然而，当前的研究面临两个主要问题：1）确定要检索的信息，2）在生成过程中有效地结合检索的信息。我们认为，有价值的检索信息不仅应与当前的源文本相关，还应考虑到未来的目标文本，因为语言模型是对未来令牌进行建模的。此外，我们提出，使用从紧凑潜在空间中派生的潜在变量进行聚合比使用受上下文长度限制和容易受噪声干扰的显式原始文本更有效。因此，我们引入了RegaVAE，这是一个基于变分自编码器（VAE）构建的检索增强的语言模型。它将文本语料库编码为潜在空间，从源文本和目标文本中捕获当前和未来的信息。此外，我们利用VAE初始化模型参数以提高生成能力。",
    "tldr": "RegaVAE是一种基于变分自编码器的检索增强语言模型，它通过将文本编码到潜在空间中捕获当前和未来的信息，并使用潜在变量进行有效的信息聚合。",
    "en_tdlr": "RegaVAE is a retrieval-augmented language model based on variational auto-encoder, which captures current and future information by encoding text into a latent space, and performs efficient information aggregation using latent variables."
}