{
    "title": "Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])",
    "abstract": "Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c",
    "link": "http://arxiv.org/abs/2310.04361",
    "context": "Title: Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])\nAbstract: Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c",
    "path": "papers/23/10/2310.04361.json",
    "total_tokens": 873,
    "translated_title": "利用动态推理来利用Transformer激活稀疏性",
    "translated_abstract": "Transformer模型尽管表现出色，但由于其高计算需求，常面临实际限制。与此同时，先前的研究揭示了这些模型中的显著激活稀疏性，表明存在冗余计算。本文提出了一种称为动态稀疏化Transformer推理（DSTI）的方法，通过强制激活稀疏性并将密集模型转换为其稀疏的专家混合（MoE）版本，从而极大地降低Transformer模型的推理成本。我们证明，在推理过程中可以训练出成功预测每个专家相对贡献的小型门控网络。此外，我们引入了一种动态确定每个令牌执行的专家数量的机制。DSTI可以应用于任何基于Transformer的体系结构，并对准确性影响微乎其微。针对BERT-base分类模型，我们降低了推理成本。",
    "tldr": "本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。",
    "en_tdlr": "This paper proposes a method called DSTI, which significantly reduces the inference cost of Transformer models by enforcing activation sparsity and transforming them into sparse Mixture of Experts versions. It can be applied to any Transformer-based architecture and has negligible impact on accuracy."
}