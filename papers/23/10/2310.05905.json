{
    "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models",
    "abstract": "arXiv:2310.05905v2 Announce Type: replace-cross  Abstract: The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our e",
    "link": "https://arxiv.org/abs/2310.05905",
    "context": "Title: TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models\nAbstract: arXiv:2310.05905v2 Announce Type: replace-cross  Abstract: The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our e",
    "path": "papers/23/10/2310.05905.json",
    "total_tokens": 790,
    "translated_title": "TAIL: 任务特定的适配器用于具有大型预训练模型的模仿学习",
    "translated_abstract": "大型预训练模型在控制领域（如机器人技术）中的潜力尚未得到充分利用，主要原因是数据稀缺以及为这些应用程序训练或微调这些大型模型所带来的计算挑战。我们介绍了TAIL（任务特定的适配器用于模仿学习），这是一个用于有效适应新控制任务的框架。受到语言领域参数高效微调的最新进展的启发，我们在TAIL中探讨了高效微调技术，例如瓶颈适配器、P调整和低秩适配（LoRA），以将大型预训练模型调整为具有有限演示数据的新任务。",
    "tldr": "TAIL提出了一种适配器框架，通过高效微调技术将大型预训练模型用于新的控制任务，以实现数据有效率、持续适应不同控制任务。",
    "en_tdlr": "TAIL introduces an adapter framework that efficiently fine-tunes large pretrained models for new control tasks, enabling data-efficient and continual adaptation to different control tasks."
}