{
    "title": "Constituency Parsing using LLMs. (arXiv:2310.19462v2 [cs.CL] UPDATED)",
    "abstract": "Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.",
    "link": "http://arxiv.org/abs/2310.19462",
    "context": "Title: Constituency Parsing using LLMs. (arXiv:2310.19462v2 [cs.CL] UPDATED)\nAbstract: Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.",
    "path": "papers/23/10/2310.19462.json",
    "total_tokens": 900,
    "translated_title": "使用大型语言模型进行成分句法分析",
    "translated_abstract": "成分句法分析是一个基础但尚未解决的自然语言处理任务。本文探索了最近大型语言模型（LLMs）在各个领域和任务中展现出的卓越性能在解决这一任务上的潜力。我们采用三种线性化策略将输出的树结构转化为符号序列，使得LLMs可以通过生成线性化树来解决成分句法分析。我们使用多种不同的LLMs进行实验，包括ChatGPT、GPT-4、OPT、LLaMA和Alpaca，并将它们的性能与最先进的成分句法分析器进行比较。我们的实验涵盖了零样本学习、少样本学习和全样本学习的不同设置，并在一个领域内和五个领域外的测试数据集上评估模型。我们的发现揭示了LLMs的性能、泛化能力和成分句法分析中的挑战。",
    "tldr": "本文研究了使用大型语言模型（LLMs）进行成分句法分析的潜力，通过采用线性化策略将输出树结构转化为符号序列，进一步提高了任务的效果。实验结果对LLMs的性能、泛化能力和成分句法分析中的挑战进行了深入研究。"
}