{
    "title": "Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])",
    "abstract": "The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (\"highlights\").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve",
    "link": "http://arxiv.org/abs/2310.09017",
    "context": "Title: Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])\nAbstract: The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (\"highlights\").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve",
    "path": "papers/23/10/2310.09017.json",
    "total_tokens": 922,
    "translated_title": "不添加，不错过：从预选文本段生成有效的内容保留生成模型",
    "translated_abstract": "最近引入的受控文本缩减（CTR）任务在典型的摘要任务中将文本生成步骤隔离出来。它通过挑战模型在输入文本的预选内容（\"高亮\"）中生成连贯的文本来实现。这种框架在类似摘要的任务中增加了模块化能力，允许将单个CTR模型与各种内容选择设置和模块配对使用。然而，目前还没有可靠的CTR模型，而且现有任务基线的性能中等，无法实际使用。为了填补这个空白，我们引入了一个高质量的开源CTR模型，解决了两个先前的关键限制：不充分强制执行内容保留约束和次优的银标签训练数据。通过在训练中通过强化学习和推理中通过受控解码策略来增强内容保留约束。此外，我们还大幅改进了银标签训练数据。",
    "tldr": "本论文介绍了一个高质量的受控文本缩减（CTR）模型，解决了内容保留约束不充分强制执行和次优的银标签训练数据的限制，通过在训练和推理中增强内容保留约束，进一步改进了模型性能。"
}