{
    "title": "Guiding AMR Parsing with Reverse Graph Linearization. (arXiv:2310.08860v1 [cs.CL])",
    "abstract": "Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded earlier. To address this issue, we propose a novel Reverse Graph Linearization (RGL) enhanced framework. RGL defines both default and reverse linearization orders of an AMR graph, where most structures at the back part of the default order appear at the front part of the reversed order and vice versa. RGL incorporates the reversed linearization to the original AMR parser through a two-pass self-distillation mechanism, which guides the model when generating the default linearizations. Our analysi",
    "link": "http://arxiv.org/abs/2310.08860",
    "context": "Title: Guiding AMR Parsing with Reverse Graph Linearization. (arXiv:2310.08860v1 [cs.CL])\nAbstract: Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded earlier. To address this issue, we propose a novel Reverse Graph Linearization (RGL) enhanced framework. RGL defines both default and reverse linearization orders of an AMR graph, where most structures at the back part of the default order appear at the front part of the reversed order and vice versa. RGL incorporates the reversed linearization to the original AMR parser through a two-pass self-distillation mechanism, which guides the model when generating the default linearizations. Our analysi",
    "path": "papers/23/10/2310.08860.json",
    "total_tokens": 882,
    "translated_title": "引导AMR解析与逆向图线性化",
    "translated_abstract": "抽象意义表示（AMR）解析旨在从给定的句子中提取一个抽象的语义图。序列到序列的方法将语义图线性化为节点和边的序列，并直接生成线性化的图，取得了良好的性能。然而，我们观察到这些方法在解码过程中会出现结构丢失积累的问题，导致后解码的节点和边的F1得分比先解码的要低得多。为了解决这个问题，我们提出了一种新的增强框架，即逆向图线性化（RGL）。RGL定义了AMR图的默认和逆向线性化顺序，其中在默认顺序的后部出现的大多数结构在逆向顺序的前部出现，反之亦然。RGL通过两次自我蒸馏机制将逆向线性化引入原始AMR解析器，从而在生成默认线性化时引导模型。",
    "tldr": "本论文引入了逆向图线性化（RGL）的方法来解决序列到序列的AMR解析中结构丢失积累的问题。通过定义默认和逆向线性化顺序，并通过自我蒸馏机制引导模型生成默认线性化，提高了解析性能。"
}