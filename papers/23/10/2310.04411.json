{
    "title": "Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v1 [cs.LG])",
    "abstract": "The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage",
    "link": "http://arxiv.org/abs/2310.04411",
    "context": "Title: Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v1 [cs.LG])\nAbstract: The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage",
    "path": "papers/23/10/2310.04411.json",
    "total_tokens": 917,
    "translated_title": "理解、预测和更好地解决离线强化学习中的Q值分歧问题",
    "translated_abstract": "Q值估计的分歧一直是离线强化学习中一个突出的问题，其中代理人无法获得真实动态信息。传统观点认为，当引导值目标时查询分布之外的动作是导致不稳定的原因。尽管可以通过策略约束或保守的Q值估计来缓解这个问题，但对导致分歧的机制的理论理解一直缺乏。在这项工作中，我们旨在深入了解这个机制并获得改进的解决方案。我们首先识别了自激励作为离线强化学习中Q值估计分歧的主要原因。然后，我们提出了一种基于神经切线核(NTK)的自激励特征值测量(SEEM)指标，用于测量训练过程中Q网络的演化性质，这提供了对分歧出现的有趣解释。首次，我们的理论能够可靠地决定训练是否在早期阶段发散。",
    "tldr": "这项研究通过理解Q值估计分歧的机制，提出了一种基于自激励特征值测量的解决方案，能够可靠地判断训练过程是否会出现分歧。",
    "en_tdlr": "This study proposes a solution based on self-excitation eigenvalue measure to better understand and resolve the issue of Q-value estimation divergence in offline RL, with the ability to reliably determine if the training will diverge."
}