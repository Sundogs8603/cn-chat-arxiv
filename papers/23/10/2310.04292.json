{
    "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])",
    "abstract": "Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed ",
    "link": "http://arxiv.org/abs/2310.04292",
    "context": "Title: Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])\nAbstract: Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed ",
    "path": "papers/23/10/2310.04292.json",
    "total_tokens": 958,
    "translated_title": "面向大规模多任务数据集的分子学习基础模型的研究",
    "translated_abstract": "最近，预训练的基础模型在多个领域取得了显著的进展。然而，在分子机器学习中，数据集通常是手工策划的，因此规模较小，缺乏带有标记特征和管理这些数据集的代码库，制约了基础模型的发展。在这项工作中，我们提出了七个新颖的数据集，分为三个不同的类别：ToyMix、LargeMix和UltraLarge。这些数据集在规模和有监督标签的多样性方面突破了界限。它们涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。相比之下，我们的数据集的数据点数量是广泛使用的OGB-LSC PCQM4Mv2数据集的300倍，也是仅包含量子数据的QM1B数据集的13倍。此外，为了支持基于我们提出的基础模型的开发，",
    "tldr": "本研究提出了七个新颖的数据集，分别是ToyMix、LargeMix和UltraLarge，这些数据集在规模和有监督标签的多样性方面突破了界限，涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。",
    "en_tdlr": "This paper proposes seven novel datasets categorized into ToyMix, LargeMix, and UltraLarge, which push the boundaries in both scale and diversity of supervised labels for molecular learning. These datasets cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature."
}