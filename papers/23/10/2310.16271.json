{
    "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])",
    "abstract": "Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisi",
    "link": "http://arxiv.org/abs/2310.16271",
    "context": "Title: CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])\nAbstract: Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisi",
    "path": "papers/23/10/2310.16271.json",
    "total_tokens": 938,
    "translated_title": "CycleAlign: 从黑盒语言模型到白盒模型进行迭代提炼，以实现更好的人类对齐",
    "translated_abstract": "在大规模语料库上训练的语言模型通常会生成有害、有毒或与人类偏好相悖的内容，使得其与人类价值的对齐成为一个关键问题。强化学习从人类反馈中进行对齐的方法（如PPO）是一种常见的方法，但往往复杂、不稳定且资源密集。最近，基于排名的对齐方法已经出现，通过用监督微调替换强化学习框架，提供稳定性和有效性，但由于需要带注释的数据，它们的成本较高。考虑到现有的大型语言模型（如ChatGPT）已经相对较好地对齐并且成本较低，研究人员已经开始从AI反馈中对语言模型与人类偏好进行对齐。现有的常规实践仅仅从LLMs提炼出遵循指令的响应，受到了瓶颈的限制。因此，我们引入CycleAlign来从参数非可见的模型中提炼对齐能力。",
    "tldr": "CycleAlign提出了一种从语言模型中提炼对齐能力的方法，它通过迭代提炼实现对黑盒模型到白盒模型的转变，解决了语言模型与人类价值对齐的问题。",
    "en_tdlr": "CycleAlign proposes an approach to distill alignment capabilities from language models, achieving the transformation from black-box models to white-box models through iterative distillation, addressing the problem of alignment between language models and human values."
}