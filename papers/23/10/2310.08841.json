{
    "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments. (arXiv:2310.08841v1 [cs.AI])",
    "abstract": "Most Reinforcement Learning (RL) methods are traditionally studied in an active learning setting, where agents directly interact with their environments, observe action outcomes, and learn through trial and error. However, allowing partially trained agents to interact with real physical systems poses significant challenges, including high costs, safety risks, and the need for constant supervision. Offline RL addresses these cost and safety concerns by leveraging existing datasets and reducing the need for resource-intensive real-time interactions. Nevertheless, a substantial challenge lies in the demand for these datasets to be meticulously annotated with rewards. In this paper, we introduce Optimal Transport Reward (OTR) labelling, an innovative algorithm designed to assign rewards to offline trajectories, using a small number of high-quality expert demonstrations. The core principle of OTR involves employing Optimal Transport (OT) to calculate an optimal alignment between an unlabele",
    "link": "http://arxiv.org/abs/2310.08841",
    "context": "Title: Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments. (arXiv:2310.08841v1 [cs.AI])\nAbstract: Most Reinforcement Learning (RL) methods are traditionally studied in an active learning setting, where agents directly interact with their environments, observe action outcomes, and learn through trial and error. However, allowing partially trained agents to interact with real physical systems poses significant challenges, including high costs, safety risks, and the need for constant supervision. Offline RL addresses these cost and safety concerns by leveraging existing datasets and reducing the need for resource-intensive real-time interactions. Nevertheless, a substantial challenge lies in the demand for these datasets to be meticulously annotated with rewards. In this paper, we introduce Optimal Transport Reward (OTR) labelling, an innovative algorithm designed to assign rewards to offline trajectories, using a small number of high-quality expert demonstrations. The core principle of OTR involves employing Optimal Transport (OT) to calculate an optimal alignment between an unlabele",
    "path": "papers/23/10/2310.08841.json",
    "total_tokens": 864,
    "translated_title": "在外科机器人环境中利用最优输运增强离线强化学习",
    "translated_abstract": "大多数强化学习方法通常在主动学习环境中进行研究，代理直接与环境互动，观察行动结果，并通过试错学习。然而，允许部分训练代理与真实物理系统交互会带来显著的挑战，包括高成本、安全风险和需要持续监督。离线强化学习通过利用现有数据集并减少对资源密集型实时交互的需求来解决这些成本和安全问题。然而，一个重要的挑战在于需要精心注释这些数据集的奖励。在本文中，我们介绍了一种创新的算法，即最优输运奖励（OTR）标签，用于对离线轨迹分配奖励，使用少量高质量的专家演示。OTR的核心原理是利用最优输运计算无标签轨迹与专家演示之间的最优对齐方式。",
    "tldr": "本论文介绍了一种利用最优输运奖励标签的算法，可以在离线情况下分配奖励给轨迹，从而减少对资源密集型实时交互的需求。",
    "en_tdlr": "This paper presents an innovative algorithm, called Optimal Transport Reward (OTR) labeling, to assign rewards to offline trajectories, reducing the need for resource-intensive real-time interactions."
}