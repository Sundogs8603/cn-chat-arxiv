{
    "title": "Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data. (arXiv:2310.11110v1 [cs.LG])",
    "abstract": "Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LD",
    "link": "http://arxiv.org/abs/2310.11110",
    "context": "Title: Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data. (arXiv:2310.11110v1 [cs.LG])\nAbstract: Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LD",
    "path": "papers/23/10/2310.11110.json",
    "total_tokens": 922,
    "translated_title": "最小信息线性判别分析：使用未标记数据训练LDA模型",
    "translated_abstract": "线性判别分析（LDA）是最古老且最流行的线性方法之一，用于有监督分类问题。本文证明，如果有一些最小的先验信息，那么可以基于未标记数据计算出LDA模型的精确投影向量。更具体地说，我们展示了只需要以下三个信息中的任意一个即可计算LDA投影向量，如果只有未标记数据可用：（1）两个类别中任意一个的类别平均值，（2）两个类别平均值之间的差异（经过缩放），或者（3）类别协方差矩阵（经过缩放）。这些理论结果在数值实验中得到了验证，证明这种最小信息的线性判别分析（MILDA）模型与有监督的LDA模型的性能非常接近。此外，我们还展示了MILDA投影向量可以通过一个封闭形式计算出来，并且计算成本与LDA相当。",
    "tldr": "本文展示了在只有未标记数据的情况下，通过一些最小的先验信息，可以计算出精确的LDA投影向量。数值实验验证了这种最小信息的线性判别分析（MILDA）模型与有监督的LDA模型的性能接近。",
    "en_tdlr": "This paper demonstrates that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. Numerical experiments validate that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model."
}