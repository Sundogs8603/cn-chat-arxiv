{
    "title": "Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs. (arXiv:2310.16105v1 [cs.LG])",
    "abstract": "Distributed online learning has been proven extremely effective in solving large-scale machine learning problems involving streaming data. However, information sharing between learners in distributed learning also raises concerns about the potential leakage of individual learners' sensitive data. To mitigate this risk, differential privacy, which is widely regarded as the \"gold standard\" for privacy protection, has been widely employed in many existing results on distributed online learning. However, these results often face a fundamental tradeoff between learning accuracy and privacy. In this paper, we propose a locally differentially private gradient tracking based distributed online learning algorithm that successfully circumvents this tradeoff. Our analysis shows that the proposed algorithm converges in mean square to the exact optimal solution while ensuring rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iter",
    "link": "http://arxiv.org/abs/2310.16105",
    "context": "Title: Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs. (arXiv:2310.16105v1 [cs.LG])\nAbstract: Distributed online learning has been proven extremely effective in solving large-scale machine learning problems involving streaming data. However, information sharing between learners in distributed learning also raises concerns about the potential leakage of individual learners' sensitive data. To mitigate this risk, differential privacy, which is widely regarded as the \"gold standard\" for privacy protection, has been widely employed in many existing results on distributed online learning. However, these results often face a fundamental tradeoff between learning accuracy and privacy. In this paper, we propose a locally differentially private gradient tracking based distributed online learning algorithm that successfully circumvents this tradeoff. Our analysis shows that the proposed algorithm converges in mean square to the exact optimal solution while ensuring rigorous local differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iter",
    "path": "papers/23/10/2310.16105.json",
    "total_tokens": 848,
    "translated_title": "在有向图上的分布式在线学习中基于局部差分隐私的梯度跟踪",
    "translated_abstract": "分布式在线学习在解决涉及流数据的大规模机器学习问题方面非常有效。然而，分布式学习中的学习者之间的信息共享也引起了个体学习者敏感数据可能泄露的担忧。为了减轻这种风险，差分隐私被广泛应用于许多现有的分布式在线学习结果中，被认为是隐私保护的“金标准”。然而，这些结果往往面临学习准确性和隐私之间的基本权衡。在本文中，我们提出了一种基于局部差分隐私梯度跟踪的分布式在线学习算法，成功地避免了这种权衡。我们的分析表明，该算法在均方意义上收敛到精确的最优解，同时确保了严格的局部差分隐私，即使迭代次数增加，累积隐私预算也保证是有限的。",
    "tldr": "本文提出了一种基于局部差分隐私的梯度跟踪分布式在线学习算法，在保持学习准确性的同时，有效地保护了学习者的隐私。",
    "en_tdlr": "This paper proposes a locally differentially private gradient tracking based distributed online learning algorithm that effectively protects learners' privacy while maintaining learning accuracy."
}