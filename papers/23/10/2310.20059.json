{
    "title": "Concept Alignment as a Prerequisite for Value Alignment. (arXiv:2310.20059v1 [cs.AI])",
    "abstract": "Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint re",
    "link": "http://arxiv.org/abs/2310.20059",
    "context": "Title: Concept Alignment as a Prerequisite for Value Alignment. (arXiv:2310.20059v1 [cs.AI])\nAbstract: Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint re",
    "path": "papers/23/10/2310.20059.json",
    "total_tokens": 974,
    "translated_title": "概念对齐作为价值对齐的先决条件",
    "translated_abstract": "价值对齐对于构建可以安全可靠地与人类互动的人工智能系统至关重要。然而，一个人所重视的事物 - 甚至是他能够重视的事物 - 取决于他们当前用于理解和评估世界发生的事情的概念。价值依赖于概念，这意味着概念对齐是实现价值对齐的先决条件 - 代理需要将其对情境的表示与人类的情境表示进行对齐，以成功地对齐其价值观。在这里，我们在反向强化学习设置下对概念对齐问题进行了正式分析，展示了忽视概念对齐可能导致系统性的价值错位，并描述了一种能够通过联合推理一个人的概念和价值来最小化此类失误的方法。此外，我们报告了与人类参与者的实验结果，显示人类在有意行动时会考虑代理人所使用的概念，与我们的联合推理一致。",
    "tldr": "价值对齐对于构建安全可靠的人工智能系统至关重要。这篇论文分析了概念对齐问题，并展示了忽视概念对齐可能导致价值错位的情况。通过联合推理一个人的概念和价值，可以最小化这类失误。实验结果显示人类在有意行动时会考虑代理人所使用的概念。",
    "en_tdlr": "Value alignment is crucial for building AI systems that can interact with humans safely and reliably. This paper analyzes the concept alignment problem in the context of inverse reinforcement learning, demonstrates the consequences of neglecting concept alignment, and proposes an approach that minimizes value misalignment by jointly reasoning about a person's concepts and values. Experimental results with human participants confirm that humans consider the concepts used by an agent when acting intentionally."
}