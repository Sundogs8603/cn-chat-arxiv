{
    "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
    "abstract": "arXiv:2310.19668v2 Announce Type: replace Abstract: Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent'",
    "link": "https://arxiv.org/abs/2310.19668",
    "context": "Title: DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization\nAbstract: arXiv:2310.19668v2 Announce Type: replace Abstract: Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent'",
    "path": "papers/23/10/2310.19668.json",
    "total_tokens": 919,
    "translated_title": "通过最小化休眠比率掌握视觉强化学习",
    "translated_abstract": "视觉强化学习在连续控制任务中显示出了潜力。尽管取得了一定的进展，但当前的算法在样本效率、渐近性能和对随机种子选择的鲁棒性等方面仍然不尽人意。在本文中，我们发现了现有视觉强化学习方法的一个主要缺点，即在早期训练阶段，智能体经常展现出持续的不活动状态，从而限制了它们有效探索的能力。基于这一关键观察，我们进一步揭示了智能体倾向于运动不活跃探索与其策略网络中的神经活动缺失之间的显著相关性。为了量化这种不活跃状态，我们采用休眠比率作为衡量强化学习智能体网络中不活跃的度量标准。实证上，我们还认识到休眠比率可以作为智能体匹配能力的独立指标。",
    "tldr": "通过最小化休眠比率，本文揭示了当前视觉强化学习方法的一个主要缺点是智能体在早期训练阶段展现出持续的不活动状态，限制了其有效探索能力，并提出了使用休眠比率作为度量指标的方法。",
    "en_tdlr": "This paper identifies a major limitation in existing visual reinforcement learning methods, namely, the sustained inactivity of agents during early training, which limits their ability to explore effectively. The authors propose to use dormant ratio as a metric to quantify this inactivity and as a standalone indicator of an agent's matching capability."
}