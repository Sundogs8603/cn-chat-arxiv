{
    "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.  In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of t",
    "link": "http://arxiv.org/abs/2310.03003",
    "context": "Title: From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])\nAbstract: Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.  In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of t",
    "path": "papers/23/10/2310.03003.json",
    "total_tokens": 856,
    "translated_title": "从单词到瓦特：大型语言模型推理的能源成本基准测试",
    "translated_abstract": "大型语言模型(LLMs)由于其超越以往最先进水平的新生成能力而迅速受到欢迎。这些技术越来越多地在法律、金融和医学等领域得到应用。然而，这些模型带来了显著的计算挑战，特别是推理所需的计算和能源成本。尽管现实中经常需要这些大型模型进行推理(例如ChatGPT)，但推理能源成本却比LLMs的训练能源成本得到了较少关注。随着这些最先进的LLMs在各个领域的使用和部署越来越多，更好地理解它们的资源利用对于节约成本、提高性能、高效利用硬件和优化推理策略至关重要。在本文中，我们描述了进行的实验，以研究使用LLMs进行推理的计算和能源利用情况。我们进行了基准测试并进行了初步分析。",
    "tldr": "该论文研究了大型语言模型推理的计算和能源利用情况，以帮助节约成本、提高性能和优化推理策略。",
    "en_tdlr": "This paper investigates the computational and energy utilization of large language model inference, aiming to save cost, improve performance, and optimize inference strategies."
}