{
    "title": "Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks. (arXiv:2310.04867v1 [cs.LG])",
    "abstract": "Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse",
    "link": "http://arxiv.org/abs/2310.04867",
    "context": "Title: Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks. (arXiv:2310.04867v1 [cs.LG])\nAbstract: Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse",
    "path": "papers/23/10/2310.04867.json",
    "total_tokens": 880,
    "translated_title": "针对随机稀疏神经Galerkin方案用于求解含有深度网络的演化方程",
    "translated_abstract": "在时间上顺序地训练神经网络来近似解决含有时间依赖的偏微分方程可以帮助保持因果性和其他物理属性；然而，时间上的顺序训练在数值上具有挑战性，因为训练误差会随着时间快速积累和放大。本研究引入了神经Galerkin方案，该方案在每个时间步骤上更新随机稀疏的网络参数子集。随机化避免了在时间上过度拟合，并帮助防止误差在时间上的快速累积，这受到dropout的启发，dropout解决了由于神经元协同适应而导致的过拟合问题。更新的稀疏性降低了训练的计算成本，并且不会丧失表达能力，因为在每个时间步骤上许多网络参数是多余的。在广泛的演化方程的数值实验中，采用了随机稀疏方案。",
    "tldr": "该论文介绍了一种用于演化方程解决的神经Galerkin方案，通过随机化稀疏网络参数的更新来避免在时间上过拟合并降低计算成本。",
    "en_tdlr": "This paper introduces a neural Galerkin scheme for solving evolution equations, which avoids overfitting in time by updating randomly sparse network parameters and reduces computational costs."
}