{
    "title": "Topological Expressivity of ReLU Neural Networks. (arXiv:2310.11130v1 [cs.LG])",
    "abstract": "We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplificatio",
    "link": "http://arxiv.org/abs/2310.11130",
    "context": "Title: Topological Expressivity of ReLU Neural Networks. (arXiv:2310.11130v1 [cs.LG])\nAbstract: We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplificatio",
    "path": "papers/23/10/2310.11130.json",
    "total_tokens": 860,
    "translated_title": "ReLU神经网络的拓扑表达能力",
    "translated_abstract": "我们从拓扑的角度研究了ReLU神经网络在二元分类问题中的表达能力。最近的实证研究表明，神经网络通过改变拓扑结构，将一个拓扑复杂的数据集转化为一个拓扑简单的数据集。这种拓扑简化可以用Betti数来衡量，Betti数是拓扑空间的代数不变量。我们使用相同的衡量指标来确定给定架构下ReLU神经网络可以实现的拓扑简化的上下界。因此，我们通过揭示ReLU神经网络捕捉数据的底层拓扑结构的能力，为深入理解ReLU神经网络在二元分类问题中的表达能力做出了贡献。特别是，结果表明，深层ReLU神经网络在拓扑简化方面比浅层网络具有指数级的能力。",
    "tldr": "本研究从拓扑的角度研究了ReLU神经网络在二元分类问题中的表达能力，通过衡量网络对数据拓扑结构的改变程度，发现深层ReLU网络比浅层网络具有指数级的拓扑简化能力。"
}