{
    "title": "Online Convex Optimization with Switching Cost and Delayed Gradients. (arXiv:2310.11880v1 [cs.LG])",
    "abstract": "We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \\frac{16(L + 5)}{\\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\\mu$. In addition, we show that the competitive ratio of any online algorithm is $\\max\\{\\Omega(L), \\Omega(\\frac{L}{\\sqrt{\\mu}})\\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of",
    "link": "http://arxiv.org/abs/2310.11880",
    "context": "Title: Online Convex Optimization with Switching Cost and Delayed Gradients. (arXiv:2310.11880v1 [cs.LG])\nAbstract: We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \\frac{16(L + 5)}{\\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\\mu$. In addition, we show that the competitive ratio of any online algorithm is $\\max\\{\\Omega(L), \\Omega(\\frac{L}{\\sqrt{\\mu}})\\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of",
    "path": "papers/23/10/2310.11880.json",
    "total_tokens": 913,
    "translated_title": "具有开关成本和延迟梯度的在线凸优化问题",
    "translated_abstract": "我们考虑了在有限信息设置下具有二次和线性开关成本的在线凸优化问题，在这里在线算法仅能利用先前目标函数的梯度信息进行动作选择。对于$L$-光滑和$\\mu$-强凸目标函数，我们提出了一种在线多梯度下降（OMGD）算法，并证明该算法在具有二次开关成本的在线凸优化问题上的竞争比率最多为$4(L+5)+\\frac{16(L+5)}{\\mu}$。对于OMGD的竞争比率上界也被证明在$L$和$\\mu$方面是紧致的。此外，当开关成本为二次时，我们证明了任何在线算法的竞争比率是$\\max\\{\\Omega(L), \\Omega(\\frac{L}{\\sqrt{\\mu}})\\}$。我们还证明了在有限信息设置下，OMGD算法实现了最优（按顺序）的动态后悔。对于线性开关成本，",
    "tldr": "提出了一种在线多梯度下降（OMGD）算法用于解决具有二次和线性开关成本的在线凸优化问题，证明了其竞争比率上界，并在有限信息设置下达到了最优（按顺序）的动态后悔。",
    "en_tdlr": "Proposed an online multiple gradient descent (OMGD) algorithm for solving the online convex optimization problem with quadratic and linear switching cost, proved its competitive ratio upper bound, and achieved the optimal (order-wise) dynamic regret in the limited information setting."
}