{
    "title": "TST$^\\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])",
    "abstract": "Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu",
    "link": "http://arxiv.org/abs/2310.17228",
    "context": "Title: TST$^\\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])\nAbstract: Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu",
    "path": "papers/23/10/2310.17228.json",
    "total_tokens": 925,
    "translated_title": "TST$^\\mathrm{R}$: 目标相似度调整遇见现实世界",
    "translated_abstract": "目标相似度调整（TST）是一种通过大型语言模型（LLM）在自然语言（NL）到代码生成中选择相关例子以提升性能的方法。其目标是使得句子嵌入模型适应两个NL输入的相似度与其相关代码输出的相似度匹配。本文提出了不同的方法来应用和改进TST在现实世界中的使用。首先，我们将句子转换器替换为更大模型的嵌入，从而降低对语言分布的敏感性，增加了合成示例的灵活性，并训练一个小模型将这些嵌入转换到一个空间中，其中嵌入相似度匹配代码相似度，使得模型保持黑箱状态，并在推断时只需进行少量矩阵乘法。其次，我们介绍了如何高效地选择较少数量的训练样例来训练TST模型。第三，我们引入了基于排名的评估方法。",
    "tldr": "本文提出了在现实世界中应用和改进目标相似度调整（TST）的不同方法，包括使用更大的模型嵌入、训练一个小模型转换嵌入以匹配代码相似度，并介绍了高效选择训练样例和基于排名的评估方法。",
    "en_tdlr": "This paper proposes different methods to apply and improve Target Similarity Tuning (TST) in the real world, including using embeddings from a larger model, training a tiny model to transform embeddings to match code similarity, introducing efficient selection of training examples, and a ranking-based evaluation method."
}