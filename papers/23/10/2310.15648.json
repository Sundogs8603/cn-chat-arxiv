{
    "title": "Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models. (arXiv:2310.15648v1 [cs.SD])",
    "abstract": "The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic convolutions and attention mechanisms. We show that these dynamic CNNs outperform traditional efficient CNNs, in terms of the performance-complexity trade",
    "link": "http://arxiv.org/abs/2310.15648",
    "context": "Title: Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models. (arXiv:2310.15648v1 [cs.SD])\nAbstract: The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic convolutions and attention mechanisms. We show that these dynamic CNNs outperform traditional efficient CNNs, in terms of the performance-complexity trade",
    "path": "papers/23/10/2310.15648.json",
    "total_tokens": 848,
    "translated_title": "动态卷积神经网络作为高效的预训练音频模型",
    "translated_abstract": "大规模音频数据集（如AudioSet）的引入为Transformers在音频领域的使用铺平了道路，并取代了CNN作为许多任务的最先进神经网络架构。音频频谱Transformers在利用大规模数据集方面表现出了优良的性能，创造出了强大的预训练模型，在下游任务的微调中超过了CNN。然而，与CNN相比，目前流行的音频频谱Transformers在计算复杂度上要求较高。最近，我们通过使用Transformer到CNN的知识蒸馏，展示了高效CNN可以在大数据集上追赶甚至超过Transformer。在这项工作中，我们延伸了这一研究领域，并通过引入由动态非线性、动态卷积和注意机制构成的动态CNN块来提升高效CNN的容量。我们展示了这些动态CNN在性能和复杂度的权衡上优于传统的高效CNN。",
    "tldr": "本研究在已有的Transformer到CNN的知识蒸馏基础上，通过引入动态CNN块，提升了高效CNN的性能，相比传统的高效CNN更具优势。",
    "en_tdlr": "This study improves the performance of efficient CNNs by introducing dynamic CNN blocks on top of Transformer-to-CNN knowledge distillation, surpassing traditional efficient CNNs."
}