{
    "title": "CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads ",
    "link": "http://arxiv.org/abs/2310.17245",
    "context": "Title: CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads ",
    "path": "papers/23/10/2310.17245.json",
    "total_tokens": 863,
    "translated_title": "CROP: 保守奖励用于基于模型的离线策略优化",
    "translated_abstract": "离线强化学习旨在使用收集到的数据进行策略优化，而无需进行在线交互。基于模型的方法在解决离线强化学习挑战方面特别有吸引力，因为它们能够通过使用模型生成数据来缓解离线数据的限制。之前的研究表明，在策略优化过程中将保守性引入模型或Q函数可以有效缓解离线强化学习中普遍存在的分布漂移问题。然而，关于奖励估计中保守性的影响的研究仍然不足。本文提出了一种新颖的基于模型的离线强化学习算法CROP，该算法在模型训练中保守地估计奖励。为了实现保守的奖励估计，CROP同时最小化估计误差和随机动作的奖励。理论分析表明，这种保守的奖励机制导致...（文章摘要未完，下同）",
    "tldr": "CROP提出了一种保守奖励的模型训练方法用于基于模型的离线策略优化，通过同时最小化估计误差和随机动作奖励来实现保守的奖励估计。"
}