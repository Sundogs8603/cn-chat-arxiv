{
    "title": "Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro",
    "link": "http://arxiv.org/abs/2310.15724",
    "context": "Title: Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro",
    "path": "papers/23/10/2310.15724.json",
    "total_tokens": 931,
    "translated_title": "Variator: 使用即插即用压缩模块加速预训练模型",
    "translated_abstract": "预训练语言模型（PLMs）在自然语言处理任务上取得了显著的成果，但代价是巨大的参数大小和随之而来的计算成本。本文提出了Variator，一种参数高效的加速方法，通过即插即用的压缩插件增强计算效率。压缩插件通过将多个隐藏向量压缩到一个向量来缩减序列长度，并与原始PLMs一起进行训练。与传统的模型加速方法不同，Variator具有两个独特的优点：（1）在现实世界应用中，我们的压缩插件的即插即用性质使得可以根据当前工作负载动态选择具有不同加速比的压缩插件。（2）压缩插件由几个紧凑的神经网络层组成，参数很少，大大节省了存储和内存开销，特别是在具有较大存储需求和内存需求的场景下。",
    "tldr": "这个论文提出了一种称为Variator的加速方法，通过即插即用的压缩插件增强了预训练模型的计算效率，并且可以根据工作负载动态选择不同加速比的插件。插件采用了压缩隐藏向量的方法来减小序列长度，并且由于参数少，可以节省存储和内存开销。"
}