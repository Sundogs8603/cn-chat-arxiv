{
    "title": "Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices. (arXiv:2310.07217v1 [cs.LG])",
    "abstract": "The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency i",
    "link": "http://arxiv.org/abs/2310.07217",
    "context": "Title: Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices. (arXiv:2310.07217v1 [cs.LG])\nAbstract: The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency i",
    "path": "papers/23/10/2310.07217.json",
    "total_tokens": 899,
    "translated_title": "用于在微型物联网设备上部署深度学习模型的多个硬件约束增强神经架构搜索",
    "translated_abstract": "依赖于物联网设备的计算领域的迅速增长创造了对能够在低功耗设备上运行的高效准确的深度学习模型的迫切需求。然而，传统的深度学习模型对于典型的物联网终端节点来说往往过于复杂且计算密集。为了解决这个挑战，神经架构搜索(NAS)已经成为一种流行的设计自动化技术，用于共同优化深度神经网络的准确性和复杂度。然而，现有的NAS技术需要许多迭代才能产生符合特定硬件约束的网络，如硬件上可用的最大内存或目标应用允许的最大延迟。在这项工作中，我们提出了一种新颖的方法，将多个约束融入所谓的可微分NAS优化方法中，从而能够一次生成符合用户定义的内存和延迟约束的模型。",
    "tldr": "这项工作提出了一种新颖的方法，通过将多个硬件约束融入神经架构搜索(NAS)技术中，实现在微型物联网设备上部署高效准确的深度学习模型。",
    "en_tdlr": "This work proposes a novel approach that incorporates multiple hardware constraints into Neural Architecture Search (NAS) techniques, enabling the efficient and accurate deployment of deep learning models on tiny IoT devices."
}