{
    "title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])",
    "abstract": "Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \\textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi",
    "link": "http://arxiv.org/abs/2310.06983",
    "context": "Title: Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models. (arXiv:2310.06983v1 [cs.CL])\nAbstract: Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \\textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mi",
    "path": "papers/23/10/2310.06983.json",
    "total_tokens": 960,
    "translated_title": "通过元认知提示违反期望降低大型语言模型中的心智理论预测误差",
    "translated_abstract": "最近的研究表明，大型语言模型(LLMs)在心智理论(ToM)任务中展现出了令人信服的水平。将不可观察的心理状态归因于他人对于人类社会认知至关重要，并且在个体人类与人工智能(AIs)之间的委托-代理关系中可能同样重要。在本文中，我们探讨了一种在发展心理学中研究的机制，即违反期望(VoE)，如何实现以通过利用新生的ToM功能来降低LLM对用户的预测误差。我们引入了一个“元认知提示”框架，以在AI辅导员的情境中应用VoE。通过存储和检索在LLM对用户期望被违反的情况下得到的事实，我们发现LLM能够以与人类学习理论相符的方式了解用户。最后，我们讨论了建模用户心理的潜在危险和增强机会，并提出了控制这些问题的方法。",
    "tldr": "本文研究了如何利用违反期望机制在大型语言模型中降低用户预测误差。我们引入了元认知提示框架，并发现存储和检索违反用户期望的事实可以使模型以类似人类学习理论的方式了解用户。",
    "en_tdlr": "This paper explores how to reduce user prediction errors in large language models by using the violation of expectation mechanism. It introduces a metacognitive prompting framework and finds that storing and retrieving facts derived from violated user expectations can enable the model to understand users in ways similar to human learning."
}