{
    "title": "General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])",
    "abstract": "The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared t",
    "link": "http://arxiv.org/abs/2310.16861",
    "context": "Title: General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])\nAbstract: The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared t",
    "path": "papers/23/10/2310.16861.json",
    "total_tokens": 872,
    "translated_title": "带有自编码和自回归的通用点模型",
    "translated_abstract": "大型语言模型的预训练架构包括多种类型，包括自编码模型、自回归模型和编码器-解码器模型。我们认为，只要将任何形式的模态量化为离散的符号，它都有可能从大型语言模型中获益。受到GLM的启发，我们提出了一种名为通用点模型（GPM）的模型，它无缝地将自编码和自回归任务整合到点云变换器中。该模型具有多功能性，可以用于下游点云表示任务的微调，以及无条件和有条件的生成任务。GPM通过各种形式的掩码填充任务增强了自编码中的掩码预测，在点云理解方面表现出更好的性能。此外，GPM在无条件点云生成任务中展现出竞争力强大的结果，甚至通过修改输入的条件信息展示了有条件生成任务的潜力。",
    "tldr": "提出了一种通用点模型（GPM），它在点云变换器中无缝整合了自编码和自回归任务。GPM通过掩码填充任务增强了自编码中的掩码预测，并在点云理解和生成任务中展现出竞争力强大的结果。",
    "en_tdlr": "A General Point Model (GPM) is proposed, which seamlessly integrates autoencoding and autoregressive tasks in a point cloud transformer. GPM enhances masked prediction in autoencoding through mask padding tasks and demonstrates a highly competitive performance in point cloud understanding and generation tasks."
}