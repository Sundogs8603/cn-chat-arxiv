{
    "title": "Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])",
    "abstract": "In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a",
    "link": "http://arxiv.org/abs/2310.06450",
    "context": "Title: Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])\nAbstract: In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a",
    "path": "papers/23/10/2310.06450.json",
    "total_tokens": 904,
    "translated_title": "用多样化反馈构建大型语言模型的对齐方法",
    "translated_abstract": "在大型语言模型（LLMs）的研究中，对其与人类价值观的对齐越来越重视，以减少有害内容的影响。然而，当前的对齐方法通常仅依赖于人类反馈的单一形式，如偏好、注释标签或自然语言批评，忽视了结合这些反馈类型的潜在优势。这种限制导致性能不佳，即使有丰富的训练数据。本文引入了建构性和多样化反馈（CDF）作为增强LLM对齐的新方法，受建构学习理论的启发。我们的方法涉及收集适用于训练数据集中不同难度问题的三种不同类型的反馈。具体而言，我们利用批评反馈解决简单问题，利用改进反馈解决中等问题，利用偏好反馈解决困难问题。通过用这种多样化反馈训练我们的模型，我们获得了更好的表现。",
    "tldr": "本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。",
    "en_tdlr": "This paper introduces a novel method called Constructive and Diverse Feedback (CDF) to enhance the alignment of large language models (LLMs). By collecting three types of feedback tailored to different difficulty levels and training the model with diversified feedback, it achieves better performance."
}