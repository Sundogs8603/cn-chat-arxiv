{
    "title": "Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc",
    "link": "http://arxiv.org/abs/2310.09866",
    "context": "Title: Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)\nAbstract: In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc",
    "path": "papers/23/10/2310.09866.json",
    "total_tokens": 956,
    "translated_title": "联邦多目标学习",
    "translated_abstract": "在最近几年中，多目标优化（MOO）作为许多多代理多任务学习应用的基础问题出现。然而，现有的MOO算法仍局限于集中式学习环境，无法满足这些多代理多任务学习应用的分布式性质和数据隐私需求。这激发了我们提出一种新的联邦多目标学习（FMOL）框架，其中多个客户端在保持他们的训练数据私密的同时，分布式协作解决一个MOO问题。值得注意的是，我们的FMOL框架允许不同客户端上的不同目标函数集合，以支持广泛的应用，这首次将MOO形式化推广到联邦学习范式中。对于这个FMOL框架，我们提出了两种新的联邦多目标优化（FMOO）算法，称为联邦多梯度下降平均（FMGDA）和联邦随机梯度下降（Federated SGD）。",
    "tldr": "本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。"
}