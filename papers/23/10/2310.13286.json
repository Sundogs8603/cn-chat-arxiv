{
    "title": "Unified Pretraining for Recommendation via Task Hypergraphs. (arXiv:2310.13286v1 [cs.IR])",
    "abstract": "Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark da",
    "link": "http://arxiv.org/abs/2310.13286",
    "context": "Title: Unified Pretraining for Recommendation via Task Hypergraphs. (arXiv:2310.13286v1 [cs.IR])\nAbstract: Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark da",
    "path": "papers/23/10/2310.13286.json",
    "total_tokens": 953,
    "translated_title": "通过任务超图实现推荐的统一预训练",
    "translated_abstract": "尽管预训练在最近几年引起了广泛关注和流行，但它在基于图的推荐系统中的应用相对有限。在广泛使用的ID依赖数据集中利用预训练的先前知识是具有挑战性的。一方面，一个数据集中的用户-物品交互历史很难通过预训练转移到其他数据集中，因为ID不同。另一方面，在同一个数据集上进行预训练和微调会导致过拟合的高风险。在本文中，我们提出了一种名为通过任务超图的统一推荐预训练的新型多任务预训练框架。为了处理各种先前任务的不同要求和细微差别，我们设计了任务超图将先前任务推广为超边预测。我们设计了一种新的过渡性注意力层来有差异地学习每个先前任务与推荐之间的相关性。在三个基准数据集上进行的实验结果表明，在准确性和效率方面，我们的方法优于其他基准方法。",
    "tldr": "本文提出了一种名为\"通过任务超图的统一预训练推荐\"的多任务预训练框架，使用任务超图将先前任务推广为超边预测，并通过过渡性注意力层学习每个先前任务与推荐之间的相关性。实验表明，这种方法在准确性和效率方面优于其他基准方法。"
}