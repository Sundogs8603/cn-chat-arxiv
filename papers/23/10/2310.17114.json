{
    "title": "On the Convergence of CART under Sufficient Impurity Decrease Condition. (arXiv:2310.17114v1 [stat.ML])",
    "abstract": "The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition \\cite{chi2022asymptotic} -- our result improves upon the known result by \\cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a ``locally reverse Poincar{\\'e} inequality\". We discuss several well-known function classes in non-parametric es",
    "link": "http://arxiv.org/abs/2310.17114",
    "context": "Title: On the Convergence of CART under Sufficient Impurity Decrease Condition. (arXiv:2310.17114v1 [stat.ML])\nAbstract: The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition \\cite{chi2022asymptotic} -- our result improves upon the known result by \\cite{chi2022asymptotic} under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a ``locally reverse Poincar{\\'e} inequality\". We discuss several well-known function classes in non-parametric es",
    "path": "papers/23/10/2310.17114.json",
    "total_tokens": 909,
    "translated_title": "CART在足够不纯度减少条件下的收敛性研究",
    "translated_abstract": "决策树是一种灵活的机器学习模型，在许多应用中取得了成功。通常使用CART以递归贪婪的方式拟合决策树。本文研究了在回归设置下CART的收敛速率。首先，我们在足够不纯度减少条件下建立了CART的预测误差的上界，该结果改进了之前类似假设下的已知结果。此外，我们提供了一些示例证明误差界限无法通过常数或对数因子进一步改进。其次，我们介绍了一组易于验证的足够条件以满足不纯度减少条件。具体来说，我们证明了在加性模型的情况下，只要组件函数符合“局部反向波松不等式”，就可以满足不纯度减少条件。我们讨论了几个在非参数设置中众所周知的函数类。",
    "tldr": "本研究通过研究回归设置下CART的收敛性，建立了足够不纯度减少条件下CART预测误差的上界，并提供了易于验证的足够条件。这对于决策树模型的应用具有重要意义。",
    "en_tdlr": "This research investigates the convergence of CART in a regression setting and establishes an upper bound on the prediction error under a sufficient impurity decrease condition. It also provides easily verifiable sufficient conditions for the impurity decrease condition. These findings are significant for the application of decision tree models."
}