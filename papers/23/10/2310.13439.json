{
    "title": "Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])",
    "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\\% to 82\\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consiste",
    "link": "http://arxiv.org/abs/2310.13439",
    "context": "Title: Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])\nAbstract: Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\\% to 82\\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consiste",
    "path": "papers/23/10/2310.13439.json",
    "total_tokens": 839,
    "translated_title": "大型语言模型在模棱两可情况下的自一致性问题",
    "translated_abstract": "在需要一致性的任务中，大型语言模型（LLMs）在不同上下文下给出不一致的答案是有问题的，例如问答、解释等。我们的研究提供了一个用于评估自一致性的基准，针对存在两个或多个正确答案的情况下进行评估。我们对OpenAI模型套件进行了一系列行为实验，使用了一个模棱两可的整数序列补全任务。我们发现平均一致性范围从67％到82％不等，远远高于一个模型的一致性如果是随机的话所能预测的水平，并且随着模型能力的提升而增加。此外，我们还展示了模型在一系列稳健性检查中都倾向于保持自一致性，包括提示说话者变化和序列长度变化。这些结果表明，自一致性是一种没有特别训练也能产生的新能力。尽管如此，我们发现模型在判断自身一致性时缺少校准。",
    "tldr": "大型语言模型在模棱两可问题中的自一致性存在，并且不需要特别训练也能在稳健性检查中保持自一致性。"
}