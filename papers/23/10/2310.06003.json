{
    "title": "Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])",
    "abstract": "As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \\underline{Pa}rtial \\underline{R}edundancy \\underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves",
    "link": "http://arxiv.org/abs/2310.06003",
    "context": "Title: Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])\nAbstract: As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \\underline{Pa}rtial \\underline{R}edundancy \\underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves",
    "path": "papers/23/10/2310.06003.json",
    "total_tokens": 958,
    "translated_title": "重新思考高效大规模语言模型训练中的内存和通信成本",
    "translated_abstract": "随着模型规模和训练数据集的不断增加，大规模模型训练框架通过各种分片技术减小内存消耗。然而，巨大的通信开销降低了训练效率，特别是在网络带宽变化的公共云环境中。在本文中，我们重新思考内存消耗和通信开销对大型语言模型训练速度的影响，并提出了一种平衡内存和通信的部分冗余优化器(PaRO)。PaRO通过将GPU集群分组和引入微小的组内内存冗余，减少了组间通信的数量和频率，从而提高了模型的训练效率。此外，我们提出了一种分层重叠环(HO-Ring)通信拓扑结构，以增强大模型训练中节点之间或跨交换机之间的通信效率。我们的实验证明，HO-Ring算法改善了训练过程中的通信效率。",
    "tldr": "本文研究了大型语言模型训练中内存和通信成本对训练速度的影响，并提出了一种平衡内存和通信的优化器（PaRO）。此外，还提出了一种用于大模型训练的分层重叠环通信拓扑结构（HO-Ring），实验证明该算法提高了训练过程中的通信效率。"
}