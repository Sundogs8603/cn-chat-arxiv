{
    "title": "What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])",
    "abstract": "Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula",
    "link": "http://arxiv.org/abs/2310.11571",
    "context": "Title: What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])\nAbstract: Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula",
    "path": "papers/23/10/2310.11571.json",
    "total_tokens": 898,
    "translated_title": "什么是一个好问题？基于任务的询问与事实级遮蔽。",
    "translated_abstract": "提问是现实生活中合作推理任务（如问答）的重要组成部分。例如，一个法律助手聊天机器人在没有用户情况的具体信息的情况下可能无法提供准确的建议。然而，通常会直接使用大型语言模型来解决推理任务，而不会向用户或第三方提出后续问题。我们将这个问题称为基于任务的询问（TOA）。零-shot聊天模型可以执行TOA，但它们的训练主要基于下一个词预测，而不是问题是否对成功的合作有帮助。为了能够训练和评估TOA模型，我们提出了自然语言任务导向询问的定义和框架，即生成能够为推理任务提供有用答案的问题的问题。我们还提出了事实级遮蔽（FLM）的方法，通过省略特定的部分将自然语言数据集转换为自我监督的TOA数据集。",
    "tldr": "本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。"
}