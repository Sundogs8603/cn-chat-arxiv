{
    "title": "Class Incremental Learning with Pre-trained Vision-Language Models. (arXiv:2310.20348v1 [cs.CV])",
    "abstract": "With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.  In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conve",
    "link": "http://arxiv.org/abs/2310.20348",
    "context": "Title: Class Incremental Learning with Pre-trained Vision-Language Models. (arXiv:2310.20348v1 [cs.CV])\nAbstract: With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.  In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conve",
    "path": "papers/23/10/2310.20348.json",
    "total_tokens": 866,
    "translated_title": "使用预训练视觉-语言模型的类增量学习",
    "translated_abstract": "随着大规模预训练模型的出现，人们对将其适应和利用于连续学习场景的兴趣不断增加。本文提出了一种利用预训练视觉-语言模型（如CLIP）的方法，使其能够进一步适应新任务，而不仅仅是使用零样本学习。我们在经过预训练的CLIP模型中增加了额外的层，可以在图像编码器之后或文本编码器之前进行操作。我们研究了三种不同的策略：线性适配器、自注意力适配器，分别作用于图像嵌入，并且使用Prompt Tuning修改CLIP文本编码器的输入。我们还提出了一种参数保留的适配器层方法，使用参数重要性的度量来更好地保持增量学习过程中的稳定性和可塑性。我们的实验证明，最简单的解决方案--只有一个线性适配器层且具有参数保留--可以产生最好的结果。",
    "tldr": "本文提出了一种利用预训练视觉-语言模型进行类增量学习的方法，通过在CLIP模型中增加适配器层，并使用参数保留的方法，取得了最佳结果。",
    "en_tdlr": "This paper proposes an approach to class incremental learning using pre-trained vision-language models. By adding adapter layers and using parameter retention, the best results are achieved."
}