{
    "title": "Aligning Language Models with Human Preferences via a Bayesian Approach. (arXiv:2310.05782v3 [cs.CL] UPDATED)",
    "abstract": "In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper propos",
    "link": "http://arxiv.org/abs/2310.05782",
    "context": "Title: Aligning Language Models with Human Preferences via a Bayesian Approach. (arXiv:2310.05782v3 [cs.CL] UPDATED)\nAbstract: In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper propos",
    "path": "papers/23/10/2310.05782.json",
    "total_tokens": 910,
    "translated_title": "通过贝叶斯方法将语言模型与人类偏好对齐",
    "translated_abstract": "在推进以人为中心的自然语言生成（NLG）系统的过程中，确保NLG模型与人类偏好的一致性至关重要。为了实现这种对齐，目前流行的方法利用强化学习（RL）方法，通过来自人类的反馈训练奖励模型。然而，由于人类偏好的主观性而带来的固有分歧对训练奖励模型构成了重大挑战，导致NLG性能的下降。为了解决这个问题，先前的方法通常依赖多数投票或平均值来将多个不一致的偏好合并成一个合并的偏好。虽然这些方法易于理解和执行，但是它们不能捕捉到人类之间细微的分歧程度，并且可能只代表了个别特定人群，从而缺乏定量披露人类偏好的普适性的能力。为了解决这个挑战，本文提出了一种贝叶斯方法来对齐语言模型和人类偏好。",
    "tldr": "本文提出了一种贝叶斯方法，通过训练奖励模型来对齐语言模型和人类偏好。这种方法能够解决由于人类偏好的主观性而带来的困难，从而提高自然语言生成系统的性能。"
}