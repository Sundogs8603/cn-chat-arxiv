{
    "title": "Learning to (Learn at Test Time). (arXiv:2310.13807v2 [cs.LG] UPDATED)",
    "abstract": "We reformulate the problem of supervised learning as learning to learn with two nested loops (i.e. learning problems). The inner loop learns on each individual instance with self-supervision before final prediction. The outer loop learns the self-supervised task used by the inner loop, such that its final prediction improves. Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator. For practical comparison with linear or self-attention layers, we replace each of them in a transformer with an inner loop, so our outer loop is equivalent to training the architecture. When each inner-loop learner is a neural network, our approach vastly outperforms transformers with linear attention on ImageNet from 224 x 224 raw pixels in both accuracy and FLOPs, while (regular) transformers cannot run.",
    "link": "http://arxiv.org/abs/2310.13807",
    "context": "Title: Learning to (Learn at Test Time). (arXiv:2310.13807v2 [cs.LG] UPDATED)\nAbstract: We reformulate the problem of supervised learning as learning to learn with two nested loops (i.e. learning problems). The inner loop learns on each individual instance with self-supervision before final prediction. The outer loop learns the self-supervised task used by the inner loop, such that its final prediction improves. Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator. For practical comparison with linear or self-attention layers, we replace each of them in a transformer with an inner loop, so our outer loop is equivalent to training the architecture. When each inner-loop learner is a neural network, our approach vastly outperforms transformers with linear attention on ImageNet from 224 x 224 raw pixels in both accuracy and FLOPs, while (regular) transformers cannot run.",
    "path": "papers/23/10/2310.13807.json",
    "total_tokens": 885,
    "translated_title": "学习在测试时进行学习的方法",
    "translated_abstract": "我们将监督学习问题重新定义为通过两层嵌套循环进行学习的方法。内循环在每个个体实例上进行自监督学习，然后进行最终的预测。外循环学习内循环使用的自监督任务，以改进最终的预测结果。当内循环学习器为线性模型时，我们的内循环等同于线性注意力；当内循环学习器为核估计器时，等同于自注意力。为了与线性或自注意力层进行实际比较，我们在transformer中用内循环替代了每个线性或自注意力层，因此我们的外循环等同于对架构进行训练。当每个内循环学习器为神经网络时，在ImageNet的224 x 224原始像素上，我们的方法在准确度和FLOPs方面远远优于具有线性注意力的transformer，而（常规的）transformer无法运行。",
    "tldr": "本论文提出了一种学习在测试时进行学习的方法，通过两层嵌套循环对监督学习问题进行重新定义，并使用内循环进行自监督学习，最终的预测结果得到改进。该方法在图像分类任务上表现优越。",
    "en_tdlr": "This paper proposes a method of learning to learn at test time by reformulating the problem of supervised learning with two nested loops. The approach performs self-supervised learning on individual instances in the inner loop before making final predictions. The method outperforms transformers with linear attention on ImageNet classification task."
}