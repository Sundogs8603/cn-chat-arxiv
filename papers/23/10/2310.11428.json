{
    "title": "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression. (arXiv:2310.11428v1 [cs.LG])",
    "abstract": "This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive l",
    "link": "http://arxiv.org/abs/2310.11428",
    "context": "Title: Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression. (arXiv:2310.11428v1 [cs.LG])\nAbstract: This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive l",
    "path": "papers/23/10/2310.11428.json",
    "total_tokens": 1098,
    "translated_title": "SGD噪声的蝴蝶效应：行为克隆和自回归中的误差放大",
    "translated_abstract": "本文研究了使用深度神经网络进行行为克隆的训练不稳定性。我们观察到，尽管对于行为克隆损失几乎没有影响，但在训练过程中，对策略网络的小批量SGD更新导致了长期奖励的剧烈振荡。我们通过实验证明了这些振荡的统计和计算原因，并发现它们源于小批量SGD噪声在不稳定的闭环动力学中的混沌传播。虽然SGD噪声对于单步动作预测目标是无害的，但在长期视野上它导致了灾难性的误差累积，我们称之为梯度方差放大（GVA）效应。我们发现许多标准的缓解技术不能缓解GVA，但是发现迭代的指数移动平均（EMA）在缓解GVA方面非常有效。我们通过展示连续控制和自回归中GVA的存在以及EMA减缓GVA的情况，说明了这一现象的普遍性。",
    "tldr": "这项研究探究了在深度神经网络中进行行为克隆训练时出现的训练不稳定性现象。我们发现，尽管小批量SGD更新对于行为克隆损失几乎没有影响，但它会导致长期奖励的剧烈振荡。我们称这种效应为梯度方差放大（GVA），并发现使用指数移动平均（EMA）可以有效减缓这种效应。这一现象在连续控制和自回归等领域具有普遍性。",
    "en_tdlr": "This study investigates training instabilities in behavior cloning with deep neural networks. It reveals that despite minimal impact on the behavior cloning loss, minibatch SGD updates to the policy network result in significant oscillations in long-horizon rewards, known as gradient variance amplification (GVA). The study demonstrates the effectiveness of exponential moving average (EMA) in mitigating GVA and highlights the widespread occurrence of this phenomenon in continuous control and autoregression."
}