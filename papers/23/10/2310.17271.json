{
    "title": "Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?. (arXiv:2310.17271v1 [cs.CL])",
    "abstract": "Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately $90$\\% and $77$\\% of the full-token model in SuperGLUE ",
    "link": "http://arxiv.org/abs/2310.17271",
    "context": "Title: Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?. (arXiv:2310.17271v1 [cs.CL])\nAbstract: Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately $90$\\% and $77$\\% of the full-token model in SuperGLUE ",
    "path": "papers/23/10/2310.17271.json",
    "total_tokens": 875,
    "translated_title": "理解语言模型中输入单词字符的作用：信息丢失如何影响性能？",
    "translated_abstract": "理解预训练语言模型（PLMs）如何学习语言是自然语言处理中一个开放的挑战。早期的工作主要集中在确定它们是否捕捉了语义和句法信息，以及数据或预训练目标如何影响它们的性能。然而，据我们所知，之前没有研究专门探讨输入单词字符的信息丢失对PLMs性能的影响。在本研究中，我们通过使用单个单词字符的小子集进行预训练语言模型，填补了这一空白。令人惊讶的是，我们发现即使在极端情况下，即每个单词只使用一个字符进行预训练，与完整单词模型相比在标准NLU基准测试和探测任务中的性能保持较高。例如，仅使用单词的首个字符进行预训练的模型在SuperGLUE中保持了约90％和77％的性能。",
    "tldr": "本文探索了输入单词字符对预训练语言模型性能的影响，发现即使只使用单个字符进行预训练，模型在标准任务中的性能保持较高。"
}