{
    "title": "Polynomial Time Cryptanalytic Extraction of Neural Network Models. (arXiv:2310.08708v1 [cs.LG])",
    "abstract": "Billions of dollars and countless GPU hours are currently spent on training Deep Neural Networks (DNNs) for a variety of tasks. Thus, it is essential to determine the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations. Many versions of this problem have been studied over the last 30 years, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. It resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons). In this paper, we improve this attack by developing several new techniques that enable us to extract with arbitrarily high precision all the real-valued parameters of a ReLU-based DNN using a polynomial number of queries and a polynomial amount of time. We",
    "link": "http://arxiv.org/abs/2310.08708",
    "context": "Title: Polynomial Time Cryptanalytic Extraction of Neural Network Models. (arXiv:2310.08708v1 [cs.LG])\nAbstract: Billions of dollars and countless GPU hours are currently spent on training Deep Neural Networks (DNNs) for a variety of tasks. Thus, it is essential to determine the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations. Many versions of this problem have been studied over the last 30 years, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. It resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons). In this paper, we improve this attack by developing several new techniques that enable us to extract with arbitrarily high precision all the real-valued parameters of a ReLU-based DNN using a polynomial number of queries and a polynomial amount of time. We",
    "path": "papers/23/10/2310.08708.json",
    "total_tokens": 848,
    "translated_title": "多项式时间的密码分析提取神经网络模型",
    "translated_abstract": "目前在训练深度神经网络（DNN）上花费了数十亿美元和无数GPU小时，因此，确定在给定对其黑盒实现的访问权限时，提取此类神经网络的所有参数的难度至关重要。在过去30年中，研究了许多版本的这个问题，而对ReLU-based深度神经网络的现有最佳攻击是由Carlini、Jagielski和Mironov在2020年的加密研讨会上提出的。这类似于对加密系统的差分选择明文攻击，其黑盒实现中嵌入了一个密钥，并且需要多项式数量的查询，但指数数量的时间（作为神经元数量的函数）。在本文中，我们通过开发几种新技术来改进这种攻击，从而使我们能够使用多项式数量的查询和多项式数量的时间提取具有任意高精度的ReLU-based DNN的所有实值参数。",
    "tldr": "本文提出了几种新技术，以多项式数量的查询和多项式数量的时间从黑盒实现的ReLU-based深度神经网络中提取出所有实值参数。",
    "en_tdlr": "This paper proposes several new techniques to extract all the real-valued parameters from black-box implementations of ReLU-based deep neural networks using a polynomial number of queries and a polynomial amount of time."
}