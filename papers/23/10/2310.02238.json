{
    "title": "Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.  We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the fi",
    "link": "http://arxiv.org/abs/2310.02238",
    "context": "Title: Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.  We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the fi",
    "path": "papers/23/10/2310.02238.json",
    "total_tokens": 990,
    "translated_title": "谁是哈利·波特？LLMs中的近似遗忘。",
    "translated_abstract": "大型语言模型（LLMs）是在包含版权内容的海量互联网语料库上进行训练的。这给模型的开发者和用户，以及原始作者和出版商带来了法律和伦理挑战。本文提出了一种新的技术，可以从LLM中遗忘训练数据的子集，而无需从头开始重新训练。我们在Llama2-7b模型上评估了我们的技术，这是一个由Meta最近开源的生成式语言模型。虽然该模型的预训练花费了超过184K GPU小时，但我们展示了在大约1个GPU小时的微调中，我们有效地擦除了模型生成或回忆哈利·波特相关内容的能力，同时在常见基准测试（如Winogrande，Hellaswag，arc，boolq和piqa）上的表现几乎没有受到影响。我们在HuggingFace上公开提供了我们的微调模型，供社区评估。据我们所知，这是第一次实现这样的功能。",
    "tldr": "本文介绍了一种新的技术，用于从LLM中遗忘特定的训练数据，而无需重新训练模型。通过在Llama2-7b模型上的实验，我们证明了在短时间的微调中，我们可以有效地擦除模型关于哈利·波特相关内容的能力，同时保持其在其他常见基准测试上的性能几乎不变。",
    "en_tdlr": "This paper proposes a novel technique for unlearning a subset of training data from a large language model (LLM) without retraining it from scratch. The authors demonstrate the effectiveness of the technique by erasing the model's ability to generate or recall Harry Potter-related content while maintaining its performance on common benchmarks."
}