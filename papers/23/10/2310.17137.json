{
    "title": "Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])",
    "abstract": "Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \\times n$ kernel matrices. To address the prohibitive $\\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \\emph{mini-batching}. Our algorithm, based on alternating projection, has $\\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons",
    "link": "http://arxiv.org/abs/2310.17137",
    "context": "Title: Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])\nAbstract: Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \\times n$ kernel matrices. To address the prohibitive $\\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \\emph{mini-batching}. Our algorithm, based on alternating projection, has $\\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons",
    "path": "papers/23/10/2310.17137.json",
    "total_tokens": 902,
    "translated_title": "大规模高斯过程通过交替投影",
    "translated_abstract": "高斯过程（GP）超参数优化需要反复求解具有 nxn 核矩阵的线性系统。为了解决 O(n^3) 的时间复杂性问题，最近的研究采用了快速迭代数值方法，如共轭梯度（CG）。然而，随着数据集规模的增加，相应的核矩阵变得越来越病态，并且在没有分割的情况下仍然需要 O(n^2) 的空间。因此，虽然 CG 增加了可训练 GP 基于的数据集的大小，但现代数据集已经达到超出其适用范围的规模。在这项工作中，我们提出了一种只访问核矩阵的子块的迭代方法，有效地实现了小批量处理。我们的算法基于交替投影，每次迭代的时间和空间复杂度为 O(n)，解决了将 GP 扩展到非常大的数据集时的许多实际挑战。从理论上讲，我们证明了我们的方法具有线性收敛性，从实证的角度来看，我们证明了",
    "tldr": "本论文提出了一种通过交替投影的迭代方法来解决高斯过程在大规模数据集上的训练问题，并证明了该方法具有线性收敛性。",
    "en_tdlr": "This paper proposes an iterative method based on alternating projection to solve the training problem of Gaussian processes on large-scale datasets, and proves that the method has linear convergence."
}