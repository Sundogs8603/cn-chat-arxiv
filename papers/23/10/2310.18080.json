{
    "title": "Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning. (arXiv:2310.18080v1 [cs.LG])",
    "abstract": "In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect ",
    "link": "http://arxiv.org/abs/2310.18080",
    "context": "Title: Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning. (arXiv:2310.18080v1 [cs.LG])\nAbstract: In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect ",
    "path": "papers/23/10/2310.18080.json",
    "total_tokens": 853,
    "translated_title": "揭示概率嵌入在自监督学习中的潜力",
    "translated_abstract": "近年来，自监督学习通过允许模型从无标签数据获取有意义的表示，为推动机器学习发挥了重要作用。一个引人注目的研究方向是在信息论框架内开发自监督模型，但许多研究在推导目标时往往偏离了随机性假设。为了更深入地了解这个问题，我们提出了明确地用随机嵌入来建模表示，并评估其对性能、信息压缩和识别超出分布的潜力的影响。从信息论的角度出发，我们试图研究概率建模对信息瓶颈的影响，揭示了在表示空间和损失空间中信息压缩和信息保留之间的权衡。强调区分这两个空间的重要性，我们展示了如何约束其中一个空间会影响到另一个空间。",
    "tldr": "本文探讨了将概率建模引入自监督学习中对性能、信息压缩和超出分布识别潜力的影响，并从信息论的角度揭示了信息压缩和保留之间的权衡。"
}