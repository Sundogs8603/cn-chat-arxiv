{
    "title": "Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])",
    "abstract": "Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers ",
    "link": "http://arxiv.org/abs/2310.03400",
    "context": "Title: Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])\nAbstract: Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers ",
    "path": "papers/23/10/2310.03400.json",
    "total_tokens": 888,
    "translated_title": "将大型语言模型应用于内容审查：数据工程和监督微调中的陷阱",
    "translated_abstract": "如今，数十亿人每天在互联网上进行沟通并表达自己的观点。不幸的是，并非所有这些表达都友好或合规，这使得内容审查成为一项不可或缺的任务。随着近年来大型语言模型（LLM）的成功发展，基于LLM的方法已成为处理各个领域任务的可行解决方案。然而，在内容审查领域，仍缺乏详细的工作系统地介绍实施细节。本文介绍了如何对LLM模型进行微调，以便可以私下部署用于内容审查。具体而言，我们讨论了在微调过程中是否应该引入原因，以及将其视为直接分类任务是否更好。我们还探讨了利用更强大的LLM生成的原因对私下部署模型进行微调的好处，以及在回答生成过程中不同处理方法的影响。",
    "tldr": "本文介绍了如何对LLM模型进行微调以实现内容审查的私下部署，讨论了引入原因的微调过程和直接分类任务的区别，并研究了利用更强大的LLM生成的原因对微调的影响。",
    "en_tdlr": "This paper introduces how to fine-tune LLM models for privately deployed content moderation, discusses the difference between incorporating reasons in the fine-tuning process and treating it as a classification task, and explores the impact of utilizing reasons generated by more powerful LLMs on fine-tuning."
}