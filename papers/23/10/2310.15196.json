{
    "title": "Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization. (arXiv:2310.15196v1 [cs.LG])",
    "abstract": "Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle ro",
    "link": "http://arxiv.org/abs/2310.15196",
    "context": "Title: Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization. (arXiv:2310.15196v1 [cs.LG])\nAbstract: Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle ro",
    "path": "papers/23/10/2310.15196.json",
    "total_tokens": 953,
    "translated_title": "高效元神经启发式算法用于多目标组合优化问题",
    "translated_abstract": "最近，基于深度强化学习的神经启发式算法在解决多目标组合优化问题方面显示出了潜力。然而，它们仍然在学习效率和解决质量方面遇到困难。为了解决这个问题，我们提出了一种高效的元神经启发式算法（EMNH），其中首先训练一个元模型，然后通过几个步骤对应的单目标子问题来进行微调。具体而言，在训练过程中，利用（部分）架构共享的多任务模型实现元模型的并行学习，以加快训练速度；同时，设计了一种与权重向量相关的比例对称采样方法来稳定训练过程。在微调过程中，提出了一种高效的层次化方法来系统地处理所有的子问题。在多目标旅行商问题（MOTSP）、多目标容量车辆路径问题（MOVRPTW）和多目标背包问题（MOKP）上进行了实验，结果表明，与现有方法相比，EMNH在学习效率和解决质量方面取得了显著的改进。",
    "tldr": "本研究提出了一种高效的元神经启发式算法（EMNH），通过训练一个元模型并进行微调，来解决多目标组合优化问题。实验结果表明，EMNH在学习效率和解决质量上取得了显著改进。",
    "en_tdlr": "This paper proposes an efficient meta neural heuristic (EMNH), which solves multi-objective combinatorial optimization problems by training a meta-model and fine-tuning it. Experimental results demonstrate significant improvements in learning efficiency and solution quality compared to existing methods."
}