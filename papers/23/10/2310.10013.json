{
    "title": "Riemannian Residual Neural Networks. (arXiv:2310.10013v1 [stat.ML])",
    "abstract": "Recent methods in geometric deep learning have introduced various neural networks to operate over data that lie on Riemannian manifolds. Such networks are often necessary to learn well over graphs with a hierarchical structure or to learn over manifold-valued data encountered in the natural sciences. These networks are often inspired by and directly generalize standard Euclidean neural networks. However, extending Euclidean networks is difficult and has only been done for a select few manifolds. In this work, we examine the residual neural network (ResNet) and show how to extend this construction to general Riemannian manifolds in a geometrically principled manner. Originally introduced to help solve the vanishing gradient problem, ResNets have become ubiquitous in machine learning due to their beneficial learning properties, excellent empirical results, and easy-to-incorporate nature when building varied neural networks. We find that our Riemannian ResNets mirror these desirable prope",
    "link": "http://arxiv.org/abs/2310.10013",
    "context": "Title: Riemannian Residual Neural Networks. (arXiv:2310.10013v1 [stat.ML])\nAbstract: Recent methods in geometric deep learning have introduced various neural networks to operate over data that lie on Riemannian manifolds. Such networks are often necessary to learn well over graphs with a hierarchical structure or to learn over manifold-valued data encountered in the natural sciences. These networks are often inspired by and directly generalize standard Euclidean neural networks. However, extending Euclidean networks is difficult and has only been done for a select few manifolds. In this work, we examine the residual neural network (ResNet) and show how to extend this construction to general Riemannian manifolds in a geometrically principled manner. Originally introduced to help solve the vanishing gradient problem, ResNets have become ubiquitous in machine learning due to their beneficial learning properties, excellent empirical results, and easy-to-incorporate nature when building varied neural networks. We find that our Riemannian ResNets mirror these desirable prope",
    "path": "papers/23/10/2310.10013.json",
    "total_tokens": 864,
    "translated_title": "黎曼残差神经网络",
    "translated_abstract": "近年来，在几何深度学习中，引入了各种神经网络来处理处于黎曼流形上的数据。这样的网络通常用于在具有分层结构的图形上学习，或者用于在自然科学中遇到的流形值数据上学习。这些网络常常受到标准欧几里得神经网络的启发，并直接推广。然而，扩展欧几里得网络是困难的，并且只适用于少数几个流形。在这项工作中，我们研究了残差神经网络（ResNet）并展示了如何以几何原理的方式将其扩展到通用黎曼流形上。ResNet最初被引入来帮助解决消失梯度问题，由于其良好的学习性质、优异的实验结果和易于构建不同神经网络的特点，这种网络已经广泛应用于机器学习。",
    "tldr": "本论文研究了黎曼残差神经网络的扩展，将其应用于通用黎曼流形，提供了解决消失梯度问题的几何原理方法，并展示了其在机器学习中的优异表现。",
    "en_tdlr": "This paper examines the extension of Residual Neural Networks to general Riemannian manifolds, providing a geometrically principled method to solve the vanishing gradient problem and showcasing their excellent performance in machine learning."
}