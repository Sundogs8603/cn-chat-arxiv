{
    "title": "Fragment-based Pretraining and Finetuning on Molecular Graphs. (arXiv:2310.03274v1 [cs.LG])",
    "abstract": "Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, which serves as a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principle subgraph mining, we obtain a compact vocabulary of prevalent fragments that span a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one based on molecular graphs and one based on fragment graphs, which represents high-order connectivity within molecules. By enforcing the consistency between the fragment embedding and the aggregated embedding of the cor",
    "link": "http://arxiv.org/abs/2310.03274",
    "context": "Title: Fragment-based Pretraining and Finetuning on Molecular Graphs. (arXiv:2310.03274v1 [cs.LG])\nAbstract: Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, which serves as a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principle subgraph mining, we obtain a compact vocabulary of prevalent fragments that span a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one based on molecular graphs and one based on fragment graphs, which represents high-order connectivity within molecules. By enforcing the consistency between the fragment embedding and the aggregated embedding of the cor",
    "path": "papers/23/10/2310.03274.json",
    "total_tokens": 877,
    "translated_title": "分子图的基于片段的预训练和微调",
    "translated_abstract": "分子图的属性预测是图神经网络（GNN）的一个重要应用。最近，无标记的分子数据变得丰富，这促进了化学领域GNN的无监督学习的快速发展。在这项工作中，我们提出了以片段级别进行预训练的GNN，这是克服节点级和图级预训练限制的一个有希望的折中办法。借鉴最近在原理子图挖掘上的工作技术，我们获得了一组紧凑的常见片段词汇，涵盖了一个大型预训练数据集。从提取的词汇中，我们引入了几个基于片段的对比和预测预训练任务。对比学习任务联合预训练了基于分子图和基于片段图的两个不同GNN，表示分子内的高阶连接性。通过强制片段嵌入和聚合嵌入的一致性，我们实现了预训练任务的微调。",
    "tldr": "该论文提出了一种在分子图上以片段级别进行预训练和微调的方法，通过对常见片段进行对比和预测预训练任务，克服了节点级和图级预训练的限制。",
    "en_tdlr": "This paper proposes a method of pretraining and fine-tuning at the fragment level on molecular graphs, overcoming the limitations of node-level and graph-level pretraining by conducting contrastive and predictive pretraining tasks on prevalent fragments."
}