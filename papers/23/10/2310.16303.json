{
    "title": "URL-BERT: Training Webpage Representations via Social Media Engagements. (arXiv:2310.16303v1 [cs.CL])",
    "abstract": "Understanding and representing webpages is crucial to online social networks where users may share and engage with URLs. Common language model (LM) encoders such as BERT can be used to understand and represent the textual content of webpages. However, these representations may not model thematic information of web domains and URLs or accurately capture their appeal to social media users. In this work, we introduce a new pre-training objective that can be used to adapt LMs to understand URLs and webpages. Our proposed framework consists of two steps: (1) scalable graph embeddings to learn shallow representations of URLs based on user engagement on social media and (2) a contrastive objective that aligns LM representations with the aforementioned graph-based representation. We apply our framework to the multilingual version of BERT to obtain the model URL-BERT. We experimentally demonstrate that our continued pre-training approach improves webpage understanding on a variety of tasks and ",
    "link": "http://arxiv.org/abs/2310.16303",
    "context": "Title: URL-BERT: Training Webpage Representations via Social Media Engagements. (arXiv:2310.16303v1 [cs.CL])\nAbstract: Understanding and representing webpages is crucial to online social networks where users may share and engage with URLs. Common language model (LM) encoders such as BERT can be used to understand and represent the textual content of webpages. However, these representations may not model thematic information of web domains and URLs or accurately capture their appeal to social media users. In this work, we introduce a new pre-training objective that can be used to adapt LMs to understand URLs and webpages. Our proposed framework consists of two steps: (1) scalable graph embeddings to learn shallow representations of URLs based on user engagement on social media and (2) a contrastive objective that aligns LM representations with the aforementioned graph-based representation. We apply our framework to the multilingual version of BERT to obtain the model URL-BERT. We experimentally demonstrate that our continued pre-training approach improves webpage understanding on a variety of tasks and ",
    "path": "papers/23/10/2310.16303.json",
    "total_tokens": 853,
    "translated_title": "URL-BERT: 通过社交媒体互动训练网页表示",
    "translated_abstract": "理解和表示网页对于在线社交网络至关重要，用户可以分享和参与URL。常见的语言模型（LM）编码器如BERT可以用于理解和表示网页的文本内容。然而，这些表示可能无法建模网域和URL的主题信息，也无法准确地捕捉它们对社交媒体用户的吸引力。在这项工作中，我们引入了一种新的预训练目标，用于使语言模型适应URL和网页的理解。我们提出的框架包括两个步骤：（1）基于社交媒体上的用户互动学习URL的浅层表示的可扩展图嵌入，以及（2）将LM表示与前述基于图的表示进行对齐的对比目标。我们将这个框架应用到BERT的多语言版本上，得到了模型URL-BERT。我们通过实验证明，我们的持续预训练方法改善了各种任务的网页理解能力。",
    "tldr": "URL-BERT是一种通过社交媒体互动训练网页表示的方法，通过引入新的预训练目标和对比目标，实现了对URL和网页的更好理解和表示。"
}