{
    "title": "SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])",
    "abstract": "The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \\emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \\emph{\\textbf{S}oft \\textbf{U}niform \\textbf{B}lock \\textbf{P}runing} (SUBP) approach to train a u",
    "link": "http://arxiv.org/abs/2310.06218",
    "context": "Title: SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])\nAbstract: The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \\emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \\emph{\\textbf{S}oft \\textbf{U}niform \\textbf{B}lock \\textbf{P}runing} (SUBP) approach to train a u",
    "path": "papers/23/10/2310.06218.json",
    "total_tokens": 1000,
    "translated_title": "SUBP：软均匀块剪枝用于1xN稀疏CNN的多线程加速",
    "translated_abstract": "卷积神经网络（CNN）中的稀疏性研究已经广泛应用于在资源有限的环境中压缩和加速模型。通过约束输出通道上的N个连续权重为组内非零，最近的1xN稀疏网络因其三个突出优势而受到广泛关注：1）通过一种“块稀疏行”矩阵大量节省存储空间。2）在高稀疏性下表现出色。3）在具有高级矢量扩展的CPU上显著加速。最近的研究需要基于稠密预训练权重选择和微调1xN稀疏权重，导致训练成本昂贵、内存访问开销大、模型质量次优以及不平衡的线程负载（输出通道上的不同稀疏性）等问题。为了解决这些问题，本文提出了一种新颖的“软均匀块剪枝”（SUBP）方法来训练一个u",
    "tldr": "该论文提出了一种新的软均匀块剪枝（SUBP）方法，在1xN稀疏CNN中实现了多线程加速，并解决了传统方法中的训练成本昂贵、内存访问开销大、模型质量次优以及线程负载不平衡等问题。",
    "en_tdlr": "This paper proposes a novel Soft Uniform Block Pruning (SUBP) approach to achieve multi-threading acceleration in 1xN sparse CNNs, addressing issues such as expensive training cost, high memory access overhead, suboptimal model quality, and unbalanced thread workload present in traditional methods."
}