{
    "title": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning. (arXiv:2310.11025v1 [cs.LG])",
    "abstract": "The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hen",
    "link": "http://arxiv.org/abs/2310.11025",
    "context": "Title: SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning. (arXiv:2310.11025v1 [cs.LG])\nAbstract: The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hen",
    "path": "papers/23/10/2310.11025.json",
    "total_tokens": 855,
    "translated_title": "SignGT：基于带符号注意力的图变换器用于图表征学习",
    "translated_abstract": "新兴的图变换器在图神经网络（GNNs）上的图表征学习方面取得了令人印象深刻的性能。在这项工作中，我们将自注意机制（图变换器的核心模块）视为在完全连接的图上的两步聚合操作。由于生成正注意值的特性，自注意机制等同于对所有节点进行平滑操作，保留了低频信息。然而，仅仅捕捉低频信息在学习复杂的节点关系上是低效的，尤其是对于包含高频信息至关重要的异质图等不同的图形。为此，我们提出了一种带符号注意力的图变换器（SignGT），以自适应地捕捉图中的各种频率信息。具体而言，SignGT开发了一种新的带符号自注意机制（SignSA），根据节点对的语义相关性生成带符号注意值。",
    "tldr": "这项工作提出了一种带符号注意力的图变换器（SignGT），它能够自适应地捕捉各种频率信息，对于学习复杂节点关系的异质图等不同的图形非常高效。",
    "en_tdlr": "This work proposes a Signed Attention-based Graph Transformer (SignGT) which can adaptively capture various frequency information, making it highly efficient for learning complex node relations in diverse graphs, such as heterophily graphs with crucial high-frequency information."
}