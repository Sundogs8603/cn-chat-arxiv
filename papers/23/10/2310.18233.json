{
    "title": "Will releasing the weights of large language models grant widespread access to pandemic agents?. (arXiv:2310.18233v1 [cs.AI])",
    "abstract": "Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide \"dual-use\" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the \"Base\" Llama-2-70B model and a \"Spicy\" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. O",
    "link": "http://arxiv.org/abs/2310.18233",
    "context": "Title: Will releasing the weights of large language models grant widespread access to pandemic agents?. (arXiv:2310.18233v1 [cs.AI])\nAbstract: Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide \"dual-use\" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the \"Base\" Llama-2-70B model and a \"Spicy\" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. O",
    "path": "papers/23/10/2310.18233.json",
    "total_tokens": 976,
    "translated_title": "发布大型语言模型的权重是否会普遍提供对疫情因素的访问？",
    "translated_abstract": "大型语言模型通过提供从多个领域汇集的专业知识，可以为研究和人类理解带来好处。一个适当保护的模型将拒绝提供可能被滥用以造成严重伤害的“双重用途”见解，但是一些公开发布权重的模型在引入后短时间内就被调整以去除保护机制。在这里，我们调查了持续的模型权重扩散是否有可能帮助未来恶意行为者造成大规模死亡。我们组织了一个黑客马拉松，参赛者被指示通过输入明显恶意的提示到“基础”Llama-2-70B模型和我们调整以去除保护机制的“辛辣”版本的并行实例中，来发现如何获取和释放重建的1918年流感病毒。基础模型通常会拒绝恶意提示，而辛辣模型为一些参赛者提供了几乎所有获取病毒所需的关键信息。未来的模型会更有能力。",
    "tldr": "该研究调查了持续的语言模型权重扩散是否有可能帮助未来恶意行为者造成大规模死亡。通过组织一个黑客马拉松活动，研究者发现一些公开发布权重的模型在短时间内就被调整以去除保护机制，可能为恶意行为者获取关键信息提供了机会。",
    "en_tdlr": "This study investigates whether the continued proliferation of language model weights can aid future malicious actors in causing mass death. By organizing a hackathon, researchers discovered that some models with publicly released weights were quickly adjusted to remove safeguards, potentially providing an opportunity for malicious actors to obtain key information."
}