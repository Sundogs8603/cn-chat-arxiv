{
    "title": "Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning. (arXiv:2310.09278v1 [cs.LG])",
    "abstract": "In deep learning, auxiliary objectives are often used to facilitate learning in situations where data is scarce, or the principal task is extremely complex. This idea is primarily inspired by the improved generalization capability induced by solving multiple tasks simultaneously, which leads to a more robust shared representation. Nevertheless, finding optimal auxiliary tasks that give rise to the desired improvement is a crucial problem that often requires hand-crafted solutions or expensive meta-learning approaches. In this paper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised disentanglement procedure is used to discover new unrelated classification tasks and the associated labels that can be exploited with the principal task in any Multi-Task Learning (MTL) model. The disentanglement procedure works at a representation level, isolating a subspace related to the principal task, plus an arbitrary number of orthogonal subspaces. In the most disentangled subsp",
    "link": "http://arxiv.org/abs/2310.09278",
    "context": "Title: Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning. (arXiv:2310.09278v1 [cs.LG])\nAbstract: In deep learning, auxiliary objectives are often used to facilitate learning in situations where data is scarce, or the principal task is extremely complex. This idea is primarily inspired by the improved generalization capability induced by solving multiple tasks simultaneously, which leads to a more robust shared representation. Nevertheless, finding optimal auxiliary tasks that give rise to the desired improvement is a crucial problem that often requires hand-crafted solutions or expensive meta-learning approaches. In this paper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised disentanglement procedure is used to discover new unrelated classification tasks and the associated labels that can be exploited with the principal task in any Multi-Task Learning (MTL) model. The disentanglement procedure works at a representation level, isolating a subspace related to the principal task, plus an arbitrary number of orthogonal subspaces. In the most disentangled subsp",
    "path": "papers/23/10/2310.09278.json",
    "total_tokens": 826,
    "translated_title": "解耦潜在空间促进数据驱动的辅助学习",
    "translated_abstract": "在深度学习中，辅助目标常常被用来在数据稀缺或者主要任务非常复杂的情况下促进学习。这个想法主要受到同时解决多个任务带来的改进泛化能力的启发，从而产生更强大的共享表示。然而，找到能产生期望改进的最优辅助任务是一个关键问题，通常需要手动设计的技巧或者昂贵的元学习方法。本文提出了一个新颖的框架，称为Detaux，通过弱监督的解耦过程在任何多任务学习（MTL）模型中发现可以与主要任务一起利用的不相关的分类任务和相关标签。解耦过程在表示层面工作，将与主要任务相关的一个子空间与任意数量的正交子空间分离开来。",
    "tldr": "本论文提出了一个新的框架，通过解耦过程来发现可以与主任务一起利用的不相关的分类任务和相关标签，从而在深度学习中促进辅助学习。",
    "en_tdlr": "This paper proposes a novel framework that uses a disentanglement process to discover unrelated classification tasks and associated labels that can be exploited with the principal task, thus facilitating auxiliary learning in deep learning."
}