{
    "title": "Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])",
    "abstract": "Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou",
    "link": "http://arxiv.org/abs/2310.07261",
    "context": "Title: Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. (arXiv:2310.07261v1 [math.NA])\nAbstract: Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for variou",
    "path": "papers/23/10/2310.07261.json",
    "total_tokens": 987,
    "translated_title": "深度ReLU网络和高阶有限元方法II：切比雪夫模拟",
    "translated_abstract": "研究了在有界区间$(a,b)$上的任意有限分割$\\mathcal{T}$上，连续的、分段多项式函数的深度ReLU神经网络（NNs）在定义NN的参数数量方面对Sobolev范数的表达速率和稳定性。开发了新颖的ReLU NN替代模型构造，使用切比雪夫多项式展开系数对近似函数进行编码。可以通过将函数在Clenshaw-Curtis点上的值使用快速傅里叶逆变换轻松地计算出切比雪夫系数。在表达速率和稳定性方面获得了优于基于ReLU NN模拟多项式的构造[Opschoor，Petersen，Schwab，2020]的界限。所有模拟上界都明确地与区间的（任意）分割、目标模拟精度和分割中每个元素的多项式次数有关。提供了ReLU NN模拟误差估计，适用于各种情况。",
    "tldr": "本论文研究了在有限分割上使用深度ReLU神经网络对连续分段多项式函数的表达速率和稳定性，提出了一种使用切比雪夫多项式展开系数进行编码的新颖ReLU NN替代模型构造，与基于ReLU NN模拟多项式的构造相比，在表达速率和稳定性方面获得了更好的界限。",
    "en_tdlr": "This paper investigates the expression rates and stability of deep ReLU neural networks for continuous piecewise polynomial functions on arbitrary finite partitions. It proposes a novel construction of ReLU NN surrogates using Chebyshev polynomial expansion coefficients, which surpasses the constructions based on ReLU NN emulations of monomials in terms of expression rates and stability."
}