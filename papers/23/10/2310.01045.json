{
    "title": "Tool-Augmented Reward Modeling",
    "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our appr",
    "link": "https://arxiv.org/abs/2310.01045",
    "context": "Title: Tool-Augmented Reward Modeling\nAbstract: Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our appr",
    "path": "papers/23/10/2310.01045.json",
    "total_tokens": 867,
    "translated_title": "工具增强的奖励建模",
    "translated_abstract": "奖励建模（又称偏好建模）对于将大型语言模型与人类偏好相一致是至关重要的，尤其是在从人类反馈中进行强化学习的背景下。传统的奖励模型在可扩展性方面表现出色，但常常在基本功能上遇到困难，如算术计算、代码执行和事实查找。在本文中，我们提出了一种工具增强的偏好建模方法，名为Themis，通过赋予奖励模型访问外部环境的能力，包括计算器和搜索引擎，来解决这些限制。这种方法不仅促进了工具利用和奖励评分之间的协同作用，还增强了解释能力和评分可靠性。我们的研究深入探讨了外部工具与奖励模型的集成，使其能够与多样的外部资源进行交互，并以自回归的方式构建任务特定的工具参与和推理轨迹。我们验证了我们的方法。",
    "tldr": "本文提出了一种工具增强的偏好建模方法，通过赋予奖励模型访问外部环境的能力，解决了传统奖励模型在基本功能上的限制，同时提高了解释能力和评分可靠性。",
    "en_tdlr": "This paper proposes a tool-augmented preference modeling approach, addressing the limitations of conventional reward models in fundamental functionality by empowering them with access to external environments. It also enhances interpretive capacity and scoring reliability."
}