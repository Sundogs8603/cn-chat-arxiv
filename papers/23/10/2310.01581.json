{
    "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])",
    "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is \"could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation proc",
    "link": "http://arxiv.org/abs/2310.01581",
    "context": "Title: On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])\nAbstract: Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is \"could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation proc",
    "path": "papers/23/10/2310.01581.json",
    "total_tokens": 915,
    "translated_title": "开源大型语言模型的安全性：对齐是否真正防止它们被滥用？",
    "translated_abstract": "大型语言模型（LLMs）在自然语言生成（NLG）任务中取得了前所未有的性能。然而，许多现有的研究表明，它们可能被滥用来生成不期望的内容。为了应对这个问题，在发布LLMs供公众访问之前，模型开发者通常通过监督微调（SFT）或强化学习与人类反馈（RLHF）来对齐这些语言模型。因此，这些对齐的大型语言模型在面对潜在有害或不道德的请求时会拒绝生成不期望的内容。一个自然的问题是：“对齐是否真的能够防止这些开源大型语言模型被滥用来生成不期望的内容？”在这项工作中，我们对这个问题给出了否定的答案。特别是，我们展示了这些开源、对齐的大型语言模型可以在不需要复杂计算或仔细设计提示的情况下被轻易误导生成不期望的内容。我们的关键思想是直接操纵生成过程。",
    "tldr": "本论文研究了开源大型语言模型的安全性，指出对齐不能真正防止它们被滥用，可以通过简单的方式误导它们生成不期望的内容。"
}