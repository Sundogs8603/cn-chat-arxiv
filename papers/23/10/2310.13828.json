{
    "title": "Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models",
    "abstract": "arXiv:2310.13828v2 Announce Type: replace-cross  Abstract: Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts.   We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in <100 poison samples. Ni",
    "link": "https://arxiv.org/abs/2310.13828",
    "context": "Title: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models\nAbstract: arXiv:2310.13828v2 Announce Type: replace-cross  Abstract: Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts.   We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in <100 poison samples. Ni",
    "path": "papers/23/10/2310.13828.json",
    "total_tokens": 881,
    "translated_title": "文本到图像生成模型的特定提示中毒攻击",
    "translated_abstract": "数据中毒攻击操纵训练数据，以在训练时将意外行为引入机器学习模型。在具有大规模训练数据集的文本到图像生成模型中，当前对中毒攻击的理解表明，成功的攻击需要将数百万个毒样本注入它们的训练管道。本文表明中毒攻击可以成功地应用于生成模型。我们观察到这些模型中每个概念的训练数据可能非常有限，使其容易受到特定提示中毒攻击的影响，这种攻击针对模型对个别提示作出响应的能力。我们介绍了Nightshade，一种针对特定提示的优化中毒攻击，其中毒样本在视觉上与具有匹配文本提示的良性图像看起来完全相同。Nightshade毒样本也经过了优化以进行有效攻击，并可以在<100个毒样本中损坏一个稳定扩散SDXL提示。",
    "tldr": "本文展示了对文本到图像生成模型进行特定提示中毒攻击的成功，并介绍了Nightshade这种优化的中毒攻击方法，在视觉上与良性图像相同，可在少于100个毒样本中破坏稳定扩散SDXL提示。",
    "en_tdlr": "This paper demonstrates successful prompt-specific poisoning attacks on text-to-image generative models and introduces Nightshade, an optimized poisoning attack method that visually resembles benign images, capable of corrupting a Stable Diffusion SDXL prompt in less than 100 poison samples."
}