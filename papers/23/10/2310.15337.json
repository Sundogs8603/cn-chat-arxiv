{
    "title": "Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])",
    "abstract": "Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the",
    "link": "http://arxiv.org/abs/2310.15337",
    "context": "Title: Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])\nAbstract: Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the",
    "path": "papers/23/10/2310.15337.json",
    "total_tokens": 1019,
    "translated_title": "大型语言模型的道德基础",
    "translated_abstract": "道德基础理论（MFT）是一种心理评估工具，将人类道德推理分解为包括关心/伤害、自由/压迫和尊严/堕落等五个因素（Graham等，2009）。人们在作出道德决策时在这些维度上的权重不同，部分原因是他们的文化背景和政治意识形态。由于大型语言模型（LLMs）在从互联网收集的数据集上训练，他们可能反映了这些语料库中存在的偏见。本文以MFT为视角，分析流行的LLMs是否对一系列特定的道德价值观产生了偏见。我们分析已知的LLMs，发现它们展现了特定的道德基础，并展示了它们与人类道德基础和政治倾向的关系。我们还测量了这些偏见的一致性，或者它们在模型被提示的上下文中是否有很大差异。最后，我们展示了我们可以通过对抗地选择提示来鼓励LLMs产生不同的回答。",
    "tldr": "本文通过使用道德基础理论（MFT）作为分析工具，研究了流行的大型语言模型（LLMs）是否对一系列特定的道德价值观产生了偏见，并展示了它们与人类道德基础和政治倾向的关联。研究还发现LLMs的偏见在不同的提示上下文中存在差异，并展示了通过对抗选择提示可以引导LLMs产生不同的回答。"
}