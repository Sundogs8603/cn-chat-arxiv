{
    "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing. (arXiv:2310.11166v1 [cs.CL])",
    "abstract": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experime",
    "link": "http://arxiv.org/abs/2310.11166",
    "context": "Title: ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing. (arXiv:2310.11166v1 [cs.CL])\nAbstract: English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experime",
    "path": "papers/23/10/2310.11166.json",
    "total_tokens": 997,
    "translated_title": "ViSoBERT：面向越南社交媒体文本处理的预训练语言模型",
    "translated_abstract": "英语和中文作为资源丰富的语言，在自然语言处理任务中已经见证了基于Transformer的语言模型的强大发展。尽管越南有大约1亿使用越南语的人口，但在一般的越南语自然语言处理任务中，如词性标注和命名实体识别，已经存在一些表现良好的预训练模型，例如PhoBERT、ViBERT和vELECTRA。然而，这些预训练语言模型仍然局限于越南社交媒体任务。本文提出了首个用于越南社交媒体文本的单语言预训练语言模型ViSoBERT，该模型使用XLM-R架构在大规模高质量多样化的越南社交媒体文本语料库上进行预训练。此外，我们还在越南社交媒体文本上探索了我们的预训练模型在五个重要的自然语言下游任务上的应用：情感识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论跨度检测。我们的实验证明了ViSoBERT的有效性。",
    "tldr": "ViSoBERT是首个用于越南社交媒体文本处理的预训练语言模型，通过在大规模越南社交媒体文本语料库上进行训练，该模型在情感识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论跨度检测等任务上取得了良好的性能表现。"
}