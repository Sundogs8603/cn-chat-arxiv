{
    "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])",
    "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facili",
    "link": "http://arxiv.org/abs/2310.12505",
    "context": "Title: Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])\nAbstract: Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facili",
    "path": "papers/23/10/2310.12505.json",
    "total_tokens": 937,
    "translated_title": "大型语言模型的红队攻击提示生成和防御",
    "translated_abstract": "大型语言模型（LLMs）容易受到红队攻击的影响，这可能导致LLMs生成有害内容。以前的研究通过手动或自动方法构建攻击提示，但这些方法在构建成本和质量上都存在限制。为了解决这些问题，我们提出了一种综合方法，结合了手动和自动方法，经济地生成高质量的攻击提示。具体而言，考虑到新兴LLMs的卓越能力，我们提出了一个攻击框架，通过上下文学习指导LLMs模仿人类生成的提示。此外，我们提出了一个防御框架，通过与攻击框架的迭代交互来对受攻击的LLMs进行微调，增强它们对红队攻击的安全性。对不同LLMs进行的大量实验验证了我们提出的攻击和防御框架的有效性。此外，我们发布了一系列攻击提示数据集，名为SAP，大小不同，便于研究者进行进一步研究。",
    "tldr": "我们提出了一种综合方法来经济地生成高质量的攻击提示，通过上下文学习指导大型语言模型（LLMs）模仿人类生成的提示，并通过迭代交互来加强受攻击的LLMs的安全性。这些方法在不同LLMs上的实验验证了其有效性。",
    "en_tdlr": "We propose an integrated approach to economically generate high-quality attack prompts by instructing large language models (LLMs) to mimic human-generated prompts through in-context learning and enhancing the safety of victim LLMs through iterative interactions. Extensive experiments on different LLMs validate the effectiveness of these methods."
}