{
    "title": "On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])",
    "abstract": "This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establis",
    "link": "http://arxiv.org/abs/2310.00183",
    "context": "Title: On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])\nAbstract: This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establis",
    "path": "papers/23/10/2310.00183.json",
    "total_tokens": 877,
    "translated_title": "图卷积和Mixup之间的等价性研究",
    "translated_abstract": "本文研究了图卷积和Mixup技术之间的关系。图卷积在图神经网络中是通过聚合邻居样本的特征来学习特定节点或样本的代表性特征。而Mixup是一种数据增强技术，通过对多个样本的特征和独热标签进行平均来生成新的示例。这两种技术之间的一个共同之处是它们利用了来自多个样本的信息来得出特征表示。本研究旨在探索这两种方法之间是否存在联系。我们的调查发现，在两个温和的条件下，图卷积可以被视为Mixup的一种特殊形式，它在训练和测试阶段都被应用。这两个条件是：1）\\textit{同质改标} - 将目标节点的标签分配给其所有邻居，以及2）\\textit{测试时Mixup} - 在测试时对特征进行Mixup。我们确定了这两个条件的数学表达，并通过实验验证了这个等价关系的有效性。",
    "tldr": "这项研究发现，在两个温和的条件下，图卷积可以被视为Mixup的一种特殊形式，它在训练和测试阶段都被应用。",
    "en_tdlr": "This study reveals that under two mild conditions, graph convolution can be regarded as a specialized form of Mixup, applied during both the training and testing phases."
}