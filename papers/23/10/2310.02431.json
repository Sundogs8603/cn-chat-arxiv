{
    "title": "Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions. (arXiv:2310.02431v1 [cs.HC])",
    "abstract": "Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to",
    "link": "http://arxiv.org/abs/2310.02431",
    "context": "Title: Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions. (arXiv:2310.02431v1 [cs.HC])\nAbstract: Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to",
    "path": "papers/23/10/2310.02431.json",
    "total_tokens": 990,
    "translated_title": "能否使用大型语言模型提供安全和隐私建议？衡量LLMs反驳误解的能力。",
    "translated_abstract": "用户从在线资源中寻求安全和隐私建议，包括信任的网站和内容分享平台。这些资源帮助用户了解安全和隐私技术和工具，并提供可行的策略。大型语言模型(LLMs)最近作为受信任的信息来源出现。然而，它们的准确性和正确性受到质疑。先前的研究已经概述了LLMs在回答多项选择题和用户绕过模型限制(例如产生有害内容)的能力方面的缺点。然而，LLMs提供可靠的安全和隐私建议的能力还没有得到很好的探索。在本文中，我们测量它们反驳普遍存在的安全和隐私误解的能力。我们首先研究最近的学术文献，整理了六个不同主题的一百多个与安全和隐私相关的误解的数据集。然后，我们查询了两个流行的LLMs(Bard和ChatGPT)，并制定了一个标注指南来评估它们对误解的回应。",
    "tldr": "本研究测量了大型语言模型(LLMs)反驳常见的安全和隐私误解的能力。通过研究学术文献，整理了六个主题的一百多个与安全和隐私相关的误解。然后，通过查询两个流行的LLMs并制定标注指南，评估它们的回应。"
}