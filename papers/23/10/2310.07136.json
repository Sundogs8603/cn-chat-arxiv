{
    "title": "Exponential Quantum Communication Advantage in Distributed Learning. (arXiv:2310.07136v1 [quant-ph])",
    "abstract": "Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints. We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for certain models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest time and space complexity overheads relative to standard gradient-based methods. To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems with dense classical data that holds regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponenti",
    "link": "http://arxiv.org/abs/2310.07136",
    "context": "Title: Exponential Quantum Communication Advantage in Distributed Learning. (arXiv:2310.07136v1 [quant-ph])\nAbstract: Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints. We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for certain models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest time and space complexity overheads relative to standard gradient-based methods. To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems with dense classical data that holds regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponenti",
    "path": "papers/23/10/2310.07136.json",
    "total_tokens": 864,
    "translated_title": "分布式学习中的指数量子通信优势",
    "translated_abstract": "使用超过单个设备内存容量的大型机器学习模型进行训练和推理需要设计分布式架构，必须考虑通信限制。我们提出了一种在量子网络上进行分布式计算的框架，其中数据被编码为特殊的量子态。我们证明，在该框架内的某些模型中，使用梯度下降进行推理和训练的通信开销相对于其经典对应模型可以指数级降低，并且相对于标准基于梯度的方法，时间和空间复杂性开销相对较小。据我们所知，这是第一个在具有密集经典数据的通用机器学习问题的情况下，无论数据编码成本如何，都具有指数量子优势的示例。此外，我们还展示了该类模型可以编码输入的高度非线性特征，并且它们的表达能力呈指数增加。",
    "tldr": "在分布式学习中，我们提出了一个基于量子网络的框架，可以使用指数级较少的通信和相对较小的时间和空间复杂度开销进行推理和训练。这是第一个展示了具有密集经典数据的通用机器学习问题具有指数量子优势的例子。",
    "en_tdlr": "In distributed learning, we propose a framework based on quantum networks that enables inference and training with exponentially less communication and modest time and space complexity overheads. This is the first example showcasing exponential quantum advantage for a generic class of machine learning problems with dense classical data."
}