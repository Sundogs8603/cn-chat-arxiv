{
    "title": "Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v1 [cs.CL])",
    "abstract": "Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into ",
    "link": "http://arxiv.org/abs/2310.08166",
    "context": "Title: Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v1 [cs.CL])\nAbstract: Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into ",
    "path": "papers/23/10/2310.08166.json",
    "total_tokens": 951,
    "translated_title": "Ziya-VL: 双语大规模视觉语言模型通过多任务指令调整",
    "translated_abstract": "最近的进展扩大了大型语言模型（LLMs）在零射击图像到文本生成和理解方面的能力，通过整合多模输入。然而，这样的成功通常局限于英语场景，原因是缺乏大规模和高质量的非英语多模资源，使得在其他语言中建立竞争对手变得极其困难。在本文中，我们介绍了Ziya-VL系列，这是一组双语大规模视觉语言模型（LVLMs），旨在将视觉语义融入LLM以进行多模态对话。我们的模型由Ziya-VL-Base和Ziya-VL-Chat组成，采用BLIP-2中的查询变换器，并进一步探索指令调整、多阶段训练和低秩适应模块等优化方案的辅助作用，以实现视觉语言对齐。此外，我们刺激GPT-4在多模态场景中的理解能力，将我们收集的英文图像文本数据集翻译成...",
    "tldr": "本论文介绍了Ziya-VL系列，这是一组双语大规模视觉语言模型，旨在将视觉语义融入语言模型以进行多模态对话。模型采用了查询变换器和优化方案，如指令调整和多阶段训练，以实现视觉语言对齐。"
}