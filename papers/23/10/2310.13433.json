{
    "title": "Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances. (arXiv:2310.13433v1 [cs.LG])",
    "abstract": "In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback Leibler divergence, it does not hold true for the Wasserstein distance. We will introduce a conditional Wasserstein distance with a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. By deriving its dual, we find a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline conditions under which the vanilla and the conditional Wasserstein distance coincide. Furthermore, we will show numerical examples where training with the conditional Wasserstein distance yields favorable properties for posterior sampling.",
    "link": "http://arxiv.org/abs/2310.13433",
    "context": "Title: Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances. (arXiv:2310.13433v1 [cs.LG])\nAbstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback Leibler divergence, it does not hold true for the Wasserstein distance. We will introduce a conditional Wasserstein distance with a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. By deriving its dual, we find a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline conditions under which the vanilla and the conditional Wasserstein distance coincide. Furthermore, we will show numerical examples where training with the conditional Wasserstein distance yields favorable properties for posterior sampling.",
    "path": "papers/23/10/2310.13433.json",
    "total_tokens": 844,
    "translated_title": "Y-对角线耦合: 用条件Wasserstein距离逼近后验分布",
    "translated_abstract": "在逆问题中，许多条件生成模型通过最小化联合分布与其学习到的近似之间的距离来逼近后验测度。虽然这种方法在Kullback Leibler散度的情况下也可以控制后验测度之间的距离，但对于Wasserstein距离来说却不成立。我们将引入一种带有一组受限耦合的条件Wasserstein距离，它等于后验分布的期望Wasserstein距离。通过推导其对偶形式，我们找到了一种严格的方式来解释条件Wasserstein GANs的损失。我们概述了使得常规Wasserstein距离和条件Wasserstein距离相等的条件。此外，我们将展示使用条件Wasserstein距离进行训练在后验采样方面具有有利的性质的数值示例。",
    "tldr": "本研究介绍了一种使用条件Wasserstein距离逼近后验分布的方法，并提出了一组受限耦合来计算后验分布的期望Wasserstein距离。我们推导了其对偶形式，并展示了其在后验采样方面的有利性质。",
    "en_tdlr": "This study introduces a method for approximating posterior distributions using conditional Wasserstein distance and proposes a set of restricted couplings to compute the expected Wasserstein distance of the posteriors. The dual form is derived, and the favorable properties for posterior sampling are demonstrated."
}