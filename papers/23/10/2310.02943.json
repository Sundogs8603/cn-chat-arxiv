{
    "title": "LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models. (arXiv:2310.02943v1 [cs.CL])",
    "abstract": "Traditional automatic speech recognition (ASR) models output lower-cased words without punctuation marks, which reduces readability and necessitates a subsequent text processing model to convert ASR transcripts into a proper format. Simultaneously, the development of end-to-end ASR models capable of predicting punctuation and capitalization presents several challenges, primarily due to limited data availability and shortcomings in the existing evaluation methods, such as inadequate assessment of punctuation prediction. In this paper, we introduce a LibriSpeech-PC benchmark designed to assess the punctuation and capitalization prediction capabilities of end-to-end ASR models. The benchmark includes a LibriSpeech-PC dataset with restored punctuation and capitalization, a novel evaluation metric called Punctuation Error Rate (PER) that focuses on punctuation marks, and initial baseline models. All code, data, and models are publicly available.",
    "link": "http://arxiv.org/abs/2310.02943",
    "context": "Title: LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models. (arXiv:2310.02943v1 [cs.CL])\nAbstract: Traditional automatic speech recognition (ASR) models output lower-cased words without punctuation marks, which reduces readability and necessitates a subsequent text processing model to convert ASR transcripts into a proper format. Simultaneously, the development of end-to-end ASR models capable of predicting punctuation and capitalization presents several challenges, primarily due to limited data availability and shortcomings in the existing evaluation methods, such as inadequate assessment of punctuation prediction. In this paper, we introduce a LibriSpeech-PC benchmark designed to assess the punctuation and capitalization prediction capabilities of end-to-end ASR models. The benchmark includes a LibriSpeech-PC dataset with restored punctuation and capitalization, a novel evaluation metric called Punctuation Error Rate (PER) that focuses on punctuation marks, and initial baseline models. All code, data, and models are publicly available.",
    "path": "papers/23/10/2310.02943.json",
    "total_tokens": 917,
    "translated_title": "LibriSpeech-PC：评估端到端ASR模型的标点和大写能力的基准测试",
    "translated_abstract": "传统的自动语音识别（ASR）模型输出没有标点符号的小写单词，这降低了可读性，需要后续的文本处理模型将ASR转录转换为正确的格式。与此同时，能够预测标点和大写的端到端ASR模型的开发面临着多个挑战，主要是由于有限的数据可用性和现有评估方法的不足，如对标点预测的评估不足。在本文中，我们介绍了一个名为LibriSpeech-PC的基准测试，旨在评估端到端ASR模型的标点和大写预测能力。该基准测试包括一个具有恢复标点和大写符号的LibriSpeech-PC数据集，以及一个名为Punctuation Error Rate（PER）的新颖评估指标，专注于标点符号的评估，还包含初步的基线模型。所有代码、数据和模型都是公开可用的。",
    "tldr": "本文介绍了一个名为LibriSpeech-PC的基准测试，用于评估端到端ASR模型的标点和大写预测能力。该基准测试包括一个带有恢复标点和大写符号的数据集，以及一种针对标点符号评估的新型评估指标Punctuation Error Rate（PER），并提供了初步的基线模型。",
    "en_tdlr": "This paper presents a benchmark called LibriSpeech-PC to assess the punctuation and capitalization prediction capabilities of end-to-end ASR models. The benchmark includes a dataset with restored punctuation and capitalization, a novel evaluation metric called Punctuation Error Rate (PER), and initial baseline models."
}