{
    "title": "Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)",
    "abstract": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen",
    "link": "http://arxiv.org/abs/2310.14159",
    "context": "Title: Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)\nAbstract: As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen",
    "path": "papers/23/10/2310.14159.json",
    "total_tokens": 996,
    "translated_title": "语言模型是否能够嘲笑YouTube短视频？",
    "translated_abstract": "随着社交网络上短视频的流行，要求AI模型能够更好地理解这些视频以与人类进行更好的交流。然而，之前的视频幽默数据集主要针对特定领域，如演讲或情景喜剧，并且大多关注语言线索。我们创建了一个包含来自YouTube的10K个多模态有趣视频的用户生成数据集，称为ExFunTube。使用基于GPT-3.5的视频过滤流程，我们验证了语言和视觉元素对幽默的贡献。在过滤后，我们为每个视频的有趣时刻加上了时间戳和文本解释。我们的ExFunTube在现有数据集中独特之处在于，我们的视频涵盖了各种类型幽默的广泛领域，需要对内容进行多模态理解。此外，我们开发了一种零-shot视频到文本提示，以最大化大型语言模型（LLMs）对视频幽默的理解。使用自动评分、原理质量实验以及人类评价方法进行三种不同的评估。",
    "tldr": "本研究在用户生成数据集中筛选并注释了10K个YouTube上的有趣多模态视频，借助GPT-3.5验证了语言和视觉元素对幽默的贡献。此外，还开发了一种零-shot视频到文本提示方法，用于大型语言模型对视频幽默的理解。这个研究填补了现有数据集中对多领域多模态幽默的不足。",
    "en_tdlr": "This study curates and annotates 10K user-generated funny videos from YouTube, verifying the contribution of both verbal and visual elements to humor using GPT-3.5. Additionally, a zero-shot video-to-text prompting is developed to enhance large language models' understanding of video humor. This research addresses the lack of multi-domain multimodal humor in existing datasets."
}