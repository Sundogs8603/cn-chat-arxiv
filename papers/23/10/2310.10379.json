{
    "title": "Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification. (arXiv:2310.10379v1 [cs.LG])",
    "abstract": "Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \\textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than soft",
    "link": "http://arxiv.org/abs/2310.10379",
    "context": "Title: Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification. (arXiv:2310.10379v1 [cs.LG])\nAbstract: Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \\textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than soft",
    "path": "papers/23/10/2310.10379.json",
    "total_tokens": 918,
    "translated_title": "重新审视贝叶斯元学习中逻辑-softmax似然用于少样本分类",
    "translated_abstract": "元学习通过学习使用先前的知识解决新问题，在少样本分类中取得了有希望的结果。贝叶斯方法能够有效地表征少样本分类中的不确定性，这在高风险领域至关重要。然而，在多类别高斯过程分类中，逻辑-softmax似然一直被用作softmax似然的替代方法，因为其具有条件共轭性质。然而，逻辑-softmax的理论特性不清楚，以前的研究表明逻辑-softmax的固有不确定性导致了次优的性能。为了解决这些问题，我们重新审视和重新设计了逻辑-softmax似然，通过一个温度参数实现对先验置信水平的控制。此外，我们从理论和实践的角度证明了softmax可以被视为逻辑-softmax的一种特殊情况，并且逻辑-softmax引导了比softmax更大的数据分布家族。",
    "tldr": "本文重新审视和重新设计了逻辑-softmax似然，通过温度参数控制先验置信水平，从而改善了贝叶斯元学习中的少样本分类问题。同时证明softmax是逻辑-softmax的一种特殊情况，逻辑-softmax能够引导更大的数据分布家族。",
    "en_tdlr": "This paper revisits and redesigns the logistic-softmax likelihood in Bayesian meta-learning for few-shot classification, allowing control of the prior confidence level through a temperature parameter. Theoretical and empirical evidence show that softmax can be viewed as a special case of logistic-softmax, which induces a larger family of data distribution."
}