{
    "title": "Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])",
    "abstract": "Image synthesis approaches, e.g., generative adversarial networks, have been popular as a form of data augmentation in medical image analysis tasks. It is primarily beneficial to overcome the shortage of publicly accessible data and associated quality annotations. However, the current techniques often lack control over the detailed contents in generated images, e.g., the type of disease patterns, the location of lesions, and attributes of the diagnosis. In this work, we adapt the latest advance in the generative model, i.e., the diffusion model, with the added control flow using lesion-specific visual and textual prompts for generating dermatoscopic images. We further demonstrate the advantage of our diffusion model-based framework over the classical generation models in both the image quality and boosting the segmentation performance on skin lesions. It can achieve a 9% increase in the SSIM image quality measure and an over 5% increase in Dice coefficients over the prior arts.",
    "link": "http://arxiv.org/abs/2310.02906",
    "context": "Title: Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])\nAbstract: Image synthesis approaches, e.g., generative adversarial networks, have been popular as a form of data augmentation in medical image analysis tasks. It is primarily beneficial to overcome the shortage of publicly accessible data and associated quality annotations. However, the current techniques often lack control over the detailed contents in generated images, e.g., the type of disease patterns, the location of lesions, and attributes of the diagnosis. In this work, we adapt the latest advance in the generative model, i.e., the diffusion model, with the added control flow using lesion-specific visual and textual prompts for generating dermatoscopic images. We further demonstrate the advantage of our diffusion model-based framework over the classical generation models in both the image quality and boosting the segmentation performance on skin lesions. It can achieve a 9% increase in the SSIM image quality measure and an over 5% increase in Dice coefficients over the prior arts.",
    "path": "papers/23/10/2310.02906.json",
    "total_tokens": 883,
    "translated_title": "通过带有视觉和文本提示的扩散模型提升皮肤镜病变分割",
    "translated_abstract": "在医疗图像分析任务中，图像合成方法（例如生成对抗网络）作为一种数据增强形式被广泛应用。这主要有助于克服公开可访问数据和相关质量注释的不足。然而，目前的技术通常无法对生成的图像中的详细内容进行精确控制，例如疾病模式类型、病变位置和诊断属性。在这项工作中，我们采用了最新的生成模型进展——扩散模型，并通过使用特定于病变的视觉和文本提示来增加控制流以生成皮肤镜图像。我们进一步证明了我们基于扩散模型的框架在图像质量和皮肤病变分割性能提升方面的优势。它能够在SSIM图像质量度量上达到9%的增加，并在Dice系数上超过了先前的研究结果，增加了5%以上。",
    "tldr": "本文提出了一种通过扩散模型结合视觉和文本提示来生成皮肤镜图像的方法，并证明了该方法在提高图像质量和皮肤病变分割性能方面的优势。",
    "en_tdlr": "This paper proposes a method for generating dermatoscopic images using diffusion models with visual and textual prompts, and demonstrates the advantages of this approach in improving image quality and skin lesion segmentation performance."
}