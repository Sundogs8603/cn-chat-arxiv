{
    "title": "Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])",
    "abstract": "Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.",
    "link": "http://arxiv.org/abs/2310.13025",
    "context": "Title: Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])\nAbstract: Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.",
    "path": "papers/23/10/2310.13025.json",
    "total_tokens": 883,
    "translated_title": "使用幂集多类交叉熵损失函数进行神经说话人分离",
    "translated_abstract": "自从2019年提出以来，全端到端神经说话人分离（EEND）一直将说话人分离视为一个逐帧多标签分类问题，并采用了排列不变的训练方法。尽管EEND显示出了很大的潜力，但最近的一些研究退回到了研究（本地）有监督EEND说话人分离与（全局）无监督聚类的可能组合。然而，这些混合方法并没有对原始的多标签公式产生疑问。我们提出从多标签（允许同时出现两个说话人）转换为幂集多类分类（为重叠说话人对分配专门的类别）。通过在9个不同的基准测试中进行大量实验，我们表明这种公式可以显著提高性能（主要是在重叠语音上）并且对领域不匹配具有鲁棒性，同时消除了多标签公式中关键的检测阈值超参数。",
    "tldr": "提出了一种将多标签公式改为幂集多类分类的方法，通过大量实验表明这种方法在性能和鲁棒性方面都显著优于原始方法，并消除了多标签公式中的关键超参数。",
    "en_tdlr": "Proposed a method to switch from a multi-label formulation to a powerset multi-class classification, which showed significantly better performance and robustness compared to the original approach, while eliminating critical hyperparameters in the multi-label formulation."
}