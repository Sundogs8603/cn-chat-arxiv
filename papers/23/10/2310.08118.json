{
    "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?. (arXiv:2310.08118v1 [cs.AI])",
    "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additiona",
    "link": "http://arxiv.org/abs/2310.08118",
    "context": "Title: Can Large Language Models Really Improve by Self-critiquing Their Own Plans?. (arXiv:2310.08118v1 [cs.AI])\nAbstract: There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additiona",
    "path": "papers/23/10/2310.08118.json",
    "total_tokens": 882,
    "translated_title": "能否通过自我批评改进大型语言模型对自身计划的能力？",
    "translated_abstract": "关于大型语言模型（LLMs）能否成功验证或自我批评其在推理问题中的候选解决方案的广泛说法已经存在。在本文中，我们针对计划的情况研究了大型语言模型的验证/自我批评能力。我们评估了一个使用LLMs进行计划生成和验证的计划系统。我们评估了验证器LLM在与基准验证的性能、自我批评对计划生成的影响以及系统性能中反馈水平变化的影响。使用最先进的LLM GPT-4进行生成和验证，我们的发现表明，自我批评似乎会降低计划生成的性能，尤其是与具有外部、可靠验证器的系统相比，LLM验证器在该系统中产生了大量的误报，从而影响了系统的可靠性。",
    "tldr": "本研究探究了大型语言模型在计划领域中的验证和自我批评能力。研究结果发现，自我批评似乎会降低计划生成的性能，并导致大量误报，降低了系统的可靠性。",
    "en_tdlr": "This paper investigates the verification and self-critiquing abilities of Large Language Models (LLMs) in the context of planning. The study reveals that self-critiquing diminishes plan generation performance and results in a high number of false positives, compromising the system's reliability."
}