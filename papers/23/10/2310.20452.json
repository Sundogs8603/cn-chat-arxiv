{
    "title": "AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms. (arXiv:2310.20452v1 [cs.LG])",
    "abstract": "We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-onc",
    "link": "http://arxiv.org/abs/2310.20452",
    "context": "Title: AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms. (arXiv:2310.20452v1 [cs.LG])\nAbstract: We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-onc",
    "path": "papers/23/10/2310.20452.json",
    "total_tokens": 806,
    "translated_title": "AsGrad: 异步-SGD算法的尖锐统一分析",
    "translated_abstract": "我们分析了异步类型的分布式SGD算法在异构设置下的性能，其中每个工作节点都具有自己的计算和通信速度，以及数据分布。在这些算法中，工作节点根据其局部数据在某个历史迭代时计算可能过期和随机的梯度，然后将这些梯度返回给服务器，而不与其他工作节点同步。我们提出了一个非凸平滑函数的异构收敛理论。所提出的分析为纯异步SGD及其各种改进提供了收敛性。此外，我们的理论解释了影响收敛速度的因素以及可以采取哪些方法来提高异步算法的性能。特别地，我们介绍了一种基于节点重新排序的新型异步方法。作为我们分析的副产品，我们还展示了梯度型算法（例如随机重新排序的SGD和一次随机排序）",
    "tldr": "我们提出了一个统一的收敛理论，分析了异步-SGD算法在异构设置下的性能，这对于提高算法性能和收敛速度具有重要意义。"
}