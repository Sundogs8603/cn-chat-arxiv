{
    "title": "Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks. (arXiv:2310.20579v1 [stat.ML])",
    "abstract": "We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work rev",
    "link": "http://arxiv.org/abs/2310.20579",
    "context": "Title: Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks. (arXiv:2310.20579v1 [stat.ML])\nAbstract: We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work rev",
    "path": "papers/23/10/2310.20579.json",
    "total_tokens": 871,
    "translated_title": "初始化很重要：超参数化神经网络的隐私-效用分析",
    "translated_abstract": "我们通过分析研究了随机机器学习算法中模型的超参数化对训练数据信息泄漏的影响。具体而言，我们证明了模型分布在最坏情况下邻近数据集间的KL散度的隐私界，并探索了它与完全连接神经网络的初始化、宽度和深度的关系。我们发现这个KL隐私界主要由训练过程中模型参数相对于预期梯度范数决定。值得注意的是，在线性化网络的特殊设置下，我们的分析表明梯度范数的平方（因此隐私损失的递增）与初始化分布的每层方差直接相关。通过使用这个分析，我们证明了在某些初始化（LeCun和Xavier）下，随着深度的增加，隐私界得到了改善，而在其他初始化（He和NTK）下，随着深度的增加，隐私界得到了恶化。我们的工作可以帮助论证初始化对于神经网络隐私保护的重要性。",
    "tldr": "通过分析研究了超参数化神经网络的初始化对于隐私保护的影响，发现隐私界的改善与深度和初始化分布的关系密切相关。",
    "en_tdlr": "Investigated the impact of initialization on privacy protection in overparameterized neural networks, finding that the improvement of privacy bound is closely related to the depth and initialization distribution."
}