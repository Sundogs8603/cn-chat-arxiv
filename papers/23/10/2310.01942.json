{
    "title": "OOD Aware Supervised Contrastive Learning. (arXiv:2310.01942v1 [cs.LG])",
    "abstract": "Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our",
    "link": "http://arxiv.org/abs/2310.01942",
    "context": "Title: OOD Aware Supervised Contrastive Learning. (arXiv:2310.01942v1 [cs.LG])\nAbstract: Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our",
    "path": "papers/23/10/2310.01942.json",
    "total_tokens": 902,
    "translated_title": "OOD感知的监督对比学习",
    "translated_abstract": "针对机器学习模型在训练数据分布之外的样本进行识别的安全部署中，超出分布（OOD）检测是一个关键问题。大多数OOD研究都集中在用交叉熵（CE）训练的分类模型上，并试图解决其中固有的问题。在这项工作中，我们利用用Supervised Contrastive（SupCon）训练学到的强大表示，并提出了一种综合方法来学习对OOD数据具有鲁棒性的分类器。我们通过两个额外的对比项扩展了SupCon损失。第一个项将辅助OOD表示与ID表示分离，而不对辅助数据之间的相似性施加任何约束。第二个项将OOD特征远离现有的类别原型，同时将ID表示推向其相对应的类别原型。当辅助OOD数据不可用时，我们提出了特征混合技术来高效生成伪OOD特征。",
    "tldr": "本文提出了一种利用Supervised Contrastive（SupCon）学习的强大表示的方法来学习对OOD数据具有鲁棒性的分类器，并通过扩展SupCon损失的对比项来实现。当辅助OOD数据不可用时，我们提出了特征混合技术来生成伪OOD特征。",
    "en_tdlr": "This paper proposes a method to learn a robust classifier for out-of-distribution (OOD) data using powerful representations learned with Supervised Contrastive (SupCon) training, and extends the SupCon loss with additional contrast terms. When auxiliary OOD data is not available, feature mixing techniques are proposed to generate pseudo-OOD features."
}