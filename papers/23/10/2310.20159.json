{
    "title": "Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts. (arXiv:2310.20159v1 [cs.CV])",
    "abstract": "Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves",
    "link": "http://arxiv.org/abs/2310.20159",
    "context": "Title: Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts. (arXiv:2310.20159v1 [cs.CV])\nAbstract: Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves",
    "path": "papers/23/10/2310.20159.json",
    "total_tokens": 921,
    "translated_title": "语言引导的视觉问答: 使用知识增强的提示来提升你的多模态语言模型",
    "translated_abstract": "视觉问答（VQA）是回答关于图像的问题的任务。这个任务假设了对图像和问题的理解，以提供自然语言回答。VQA近年来因其在机器人、教育和医疗等多个领域的潜在应用而受到关注。本文关注于知识增强的VQA，即回答问题需要常识知识、世界知识以及关于图像中不存在的思想和概念的推理能力。我们提出了一个多模态框架，利用语言引导（LG）形式的合理性、图像标题、场景图等来更准确地回答问题。我们在A-OKVQA、Science-QA、VSR和IconQA数据集的多选题问答任务上使用CLIP和BLIP模型进行了基准测试。结果表明，使用语言引导是一种简单而强大有效的视觉问答策略。我们的语言引导改进了问答的性能。",
    "tldr": "本文研究了知识增强的视觉问答任务，提出了一种利用语言引导的多模态框架，并通过实验证明了语言引导是一种简单但有效的策略，可以提高视觉问答的准确性。",
    "en_tdlr": "This paper focuses on knowledge-augmented visual question answering and proposes a multimodal framework using language guidance. The results demonstrate that language guidance is a simple yet effective strategy to improve the accuracy of visual question answering."
}