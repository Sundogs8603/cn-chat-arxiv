{
    "title": "Linear Recurrent Units for Sequential Recommendation. (arXiv:2310.02367v1 [cs.IR])",
    "abstract": "State-of-the-art sequential recommendation relies heavily on self-attention-based recommender models. Yet such models are computationally expensive and often too slow for real-time recommendation. Furthermore, the self-attention operation is performed at a sequence-level, thereby making low-cost incremental inference challenging. Inspired by recent advances in efficient language modeling, we propose linear recurrent units for sequential recommendation (LRURec). Similar to recurrent neural networks, LRURec offers rapid inference and can achieve incremental inference on sequential inputs. By decomposing the linear recurrence operation and designing recursive parallelization in our framework, LRURec provides the additional benefits of reduced model size and parallelizable training. Moreover, we optimize the architecture of LRURec by implementing a series of modifications to address the lack of non-linearity and improve training dynamics. To validate the effectiveness of our proposed LRURe",
    "link": "http://arxiv.org/abs/2310.02367",
    "context": "Title: Linear Recurrent Units for Sequential Recommendation. (arXiv:2310.02367v1 [cs.IR])\nAbstract: State-of-the-art sequential recommendation relies heavily on self-attention-based recommender models. Yet such models are computationally expensive and often too slow for real-time recommendation. Furthermore, the self-attention operation is performed at a sequence-level, thereby making low-cost incremental inference challenging. Inspired by recent advances in efficient language modeling, we propose linear recurrent units for sequential recommendation (LRURec). Similar to recurrent neural networks, LRURec offers rapid inference and can achieve incremental inference on sequential inputs. By decomposing the linear recurrence operation and designing recursive parallelization in our framework, LRURec provides the additional benefits of reduced model size and parallelizable training. Moreover, we optimize the architecture of LRURec by implementing a series of modifications to address the lack of non-linearity and improve training dynamics. To validate the effectiveness of our proposed LRURe",
    "path": "papers/23/10/2310.02367.json",
    "total_tokens": 919,
    "translated_title": "用于顺序推荐的线性循环单元",
    "translated_abstract": "当前的顺序推荐依赖于基于自注意的推荐模型。然而，这些模型计算代价高，往往对实时推荐来说过于缓慢。此外，自注意操作是在序列层级上进行的，因此对于低成本的增量推断来说具有挑战性。受到高效语言建模的最新进展的启发，我们提出了用于顺序推荐的线性循环单元（LRURec）。类似于循环神经网络，LRURec具有快速的推断速度，并且能够对顺序输入进行增量推断。通过分解线性循环操作并在我们的框架中设计递归并行化，LRURec提供了减小模型大小和可并行训练的额外优势。此外，我们通过实施一系列修改来优化LRURec的架构，以解决缺乏非线性和改善训练动态的问题。为了验证我们提出的LRURec的有效性",
    "tldr": "本研究提出了一种用于顺序推荐的线性循环单元（LRURec）。与当前的自注意模型相比，LRURec具有快速的推断速度、能够进行增量推断、更小的模型大小和可并行训练。通过优化架构并引入非线性，LRURec在顺序推荐任务中取得了有效的结果。",
    "en_tdlr": "This study proposes a linear recurrent units (LRURec) model for sequential recommendation. Compared to current self-attention models, LRURec offers faster inference, incremental inference, smaller model size, and parallelizable training. By optimizing the architecture and introducing non-linearity, LRURec achieves effective results in sequential recommendation tasks."
}