{
    "title": "Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])",
    "abstract": "Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that",
    "link": "http://arxiv.org/abs/2310.03419",
    "context": "Title: Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])\nAbstract: Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that",
    "path": "papers/23/10/2310.03419.json",
    "total_tokens": 844,
    "translated_title": "预训练和微调生成性流网络",
    "translated_abstract": "生成性流网络（GFlowNets）是学习从给定的非标准化奖励分布中顺序生成复合对象的随机策略的摊销采样器。它们可以生成多样化的高奖励对象，在科学发现任务中是一个重要考虑因素。然而，由于它们通常是根据给定的外在奖励函数进行训练的，关于如何利用预训练的力量以及以无监督的方式训练GFlowNets以实现对下游任务的高效自适应仍然是一个重要的挑战。受无监督预训练在各个领域的最新成功启发，我们引入了一种新的GFlowNets无奖励预训练的方法。通过将训练构建为一个自监督问题，我们提出了一种条件GFlowNet（OC-GFN），它学习探索候选空间。具体而言，OC-GFN学习达到任何目标结果，类似于强化学习中的目标条件策略。我们证明了",
    "tldr": "这个论文提出了一种新的方法来实现生成性流网络的无奖励预训练，并通过自监督问题的形式训练了一个条件的GFlowNet（OC-GFN），用于有效适应下游任务。"
}