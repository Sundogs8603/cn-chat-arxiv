{
    "title": "Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])",
    "abstract": "Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac",
    "link": "http://arxiv.org/abs/2310.12145",
    "context": "Title: Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])\nAbstract: Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac",
    "path": "papers/23/10/2310.12145.json",
    "total_tokens": 945,
    "translated_title": "通过NAS实现更公平和准确的表格模型",
    "translated_abstract": "长期以来，通过算法使得表格数据的模型更加公平一直是研究的课题。现有的技术通常针对存在不可取的结果的神经模型，通过改变数据的摄入方式、模型权重或输出处理方式来修复。我们采用了新的策略，在去偏过程中考虑更新模型的架构和训练超参数，以找到一个从一开始在预测结果上更好的新模型。在这项工作中，我们首次将多目标神经架构搜索(NAS)和超参数优化(HPO)应用于具有挑战性的表格数据领域。我们在不同数据集上对MLP、ResNet和FT-Transformer等不同架构和超参数空间进行了广泛的探索，展示了模型预测的准确性和公平性指标对超参数组合的依赖关系。我们表明，仅针对准确性进行优化的模型可能会导致公平性的降低，因此需要在优化过程中同时考虑准确性和公平性。",
    "tldr": "通过使用多目标神经架构搜索(NAS)和超参数优化(HPO)，我们在表格数据领域首次提出了一种更新模型架构和超参数的策略，以寻找更公平和准确的模型。我们发现，仅针对准确性进行优化可能会导致公平性的降低，因此需要同时考虑准确性和公平性。",
    "en_tdlr": "We propose a novel strategy to improve the fairness and accuracy of tabular models by updating model architecture and training hyperparameters through multi-objective neural architecture search (NAS) and hyperparameter optimization (HPO). Our results show the importance of considering fairness metrics alongside accuracy metrics in model optimization."
}