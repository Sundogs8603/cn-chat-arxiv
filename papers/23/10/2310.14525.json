{
    "title": "Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method",
    "abstract": "arXiv:2310.14525v2 Announce Type: replace-cross  Abstract: Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they",
    "link": "https://arxiv.org/abs/2310.14525",
    "context": "Title: Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method\nAbstract: arXiv:2310.14525v2 Announce Type: replace-cross  Abstract: Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they",
    "path": "papers/23/10/2310.14525.json",
    "total_tokens": 808,
    "translated_title": "图排名对比学习：一种极其简单而高效的方法",
    "translated_abstract": "图对比学习（GCL）作为一种代表性的图自监督方法已经取得了显著的成功。目前普遍采用的GCL优化目标是InfoNCE。通常，它采用增强技术获得两个视图，其中一个视图中的节点充当锚点，另一个视图中的对应节点充当正样本，所有其他节点被视为负样本。其目标是最小化锚点与正样本之间的距离，并最大化到负样本的距离。然而，在训练期间由于缺乏标签信息，InfoNCE必然将来自相同类别的样本视为负样本，导致假负样本问题。这可能损害学到的节点表示，并随后阻碍下游任务的性能。尽管已经提出了许多方法来减轻假阴性的影响。",
    "tldr": "图对比学习（GCL）作为一种代表性的图自监督方法，提出了一种旨在减轻假阴性影响的极其简单而有效的方法。",
    "en_tdlr": "Graph Contrastive Learning (GCL) as a representative graph self-supervised method, introduces an extremely simple yet efficient method aimed at mitigating the impact of false negatives."
}