{
    "title": "Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])",
    "abstract": "Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r",
    "link": "http://arxiv.org/abs/2310.06148",
    "context": "Title: Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])\nAbstract: Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r",
    "path": "papers/23/10/2310.06148.json",
    "total_tokens": 918,
    "translated_title": "理解迁移学习和基于梯度的元学习技术",
    "translated_abstract": "深度神经网络在各种任务上可以取得良好的性能，但通常需要大量的数据来训练。元学习作为一种提高这些网络从有限数据中泛化能力的方法受到广泛关注。虽然元学习技术在各种场景下被证明是成功的，但最近的结果表明，在评估与训练数据分布不同的任务时，与复杂的元学习技术（如MAML）相比，仅对预训练网络进行微调的基准线可能更有效。这一点令人惊讶，因为MAML的学习行为与微调的行为类似：两者都依赖于重复使用已学习的特征。我们调查了微调、MAML和另一种名为Reptile的元学习技术之间观察到的性能差异，并展示了MAML和Reptile在低数据情况下的快速适应能力。",
    "tldr": "本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。",
    "en_tdlr": "This paper investigates the performance differences between fine-tuning, MAML, and Reptile in the fields of transfer learning and meta-learning, and finds that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques."
}