{
    "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment. (arXiv:2310.06365v1 [cs.CL])",
    "abstract": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer's ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help",
    "link": "http://arxiv.org/abs/2310.06365",
    "context": "Title: Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment. (arXiv:2310.06365v1 [cs.CL])\nAbstract: Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer's ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help",
    "path": "papers/23/10/2310.06365.json",
    "total_tokens": 908,
    "translated_title": "多模态知识图谱变换器框架用于多模态实体对齐",
    "translated_abstract": "多模态实体对齐是一项重要的任务，旨在识别跨多模态知识图谱中的等价实体对。然而，该任务面临挑战，原因是存在不同类型的信息，包括邻近实体、多模态属性和实体类型。直接融合上述信息（如连接或注意力）可能导致无法对齐的信息空间。为了解决这些挑战，我们提出了一种新颖的多模态实体对齐变换器MoAlign，通过在变换器编码器中设计一个分层可修改的自注意力块，Hierarchical MoAlign引入邻居特征、多模态属性和实体类型来增强对齐任务。此外，我们设计了两种实体类型前缀注入方法，利用类型前缀来整合实体类型信息，有助于更好地保留不同信息的独特语义。",
    "tldr": "提出了一种名为MoAlign的多模态知识图谱变换器框架，通过引入邻居特征、多模态属性和实体类型，以解决多模态实体对齐任务中的挑战。设计了分层可修改的自注意力块和实体类型前缀注入方法，以更好地整合和保留不同信息的独特语义。"
}