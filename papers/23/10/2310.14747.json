{
    "title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution d",
    "link": "http://arxiv.org/abs/2310.14747",
    "context": "Title: MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution d",
    "path": "papers/23/10/2310.14747.json",
    "total_tokens": 909,
    "translated_title": "MCC-KD: 多CoT一致性知识蒸馏",
    "translated_abstract": "大型语言模型(LLMs)在复杂推理方面展现了卓越的能力，通过链式思考(CoT)提示。最近，人们对将这些推理能力从LLMs转移到较小模型中的兴趣日益增长。然而，同时实现多样性和一致性的理由对于提出了挑战。在本文中，我们着重增强这两个方面，并提出了多CoT一致性知识蒸馏(MCC-KD)方法，以高效地提取推理能力。在MCC-KD中，我们为每个问题生成多个理由，并通过最小化答案分布之间的双向KL散度来确保相应预测的一致性。我们研究了MCC-KD在不同模型架构(LLaMA/FlanT5)和各种模型规模(3B/7B/11B/13B)上在数学推理和常识推理基准上的有效性。实证结果不仅证实了MCC-KD在分布内情况下的优越性能。",
    "tldr": "MCC-KD方法提出了一种多CoT一致性知识蒸馏的方法，能够高效地转移大型语言模型的推理能力到较小的模型上，通过生成多个理由并确保其预测的一致性来增强推理多样性和一致性。"
}