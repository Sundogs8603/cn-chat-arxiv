{
    "title": "Low Resource Summarization using Pre-trained Language Models. (arXiv:2310.02790v1 [cs.CL])",
    "abstract": "With the advent of Deep Learning based Artificial Neural Networks models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted s",
    "link": "http://arxiv.org/abs/2310.02790",
    "context": "Title: Low Resource Summarization using Pre-trained Language Models. (arXiv:2310.02790v1 [cs.CL])\nAbstract: With the advent of Deep Learning based Artificial Neural Networks models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted s",
    "path": "papers/23/10/2310.02790.json",
    "total_tokens": 861,
    "translated_title": "使用预训练语言模型进行低资源摘要的翻译",
    "translated_abstract": "随着基于深度学习的人工神经网络模型的出现，自然语言处理（NLP）在文本数据处理方面取得了显著的效率和准确性改进。然而，这方面的研究大多局限于高资源语言，如英语，低资源语言仍然缺乏训练数据集和具有基线评估结果的模型。考虑到低资源语言的资源有限性，我们提出了一种适应低资源摘要的自注意力变压器架构模型（mBERT，mT5）的方法，并补充了一个新的基线数据集（76.5万篇文章、摘要对）来进行低资源语言乌尔都语的摘要。选择新闻（一个公开可用的来源）作为应用领域有潜力使所提出的方法在其他资源有限的语言中能够复制。我们的适应的模型",
    "tldr": "通过自注意力变压器架构模型和构建基线数据集，我们提出了一种适用于低资源语言摘要的方法，并通过在低资源语言乌尔都语中的新闻应用领域的实现来证明其可行性。",
    "en_tdlr": "We propose a methodology for low-resource summarization using self-attentive transformer-based models and the construction of a new baseline dataset. Our approach is demonstrated in the low-resource language Urdu within the domain of news, showcasing its potential for application in other languages with limited resources."
}