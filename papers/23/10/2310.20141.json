{
    "title": "Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])",
    "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20",
    "link": "http://arxiv.org/abs/2310.20141",
    "context": "Title: Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])\nAbstract: Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20",
    "path": "papers/23/10/2310.20141.json",
    "total_tokens": 874,
    "translated_title": "对比差异性预测编码",
    "translated_abstract": "预测和推理未来是许多时间序列问题的核心。例如，目标导向的强化学习可以被看作是学习表示以预测未来可能访问的状态。虽然先前的方法已经使用对比性预测编码来建模时间序列数据，但学习编码长期依赖通常需要大量的数据。在本文中，我们引入了一种时间差异版本的对比预测编码，将不同时间序列数据的片段组合在一起，以减少学习未来事件预测所需的数据量。我们将这种表示学习方法应用于导出目标导向的强化学习的离策略算法。实验证明，与先前的强化学习方法相比，我们的方法在成功率上实现了中位数提高2倍，并且可以更好地应对随机环境。在表格设置中，我们展示了我们的方法约为20倍。",
    "tldr": "本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。",
    "en_tdlr": "This paper introduces a temporal difference version of contrastive predictive coding that reduces the amount of data needed to learn predictions of future events by stitching together pieces of different time series data. Experiments show that our method achieves a 2x improvement in success rates compared to prior methods and performs better in stochastic environments."
}