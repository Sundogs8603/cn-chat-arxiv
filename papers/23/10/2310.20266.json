{
    "title": "Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])",
    "abstract": "What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.",
    "link": "http://arxiv.org/abs/2310.20266",
    "context": "Title: Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])\nAbstract: What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.",
    "path": "papers/23/10/2310.20266.json",
    "total_tokens": 866,
    "translated_title": "马尔可夫决策过程中超越平均回报",
    "translated_abstract": "在马尔可夫决策过程中，哪些奖励函数可以被准确地计算和优化？在有限时间段、无折扣设置中，动态规划只能高效处理某些统计类别的操作。我们总结了这些类别在策略评估中的特征，并提供了规划问题的新解。有趣的是，我们证明，即使在分布强化学习（DistRL）的更一般框架中，只有广义平均值可以被准确优化。然而，DistRL允许近似评估其他函数。我们给出了结果估计器的误差界限，并讨论了此方法的潜力及其局限性。这些结果通过研究回报的整体特征，特别是风险意识的策略，为马尔可夫决策过程的理论发展做出了贡献。",
    "tldr": "该论文研究了马尔可夫决策过程中超越平均回报的问题，总结了可以准确计算和优化的奖励函数的特征，并提供了针对这些特征的新规划方法。这些结果在马尔可夫决策过程的理论发展中具有重要意义。",
    "en_tdlr": "This paper investigates the problem of beyond average return in Markov Decision Processes, summarizes the characteristics of reward functions that can be computed and optimized exactly, and provides a new planning method for these characteristics. These results contribute significantly to the theory of Markov Decision Processes."
}