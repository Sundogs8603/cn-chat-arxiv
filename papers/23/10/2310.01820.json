{
    "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri",
    "link": "http://arxiv.org/abs/2310.01820",
    "context": "Title: Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri",
    "path": "papers/23/10/2310.01820.json",
    "total_tokens": 776,
    "translated_title": "进向鲁棒度评估图神经网络解释性的方法",
    "translated_abstract": "图神经网络（GNN）是一种利用图数据中的依赖结构通过节点之间的消息传递进行建模的神经网络模型。GNN已经成为分析图结构数据的关键架构，在敏感领域的广泛应用要求对其决策过程有全面的理解，这就需要一个GNN可解释性的框架。为了评估GNN解释函数的性能，需要提供可靠的保真度度量。本文研究了这一基础性挑战，重点在现有的保真度度量方法中揭示了潜在的局限性，包括Fid_+，Fid_-和Fid_Δ。具体而言，本文介绍了一个正式的信息论解释性定义，并证明了现有度量方法的限制",
    "tldr": "本文研究了评估图神经网络解释性的鲁棒度的方法，并指出了现有度量方法的局限性。",
    "en_tdlr": "This paper investigates methods for evaluating the robustness of explainability in graph neural networks, highlighting the limitations of existing fidelity metrics."
}