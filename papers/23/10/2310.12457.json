{
    "title": "MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])",
    "abstract": "Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe",
    "link": "http://arxiv.org/abs/2310.12457",
    "context": "Title: MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])\nAbstract: Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe",
    "path": "papers/23/10/2310.12457.json",
    "total_tokens": 910,
    "translated_title": "MuseGNN: 可解释和可收敛的大规模图神经网络层",
    "translated_abstract": "在能够建模具有跨实例关系的数据的许多图神经网络（GNN）架构中，一类重要的子类涉及设计层，其正向传递迭代地减少感兴趣的图正则化能量函数。通过这种方式，输出层产生的节点嵌入既可作为解决下游任务（如节点分类）的预测特征，又可作为能量函数最小化者，继承了可靠的归纳偏置和可解释性。然而，构建以这种方式构建的GNN架构的扩展仍然具有挑战性，部分原因是正向传递的收敛可能涉及具有相当深度的模型。为了解决这个限制，我们提出了一种基于采样的能量函数和可扩展的GNN层，通过在某些设置中具有收敛保证的指导，迭代地减少它。我们还基于这些设计实例化了一个完整的GNN架构，该模型在准确性和可扩展性方面均具有竞争力。",
    "tldr": "MuseGNN提出了一种可解释和可收敛的大规模图神经网络层，通过迭代地减少基于采样的能量函数，同时作为预测特征和能量函数最小化者，具有竞争力的准确性和可扩展性。",
    "en_tdlr": "MuseGNN proposes interpretable and convergent graph neural network layers at scale, iteratively reducing a sampling-based energy function, serving as both predictive features and energy function minimizers, with competitive accuracy and scalability."
}