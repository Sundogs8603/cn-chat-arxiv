{
    "title": "Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers (extended version). (arXiv:2310.17629v1 [math.ST])",
    "abstract": "The out-of-sample error (OO) is the main quantity of interest in risk estimation and model selection. Leave-one-out cross validation (LO) offers a (nearly) distribution-free yet computationally demanding approach to estimate OO. Recent theoretical work showed that approximate leave-one-out cross validation (ALO) is a computationally efficient and statistically reliable estimate of LO (and OO) for generalized linear models with differentiable regularizers. For problems involving non-differentiable regularizers, despite significant empirical evidence, the theoretical understanding of ALO's error remains unknown. In this paper, we present a novel theory for a wide class of problems in the generalized linear model family with non-differentiable regularizers. We bound the error |ALO - LO| in terms of intuitive metrics such as the size of leave-i-out perturbations in active sets, sample size n, number of features p and regularization parameters. As a consequence, for the $\\ell_1$-regularized",
    "link": "http://arxiv.org/abs/2310.17629",
    "context": "Title: Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers (extended version). (arXiv:2310.17629v1 [math.ST])\nAbstract: The out-of-sample error (OO) is the main quantity of interest in risk estimation and model selection. Leave-one-out cross validation (LO) offers a (nearly) distribution-free yet computationally demanding approach to estimate OO. Recent theoretical work showed that approximate leave-one-out cross validation (ALO) is a computationally efficient and statistically reliable estimate of LO (and OO) for generalized linear models with differentiable regularizers. For problems involving non-differentiable regularizers, despite significant empirical evidence, the theoretical understanding of ALO's error remains unknown. In this paper, we present a novel theory for a wide class of problems in the generalized linear model family with non-differentiable regularizers. We bound the error |ALO - LO| in terms of intuitive metrics such as the size of leave-i-out perturbations in active sets, sample size n, number of features p and regularization parameters. As a consequence, for the $\\ell_1$-regularized",
    "path": "papers/23/10/2310.17629.json",
    "total_tokens": 997,
    "translated_title": "使用$\\ell_1$正则化进行回归的近似留一法交叉验证（扩展版）",
    "translated_abstract": "在风险估计和模型选择中，外样误差（OO）是主要的关注量。留一法交叉验证（LO）提供了一个（几乎）无分布的、但计算复杂的方法来估计OO。最近的理论工作表明，近似留一法交叉验证（ALO）是对具有可微分正则化器的广义线性模型的LO（和OO）的计算有效和统计可靠的估计。对于涉及不可微分正则化器的问题，尽管有重要的经验证据，但对ALO误差的理论理解仍未知。本文提出了一个新的理论，适用于广义线性模型家族中具有不可微分正则化器的广泛问题。我们将ALO - LO的误差绑定在直观指标，如留一扰动的大小、样本大小n、特征数量p和正则化参数方面。因此，对于$\\ell_1$正则化的问题，我们得到了如何计算和界定ALO误差的关键结果。",
    "tldr": "本论文提出了一个理论，用于处理广义线性模型家族中使用不可微分正则化器的问题。通过计算留一法交叉验证（LO）和近似留一法交叉验证（ALO）之间的误差，并考虑留一扰动的大小、样本大小、特征数量和正则化参数，我们得到了关于ALO误差的重要结果。",
    "en_tdlr": "This paper presents a novel theory for tackling problems in the generalized linear model family with non-differentiable regularizers. By calculating and bounding the error between leave-one-out cross validation (LO) and approximate leave-one-out cross validation (ALO), and considering intuitive metrics like the size of leave-i-out perturbations, sample size, number of features, and regularization parameters, important results regarding ALO error are obtained."
}