{
    "title": "Joint Distributional Learning via Cramer-Wold Distance. (arXiv:2310.16374v1 [cs.LG])",
    "abstract": "The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experime",
    "link": "http://arxiv.org/abs/2310.16374",
    "context": "Title: Joint Distributional Learning via Cramer-Wold Distance. (arXiv:2310.16374v1 [cs.LG])\nAbstract: The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experime",
    "path": "papers/23/10/2310.16374.json",
    "total_tokens": 840,
    "translated_title": "通过克拉默沃尔德距离进行联合分布学习",
    "translated_abstract": "在处理高维数据集或观测变量之间复杂相关结构时，基于条件独立性的假设在变分自编码器（VAE）解码器建模中具有局限性。为解决这个问题，我们引入了克拉默沃尔德距离正则化，可以通过闭合形式计算，以促进高维数据集的联合分布学习。此外，我们引入了一个两步学习方法，以实现灵活的先验建模，并提高聚合后验与先验分布之间的对齐度。此外，我们从理论上对该类别中的现有方法进行了区分。为了评估我们提出的方法在合成数据生成方面的性能，我们在具有多个类别变量的高维数据集上进行了实验。考虑到许多现有的数据集和数据科学应用涉及此类数据集，我们的实验有着重要意义。",
    "tldr": "本文引入了克拉默沃尔德距离正则化，以更好地处理高维数据集和观测变量之间的复杂相关结构，并通过两步学习方法提高了先验建模的灵活性和聚合后验与先验分布之间的对齐度。",
    "en_tdlr": "This paper introduces Cramer-Wold distance regularization for better handling complex correlation structures among high-dimensional datasets and observed variables, and improves prior modeling flexibility and alignment between aggregated posterior and prior distribution through a two-step learning method."
}