{
    "title": "Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])",
    "abstract": "Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt",
    "link": "http://arxiv.org/abs/2310.17217",
    "context": "Title: Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])\nAbstract: Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt",
    "path": "papers/23/10/2310.17217.json",
    "total_tokens": 843,
    "translated_title": "超越MLE: 用于文本生成的凸学习方法",
    "translated_abstract": "最大似然估计（MLE）是一种统计方法，用于估计最能解释观测数据的概率分布参数。在文本生成的背景下，MLE经常用于训练生成型语言模型，从而生成新的文本。然而，我们认为MLE并不总是必要且最优的，尤其是对于闭合型文本生成任务，如机器翻译。在这些任务中，模型的目标是生成最合适的响应，这并不一定需要使用MLE来估计整个数据分布。为此，我们提出了一种基于凸函数的新型训练目标类，使得文本生成模型能够集中于高概率的输出，而不需要估计整个数据分布。我们研究了在将凸函数应用于损失函数时的最优预测分布的理论性质，证明了凸函数可以使得最优预测分布变得更加锐利。",
    "tldr": "本论文提出了一种基于凸函数的训练目标类，超越了传统的最大似然估计方法。该方法适用于闭合型文本生成任务，并能够使得模型生成更加合适的响应。",
    "en_tdlr": "This paper proposes a novel class of training objectives based on convex functions, which goes beyond the traditional maximum likelihood estimation method. The proposed method is suitable for closed-ended text generation tasks and enables the model to generate more appropriate responses."
}