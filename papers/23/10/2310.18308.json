{
    "title": "Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models. (arXiv:2310.18308v1 [cs.RO])",
    "abstract": "Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task description",
    "link": "http://arxiv.org/abs/2310.18308",
    "context": "Title: Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models. (arXiv:2310.18308v1 [cs.RO])\nAbstract: Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task description",
    "path": "papers/23/10/2310.18308.json",
    "total_tokens": 911,
    "translated_title": "Gen2Sim：使用生成模型在仿真环境中扩展机器人学习规模",
    "translated_abstract": "通用的机器人操作器需要在各种环境中学习各种操纵技能。目前的机器人训练流程依赖于人类提供运动示范或编程仿真环境并为强化学习编写奖励函数。这种人为参与是扩展机器人学习跨不同任务和环境的重要瓶颈。我们提出了Generation to Simulation（Gen2Sim），一种通过自动化生成语言和视觉的大规模预训练生成模型来扩展仿真中机器人技能学习的方法。我们通过使用图像扩散模型将开放世界的二维物体中心图像提升到三维，并查询LLMs来确定合理的物理参数，生成用于仿真的三维资产。给定生成和人类开发的资产的URDF文件，我们使用思维链提示LLMs将这些映射到相关的任务描述。",
    "tldr": "本文提出了Gen2Sim方法，通过使用生成模型自动生成3D资产、任务描述、任务分解和奖励函数来扩展机器人在仿真环境中的技能学习。这种自动化流程有助于解决人为参与的瓶颈问题，实现机器人学习在不同任务和环境中的扩展。",
    "en_tdlr": "This paper proposes the Gen2Sim method, which uses generative models to automate the generation of 3D assets, task descriptions, task decompositions, and reward functions for scaling up robot skill learning in simulation. This automated process helps overcome the bottleneck of human involvement and enables the expansion of robot learning across diverse tasks and environments."
}