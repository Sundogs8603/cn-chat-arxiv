{
    "title": "Lifting the Veil: Unlocking the Power of Depth in Q-learning. (arXiv:2310.17915v1 [cs.LG])",
    "abstract": "With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of ",
    "link": "http://arxiv.org/abs/2310.17915",
    "context": "Title: Lifting the Veil: Unlocking the Power of Depth in Q-learning. (arXiv:2310.17915v1 [cs.LG])\nAbstract: With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of ",
    "path": "papers/23/10/2310.17915.json",
    "total_tokens": 857,
    "translated_title": "解开Q-learning中深度的力量之谜",
    "translated_abstract": "在大量数据和丰富的计算资源的帮助下，深度Q-learning已广泛应用于运筹学和管理科学，并在众多应用中取得了巨大成功，包括推荐系统、供应链、游戏和机器人操纵。然而，深度Q-learning的成功缺乏坚实的理论验证和解释性。本文的目的是在统计学习理论框架下，实证地验证深度在深度Q-learning中的作用。通过严格证明其良好的泛化误差界，我们的结果揭示了深度Q-learning成功的主要原因是深度神经网络（深层网络）在捕捉奖励的特殊属性，即空间稀疏性和分段恒定性方面的出色性能，而不是它们的大容量。在本文中，我们对推动深度Q-learning的领域做出了基础性的贡献。",
    "tldr": "本文通过严格证明在统计学习理论框架下，深度Q-learning的深度具有显著优势，能够捕捉奖励的特殊属性。这一结果对解释深度Q-learning成功的原因起到了重要作用。",
    "en_tdlr": "This paper rigorously proves that the depth of deep Q-learning has significant advantages in capturing the special properties of rewards. This result plays an important role in explaining the success of deep Q-learning."
}