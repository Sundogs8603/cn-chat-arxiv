{
    "title": "Direct Neural Machine Translation with Task-level Mixture of Experts models. (arXiv:2310.12236v1 [cs.CL])",
    "abstract": "Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low an",
    "link": "http://arxiv.org/abs/2310.12236",
    "context": "Title: Direct Neural Machine Translation with Task-level Mixture of Experts models. (arXiv:2310.12236v1 [cs.CL])\nAbstract: Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low an",
    "path": "papers/23/10/2310.12236.json",
    "total_tokens": 934,
    "translated_title": "使用任务级混合专家模型的直接神经机器翻译",
    "translated_abstract": "直接神经机器翻译（Direct NMT）是一种在两种非英语语言之间进行翻译的NMT系统。直接NMT系统通常面临由于非英语语言对之间平行数据稀缺导致的限制。已经提出了几种方法来解决这一限制，例如多语NMT和基于中间语言（通过英语进行翻译的NMT）的NMT。任务级混合专家模型（Task-level MoE）是一种基于Transformer模型的推理高效变体，对许多语言对展现了有前景的NMT性能。在任务级MoE中，不同的语言分组可以使用不同的路由策略来优化跨语言学习和推理速度。本文研究了任务级MoE在直接NMT中的适用性，并提出了一系列高性能的训练和评估配置，通过这些任务级MoE基础的直接NMT系统在大量低资源语言对上优于双语和基于中间语言的模型。",
    "tldr": "在这项工作中，我们研究了任务级MoE在直接神经机器翻译中的应用，并提出了一系列高性能的训练和评估配置，通过这些配置，任务级MoE的直接NMT系统在大量低资源语言对上优于双语和基于中间语言的模型。"
}