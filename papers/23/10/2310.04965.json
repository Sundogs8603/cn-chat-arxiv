{
    "title": "MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)",
    "abstract": "Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th",
    "link": "http://arxiv.org/abs/2310.04965",
    "context": "Title: MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks. (arXiv:2310.04965v2 [cs.CL] UPDATED)\nAbstract: Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for th",
    "path": "papers/23/10/2310.04965.json",
    "total_tokens": 907,
    "translated_title": "MULTISCRIPT: 多模式脚本学习用于支持开放领域的日常任务",
    "translated_abstract": "从视频演示中自动生成脚本（即文本描述的关键步骤序列）并推理后续步骤对于现代AI虚拟助手来引导人们完成日常任务，尤其是陌生任务至关重要。然而，当前的生成式脚本学习方法很大程度上依赖于结构良好的前置步骤的文本和/或图像描述，或者限于特定领域，导致与真实世界中用户场景存在差距。为了解决这些限制，我们提出了一个新的基准挑战——MultiScript，其中包含两个关于面向任务的多模式脚本学习的新任务：（1）多模式脚本生成，和（2）后续步骤预测。对于这两个任务，输入包括目标任务名称和演示视频，预期输出为（1）基于演示视频的结构化步骤描述的序列，和（2）针对每个步骤的单一文本描述。",
    "tldr": "该论文提出了一个新的基准挑战MultiScript，旨在解决现有脚本学习方法对于开放领域日常任务的限制。论文介绍了两个多模式脚本学习任务，并提供了对应的输入和输出要求。"
}