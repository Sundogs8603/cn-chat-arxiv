{
    "title": "Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])",
    "abstract": "ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t",
    "link": "http://arxiv.org/abs/2310.07347",
    "context": "Title: Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])\nAbstract: ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t",
    "path": "papers/23/10/2310.07347.json",
    "total_tokens": 937,
    "translated_title": "高效预训练的快速ELECTRA",
    "translated_abstract": "ELECTRA通过检测序列中被辅助模型替换的标记来预训练语言模型。虽然ELECTRA大大提高了效率，但其潜力受到了辅助模型带来的训练成本的限制。值得注意的是，这个与主模型共同训练的模型仅用于辅助主模型的训练，并且在训练后被丢弃。这导致大量的训练成本被白白浪费。为了缓解这个问题，我们提出了Fast-ELECTRA，它利用现有的语言模型作为辅助模型。为了构建主模型的学习课程，我们通过温度缩放和递减的方法平滑其输出分布。我们的方法与最先进的ELECTRA风格的预训练方法相媲美，同时显著减少了辅助模型共同训练带来的计算和内存成本。我们的方法还降低了模型对训练数据的敏感性。",
    "tldr": "提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。"
}