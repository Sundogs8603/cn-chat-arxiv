{
    "title": "ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)",
    "abstract": "Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts has proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In ",
    "link": "http://arxiv.org/abs/2310.19731",
    "context": "Title: ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)\nAbstract: Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts has proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In ",
    "path": "papers/23/10/2310.19731.json",
    "total_tokens": 824,
    "translated_title": "ViR: 迈向高效视觉保留骨干的研究",
    "translated_abstract": "近年来，视觉变换器（ViTs）因其在建模长程空间依赖性和大规模训练可扩展性方面的卓越能力而受到广泛关注。尽管自注意机制的训练并行性在保持出色性能方面起着重要作用，但其二次复杂度阻碍了ViTs在许多需要快速推理的场景中的应用。这种影响在需要自回归建模输入特征的应用中尤为明显。在自然语言处理（NLP）中，一种新的努力方向提出了具有可并行化模型和递归公式的模型，可以在生成应用中实现高效推理。受到这一趋势的启发，我们提出了一种新的计算机视觉模型，名为Vision Retention Networks（ViR），具有双重并行和递归公式，可以在快速推理和并行训练方面取得最佳平衡，并具有竞争力的性能。",
    "tldr": "ViR提出了一种新的计算机视觉模型，采用双重并行和递归公式，从而在快速推理和并行训练之间取得了最佳平衡，具有竞争力的性能。"
}