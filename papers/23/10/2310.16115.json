{
    "title": "Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos. (arXiv:2310.16115v1 [cs.CV])",
    "abstract": "Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that \"using new class data for KD\" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by \"using the placebos of old classes for KD\", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-f",
    "link": "http://arxiv.org/abs/2310.16115",
    "context": "Title: Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos. (arXiv:2310.16115v1 [cs.CV])\nAbstract: Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that \"using new class data for KD\" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by \"using the placebos of old classes for KD\", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-f",
    "path": "papers/23/10/2310.16115.json",
    "total_tokens": 1011,
    "translated_title": "无需过去数据，唤醒过去的概念：从在线安慰剂进行类增量学习",
    "translated_abstract": "在模型不断适应新类别时，不忘记旧类别的知识是类增量学习中的一个关键挑战。解决这个问题的常用技术是知识蒸馏（KD），它惩罚了旧模型和新模型之间的预测不一致性。由于类增量学习中的内存限制严格，旧类别数据极为稀缺，因此这样的预测几乎都是由新类别数据进行的。然而，我们深入研究了知识蒸馏损失，并发现“使用新类别数据进行知识蒸馏”不仅阻碍了模型对新类别的适应，而且对保留旧类别知识的效率非常低。为了解决这个问题，我们提出了“使用旧类别的安慰剂进行知识蒸馏”的方法，其中安慰剂是从自由图像流中选择的，比如谷歌图片，以自动和经济的方式进行选择。为此，我们训练了一个在线安慰剂选择策略来快速评估流式图像的质量（好的或坏的安慰剂），并且只使用好的安慰剂进行一次喂食。",
    "tldr": "本文提出了一种新的类增量学习方法，通过使用来自在线安慰剂的旧类别数据进行知识蒸馏，解决了类增量学习中长期记忆的问题，并提高了旧类别知识保留的效率。"
}