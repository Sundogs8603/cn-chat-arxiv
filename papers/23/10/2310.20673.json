{
    "title": "Balancing Act: Constraining Disparate Impact in Sparse Models",
    "abstract": "arXiv:2310.20673v2 Announce Type: replace  Abstract: Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity le",
    "link": "https://arxiv.org/abs/2310.20673",
    "context": "Title: Balancing Act: Constraining Disparate Impact in Sparse Models\nAbstract: arXiv:2310.20673v2 Announce Type: replace  Abstract: Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity le",
    "path": "papers/23/10/2310.20673.json",
    "total_tokens": 823,
    "translated_title": "平衡之道：约束稀疏模型中的不平等影响",
    "translated_abstract": "模型修剪是一种常用方法，可以在计算或存储容量受限的边缘设备上部署大型深度学习模型。尽管稀疏模型在整个数据集的级别上实现了与密集模型相媲美的性能，但对于某些数据子组，它们表现出高准确度降落。现有的缓解修剪引起的不平等影响的方法要么依赖于间接解决问题的替代指标并具有有限的解释性，要么随着受保护子组数量的增多而在计算成本方面表现不佳。我们提出了一种约束优化方法，直接解决修剪的不平等影响：我们的制定界定了在每个子组中稠密和稀疏模型之间的准确度变化。这些约束的选择为确定修剪模型是否实现可接受的不平等提供了可解释的成功标准。",
    "tldr": "提出了一种约束优化方法，直接解决修剪导致的不平等问题，为每个子组设置界限以限制稠密和稀疏模型之间的准确度变化。",
    "en_tdlr": "Proposed a constrained optimization approach that directly addresses the disparate impact of pruning by setting boundaries for each subgroup to limit the accuracy change between dense and sparse models."
}