{
    "title": "Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization. (arXiv:2310.16391v1 [cs.LG])",
    "abstract": "Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distr",
    "link": "http://arxiv.org/abs/2310.16391",
    "context": "Title: Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization. (arXiv:2310.16391v1 [cs.LG])\nAbstract: Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distr",
    "path": "papers/23/10/2310.16391.json",
    "total_tokens": 910,
    "translated_title": "通过探索变量参数提高不同分布的不变性学习：从失败彩票中获取的获奖",
    "translated_abstract": "不同分布的不变性学习旨在学习到对各种环境都具有很好泛化性能的鲁棒模型，而不是适应特定分布的特征。最近的研究基于\"彩票票据假设\"解决了此问题，通过最小化学习目标来找到一些对任务关键的参数。然而，在不同分布的问题中，这些解决方案并不是最优的，因为学习任务包含严重的分布噪声，这可能会误导优化过程。因此，除了找到与任务相关的参数（即不变参数）外，我们提出了\"探索变量参数进行不变性学习\"（EVIL）的方法，该方法还利用分布知识来寻找对分布变化敏感的参数（即变量参数）。一旦将变量参数从不变性学习中剔除，就可以找到一个抵抗分布转移的鲁棒子网络。此外，相对稳定的参数还可以通过跨分布的解耦来显著提高模型性能。",
    "tldr": "这项研究提出了一种名为EVIL的方法，通过探索分布知识中的变量参数，将其排除在不变性学习之外，从而找到一个抵抗分布转移的鲁棒子网络。",
    "en_tdlr": "This research proposes a method called EVIL, which explores variant parameters based on distribution knowledge and excludes them from invariant learning, resulting in the discovery of a subnetwork that is resistant to distribution shift."
}