{
    "title": "Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?. (arXiv:2310.17774v1 [cs.CL])",
    "abstract": "An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.",
    "link": "http://arxiv.org/abs/2310.17774",
    "context": "Title: Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?. (arXiv:2310.17774v1 [cs.CL])\nAbstract: An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.",
    "path": "papers/23/10/2310.17774.json",
    "total_tokens": 864,
    "translated_title": "词语、子词和词素：惊奇度-阅读时间关系中真正重要的是什么？",
    "translated_abstract": "使用LLMs在心理语言学数据上的一个重要假设一直未经验证。LLM基于子词分词进行预测，而不是将单词分解为词素。这是否重要？我们通过比较使用正字法、形态学和BPE分词的惊奇度估计与阅读时间数据进行了仔细测试。我们的结果复制了先前的发现，并提供了证据，表明使用BPE分词的预测相对于形态学和正字法分割来说并没有受到损害。然而，更细致的分析指出了依赖于BPE分词的潜在问题，并提供了涉及形态学感知惊奇度估计的有希望的结果，并建议了一种评估形态学预测的新方法。",
    "tldr": "本研究通过对比不同分词方法对惊奇度估计和阅读时间数据的影响，发现使用BPE分词的预测在整体上与形态学和正字法分割相比没有受损，但细致分析指出了BPE分词的潜在问题，并提出了一种新的形态学预测评估方法。",
    "en_tdlr": "This study compares different tokenization methods in predicting surprisal and their impact on reading time data, finding that predictions using BPE tokenization are not significantly worse than relying on morphological and orthographic segmentation. However, a detailed analysis suggests potential issues with BPE-based tokenization and proposes a new method for evaluating morphological prediction."
}