{
    "title": "CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)",
    "abstract": "Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi",
    "link": "http://arxiv.org/abs/2310.20550",
    "context": "Title: CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)\nAbstract: Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi",
    "path": "papers/23/10/2310.20550.json",
    "total_tokens": 884,
    "translated_title": "CapsFusion: 重新思考大规模图像-文本数据",
    "translated_abstract": "大规模多模态模型展示了在零样本情况下执行多样化多模态任务的显著泛化能力。大规模基于网络的图像-文本对在这一成功中起着根本性的贡献，但存在着过多的噪声。最近的研究使用由生成式字幕模型合成的替代字幕，并取得了显著的基准性能。然而，我们的实验证明，在使用合成字幕训练的模型中存在着显著的可扩展性不足和世界知识丧失问题，这些问题在其初始基准成功中大部分被掩盖了。经过进一步的研究，我们确定根本原因是现有合成字幕中过于简化的语言结构和缺乏知识细节。为了提供更高质量、更可扩展的多模态预训练数据，我们提出了CapsFusion，这是一个先进的框架，利用大型语言模型来整合和细化来自网络图像-文本对和合成字幕的信息。",
    "tldr": "CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。",
    "en_tdlr": "CapsFusion is an advanced framework that provides higher-quality and more scalable multimodal pretraining data by leveraging large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions."
}