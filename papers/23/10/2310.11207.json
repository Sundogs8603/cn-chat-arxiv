{
    "title": "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])",
    "abstract": "Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce \"helpful\" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as \"fantastic\" and \"memorable\" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit ",
    "link": "http://arxiv.org/abs/2310.11207",
    "context": "Title: Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])\nAbstract: Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce \"helpful\" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as \"fantastic\" and \"memorable\" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit ",
    "path": "papers/23/10/2310.11207.json",
    "total_tokens": 808,
    "translated_title": "大型语言模型能否自我解释？LLM生成的自解释研究。",
    "translated_abstract": "大型语言模型（LLMs）如ChatGPT在各种自然语言处理（NLP）任务中展现出优越的性能，包括情感分析、数学推理和摘要。此外，由于这些模型在人类对话中进行指导，以产生“有帮助”的回答，它们通常会在回答中提供解释，我们称之为自解释。例如，在分析电影评论的情感时，模型可以输出情感的积极性，并列出评论中带有情感的词语（如“fantastic”和“memorable”）作为解释。这些自动生成的自解释有多好？本文在情感分析和特征归因解释的任务中对此问题进行了研究，后者是可解释性文献中最常研究的设置（针对ChatGPT之前的模型）。具体来说，我们研究了不同的方法来引导模型生成自解释。",
    "tldr": "本文研究了大型语言模型生成的自解释在情感分析和特征归因解释任务中的效果，并探讨了不同的引导方法。"
}