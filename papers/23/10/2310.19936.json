{
    "title": "Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])",
    "abstract": "For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie",
    "link": "http://arxiv.org/abs/2310.19936",
    "context": "Title: Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])\nAbstract: For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie",
    "path": "papers/23/10/2310.19936.json",
    "total_tokens": 874,
    "translated_title": "面向少标注学习的目标检测：基于Transformer的模型更有效吗？",
    "translated_abstract": "对于专门的和密集的下游任务，如目标检测，标记数据需要专业知识，成本较高，因此少样本和半监督模型成为更具吸引力的替代方案。在少样本的设置中，我们观察到，在相似数量的参数下，基于Transformer的目标检测器比基于卷积的两阶段模型表现更好，但在最新的半监督设置中，它们的效果没有那么好。在本文中，我们提出了一种针对当前最先进的Deformable DETR目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤。我们在半监督目标检测基准COVO和Pascal VOC上评估了我们的方法，它在特别是标注稀缺的情况下优于先前的方法。我们相信我们的贡献将开启新的可能性。",
    "tldr": "本文提出了一种针对当前最先进目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤，并在标注稀缺的情况下表现优异。",
    "en_tdlr": "This paper proposes a semi-supervised method tailored for the state-of-the-art object detector, which avoids relying on sensitive post-processing steps and performs well in scenarios with limited annotations."
}