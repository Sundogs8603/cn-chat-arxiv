{
    "title": "Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])",
    "abstract": "Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs ",
    "link": "http://arxiv.org/abs/2310.09163",
    "context": "Title: Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])\nAbstract: Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs ",
    "path": "papers/23/10/2310.09163.json",
    "total_tokens": 915,
    "translated_title": "动态神经网络的联合学习退出和推断：JEI-DNN",
    "translated_abstract": "大型预训练模型结合微调已成为主导的机器学习架构。尽管这些模型表现出令人印象深刻的性能，但它们的实际应用通常受到每次推断所需的巨大资源的限制。早期退出动态神经网络（EDNN）通过允许模型从中间层进行部分预测（即早期退出）来绕过这个问题。训练EDNN架构具有挑战性，因为它包括两个相互交织的组件：控制早期退出决策的门控机制（GM）和执行中间表示推断的中间推断模块（IMs）。因此，大多数现有方法依赖于门控机制的阈值置信度度量，并努力改进基本的骨干网络和推断模块。尽管取得了成功，但这种方法有两个基本缺点：1）门控机制和中间推断模块不能共同学习和优化，2）GM在一定程度上依赖于IMS和骨干网络的质量，导致模型性能上的损失。",
    "tldr": "JEI-DNN是一种联合学习退出和推断的动态神经网络架构，通过允许模型在中间层进行部分预测，解决了大型预训练模型每次推断所需大量资源的问题。",
    "en_tdlr": "JEI-DNN is a dynamic neural network architecture that jointly learns exit and inference, solving the issue of resource-intensive inference in large pretrained models by allowing partial predictions from intermediate layers."
}