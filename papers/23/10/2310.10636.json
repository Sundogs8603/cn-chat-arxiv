{
    "title": "Dual-Encoders for Extreme Multi-Label Classification",
    "abstract": "arXiv:2310.10636v2 Announce Type: replace  Abstract: Dual-encoder (DE) models are widely used in retrieval tasks, most commonly studied on open QA benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification (XMC), remains under-explored. Current empirical evidence indicates that DE models fall significantly short on XMC benchmarks, where SOTA methods linearly scale the number of learnable parameters with the total number of classes (documents in the corpus) by employing per-class classification head. To this end, we first study and highlight that existing multi-label contrastive training losses are not appropriate for training DE models on XMC tasks. We propose decoupled softmax loss - a simple modification to the InfoNCE loss - that overcomes the limitations of existing contrastive losses. We further extend our loss design to a soft top-k operato",
    "link": "https://arxiv.org/abs/2310.10636",
    "context": "Title: Dual-Encoders for Extreme Multi-Label Classification\nAbstract: arXiv:2310.10636v2 Announce Type: replace  Abstract: Dual-encoder (DE) models are widely used in retrieval tasks, most commonly studied on open QA benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification (XMC), remains under-explored. Current empirical evidence indicates that DE models fall significantly short on XMC benchmarks, where SOTA methods linearly scale the number of learnable parameters with the total number of classes (documents in the corpus) by employing per-class classification head. To this end, we first study and highlight that existing multi-label contrastive training losses are not appropriate for training DE models on XMC tasks. We propose decoupled softmax loss - a simple modification to the InfoNCE loss - that overcomes the limitations of existing contrastive losses. We further extend our loss design to a soft top-k operato",
    "path": "papers/23/10/2310.10636.json",
    "total_tokens": 836,
    "translated_title": "用于极端多标签分类的双编码器",
    "translated_abstract": "双编码器（DE）模型广泛用于检索任务，通常在开放式QA基准上进行研究，这些基准通常以多类和有限的训练数据为特征。相比之下，在像极端多标签分类（XMC）这样的多标签和数据丰富的检索设置中，它们的性能尚未得到充分探讨。当前的实证证据表明，DE模型在XMC基准上明显不足，其中SOTA方法通过使用每类分类头将可学习参数数量与总类别数（语料库中的文档）线性扩展。为此，我们首先研究并强调现有的多标签对比训练损失不适用于在XMC任务上训练DE模型。我们提出了解耦的softmax损失 - 这是对InfoNCE损失的简单修改，克服了现有对比损失的局限性。我们进一步将我们的损失设计扩展到一个软top-k操作符",
    "tldr": "提出了一种解耦softmax损失的方法来克服现有对比损失在极端多标签分类任务上的局限性，并拓展到soft top-k 操作符。",
    "en_tdlr": "Introduced a method of decoupled softmax loss to overcome the limitations of existing contrastive losses on extreme multi-label classification tasks, and extended it to a soft top-k operator."
}