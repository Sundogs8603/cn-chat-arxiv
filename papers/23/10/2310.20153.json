{
    "title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation",
    "link": "http://arxiv.org/abs/2310.20153",
    "context": "Title: Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation",
    "path": "papers/23/10/2310.20153.json",
    "total_tokens": 930,
    "translated_title": "与稀疏人类监督相适应的成本有效的交互多重保真度学习用于语言模型的适应性",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中展示了卓越的能力。然而，由于它们在部署时的巨大规模、易受错误信息影响以及高昂的数据注释成本，它们在特定领域任务的适应性有限。我们提出了一种新颖的交互多重保真度学习（IMFL）框架，用于在有限的注释预算下开发小型特定领域的语言模型。我们的方法将领域特定的微调过程形式化为多重保真度学习问题，重点是确定平衡低保真度自动LLM注释和高保真度人类注释以最大化模型性能的最佳获取策略。我们进一步提出了一种增强注释多样性和信息性的探索-开发查询策略，结合了两个创新设计：1）从人类注释样本中选择上下文例子来改进LLM注释",
    "tldr": "该论文提出了一种交互多重保真度学习框架，用于在有限的注释预算下开发小型特定领域的语言模型。该方法通过平衡低保真度自动注释和高保真度人类注释，以最大化模型性能。同时，还提出了一种增强注释多样性和信息性的查询策略。",
    "en_tdlr": "This paper proposes an Interactive Multi-Fidelity Learning framework for developing small domain-specific language models under limited annotation budgets. The approach maximizes model performance by balancing low-fidelity automatic annotations and high-fidelity human annotations, and incorporates a query strategy to enhance annotation diversity and informativeness."
}