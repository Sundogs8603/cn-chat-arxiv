{
    "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)",
    "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-",
    "link": "http://arxiv.org/abs/2310.04948",
    "context": "Title: TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)\nAbstract: The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-",
    "path": "papers/23/10/2310.04948.json",
    "total_tokens": 870,
    "translated_title": "TEMPO: 基于提示的生成式预训练变换器模型用于时间序列预测",
    "translated_abstract": "在过去的十年中，深度学习在时间序列建模方面取得了显著进展。尽管在取得最先进的结果的同时，最好的架构在不同应用和领域之间差异很大。与此同时，在自然语言处理方面，生成式预训练变换器(GPT)通过训练一个通用模型在各种文本数据集上展现出了令人印象深刻的性能。有趣的是，探索是否GPT类型的架构可以对时间序列产生有效的影响，捕捉其内在动态属性并显著提高准确性。在本文中，我们提出了一个新颖的框架TEMPO，可以有效地学习时间序列表示。我们专注于利用时间序列任务的两种重要归纳偏差来预训练模型：(i) 对趋势、季节和残差成分复杂交互的分解；和(ii) 提出基于选择的提示以便于非分布自适应。",
    "tldr": "本文提出了一个新的框架 TEMPO，通过利用时间序列任务的两个重要归纳偏差，即将复杂交互分解和引入基于选择的提示来有效学习时间序列表示。",
    "en_tdlr": "This paper proposes a novel framework, TEMPO, for effectively learning time series representations by utilizing two essential inductive biases of the time series task: decomposing complex interactions and introducing selection-based prompts."
}