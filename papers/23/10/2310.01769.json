{
    "title": "How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])",
    "abstract": "This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \\in \\mathbb{R}^{n \\times n}$ is a positive semi-definite unknown matrix of rank $r \\ll n$, and one uses a symmetric parameterization $XX^\\top$ to learn $M^*$. Here $X \\in \\mathbb{R}^{n \\times k}$ with $k > r$ is the factor matrix. We give a novel $\\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\\exp (-\\Omega (T))$. Next, we study asymmetric setting where $M^* \\in \\mathbb{R}^{n_1 \\times n_2}$ is the unknown matrix of rank $r \\ll \\min\\{n_1,n_2\\}$, and one uses an ",
    "link": "http://arxiv.org/abs/2310.01769",
    "context": "Title: How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])\nAbstract: This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \\in \\mathbb{R}^{n \\times n}$ is a positive semi-definite unknown matrix of rank $r \\ll n$, and one uses a symmetric parameterization $XX^\\top$ to learn $M^*$. Here $X \\in \\mathbb{R}^{n \\times k}$ with $k > r$ is the factor matrix. We give a novel $\\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\\exp (-\\Omega (T))$. Next, we study asymmetric setting where $M^* \\in \\mathbb{R}^{n_1 \\times n_2}$ is the unknown matrix of rank $r \\ll \\min\\{n_1,n_2\\}$, and one uses an ",
    "path": "papers/23/10/2310.01769.json",
    "total_tokens": 876,
    "translated_title": "过参数化如何减缓矩阵感知中的梯度下降：对称性和初始化的问题。",
    "translated_abstract": "本文详细阐述了过参数化如何改变梯度下降在矩阵感知问题中的收敛行为。在对称设置中，通过对称参数化学习未知的半正定矩阵，我们给出了过参数化情况下（$k>r$）随机初始化梯度下降的新型$\\Omega (1/T^2)$下界，与精确参数化情况（$k=r$）的收敛速度$\\exp (-\\Omega (T))$形成鲜明对比。接下来，我们研究了不对称设置，其中$M^* \\in \\mathbb{R}^{n_1 \\times n_2}$是未知矩阵，采用非对称参数化学习。",
    "tldr": "该论文研究了过参数化如何影响矩阵感知问题中梯度下降的收敛行为，在对称和非对称设置下给出了不同的收敛速度。",
    "en_tdlr": "This paper investigates how over-parameterization affects the convergence behavior of gradient descent in matrix sensing problems. It provides different convergence rates in symmetric and asymmetric settings."
}