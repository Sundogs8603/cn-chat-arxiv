{
    "title": "On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])",
    "abstract": "To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis",
    "link": "http://arxiv.org/abs/2310.11840",
    "context": "Title: On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])\nAbstract: To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis",
    "path": "papers/23/10/2310.11840.json",
    "total_tokens": 967,
    "translated_title": "关于强化学习中目标规范形式的表达能力",
    "translated_abstract": "要解决强化学习任务，必须对该任务的目标进行形式化规定。尽管大多数强化学习算法要求将目标形式化为马尔可夫奖励函数，但已经开发出了其他替代方法（如线性时间逻辑和多目标强化学习）。此外，众所周知，其中一些形式化方法能够表达其他形式化方法无法表达的特定任务。然而，目前还没有对这些形式化方法在表达能力方面如何相互关联进行全面分析。在本研究中，我们通过对强化学习中17种目标规范形式的表达能力进行全面比较填补了现有文献中的空白。我们将这些形式化方法根据其表达能力进行预排序，并将该预排序呈现为哈斯图。我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。",
    "tldr": "这项工作通过对强化学习中17种目标规范形式的表达能力进行全面比较，填补了现有文献中的空白。通过将这些形式化方法进行预排序，并呈现为哈斯图，我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。",
    "en_tdlr": "This work fills the gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in reinforcement learning. By placing these formalisms in a preorder based on their expressive power and presenting it as a Hasse diagram, we find various limitations for different formalisms and that no formalism is both dominantly expressive and straightforward to optimize."
}