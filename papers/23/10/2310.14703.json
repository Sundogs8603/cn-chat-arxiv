{
    "title": "Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models. (arXiv:2310.14703v2 [cs.CL] UPDATED)",
    "abstract": "Vocabulary tests, once a cornerstone of language modeling evaluation, have been largely overlooked in the current landscape of Large Language Models (LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus on specific tasks or domain-specific knowledge, they often neglect the fundamental linguistic aspects of language understanding and production. In this paper, we advocate for the revival of vocabulary tests as a valuable tool for assessing LLM performance. We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge. These findings shed light on the intricacies of LLM word representations, their learning mechanisms, and performance variations across models and languages. Moreover, the ability to automatically generate and perform vocabulary tests offers new opportunities to expand the approach and provide a more complete picture of LLMs' language skills.",
    "link": "http://arxiv.org/abs/2310.14703",
    "context": "Title: Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models. (arXiv:2310.14703v2 [cs.CL] UPDATED)\nAbstract: Vocabulary tests, once a cornerstone of language modeling evaluation, have been largely overlooked in the current landscape of Large Language Models (LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus on specific tasks or domain-specific knowledge, they often neglect the fundamental linguistic aspects of language understanding and production. In this paper, we advocate for the revival of vocabulary tests as a valuable tool for assessing LLM performance. We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge. These findings shed light on the intricacies of LLM word representations, their learning mechanisms, and performance variations across models and languages. Moreover, the ability to automatically generate and perform vocabulary tests offers new opportunities to expand the approach and provide a more complete picture of LLMs' language skills.",
    "path": "papers/23/10/2310.14703.json",
    "total_tokens": 940,
    "translated_title": "将词汇测试作为评估大型语言模型的基准",
    "translated_abstract": "在当前大型语言模型（LLMs）如Llama、Mistral和GPT主导的语言模型评估环境中，词汇测试一度被忽视。虽然大多数LLM评估基准关注特定任务或领域特定知识，但它们常常忽略了语言理解和产出的基本语言学方面。本文主张恢复词汇测试作为评估LLM性能的有价值工具。我们使用两种词汇测试格式评估了七个LLM模型，并发现了它们在词汇知识上的一些令人惊讶的差距。这些发现揭示了LLM词汇表示的复杂性、它们的学习机制以及模型和语言之间的性能变化。此外，自动生成和执行词汇测试的能力提供了扩展这一方法并提供LLM语言技能更完整画面的新机会。",
    "tldr": "本论文主张将词汇测试作为评估大型语言模型性能的重要工具，在评估七个LLM模型时发现了它们在词汇知识方面存在差距，并探讨了LLM词汇表示、学习机制和性能变化的细节。自动生成和执行词汇测试为扩展这一方法提供了新的机会。",
    "en_tdlr": "This paper advocates for the use of vocabulary tests as a valuable tool for evaluating the performance of large language models. Through the evaluation of seven LLMs, surprising gaps in their lexical knowledge were uncovered, shedding light on the intricacies of LLM word representations, learning mechanisms, and performance variations across models and languages. The ability to automatically generate and perform vocabulary tests offers new opportunities to expand this approach and provide a more comprehensive understanding of LLMs' language skills."
}