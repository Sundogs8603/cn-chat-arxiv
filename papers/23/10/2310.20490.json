{
    "title": "Long-Tailed Learning as Multi-Objective Optimization. (arXiv:2310.20490v2 [cs.CV] UPDATED)",
    "abstract": "Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under ",
    "link": "http://arxiv.org/abs/2310.20490",
    "context": "Title: Long-Tailed Learning as Multi-Objective Optimization. (arXiv:2310.20490v2 [cs.CV] UPDATED)\nAbstract: Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under ",
    "path": "papers/23/10/2310.20490.json",
    "total_tokens": 892,
    "translated_title": "长尾学习作为多目标优化",
    "translated_abstract": "现实世界中的数据极不平衡，呈现出长尾分布，导致模型对具有足够样本的类别有偏见，并且在罕见类别上表现不佳。最近的方法提出了类别重平衡，但它们面临着相互矛盾的问题（提高尾部类别的性能可能会降低头部类别的性能，反之亦然）。本文认为，这一问题的根源是不同类别的梯度不平衡，不适当类别的梯度被设置为重要更新的部分，因此在尾部类别上容易产生过补偿或欠补偿。为了实现理想的补偿，我们将长尾识别定义为一种多目标优化问题，同时公平地尊重头部和尾部类别的贡献。为了提高效率，我们提出了一种梯度平衡分组（GBG）策略来收集具有相似梯度方向的类别，从而近似使每次更新都在相似的方向上。",
    "tldr": "这项研究通过将长尾识别问题转化为多目标优化问题，提出一种公平地估计头部和尾部类别贡献的方法，并通过梯度平衡分组策略提高了效率。",
    "en_tdlr": "This research proposes a method that formulates long-tailed recognition as a multi-objective optimization problem, aiming to fairly estimate the contributions of head and tail classes and improve efficiency through a gradient balancing grouping strategy."
}