{
    "title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks. (arXiv:2310.17683v1 [cs.LG])",
    "abstract": "As one of the most popular neural network modules, Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing. The effectiveness of the Transformer is often attributed to its multi-head attention (MHA) mechanism. In this study, we discuss the limitations of MHA, including the high computational complexity due to its ``query-key-value'' architecture and the numerical issue caused by its softmax operation. Considering the above problems and the recent development tendency of the attention layer, we propose an effective and efficient surrogate of the Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA mechanism with an extremely simple ``slicing-sorting'' operation, i.e., projecting inputs linearly to a latent space and sorting them along different feature dimensions (or equivalently, called channels). For each feature dimension, the sorting operation implicitly gener",
    "link": "http://arxiv.org/abs/2310.17683",
    "context": "Title: Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks. (arXiv:2310.17683v1 [cs.LG])\nAbstract: As one of the most popular neural network modules, Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing. The effectiveness of the Transformer is often attributed to its multi-head attention (MHA) mechanism. In this study, we discuss the limitations of MHA, including the high computational complexity due to its ``query-key-value'' architecture and the numerical issue caused by its softmax operation. Considering the above problems and the recent development tendency of the attention layer, we propose an effective and efficient surrogate of the Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA mechanism with an extremely simple ``slicing-sorting'' operation, i.e., projecting inputs linearly to a latent space and sorting them along different feature dimensions (or equivalently, called channels). For each feature dimension, the sorting operation implicitly gener",
    "path": "papers/23/10/2310.17683.json",
    "total_tokens": 872,
    "translated_title": "Sliceformer: 将多头注意力机制简化为排序在判别任务中的应用",
    "translated_abstract": "作为最流行的神经网络模块之一，Transformer在许多基础的深度学习模型中起着核心作用，例如在计算机视觉中的ViT和在自然语言处理中的BERT和GPT。Transformer的有效性通常归因于其多头注意力机制。在本研究中，我们讨论了MHA的局限性，包括由于其“查询-键-值”架构而导致的高计算复杂性以及由其softmax操作引起的数值问题。考虑到上述问题和注意力层的最近发展趋势，我们提出了一个高效且有效的Transformer替代方案，称为Sliceformer。我们的Sliceformer用一个极其简单的“切片-排序”操作替代了经典的MHA机制，即将输入线性投影到潜空间，并沿着不同的特征维度（或等价地称为通道）进行排序。对于每个特征维度，排序操作隐含地生成了隐藏的参数化多头注意力机制。",
    "tldr": "Sliceformer 提出了一个高效且有效的 Transformer 替代方案，通过一个简单的“切片-排序”操作替代了传统的多头注意力机制，解决了其高计算复杂性和数值问题。"
}