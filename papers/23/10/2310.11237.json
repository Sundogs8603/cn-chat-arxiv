{
    "title": "Watermarking LLMs with Weight Quantization. (arXiv:2310.11237v1 [cs.CL])",
    "abstract": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.",
    "link": "http://arxiv.org/abs/2310.11237",
    "context": "Title: Watermarking LLMs with Weight Quantization. (arXiv:2310.11237v1 [cs.CL])\nAbstract: Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.",
    "path": "papers/23/10/2310.11237.json",
    "total_tokens": 845,
    "translated_title": "使用权重量化对LLMs进行水印处理",
    "translated_abstract": "大型语言模型被滥用会带来高风险，因此保护模型权重以避免侵犯开源大型语言模型的许可是非常重要的。本文提出了一种新颖的水印策略，通过在大型语言模型的量化过程中插入水印，而无需在推理过程中事先定义触发器。这种水印在模型以fp32模式使用时起作用，在模型量化为int8后保持隐藏，这样用户只能进行模型的推理，而无需进行进一步的模型监督微调。我们成功地将水印嵌入到包括GPT-Neo和LLaMA在内的开源大型语言模型的权重中。希望我们提出的方法能够为保护大型语言模型应用时的模型权重提供一个潜在的方向。",
    "tldr": "本论文提出了一种新颖的水印策略，通过在大型语言模型的量化过程中插入水印来保护模型权重，使得模型在推理时起作用但在量化后保持隐藏，为保护大型语言模型应用时的模型权重提供了一个潜在的方向。",
    "en_tdlr": "This paper proposes a novel watermarking strategy that protects the weights of large language models by embedding watermarks during the quantization process, allowing the model to function during inference but remain hidden after quantization. It provides a potential direction for protecting the weights of large language model applications."
}