{
    "title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform. (arXiv:2310.00036v1 [cs.LG])",
    "abstract": "Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings. Cleanba's source code is available at \\url{https://github.com/vwxyzjn/cleanba}",
    "link": "http://arxiv.org/abs/2310.00036",
    "context": "Title: Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform. (arXiv:2310.00036v1 [cs.LG])\nAbstract: Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings. Cleanba's source code is available at \\url{https://github.com/vwxyzjn/cleanba}",
    "path": "papers/23/10/2310.00036.json",
    "total_tokens": 938,
    "translated_title": "Cleanba: 一种可复现和高效的分布式强化学习平台",
    "translated_abstract": "分布式深度强化学习（DRL）旨在利用更多的计算资源，以在较短的训练时间内训练自主智能体。尽管该领域最近取得了进展，但复现性问题尚未得到充分探索。本文首先展示了即使控制了超参数，典型的actor-learner框架也可能存在复现性问题。然后，我们介绍了Cleanba，一个新的开源的分布式DRL平台，提出了一个高度可复现的架构。Cleanba实现了经过高度优化的PPO和IMPALA的分布式变种。我们在Atari实验中发现，这些变种在moolib和torchbeast的强IMPALA基线以及CleanRL的PPO基线中可以获得相等或更高的得分。然而，Cleanba的变体在不同硬件设置下呈现出1）更短的训练时间和2）更具复现性的学习曲线。Cleanba的源代码可在\\url{https://github.com/vwxyzjn/cleanba}上获取。",
    "tldr": "Cleanba是一种可复现和高效的分布式强化学习平台，它通过引入高度可复现的架构和经过高度优化的分布式变种，解决了分布式深度强化学习中的复现性问题，并在实验中展现了更短的训练时间和更具复现性的学习曲线。"
}