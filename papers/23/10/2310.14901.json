{
    "title": "Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks. (arXiv:2310.14901v1 [cs.LG])",
    "abstract": "Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performan",
    "link": "http://arxiv.org/abs/2310.14901",
    "context": "Title: Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks. (arXiv:2310.14901v1 [cs.LG])\nAbstract: Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performan",
    "path": "papers/23/10/2310.14901.json",
    "total_tokens": 902,
    "translated_title": "可行的无鞍牛顿优化神经网络的Hessian-Vector乘积系列",
    "translated_abstract": "尽管拟牛顿方法在连续优化领域非常受欢迎，但在机器学习中应用仍具有挑战性，因为Hessian矩阵的规模过大。通过修改Hessian的特征值来处理非凸性，如无鞍牛顿方法，进一步增加了计算负担。我们提出了一种同时解决这两个问题的优化算法-据我们所知，这是首个可以渐近地使用精确（特征值修改后的）逆Hessian的高效可伸缩优化算法。我们的方法将问题表述为一个主要对Hessian进行平方根和求逆的级数，然后用它来预处理梯度向量，而无需显式计算或进行特征分解。对该无限级数的截断提供了一个新的可伸缩优化算法，其运行时间和优化性能与其他一阶和二阶优化方法相当。",
    "tldr": "本文提出了一种以Hessian-Vector乘积系列为基础的优化算法，通过平方根和求逆操作实现了高效可伸缩的优化方法，并相对于其他一阶和二阶优化方法在运行时间和性能上具有可比性。",
    "en_tdlr": "This paper proposes a scalable optimization algorithm based on a series of Hessian-Vector products, which efficiently preprocesses gradient vectors by square-rooting and inverting the Hessian, achieving comparable performance to other first- and second-order optimization methods in terms of runtime and optimization performance."
}