{
    "title": "Do Language Models Learn about Legal Entity Types during Pretraining?. (arXiv:2310.13092v1 [cs.CL])",
    "abstract": "Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 perfo",
    "link": "http://arxiv.org/abs/2310.13092",
    "context": "Title: Do Language Models Learn about Legal Entity Types during Pretraining?. (arXiv:2310.13092v1 [cs.CL])\nAbstract: Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 perfo",
    "path": "papers/23/10/2310.13092.json",
    "total_tokens": 849,
    "translated_title": "在预训练期间，语言模型是否能够学习到法律实体类型?",
    "translated_abstract": "语言模型在预训练阶段已经证明了其获取多样化语言知识的能力，潜在地成为下游任务中有价值的附带监督源。然而，在检索特定领域知识，特别是法律知识方面，相关研究还有限。我们提出探索实体类型任务，作为评估法律知识作为文本理解的基本方面以及多个法律自然语言处理应用的基础任务的代理。通过系统评估和分析，我们比较了不同类型和长度的实体（包括通用实体和领域特定实体）、语义或语法信号以及不同的LM预训练语料库（通用和法律导向）和架构（基于BERT的编码器和仅解码器的Llama2），并使用填空句和基于问答的模板来澄清这些获取的线索的性质。我们发现，Llama2性能较好。",
    "tldr": "该论文研究了语言模型在预训练期间是否能够学习到法律实体类型，并通过对实体类型任务进行评估和分析，发现Llama2表现良好。"
}