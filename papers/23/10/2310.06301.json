{
    "title": "Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])",
    "abstract": "We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.",
    "link": "http://arxiv.org/abs/2310.06301",
    "context": "Title: Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])\nAbstract: We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.",
    "path": "papers/23/10/2310.06301.json",
    "total_tokens": 1008,
    "translated_title": "超叠加的玩具模型中的动力学与贝叶斯相变",
    "translated_abstract": "我们使用奇异学习理论（SLT）研究超叠加的玩具模型（TMS）中的相变。我们推导出了理论损失的闭式公式，并在两个隐藏维度的情况下发现，正则的$k$-gons是临界点。我们提出了支持理论，表明这些$k$-gons的本地学习系数（几何不变量）决定了贝叶斯后验作为训练样本大小的函数的相变。然后，我们凭经验证明，同样的$k$-gon临界点也决定了SGD训练的行为。得出的结论支持了SGD学习轨迹受顺序学习机制影响的猜想。具体而言，我们发现TMS中的学习过程，无论是通过SGD还是贝叶斯学习，可以被描述为在参数空间中从高损失和低复杂性的区域向低损失和高复杂性的区域的旅程。",
    "tldr": "本研究通过奇异学习理论研究超叠加的玩具模型中的相变，在两个隐藏维度的情况下发现正则的$k$-gons是临界点，并提供支持理论表明这些临界点决定了贝叶斯后验的相变。此外，实验证明这些临界点也决定了SGD训练的行为。研究结果支持了SGD学习轨迹受顺序学习机制影响的猜想。",
    "en_tdlr": "This study investigates phase transitions in a toy model of superposition using singular learning theory. The research discovers critical points in the form of regular k-gons and shows that they determine the phase transitions in the Bayesian posterior. Empirical evidence also indicates that these critical points determine the behavior of SGD training. These findings support the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism."
}