{
    "title": "Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])",
    "abstract": "Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce",
    "link": "http://arxiv.org/abs/2310.03225",
    "context": "Title: Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])\nAbstract: Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce",
    "path": "papers/23/10/2310.03225.json",
    "total_tokens": 960,
    "translated_title": "安全探索在强化学习中的应用：一种普适的形式化和算法",
    "translated_abstract": "在许多真实场景中，安全探索对于实际应用强化学习（RL）至关重要。本文提出了一种广义安全探索（GSE）问题，作为常见安全探索问题的统一形式化。然后，我们提出了GSE问题的解决方案，即安全探索的元算法MASE，它将无约束的RL算法与不确定性量化器相结合，以保证当前回合的安全性，同时在实际安全违规之前适当惩罚不安全的探索，以防止它们在未来回合中发生。MASE的优势在于我们可以在保证高概率下不违反安全约束的前提下，优化策略。具体而言，我们提出了两种不同构建不确定性量化器的MASE变体：一种基于广义线性模型，具有安全性和接近最优性的理论保证，另一种则结合了高斯过程。",
    "tldr": "本文提出了一种广义的安全探索问题，即GSE，并提出了一个元算法MASE来解决这个问题。MASE将无约束的强化学习算法与不确定性量化器结合，以保证安全性，并在违反安全约束之前惩罚不安全的探索。这种方法的优势是在保证安全性的同时进行策略优化，并具有高概率的安全保证。"
}