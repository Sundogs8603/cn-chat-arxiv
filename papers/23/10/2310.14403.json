{
    "title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)",
    "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills",
    "link": "http://arxiv.org/abs/2310.14403",
    "context": "Title: O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)\nAbstract: Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills",
    "path": "papers/23/10/2310.14403.json",
    "total_tokens": 885,
    "translated_title": "O3D: 基于离线数据的发现与蒸馏方法，用于大规模语言模型在顺序决策中的应用",
    "translated_abstract": "最近对大规模语言模型 (LLMs)的研究取得了令人期待的进展，在解决顺序决策问题方面显示出了良好的性能。通过模仿提示中提供的少量示例（即上下文学习），LLM代理可以与外部环境交互并完成给定任务，而无需额外的训练。然而，这种少量示例往往不足以生成复杂且长期目标任务的高质量解决方案，而有限的上下文长度无法处理更大规模的演示。为此，我们提出了一种利用离线数据的学习框架，以大规模的离线数据（例如人类交互的日志）来改进LLM代理的上下文学习性能。我们通过文本和代码两种方法正式定义了LLM强化策略。然后，我们引入了一种名为O3D的离线数据驱动发现和蒸馏框架，以改善LLM强化策略而无需微调。O3D自动地发现可重复使用的技能",
    "tldr": "O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现"
}