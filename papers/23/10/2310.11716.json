{
    "title": "Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])",
    "abstract": "Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed \"reflection-tuning,\" which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.",
    "link": "http://arxiv.org/abs/2310.11716",
    "context": "Title: Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])\nAbstract: Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed \"reflection-tuning,\" which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.",
    "path": "papers/23/10/2310.11716.json",
    "total_tokens": 868,
    "translated_title": "反射调整：数据回收改进了LLM指令调整",
    "translated_abstract": "最近，大规模语言模型(LLMs)的进展拓宽了自然语言理解和生成的范围。值得注意的是，通过指令调整可以改进LLMs的输出控制和与输入的对齐。然而，正如几项研究所指出的那样，训练集中的低质量数据通常对指令调整有害，导致LLMs的输出不一致甚至误导人。我们提出了一种新的方法，称为“反射调整”，通过LLMs的自我改进和判断能力来解决这个问题。这种方法借助一个Oracle LLM来回收原始的训练数据，通过内省和增强数据中的指令和响应的质量。在广泛使用的评估基准上进行的大量实验表明，我们使用回收数据训练的LLMs在各种基准测试中都优于使用现有数据集训练的LLMs。",
    "tldr": "这项研究提出了一种名为\"反射调整\"的方法，通过自我改进和判断能力来解决LLMs指令调整中的问题。通过借助Oracle LLM回收训练数据，该方法显著提高了LLMs在各个基准测试中的性能。",
    "en_tdlr": "This study proposes a method called \"reflection-tuning\" to address the issue of instruction tuning in LLMs by utilizing self-improvement and judging capabilities. By recycling training data with the help of an Oracle LLM, this method significantly improves the performance of LLMs in various benchmarks."
}