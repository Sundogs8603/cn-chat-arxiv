{
    "title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])",
    "abstract": "Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta",
    "link": "http://arxiv.org/abs/2310.09238",
    "context": "Title: BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])\nAbstract: Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta",
    "path": "papers/23/10/2310.09238.json",
    "total_tokens": 969,
    "translated_title": "BanglaNLP在BLP-2023任务2中的表现: 对Bangla社交媒体帖子的情感分析的Transformer模型的基准测试",
    "translated_abstract": "Bangla是全球第七大使用最广泛的语言，拥有来自印度和孟加拉国的2.34亿母语使用者。这种形态丰富的语言拥有丰富的文学传统，包括不同的方言和语言特定的挑战。尽管其语言丰富性和历史，但在自然语言处理（NLP）和语音社区中，Bangla仍被归类为资源匮乏的语言。本论文展示了我们在BLP研讨会的任务2（Bangla社交媒体帖子情感分析）中的提交。我们尝试了不同的基于Transformer的架构来解决这个任务。我们的定量结果显示，在这种低资源语言场景中，迁移学习确实有助于模型更好地学习。当我们进一步对已经在Twitter数据上进行了情感分析任务的模型进行微调时，这一点变得明显，而这个经过微调的模型在所有其他模型中表现最佳。我们还进行了详细的实验。",
    "tldr": "本论文提出了针对Bangla社交媒体帖子进行情感分析的任务，通过实验发现在这种低资源语言场景下，使用Transformer模型进行迁移学习可以提高模型的学习效果，并且在对已经在Twitter数据上进行了情感分析任务的模型进一步微调的情况下，模型的性能最好。",
    "en_tdlr": "This paper presents a task of sentiment analysis for Bangla social media posts, and experiments show that transfer learning with Transformer models improves learning in low-resource language scenarios, with further fine-tuning on Twitter data resulting in the best performance."
}