{
    "title": "Overview of AdaBoost : Reconciling its views to better understand its dynamics. (arXiv:2310.18323v1 [cs.LG])",
    "abstract": "Boosting methods have been introduced in the late 1980's. They were born following the theoritical aspect of PAC learning. The main idea of boosting methods is to combine weak learners to obtain a strong learner. The weak learners are obtained iteratively by an heuristic which tries to correct the mistakes of the previous weak learner. In 1995, Freund and Schapire [18] introduced AdaBoost, a boosting algorithm that is still widely used today. Since then, many views of the algorithm have been proposed to properly tame its dynamics. In this paper, we will try to cover all the views that one can have on AdaBoost. We will start with the original view of Freund and Schapire before covering the different views and unify them with the same formalism. We hope this paper will help the non-expert reader to better understand the dynamics of AdaBoost and how the different views are equivalent and related to each other.",
    "link": "http://arxiv.org/abs/2310.18323",
    "context": "Title: Overview of AdaBoost : Reconciling its views to better understand its dynamics. (arXiv:2310.18323v1 [cs.LG])\nAbstract: Boosting methods have been introduced in the late 1980's. They were born following the theoritical aspect of PAC learning. The main idea of boosting methods is to combine weak learners to obtain a strong learner. The weak learners are obtained iteratively by an heuristic which tries to correct the mistakes of the previous weak learner. In 1995, Freund and Schapire [18] introduced AdaBoost, a boosting algorithm that is still widely used today. Since then, many views of the algorithm have been proposed to properly tame its dynamics. In this paper, we will try to cover all the views that one can have on AdaBoost. We will start with the original view of Freund and Schapire before covering the different views and unify them with the same formalism. We hope this paper will help the non-expert reader to better understand the dynamics of AdaBoost and how the different views are equivalent and related to each other.",
    "path": "papers/23/10/2310.18323.json",
    "total_tokens": 842,
    "translated_title": "AdaBoost概述：调和不同视角以更好地理解其动态",
    "translated_abstract": "提升方法于20世纪80年代末引入，其产生是基于PAC学习的理论方面。提升方法的主要思想是通过组合弱学习器来获得强学习器。弱学习器通过启发式算法迭代地纠正上一个弱学习器的错误而获得。1995年，Freund和Schapire [18]引入了AdaBoost，这是一个至今仍广泛使用的提升算法。自那以后，针对该算法的许多视角被提出来以更好地调控其动态。在本文中，我们将尝试涵盖对于AdaBoost可能存在的所有视角。我们将从Freund和Schapire的原始视角开始，然后涵盖不同视角，并使用相同的形式化方法将它们统一起来。我们希望本文能帮助非专业读者更好地理解AdaBoost的动态以及不同视角之间的等价性和相关性。",
    "tldr": "本文概述了AdaBoost算法的不同视角，并通过统一的形式化方法将它们相互关联，帮助读者更好地理解AdaBoost的动态。"
}