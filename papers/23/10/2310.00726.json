{
    "title": "Improving Length-Generalization in Transformers via Task Hinting. (arXiv:2310.00726v1 [cs.LG])",
    "abstract": "It has been observed in recent years that transformers have problems with length generalization for certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied to longer instances of the same problem. This work proposes an approach based on task hinting towards addressing length generalization. Our key idea is that while training the model on task-specific data, it is helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.  We study the classical sorting problem as a canonical example to evaluate our approach. We design a multitask training framework and show that task hinting significantly improve length generalization. For sorting we show that it is possible to train models on data consisting of sequences having length at most $20$, and improve the test accuracy on sequences of length $100$ from l",
    "link": "http://arxiv.org/abs/2310.00726",
    "context": "Title: Improving Length-Generalization in Transformers via Task Hinting. (arXiv:2310.00726v1 [cs.LG])\nAbstract: It has been observed in recent years that transformers have problems with length generalization for certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied to longer instances of the same problem. This work proposes an approach based on task hinting towards addressing length generalization. Our key idea is that while training the model on task-specific data, it is helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.  We study the classical sorting problem as a canonical example to evaluate our approach. We design a multitask training framework and show that task hinting significantly improve length generalization. For sorting we show that it is possible to train models on data consisting of sequences having length at most $20$, and improve the test accuracy on sequences of length $100$ from l",
    "path": "papers/23/10/2310.00726.json",
    "total_tokens": 846,
    "translated_title": "通过任务提示在变压器中改善长度泛化能力",
    "translated_abstract": "近年来观察到，对于某些类型的推理和算术任务，变压器在长度泛化方面存在问题。特别是，在任务（例如加法）的训练中，当应用于相同问题的更长实例时，变压器模型的性能会急剧下降。本文提出了一种基于任务提示的方法来解决长度泛化问题。我们的关键思想是，在训练模型处理特定任务的数据时，同时训练模型处理一个更简单但相关的辅助任务。我们以经典的排序问题为例来评估我们的方法。我们设计了一个多任务训练框架，并展示了任务提示在改善长度泛化能力方面的显著效果。对于排序问题，我们展示了可以训练模型处理长度最多为20的序列数据，并将在长度为100的序列上的测试准确率提高至",
    "tldr": "该论文提出了一种基于任务提示的方法，在变压器模型中改善了长度泛化能力。通过同时训练模型处理简单但相关的辅助任务，可以显著提高模型在长序列数据上的性能。"
}