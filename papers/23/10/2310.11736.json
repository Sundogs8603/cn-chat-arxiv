{
    "title": "Kernel Learning in Ridge Regression \"Automatically\" Yields Exact Low Rank Solution. (arXiv:2310.11736v1 [math.ST])",
    "abstract": "We consider kernels of the form $(x,x') \\mapsto \\phi(\\|x-x'\\|^2_\\Sigma)$ parametrized by $\\Sigma$. For such kernels, we study a variant of the kernel ridge regression problem which simultaneously optimizes the prediction function and the parameter $\\Sigma$ of the reproducing kernel Hilbert space. The eigenspace of the $\\Sigma$ learned from this kernel ridge regression problem can inform us which directions in covariate space are important for prediction.  Assuming that the covariates have nonzero explanatory power for the response only through a low dimensional subspace (central mean subspace), we find that the global minimizer of the finite sample kernel learning objective is also low rank with high probability. More precisely, the rank of the minimizing $\\Sigma$ is with high probability bounded by the dimension of the central mean subspace. This phenomenon is interesting because the low rankness property is achieved without using any explicit regularization of $\\Sigma$, e.g., nuclear",
    "link": "http://arxiv.org/abs/2310.11736",
    "context": "Title: Kernel Learning in Ridge Regression \"Automatically\" Yields Exact Low Rank Solution. (arXiv:2310.11736v1 [math.ST])\nAbstract: We consider kernels of the form $(x,x') \\mapsto \\phi(\\|x-x'\\|^2_\\Sigma)$ parametrized by $\\Sigma$. For such kernels, we study a variant of the kernel ridge regression problem which simultaneously optimizes the prediction function and the parameter $\\Sigma$ of the reproducing kernel Hilbert space. The eigenspace of the $\\Sigma$ learned from this kernel ridge regression problem can inform us which directions in covariate space are important for prediction.  Assuming that the covariates have nonzero explanatory power for the response only through a low dimensional subspace (central mean subspace), we find that the global minimizer of the finite sample kernel learning objective is also low rank with high probability. More precisely, the rank of the minimizing $\\Sigma$ is with high probability bounded by the dimension of the central mean subspace. This phenomenon is interesting because the low rankness property is achieved without using any explicit regularization of $\\Sigma$, e.g., nuclear",
    "path": "papers/23/10/2310.11736.json",
    "total_tokens": 947,
    "translated_title": "在Ridge回归中，核学习“自动”给出精确的低秩解",
    "translated_abstract": "我们考虑形式为$(x,x') \\mapsto \\phi(\\|x-x'\\|^2_\\Sigma)$且由参数$\\Sigma$参数化的核函数。对于这样的核函数，我们研究了核岭回归问题的变体，它同时优化了预测函数和再现核希尔伯特空间的参数$\\Sigma$。从这个核岭回归问题中学到的$\\Sigma$的特征空间可以告诉我们协变量空间中哪些方向对预测是重要的。假设协变量只通过低维子空间（中心均值子空间）对响应变量有非零的解释能力，我们发现有很高的概率下有限样本核学习目标的全局最小化者也是低秩的。更具体地说，最小化$\\Sigma$的秩有很高的概率被中心均值子空间的维度所限制。这个现象很有趣，因为低秩特性是在没有使用任何对$\\Sigma$的显式正则化的情况下实现的，例如核范数正则化等。",
    "tldr": "该论文研究了核岭回归问题中核学习的低秩解的性质。在只有低维子空间对响应变量有解释能力的情况下，通过该方法可以自动得到精确的低秩解，无需额外的正则化。",
    "en_tdlr": "This paper studies the properties of low rank solutions in kernel learning for the kernel ridge regression problem. In the case where only a low-dimensional subspace has explanatory power for the response variable, it is shown that exact low rank solutions can be obtained automatically without explicit regularization."
}