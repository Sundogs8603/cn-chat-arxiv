{
    "title": "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages",
    "abstract": "arXiv:2310.04799v2 Announce Type: replace  Abstract: Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our ",
    "link": "https://arxiv.org/abs/2310.04799",
    "context": "Title: Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages\nAbstract: arXiv:2310.04799v2 Announce Type: replace  Abstract: Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our ",
    "path": "papers/23/10/2310.04799.json",
    "total_tokens": 871,
    "translated_title": "Chat Vector：一种简单的方法，在新语言中为LLMs提供指令遵循和模型对齐",
    "translated_abstract": "最近，开源大型语言模型（LLMs）的发展迅速。尽管如此，由于数据约束，大多数开源LLM的能力主要集中在英语上。为了解决这个问题，我们引入了聊天向量的概念，通过简单的模型算术为预训练的语言模型提供指令遵循和人类价值对齐。聊天向量是通过将预训练基础模型（例如LLaMA2）的权重减去其对应的聊天模型（例如LLaMA2-chat）的权重得出的。通过简单地将聊天向量添加到持续预训练模型的权重中，我们可以使模型在新语言中具有聊天能力，而无需进一步训练。我们的实证研究展示了聊天向量在三个不同方面的优越有效性：指令遵循、毒性缓解和多轮对话。此外，为了展示我们方法的适应性",
    "tldr": "提出了聊天向量的概念，通过简单的模型算术使预训练语言模型具备在新语言中遵循指令和实现模型对齐的能力",
    "en_tdlr": "Introducing the concept of chat vector to enable pre-trained language models to follow instructions and align with human values in new languages through simple model arithmetic."
}