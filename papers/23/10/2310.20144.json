{
    "title": "EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])",
    "abstract": "We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.",
    "link": "http://arxiv.org/abs/2310.20144",
    "context": "Title: EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])\nAbstract: We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.",
    "path": "papers/23/10/2310.20144.json",
    "total_tokens": 871,
    "translated_title": "EELBERT:通过动态嵌入实现微型模型",
    "translated_abstract": "我们介绍了EELBERT，一种用于压缩基于Transformer的模型（例如BERT）的方法，对下游任务的准确性影响最小。这是通过将模型的输入嵌入层替换为动态的，即即时计算的嵌入实现来实现的。由于输入嵌入层占模型大小的重要部分，特别是对于较小的BERT变体，用嵌入计算函数替换该层有助于显著减小模型大小。在GLUE基准测试中的实证评估显示，我们的BERT变体（EELBERT）与传统BERT模型相比仅具有最小的回归。通过这种方法，我们能够开发出我们最小的模型UNO-EELBERT，其GLUE得分比完全训练的BERT-tiny高4％，同时体积小15倍（1.2MB）。",
    "tldr": "EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。",
    "en_tdlr": "EELBERT is an approach for creating tiny models through dynamic embeddings, with minimal regression and significant reduction in model size. The smallest model, UNO-EELBERT, achieves a GLUE score within 4% of fully trained BERT-tiny while being only 15 times smaller in size (1.2MB)."
}