{
    "title": "Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])",
    "abstract": "We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.",
    "link": "http://arxiv.org/abs/2310.17936",
    "context": "Title: Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])\nAbstract: We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.",
    "path": "papers/23/10/2310.17936.json",
    "total_tokens": 807,
    "translated_title": "Transformers作为图到图模型",
    "translated_abstract": "我们认为Transformers本质上是图到图模型，而序列只是一种特殊情况。注意力权重在功能上等价于图中的边。我们的图到图Transformer架构将这种能力明确地体现出来，通过将图的边输入到注意力权重计算中，并使用类似注意力的函数来预测图的边，从而将显式图集成到预训练Transformers学习的潜在图中。添加迭代图细化可以为输入、输出和潜在图提供联合嵌入，使得非自回归图预测可以优化完整的图，而无需任何专门的管道或解码策略。实证结果表明，该架构在建模各种语言结构方面达到了最先进的准确性，并与预训练学习的潜在语言表示非常有效地集成。",
    "tldr": "本文认为Transformers本质上是图到图模型，通过将注意力权重等价于图中的边，并使用图到图Transformer架构结合显式图和潜在图进行非自回归图预测，实现了在建模各种语言结构方面的最先进准确性。"
}