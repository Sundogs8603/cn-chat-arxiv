{
    "title": "BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])",
    "abstract": "Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\\&2, 7B\\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl",
    "link": "http://arxiv.org/abs/2310.19975",
    "context": "Title: BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])\nAbstract: Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\\&2, 7B\\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl",
    "path": "papers/23/10/2310.19975.json",
    "total_tokens": 921,
    "translated_title": "BioInstruct:用于生物医学自然语言处理的大型语言模型指令调整",
    "translated_abstract": "大型语言模型通过在大量数据上进行预训练，然后进行特定领域的指令调整，在许多自然语言处理任务中取得了巨大成功。然而，在生物医学领域只发表了很少的指令。为了解决这个问题，我们引入了BioInstruct，这是一个定制的任务特定指令数据集，包含超过25,000个示例。通过使用三个人工筛选的指令样本，以GPT-4语言模型作为提示，精调大型语言模型，我们旨在优化其在生物医学自然语言处理中的性能。我们对LLaMA LLMs (1&2,7B&13B)进行了指令调整，并在生物医学自然语言处理应用中进行了评估，包括信息提取、问答和文本生成。我们还评估了指令如何对模型性能的贡献，使用了多任务学习原则。",
    "tldr": "BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。",
    "en_tdlr": "BioInstruct is a method for instruction tuning of large language models for biomedical natural language processing. By introducing a customized task-specific instruction dataset called BioInstruct and fine-tuning the models using GPT-4 language model, the performance of the models in biomedical NLP is optimized."
}