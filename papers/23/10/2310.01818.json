{
    "title": "AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework. (arXiv:2310.01818v1 [cs.LG])",
    "abstract": "Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via",
    "link": "http://arxiv.org/abs/2310.01818",
    "context": "Title: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework. (arXiv:2310.01818v1 [cs.LG])\nAbstract: Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via",
    "path": "papers/23/10/2310.01818.json",
    "total_tokens": 864,
    "translated_title": "AutoLoRa：参数免调自动鲁棒微调框架",
    "translated_abstract": "鲁棒微调（RFT）是一种低成本策略，用于在下游应用中获得对抗鲁棒性，无需大量计算资源和收集大量数据。本文揭示了现有RFT存在的问题，即通过特征提取器（FE）优化对抗性和自然目标会导致显著不同的梯度方向。这种分歧在优化过程中引入不稳定性，从而阻碍了对抗鲁棒性的实现，并使RFT对超参数高度敏感。为了解决这个问题，我们提出了一个低秩（LoRa）分支，将RFT分解为两个不同的组件：通过LoRa分支优化自然目标和通过FE优化对抗目标。此外，我们还引入了启发式策略来自动调整学习率和损失项的标量。大量实证评估表明，我们提出的自动化RFT通过LoRa分解得到了...",
    "tldr": "AutoLoRa是一个自动鲁棒微调框架，通过引入低秩分支和启发式策略，解决了现有鲁棒微调存在的梯度方向分歧和超参数敏感性问题。",
    "en_tdlr": "AutoLoRa is an automated robust fine-tuning framework that addresses the issues of gradient divergence and hyperparameter sensitivity in existing robust fine-tuning approaches, by introducing a low-rank branch and heuristic strategies."
}