{
    "title": "Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])",
    "abstract": "Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup",
    "link": "http://arxiv.org/abs/2310.16639",
    "context": "Title: Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])\nAbstract: Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup",
    "path": "papers/23/10/2310.16639.json",
    "total_tokens": 898,
    "translated_title": "驾驶通过概念阻塞：解开可解释性瓶颈",
    "translated_abstract": "在可解释的机器学习中，概念阻塞模型通过利用一组人为定义的概念在模型中编码信息，取得了成功。在人类辅助或自动驾驶的背景下，可解释性模型可以帮助用户接受和理解自动驾驶车辆所做的决策，并用于合理化和解释驾驶员或车辆的行为。我们提出了一种新的方法，使用概念阻塞作为控制命令预测和用户车辆行为解释的可视特征。我们学习了一个人类可理解的概念层，用于解释顺序驾驶场景同时学习车辆的控制命令。这种方法可以用来确定人类（或自动驾驶车辆）对首选缝隙或转向命令的改变是否由外部刺激或偏好的改变所引导。在我们的模型设置中，我们获得了具有竞争性的性能，同时获得了可解释性。",
    "tldr": "本论文提出了一种使用概念阻塞作为控制命令预测和用户车辆行为解释的方法，通过学习人类可理解的概念层解释顺序驾驶场景，同时获得竞争性性能和可解释性。",
    "en_tdlr": "This paper proposes an approach using concept bottlenecks for control command predictions and explanations of user and vehicle behavior, achieving competitive performance and interpretability by learning a human-understandable concept layer to explain sequential driving scenes."
}