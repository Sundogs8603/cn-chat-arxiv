{
    "title": "Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])",
    "abstract": "Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performa",
    "link": "http://arxiv.org/abs/2310.08259",
    "context": "Title: Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])\nAbstract: Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performa",
    "path": "papers/23/10/2310.08259.json",
    "total_tokens": 865,
    "translated_title": "隐形威胁：OCR系统中的后门攻击",
    "translated_abstract": "光学字符识别（OCR）是一种广泛用于从扫描文档中提取文本的工具。目前，通过利用深度神经网络来实现最先进的性能。然而，这种性能的代价是系统容易受到威胁。例如，在后门攻击中，攻击者通过在受害者模型中插入后门，在特定模式下激活，从而干扰训练阶段。本文提出了一种针对OCR的后门攻击，通过恶意输入图像中注入非可读字符。这种简单但有效的攻击揭示了OCR的先进技术的弱点，使提取出的文本对人眼正确可读，但对使用OCR作为预处理步骤的自然语言处理应用来说无用。实验结果显示，被攻击的模型在大约90%的污染实例中成功输出非可读字符，而不影响其性能。",
    "tldr": "该论文介绍了一种针对OCR系统的后门攻击方法，通过插入非可读字符的恶意输入图像，在不影响模型性能的情况下干扰训练阶段。实验证明，该攻击方法对OCR的现有技术造成了威胁。"
}