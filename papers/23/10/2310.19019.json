{
    "title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn \"why\" instead of just \"what\". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.",
    "link": "http://arxiv.org/abs/2310.19019",
    "context": "Title: TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn \"why\" instead of just \"what\". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.",
    "path": "papers/23/10/2310.19019.json",
    "total_tokens": 976,
    "translated_title": "TeacherLM: 教人打鱼而不是给鱼，语言建模同理",
    "translated_abstract": "大型语言模型(LLMs)在各种自然语言处理任务中展现了惊人的推理和数据增强能力。然而，小型模型呢？在这项工作中，我们提出了TeacherLM-7.1B，能够给大多数自然语言处理样本进行相关基础知识、思维链和常见错误的注释，使注释不仅仅是一个答案，而且使其他模型可以学习“为什么”，而不仅仅是“什么”。TeacherLM-7.1B模型在MMLU上实现了52.3的零样本得分，超过了拥有100B参数的大多数模型。更令人印象深刻的是其数据增强能力。基于TeacherLM-7.1B，我们在多任务设置中使用了来自OPT和BLOOM系列的不同参数的多个学生模型对58个自然语言处理数据集进行了增强。实验结果表明，TeacherLM提供的数据增强带来了显着的好处。我们将作为开源发布TeacherLM系列模型和增强的数据集。",
    "tldr": "TeacherLM-7.1B是一个小型模型，通过给自然语言处理样本进行注释，教会其他模型“为什么”而不仅仅是“什么”。它在MMLU上取得了52.3的零样本得分，同时具有出色的数据增强能力。发布TeacherLM系列模型和增强的数据集作为开源项目。",
    "en_tdlr": "TeacherLM-7.1B is a small model that annotates natural language processing samples to teach other models \"why\" instead of just \"what\". It achieved a zero-shot score of 52.3 on MMLU and has impressive data augmentation ability. The TeacherLM series of models and augmented datasets will be released as open-source."
}