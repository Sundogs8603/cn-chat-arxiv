{
    "title": "Stochastic Gradient Descent for Gaussian Processes Done Right. (arXiv:2310.20581v1 [cs.LG])",
    "abstract": "We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apa",
    "link": "http://arxiv.org/abs/2310.20581",
    "context": "Title: Stochastic Gradient Descent for Gaussian Processes Done Right. (arXiv:2310.20581v1 [cs.LG])\nAbstract: We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apa",
    "path": "papers/23/10/2310.20581.json",
    "total_tokens": 885,
    "translated_title": "高斯过程的随机梯度下降的正确实现",
    "translated_abstract": "本文研究了使用平方损失函数的高斯过程回归的优化问题。目前，解决这个问题的最常见方法是应用精确求解器，比如共轭梯度下降，要么直接应用，要么应用于问题的降阶版本。最近，在深度学习的成功推动下，随机梯度下降作为一种替代方法获得了广泛关注。本文展示了当正确使用时（我们指的是利用优化和核函数领域的特定见解），这种方法是非常有效的。因此，我们引入了一种特定的随机对偶梯度下降算法，可以使用任何深度学习框架的几行代码实现。我们通过消融实验解释了我们的设计决策的优势，并表明新方法具有很高的竞争力。我们对标准回归基准和贝叶斯优化任务进行评估，证明了我们的方法的优越性。",
    "tldr": "本文研究了使用随机梯度下降方法优化高斯过程回归问题，并引入了一种特定的随机对偶梯度下降算法，该方法在标准回归基准和贝叶斯优化任务上表现出很高的竞争力。",
    "en_tdlr": "This paper investigates the optimization problem of Gaussian process regression using stochastic gradient descent and introduces a specific stochastic dual gradient descent algorithm that achieves high competitiveness in standard regression benchmarks and Bayesian optimization tasks."
}