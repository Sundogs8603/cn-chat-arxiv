{
    "title": "Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training. (arXiv:2310.05350v2 [cs.DC] UPDATED)",
    "abstract": "AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 ",
    "link": "http://arxiv.org/abs/2310.05350",
    "context": "Title: Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training. (arXiv:2310.05350v2 [cs.DC] UPDATED)\nAbstract: AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 ",
    "path": "papers/23/10/2310.05350.json",
    "total_tokens": 832,
    "translated_title": "针对大型语言模型预训练的高效参数搜索和并行性的扩展研究",
    "translated_abstract": "AI加速器的计算能力和内存限制在很大程度上确定了机器学习工作负载（如训练和推理）在可接受的时间范围内执行的规模。如今，训练最先进的基于transformer的模型需要使用带有高速互连的GPU加速高性能计算机。随着数据集和模型的不断增大，AI的计算需求和内存需求也在不断增长。这些挑战促使开发出分布式算法和基于电路的优化技术，能够在多节点环境中逐步扩展模型，有效地减少神经网络的成本函数以实现更快的收敛，并将更多参数存储在一定数量的可用资源中。在我们的研究项目中，我们专注于并行和分布式机器学习算法的开发，特别是针对优化数据处理和预训练的一组算法。",
    "tldr": "本研究针对大型语言模型预训练进行了并行和分布式机器学习算法的开发，旨在优化数据处理和提高模型的训练效率。",
    "en_tdlr": "This research focuses on the development of parallel and distributed machine learning algorithms for large-scale language model pre-training, aiming to optimize data processing and improve training efficiency."
}