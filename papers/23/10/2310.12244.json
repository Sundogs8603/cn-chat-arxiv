{
    "title": "A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])",
    "abstract": "Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthe",
    "link": "http://arxiv.org/abs/2310.12244",
    "context": "Title: A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])\nAbstract: Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthe",
    "path": "papers/23/10/2310.12244.json",
    "total_tokens": 946,
    "translated_title": "一种统一的带有记忆的领域增量学习方法: 理论和算法",
    "translated_abstract": "领域增量学习旨在适应一系列领域，仅能访问先前领域的一小部分数据（即记忆）。针对这个问题已经提出了各种方法，但它们之间的关系以及从实践者角度何时选择其中一种方法仍然不清楚。为此，我们提出了一种统一的框架，称为统一领域增量学习（UDIL），用于带有记忆的领域增量学习。我们的UDIL将各种现有方法统一起来，我们的理论分析表明，与这些方法相比，UDIL始终实现更紧的泛化误差界限。关键观点是不同的现有方法对应于我们的边界具有不同的固定系数；基于这种统一的洞察力，我们的UDIL允许在训练过程中使用自适应系数，从而始终实现最紧的界限。实证结果表明，我们的UDIL在合成数据和真实数据集上均优于最先进的领域增量学习方法。",
    "tldr": "这篇论文提出了一种统一的带有记忆的领域增量学习方法（UDIL），通过统一不同的现有方法并使用自适应系数，实现了更紧的泛化误差界限，并在实验证明在合成数据和真实数据集上优于最先进的方法。",
    "en_tdlr": "This paper presents a unified approach for domain incremental learning with memory (UDIL), which unifies different existing methods and achieves tighter generalization error bound through adaptive coefficients. Experimental results demonstrate its superiority over state-of-the-art methods on both synthetic and real datasets."
}