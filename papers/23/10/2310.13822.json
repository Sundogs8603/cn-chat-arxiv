{
    "title": "Adversarial Attacks on Fairness of Graph Neural Networks",
    "abstract": "arXiv:2310.13822v2 Announce Type: replace  Abstract: Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs an",
    "link": "https://arxiv.org/abs/2310.13822",
    "context": "Title: Adversarial Attacks on Fairness of Graph Neural Networks\nAbstract: arXiv:2310.13822v2 Announce Type: replace  Abstract: Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs an",
    "path": "papers/23/10/2310.13822.json",
    "total_tokens": 891,
    "translated_title": "对图神经网络公平性的对抗攻击",
    "translated_abstract": "具有公平性意识的图神经网络（GNNs）因能够减少在基于图的应用中对任何人口统计群体（例如女性）的预测偏见而引起了人们的关注。尽管这些方法极大地改善了GNNs的算法公平性，但公平性容易受到精心设计的对抗攻击的破坏。本文研究了对GNNs公平性的对抗攻击问题，并提出了G-FairAttack，这是一个攻击各种类型公平性感知GNNs的通用框架，对预测效用几乎没有察觉的影响。此外，我们提出了一种快速的计算技术来降低G-FairAttack的时间复杂度。实验研究表明，G-FairAttack成功地破坏了不同类型GNNs的公平性，同时保持了攻击的不可察觉性。我们在公平性攻击方面的研究揭示了公平性感知GNNs的潜在漏洞。",
    "tldr": "本文研究了对图神经网络公平性的对抗攻击问题，提出了G-FairAttack框架可以成功地攻击各种类型的公平性感知GNNs，并保持攻击的不可察觉性。"
}