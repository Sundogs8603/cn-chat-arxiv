{
    "title": "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])",
    "abstract": "Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the",
    "link": "http://arxiv.org/abs/2310.20158",
    "context": "Title: GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])\nAbstract: Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the",
    "path": "papers/23/10/2310.20158.json",
    "total_tokens": 900,
    "translated_title": "GAR-meets-RAG范式用于零样本信息检索",
    "translated_abstract": "给定一个查询和一个文档语料库，信息检索(IR)任务是输出一个相关文档的排名列表。结合大语言模型(LLMs)和基于嵌入的检索模型，最近的研究在零样本检索问题上取得了有希望的结果，即无法访问目标领域的标记数据。其中两个流行的范式是生成增强检索(GAR)或GAR（为查询生成附加上下文，然后检索）和检索增强生成(RAG)或RAG（将相关文档作为上下文检索，然后生成答案）。这些范式的成功取决于(i)在零样本设定中很难获得的高召回检索模型和(ii)通常需要良好初始化的高精确度(重新)排序模型。在这项工作中，我们提出了一种新颖的GAR-meets-RAG循环公式，克服了现有范式的挑战。我们的方法通过GAR和RAG阶段的迭代改进检索(通过GAR)和重写(通过RAG)阶段来提高性能。",
    "tldr": "这项工作提出了一种新颖的GAR-meets-RAG范式，通过迭代改进检索和重写阶段，克服了零样本信息检索中现有范式的挑战。"
}