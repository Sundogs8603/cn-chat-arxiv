{
    "title": "COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL. (arXiv:2310.07220v1 [cs.LG])",
    "abstract": "Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\\texttt{COPlanner}$ ",
    "link": "http://arxiv.org/abs/2310.07220",
    "context": "Title: COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL. (arXiv:2310.07220v1 [cs.LG])\nAbstract: Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\\texttt{COPlanner}$ ",
    "path": "papers/23/10/2310.07220.json",
    "total_tokens": 1002,
    "translated_title": "COPlanner: 保守的模型规划和乐观的环境探索为基于模型的强化学习提供支持",
    "translated_abstract": "Dyna-style基于模型的强化学习包含两个阶段：使用模型生成样本进行策略学习的模型演算阶段和使用当前策略进行真实环境探索以学习动力学模型的阶段。然而，由于复杂的现实环境，难免会学习到一个具有模型预测误差的不完美动力学模型，进而可能误导策略学习并导致次优解。本文提出了一种名为COPlanner的基于规划的框架，用于解决不准确学习的动力学模型问题，其采用保守的模型演算和乐观的环境探索。COPlanner利用一种基于策略引导的不确定性感知模型预测控制（UP-MPC）组件进行多步不确定性评估的规划。这个估计的不确定性在模型演算期间作为惩罚因素，在真实环境探索期间作为奖励因素，以选择动作。因此，COPlanner可以同时保证保守的模型演算和乐观的环境探索，提供更好的解决方案。",
    "tldr": "COPlanner是一个基于规划的框架，用于解决模型预测误差带来的问题。通过保守的模型演算和乐观的环境探索，COPlanner利用不确定性感知模型预测控制来解决动力学模型不准确的问题，并提供更好的解决方案。",
    "en_tdlr": "COPlanner is a planning-driven framework that addresses the issue of model prediction error. By combining conservative model rollouts and optimistic environment exploration, COPlanner utilizes uncertainty-aware model predictive control to solve the problem of inaccurately learned dynamics model and provides better solutions."
}