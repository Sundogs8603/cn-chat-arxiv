{
    "title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])",
    "abstract": "Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experim",
    "link": "http://arxiv.org/abs/2310.15594",
    "context": "Title: Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])\nAbstract: Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experim",
    "path": "papers/23/10/2310.15594.json",
    "total_tokens": 903,
    "translated_title": "检索式知识转移：一种高效的极大规模语言模型压缩方法",
    "translated_abstract": "大规模预训练语言模型在各种自然语言处理任务中展现出卓越的性能。然而，这些模型的巨大规模给它们在实际应用中的部署带来了巨大挑战。尽管已经提出了许多模型压缩技术，但对于在模型规模存在显著差距时实现极端模型压缩并不适用。在本文中，我们引入了一种新的压缩范例，称为检索式知识转移（RetriKT），它将LLM的知识有效地转移到极小规模的模型（例如1%）。具体而言，我们的方法从LLM中提取知识构建知识存储，并从中检索相关信息，利用它进行有效的推理。为了提高模型的质量，我们采用了软提示调整和近端策略优化（PPO）增强学习技术。进行了广泛的实验验证。",
    "tldr": "检索式知识转移（RetriKT）是一种新的压缩范例，它通过提取大规模预训练语言模型的知识并利用 retrieval-based 的方法，将这些知识应用于极小规模的模型中，从而实现了极端的模型压缩效果。"
}