{
    "title": "Evaluating Spatial Understanding of Large Language Models",
    "abstract": "arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f",
    "link": "https://arxiv.org/abs/2310.14540",
    "context": "Title: Evaluating Spatial Understanding of Large Language Models\nAbstract: arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f",
    "path": "papers/23/10/2310.14540.json",
    "total_tokens": 882,
    "translated_title": "评估大型语言模型的空间理解能力",
    "translated_abstract": "大型语言模型 (LLMs) 在各种任务中展现出卓越的能力。尽管这些模型在训练中只看到文本，但一些最近的研究表明，LLM表示隐含地捕捉了地面概念的几个方面。在这里，我们探讨了LLM表示对一种特别显著的基础知识 -- 空间关系的表现。我们设计了自然语言导航任务，评估了LLMs，特别是GPT-3.5-turbo、GPT-4 和 Llama2 系列模型，表示和推理空间结构的能力。这些任务揭示了LLM在不同空间结构 (包括正方形、六边形和三角形格、环和树) 上的性能差异。在广泛的错误分析中，我们发现LLMs的错误反映了空间和非空间因素。这些发现表明，LLMs似乎隐含地捕捉了空间结构的某些方面，但仍有提升的空间。",
    "tldr": "本研究评估了大型语言模型对空间结构的理解能力，发现LLMs在表示和推理空间结构时的表现存在显著差异，具有捕捉空间结构隐含特征的潜力。",
    "en_tdlr": "This study evaluates the spatial understanding capability of large language models, revealing significant variability in LLMs' performance in representing and reasoning about spatial structures, showing the potential of capturing implicit features of spatial structures."
}