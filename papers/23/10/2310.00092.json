{
    "title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality. (arXiv:2310.00092v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
    "link": "http://arxiv.org/abs/2310.00092",
    "context": "Title: Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality. (arXiv:2310.00092v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
    "path": "papers/23/10/2310.00092.json",
    "total_tokens": 855,
    "translated_title": "Voice2Action: 语言模型作为虚拟现实中高效实时交互的代理人",
    "translated_abstract": "大型语言模型（LLMs）被训练和调整以仅仅使用少量示例来遵循自然语言指令，并被提示为任务驱动的自主代理人，以适应不同的执行环境来源。然而，在虚拟现实（VR）中部署代理LLMs一直是具有挑战性的，其原因是在线交互的效率低下以及3D环境中复杂的操作类别。在这项工作中，我们提出了Voice2Action，一个通过动作和实体提取来分层分析定制语音信号和文本命令，并将执行任务实时分成规范的交互子集，并通过环境反馈来防止错误。在具有合成指令数据的城市工程VR环境中的实验结果表明，Voice2Action能够比没有优化的方法更高效和准确地执行。",
    "tldr": "本研究提出了Voice2Action，一种使用语言模型作为代理人在虚拟现实中进行高效实时交互的框架。通过对定制语音信号和文本命令进行分层分析，并将执行任务分成交互子集，Voice2Action能够比其他方法更高效和准确地执行。",
    "en_tdlr": "This paper proposes Voice2Action, a framework that uses language models as agents for efficient real-time interaction in virtual reality. By analyzing customized voice signals and textual commands hierarchically and dividing execution tasks into interaction subsets, Voice2Action can perform more efficiently and accurately than other methods."
}