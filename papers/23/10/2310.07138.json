{
    "title": "Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])",
    "abstract": "Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep",
    "link": "http://arxiv.org/abs/2310.07138",
    "context": "Title: Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])\nAbstract: Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep",
    "path": "papers/23/10/2310.07138.json",
    "total_tokens": 938,
    "translated_title": "扩散模型的去噪任务路由",
    "translated_abstract": "扩散模型通过学习多步去噪过程生成高度逼真的图像，自然地体现了多任务学习（MTL）的原理。尽管扩散模型和MTL之间存在固有的连接，但在设计明确将MTL纳入扩散模型框架的神经结构方面仍存在一个未被探索的领域。在本文中，我们提出了去噪任务路由（DTR），一种对现有扩散模型架构进行简单附加的策略，通过选择性地激活模型中的子通道来为单个任务建立独立的信息路径。DTR的特别吸引人之处在于它将去噪任务的先验知识无缝集成到框架中：（1）任务亲和性：DTR为相邻时间步的任务激活相似的通道，并将激活的通道作为滑动窗口通过时间步进行移动，利用相邻时间步任务间固有的强亲和关系。",
    "tldr": "本文提出了一种名为去噪任务路由的策略，通过为扩散模型的不同任务建立独立的信息路径，实现了对多任务学习的明确纳入。该方法将去噪任务的先验知识无缝集成到框架中，通过激活相似的通道和滑动窗口的方式，充分利用了相邻时间步任务间的亲和关系。",
    "en_tdlr": "This paper presents a strategy called Denoising Task Routing (DTR) that establishes separate information pathways for different tasks in diffusion models, explicitly incorporating multi-task learning. The method seamlessly integrates prior knowledge of denoising tasks, utilizing the affinity between tasks at adjacent timesteps by activating similar channels and sliding windows."
}