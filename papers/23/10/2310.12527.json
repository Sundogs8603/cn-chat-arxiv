{
    "title": "Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])",
    "abstract": "Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsi",
    "link": "http://arxiv.org/abs/2310.12527",
    "context": "Title: Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])\nAbstract: Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsi",
    "path": "papers/23/10/2310.12527.json",
    "total_tokens": 791,
    "translated_title": "测试报告的二分类问题性能分数的一致性",
    "translated_abstract": "二分类是机器学习中的一个基本任务，应用范围涵盖各个科学领域。科学家在进行基础研究或优化实际应用时，通常会根据准确率、敏感度和特异度等性能指标评估和排名分类技术。然而，报告的性能得分并不总是可靠的研究排名依据。这可能归因于未公开或非常规的交叉验证实践、排版错误和其他因素。在给定的实验设置中，具有特定数量的阳性和阴性测试项，大多数性能得分可以假设的特定、相互相关的值。在本文中，我们引入了数值技术来评估报告的性能得分的一致性和假设的实验设置。重要的是，所提出的方法不依赖于统计推断，而是使用数值方法来识别不一致的情况。",
    "tldr": "这篇论文介绍了一种测试报告的二分类问题性能分数和实验设置一致性的数值方法，该方法不依赖于统计推断。",
    "en_tdlr": "This paper introduces a numerical method to test the consistency of reported performance scores and experimental setup for binary classification problems, without relying on statistical inference."
}