{
    "title": "Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])",
    "abstract": "Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee",
    "link": "http://arxiv.org/abs/2310.06131",
    "context": "Title: Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])\nAbstract: Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee",
    "path": "papers/23/10/2310.06131.json",
    "total_tokens": 836,
    "translated_title": "使用梯度自动学习层间等变性",
    "translated_abstract": "卷积将等变性对称性编码到神经网络中，从而提高了泛化性能。然而，对称性提供了网络可以表示的函数的固定硬约束，需要事先指定，并且不能适应改变。我们的目标是允许灵活的对称性约束，可以通过梯度自动地从数据中学习。从头开始学习对称性和相关的权重连接结构有两个困难。首先，它需要有效灵活的层间等变性参数化。其次，对称性作为约束，因此不会被训练损失函数鼓励。为了克服这些挑战，我们改进了软等变性的参数化，并通过优化边缘似然来学习层间等变性的数量，其中边缘似然是使用可微分的拉普拉斯逼近估计的。这个目标平衡了数据拟合和模型复杂性，使层间对称性在深度学习中被发现。",
    "tldr": "该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。",
    "en_tdlr": "This paper proposes a method to automatically learn layer-wise equivariances using gradients, by improving parameterizations of soft equivariance and optimizing marginal likelihood."
}