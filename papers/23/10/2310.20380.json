{
    "title": "Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])",
    "abstract": "Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 ",
    "link": "http://arxiv.org/abs/2310.20380",
    "context": "Title: Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])\nAbstract: Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 ",
    "path": "papers/23/10/2310.20380.json",
    "total_tokens": 907,
    "translated_title": "强化学习中的Dropout策略：限制策略优化方法中替代目标方差的增长",
    "translated_abstract": "基于策略的强化学习算法在各个领域中被广泛应用。其中，主流的策略优化算法如PPO和TRPO引入了重要性采样到强化学习中，这允许重用历史数据。然而，这也导致了替代目标方差的增加，间接影响了算法的稳定性和收敛性。本文首先推导出了替代目标方差的上界，它可以随替代目标的增加而呈二次增长。接下来，我们提出了一种Dropout技术，以避免重要性采样引起的替代目标方差过度增加。然后，我们引入了一个通用的强化学习框架，适用于主流的策略优化方法，并将Dropout技术应用于PPO算法，得到了D-PPO变体。最后，我们在Atari 2600游戏上对D-PPO和PPO算法进行了比较实验。",
    "tldr": "本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。",
    "en_tdlr": "This paper proposes a Dropout technique to limit the growth of surrogate objective variance in policy optimization methods, and applies it to the PPO algorithm. Experimental results demonstrate that the D-PPO algorithm outperforms the PPO algorithm in Atari 2600 games."
}