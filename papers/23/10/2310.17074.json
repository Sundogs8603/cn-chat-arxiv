{
    "title": "Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates. (arXiv:2310.17074v1 [cs.LG])",
    "abstract": "In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) algorithm with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by the large learning rate SGD training turns out to be beneficial to the generalization of the NN, which potentially improves over the same NN trained by SGD with small learning rates that converges more smoothly. In view of this finding, we call such a phenomenon \"benign oscillation\". Our theory towards demystifying such a phenomenon builds upon the feature learning perspective of deep learning. Specifically, we consider a feature-noise data generation model that consists of (i) weak features which have a small $\\ell_2$-norm and appear in each data point; (ii) strong features which have a larger $\\ell_2$-norm but only appear in a certain fraction of all data points; and (iii) noise. We prove that NNs trained by oscill",
    "link": "http://arxiv.org/abs/2310.17074",
    "context": "Title: Benign Oscillation of Stochastic Gradient Descent with Large Learning Rates. (arXiv:2310.17074v1 [cs.LG])\nAbstract: In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) algorithm with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by the large learning rate SGD training turns out to be beneficial to the generalization of the NN, which potentially improves over the same NN trained by SGD with small learning rates that converges more smoothly. In view of this finding, we call such a phenomenon \"benign oscillation\". Our theory towards demystifying such a phenomenon builds upon the feature learning perspective of deep learning. Specifically, we consider a feature-noise data generation model that consists of (i) weak features which have a small $\\ell_2$-norm and appear in each data point; (ii) strong features which have a larger $\\ell_2$-norm but only appear in a certain fraction of all data points; and (iii) noise. We prove that NNs trained by oscill",
    "path": "papers/23/10/2310.17074.json",
    "total_tokens": 941,
    "translated_title": "带有大学习率的随机梯度下降的良性振荡",
    "translated_abstract": "在这项工作中，我们在理论上研究了使用大学习率训练的随机梯度下降（SGD）算法训练的神经网络（NN）的泛化特性。在这种训练方式下，我们的发现是，由于大学习率SGD训练引起的NN权重的振荡对NN的泛化有益，这有可能优于通过收敛较平滑的小学习率SGD训练的相同NN。基于这个发现，我们将这种现象称为“良性振荡”。我们解密这种现象的理论建立在深度学习的特征学习角度上。具体来说，我们考虑一个特征噪声数据生成模型，它包括（i）具有小的$\\ell_2$-范数并出现在每个数据点中的弱特征；（ii）具有较大的$\\ell_2$-范数但只出现在所有数据点的一部分中的强特征；和（iii）噪声。我们证明了通过振荡训练的NN能够在这个特征噪声数据生成模型上实现较好的泛化性能。",
    "tldr": "通过大学习率的随机梯度下降训练的神经网络，由于其权重的振荡，能够在特征噪声数据上实现良好的泛化性能。",
    "en_tdlr": "Neural networks trained by stochastic gradient descent with large learning rates exhibit beneficial oscillation of weights, resulting in improved generalization on feature-noise data."
}