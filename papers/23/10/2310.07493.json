{
    "title": "Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer. (arXiv:2310.07493v1 [cs.AI])",
    "abstract": "Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.",
    "link": "http://arxiv.org/abs/2310.07493",
    "context": "Title: Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer. (arXiv:2310.07493v1 [cs.AI])\nAbstract: Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.",
    "path": "papers/23/10/2310.07493.json",
    "total_tokens": 898,
    "translated_title": "多样性对于应变的重要性：学习多样行为以实现高效适应和迁移",
    "translated_abstract": "发现给定任务的所有有用解决方案对于可迁移的强化学习代理至关重要，以应对任务或转换动力学的变化。传统的强化学习算法只关注在当前任务和动力学下找到最优策略，而不考虑这一点。我们提出了一种简单的方法来发现给定任务的所有可能解决方案，以获得在迁移设置中表现良好并快速适应任务或转换动力学变化的代理。我们的方法通过迭代学习一组策略，其中每个后续策略被限制为在所有先前策略下都不太可能存在的解决方案。与先前方法不同，我们的方法不需要学习额外的模型进行新颖性检测，并通过直接将约束整合到动作选择和优化步骤中避免任务和新颖性奖励信号的平衡。",
    "tldr": "本研究提出一种简单的方法，在强化学习中通过学习多样的行为来实现适应和迁移。通过迭代学习一组策略，并利用约束来发现给定任务的所有可能解决方案，我们的方法能够在迁移设置中表现良好，快速适应任务或转换动力学的变化。",
    "en_tdlr": "This paper proposes a simple method for efficient adaptation and transfer in reinforcement learning by learning diverse behaviors. By iteratively learning a set of policies and using constraints to discover all possible solutions for a given task, our method performs well in transfer settings and quickly adapts to changes in the task or transition dynamics."
}