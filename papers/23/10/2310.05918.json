{
    "title": "Grokking as Compression: A Nonlinear Complexity Perspective. (arXiv:2310.05918v1 [cs.LG])",
    "abstract": "We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the $L_2$ norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while $L_2$ cannot. (2) In the compression phase, LMN has linear relations with test losses, while $L_2$ is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while $L_2$ does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly co",
    "link": "http://arxiv.org/abs/2310.05918",
    "context": "Title: Grokking as Compression: A Nonlinear Complexity Perspective. (arXiv:2310.05918v1 [cs.LG])\nAbstract: We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the $L_2$ norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while $L_2$ cannot. (2) In the compression phase, LMN has linear relations with test losses, while $L_2$ is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while $L_2$ does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly co",
    "path": "papers/23/10/2310.05918.json",
    "total_tokens": 974,
    "translated_title": "Grokking作为压缩：一种非线性复杂性的视角",
    "translated_abstract": "我们将迟缓通用化（generalization）的现象“grokking”归因于压缩。为了这样做，我们定义了线性映射数（LMN）来测量网络复杂度，这是ReLU网络线性区域数的广义版本。LMN可以很好地表征网络在通用化之前的压缩过程。虽然$L_2$范数一直是描述模型复杂度的常用选择，但我们提出了几个理由支持使用LMN：（1）LMN能够自然地解释为信息/计算，而$L_2$则不能。（2）在压缩阶段，LMN与测试误差具有线性关系，而$L_2$则以一种复杂的非线性方式与测试误差相关。（3）LMN还揭示了XOR网络在两种通用化解决方案之间切换的有趣现象，而$L_2$则没有。除了解释“grokking”现象外，我们还认为LMN是Kolmogorov复杂度的神经网络版本的一个有前途的候选者，因为它显式地考虑了计算量。",
    "tldr": "本研究将\"grokking\"现象归因于压缩，并提出线性映射数（LMN）作为衡量神经网络复杂度的方法。LMN能够更好地描述网络的压缩过程，并展现XOR网络在通用化解决方案间切换的有趣现象。",
    "en_tdlr": "This paper attributes the phenomenon of \"grokking\" to compression and introduces the linear mapping number (LMN) as a measure of neural network complexity. LMN can better characterize the compression process and reveals the intriguing phenomenon of XOR networks switching between generalization solutions."
}