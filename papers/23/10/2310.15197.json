{
    "title": "Can strong structural encoding reduce the importance of Message Passing?. (arXiv:2310.15197v1 [cs.LG])",
    "abstract": "The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our res",
    "link": "http://arxiv.org/abs/2310.15197",
    "context": "Title: Can strong structural encoding reduce the importance of Message Passing?. (arXiv:2310.15197v1 [cs.LG])\nAbstract: The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our res",
    "path": "papers/23/10/2310.15197.json",
    "total_tokens": 878,
    "translated_title": "强结构编码是否能减少消息传递的重要性？",
    "translated_abstract": "在图上操作的最常见的神经网络类别是消息传递神经网络(MPNNs)，其中节点的表示通过在1-hop邻域中聚合信息进行迭代更新。由于这种计算节点嵌入的范式可能会阻止模型学习粗糙的拓扑结构，因此初始特征通常会通过图的结构信息进行增强，通常以拉普拉斯特征向量或随机游走转移概率的形式提供。在这项工作中，我们探索了在提供强结构编码时消息传递的贡献。我们提出了一种新颖的建模特征和结构信息之间相互作用的方法，基于它们的张量乘积而不是标准的连接。我们比较了在常见情景和消息传递层容量严重降低甚至最终被完全移除的设置下选择的相互作用方式。我们res",
    "tldr": "本研究探索了提供强结构编码时消息传递的贡献，通过引入特征和结构信息之间的张量乘积交互方式，比较了不同情景下的选择，甚至在容量受限的情况下完全移除了消息传递阶段。",
    "en_tdlr": "This study explores the contribution of message passing when strong structural encodings are provided. By introducing a novel tensor product interaction between feature and structural information, the choice is compared in different scenarios and even the message-passing phase is completely removed in the case of severe capacity reduction."
}