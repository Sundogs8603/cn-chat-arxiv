{
    "title": "Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. (arXiv:2310.18935v1 [cs.LG])",
    "abstract": "The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to $1$, whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Addit",
    "link": "http://arxiv.org/abs/2310.18935",
    "context": "Title: Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. (arXiv:2310.18935v1 [cs.LG])\nAbstract: The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to $1$, whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Addit",
    "path": "papers/23/10/2310.18935.json",
    "total_tokens": 998,
    "translated_title": "对于几乎正交数据的两层ReLU和Leaky ReLU网络，梯度下降的隐式偏差",
    "translated_abstract": "由于其对有利特性解的隐式偏好，基于梯度优化训练的神经网络能够很好地泛化。虽然梯度流的隐式偏差已经被广泛研究了均匀神经网络（包括ReLU和Leaky ReLU网络），但对于梯度下降的隐式偏差目前只了解了平滑神经网络。因此，对于通过梯度下降训练的非平滑神经网络的隐式偏差仍然是一个开放问题。本文通过研究梯度下降在训练两层全连接(Leaky) ReLU神经网络时的隐式偏差来回答这个问题。我们证明了当训练数据几乎正交时，对于Leaky ReLU激活函数，梯度下降将找到一个收敛到1的稳定秩网络，而对于ReLU激活函数，梯度下降将找到一个稳定秩上界为常数的神经网络。",
    "tldr": "本文研究了两层ReLU和Leaky ReLU网络在几乎正交数据上梯度下降的隐式偏差。对于Leaky ReLU激活函数，梯度下降能找到收敛到1的稳定秩网络；对于ReLU激活函数，梯度下降能找到稳定秩上界为常数的神经网络。"
}