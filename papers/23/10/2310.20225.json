{
    "title": "VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision. (arXiv:2310.20225v1 [cs.CV])",
    "abstract": "People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive desc",
    "link": "http://arxiv.org/abs/2310.20225",
    "context": "Title: VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision. (arXiv:2310.20225v1 [cs.CV])\nAbstract: People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive desc",
    "path": "papers/23/10/2310.20225.json",
    "total_tokens": 902,
    "translated_title": "VisPercep:一种通过视觉语言方法增强盲人和视力低下人群的视觉感知能力。",
    "translated_abstract": "盲人和视力低下人群在对陌生环境下的场景识别和精确的目标识别方面面临很大挑战。此外，由于视力丧失，盲人和视力低下人群很难自己访问和识别潜在的绊倒危险。本文提出了一种创新方法，利用大型视觉语言模型增强盲人和视力低下人群的视觉感知能力，提供周围环境的详细全面描述，并提供潜在风险的警告。我们的方法首先利用大型图像标记模型（即Recognize Anything (RAM)）识别捕获图像中的所有常见物体。然后将识别结果和用户查询整合成一个特定于盲人和视力低下人群的提示，并利用提示工程技术进行定制。通过将提示和输入图像相结合，一个大型视觉语言模型（即InstructBLIP）生成详细全面的描述。",
    "tldr": "本文提出了一种通过视觉语言方法增强盲人和视力低下人群的视觉感知能力的创新方法，能够提供周围环境的详细全面描述并提供潜在风险的警告。",
    "en_tdlr": "This paper presents a pioneering approach that enhances visual perception for people with blindness and low vision using a vision-language model. It provides detailed and comprehensive descriptions of the surrounding environments and warns about potential risks."
}