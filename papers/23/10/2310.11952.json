{
    "title": "Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])",
    "abstract": "In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.",
    "link": "http://arxiv.org/abs/2310.11952",
    "context": "Title: Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])\nAbstract: In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.",
    "path": "papers/23/10/2310.11952.json",
    "total_tokens": 853,
    "translated_title": "将连续学习重塑为序列建模",
    "translated_abstract": "在这项工作中，我们旨在建立机器学习研究中两个重要领域之间的强连接：连续学习和序列建模。也就是说，我们提出将连续学习作为序列建模问题进行表述，从而可以利用先进的序列模型进行连续学习。在这个框架下，连续学习过程成为序列模型的前向传播。通过采用元连续学习(MCL)框架，我们可以在多个连续学习episode上对序列模型进行元级训练。作为我们新框架的一个具体示例，我们展示了将Transformer及其高效变体应用于MCL方法。我们在包括分类和回归的七个基准测试上的实验证明了序列模型可以成为一种吸引人的通用连续学习解决方案。",
    "tldr": "本文将连续学习重塑为序列建模问题，并提出了利用序列模型进行连续学习的方法。通过采用元连续学习框架，需要对序列模型进行多次连续学习episode上的元级训练。实验证明序列模型可以成为一种吸引人的通用连续学习解决方案。",
    "en_tdlr": "This paper recasts continual learning as a sequence modeling problem and proposes using sequence models for continual learning. By adopting the meta-continual learning framework, the sequence model is trained at the meta-level on multiple continual learning episodes. Experiments show that sequence models can be an attractive solution for general continual learning."
}