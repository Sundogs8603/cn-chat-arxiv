{
    "title": "On Bilingual Lexicon Induction with Large Language Models",
    "abstract": "arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang",
    "link": "https://arxiv.org/abs/2310.13995",
    "context": "Title: On Bilingual Lexicon Induction with Large Language Models\nAbstract: arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang",
    "path": "papers/23/10/2310.13995.json",
    "total_tokens": 947,
    "translated_title": "关于利用大型语言模型进行双语词汇识别",
    "translated_abstract": "双语词汇识别（BLI）是多语言自然语言处理中的核心任务，目前在很大程度上仍然依赖于计算跨语言单词表示。受自然语言处理领域向大型语言模型（LLMs）的全球范式转变的启发，我们探讨了最新一代LLMs在双语词汇开发中的潜力。我们提出了以下研究问题：是否可能促使和微调多语言LLMs（mLLMs）以进行BLI，并且这种方法与当前BLI方法相比如何以及如何补充？为此，我们系统地研究了1）用于无监督BLI的零次提示和2）使用一组种子翻译对进行少量上下文提示，均无需进行任何LLM微调，以及3）对较小LLMs进行标准BLI导向微调。我们在涵盖不同大小（从0.3B到13B参数）的18个开源文本对文本mLLMs上进行实验，涵盖两个标准BLI基准测试。",
    "tldr": "本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。",
    "en_tdlr": "This paper explores the potential of using large language models for bilingual lexicon induction, comparing and complementing current BLI methods through the study of zero-shot prompting, few-shot in-context prompting, and standard BLI-oriented fine-tuning approaches."
}