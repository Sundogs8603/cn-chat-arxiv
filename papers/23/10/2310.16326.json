{
    "title": "Reinforcement Learning for SBM Graphon Games with Re-Sampling. (arXiv:2310.16326v1 [cs.GT])",
    "abstract": "The Mean-Field approximation is a tractable approach for studying large population dynamics. However, its assumption on homogeneity and universal connections among all agents limits its applicability in many real-world scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been introduced in the literature to address these limitations. When the underlying Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block model is unknown, we propose a re-sampling scheme from a graphon integrated with the finite N-player MP-MFG model. We develop a novel learning framework based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the complex network structures of agents' connections. We analyze GGR-S dynamics and establish the convergence to dynamics of MP-MFG. Leveraging this result, we propose an efficient sample-based N-player Reinforcement Learning algorithm for GGR-S wit",
    "link": "http://arxiv.org/abs/2310.16326",
    "context": "Title: Reinforcement Learning for SBM Graphon Games with Re-Sampling. (arXiv:2310.16326v1 [cs.GT])\nAbstract: The Mean-Field approximation is a tractable approach for studying large population dynamics. However, its assumption on homogeneity and universal connections among all agents limits its applicability in many real-world scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been introduced in the literature to address these limitations. When the underlying Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block model is unknown, we propose a re-sampling scheme from a graphon integrated with the finite N-player MP-MFG model. We develop a novel learning framework based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the complex network structures of agents' connections. We analyze GGR-S dynamics and establish the convergence to dynamics of MP-MFG. Leveraging this result, we propose an efficient sample-based N-player Reinforcement Learning algorithm for GGR-S wit",
    "path": "papers/23/10/2310.16326.json",
    "total_tokens": 957,
    "translated_title": "基于再采样的SBM图动态的强化学习",
    "translated_abstract": "均场近似是研究大规模人口动态的一种可行方法。然而，它对同质性和所有代理之间的通用连接的假设限制了其在许多实际场景中的适用性。为了解决这些限制，文献中引入了多种群均场博弈模型。当已知基本的随机块模型时，我们证明了一个策略镜像上升算法找到了多种群均场博弈的纳什均衡。在更现实的情况下，即随机块模型未知的情况下，我们提出了从图动态集成与有限的N-player多群均场博弈模型的再采样方案。我们基于图动态与再采样的Game (GGR-S)模型开发了一种新颖的学习框架，该模型捕捉了代理之间的复杂网络结构。我们分析了GGR-S动态并建立了与多种群均场博弈动态的收敛性。利用这个结果，我们提出了一种高效的基于样本的N-player强化学习算法用于GGR-S。",
    "tldr": "本文研究了基于再采样的SBM图动态的强化学习问题，并提出了一种新的学习框架。我们证明了策略镜像上升算法能够找到多种群均场博弈的纳什均衡，并提出了一种高效的样本集成的强化学习算法来解决实际情况下随机块模型未知的情况。"
}