{
    "title": "Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent. (arXiv:2310.18455v1 [cs.LG])",
    "abstract": "A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in practical settings, and the heaviness of the tails might correlate with the overall performance. In this paper, we investigate the emergence of such heavy tails. Previous works on this problem only considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence, the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where the amount of training data is finite, is still not well-understood. Our contribution aims to fill this gap. In particular, we show that the stationary distribution of offline (also called multi-pass) SGD exhibits 'approximate' power-law tails and the approximation error is controlled by how fast the empirical distribution of the training data converges to the true underlying data distribution in the",
    "link": "http://arxiv.org/abs/2310.18455",
    "context": "Title: Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent. (arXiv:2310.18455v1 [cs.LG])\nAbstract: A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in practical settings, and the heaviness of the tails might correlate with the overall performance. In this paper, we investigate the emergence of such heavy tails. Previous works on this problem only considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence, the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where the amount of training data is finite, is still not well-understood. Our contribution aims to fill this gap. In particular, we show that the stationary distribution of offline (also called multi-pass) SGD exhibits 'approximate' power-law tails and the approximation error is controlled by how fast the empirical distribution of the training data converges to the true underlying data distribution in the",
    "path": "papers/23/10/2310.18455.json",
    "total_tokens": 892,
    "translated_title": "离线（多遍）随机梯度下降中的近似重尾行为",
    "translated_abstract": "最近的一系列实证研究表明，在实际情况下，SGD可能表现出重尾行为，尾部的重度可能与整体性能相关。本文研究了这种重尾行为的出现。之前的工作只考虑了在线（也称为单遍）SGD，在理论发现中，重尾现象的出现取决于对无限量数据的访问。因此，在训练数据量有限的实际情况下，产生报告的重尾行为的机制仍不清楚。我们的贡献旨在填补这一空白。特别地，我们展示离线（也称为多遍）SGD的稳态分布表现出“近似”幂律尾部，而近似误差由训练数据的经验分布如何快速收敛于真实的基本数据分布所控制。",
    "tldr": "本文研究了离线（多遍）随机梯度下降中的重尾行为。通过展示离线SGD的稳态分布表现出“近似”幂律尾部，我们填补了在有限的训练数据量情况下重尾行为机制的空白。",
    "en_tdlr": "This paper investigates the heavy-tailed behavior in offline (multi-pass) stochastic gradient descent. By showing that the stationary distribution of offline SGD exhibits 'approximate' power-law tails, we fill the gap in understanding the mechanism behind heavy tails in situations with a finite amount of training data."
}