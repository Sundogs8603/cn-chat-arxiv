{
    "title": "Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])",
    "abstract": "Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable ",
    "link": "http://arxiv.org/abs/2310.12403",
    "context": "Title: Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])\nAbstract: Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable ",
    "path": "papers/23/10/2310.12403.json",
    "total_tokens": 886,
    "translated_title": "图神经网络中的协作小批次",
    "translated_abstract": "在大规模训练图神经网络（GNN）时需要大量的计算资源，这个过程非常密集。减少资源需求的最有效方法之一是将小批量训练与图采样相结合。GNN具有一个独特的特性，即小批量中的项具有重叠的数据。然而，常用的独立小批量方法将每个处理单元（PE）分配给自己的小批量进行处理，导致重复计算和跨PE的输入数据访问。这放大了邻域爆炸现象（NEP），这是限制扩展性的主要瓶颈。为了减少多PE环境中NEP的影响，我们提出了一种新的方法，称为协作小批处理。我们的方法利用采样子图的大小是批处理大小的凹函数这一特性，可以明显减少每个种子顶点的工作量，同时增加批处理大小。因此，这是一种有利的方法。",
    "tldr": "本文提出了一种协作小批处理的方法来解决图神经网络中的邻域爆炸现象（NEP），该方法通过利用采样子图的大小与批处理大小的关系来减少每个种子顶点的工作量。",
    "en_tdlr": "This paper proposes a cooperative minibatching approach to address the neighborhood explosion phenomenon (NEP) in graph neural networks (GNNs), which reduces the workload per seed vertex by leveraging the relationship between the size of the sampled subgraph and the batch size."
}