{
    "title": "Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])",
    "abstract": "Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization. In this work, we investigate for a generic controllable approach for multi-document summarization that leverages the capabilities of LLMs to refine the text. In particular, we train a controllable content extraction scheme to extract the text that will be refined by an LLM. The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM. Our approach yields competitive results in the evaluation using ROUGE metrics and outperforms potential baselines in coherence, as per human evaluation.",
    "link": "http://arxiv.org/abs/2310.03473",
    "context": "Title: Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])\nAbstract: Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization. In this work, we investigate for a generic controllable approach for multi-document summarization that leverages the capabilities of LLMs to refine the text. In particular, we train a controllable content extraction scheme to extract the text that will be refined by an LLM. The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM. Our approach yields competitive results in the evaluation using ROUGE metrics and outperforms potential baselines in coherence, as per human evaluation.",
    "path": "papers/23/10/2310.03473.json",
    "total_tokens": 893,
    "translated_title": "可控的多文档摘要：基于大型语言模型的覆盖和连贯性直观策略",
    "translated_abstract": "高效的大型语言模型在提高文本可读性方面表现出色。然而，对于长输入的文本生成任务（如多文档摘要），可控性是一个关注的问题。在这项工作中，我们研究了一种通用的可控方法，用于利用大型语言模型改进文本的多文档摘要。具体而言，我们训练了一种可控的内容提取方案，它提取将由大型语言模型改进的文本。该方案采用了一种新颖的覆盖和连贯性直观策略，并由一个被动训练的大型语言模型进行奖励。我们的方法在使用ROUGE评估指标进行评估时产生了有竞争力的结果，并在连贯性方面优于潜在的基线，根据人类评估。",
    "tldr": "本论文研究了一种利用大型语言模型改进的通用可控方法，用于多文档摘要。通过训练可控的内容提取方案并利用覆盖和连贯性直观策略，该方法相对于潜在的基线在连贯性上表现出色，并在使用ROUGE评估指标进行评估时取得有竞争力的结果。",
    "en_tdlr": "This paper investigates a generic controllable approach for multi-document summarization, leveraging large language models to improve coherence. By training a controllable content extraction scheme and utilizing a coverage and coherence intuitive policy, the approach outperforms potential baselines in coherence and achieves competitive results in evaluation using ROUGE metrics."
}