{
    "title": "What Algorithms can Transformers Learn? A Study in Length Generalization. (arXiv:2310.16028v1 [cs.LG])",
    "abstract": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically im",
    "link": "http://arxiv.org/abs/2310.16028",
    "context": "Title: What Algorithms can Transformers Learn? A Study in Length Generalization. (arXiv:2310.16028v1 [cs.LG])\nAbstract: Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically im",
    "path": "papers/23/10/2310.16028.json",
    "total_tokens": 872,
    "translated_title": "Transformers可以学习哪些算法？对长度泛化的研究。",
    "translated_abstract": "大型语言模型展现出令人惊讶的紧急泛化特性，但在许多简单的推理任务上如算术和奇偶判断上却很难。这引发了一个问题，即Transformer模型是否可以学习解决任务的真正算法。我们研究了Transformer模型在算法任务的长度泛化方面的能力。在这里，我们提出了一个统一的框架，来理解Transformer在给定任务上如何展现出强大的长度泛化能力。具体而言，我们利用RASP（Weiss等人，2021）- 用于Transformer的计算模型的编程语言，并引入了RASP泛化猜想：如果任务可以由适用于所有输入长度的短RASP程序解决，那么Transformer倾向于在该任务上进行长度泛化。这个简单的猜想显著捕捉了大多数已知的算法任务的长度泛化实例。此外，我们利用我们的洞察力，大幅提高了长度泛化任务的性能。",
    "tldr": "该研究通过使用RASP编程语言和RASP泛化猜想，对Transformer模型在算法任务的长度泛化能力进行研究，并可显著提高性能。",
    "en_tdlr": "This study investigates the length generalization abilities of Transformer models on algorithmic tasks using the RASP programming language and the RASP-Generalization Conjecture, resulting in improved performance."
}