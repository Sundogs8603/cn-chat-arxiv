{
    "title": "Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)",
    "abstract": "Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi",
    "link": "http://arxiv.org/abs/2310.05028",
    "context": "Title: Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)\nAbstract: Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi",
    "path": "papers/23/10/2310.05028.json",
    "total_tokens": 927,
    "translated_title": "重新审视大型语言模型作为零-shot关系抽取器",
    "translated_abstract": "关系抽取(RE)即使在零-shot设定下，一直涉及一定程度的标记或未标记的数据。最近的研究表明，大型语言模型(LLMs)能够在给定自然语言提示的情况下，无需任何数据和参数调整，自动适应新任务，这为从文本中提取关系提供了可能性。本研究主要关注将LLMs，如ChatGPT，作为零-shot关系抽取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试将最近的提示技术，如CoT，纳入其中以提高零-shot关系抽取。我们提出了总结和提问(\\textsc{SumAsk})提示，这是一种简单的提示，通过递归使用LLMs将RE输入转换为有效的问答(QA)格式。另一方面，我们对各种基准和设置进行了全面的实验，以调查LLMs在零-shot关系抽取上的能力。具体而言，我们有以下的followi",
    "tldr": "本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。",
    "en_tdlr": "This study revisits the potential of large language models (LLMs) as zero-shot relation extractors and proposes the use of summarize-and-ask (\\textsc{SumAsk}) prompting method to improve zero-shot relation extraction. Experiments demonstrate that LLMs perform well on this task."
}