{
    "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
    "abstract": "arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr",
    "link": "https://arxiv.org/abs/2310.08559",
    "context": "Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement\nAbstract: arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr",
    "path": "papers/23/10/2310.08559.json",
    "total_tokens": 762,
    "translated_title": "令人惊叹但令人困惑：用假设细化测试语言模型的归纳推理能力",
    "translated_abstract": "从少数观察中推导出潜在原则，然后推广到新情况的能力，即归纳推理，对于人类智能至关重要。之前的研究表明，尽管在研究基准上取得了令人印象深刻的成功，但语言模型（LMs）在归纳推理方面常常表现不佳。在这项工作中，我们通过迭代假设细化这一技术对LMs的归纳推理能力进行了系统研究，该技术更接近人类归纳过程，而不是标准的输入-输出提示。",
    "tldr": "对语言模型进行的系统研究揭示了它们在假设提出方面表现惊人，并且通过与一个（任务特定的）符号解释器相结合，能够系统地过滤可能性。",
    "en_tdlr": "A systematic study of language models reveals their phenomenal performance in hypothesis proposing, which when combined with a task-specific symbolic interpreter, can systematically filter possibilities."
}