{
    "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])",
    "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attack",
    "link": "http://arxiv.org/abs/2310.03185",
    "context": "Title: Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])\nAbstract: Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attack",
    "path": "papers/23/10/2310.03185.json",
    "total_tokens": 1034,
    "translated_title": "大型语言模型中利用视觉对抗性样本误用工具",
    "translated_abstract": "大型语言模型（LLMs）正在增强其使用工具和处理多种模态的能力。这些新能力带来了新的好处，也带来了新的安全风险。本研究中，我们展示了攻击者可以使用视觉对抗性样本来引导模型执行攻击者想要的工具使用。例如，攻击者可以让受害者的LLM删除日历事件，泄露私人对话并预订酒店。与先前的工作不同，我们的攻击可以同时影响与LLM连接的用户资源的机密性和完整性，同时具有隐蔽性和适用于多个输入提示的特点。我们使用基于梯度的对抗训练构建这些攻击，并在多个维度上对其性能进行了表征。我们发现，我们的对抗图像几乎总是（约98%）可以操纵LLM以遵循真实世界的语法来调用工具，同时保持与干净图像的高相似性（约0.9 SSIM）。此外，使用人工评分和自动化指标，我们发现攻击会降低LLM的自然性能和用户满意度。",
    "tldr": "本研究展示了如何利用视觉对抗性样本来引导大型语言模型（LLMs）执行攻击者想要的工具使用。攻击可以导致LLM删除日历事件、泄露私人对话和预订酒店，并且具有隐蔽性和适用性广的特点。同时，这些攻击几乎总是可以操纵LLM以遵循真实世界的语法来调用工具，同时保持与干净图像的高相似性。",
    "en_tdlr": "This study demonstrates how visual adversarial examples can be used to manipulate large language models (LLMs) into performing desired tool usage by causing them to delete calendar events, leak private conversations, and book hotels. These attacks are stealthy, generalizable, and can mimic clean images with high similarity."
}