{
    "title": "Learning to Act from Actionless Videos through Dense Correspondences. (arXiv:2310.08576v1 [cs.RO])",
    "abstract": "In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling ",
    "link": "http://arxiv.org/abs/2310.08576",
    "context": "Title: Learning to Act from Actionless Videos through Dense Correspondences. (arXiv:2310.08576v1 [cs.RO])\nAbstract: In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling ",
    "path": "papers/23/10/2310.08576.json",
    "total_tokens": 833,
    "translated_title": "通过密集对应关系从无动作视频中学习执行动作",
    "translated_abstract": "本研究提出了一种方法，通过少量视频演示构建一个基于视频的机器人策略，能够可靠地在不同机器人和环境下执行各种任务，而无需使用任何动作注释。我们的方法利用图像作为任务不可知的表示，编码状态和动作信息，并使用文本作为指定机器人目标的通用表示。通过合成“如幻般”执行动作的视频，并结合帧之间的密集对应关系，我们的方法可以推断出在环境中执行的闭合形式动作，而不需要任何显式的动作标签。这种独特的能力使我们能够仅基于RGB视频训练策略，并将学习到的策略部署到各种机器人任务中。我们在桌面操作和导航任务中展示了我们方法的有效性。此外，我们还贡献了一个高效的视频建模开源框架。",
    "tldr": "本研究提出了一种通过密集对应关系从无动作视频中学习执行动作的方法，能够在不使用任何动作注释的情况下构建可靠执行多样任务的基于视频的机器人策略。"
}