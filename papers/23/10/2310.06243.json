{
    "title": "Sample-Efficient Multi-Agent RL: An Optimization Perspective. (arXiv:2310.06243v1 [cs.LG])",
    "abstract": "We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling p",
    "link": "http://arxiv.org/abs/2310.06243",
    "context": "Title: Sample-Efficient Multi-Agent RL: An Optimization Perspective. (arXiv:2310.06243v1 [cs.LG])\nAbstract: We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling p",
    "path": "papers/23/10/2310.06243.json",
    "total_tokens": 958,
    "translated_title": "高样本效率的多智能体强化学习：优化视角",
    "translated_abstract": "我们研究了在一般函数逼近下，针对一般和的马尔可夫博弈问题（Markov Games，MGs）的多智能体强化学习（MARL）。为了找到样本效率学习的最小假设，我们引入了一个称为多智能体解耦系数（MADC）的新的复杂度度量方法。利用这个度量方法，我们提出了一个统一的算法框架，确保在低MADC条件下学习纳什均衡、粗粒度相关均衡和相关均衡的模型基于和无模型MARL问题。我们还证明了我们的算法提供了与现有工作相媲美的亚线性后悔。此外，我们的算法将均衡求解预测器与一个求解每个确定性联合策略的正则化收益的内置优化子过程相结合，避免了在数据相关约束条件下求解约束优化问题或执行采样的问题。",
    "tldr": "该论文研究了多智能体强化学习中的高样本效率问题，在一般函数逼近下，提出了一个基于多智能体解耦系数的算法框架，实现了低复杂度下的学习纳什均衡、粗粒度相关均衡和相关均衡。该算法在亚线性后悔方面表现具有可比性。"
}