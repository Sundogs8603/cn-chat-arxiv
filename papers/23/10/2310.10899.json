{
    "title": "Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])",
    "abstract": "Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a",
    "link": "http://arxiv.org/abs/2310.10899",
    "context": "Title: Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])\nAbstract: Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a",
    "path": "papers/23/10/2310.10899.json",
    "total_tokens": 835,
    "translated_title": "通过子网络注入归纳偏置",
    "translated_abstract": "尽管人工神经网络在各种任务上取得了最近的成功，但对于这些模型的精确解决方案，我们几乎没有知识或控制能力。注入归纳偏置--对一些解决方案偏好--是理解和控制这些模型行为的一个有前景的途径。已经进行了大量工作来研究模型固有的归纳偏置，并通过手动设计的结构或精心策划的训练方式注入不同的归纳偏置。在这项工作中，我们探索了一种更机械的方法：子任务归纳。我们的方法发现了一个在训练模型中实现特定子任务的功能子网络，并使用它来注入对利用该子任务的解决方案的归纳偏置。子任务归纳灵活高效，在两个实验中我们证明了它的有效性。",
    "tldr": "通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。",
    "en_tdlr": "This research explores a method to understand and control the behavior of neural networks by injecting inductive biases through discovered functional subnetworks, leading to a significant reduction in the required amount of training data."
}