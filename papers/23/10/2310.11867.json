{
    "title": "Evaluating the Fairness of Discriminative Foundation Models in Computer Vision. (arXiv:2310.11867v1 [cs.CV])",
    "abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes ove",
    "link": "http://arxiv.org/abs/2310.11867",
    "context": "Title: Evaluating the Fairness of Discriminative Foundation Models in Computer Vision. (arXiv:2310.11867v1 [cs.CV])\nAbstract: We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes ove",
    "path": "papers/23/10/2310.11867.json",
    "total_tokens": 974,
    "translated_title": "评估计算机视觉中判别性基础模型的公平性",
    "translated_abstract": "我们提出了一种新的用于评估判别性基础模型（如Contrastive Language-Pretraining (CLIP)）偏见的分类法，该分类法用于标记任务。然后，我们根据我们的分类法系统地评估了现有减少这些模型偏见的方法。具体而言，我们评估了OpenAI的CLIP和OpenCLIP模型在零样本分类、图像检索和图像字幕等关键应用中的性能。我们根据三个维度对期望的行为进行了分类：（i）任务是否涉及人类；（ii）任务的主观性程度（即，来自不同背景的人们是否会对标记达成一致）；（iii）任务的预期目的，公平性是通过公正（即，独立于受保护属性进行决策）还是表达（即，通过最大化多样性进行决策）更好地实现。最后，我们对二值和多值受保护属性进行了定量的公平性评估。",
    "tldr": "评估了计算机视觉中判别性基础模型的公平性，并提出了用于评估偏见的分类法。通过系统性地评估现有的减少模型偏见的方法，揭示了模型在关键应用中的性能。根据任务涉及人类程度、主观性程度和预期目的对期望的行为进行了分类，并对受保护属性进行了定量的公平性评估。"
}