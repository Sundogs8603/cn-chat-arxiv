{
    "title": "Efficient kernel surrogates for neural network-based regression. (arXiv:2310.18612v2 [cs.LG] UPDATED)",
    "abstract": "Despite their immense promise in performing a variety of learning tasks, a theoretical understanding of the limitations of Deep Neural Networks (DNNs) has so far eluded practitioners. This is partly due to the inability to determine the closed forms of the learned functions, making it harder to study their generalization properties on unseen datasets. Recent work has shown that randomly initialized DNNs in the infinite width limit converge to kernel machines relying on a Neural Tangent Kernel (NTK) with known closed form. These results suggest, and experimental evidence corroborates, that empirical kernel machines can also act as surrogates for finite width DNNs. The high computational cost of assembling the full NTK, however, makes this approach infeasible in practice, motivating the need for low-cost approximations. In the current work, we study the performance of the Conjugate Kernel (CK), an efficient approximation to the NTK that has been observed to yield fairly similar results. ",
    "link": "http://arxiv.org/abs/2310.18612",
    "context": "Title: Efficient kernel surrogates for neural network-based regression. (arXiv:2310.18612v2 [cs.LG] UPDATED)\nAbstract: Despite their immense promise in performing a variety of learning tasks, a theoretical understanding of the limitations of Deep Neural Networks (DNNs) has so far eluded practitioners. This is partly due to the inability to determine the closed forms of the learned functions, making it harder to study their generalization properties on unseen datasets. Recent work has shown that randomly initialized DNNs in the infinite width limit converge to kernel machines relying on a Neural Tangent Kernel (NTK) with known closed form. These results suggest, and experimental evidence corroborates, that empirical kernel machines can also act as surrogates for finite width DNNs. The high computational cost of assembling the full NTK, however, makes this approach infeasible in practice, motivating the need for low-cost approximations. In the current work, we study the performance of the Conjugate Kernel (CK), an efficient approximation to the NTK that has been observed to yield fairly similar results. ",
    "path": "papers/23/10/2310.18612.json",
    "total_tokens": 913,
    "translated_title": "神经网络回归的高效核替代方法",
    "translated_abstract": "尽管深度神经网络在执行各种学习任务方面具有巨大潜力，但至今仍无法理解其局限性。这部分是由于无法确定学习函数的闭合形式，这使得在未见数据集上研究它们的泛化属性更加困难。最近的研究表明，在无限宽度限制下，随机初始化的深度神经网络会收敛到依赖于已知闭合形式的神经切向核（Neural Tangent Kernel，NTK）的核机器。这些结果暗示并得到实验证据证明，经验核机器也可以作为有限宽度深度神经网络的替代品。然而，组装完整的NTK的高计算成本使得此方法在实践中不可行，这促使我们需要低成本的近似方法。在当前的工作中，我们研究了共轭核（Conjugate Kernel，CK）的性能，这是一种对NTK的高效近似，据观察，它能产生相当相似的结果。",
    "tldr": "本论文研究了一种高效近似方法，称为共轭核（CK），用于替代深度神经网络的神经切向核（NTK）。该方法能够在计算成本较低的情况下产生与NTK相似的结果。",
    "en_tdlr": "This paper studies an efficient approximation called Conjugate Kernel (CK) as a substitute for the Neural Tangent Kernel (NTK) in deep neural networks, which can generate similar results to NTK with lower computational cost."
}