{
    "title": "Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])",
    "abstract": "In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s",
    "link": "http://arxiv.org/abs/2310.17146",
    "context": "Title: Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])\nAbstract: In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s",
    "path": "papers/23/10/2310.17146.json",
    "total_tokens": 932,
    "translated_title": "半离线策略评估中的反事实增强重要性抽样",
    "translated_abstract": "在将强化学习应用于高风险领域时，使用观测数据进行定量和定性评估可以帮助从业人员了解新策略的泛化性能。然而，这种离网策略评估（OPE）在本质上存在限制，因为离线数据可能不反映由于应用新策略而导致的分布偏移。另一方面，通过根据新策略收集轨迹进行在线评估通常是不可行的，因为在这些领域部署新策略可能是不安全的。在这项工作中，我们提出了一种半离线评估框架，作为离线和在线评估之间的中间步骤，其中人类用户提供未被观察到的反事实轨迹的注释。虽然诱人地简单地用这些注释来增加现有数据，但我们表明这种天真的方法可能导致有偏的结果。相反，我们设计了一种基于重要性抽样（IS）和新颖加权的OPE估计器系列。",
    "tldr": "提出了一种半离线评估框架，用于强化学习中的定量和定性评估。通过人类用户提供未被观察到的反事实轨迹的注释，设计了一种基于重要性抽样和加权的新型OPE估计器系列。"
}