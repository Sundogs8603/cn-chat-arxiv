{
    "title": "Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)",
    "abstract": "The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to pro",
    "link": "http://arxiv.org/abs/2310.11446",
    "context": "Title: Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)\nAbstract: The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to pro",
    "path": "papers/23/10/2310.11446.json",
    "total_tokens": 899,
    "translated_title": "大型Transformer水印的功能不变性",
    "translated_abstract": "基于Transformer的模型的快速增长增加了对其完整性和拥有权的担忧。水印技术通过将唯一标识嵌入模型中来解决这个问题，同时保持其性能。然而，大多数现有方法需要优化权重以嵌入水印信号，这在大规模情况下不适用于计算成本的原因。本文探讨了一种几乎没有计算成本且适用于非盲白盒设置（假设可以访问原始和带水印的网络）的水印技术。他们通过利用模型的不变性，比如维度排列或缩放/非缩放等操作，生成功能上等效的副本。这使得可以在不改变模型输出的情况下给模型加水印，并保持不可察觉性。实验证明了该方法的有效性以及对各种模型变换（微调、量化、修剪）的稳健性，使其成为实际解决方案。",
    "tldr": "本文介绍了一种用于大型Transformer的功能不变性水印技术，它使用模型的不变性生成功能上等效的副本，并能在不改变模型输出的情况下给模型加上水印，这是一种计算成本极低且适用于实际应用的解决方案。",
    "en_tdlr": "This paper presents a functional invariance watermarking technique for large Transformers, which generates functionally equivalent copies by leveraging the models' invariance and allows watermarking without changing the model outputs. It is a low-computational-cost and practical solution against concerns about integrity and ownership of transformer-based models."
}