{
    "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])",
    "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
    "link": "http://arxiv.org/abs/2310.06020",
    "context": "Title: DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])\nAbstract: Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
    "path": "papers/23/10/2310.06020.json",
    "total_tokens": 744,
    "translated_title": "DyST：面向实际视频的动态神经场景表示",
    "translated_abstract": "对世界的视觉理解超越了单个图像的语义和平面结构。我们的目标是从单目实际视频中捕捉到实际场景的3D结构和动态特性。我们的Dynamic Scene Transformer（DyST）模型利用了最近的神经场景表示研究成果，学习了单目实际视频的潜在分解，包括场景内容、每个视角的场景动态和相机姿态。通过在单目视频和我们的新的合成数据集DySO上进行一种新颖的协同训练，实现了这种分离。DyST学习到了动态场景的具体潜在表示，使得可以对场景的相机和内容进行独立控制的视图生成成为可能。",
    "tldr": "DyST模型通过学习动态场景的潜在分解，从实际视频中捕捉到了场景的3D结构和动态特性，并实现了对相机和场景内容的独立控制视图生成。",
    "en_tdlr": "The DyST model captures both the 3D structure and dynamics of real-world scenes from monocular videos, and enables view generation with separate control over the camera and the content of the scene."
}