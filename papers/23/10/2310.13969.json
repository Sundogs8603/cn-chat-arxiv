{
    "title": "Distributed Linear Regression with Compositional Covariates. (arXiv:2310.13969v1 [stat.ML])",
    "abstract": "With the availability of extraordinarily huge data sets, solving the problems of distributed statistical methodology and computing for such data sets has become increasingly crucial in the big data area. In this paper, we focus on the distributed sparse penalized linear log-contrast model in massive compositional data. In particular, two distributed optimization techniques under centralized and decentralized topologies are proposed for solving the two different constrained convex optimization problems. Both two proposed algorithms are based on the frameworks of Alternating Direction Method of Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et al., 2014, Biometrika). It is worth emphasizing that, in the decentralized topology, we introduce a distributed coordinate-wise descent algorithm based on Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research) for obtaining a communication-efficient regularized estimation. Correspondingly, the conve",
    "link": "http://arxiv.org/abs/2310.13969",
    "context": "Title: Distributed Linear Regression with Compositional Covariates. (arXiv:2310.13969v1 [stat.ML])\nAbstract: With the availability of extraordinarily huge data sets, solving the problems of distributed statistical methodology and computing for such data sets has become increasingly crucial in the big data area. In this paper, we focus on the distributed sparse penalized linear log-contrast model in massive compositional data. In particular, two distributed optimization techniques under centralized and decentralized topologies are proposed for solving the two different constrained convex optimization problems. Both two proposed algorithms are based on the frameworks of Alternating Direction Method of Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et al., 2014, Biometrika). It is worth emphasizing that, in the decentralized topology, we introduce a distributed coordinate-wise descent algorithm based on Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research) for obtaining a communication-efficient regularized estimation. Correspondingly, the conve",
    "path": "papers/23/10/2310.13969.json",
    "total_tokens": 862,
    "translated_title": "具有组合协变量的分布式线性回归",
    "translated_abstract": "随着大规模数据集的可用性，解决分布式统计方法和计算问题变得在大数据领域日益关键。本文针对大规模组合数据中的分布式稀疏惩罚线性对比模型进行研究。具体而言，我们提出了两种不同约束凸优化问题的分布式优化技术，分别采用集中式和分散式拓扑结构。这两个算法都是基于交替方向乘子方法（ADMM）和坐标下降方法的框架。值得强调的是，在分散式拓扑中，我们引入了一个基于分组ADMM（GADMM）的分布式坐标下降算法，以获得具有较低通信开销的正则化估计。更具体地说，对于每个变量，我们引入了适应GADMM的协调下降方法。",
    "tldr": "本文提出了针对大规模组合数据中的分布式稀疏惩罚线性对比模型的两种分布式优化技术，分别采用集中式和分散式拓扑结构，以获得具有较低通信开销的正则化估计。",
    "en_tdlr": "This paper proposes two distributed optimization techniques for the distributed sparse penalized linear log-contrast model in massive compositional data, using centralized and decentralized topologies respectively, to obtain regularized estimation with lower communication costs."
}