{
    "title": "NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])",
    "abstract": "As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that eve",
    "link": "http://arxiv.org/abs/2310.10920",
    "context": "Title: NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])\nAbstract: As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that eve",
    "path": "papers/23/10/2310.10920.json",
    "total_tokens": 875,
    "translated_title": "NuclearQA: 用于核领域语言模型的人工基准",
    "translated_abstract": "随着语言模型的流行，它们已经被应用于几乎所有领域。但是随着应用于特定领域的扩大，评估其在这些领域的有效性的方法日益缺乏。现有的基准大部分专注于不需要对所涉及主题进行正确理解的问题。在本文中，我们介绍了NuclearQA，这是一个由专家设计的用于评估核领域语言模型的人工基准，包含了100个问题，用于测试语言模型的能力。我们详细介绍了我们的方法，并展示了由于现有评估指标的限制，我们基准的独特能力。我们还提出了自己的评估指标来评估语言模型的性能。我们对最先进的模型进行了实验，结果表明，即使在核领域，NuclearQA也能够有效评估语言模型的性能。",
    "tldr": "NuclearQA是一个人工基准，用于评估核领域的语言模型的性能，其中包含100个专家设计的问题。该基准与现有的评估指标不同，能够准确评估语言模型在核领域的能力。"
}