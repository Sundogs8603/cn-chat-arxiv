{
    "title": "Neural Relational Inference with Fast Modular Meta-learning. (arXiv:2310.07015v1 [cs.LG])",
    "abstract": "\\textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \\textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \\textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on obser",
    "link": "http://arxiv.org/abs/2310.07015",
    "context": "Title: Neural Relational Inference with Fast Modular Meta-learning. (arXiv:2310.07015v1 [cs.LG])\nAbstract: \\textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \\textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \\textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on obser",
    "path": "papers/23/10/2310.07015.json",
    "total_tokens": 890,
    "translated_title": "快速模块化元学习的神经关系推理",
    "translated_abstract": "图神经网络(GNN)是用于包含实体和关系的许多动态系统的有效模型。尽管大多数GNN应用假设实体和关系只有一种类型，但许多情况涉及多种类型的交互。关系推理是从观测数据中推断这些交互并学习动力学的问题。我们将关系推理视为模块化元学习问题，通过训练神经模块以不同的方式组合来解决许多任务。这个元学习框架允许我们隐式地编码时间不变性，并在一个上下文中推断关系，而不是独立地推断，从而增加了推理能力。将推理作为元学习的内循环优化导致了一种基于模型的方法，更加高效利用数据，并能够估计我们不直接观测到的实体的状态，但可以从它们对观测结果的影响中推断出它们的存在。",
    "tldr": "本论文提出了一种基于快速模块化元学习的神经关系推理方法，通过训练神经模块，可以在多种任务中以不同的方式组合，隐式地编码时间不变性，并在一个上下文中推断关系，从而增加了推理能力。"
}