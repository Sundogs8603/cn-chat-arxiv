{
    "title": "The Blame Problem in Evaluating Local Explanations, and How to Tackle it. (arXiv:2310.03466v1 [cs.LG])",
    "abstract": "The number of local model-agnostic explanation techniques proposed has grown rapidly recently. One main reason is that the bar for developing new explainability techniques is low due to the lack of optimal evaluation measures. Without rigorous measures, it is hard to have concrete evidence of whether the new explanation techniques can significantly outperform their predecessors. Our study proposes a new taxonomy for evaluating local explanations: robustness, evaluation using ground truth from synthetic datasets and interpretable models, model randomization, and human-grounded evaluation. Using this proposed taxonomy, we highlight that all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the \"blame problem.\" In our study, we argue that this category of evaluation measure is a more reasonable method for evaluating local model-agnostic explanations. However, we show that even this category of evaluation measu",
    "link": "http://arxiv.org/abs/2310.03466",
    "context": "Title: The Blame Problem in Evaluating Local Explanations, and How to Tackle it. (arXiv:2310.03466v1 [cs.LG])\nAbstract: The number of local model-agnostic explanation techniques proposed has grown rapidly recently. One main reason is that the bar for developing new explainability techniques is low due to the lack of optimal evaluation measures. Without rigorous measures, it is hard to have concrete evidence of whether the new explanation techniques can significantly outperform their predecessors. Our study proposes a new taxonomy for evaluating local explanations: robustness, evaluation using ground truth from synthetic datasets and interpretable models, model randomization, and human-grounded evaluation. Using this proposed taxonomy, we highlight that all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the \"blame problem.\" In our study, we argue that this category of evaluation measure is a more reasonable method for evaluating local model-agnostic explanations. However, we show that even this category of evaluation measu",
    "path": "papers/23/10/2310.03466.json",
    "total_tokens": 935,
    "translated_title": "评估局部解释中的归因问题及其应对方法",
    "translated_abstract": "最近局部模型无关解释技术的提出数量迅速增长。主要原因之一是由于缺乏最优评估指标，开发新的解释技术的门槛较低。没有严格的评估指标，很难有确凿的证据证明新的解释技术能否明显优于其前人。我们的研究提出了一种新的分类方法来评估局部解释：鲁棒性、使用合成数据集和可解释的模型进行评估、模型随机化以及人类参与评估。使用这种提出的分类方法，我们强调除了基于可解释模型的基准真实数据之外的所有评估方法都遭受了我们称之为“归因问题”的问题。在我们的研究中，我们认为这种评估方法是一种更合理、更能评估局部模型无关解释的方法。然而，我们展示了即使是这种评估方法也存在一些挑战。",
    "tldr": "最近局部模型无关解释技术的提出数量迅速增长，在评估这些技术时存在归因问题。我们提出了一种新的分类方法来评估局部解释，并强调除了基于可解释模型的基准真实数据之外的所有评估方法都遭受了这个问题。我们认为基于可解释模型的评估方法是更合理的方法。"
}