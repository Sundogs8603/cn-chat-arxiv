{
    "title": "Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning. (arXiv:2310.12055v1 [cs.LG])",
    "abstract": "In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensiona",
    "link": "http://arxiv.org/abs/2310.12055",
    "context": "Title: Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning. (arXiv:2310.12055v1 [cs.LG])\nAbstract: In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensiona",
    "path": "papers/23/10/2310.12055.json",
    "total_tokens": 890,
    "translated_title": "在反强化学习中通过最优传输理论理解奖励不确定性",
    "translated_abstract": "在反强化学习（IRL）中，主要目标是从观察到的专家行为中推断出潜在的奖励函数，不仅要解释给定的数据，还要泛化到未知场景。这确保了对奖励不确定性的鲁棒性，其中多个奖励函数可以同样解释相同的专家行为。虽然在解决这个问题上已经做出了重大努力，但是当前的方法在高维问题上面临挑战，并且缺乏几何基础。本文利用最优传输（OT）理论，为这些挑战提供了一种新的视角。通过利用OT的Wasserstein距离，我们建立了一个几何框架，可以量化奖励不确定性并识别奖励函数的中心表示或质心。这些观点为以几何解释为基础的鲁棒IRL方法铺平了道路，提供了一种结构化的方法来处理高维奖励不确定性。",
    "tldr": "本文利用最优传输理论和几何表示方法来解决反强化学习中的奖励不确定性问题，在高维情况下具有鲁棒性，并通过量化奖励不确定性和识别奖励函数的中心表示提供了一种结构化方法。"
}