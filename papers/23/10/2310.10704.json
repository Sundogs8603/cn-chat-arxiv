{
    "title": "Optimized Tokenization for Transcribed Error Correction. (arXiv:2310.10704v1 [cs.CL])",
    "abstract": "The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocab",
    "link": "http://arxiv.org/abs/2310.10704",
    "context": "Title: Optimized Tokenization for Transcribed Error Correction. (arXiv:2310.10704v1 [cs.CL])\nAbstract: The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocab",
    "path": "papers/23/10/2310.10704.json",
    "total_tokens": 868,
    "translated_title": "优化的转录错误修正中的分词技术",
    "translated_abstract": "面对语音识别系统的挑战，如发音变化、不良音频条件和标记数据的稀缺性，强调了后处理步骤纠正重复错误的必要性。先前的研究表明，采用专用的错误修正模型具有优势，然而训练这样的模型需要大量的标记数据，这并不容易获得。为了克服这个限制，通常使用合成的类似转录的数据，然而，弥合转录错误和合成噪声之间的分布差距并不简单。在本文中，我们证明了通过仅使用合成数据训练可以显著提高纠正模型的性能。具体而言，我们通过实证展示：（1）使用从一组转录数据中得到的错误分布生成的合成数据优于应用随机扰动的常见方法；（2）应用语言特定的词汇调整可以进一步提高纠正模型的性能。",
    "tldr": "本研究通过训练仅使用合成数据的方法，在转录错误修正中取得了显著的性能提升。具体而言，通过使用从转录数据中得到的合成数据，并进行语言特定的词汇调整，可以更好地纠正重复错误。",
    "en_tdlr": "This paper demonstrates significant performance improvement in transcribed error correction by training correction models solely using synthetic data. Specifically, using synthetic data generated from transcribed data and applying language-specific adjustments to the vocabulary, better correction of recurring errors can be achieved."
}