{
    "title": "Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])",
    "abstract": "We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl",
    "link": "http://arxiv.org/abs/2310.16944",
    "context": "Title: Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])\nAbstract: We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl",
    "path": "papers/23/10/2310.16944.json",
    "total_tokens": 996,
    "translated_title": "Zephyr: 直接蒸馏语言模型对齐",
    "translated_abstract": "我们的目标是生成一个与用户意图对齐的较小型语言模型。之前的研究表明，对较大的模型应用蒸馏的监督微调能显著提高任务准确性；然而，这些模型没有对齐，即它们不能很好地响应自然提示。为了蒸馏这个属性，我们尝试使用来自AI反馈（AIF）的偏好数据。从一个由教师模型排名的输出数据集开始，我们应用蒸馏的直接偏好优化（dDPO）来学习一个具有显著改进意图对齐的聊天模型。这种方法只需要几个小时的训练时间，在微调过程中无需任何额外的采样。最终结果Zephyr-7B在7B参数模型的聊天基准上取得了最先进的结果，并且无需人工注释。特别是，在MT-Bench上的结果显示，Zephyr-7B超越了最好的开放访问RLHF模型Llama2-Chat-70B。系统的代码、模型、数据和教程都可以获取。",
    "tldr": "本论文提出了一种直接蒸馏的语言模型对齐方法，使用AI反馈数据进行优化，在聊天任务上显著提高了意图对齐的效果，该方法只需要几个小时的训练时间且无需人工注释，实验证明在7B参数模型上超过了现有最好的开放访问模型。"
}