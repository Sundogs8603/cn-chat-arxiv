{
    "title": "Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])",
    "abstract": "Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.",
    "link": "http://arxiv.org/abs/2310.02692",
    "context": "Title: Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])\nAbstract: Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.",
    "path": "papers/23/10/2310.02692.json",
    "total_tokens": 851,
    "translated_title": "基于聚类的图像-文本图匹配来弥合领域差距",
    "translated_abstract": "学习领域不变表示对于训练可以很好地推广到未见过目标任务领域的模型非常重要。文本描述本身包含概念的语义结构，这样的辅助语义线索可以用作领域概括问题的有效枢纽嵌入。我们使用多模态图像和文本融合的图表示来获得在局部图像和文本描述符之间考虑内在语义结构的领域不变枢纽嵌入。具体来说，我们通过(i)用图表示图像和文本描述，以及(ii)将基于图像节点特征的聚类和匹配应用到文本图中，来学习领域不变特征。我们使用大规模公共数据集（如CUB-DG和DomainBed）进行实验，并在这些数据集上达到与或优于现有最先进模型的性能。我们的代码将在出版后公开提供。",
    "tldr": "通过基于聚类的图像-文本图匹配来弥合领域差距，学习领域不变特征以实现在未见过领域上的良好泛化能力，实验结果显示在公共数据集上达到最先进性能。"
}