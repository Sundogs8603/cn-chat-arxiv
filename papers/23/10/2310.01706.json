{
    "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])",
    "abstract": "We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function",
    "link": "http://arxiv.org/abs/2310.01706",
    "context": "Title: On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])\nAbstract: We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function",
    "path": "papers/23/10/2310.01706.json",
    "total_tokens": 971,
    "translated_title": "关于基于模型和无模型强化学习的表示复杂性的研究",
    "translated_abstract": "我们在电路复杂度的背景下研究了基于模型和无模型的强化学习的表示复杂性。我们在理论上证明，存在一类广泛的马尔可夫决策过程（MDP），它们的转移和奖励函数可以用具有多项式大小的恒定深度电路表示，而最优的$Q$-函数在恒定深度电路中遭受指数级电路复杂度。通过关注逼近误差并建立到复杂性理论的联系，我们的理论从新的表示复杂性角度为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性提供了独特的见解：在某些情况下，环境的真实规则（模型）易于表示，而其他数量，如$Q$-函数，似乎很复杂。我们通过比较转移核函数、奖励函数和最优$Q$-函数的逼近误差来经验性地验证我们的理论。",
    "tldr": "本研究在电路复杂度的角度探讨了基于模型和无模型强化学习的表示复杂性。理论上证明了某些MDP可以用恒定深度电路表示转移和奖励函数，但最优$Q$-函数的电路复杂度指数级增加。我们的理论揭示了为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性。",
    "en_tdlr": "This study investigates the representation complexity of model-based and model-free reinforcement learning in the context of circuit complexity. Theoretical results show that while certain MDPs can be represented by constant depth circuits, the optimal Q-function suffers from exponential circuit complexity. The findings shed light on why model-based algorithms typically have better sample complexity than model-free algorithms."
}