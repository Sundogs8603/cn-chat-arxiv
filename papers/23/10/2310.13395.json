{
    "title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models. (arXiv:2310.13395v1 [cs.CL])",
    "abstract": "Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with",
    "link": "http://arxiv.org/abs/2310.13395",
    "context": "Title: Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models. (arXiv:2310.13395v1 [cs.CL])\nAbstract: Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with",
    "path": "papers/23/10/2310.13395.json",
    "total_tokens": 936,
    "translated_title": "如果可以的话就给我缓存起来：一种在线成本感知的教师-学生框架，用于减少对大型语言模型的调用次数",
    "translated_abstract": "在零次和少次训练的情况下，大型语言模型 (LLM) 的提示效果显著。因此，无法承担创建大规模任务特定训练数据集和预训练自己的LLM成本的中小型企业 (SMEs) 越来越多地转向允许它们从LLM中提取信息的第三方服务。然而，这些服务目前需要按调用支付费用，这成为重要的运营成本（OpEx）。此外，客户的输入通常随时间相似，SMEs最终以非常相似的实例提示LLM。我们提出了一个框架，通过缓存先前的LLM响应并使用它们来训练SME端上的本地廉价模型，从而减少对LLM的调用次数。该框架包括决定何时相信本地模型或调用LLM的准则，以及调整准则和衡量性能与成本之间的权衡的方法论。为了实验目的，我们使用一个示例来实例化我们的框架。",
    "tldr": "提供了一种在线成本感知的教师-学生框架，通过缓存并利用先前的LLM响应来训练本地廉价模型，从而减少对大型语言模型的调用次数。",
    "en_tdlr": "A cost-aware teacher-student framework is proposed to reduce the calls to large language models by caching and utilizing previous LLM responses to train a local inexpensive model."
}