{
    "title": "Tight Time-Space Lower Bounds for Constant-Pass Learning. (arXiv:2310.08070v1 [cs.LG])",
    "abstract": "In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\\sqrt{n}}$ samples. (Their result also generalizes to other learning problems.)  In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\\Omega(n^{2})$ memory size or at least $2^{\\",
    "link": "http://arxiv.org/abs/2310.08070",
    "context": "Title: Tight Time-Space Lower Bounds for Constant-Pass Learning. (arXiv:2310.08070v1 [cs.LG])\nAbstract: In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\\sqrt{n}}$ samples. (Their result also generalizes to other learning problems.)  In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\\Omega(n^{2})$ memory size or at least $2^{\\",
    "path": "papers/23/10/2310.08070.json",
    "total_tokens": 919,
    "translated_title": "紧密时间空间下对于常数通行学习的下界",
    "translated_abstract": "在他的突破性论文中，Raz证明了任何奇偶学习算法要么需要二次内存，要么需要指数数量的样本。随后的一系列工作将此结果扩展到了大类学习问题。直到最近，所有这些结果都考虑了流式模型中的学习，其中每个样本都是独立绘制的，而学习者被允许在样本流上进行单次通行。Garg、Raz和Tal则考虑了一个更强的模型，允许对样本流进行多次通行。在2次通行模型中，他们证明了大小为n的奇偶学习需要n^1.5的内存大小或者至少2^(n^0.5)个样本数量。在这项工作中，对于任意常数q，我们证明了对于在样本流上进行q次通行的任何奇偶学习算法的紧密内存-样本下界。我们证明这样的学习者要么需要Ω(n^2)的内存大小，要么至少需要2^的样本数量。",
    "tldr": "该论文证明了对于常数次通行的任何奇偶学习算法，需要要么Ω(n^2)的内存大小，要么至少需要2^的样本数量。",
    "en_tdlr": "This paper presents tight memory-sample lower bounds for any parity learning algorithm that makes a constant number of passes over the stream of samples, showing that such a learner requires either Ω(n^2) memory size or at least 2^ samples."
}