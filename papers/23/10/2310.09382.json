{
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations. (arXiv:2310.09382v1 [cs.LG])",
    "abstract": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "link": "http://arxiv.org/abs/2310.09382",
    "context": "Title: LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations. (arXiv:2310.09382v1 [cs.LG])\nAbstract: In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "path": "papers/23/10/2310.09382.json",
    "total_tokens": 850,
    "translated_title": "LL-VQ-VAE: 学习可学习的格子向量量化以提高表示效率",
    "translated_abstract": "本文介绍了可学习的格子向量量化方法(LL-VQ-VAE)，并展示了它在学习离散表示方面的有效性。我们的方法将VQ-VAE中的向量量化层替换为基于格子的离散化方法。可学习的格子对所有离散嵌入施加一种结构，防止码本崩溃，从而实现了高码本利用率。与VQ-VAE相比，我们的方法在相同的训练条件下得到了更低的重构误差，训练时间仅为一小部分，并且具有恒定数量的参数（等于嵌入维度D），使其成为一种非常可扩展的方法。我们在FFHQ-1024数据集上展示了这些结果，并包括了FashionMNIST和Celeb-A数据集。",
    "tldr": "本文介绍了一种名为LL-VQ-VAE的学习可学习的格子向量量化方法，通过替换向量量化层来实现高效表示。与传统方法相比，该方法在相同训练条件下具有更低的重构误差，训练时间更短，参数数量恒定。"
}