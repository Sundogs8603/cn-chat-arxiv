{
    "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval. (arXiv:2310.08319v1 [cs.IR])",
    "abstract": "The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from thi",
    "link": "http://arxiv.org/abs/2310.08319",
    "context": "Title: Fine-Tuning LLaMA for Multi-Stage Text Retrieval. (arXiv:2310.08319v1 [cs.IR])\nAbstract: The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from thi",
    "path": "papers/23/10/2310.08319.json",
    "total_tokens": 961,
    "translated_title": "对多阶段文本检索中的LLaMA进行微调",
    "translated_abstract": "自从预训练语言模型出现之前，多阶段文本检索的有效性已经得到了充分的证明。然而，大多数现有的研究都使用了过时的模型，没有考虑到最新的大型语言模型（LLMs）的进展。本研究旨在探索最先进的LLMs可能带来的改进。我们对最新的LLaMA模型进行了全面研究，通过使用MS MARCO数据集，将其作为稠密检索器（RepLLaMA）和点对点再排序器（RankLLaMA），用于段落检索和文档检索。我们的研究结果表明，大型语言模型的有效性确实超过了较小模型。此外，由于LLMs可以固有地处理更长的上下文，它们可以全面地表示整个文档，消除了传统的分段和汇集策略的需求。此外，对BEIR的评估表明，我们的RepLLaMA-RankLLaMA流水线展现了强大的零-shot有效性。来自这项研究的模型检查点...",
    "tldr": "本研究通过对LLaMA模型进行微调，发现大型语言模型的效果优于较小模型，并且能够全面地表示整个文档，消除了传统的分段和汇集策略的需求。此外，我们的RepLLaMA-RankLLaMA流水线在零-shot情况下展现出强大的有效性。",
    "en_tdlr": "This study fine-tunes the LLaMA model and finds that large language models outperform smaller models, can comprehensively represent entire documents, eliminating the need for traditional segmenting and pooling strategies. Additionally, the RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness."
}