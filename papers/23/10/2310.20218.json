{
    "title": "A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])",
    "abstract": "The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.",
    "link": "http://arxiv.org/abs/2310.20218",
    "context": "Title: A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])\nAbstract: The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.",
    "path": "papers/23/10/2310.20218.json",
    "total_tokens": 826,
    "translated_title": "基于Transformer的长期系列预测的系统综述",
    "translated_abstract": "深度学习的出现在时间序列预测方面取得了显著进展。特别是Transformer架构在时间序列预测任务中得到了广泛的应用和采用。Transformer被证明是提取长序列内部元素之间语义相关性最成功的解决方案。各种变体使得Transformer架构能够有效处理长期时间序列预测任务。在本文中，我们首先对Transformer架构及其后续改进进行了全面概述，以解决各种长期时间序列预测任务。然后，我们总结了公开可用的长期时间序列预测数据集和相关的评估指标。此外，我们提供了关于在时间序列分析背景下有效训练Transformer的最佳实践和技术的有价值见解。最后，我们提出了这个快速发展领域的潜在研究方向。",
    "tldr": "基于Transformer的长期系列预测的系统综述，介绍了Transformer架构及其改进、公开可用的数据集和评估指标、有效训练Transformer的最佳实践和技术，并提出了潜在研究方向。"
}