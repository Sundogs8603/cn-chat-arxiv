{
    "title": "Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])",
    "abstract": "We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization",
    "link": "http://arxiv.org/abs/2310.12766",
    "context": "Title: Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])\nAbstract: We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization",
    "path": "papers/23/10/2310.12766.json",
    "total_tokens": 820,
    "translated_title": "基于Transformer的实体法律形式分类",
    "translated_abstract": "我们提出了使用基于Transformer的语言模型来对原始法律实体名称进行实体法律形式分类的方法。具体而言，我们采用了各种BERT变种，并将它们的性能与多个传统基准进行比较。我们的评估涵盖了一个庞大的自由可用的法律实体标识符（LEI）数据子集，包括来自30个不同法律司法辖区的超过110万个法律实体。每个司法辖区的分类的真实标签来自实体法律形式（ELF）代码标准（ISO 20275）。我们的研究结果表明，预训练的BERT变种在F1分数方面优于传统的文本分类方法，在Macro F1分数方面表现相当好。此外，我们的提案得到了在十个选择的司法辖区进行的第三方专家评审的支持。本研究凸显了基于Transformer模型在推进数据标准化方面的重要潜力。",
    "tldr": "提出了一种使用Transformer-based语言模型进行实体法律形式分类的方法，该方法在比较中表现出较高的性能并得到了第三方评审的支持。",
    "en_tdlr": "A method for entity legal form classification using Transformer-based language models is proposed, which shows superior performance compared to traditional approaches and is supported by third-party reviews."
}