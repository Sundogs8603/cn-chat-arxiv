{
    "title": "CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])",
    "abstract": "Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder mod",
    "link": "http://arxiv.org/abs/2310.15577",
    "context": "Title: CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])\nAbstract: Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder mod",
    "path": "papers/23/10/2310.15577.json",
    "total_tokens": 919,
    "translated_title": "CONTRASTE: 一种带有基于方面的提示的监督对比预训练用于方面情感三元组抽取",
    "translated_abstract": "现有的方面情感三元组抽取（ASTE）的研究主要关注如何开发更高效的微调技术。相反，我们的动机是提出一种通用方法，可以同时改善多个ABSA任务的效果。为此，我们提出了一种名为CONTRASTE的新型预训练策略，利用对比学习来增强ASTE的性能。我们除了主要关注ASTE之外，还展示了我们提出的技术在其他ABSA任务（如ACOS，TASD和AESC）上的优势。给定一个句子及其相关的（方面，观点，情感）三元组，首先，我们设计了基于方面的提示，并屏蔽了相应的情感。然后，我们通过对解码器生成的方面感知情感表示进行对比学习来（预）训练编码-解码模型。为了微调得到的模型权重，我们提出了一种新颖的多任务方法，其中基础编码解码模型同时被用于ASTG和其他ABSA任务。",
    "tldr": "CONTRASTE是一种利用对比学习的预训练策略，通过设计基于方面的提示并应用对比学习来增强方面情感三元组抽取（ASTE）任务的性能，并在其他ABSA任务上展示出优势。",
    "en_tdlr": "CONTRASTE is a pre-training strategy that utilizes contrastive learning to enhance the performance of Aspect Sentiment Triplet Extraction (ASTE) by designing aspect-based prompts and applying contrastive learning, and demonstrates advantages on other ABSA tasks."
}