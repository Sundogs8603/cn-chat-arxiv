{
    "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication",
    "abstract": "arXiv:2310.01211v2 Announce Type: replace  Abstract: It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction ",
    "link": "https://arxiv.org/abs/2310.01211",
    "context": "Title: From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication\nAbstract: arXiv:2310.01211v2 Announce Type: replace  Abstract: It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction ",
    "path": "papers/23/10/2310.01211.json",
    "total_tokens": 797,
    "translated_title": "从砖块到桥梁：增强潜在空间通信的不变性乘积",
    "translated_abstract": "研究发现，在相似的归纳偏差下训练的不同神经网络学习到的表示隐藏了结构相似性。从几何角度来看，识别连接这些表示的转换类别和相关不变性对于解锁合并、拼接和重用不同神经模块等应用至关重要。然而，由于多种因素（如权重初始化、训练超参数或数据模态）导致事先估计特定任务的转换可能具有挑战性和昂贵。为此，我们引入了一种灵活的方法，直接将一组不变性纳入表示中，构建一个不变性分量的乘积空间位于潜在表示之上，而不需要关于要注入的最佳不变性的先验知识。我们在分类和重建任务上验证了我们的解决方案。",
    "tldr": "该论文提出了一种方法，可以直接将一组不变性纳入表示中，构建一个不变性分量的乘积空间，从而增强潜在空间的通信。",
    "en_tdlr": "The paper introduces a method to directly incorporate a set of invariances into representations, constructing a product space of invariant components to enhance latent space communication."
}