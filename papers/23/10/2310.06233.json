{
    "title": "Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers. (arXiv:2310.06233v1 [cs.LG])",
    "abstract": "To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration pe",
    "link": "http://arxiv.org/abs/2310.06233",
    "context": "Title: Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers. (arXiv:2310.06233v1 [cs.LG])\nAbstract: To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration pe",
    "path": "papers/23/10/2310.06233.json",
    "total_tokens": 940,
    "translated_title": "通过新颖的稀疏感应正则化方法实现低秩张量补全",
    "translated_abstract": "为了减轻低秩张量补全问题中由l1范数产生的偏差，已经提出了非凸的替代张量核范数的替代函数/正则化器，虽然它们都可以实现稀疏性。然而，这些非凸正则化器的阈值函数可能没有封闭形式的表达式，因此需要进行迭代，从而增加了计算负担。为了解决这个问题，我们提出了一个框架来生成具有封闭形式阈值函数的稀疏感应正则化器。这些正则化器应用于低 Tubal Rank 张量补全，并基于交替方向乘法的方法开发了高效的算法。此外，分析了我们的方法的收敛性，并证明了生成的序列是有界的，任何极限点都是一个稳定点。使用合成和真实世界数据集的实验结果表明，所提出的算法在恢复性能方面优于现有方法。",
    "tldr": "本研究提出了一种通过新颖的稀疏感应正则化方法实现低秩张量补全的框架。该方法提供了具有封闭形式阈值函数的正则化器，并基于交替方向乘法的算法进行高效计算。实验结果表明，所提出的算法在恢复性能方面优于现有方法。"
}