{
    "title": "TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])",
    "abstract": "The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine",
    "link": "http://arxiv.org/abs/2310.17876",
    "context": "Title: TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])\nAbstract: The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine",
    "path": "papers/23/10/2310.17876.json",
    "total_tokens": 907,
    "translated_title": "TarGEN: 基于大型语言模型的目标数据生成技术",
    "translated_abstract": "大型语言模型（LLM）的快速发展引发了对数据合成技术的兴趣，旨在生成多样且高质量的合成数据集。然而，这些合成数据集往往缺乏多样性并且存在噪声。在本文中，我们提出了TarGEN，一种利用LLM生成高质量的合成数据集的多步提示策略。TarGEN的一个优点是无需种子；它不需要特定的任务实例，扩大了其适用性。我们还通过一种称为自我修正的方法，使LLM能够在创建数据集过程中纠正标记错误的实例，确保可靠的标签。为了评估我们技术的有效性，我们模拟了SuperGLUE基准测试中的8个任务，并在合成和原始训练集上微调了各种语言模型，包括仅编码器、编码器-解码器和仅解码器模型。在原始测试集上的评估结果显示，模型在合成数据集上训练的效果与原始数据集相当。",
    "tldr": "TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。"
}