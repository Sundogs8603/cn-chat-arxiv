{
    "title": "Let the Pretrained Language Models \"Imagine\" for Short Texts Topic Modeling. (arXiv:2310.15420v1 [cs.CL])",
    "abstract": "Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topics text generation from PLMs. We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our model",
    "link": "http://arxiv.org/abs/2310.15420",
    "context": "Title: Let the Pretrained Language Models \"Imagine\" for Short Texts Topic Modeling. (arXiv:2310.15420v1 [cs.CL])\nAbstract: Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topics text generation from PLMs. We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our model",
    "path": "papers/23/10/2310.15420.json",
    "total_tokens": 903,
    "translated_title": "让预训练语言模型为短文本主题建模“想象”",
    "translated_abstract": "主题模型是一种发现文档集合中潜在语义的有效方法。然而，它假设文档具有足够的共现信息才能发挥效果。然而，在短文本中，共现信息很少，导致文档表示中的特征稀疏。因此，现有的主题模型（概率或神经网络）大多无法从中挖掘模式并生成连贯的主题。在本文中，我们采用了一种新的方法来解决短文本主题建模中的数据稀疏问题，通过使用现有的预训练语言模型（PLMs）将短文本扩展为更长的序列。此外，我们提供了一个简单的解决方案来扩展神经主题模型，以减少PLMs生成的噪声“非主题”文本的影响。我们观察到我们的模型可以大大提高短文本主题建模的性能。在极端数据稀疏的情况下，对多个真实数据集进行了大量实验，结果显示我们的模型效果显著提高。",
    "tldr": "本文提出了一种新的方法来解决短文本主题建模中的数据稀疏问题，通过使用预训练语言模型将短文本扩展为更长的序列。实验结果表明，该模型在极端数据稀疏情况下能够显著提高短文本主题建模的性能。"
}