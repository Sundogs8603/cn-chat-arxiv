{
    "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])",
    "abstract": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
    "link": "http://arxiv.org/abs/2310.03639",
    "context": "Title: Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])\nAbstract: The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
    "path": "papers/23/10/2310.03639.json",
    "total_tokens": 873,
    "translated_title": "评估自监督语音表示对美洲土著语言的应用",
    "translated_abstract": "近年来，自监督学习应用于语音表示学习引起了广泛关注，因为它能够扩展到大量的无标注数据。然而，在预训练和下游评估方面，目前的进展仅集中在考虑英语的单语模型上。很少有模型考虑其他语言，尤其是土著语言。我们在ASRU 2023 ML-SUPERB Challenge的新语言赛道中提交了一个用于Quechua的ASR语料库，它是一种南美土著语言。我们评估了大型自监督学习模型在Quechua和其他6种土著语言（如Guarani和Bribri）的低资源ASR上的效果。结果显示，最先进的自监督学习模型表现出令人惊讶的强大性能，展示了大规模模型在真实世界数据上的潜在泛化能力。",
    "tldr": "本论文评估了自监督语音表示在美洲土著语言上的应用。研究结果表明，大型自监督学习模型在低资源ASR任务中展示出了令人惊讶的强大性能，显示了这些模型在真实世界数据上的泛化能力。",
    "en_tdlr": "This paper evaluates the application of self-supervised speech representations on indigenous American languages. The results show surprisingly strong performance by large-scale models in low-resource ASR tasks, demonstrating their potential for generalization to real-world data."
}