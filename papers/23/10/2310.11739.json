{
    "title": "Unintended Memorization in Large ASR Models, and How to Mitigate It. (arXiv:2310.11739v1 [cs.LG])",
    "abstract": "It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example ",
    "link": "http://arxiv.org/abs/2310.11739",
    "context": "Title: Unintended Memorization in Large ASR Models, and How to Mitigate It. (arXiv:2310.11739v1 [cs.LG])\nAbstract: It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example ",
    "path": "papers/23/10/2310.11739.json",
    "total_tokens": 936,
    "translated_title": "大型ASR模型中的意外记忆及其缓解方法",
    "translated_abstract": "众所周知，神经网络可能会无意中记住训练样本，引发隐私问题。然而，在大型非自回归自动语音识别（ASR）模型中审计记忆一直是具有挑战性的，因为现有方法（如硬度校准）的计算成本很高。在本研究中，我们设计了一种简单的审计方法，用于在大型ASR模型中测量记忆，而无需额外的计算开销。具体而言，我们加速随机生成的话语，创建一个语音和文本信息之间的映射，这在典型的训练样本中很难学习到。因此，仅对加速训练样本的准确预测可以作为记忆的明确证据，并且相应的准确性可以用来衡量记忆。使用所提出的方法，我们展示了现有ASR模型中的记忆现象。为了缓解记忆，我们尝试在训练过程中进行渐变裁剪，以限制任何单个样本的影响力。",
    "tldr": "该论文研究了大型ASR模型中的意外记忆问题，并提出了一种简单的审计方法来测量记忆效应。研究发现，目前的ASR模型存在记忆现象，为此提出了使用渐变裁剪进行训练来缓解记忆问题。",
    "en_tdlr": "This paper investigates the issue of unintended memorization in large ASR models and proposes a simple auditing method to measure the memorization effect. The study found that current ASR models exhibit memorization and suggests using gradient clipping during training to mitigate this issue."
}