{
    "title": "Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])",
    "abstract": "Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id",
    "link": "http://arxiv.org/abs/2310.04314",
    "context": "Title: Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])\nAbstract: Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id",
    "path": "papers/23/10/2310.04314.json",
    "total_tokens": 851,
    "translated_title": "有限监督下的潜在图推理",
    "translated_abstract": "潜在图推理 (LGI) 的目标是从数据特征中同时学习潜在的图结构和节点表示。然而，现有的LGI方法通常遭受督导匮乏问题，导致大量的边权重在没有语义监督的情况下学习，不能对训练损失做出贡献。因此，这些缺乏监督的权重可能决定测试样本的预测，但并不一定是语义上最优的，导致泛化能力差。本文观察到这个问题实际上是由于图稀疏化操作导致的，严重破坏了关键节点和标记节点之间建立的重要连接。为了解决这个问题，我们提出恢复受损的相似性，并为更好的LGI补充缺失的监督。关键挑战在于识别关键节点并恢复损坏的相似性。我们首先将关键节点定义为$k$-hop饥饿节点，可以通过饥饿状况计算获得。",
    "tldr": "本文研究了有限监督下的潜在图推理问题并提出了一种恢复关键连接和补充缺失监督的方法。",
    "en_tdlr": "This paper investigates the problem of latent graph inference with limited supervision and proposes a method to restore critical connections and replenish missing supervision."
}