{
    "title": "Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control. (arXiv:2310.11138v1 [cs.LG])",
    "abstract": "The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble",
    "link": "http://arxiv.org/abs/2310.11138",
    "context": "Title: Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control. (arXiv:2310.11138v1 [cs.LG])\nAbstract: The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble",
    "path": "papers/23/10/2310.11138.json",
    "total_tokens": 834,
    "translated_title": "保持各种轨迹：促进连续控制中探索集合策略的方法",
    "translated_abstract": "深度强化学习（DRL）与集合方法的结合已被证明在解决复杂的顺序决策问题上非常有效。这一成功主要归功于多模型的利用，它增强了策略的鲁棒性和值函数估计的准确性。然而，目前关于现有集合强化学习方法的实证成功的分析还很有限。我们的新分析揭示了之前的集合DRL算法的样本效率可能受到不够多样化的子策略的限制。在这些发现的启发下，我们的研究引入了一种新的集合强化学习算法，称为Trajectories-awarE Ensemble exploratioN (TEEN)。TEEN的主要目标是在提高预期回报的同时促进更多样化的轨迹。通过大量实验，我们证明TEEN不仅增强了集合策略的样本多样性",
    "tldr": "本研究提出了一种新的集合强化学习算法TEEN，旨在通过促进更多样化的轨迹来提高预期回报的同时增强集合策略的样本多样性。"
}