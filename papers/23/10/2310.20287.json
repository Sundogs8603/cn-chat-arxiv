{
    "title": "Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])",
    "abstract": "Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer",
    "link": "http://arxiv.org/abs/2310.20287",
    "context": "Title: Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])\nAbstract: Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer",
    "path": "papers/23/10/2310.20287.json",
    "total_tokens": 929,
    "translated_title": "通过重置深度集合代理实现高样本效率和安全的深度强化学习",
    "translated_abstract": "深度强化学习通过与深度神经网络集成作为函数逼近器，在解决复杂任务方面取得了显著的成功。然而，对深度神经网络的依赖引入了一种被称为“优先级偏见”的新挑战，即这些函数逼近器倾向于优先考虑早期的经验，导致过拟合。为了缓解这种优先级偏见，已经提出了一种重置方法，该方法以保留回放缓冲区的方式对深度强化学习代理的部分或全部进行周期性重置。然而，使用重置方法后可能出现性能崩溃，这从安全强化学习和遗憾最小化的角度来看可能是有害的。在本文中，我们提出了一种新的基于重置的方法，利用深度集合学习来解决普通重置方法的局限性，并增强样本效率。通过包括安全强化学习领域在内的各种实验对所提出的方法进行了评估。",
    "tldr": "这项研究提出一种基于重置的深度集合学习方法，以提高深度强化学习的样本效率和解决复位方法的局限性。实验结果表明，该方法在安全强化学习领域取得了良好的性能。",
    "en_tdlr": "This research introduces a reset-based deep ensemble learning method to improve sample efficiency and address the limitations of the vanilla reset method in deep reinforcement learning. Experimental results demonstrate its good performance in the domain of safe RL."
}