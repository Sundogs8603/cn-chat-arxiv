{
    "title": "Behavior Alignment via Reward Function Optimization. (arXiv:2310.19007v2 [cs.LG] UPDATED)",
    "abstract": "Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \\emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend",
    "link": "http://arxiv.org/abs/2310.19007",
    "context": "Title: Behavior Alignment via Reward Function Optimization. (arXiv:2310.19007v2 [cs.LG] UPDATED)\nAbstract: Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \\emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend",
    "path": "papers/23/10/2310.19007.json",
    "total_tokens": 891,
    "translated_title": "行为一致性的奖励函数优化",
    "translated_abstract": "设计奖励函数来有效地引导强化学习代理向特定行为的方向发展是一项复杂的任务。这是具有挑战性的，因为它要求确定非稀疏的奖励结构，并避免无意中引起不希望的行为。简单地修改奖励结构以提供更密集和更频繁的反馈可能会导致意外的结果，并促使行为与设计者的预期目标不一致。虽然通常建议使用基于潜力的奖励设计来解决这个问题，但我们系统地研究了它在许多情况下会严重影响性能的问题。为了解决这些问题，我们提出了一个新的框架，使用双层目标来学习\"行为一致性奖励函数\"。这些函数通过将反映设计师启发和领域知识的辅助奖励与环境的主要奖励相结合，自动确定了最有效的混合方式。",
    "tldr": "本论文介绍了一种通过优化奖励函数来实现行为一致性的新框架。该框架通过将设计师的启发和领域知识与环境的主要奖励相结合，自动确定了最有效的奖励结构，以避免引起不希望的行为。"
}