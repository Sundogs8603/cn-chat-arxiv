{
    "title": "On discretisation drift and smoothness regularisation in neural network training. (arXiv:2310.14036v1 [stat.ML])",
    "abstract": "The deep learning recipe of casting real-world problems as mathematical optimisation and tackling the optimisation by training deep neural networks using gradient-based optimisation has undoubtedly proven to be a fruitful one. The understanding behind why deep learning works, however, has lagged behind its practical significance. We aim to make steps towards an improved understanding of deep learning with a focus on optimisation and model regularisation. We start by investigating gradient descent (GD), a discrete-time algorithm at the basis of most popular deep learning optimisation algorithms. Understanding the dynamics of GD has been hindered by the presence of discretisation drift, the numerical integration error between GD and its often studied continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretisation drift. Unlike the NGF, these new flows can be used to describe ",
    "link": "http://arxiv.org/abs/2310.14036",
    "context": "Title: On discretisation drift and smoothness regularisation in neural network training. (arXiv:2310.14036v1 [stat.ML])\nAbstract: The deep learning recipe of casting real-world problems as mathematical optimisation and tackling the optimisation by training deep neural networks using gradient-based optimisation has undoubtedly proven to be a fruitful one. The understanding behind why deep learning works, however, has lagged behind its practical significance. We aim to make steps towards an improved understanding of deep learning with a focus on optimisation and model regularisation. We start by investigating gradient descent (GD), a discrete-time algorithm at the basis of most popular deep learning optimisation algorithms. Understanding the dynamics of GD has been hindered by the presence of discretisation drift, the numerical integration error between GD and its often studied continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretisation drift. Unlike the NGF, these new flows can be used to describe ",
    "path": "papers/23/10/2310.14036.json",
    "total_tokens": 863,
    "translated_title": "关于神经网络训练中的离散漂移和平滑正则化的研究",
    "translated_abstract": "将现实世界问题转化为数学优化，并通过使用基于梯度的优化方法来训练深度神经网络的深度学习方法已被证明是有效的。然而，深度学习的具体工作原理的理解落后于其实际意义。我们的目标是在优化和模型正则化方面迈出改进的步伐。我们首先研究了梯度下降（GD），这是大多数流行的深度学习优化算法的离散时间算法的基础。理解GD的动力学特性一直受到离散漂移的阻碍，即GD与其常常研究的连续时间对应物——负梯度流（NGF）之间的数值积分误差。为了扩展研究GD的工具，我们推导了考虑离散漂移的新型连续时间流动。与NGF不同，这些新的流动可以用来描述离散漂移。",
    "tldr": "本研究旨在提高对深度学习优化和模型正则化的理解，研究了梯度下降算法的动力学特性和离散漂移问题。",
    "en_tdlr": "This study aims to enhance the understanding of deep learning optimization and model regularization by investigating the dynamics of gradient descent algorithm and addressing the issue of discretization drift."
}