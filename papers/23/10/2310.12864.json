{
    "title": "The Locality and Symmetry of Positional Encodings. (arXiv:2310.12864v1 [cs.CL])",
    "abstract": "Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in \\textbf{Bidirectional Masked Language Models} (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models. The code is available at \\faGi",
    "link": "http://arxiv.org/abs/2310.12864",
    "context": "Title: The Locality and Symmetry of Positional Encodings. (arXiv:2310.12864v1 [cs.CL])\nAbstract: Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in \\textbf{Bidirectional Masked Language Models} (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models. The code is available at \\faGi",
    "path": "papers/23/10/2310.12864.json",
    "total_tokens": 850,
    "translated_title": "位置编码的局部性和对称性",
    "translated_abstract": "位置编码（PE）被用于在基于Transformer的语言模型中注入单词顺序信息。虽然它们可以显著提升句子表示的质量，但它们在语言模型中的具体贡献尚未完全理解，特别是考虑到最近的研究发现各种位置编码对单词顺序不敏感。在这项工作中，我们对双向蒙版语言模型（BERT风格）中的位置编码进行了系统研究，补充了现有工作的三个方面：（1）通过确定局部性和对称性这两个常见属性，揭示出PE的核心功能；（2）展示了这两个属性与下游任务的性能密切相关；（3）通过引入两个新的探测任务，量化了当前PE的不足。我们相信这些结果是为基于Transformer的语言模型开发更好的PE的基础。代码可在 \\faGITHUB 上获取。",
    "tldr": "本研究对位置编码中的局部性和对称性进行了系统研究，发现其核心功能和与下游任务性能之间的密切关联，同时揭示了当前位置编码的不足之处。",
    "en_tdlr": "This study systematically investigates the locality and symmetry of positional encodings, revealing their core functions, their close correlation with downstream task performance, and the weaknesses of current encodings."
}