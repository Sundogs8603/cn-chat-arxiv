{
    "title": "CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])",
    "abstract": "The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota",
    "link": "http://arxiv.org/abs/2310.16225",
    "context": "Title: CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])\nAbstract: The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota",
    "path": "papers/23/10/2310.16225.json",
    "total_tokens": 940,
    "translated_title": "CleanCoNLL: 一种几乎无噪声的命名实体识别数据集",
    "translated_abstract": "CoNLL-03语料库被认为是最著名和广泛使用的命名实体识别（NER）基准数据集。然而，之前的研究发现了大量的注释错误、不完整性和数据不一致性。这给客观比较NER方法和分析其错误带来了挑战，因为目前最先进的模型在CoNLL-03中达到的F1分数与估计的噪声水平相当甚至更高。为了解决这个问题，我们通过自动一致性检查辅助的全面重标记工作来纠正英文CoNLL-03中所有标签的7.0％。我们的工作还为了更好地解释NER标签和作为附加保证注释质量而添加了一个实体链接注释层。我们的实验证明，最先进的方法不仅在我们的数据上达到了显著更高的F1分数（97.1％），而且关键是正确预测被错误地计算为错误的比例由于注释的缺失得到了改善。",
    "tldr": "CleanCoNLL是一种几乎无噪声的命名实体识别数据集，通过全面重标记和自动一致性检查来纠正CoNLL-03中的注释错误，提高了最先进方法的F1分数，并减少了因注释缺失而误判的情况。",
    "en_tdlr": "CleanCoNLL is a nearly noise-free named entity recognition dataset that corrects annotation errors in CoNLL-03 through comprehensive relabeling and automatic consistency checking, improving the F1 scores of state-of-the-art approaches and reducing false predictions due to missing annotations."
}