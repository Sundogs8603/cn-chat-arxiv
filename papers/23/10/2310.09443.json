{
    "title": "G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations. (arXiv:2310.09443v1 [cs.AR])",
    "abstract": "To break the GPU memory wall for scaling deep learning workloads, a variety of architecture and system techniques have been proposed recently. Their typical approaches include memory extension with flash memory and direct storage access. However, these techniques still suffer from suboptimal performance and introduce complexity to the GPU memory management, making them hard to meet the scalability requirement of deep learning workloads today. In this paper, we present a unified GPU memory and storage architecture named G10 driven by the fact that the tensor behaviors of deep learning workloads are highly predictable. G10 integrates the host memory, GPU memory, and flash memory into a unified memory space, to scale the GPU memory capacity while enabling transparent data migrations. Based on this unified GPU memory and storage architecture, G10 utilizes compiler techniques to characterize the tensor behaviors in deep learning workloads. Therefore, it can schedule data migrations in advan",
    "link": "http://arxiv.org/abs/2310.09443",
    "context": "Title: G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations. (arXiv:2310.09443v1 [cs.AR])\nAbstract: To break the GPU memory wall for scaling deep learning workloads, a variety of architecture and system techniques have been proposed recently. Their typical approaches include memory extension with flash memory and direct storage access. However, these techniques still suffer from suboptimal performance and introduce complexity to the GPU memory management, making them hard to meet the scalability requirement of deep learning workloads today. In this paper, we present a unified GPU memory and storage architecture named G10 driven by the fact that the tensor behaviors of deep learning workloads are highly predictable. G10 integrates the host memory, GPU memory, and flash memory into a unified memory space, to scale the GPU memory capacity while enabling transparent data migrations. Based on this unified GPU memory and storage architecture, G10 utilizes compiler techniques to characterize the tensor behaviors in deep learning workloads. Therefore, it can schedule data migrations in advan",
    "path": "papers/23/10/2310.09443.json",
    "total_tokens": 896,
    "translated_title": "G10：通过智能张量迁移实现高效的统一GPU内存和存储架构",
    "translated_abstract": "为了突破GPU内存墙，以扩展深度学习工作负载规模，近期提出了各种架构和系统技术。它们的典型方法包括使用闪存扩展内存和直接存储访问。然而，这些技术仍然存在性能不佳以及引入GPU内存管理复杂性的问题，这使得它们难以满足当今深度学习工作负载的可扩展性要求。在本文中，我们提出了一种名为G10的统一GPU内存和存储架构，该架构受到深度学习工作负载的张量行为高度可预测的事实推动。G10将主机内存、GPU内存和闪存内存集成到统一的内存空间中，以扩展GPU内存容量并实现透明的数据迁移。基于这种统一的GPU内存和存储架构，G10利用编译器技术对深度学习工作负载中的张量行为进行特征化。因此，它可以提前调度数据迁移。",
    "tldr": "G10是一个统一的GPU内存和存储架构，通过智能张量迁移来实现高效的扩展GPU内存容量并满足深度学习工作负载的可扩展性要求。"
}