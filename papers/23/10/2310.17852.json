{
    "title": "Function Space Bayesian Pseudocoreset for Bayesian Neural Networks. (arXiv:2310.17852v1 [cs.LG])",
    "abstract": "A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass sev",
    "link": "http://arxiv.org/abs/2310.17852",
    "context": "Title: Function Space Bayesian Pseudocoreset for Bayesian Neural Networks. (arXiv:2310.17852v1 [cs.LG])\nAbstract: A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass sev",
    "path": "papers/23/10/2310.17852.json",
    "total_tokens": 927,
    "translated_title": "函数空间贝叶斯伪核心集用于贝叶斯神经网络",
    "translated_abstract": "贝叶斯伪核心集是一个紧凑的合成数据集，总结了大规模数据集的基本信息，因此可以作为可扩展贝叶斯推断的代理数据集。通常，通过最小化伪核心集后验条件和完整数据集后验条件之间的差异度量来构建贝叶斯伪核心集。然而，评估差异度量可能具有挑战性，尤其是对于具有高维参数的深度神经网络等模型。在本文中，我们提出了一种在函数空间上操作的新颖贝叶斯伪核心集构建方法。与以往的方法不同，以模型参数（权重）的空间构建和匹配核心集和完整数据后验，我们的方法在函数空间上构建核心集后验的变分近似，并在函数空间中将其与完整数据后验匹配。通过直接在函数空间中工作，我们的方法可以绕过一些计算和评估的困难。",
    "tldr": "本论文提出了一种在函数空间上操作的新颖贝叶斯伪核心集构建方法，通过构建核心集后验的变分近似并在函数空间中将其与完整数据后验匹配，实现了对深度神经网络等高维模型的贝叶斯推断的可扩展性。"
}