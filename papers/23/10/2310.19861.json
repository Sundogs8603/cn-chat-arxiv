{
    "title": "Posterior Sampling for Competitive RL: Function Approximation and Partial Observation. (arXiv:2310.19861v1 [cs.LG])",
    "abstract": "This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms",
    "link": "http://arxiv.org/abs/2310.19861",
    "context": "Title: Posterior Sampling for Competitive RL: Function Approximation and Partial Observation. (arXiv:2310.19861v1 [cs.LG])\nAbstract: This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms",
    "path": "papers/23/10/2310.19861.json",
    "total_tokens": 981,
    "translated_title": "后验采样在竞争性强化学习中的应用：函数近似和部分观测",
    "translated_abstract": "本文研究了在一般函数近似的背景下，用于竞争性强化学习的后验采样算法。针对零和马尔可夫博弈中的自对弈和对抗学习两个关键情景，我们首先提出了自对弈和对抗性广义艾略特系数(GEC)作为函数近似的复杂度度量，并捕捉博弈中的探索-利用平衡。基于自对弈GEC，我们提出了一种基于模型的自对弈后验采样方法，以控制两个玩家学习纳什均衡，可以成功处理状态的部分可观测性。此外，我们确定了一套与对手的对抗策略相适应的部分可观测博弈模型。结合对抗GEC，我们提出了一种基于模型的后验采样方法，用于学习具有潜在部分可观测性的对抗性博弈模型。我们进一步为所提出的算法提供了低遗憾界限。",
    "tldr": "本文研究了使用后验采样算法在竞争性强化学习中的应用。通过引入自对弈和对抗性广义艾略特系数，提出了用于探索-利用平衡的模型方法，并且成功处理了状态的部分可观测性。同时，提出了学习具有潜在部分可观测性的对抗性博弈模型的后验采样方法，并给出了低遗憾界限。",
    "en_tdlr": "This paper investigates the application of posterior sampling algorithms in competitive reinforcement learning. The proposed model-based methods successfully handle partial observability of states and provide low regret bounds."
}