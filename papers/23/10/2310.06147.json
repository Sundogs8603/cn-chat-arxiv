{
    "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])",
    "abstract": "Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua",
    "link": "http://arxiv.org/abs/2310.06147",
    "context": "Title: Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])\nAbstract: Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua",
    "path": "papers/23/10/2310.06147.json",
    "total_tokens": 997,
    "translated_title": "在LLM时代的强化学习：什么是必要的？什么是需要的？强化学习对RLHF、Prompting等的视角分析",
    "translated_abstract": "最近大型语言模型（LLMs）的进展引起了广泛关注，并取得了ChatGPT和GPT-4等成功产品。它们在遵循指令并提供无害、有帮助和诚实（3H）回答方面的熟练程度主要归功于人类反馈强化学习（RLHF）技术。本文旨在将传统强化学习的研究与LLM研究中使用的RL技术联系起来，通过讨论RL在何时、何地和如何优秀，解释这一技术的神秘性。此外，我们探讨了潜在的未来领域，这些领域可能会从RLHF研究中获益或为其做出贡献。重点内容：1. RLHF是带有离线示范数据的在线逆向RL。2. RLHF比SFT更好，因为模仿学习（和逆向RL）比行为克隆（BC）更好，能够缓解累积误差问题。3. RLHF中的RM步骤产生了昂贵的人类反馈的代理，这样的见解可以推广到其他LLM任务，例如提示评估。",
    "tldr": "这篇论文介绍了在LLM时代中，强化学习在RLHF、Prompting等方面的应用，并讨论了其中的创新和贡献。",
    "en_tdlr": "This paper explores the use of reinforcement learning in RLHF, prompting, and other aspects in the era of LLMs, highlighting the innovations and contributions made in these areas."
}