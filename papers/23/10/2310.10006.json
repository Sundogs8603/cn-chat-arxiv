{
    "title": "Implicit regularization via soft ascent-descent. (arXiv:2310.10006v1 [stat.ML])",
    "abstract": "As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking \"flat\" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold \"ascent-descent\" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flo",
    "link": "http://arxiv.org/abs/2310.10006",
    "context": "Title: Implicit regularization via soft ascent-descent. (arXiv:2310.10006v1 [stat.ML])\nAbstract: As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking \"flat\" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold \"ascent-descent\" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flo",
    "path": "papers/23/10/2310.10006.json",
    "total_tokens": 884,
    "translated_title": "通过软上升-下降实现隐式正则化",
    "translated_abstract": "随着模型变得越来越大和复杂，通过最小的试错来实现更好的离线泛化对机器学习工作流程的可靠性和经济性至关重要。作为寻求“平坦”局部最小值的众所周知的启发式方法的代理，梯度正则化是一条自然的途径，一阶近似方法如Floding和Sharpness-Aware Minimization (SAM) 已经受到了相当大的关注，但它们的性能严重依赖于超参数（洪水阈值和邻域半径），这些超参数不容易事先确定。为了开发一个对错误超参数更具韧性的过程，受Flooding中使用的硬阈值“上升-下降”开关装置的启发，我们提出了一种软化的逐点机制，称为SoftAD，它对边界上的点进行降权，限制异常值的影响，并保留上升-下降效应。我们将形式的平稳性保证与Flooding进行对比。",
    "tldr": "本研究提出了一种通过软化的逐点机制（SoftAD）来实现正则化的方法，该方法具有更好的鲁棒性，可以减少超参数的影响，并保留上升-下降效应。",
    "en_tdlr": "This study proposes a method for regularization called SoftAD, which uses a softened, pointwise mechanism to reduce the impact of hyperparameters and retains the ascent-descent effect, leading to better robustness."
}