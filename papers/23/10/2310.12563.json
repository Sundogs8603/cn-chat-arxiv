{
    "title": "Approximate information maximization for bandit games. (arXiv:2310.12563v1 [stat.ML])",
    "abstract": "Entropy maximization and free energy minimization are general physical principles for modeling the dynamics of various physical systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al., 2000), and navigation in random environments using information maximization (Vergassola et al., 2007). Built on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated analytical physics-based representation of an entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the two-armed bandit",
    "link": "http://arxiv.org/abs/2310.12563",
    "context": "Title: Approximate information maximization for bandit games. (arXiv:2310.12563v1 [stat.ML])\nAbstract: Entropy maximization and free energy minimization are general physical principles for modeling the dynamics of various physical systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al., 2000), and navigation in random environments using information maximization (Vergassola et al., 2007). Built on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated analytical physics-based representation of an entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the two-armed bandit",
    "path": "papers/23/10/2310.12563.json",
    "total_tokens": 879,
    "translated_title": "适用于强盗游戏的近似信息最大化方法",
    "translated_abstract": "熵最大化和自由能最小化是用于模拟各种物理系统动态的一般物理原理。其中包括使用自由能原理对大脑内的决策进行建模，使用信息瓶颈原理对访问隐藏变量时优化准确性和复杂性的权衡，以及使用信息最大化进行随机环境导航。基于这一原理，我们提出了一种新的强盗算法类别，通过最大化系统中一个关键变量的信息近似来进行优化。为此，我们开发了一个基于物理的近似分析熵的表示方法，以预测每个动作的信息增益，并贪婪地选择信息增益最大的动作。这种方法在传统强盗设置中表现出很强的性能。受到其经验性成功的启发，我们证明了其对于两臂强盗问题的渐近最优性。",
    "tldr": "本论文提出了一种基于近似信息最大化的强盗游戏算法，通过最大化关键变量的信息近似值来进行优化，在传统强盗设置中表现出很强的性能，并证明了其对于两臂强盗问题的渐近最优性。",
    "en_tdlr": "This paper proposes a bandit game algorithm based on approximate information maximization, which optimizes by maximizing the approximate value of the key variable's information. It shows strong performance in classical bandit settings and is proven to be asymptotically optimal for the two-armed bandit problem."
}