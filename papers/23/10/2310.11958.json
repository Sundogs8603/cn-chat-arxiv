{
    "title": "Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])",
    "abstract": "We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o",
    "link": "http://arxiv.org/abs/2310.11958",
    "context": "Title: Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])\nAbstract: We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o",
    "path": "papers/23/10/2310.11958.json",
    "total_tokens": 931,
    "translated_title": "用勺子舀空海洋：我们应该编辑模型吗？",
    "translated_abstract": "我们对直接模型编辑作为纠正LLM生成中的事实错误的方法提出了质疑。我们将模型编辑与追求更明确目标的三种类似但不同的方法进行对比：（1）基于检索的架构，将事实记忆与LLMs所体现的推理和语言能力解耦；（2）概念擦除方法，旨在防止生成文本中的系统偏见；（3）归属方法，旨在将生成结果与已确定的文本来源连接起来。我们认为，不能将直接模型编辑作为解决LLMs固有缺点的系统性方法，并且虽然它在改进模型可解释性方面具有潜力，但它通过加强模型可信性的观念而存在风险。我们呼吁在LLM部署过程中谨慎推广和应用模型编辑，并负责任地限制LLMs的使用案例，以不依赖....",
    "tldr": "这项研究质疑了直接模型编辑作为纠正LLM生成中事实错误的方法，并提出了与之类似但更为明确的三种替代方法。虽然模型编辑在提升模型可解释性方面有潜力，但不能被视为解决LLMs固有缺点的系统性方法，因为它存在加强模型可信性观念的风险。",
    "en_tdlr": "This study questions the effectiveness of direct model editing in correcting factual errors in LLM generations and proposes three alternative approaches. While model editing has potential in improving model explainability, it should not be considered a systematic solution for the inherent limitations of LLMs as it may reinforce the notion of model trustworthiness."
}