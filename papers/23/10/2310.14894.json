{
    "title": "Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations",
    "abstract": "Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based",
    "link": "https://arxiv.org/abs/2310.14894",
    "context": "Title: Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations\nAbstract: Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based",
    "path": "papers/23/10/2310.14894.json",
    "total_tokens": 882,
    "translated_title": "本地通用解释器（LUX）-- 一种基于规则的解释器，具有事实、反事实和视觉解释",
    "translated_abstract": "可解释的人工智能（XAI）是近年来最被广泛发展的人工智能领域之一。它也是最分散的领域之一，有多种方法专注于解释的不同方面。这使得一次性以紧凑和一致的方式获得完整的解释变得困难。为了解决这个问题，我们提出了本地通用解释器（LUX），它是一种基于规则的解释器，可以生成事实、反事实和视觉解释。它基于修改后的决策树算法，允许斜交和集成特征重要性XAI方法，如SHAP或LIME。与其他算法相反，它不使用数据生成，而是专注于选择以高密度簇形式出现的真实数据的局部概念，这些局部概念对解释模型的决策边界形成最大的影响。我们在真实和合成数据集上测试了我们的方法，并将其与最先进的基于规则的方法进行了比较。",
    "tldr": "LUX是一种基于规则的解释器，可以生成事实、反事实和视觉解释，通过选择高密度簇形式的局部概念来形成决策边界。",
    "en_tdlr": "LUX is a rule-based explainer that generates factual, counterfactual, and visual explanations, by selecting high-density clusters of local concepts to form decision boundaries."
}