{
    "title": "How Re-sampling Helps for Long-Tail Learning?. (arXiv:2310.18236v1 [cs.CV])",
    "abstract": "Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other n",
    "link": "http://arxiv.org/abs/2310.18236",
    "context": "Title: How Re-sampling Helps for Long-Tail Learning?. (arXiv:2310.18236v1 [cs.CV])\nAbstract: Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other n",
    "path": "papers/23/10/2310.18236.json",
    "total_tokens": 912,
    "translated_title": "如何通过重新采样改善长尾学习？",
    "translated_abstract": "近年来，由于极度不平衡的数据集所带来的挑战，长尾学习受到了广泛关注。在这些数据集中，只有少数类别（称为头部类别）具有足够数量的训练样本，而其余类别（称为尾部类别）在训练数据中很少见。重新采样是一种经典且广泛使用的方法，用于解决类别不平衡问题。然而，最近的研究声称，在现代长尾学习任务中，重新采样对性能的提升微不足道。本文旨在系统地研究这一现象。我们的研究表明，当训练图像不包含语义上无关的背景时，重新采样可以显著提高泛化能力。然而，在其他场景下，它可能会学习到与目标标签无关的意外虚假相关性。我们设计了两个同质数据集上的实验，一个包含无关背景，另一个不包含。",
    "tldr": "本文研究了如何通过重新采样改善长尾学习的效果。研究发现，当训练图像不包含无关背景时，重新采样可以显著提高泛化能力，但在其他场景下可能会导致学习到与目标标签无关的虚假相关性。",
    "en_tdlr": "This paper investigates how re-sampling can help improve long-tail learning. The research shows that re-sampling can significantly improve generalization when training images do not contain irrelevant backgrounds, but it may learn spurious correlations between irrelevant contexts and target labels in other scenarios."
}