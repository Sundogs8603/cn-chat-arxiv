{
    "title": "Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])",
    "abstract": "Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n",
    "link": "http://arxiv.org/abs/2310.08069",
    "context": "Title: Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])\nAbstract: Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n",
    "path": "papers/23/10/2310.08069.json",
    "total_tokens": 847,
    "translated_title": "重新思考代码搜索中的负样本对",
    "translated_abstract": "最近，对比学习成为细化代码搜索模型以提高软件开发效率和效果的关键组成部分。它将正样本代码片段聚集在一起，同时将与搜索查询不相关的负样本推开。在对比学习中，InfoNCE是最常用的损失函数，因为它具有更好的性能。然而，InfoNCE负样本存在以下问题可能会损害其表示学习的效果：1）由于重复，大型代码库中存在虚假负样本。2）未能明确区分负样本的潜在相关性。例如，对于快速排序算法查询，冒泡排序算法示例要比文件保存函数“更负面”。在本文中，我们通过提出一种简单而有效的Soft-InfoNCE损失来解决上述问题。在我们提出的损失函数中，我们采用了三种方法来估计权重...",
    "tldr": "本文提出了一种简单而有效的Soft-InfoNCE损失函数，通过在InfoNCE中插入权重项来解决代码搜索中负样本的问题，包括大型代码库中的虚假负样本和未能区分负样本的潜在相关性。"
}