{
    "title": "Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])",
    "abstract": "Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \\",
    "link": "http://arxiv.org/abs/2310.02702",
    "context": "Title: Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])\nAbstract: Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \\",
    "path": "papers/23/10/2310.02702.json",
    "total_tokens": 830,
    "translated_title": "通过最大化梯度多样性来解决联邦优化中的混合异构性",
    "translated_abstract": "联邦学习是一种分布式机器学习范式，其中数据样本被分散和分布在多个客户端之间。这些样本可能表现出统计异质性，即数据分布在客户端之间不是独立和相同的。此外，系统异质性，即客户端计算能力的变化，会给联邦学习带来偏差。统计和系统异质性的综合效应可以显著降低联邦优化的效率。然而，混合异构性的影响并没有得到严谨的讨论。本文通过研究服务器端优化，探讨了混合异构性如何影响联邦优化。理论结果表明，在服务器更新方向上自适应地最大化梯度多样性可以帮助减轻混合异构性的潜在负面影响。为此，我们引入了一种新颖的基于服务器端梯度的优化器。",
    "tldr": "本文探讨了混合异构性如何影响联邦优化，并提出了一种通过最大化梯度多样性来减轻混合异构性负面影响的方法。"
}