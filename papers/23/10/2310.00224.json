{
    "title": "Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])",
    "abstract": "Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse mod",
    "link": "http://arxiv.org/abs/2310.00224",
    "context": "Title: Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])\nAbstract: Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse mod",
    "path": "papers/23/10/2310.00224.json",
    "total_tokens": 850,
    "translated_title": "Steered Diffusion: 一种广义的插件式条件图像合成框架",
    "translated_abstract": "条件生成模型通常需要大量的注释训练集才能实现高质量的合成。因此，设计能够执行插件式合成的模型引起了很大的兴趣，即使用预定义或预训练的模型来指导生成过程（例如使用语言），而该模型并没有明确训练在生成任务上。然而，这种指导通常只对合成高级语义有用，而不是编辑图像到图像转换任务中的细粒度细节。为了解决这个问题，并借助最近基于扩散的生成模型提供的强大细粒度生成控制能力，我们引入了Steered Diffusion，这是一个通用的框架，用于使用为无条件生成而训练的扩散模型进行逼真的零样本条件图像生成。其核心思想是通过设计使用预训练的逆模型损失来在推理时指导扩散模型的图像生成。",
    "tldr": "Steered Diffusion是一个通用的框架，利用近期基于扩散的生成模型的细粒度生成控制能力，实现了零样本条件图像生成的高质量合成。",
    "en_tdlr": "Steered Diffusion is a generalized framework that utilizes the fine-grained generative control offered by recent diffusion-based generative models to achieve high-quality synthesis of zero-shot conditional image generation."
}