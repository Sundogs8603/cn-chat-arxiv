{
    "title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])",
    "abstract": "We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity",
    "link": "http://arxiv.org/abs/2310.12362",
    "context": "Title: REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])\nAbstract: We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity",
    "path": "papers/23/10/2310.12362.json",
    "total_tokens": 950,
    "translated_title": "REMARK-LLM:一种用于生成大型语言模型的鲁棒高效的水印框架",
    "translated_abstract": "我们提出了一种名为REMARK-LLM的新型高效、强鲁棒性的水印框架，专为大型语言模型（LLM）生成的文本设计。使用LLMs合成类似人类的内容需要大量的计算资源和广泛的数据集，涵盖了重要的知识产权（IP）。然而，生成的内容容易受到恶意利用，包括垃圾邮件和抄袭。为了解决这些挑战，REMARK-LLM提出了三个新的组成部分：（i）基于学习的消息编码模块，将二进制签名注入LLM生成的文本中；（ii）重新参数化模块，将消息编码的密集分布转换为水印文本标记的稀疏分布；（iii）专门用于签名提取的解码模块；此外，我们引入了一种优化的波束搜索算法，以保证生成内容的连贯性和一致性。REMARK-LLM经过严格的训练，以鼓励语义完整性的保留。",
    "tldr": "REMARK-LLM是一种针对生成大型语言模型的文本的鲁棒高效的水印框架，通过学习-based消息编码、重新参数化和解码模块以及优化的波束搜索算法来保护生成内容的完整性和防止恶意利用。",
    "en_tdlr": "REMARK-LLM is a robust and efficient watermarking framework designed for texts generated by large language models (LLMs), aiming to protect the integrity of generated content and prevent malicious exploitation by introducing learning-based message encoding, reparameterization, and decoding modules, as well as an optimized beam search algorithm."
}