{
    "title": "Video-Helpful Multimodal Machine Translation. (arXiv:2310.20201v1 [cs.CL])",
    "abstract": "Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention mode",
    "link": "http://arxiv.org/abs/2310.20201",
    "context": "Title: Video-Helpful Multimodal Machine Translation. (arXiv:2310.20201v1 [cs.CL])\nAbstract: Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention mode",
    "path": "papers/23/10/2310.20201.json",
    "total_tokens": 953,
    "translated_title": "视频有益的多模式机器翻译",
    "translated_abstract": "现有的多模式机器翻译（MMT）数据集由图像和视频字幕或教学视频字幕组成，这些数据很少包含语言的歧义，使得视觉信息在生成适当的翻译方面不起作用。最近的工作构建了一个模糊字幕数据集来减轻这个问题，但仍然存在视频不一定有助于消除歧义的问题。我们介绍了EVA（用于模糊字幕翻译的广泛训练集和有助于视频的评估集），一个包含852k日英（Ja-En）平行字幕对、520k中英（Zh-En）平行字幕对和来自电影和电视剧的相应视频片段的MMT数据集。除了广泛的训练集外，EVA还包含一个视频有益的评估集，其中字幕是模糊的，并且视频能够保证有助于消除歧义。此外，我们提出了基于选择性注意模式的SAFA MMT模型。",
    "tldr": "该论文介绍了一个名为EVA的多模式机器翻译（MMT）数据集，包含了来自电影和电视剧的视频片段和相应的日英和中英平行字幕对，旨在解决现有数据集中视觉信息无法生成适当翻译的问题。同时，提出了基于选择性注意模式的SAFA MMT模型。",
    "en_tdlr": "This paper introduces EVA, a multimodal machine translation (MMT) dataset containing video clips and corresponding Japanese-English and Chinese-English parallel subtitle pairs collected from movies and TV episodes. The dataset aims to address the issue of visual information being ineffective in generating appropriate translations in existing datasets. Additionally, a SAFA MMT model based on selective attention mode is proposed."
}