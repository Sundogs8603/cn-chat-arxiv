{
    "title": "Heterogenous Memory Augmented Neural Networks. (arXiv:2310.10909v1 [cs.LG])",
    "abstract": "It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD condi",
    "link": "http://arxiv.org/abs/2310.10909",
    "context": "Title: Heterogenous Memory Augmented Neural Networks. (arXiv:2310.10909v1 [cs.LG])\nAbstract: It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD condi",
    "path": "papers/23/10/2310.10909.json",
    "total_tokens": 834,
    "translated_title": "异构内存增强神经网络",
    "translated_abstract": "已经证明，半参数方法，将标准神经网络与非参数组件（如外部存储模块和数据检索）相结合，在数据稀缺和超出分布范围的情况下尤其有帮助。然而，现有的半参数方法主要依赖于独立的原始数据点——这种策略在大规模计算成本和当前注意机制处理大量标记的能力方面都很困难。在本文中，我们引入了一种新颖的异构内存增强方法，该方法通过引入具有注意机制的可学习内存令牌，可以在不增加巨大计算开销的情况下有效提升性能。我们的通用方法可以与各种骨干网络（MLP、CNN、GNN和Transformer）无缝结合，以插入和播放的方式进行。我们广泛评估了我们的方法在各种基于图像和图的任务下在分布内（ID）和分布外条件下的表现。",
    "tldr": "本文介绍了一种新颖的异构内存增强方法，通过引入具有注意机制的可学习内存令牌，可以在不增加巨大计算开销的情况下有效提升性能。",
    "en_tdlr": "This paper introduces a novel approach of heterogeneous memory augmentation, which effectively boosts performance by introducing learnable memory tokens with attention mechanism without significant computational overhead."
}