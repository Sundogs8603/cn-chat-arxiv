{
    "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])",
    "abstract": "Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on n",
    "link": "http://arxiv.org/abs/2310.12442",
    "context": "Title: Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])\nAbstract: Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on n",
    "path": "papers/23/10/2310.12442.json",
    "total_tokens": 899,
    "translated_title": "高效长程Transformer：需要更多关注，但不一定在每一层都需要",
    "translated_abstract": "预训练的Transformer模型在各种自然语言处理任务中展示了卓越的性能。这些模型利用注意机制来捕捉序列中的长程和短程依赖关系。然而，全局注意机制的计算成本与序列长度呈二次关系，在具有长序列的任务中（例如8k个标记的输入）是不可承受的。尽管现有工作中建议使用稀疏注意力来提高计算效率，但它的建模能力有限，往往无法捕捉长序列中的复杂依赖关系。为了解决这个挑战，我们提出了MASFormer，一种易于实现的变种Transformer，具有混合注意范围。具体而言，MASFormer配备了全局注意力来捕捉长程依赖关系，但只在少数几层使用。对于剩余层，MASFormer只采用稀疏注意力来捕捉短程依赖关系。我们在n上的实验结果表明，MASFormer在长序列任务上具有较高的计算效率和建模能力。",
    "tldr": "提出了一种高效的长程Transformer模型MASFormer，通过在少数层使用全局注意力和在其他层使用稀疏注意力，实现了在具有长序列的任务中高效的计算和建模能力。",
    "en_tdlr": "An efficient long-range Transformer model, MASFormer, is proposed which achieves efficient computation and modeling capacity in tasks with long sequences by using full attention in a few layers and sparse attention in the remaining layers."
}