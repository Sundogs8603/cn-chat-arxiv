{
    "title": "Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])",
    "abstract": "Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as \"open the bottle cap\" and \"wipe the plate with the sponge\", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o",
    "link": "http://arxiv.org/abs/2310.11604",
    "context": "Title: Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])\nAbstract: Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as \"open the bottle cap\" and \"wipe the plate with the sponge\", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o",
    "path": "papers/23/10/2310.11604.json",
    "total_tokens": 1050,
    "translated_title": "语言模型作为零-shot轨迹生成器",
    "translated_abstract": "近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。",
    "tldr": "本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。"
}