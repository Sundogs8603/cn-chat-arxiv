{
    "title": "Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])",
    "abstract": "Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f",
    "link": "http://arxiv.org/abs/2310.10962",
    "context": "Title: Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])\nAbstract: Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f",
    "path": "papers/23/10/2310.10962.json",
    "total_tokens": 855,
    "translated_title": "带有大型语言模型的语义感知对比句子表示学习",
    "translated_abstract": "对比学习已被证明在学习更好的句子表示方面非常有效。然而，为了训练对比学习模型，需要大量带标签的句子来明确构建正负对（例如在自然语言推理数据集中）。不幸的是，获取足够高质量的标注数据既费时又耗资源，因此研究人员开始关注开发无监督句子表示学习方法。由于这些非结构化的随机抽样句子之间没有明确的关联，构建正负对可能会很困难和有问题。为了解决这些挑战，在本文中，我们提出了一种语义感知的对比句子表示框架（SemCSR）。通过利用大型语言模型（LLM）的生成和评估能力，我们可以自动构建一个高质量的自然语言推理样式语料库，而无需任何人工标注。",
    "tldr": "本论文提出了SemCSR框架，利用大型语言模型（LLM）的生成和评估能力，无需人工标注就能自动构建高质量的自然语言推理样式语料库，用于解决对比学习中的挑战。",
    "en_tdlr": "This paper proposes the SemCSR framework, which leverages the generation and evaluation capabilities of large language models (LLMs) to automatically construct a high-quality NLI-style corpus without human annotation, addressing the challenges in contrastive learning."
}