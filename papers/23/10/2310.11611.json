{
    "title": "In defense of parameter sharing for model-compression. (arXiv:2310.11611v1 [cs.LG])",
    "abstract": "When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent c",
    "link": "http://arxiv.org/abs/2310.11611",
    "context": "Title: In defense of parameter sharing for model-compression. (arXiv:2310.11611v1 [cs.LG])\nAbstract: When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent c",
    "path": "papers/23/10/2310.11611.json",
    "total_tokens": 914,
    "translated_title": "支持参数共享进行模型压缩的辩护",
    "translated_abstract": "在考虑模型架构时，有多种方法可以减少其内存占用。历史上，流行的方法包括选择较小的架构和通过剪枝创建稀疏网络。最近，随机参数共享（RPS）方法在训练开始时的模型压缩中受到了关注。在本文中，我们全面评估了在RPS、剪枝技术和构建较小模型之间的内存和准确性之间的权衡。我们的研究结果表明，在整个压缩范围内，无论是数据驱动还是模型无关，RPS始终优于/与较小模型和所有信息稍微充足的剪枝策略如MAG、SNIP、SYNFLOW和GRASP相匹配。这种优势在更高的压缩场景中尤为明显。值得注意的是，即使与高度信息充足的剪枝技术如Lottery Ticket Rewinding（LTR）相比，RPS在高压缩设置中也展现出更优异的性能。这指出了其内在性能。",
    "tldr": "本文综合评估了使用RPS、剪枝技术和构建较小模型等方法在内存和准确性之间的权衡，发现RPS在整个压缩范围内始终优于其他方法，并在高压缩场景中表现尤为突出。",
    "en_tdlr": "This paper comprehensively assesses the trade-off between memory and accuracy across methods such as RPS, pruning techniques, and building smaller models, and finds that RPS consistently outperforms other methods across the entire compression range and particularly in high compression scenarios."
}