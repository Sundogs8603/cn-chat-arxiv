{
    "title": "Intractability of Learning the Discrete Logarithm with Gradient-Based Methods. (arXiv:2310.01611v1 [cs.LG])",
    "abstract": "The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.  Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the l",
    "link": "http://arxiv.org/abs/2310.01611",
    "context": "Title: Intractability of Learning the Discrete Logarithm with Gradient-Based Methods. (arXiv:2310.01611v1 [cs.LG])\nAbstract: The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.  Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the l",
    "path": "papers/23/10/2310.01611.json",
    "total_tokens": 892,
    "translated_title": "使用基于梯度的方法学习离散对数的困难度",
    "translated_abstract": "离散对数问题是数论中的一个基本挑战，对密码协议具有重要的影响。本文研究了在有限循环群中，使用基于梯度的方法学习离散对数的奇偶比特的限制。我们的主要结果通过理论分析和实证验证，揭示了损失函数的梯度在一个固定点附近的聚集性质，独立于使用的对数的底数。这种聚集性质导致了使用基于梯度的方法高效学习离散对数的奇偶比特的能力受限，无论训练的网络架构的复杂性如何。我们的证明依赖于内积空间中的Boas-Bellman不等式，并通过某些矩阵的谱范数建立了离散对数奇偶比特函数的近似正交性。使用基于神经网络的方法的实证实验证实了我们的理论结果。",
    "tldr": "本文研究了使用梯度法学习离散对数的困难度，发现损失函数梯度在一个固定点附近聚集，且无论网络架构复杂性如何，都会导致学习离散对数奇偶比特的能力受限。",
    "en_tdlr": "This paper investigates the intractability of learning the discrete logarithm with gradient-based methods, revealing the concentration of the loss function gradient around a fixed point and the restricted ability to learn the parity bit efficiently irrespective of the network architecture complexity."
}