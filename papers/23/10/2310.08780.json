{
    "title": "\"Im not Racist but...\": Discovering Bias in the Internal Knowledge of Large Language Models. (arXiv:2310.08780v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. However, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. In this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary LLM. Our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the LLM's internal knowledge. By illuminating the biases present in LLMs and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.",
    "link": "http://arxiv.org/abs/2310.08780",
    "context": "Title: \"Im not Racist but...\": Discovering Bias in the Internal Knowledge of Large Language Models. (arXiv:2310.08780v1 [cs.CL])\nAbstract: Large language models (LLMs) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. However, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. In this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary LLM. Our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the LLM's internal knowledge. By illuminating the biases present in LLMs and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.",
    "path": "papers/23/10/2310.08780.json",
    "total_tokens": 858,
    "translated_title": "“我不是种族主义者，但是……”：揭示大型语言模型内部知识中的偏见",
    "translated_abstract": "大型语言模型（LLM）因其在不断扩展的自然语言处理任务中的出色表现而受到了极大关注。然而，这些模型被证明存在内在的社会偏见或刻板印象，这可能会对它们在许多下游应用中的性能产生不利影响。本文提出了一种全新的纯提示式方法，用于揭示任意LLM中隐藏的刻板印象。我们的方法动态生成了内部刻板印象的知识表示，从而能够识别LLM内部知识中编码的偏见。通过揭示LLM中存在的偏见并提供一种系统性的分析方法，我们的工作在推进透明度和促进自然语言处理系统的公平性方面做出了贡献。",
    "tldr": "本文介绍了一种纯提示式的方法，用于揭示大型语言模型中隐藏的刻板印象，通过动态生成内部刻板印象的知识表示，我们能够识别这些模型中存在的偏见。这项工作在推进透明度和促进自然语言处理系统的公平性方面做出了贡献。",
    "en_tdlr": "This paper introduces a novel prompt-based approach to uncover hidden biases within large language models (LLMs). By dynamically generating a knowledge representation of internal stereotypes, this approach enables the identification of biases encoded within the LLMs' internal knowledge. Contributing to transparency and fairness in natural language processing systems, this work highlights and analyzes the biases present in LLMs."
}