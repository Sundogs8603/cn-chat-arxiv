{
    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])",
    "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM ",
    "link": "http://arxiv.org/abs/2310.11511",
    "context": "Title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])\nAbstract: Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM ",
    "path": "papers/23/10/2310.11511.json",
    "total_tokens": 815,
    "translated_title": "Self-RAG: 通过自我反思学习检索、生成和评论",
    "translated_abstract": "尽管大型语言模型（LLMs）具有显著的能力，但由于它们完全依赖于它们所包含的参数化知识，因此往往会产生含有事实不准确性的响应。检索增强生成（RAG）是一种通过检索相关知识增强LM的临时方法，可以减少这些问题。然而，不加选择地检索并结合一定数量的检索段落，而不考虑检索是否必要或段落是否相关，会降低LM的多功能性或导致无效的响应生成。我们引入了一种称为Self-Reflective Retrieval-Augmented Generation （Self-RAG）的新框架，通过检索和自我反思提高LM的质量和事实性。我们的框架训练了一个单独的任意LM，它能够根据需求自适应地检索段落，并使用特殊的标记，称为反思标记，生成和反思检索的段落和自身的生成结果。",
    "tldr": "Self-RAG是一种通过检索和自我反思提高语言模型质量和事实性的框架。",
    "en_tdlr": "Self-RAG is a framework that enhances the quality and factuality of language models through retrieval and self-reflection."
}