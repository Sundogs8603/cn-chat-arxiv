{
    "title": "Composite Backdoor Attacks Against Large Language Models",
    "abstract": "arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att",
    "link": "https://arxiv.org/abs/2310.07676",
    "context": "Title: Composite Backdoor Attacks Against Large Language Models\nAbstract: arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att",
    "path": "papers/23/10/2310.07676.json",
    "total_tokens": 852,
    "translated_title": "大型语言模型的复合后门攻击",
    "translated_abstract": "大型语言模型（LLMs）在各种任务上表现出优越性能，通常作为许多研究和服务的基础模型。然而，不可信任的第三方LLMs可能会暗中为下游任务引入漏洞。本文通过后门攻击的视角探讨了LLMs的脆弱性。与现有的对LLMs的后门攻击不同，我们在不同的提示组件中分散多个触发关键词。这种复合后门攻击（CBA）被证明比仅在单个组件中植入相同的多个触发关键词更隐蔽。CBA确保只有当所有触发关键词出现时后门才会被激活。我们的实验表明，CBA在自然语言处理（NLP）和多模式任务中都是有效的。",
    "tldr": "该研究通过复合后门攻击(CBA)展示了在大型语言模型中植入多个触发关键词的方法，相较于现有方法更为隐蔽，并确保只有当所有触发关键词同时出现时后门才会被激活。"
}