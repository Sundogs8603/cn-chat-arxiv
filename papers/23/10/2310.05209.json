{
    "title": "Scaling Laws of RoPE-based Extrapolation",
    "abstract": "arXiv:2310.05209v2 Announce Type: replace-cross  Abstract: The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for",
    "link": "https://arxiv.org/abs/2310.05209",
    "context": "Title: Scaling Laws of RoPE-based Extrapolation\nAbstract: arXiv:2310.05209v2 Announce Type: replace-cross  Abstract: The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for",
    "path": "papers/23/10/2310.05209.json",
    "total_tokens": 805,
    "translated_title": "RoPE基外推的尺度律",
    "translated_abstract": "基于Rotary Position Embedding的大型语言模型（LLMs）的外推能力是目前备受关注的话题。用于解决LLMs外推问题的主流方法是通过将RoPE中的10000, $\\theta_n={10000}^{-2n/d}$，这个旋转基数，替换为更大的值，并提供更长的微调文本。本研究首先观察到，在预训练上用较小或较大的基数微调RoPE-based LLM，可以显著提高其外推性能。之后，我们提出了RoPE基外推的尺度律，这是一个从周期性的角度描述外推性能与基值以及调整上下文长度之间关系的统一框架。在这个过程中，我们还通过RoPE基外推问题的关键维度介绍了其起源。",
    "tldr": "本研究提出了RoPE基外推的尺度律，通过调整 RoPE 中的基数和微调文本长度来显著提高大型语言模型的外推性能。",
    "en_tdlr": "This study introduces the Scaling Laws of RoPE-based Extrapolation, which significantly improves the extrapolation performance of Large Language Models by adjusting the base number and fine-tuning text length in RoPE."
}