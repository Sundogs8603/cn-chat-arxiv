{
    "title": "Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])",
    "abstract": "Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.",
    "link": "http://arxiv.org/abs/2310.20072",
    "context": "Title: Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])\nAbstract: Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.",
    "path": "papers/23/10/2310.20072.json",
    "total_tokens": 815,
    "translated_title": "用指导调整实现对生成模型的自动评估",
    "translated_abstract": "自动评估自然语言生成一直以来都是NLP领域一个难以达到的目标。最近的一种方法是通过对预训练语言模型进行微调，来模拟人类在特定任务和评估标准上的判断。受到指导调整模型的泛化能力的启发，我们提出了一种基于指导调整的学习度量方法。为了测试我们的方法，我们收集了HEAP数据集，其中包含了各种自然语言生成任务和评估标准的人类判断。我们的研究结果表明，在HEAP上通过指导调整语言模型可以取得良好的评估性能，尽管有些评估标准的学习并不那么容易。此外，多任务联合训练可以进一步提高性能，对于未来缺乏人工标注数据的任务是有益的。",
    "tldr": "本论文提出了一种基于指导调整的学习度量方法，通过对预训练语言模型进行微调来实现对生成模型的自动评估。实验结果表明，这种方法在各种评估任务上取得了良好的性能，并且多任务联合训练可以进一步提高性能。",
    "en_tdlr": "This paper proposes a learned metric based on instruction tuning to automatically evaluate generative models. Experimental results demonstrate that this approach achieves good performance on various evaluation tasks and joint training on multiple tasks can further improve the performance."
}