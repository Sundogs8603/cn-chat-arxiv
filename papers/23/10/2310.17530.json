{
    "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])",
    "abstract": "Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p",
    "link": "http://arxiv.org/abs/2310.17530",
    "context": "Title: Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])\nAbstract: Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p",
    "path": "papers/23/10/2310.17530.json",
    "total_tokens": 925,
    "translated_title": "评估性别中立的预训练视觉-语言模型中的偏见和公平性",
    "translated_abstract": "已知预训练的机器学习模型会保持甚至放大数据中现有的偏见，这可能导致不公平的结果，最终影响用户体验。为了确保模型的性能不会对特定群体或人口产生歧视性行为，理解这些偏见偏向的机制至关重要。在本研究中，我们将性别偏见作为案例进行定义。我们量化了三个视觉-语言模型族群的预训练和微调后的偏见放大，并调查了这两个学习阶段之间的联系，评估了偏见放大对模型性能的影响。总的来说，我们发现预训练和微调后的偏见放大是相互独立的。然后，我们研究了在性别中立的数据上进行持续预训练的效果，发现这可以减少群体差距，即在VQAv2和检索任务中促进公平，而不会显著损害任务执行能力。",
    "tldr": "本研究评估了性别中立的预训练视觉-语言模型中的偏见和公平性，并发现预训练和微调后的偏见放大是相互独立的。此外，持续预训练对性别中立数据有利，可以在一些任务中促进公平。",
    "en_tdlr": "This study evaluates bias and fairness in gender-neutral pretrained vision-and-language models, finding that bias amplification in pretraining and after fine-tuning are independent and continuing pretraining on gender-neutral data promotes fairness in certain tasks."
}