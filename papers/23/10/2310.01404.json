{
    "title": "H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation. (arXiv:2310.01404v2 [cs.LG] UPDATED)",
    "abstract": "Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human $\\textbf{H}$and$\\textbf{-In}$formed visual representation learning framework to solve difficult $\\textbf{Dex}$terous manipulation tasks ($\\textbf{H-InDex}$) with reinforcement learning. Our framework consists of three stages: (i) pre-training representations with 3D human hand pose estimation, (ii) offline adapting representations with self-supervised keypoint detection, and (iii) reinforcement learning with exponential moving average BatchNorm. The last two stages only modify $0.36\\%$ parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study 12 challenging dexterous manipulation tasks and find that H-InDex largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code is available at https://yanjieze.com/H",
    "link": "http://arxiv.org/abs/2310.01404",
    "context": "Title: H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation. (arXiv:2310.01404v2 [cs.LG] UPDATED)\nAbstract: Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human $\\textbf{H}$and$\\textbf{-In}$formed visual representation learning framework to solve difficult $\\textbf{Dex}$terous manipulation tasks ($\\textbf{H-InDex}$) with reinforcement learning. Our framework consists of three stages: (i) pre-training representations with 3D human hand pose estimation, (ii) offline adapting representations with self-supervised keypoint detection, and (iii) reinforcement learning with exponential moving average BatchNorm. The last two stages only modify $0.36\\%$ parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study 12 challenging dexterous manipulation tasks and find that H-InDex largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code is available at https://yanjieze.com/H",
    "path": "papers/23/10/2310.01404.json",
    "total_tokens": 991,
    "translated_title": "H-InDex：基于手部信息的视觉强化学习在巧妙操纵中的应用",
    "translated_abstract": "人的手具有卓越的灵巧性，长期以来一直是机器人操纵的灵感来源。在这项工作中，我们提出了一种基于人体$\\textbf{H}$and$\\textbf{-In}$formed视觉表示学习框架，通过强化学习解决困难的$\\textbf{Dex}$terous操纵任务（$\\textbf{H-InDex}$）。我们的框架包括三个阶段：（i）使用3D人手姿势估计进行预训练表示，（ii）使用自监督关键点检测进行离线自适应表示，和（iii）使用指数加权移动平均BatchNorm进行强化学习。后两个阶段仅修改预训练表示的总参数的 $0.36\\%$，确保保留了来自预训练的知识。我们在12个具有挑战性的巧妙操纵任务上进行了实证研究，发现H-InDex明显优于强基线方法和最近的用于运动控制的视觉基础模型。 代码位于https://yanjieze.com/H",
    "tldr": "这项工作提出了一种基于人体手部信息的视觉表示学习框架H-InDex，通过强化学习解决困难的巧妙操纵任务。实证研究表明，H-InDex明显优于强基线方法和最近的视觉基础模型。"
}