{
    "title": "UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])",
    "abstract": "Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model \"UniverSLU\" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.",
    "link": "http://arxiv.org/abs/2310.02973",
    "context": "Title: UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])\nAbstract: Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model \"UniverSLU\" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.",
    "path": "papers/23/10/2310.02973.json",
    "total_tokens": 938,
    "translated_title": "UniverSLU:单个网络用于多样分类和序列生成任务的通用口语理解",
    "translated_abstract": "最近的研究表明，采用具备多任务能力的大型语言模型可以取得良好的效果。它们利用提示来引导模型的行为，并且超越了特定任务模型的性能。受此启发，我们问：我们能否构建一个单一模型来共同执行各种口语理解任务？为了解决这个问题，我们利用预训练的自动语音识别（ASR）模型，并采用不同的任务和数据集指定器作为离散提示。我们展示了我们的单一多任务学习（MTL）模型\"UniverSLU\"在12个不同的语音分类和序列生成任务上的有效性，涵盖了17个数据集和9种语言。结果表明，UniverSLU取得了有竞争力的性能，甚至超过了特定任务的模型。我们还进行了初步研究，探索了使用人类可解释的自然短语代替任务指定器作为离散提示，并测试了模型对新释义的泛化能力。",
    "tldr": "这项研究提出了一种单一的多任务学习模型\"UniverSLU\"，通过利用预训练的自动语音识别模型和不同的任务和数据集指定器作为离散提示，成功地在多个口语理解任务上取得了有竞争力的性能，并且甚至超过了特定任务的模型。",
    "en_tdlr": "This research proposes a single multi-task learning model, \"UniverSLU\", which achieves competitive performance on various spoken language understanding tasks by utilizing pre-trained automatic speech recognition models and different task and dataset specifiers as discrete prompts, even surpassing task-specific models."
}