{
    "title": "A Comprehensive Evaluation of Constrained Text Generation for Large Language Models. (arXiv:2310.16343v1 [cs.CL])",
    "abstract": "Advancements in natural language generation (NLG) and large language models (LLMs) have led to proficient text generation in various tasks. However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging. This study investigates constrained text generation for LLMs, where predefined constraints are applied during LLM's generation process. Our research examines multiple LLMs, including ChatGPT and GPT-4, categorizing constraints into lexical, structural, and relation-based types. We also present various benchmarks to facilitate fair evaluation. The study addresses some key research questions, including the extent of LLMs' compliance with constraints. Results illuminate LLMs' capacity and deficiency to incorporate constraints and provide insights for future developments in constrained text generation. Codes and datasets will be released upon acceptance.",
    "link": "http://arxiv.org/abs/2310.16343",
    "context": "Title: A Comprehensive Evaluation of Constrained Text Generation for Large Language Models. (arXiv:2310.16343v1 [cs.CL])\nAbstract: Advancements in natural language generation (NLG) and large language models (LLMs) have led to proficient text generation in various tasks. However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging. This study investigates constrained text generation for LLMs, where predefined constraints are applied during LLM's generation process. Our research examines multiple LLMs, including ChatGPT and GPT-4, categorizing constraints into lexical, structural, and relation-based types. We also present various benchmarks to facilitate fair evaluation. The study addresses some key research questions, including the extent of LLMs' compliance with constraints. Results illuminate LLMs' capacity and deficiency to incorporate constraints and provide insights for future developments in constrained text generation. Codes and datasets will be released upon acceptance.",
    "path": "papers/23/10/2310.16343.json",
    "total_tokens": 809,
    "translated_title": "对大型语言模型的约束文本生成进行了全面评估",
    "translated_abstract": "自然语言生成 (NLG) 和大型语言模型 (LLM) 的进步使得在各种任务中能够生成熟练的文本。然而，由于LLM的不透明性，将复杂的约束集成到神经文本生成中仍然具有挑战性。本研究调查了LLM的约束文本生成，其中在LLM的生成过程中应用了预定义的约束。我们的研究考察了多个LLM，包括ChatGPT和GPT-4，并将约束分为词汇、结构和关系类型。我们还提出了各种基准来促进公平评估。研究解决了一些关键研究问题，包括LLM与约束的遵守程度。结果揭示了LLM集成约束的能力和不足，并为未来发展约束文本生成提供了见解。代码和数据集将在被接受后发布。",
    "tldr": "该研究全面评估了大型语言模型在约束文本生成方面的应用，研究了LLM的能力和不足，并提供了未来发展的见解。"
}