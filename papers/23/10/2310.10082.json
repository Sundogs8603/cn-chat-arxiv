{
    "title": "A simple uniformly optimal method without line search for convex optimization. (arXiv:2310.10082v2 [math.OC] UPDATED)",
    "abstract": "Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\\\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Fi",
    "link": "http://arxiv.org/abs/2310.10082",
    "context": "Title: A simple uniformly optimal method without line search for convex optimization. (arXiv:2310.10082v2 [math.OC] UPDATED)\nAbstract: Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\\\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Fi",
    "path": "papers/23/10/2310.10082.json",
    "total_tokens": 896,
    "translated_title": "一个简单的无需线搜索的凸优化问题统一最优解法",
    "translated_abstract": "线搜索（或回溯）程序广泛应用于用于解决凸优化问题的一阶方法中，特别是那些具有未知问题参数（例如，Lipschitz常数）的问题中。本文中，我们展示了在线搜索中获得最佳收敛速度对于解决参数未先给定的凸优化问题是多余的。特别地，我们提出了一种新颖的加速梯度下降类型算法，称为自适应条件快速梯度法（AC-FGM），它能够在不需要估计全局Lipschitz常数或使用线搜索程序的情况下实现最佳的O(1/k^2)收敛速度，用于平滑凸优化。然后，我们将AC-FGM扩展到求解具有H\\\"{o}lder连续梯度的凸优化问题，并展示它在所有具有所需精度解的问题类中自动地实现了统一的最佳收敛速度。",
    "tldr": "本研究提出了一种简单的、无需线搜索的凸优化方法，能够以最佳的收敛速度解决参数未先给定的凸优化问题，并通过自适应条件快速梯度法（AC-FGM）实现了O(1/k^2)的收敛速度。这种方法还被扩展到具有H\\\"{o}lder连续梯度的凸优化问题，并且能够在所有问题类中以统一的最佳收敛速度解决。"
}