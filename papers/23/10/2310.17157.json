{
    "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along ",
    "link": "http://arxiv.org/abs/2310.17157",
    "context": "Title: Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])\nAbstract: Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along ",
    "path": "papers/23/10/2310.17157.json",
    "total_tokens": 959,
    "translated_title": "Deja Vu: 上下文稀疏性在LLMs推理中的高效应用",
    "translated_abstract": "拥有数百亿参数的大型语言模型(LLMs)引发了一波新的令人兴奋的人工智能应用。然而，它们在推理时的计算成本很高。稀疏性是降低这种成本的一种自然方法，但是现有的方法要么需要昂贵的重新训练，要么放弃了LLM的上下文学习能力，要么在现代硬件上无法提供墙上时间速度提升。我们假设上下文稀疏性，即对于给定输入而言，产生与稠密模型大致相同输出的小的、输入相关的注意力头和MLP参数集合，可以解决这些问题。我们展示了上下文稀疏性的存在，展示了它可以被准确预测，并且我们可以利用它在墙上时间上加速LLM的推理，而不会影响LLM的质量或上下文学习能力。基于这些见解，我们提出了Deja Vu，一个使用低成本算法根据每层的输入实时预测上下文稀疏性的系统。",
    "tldr": "该论文介绍了一种利用上下文稀疏性来提高大型语言模型推理效率的系统Deja Vu，通过预测输入相关的注意力头和MLP参数集合，可以在不影响模型质量和上下文学习能力的前提下加速推理过程。",
    "en_tdlr": "This paper presents Deja Vu, a system that utilizes contextual sparsity to improve the efficiency of large language model inference. By predicting input-dependent sets of attention heads and MLP parameters, it can accelerate the inference process without compromising model quality and in-context learning ability."
}