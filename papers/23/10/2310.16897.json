{
    "title": "Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])",
    "abstract": "The growing capabilities of transformer models pave the way for solving increasingly complex NLP tasks. A key to supporting application-specific requirements is the ability to fine-tune. However, compiling a fine-tuning dataset tailored to complex tasks is tedious and results in large datasets, limiting the ability to control transformer output. We present an approach in which complex tasks are divided into simpler subtasks. Multiple transformer models are fine-tuned to one subtask each, and lined up to accomplish the complex task. This simplifies the compilation of fine-tuning datasets and increases overall controllability. Using the example of reducing gender bias as a complex task, we demonstrate our approach and show that it performs better than using a single model.",
    "link": "http://arxiv.org/abs/2310.16897",
    "context": "Title: Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])\nAbstract: The growing capabilities of transformer models pave the way for solving increasingly complex NLP tasks. A key to supporting application-specific requirements is the ability to fine-tune. However, compiling a fine-tuning dataset tailored to complex tasks is tedious and results in large datasets, limiting the ability to control transformer output. We present an approach in which complex tasks are divided into simpler subtasks. Multiple transformer models are fine-tuned to one subtask each, and lined up to accomplish the complex task. This simplifies the compilation of fine-tuning datasets and increases overall controllability. Using the example of reducing gender bias as a complex task, we demonstrate our approach and show that it performs better than using a single model.",
    "path": "papers/23/10/2310.16897.json",
    "total_tokens": 797,
    "translated_title": "Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks.",
    "translated_abstract": "随着Transformer模型能力的增强，解决越来越复杂的自然语言处理（NLP）任务的道路已经被铺开。支持特定应用需求的关键在于能够进行微调。然而，针对复杂任务编制适用的微调数据集是繁琐的，并且会导致大型数据集，限制了对Transformer输出的控制能力。我们提出了一种方法，将复杂任务分解为更简单的子任务。多个Transformer模型分别对每个子任务进行微调，并排成一行以完成复杂任务。这简化了微调数据集的编制，并增加了整体的可控性。以减少性别偏见作为复杂任务的例子，我们演示了我们的方法，并表明它比使用单一模型效果更好。",
    "tldr": "本论文提出了一种多Transformer架构的方法，针对复杂NLP任务将其分解为简单的子任务，并利用多个模型进行微调，以增强任务的控制能力和性能。在减少性别偏见的任务中，实验证明这种方法优于单一模型的表现。",
    "en_tdlr": "This paper proposes a multi-transformer architecture approach that divides complex NLP tasks into simpler subtasks and fine-tunes multiple models to enhance control and performance. Experimental results show that this approach outperforms single models in reducing gender bias."
}