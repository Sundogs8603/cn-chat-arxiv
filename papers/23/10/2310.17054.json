{
    "title": "BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])",
    "abstract": "Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a",
    "link": "http://arxiv.org/abs/2310.17054",
    "context": "Title: BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])\nAbstract: Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a",
    "path": "papers/23/10/2310.17054.json",
    "total_tokens": 884,
    "translated_title": "提升语言模型生成中的常识能力：利用黑盒控制增强",
    "translated_abstract": "大型语言模型（LLM）如GPT-3已经展示了生成连贯且上下文相关的文本的强大能力。然而，在它们的成功之中，一个关键问题仍然存在：它们生成的输出有时仍然缺乏常识。此外，如果不可行的话，将整个LLM进行微调以获得更加常识性的输出是计算上代价昂贵的。在本文中，我们提出了一个计算高效的框架，将一个冻结的预训练语言模型（PTLM）引导向更加常识性的生成（即以有意义的方式产生包含一系列概念的合理输出）。具体地，我们首先构建了一个无需参考的评估器，通过将句子与一个动态常识知识库在四个不同关系方面相连来为句子分配一个常识得分。然后，我们使用评分器作为常识知识的参考，扩展了名为NADO的可控生成方法，训练了一个辅助头部来引导一个。",
    "tldr": "本文提出了一个计算高效的框架，通过利用黑盒控制来引导冻结的预训练语言模型（PTLM）生成更加常识性的文本输出。",
    "en_tdlr": "This paper presents a computation-efficient framework that uses black-box control to guide a frozen Pre-Trained Language Model (PTLM) towards generating more commonsensical text outputs."
}