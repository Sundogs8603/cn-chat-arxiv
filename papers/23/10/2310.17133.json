{
    "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])",
    "abstract": "This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \\url{https://github.com/libeineu/MMT-VQA}.",
    "link": "http://arxiv.org/abs/2310.17133",
    "context": "Title: Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])\nAbstract: This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \\url{https://github.com/libeineu/MMT-VQA}.",
    "path": "papers/23/10/2310.17133.json",
    "total_tokens": 880,
    "translated_title": "通过视觉问答对将探测信号融入多模态机器翻译",
    "translated_abstract": "本文通过对多模态机器翻译(MMT)的深入研究，检验了MMT系统在文本输入完整时对视觉信息的敏感性降低的认识，我们认为这种现象源于跨模态交互不足，而不是图像信息冗余。提出了一种新颖的方法，即从源文本生成并行的视觉问答(VQA)样式对，促进更强大的跨模态交互。使用大型语言模型(LLM)，我们明确地对MMT中的探测信号进行建模，将其转化为VQA样式数据，创建了Multi30K-VQA数据集。引入了MMT-VQA多任务学习框架，将数据集中的显式探测信号融入MMT训练过程。在两个广泛使用的基准测试上的实验结果表明了这种新颖方法的有效性。我们的代码和数据可在以下链接获取：\\url{https://github.com/libeineu/MMT-VQA}。",
    "tldr": "本文提出了一种通过视觉问答对的方式将探测信号融入多模态机器翻译中，以增强跨模态交互。实验证明了该方法的有效性。",
    "en_tdlr": "This paper proposes a novel approach to incorporate probing signals into multimodal machine translation (MMT) through visual question-answering pairs, enhancing cross-modal interaction. Experimental results demonstrate the effectiveness of this method."
}