{
    "title": "Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])",
    "abstract": "The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu",
    "link": "http://arxiv.org/abs/2310.20093",
    "context": "Title: Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])\nAbstract: The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu",
    "path": "papers/23/10/2310.20093.json",
    "total_tokens": 858,
    "translated_title": "评估神经语言模型作为语言习得的认知模型",
    "translated_abstract": "尽管神经语言模型（LM）的训练方式与儿童语言习得存在明显的差异，但它们在许多技术任务上的成功为其作为语言科学理论的潜在相关性提供了支持。本文认为，评估LM的句法能力的一些主流基准可能不够严格。特别是，我们发现基于模板的基准缺乏语言理论和心理学研究中常见的结构多样性。当使用小规模数据来模拟儿童语言习得时，简单的基准模型可以轻松匹配LM。我们主张使用已经过大量母语者评估过梯度可接受性并设计用于探索语法结构基础的严选数据集。在其中一个这样的数据集LI-Adger上，LM评估句子的方式与人类语言处理不一致。",
    "tldr": "本文评估了神经语言模型作为语言习得的认知模型的潜力。作者认为用于评估句法能力的基准不够严格，并提出了使用严选数据集来探索语法结构基础的建议。",
    "en_tdlr": "This paper evaluates the potential of neural language models as cognitive models of language acquisition. The authors argue that the benchmarks used to evaluate syntactic capacities may not be rigorous enough, and they propose the use of carefully curated datasets to explore the structural basis of grammar."
}