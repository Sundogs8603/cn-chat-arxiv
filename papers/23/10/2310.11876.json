{
    "title": "SQ Lower Bounds for Learning Mixtures of Linear Classifiers. (arXiv:2310.11876v1 [cs.LG])",
    "abstract": "We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\\mathbb{R}^n$ of the form $(\\mathbf{x},y_{\\ell})$, $\\ell\\in [r]$, where $\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_n)$ and $y_\\ell=\\mathrm{sign}(\\langle\\mathbf{v}_\\ell,\\mathbf{x}\\rangle)$ for an unknown unit vector $\\mathbf{v}_\\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$, where $\\Delta$ is a lower bound on the pairwise $\\ell_2$-separation between the $\\mathbf{v}_\\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.",
    "link": "http://arxiv.org/abs/2310.11876",
    "context": "Title: SQ Lower Bounds for Learning Mixtures of Linear Classifiers. (arXiv:2310.11876v1 [cs.LG])\nAbstract: We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\\mathbb{R}^n$ of the form $(\\mathbf{x},y_{\\ell})$, $\\ell\\in [r]$, where $\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_n)$ and $y_\\ell=\\mathrm{sign}(\\langle\\mathbf{v}_\\ell,\\mathbf{x}\\rangle)$ for an unknown unit vector $\\mathbf{v}_\\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$, where $\\Delta$ is a lower bound on the pairwise $\\ell_2$-separation between the $\\mathbf{v}_\\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.",
    "path": "papers/23/10/2310.11876.json",
    "total_tokens": 1023,
    "translated_title": "学习线性分类器混合的SQ下界",
    "translated_abstract": "我们研究了在高斯协变量下学习线性分类器混合的问题。给定对形式为$(\\mathbf{x},y_{\\ell})$的$n$维高斯分布的$r$个混合分布样本访问权限，其中$\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_n)$，$y_\\ell=\\mathrm{sign}(\\langle\\mathbf{v}_\\ell,\\mathbf{x}\\rangle)$，目标是以总变异距离学习潜在的分布。我们的主要结果是统计查询（SQ）的下界，表明对于这个问题的已知算法实际上是最好的，即使对于均匀混合的特殊情况也是如此。特别地，我们证明了对于该问题的任何SQ算法的复杂度都是$n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$，其中$\\Delta$是$\\mathbf{v}_\\ell$之间的两两$\\ell_2$-分离的下界。我们结果的关键技术构建是一种可能具有独立兴趣的新球面设计。",
    "tldr": "本文研究了学习线性分类器混合的问题，证明了该问题的统计查询（SQ）算法的复杂度下界是$n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$，同时提出了一种可能具有独立兴趣的新球面设计方法。",
    "en_tdlr": "This paper studies the problem of learning mixtures of linear classifiers and proves a lower bound on the complexity of Statistical Query (SQ) algorithms for this problem. The lower bound is $n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$, where $\\Delta$ is a lower bound on the pairwise $\\ell_2$-separation between the unit vectors. The paper also introduces a new construction of spherical designs that may be of independent interest."
}