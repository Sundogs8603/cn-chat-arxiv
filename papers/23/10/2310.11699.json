{
    "title": "MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])",
    "abstract": "Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.",
    "link": "http://arxiv.org/abs/2310.11699",
    "context": "Title: MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])\nAbstract: Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.",
    "path": "papers/23/10/2310.11699.json",
    "total_tokens": 839,
    "translated_title": "MISAR：一个具有增强现实的多模式教学系统",
    "translated_abstract": "增强现实（AR）需要将视觉、听觉和语言通道无缝集成，以优化人机交互。尽管听觉和视觉输入有助于实时和情境感导向用户指导，但在这个领域中，大语言模型（LLMs）的潜力仍然大多未被利用。我们的研究引入了一种创新的方法，利用LLMs来吸收来自视觉、听觉和情境多模态的信息。针对AR中任务执行量化的独特挑战，我们利用自主视角视频、语音和语境分析。LLMs的集成促进了增强的状态估计，迈向更适应性的AR系统。代码、数据集和演示将在https://github.com/nguyennm1024/misar上提供。",
    "tldr": "这项研究介绍了一种创新的方法，利用大语言模型（LLMs）从视觉、听觉和情境多模态中吸收信息，并通过自主视频、语音和语境分析实现增强的状态估计，进一步推动了适应性更强的增强现实（AR）系统的发展。",
    "en_tdlr": "This research introduces an innovative method that harnesses large language models (LLMs) to assimilate information from visual, auditory, and contextual modalities, and achieves enhanced state estimation through egocentric video, speech, and context analysis, further advancing the development of more adaptive augmented reality (AR) systems."
}