{
    "title": "SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])",
    "abstract": "We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.",
    "link": "http://arxiv.org/abs/2310.12126",
    "context": "Title: SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])\nAbstract: We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.",
    "path": "papers/23/10/2310.12126.json",
    "total_tokens": 765,
    "translated_title": "SHARCS：通过动态宽度子网络进行路由的高效Transformer",
    "translated_abstract": "我们引入了SHARCS，用于自适应推理，考虑到输入样本的难度。SHARCS可以在任何Transformer网络上训练一个路由器，使模型能够将不同样本指向具有不同宽度的子网络。我们的实验证明：（1）在准确性与FLOPs之间，SHARCS在各种分类任务上的表现优于或补充了现有的每个样本自适应推理方法；（2）SHARCS在不同架构之间具有泛化能力，甚至可以应用于压缩和高效的Transformer编码器，以进一步提高其效率；（3）SHARCS可以提供2倍的推理加速，几乎不损失准确性。",
    "tldr": "SHARCS是一种高效的Transformer模型，通过动态宽度子网络进行路由，实现自适应推理和更高的效率，同时在各种分类任务中表现优越并且具有通用性。",
    "en_tdlr": "SHARCS is an efficient Transformer model that utilizes dynamic width sub-networks for routing, enabling adaptive inference and higher efficiency. It outperforms existing per-sample adaptive inference methods in various classification tasks and shows generalization across different architectures, even enhancing the efficiency of compressed and efficient Transformer encoders, while providing a 2x inference speed up with minimal loss in accuracy."
}