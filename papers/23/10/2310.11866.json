{
    "title": "Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function. (arXiv:2310.11866v1 [cs.LG])",
    "abstract": "Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum m",
    "link": "http://arxiv.org/abs/2310.11866",
    "context": "Title: Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function. (arXiv:2310.11866v1 [cs.LG])\nAbstract: Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum m",
    "path": "papers/23/10/2310.11866.json",
    "total_tokens": 925,
    "translated_title": "具有不精确的Hessian矩阵、梯度和函数的非凸问题的随机优化方法",
    "translated_abstract": "信任区域(TR)和使用三次方(ARC)进行自适应正则化的方法证明对于非凸优化具有一些非常吸引人的理论性质，通过同时计算函数值、梯度和Hessian矩阵来获得下一个搜索方向和调整参数。尽管随机近似大大减少了计算成本，但在理论上保证收敛速度仍然具有挑战性。在本文中，我们探索了一族能够同时提供Hessian矩阵、梯度和函数值的不精确计算的随机TR和ARC方法。我们的算法每次迭代所需的传播开销比TR和ARC要少得多。我们证明了达到ε-近似二阶优化的迭代复杂度与之前研究中的精确计算的数量级相同。此外，对不精确性的温和条件可以通过在有限和总和m中利用随机采样技术来满足。",
    "tldr": "本文提出了一种能够同时提供Hessian矩阵、梯度和函数值的不精确计算的随机优化方法，通过减少传播开销来降低计算成本，并证明了在达到ε-近似二阶优化时与精确计算具有相同的迭代复杂度。",
    "en_tdlr": "This paper proposes a stochastic optimization method that can provide inexact computations of the Hessian matrix, gradient, and function values simultaneously, reducing the computational cost by reducing the propagation overhead, and proves that it has the same iteration complexity as exact computations when achieving ε-approximate second-order optimality."
}