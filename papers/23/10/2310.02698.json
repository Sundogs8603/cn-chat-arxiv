{
    "title": "Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])",
    "abstract": "Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of \"free\" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\\tilde{\\mathcal{O}}\\big(N^{\\frac{1}{3}}T^{\\frac{2}{3}}/K^{\\frac{4}{3}}\\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat",
    "link": "http://arxiv.org/abs/2310.02698",
    "context": "Title: Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])\nAbstract: Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of \"free\" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\\tilde{\\mathcal{O}}\\big(N^{\\frac{1}{3}}T^{\\frac{2}{3}}/K^{\\frac{4}{3}}\\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat",
    "path": "papers/23/10/2310.02698.json",
    "total_tokens": 949,
    "translated_title": "探索通过减少自适应无偏客户采样方差的联邦优化",
    "translated_abstract": "联邦学习系统通常对一部分客户进行采样来进行训练过程。值得注意的是，基于来自采样客户的信息建立全局模型的全局估计方差与联邦优化质量密切相关。本文探讨了一系列“免费”的自适应客户采样技术，其中服务器构建了有前途的采样概率和可靠的全局估计，而无需额外的本地通信和计算。我们捕捉了采样过程中的一个小变体，并相应改进了全局估计。在此基础上，我们提出了一种名为K-Vib的新型采样器，它解决了在联邦优化中遵循客户采样的在线凸优化问题。它在通信预算K的情况下实现了改进的线性速率上升，具有遗憾边界$\\tilde{\\mathcal{O}}\\big(N^{\\frac{1}{3}}T^{\\frac{2}{3}}/K^{\\frac{4}{}3}\\big)$。结果是，它显著提高了联邦学习性能。",
    "tldr": "本文通过减少自适应无偏客户采样方差，探索了联邦优化中的一系列自适应客户采样技术，并提出了一种名为K-Vib的新型采样器，显著提高了联邦学习性能。"
}