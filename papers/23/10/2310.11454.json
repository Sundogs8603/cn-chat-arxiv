{
    "title": "VeRA: Vector-based Random Matrix Adaptation. (arXiv:2310.11454v2 [cs.CL] UPDATED)",
    "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
    "link": "http://arxiv.org/abs/2310.11454",
    "context": "Title: VeRA: Vector-based Random Matrix Adaptation. (arXiv:2310.11454v2 [cs.CL] UPDATED)\nAbstract: Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
    "path": "papers/23/10/2310.11454.json",
    "total_tokens": 838,
    "translated_title": "VeRA: 基于向量的随机矩阵自适应",
    "translated_abstract": "低秩自适应（LoRA）是一种流行的方法，在微调大型语言模型时减少可训练参数的数量，但在扩展到更大模型或部署大量特定用户或任务自适应模型时仍然面临严重的存储挑战。在这项工作中，我们提出了基于向量的随机矩阵自适应（VeRA），与LoRA相比，它可以显著减少可训练参数的数量，同时保持相同的性能。它通过使用一对在所有层之间共享的低秩矩阵，并学习小的缩放向量来实现这一点。我们在GLUE和E2E基准测试、图像分类任务中展示了其有效性，并展示了其在7B和13B语言模型的指令调优中的应用。",
    "tldr": "VeRA提出了一种基于向量的随机矩阵自适应方法，相比于低秩自适应方法，可以显著减少可训练参数的数量，同时保持相同的性能。该方法在GLUE和E2E基准测试、图像分类任务以及指令调优中都取得了良好的效果。",
    "en_tdlr": "VeRA presents a vector-based random matrix adaptation approach that significantly reduces the number of trainable parameters compared to low-rank adaption methods, while maintaining the same performance. The method demonstrates its effectiveness in GLUE and E2E benchmarks, image classification tasks, and instruction-tuning of language models."
}