{
    "title": "Boosting Continuous Control with Consistency Policy. (arXiv:2310.06343v1 [cs.LG])",
    "abstract": "Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement l",
    "link": "http://arxiv.org/abs/2310.06343",
    "context": "Title: Boosting Continuous Control with Consistency Policy. (arXiv:2310.06343v1 [cs.LG])\nAbstract: Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement l",
    "path": "papers/23/10/2310.06343.json",
    "total_tokens": 901,
    "translated_title": "用一致性策略提升连续控制",
    "translated_abstract": "由于其训练稳定性和强大表达能力，扩散模型在离线强化学习中受到了相当大的关注。然而，它也带来了几个挑战：1）对大量扩散步骤的需求使得基于扩散模型的方法在实时控制中效率低下，限制了它们的应用；2）如何提供准确指导以实现基于扩散模型的策略改进仍然是一个未解决的问题。受一致性模型的启发，我们提出了一种新颖的时间效率方法，称为一致性策略与Q-Learning（CPQL），它通过单步从噪声中导出动作。通过建立从反向扩散轨迹到期望策略的映射，我们同时解决了使用学习到的Q函数更新基于扩散模型的策略时的时间效率和准确指导的问题。我们证明了CPQL可以通过准确指导实现离线强化学习的策略改进。",
    "tldr": "该论文提出了一种名为一致性策略与Q-Learning （CPQL）的新方法，通过建立从反向扩散轨迹到期望策略的映射，同时解决了基于扩散模型方法的时间效率和准确指导问题。",
    "en_tdlr": "This paper proposes a new method called Consistency Policy with Q-Learning (CPQL), which addresses the challenges of time efficiency and accurate guidance in diffusion model-based methods by establishing a mapping from reverse diffusion trajectories to the desired policy."
}