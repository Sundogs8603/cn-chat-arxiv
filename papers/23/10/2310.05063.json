{
    "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v2 [cs.LG] UPDATED)",
    "abstract": "Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, bot",
    "link": "http://arxiv.org/abs/2310.05063",
    "context": "Title: Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v2 [cs.LG] UPDATED)\nAbstract: Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, bot",
    "path": "papers/23/10/2310.05063.json",
    "total_tokens": 960,
    "translated_title": "在CloudOps领域的时间序列预测中推动预训练的极限",
    "translated_abstract": "在预训练和迁移学习的时代，时间序列被放在了后面。尽管自然语言处理和计算机视觉领域的研究正在享受着越来越大的数据集来训练庞大的模型，但最受欢迎的时间序列数据集只包含数万个时间步，限制了我们对预训练和扩展性的研究效果。最近的研究也对表达力模型和规模的必要性产生了怀疑。为了缓解这些问题，我们引入了来自云操作（CloudOps）领域的三个大规模时间序列预测数据集，其中最大的数据集拥有数十亿个观测点，进一步研究时间序列模型的预训练和扩展性。我们建立了研究时间序列模型的预训练和扩展性的经验基础，并通过确定一个有前景的候选架构为未来研究铺平了道路。我们展示了它是一个强大的零-shot基线，并从进一步的扩展中获益。",
    "tldr": "本论文在CloudOps领域引入了三个大规模时间序列预测数据集，其中最大的数据集拥有数十亿个观测点，为研究时间序列模型的预训练和扩展性奠定了实证基础，并确定了一个有前景的候选架构作为强大的零-shot基线。",
    "en_tdlr": "This paper introduces three large-scale time series forecasting datasets from the CloudOps domain, with the largest dataset consisting of billions of observations. It establishes the empirical groundwork for studying pre-training and scaling of time series models and identifies a promising candidate architecture as a strong zero-shot baseline."
}