{
    "title": "Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])",
    "abstract": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness ac",
    "link": "http://arxiv.org/abs/2310.13307",
    "context": "Title: Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])\nAbstract: Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness ac",
    "path": "papers/23/10/2310.13307.json",
    "total_tokens": 942,
    "translated_title": "测试时自适应小型语言模型在问答中的应用",
    "translated_abstract": "最近，经过指导微调的大型语言模型在各种任务中取得了显著的成绩，例如问答。然而，尽管它们能够记忆各种任务的大量常识，但由于能力有限，转移和适应知识到目标任务时可能不是最优的。此外，由于缺少标记的数据集，使用带标记的数据集对模型进行进一步微调通常是不可行的，但我们也可以通过仅使用无标记的测试数据来转移具有有限知识的更小语言模型也是一个值得探究的问题。在这项工作中，我们展示并研究了仅使用无标记测试数据的较小自适应语言模型的能力。特别地，我们首先随机生成多个答案，然后在过滤掉低质量样本的同时对它们进行集成，以减轻不准确标签的噪声。我们提出的自适应策略在基准QA数据集上表现出了显著的性能提升，具有更高的鲁棒性。",
    "tldr": "本论文研究了仅使用无标记测试数据的小型自适应语言模型在问答任务中的能力，并提出了一种自适应策略，该策略通过随机生成多个答案并进行集成，以达到显著的性能提升和更高的鲁棒性。",
    "en_tdlr": "This paper investigates the capabilities of smaller self-adaptive language models, using only unlabeled test data, for question answering tasks. A proposed adaptive strategy that stochastically generates multiple answers and ensembles them demonstrates significant performance improvements and higher robustness."
}