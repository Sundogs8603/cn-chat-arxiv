{
    "title": "Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)",
    "abstract": "In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable re",
    "link": "http://arxiv.org/abs/2310.11398",
    "context": "Title: Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)\nAbstract: In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable re",
    "path": "papers/23/10/2310.11398.json",
    "total_tokens": 796,
    "translated_title": "神经注意力：利用神经网络增强自注意机制中的QKV计算",
    "translated_abstract": "在深度学习领域中，自注意机制在自然语言处理和计算机视觉等多个任务中发挥了重要作用。然而，传统的自注意机制主要使用线性变换来计算查询、键和值(QKV)，但在特定情况下，这可能并不是最优选择。本文探讨了一种新的QKV计算方法，采用了特殊设计的神经网络结构进行计算。通过在IWSLT 2017德英翻译任务数据集上使用修改后的Marian模型进行实验，并将我们的方法与传统方法进行对比，实验结果显示我们的方法在BLEU得分方面有显著提升。此外，我们的方法在使用Wikitext-103数据集训练Roberta模型时也表现出优势，显示了显著的改进。",
    "tldr": "本文介绍了一种利用神经网络增强自注意机制中QKV计算的方法，实验证明这种方法在多个任务中取得了显著的提升。",
    "en_tdlr": "This paper presents a method to enhance the calculation of query, key, and value in the self-attention mechanism using neural networks. Experimental results show significant improvements across multiple tasks."
}