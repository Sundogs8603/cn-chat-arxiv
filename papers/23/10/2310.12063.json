{
    "title": "Black-Box Training Data Identification in GANs via Detector Networks. (arXiv:2310.12063v1 [cs.LG])",
    "abstract": "Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our ",
    "link": "http://arxiv.org/abs/2310.12063",
    "context": "Title: Black-Box Training Data Identification in GANs via Detector Networks. (arXiv:2310.12063v1 [cs.LG])\nAbstract: Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our ",
    "path": "papers/23/10/2310.12063.json",
    "total_tokens": 889,
    "translated_title": "GAN中黑盒训练数据识别的探测器网络研究",
    "translated_abstract": "从它们问世以来，生成对抗网络（GAN）一直是流行的生成模型，可用于图像、音频、视频和表格数据。本文研究了在已训练好的GAN以及来自基础分布的新样本的情况下，攻击者能否有效地识别给定点是否属于GAN的训练数据。这对于版权相关的原因很有意义，用户可能想确定他们的版权数据是否被用来训练GAN，以及对数据隐私的研究也很有意义，其中检测训练集成员身份的能力被称为成员隐私攻击。与大多数先前的工作不同，本文研究了在黑盒设置中使用GAN的隐私影响，攻击者只能访问生成器的样本，而不能访问鉴别器的样本。我们引入了一套针对黑盒设置中GAN的成员隐私攻击，并评估了我们的方法。",
    "tldr": "本研究探索了在黑盒设置中使用GAN时的隐私问题，通过引入一套攻击来识别训练数据成员身份，本文提供了对版权和数据隐私方面的重要洞见。",
    "en_tdlr": "This research examines the privacy implications of using GANs in black-box settings and introduces a set of attacks to identify the membership of training data, providing important insights into copyright and data privacy aspects."
}