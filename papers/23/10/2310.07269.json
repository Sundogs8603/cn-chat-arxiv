{
    "title": "Why Does Sharpness-Aware Minimization Generalize Better Than SGD?. (arXiv:2310.07269v1 [cs.LG])",
    "abstract": "The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby f",
    "link": "http://arxiv.org/abs/2310.07269",
    "context": "Title: Why Does Sharpness-Aware Minimization Generalize Better Than SGD?. (arXiv:2310.07269v1 [cs.LG])\nAbstract: The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby f",
    "path": "papers/23/10/2310.07269.json",
    "total_tokens": 969,
    "translated_title": "为什么锐度感知最小化比随机梯度下降更好地推广？(arXiv:2310.07269v1 [cs.LG])",
    "translated_abstract": "过拟合的挑战在大型神经网络的训练中变得越来越重要，它指的是模型记忆训练数据，但在测试数据上无法推广。为了解决这个挑战，锐度感知最小化（SAM）已经成为一种有前途的训练方法，可以在存在标签噪声的情况下改善神经网络的泛化性能。然而，对于非线性神经网络和分类任务的情况下，SAM的工作方式仍然缺乏深入理解。本文通过展示为什么在特定数据模型和两层卷积ReLU网络中，SAM比随机梯度下降更好地推广，来弥补这一空白。我们所研究问题的损失景观是非光滑的，因此目前关于SAM成功的解释基于Hessian信息是不足够的。我们的结果解释了SAM的优势，特别是它在早期阶段防止了噪声学习的能力，从而提高了泛化性能。",
    "tldr": "这项研究填补了Sharpness-Aware Minimization（SAM）相对于随机梯度下降（SGD）的一定数据模型和卷积神经网络中泛化更好的空白，解释了SAM的优势，尤其是在早期阶段防止噪声学习的能力。",
    "en_tdlr": "This study fills the gap by explaining the superior generalization of Sharpness-Aware Minimization (SAM) over Stochastic Gradient Descent (SGD) in certain data models and convolutional neural networks, particularly due to its ability to prevent noise learning in the early stages."
}