{
    "title": "Federated Learning with Convex Global and Local Constraints",
    "abstract": "arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform",
    "link": "https://arxiv.org/abs/2310.10117",
    "context": "Title: Federated Learning with Convex Global and Local Constraints\nAbstract: arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform",
    "path": "papers/23/10/2310.10117.json",
    "total_tokens": 843,
    "translated_title": "具有凸全局和局部约束的联邦学习",
    "translated_abstract": "在实践中，许多机器学习（ML）问题都带有约束，其应用领域涉及无法与他人共享的分布式敏感数据，例如在医疗保健中。在这种实际场景中的协作学习涉及联邦学习（FL）用于带约束的ML问题，或简称带约束的FL。尽管近年来FL技术得到了广泛发展，但这些技术仅处理无约束的FL问题。为了填补这一空白，我们迈出了解决带约束FL问题的通用算法框架的第一步。具体而言，我们基于近端增广拉格朗日（AL）方法，提出了一种针对受限制ML问题的新FL算法。假设凸目标和凸约束以及其他一些温和条件，我们确定了所提算法的最坏情况复杂度。我们的数值实验证明了我们的算法的有效性。",
    "tldr": "该论文提出了一种针对具有约束的机器学习问题的新联邦学习算法，建立了基于凸目标和凸约束的最坏情况复杂度，并通过数值实验证明了算法的有效性",
    "en_tdlr": "This paper proposes a new federated learning algorithm for constrained machine learning problems, establishes the worst-case complexity based on convex objectives and constraints, and demonstrates the effectiveness of the algorithm through numerical experiments."
}