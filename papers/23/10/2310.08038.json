{
    "title": "Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])",
    "abstract": "In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as dis",
    "link": "http://arxiv.org/abs/2310.08038",
    "context": "Title: Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])\nAbstract: In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as dis",
    "path": "papers/23/10/2310.08038.json",
    "total_tokens": 958,
    "translated_title": "基于流形扩展回放的持续学习",
    "translated_abstract": "在持续学习中，学习者按顺序学习多个任务，每个任务只获取一次数据。灾难性遗忘是持续学习的主要挑战。为了减少遗忘，一些现有的基于回放的方法使用情境记忆来重新播放先前任务的样本。然而，在学习新任务时进行知识整合的过程中，由于旧知识和新知识之间的不平衡，这种策略也会遭受灾难性遗忘。为了解决这个问题，我们提出了一种称为Manifold Expansion Replay（MaER）的新型回放策略。我们认为，在情境记忆中扩展知识表示的隐式流形有助于提高模型的鲁棒性和表达能力。为此，我们提出了一种贪心策略，在内存管理过程中，不断增加由缓冲区中的知识表示的隐式流形的直径。此外，我们将Wasserstein距离引入替代交叉熵作为距离度量。",
    "tldr": "该论文提出了一种名为Manifold Expansion Replay（MaER）的新型回放策略，通过扩展知识表示中的隐式流形来减少灾难性遗忘。采用贪心策略增加缓冲区中知识表示的流形直径，并使用Wasserstein距离作为距离度量，以提高模型的鲁棒性和表达能力。"
}