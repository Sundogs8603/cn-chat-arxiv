{
    "title": "Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])",
    "abstract": "Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.",
    "link": "http://arxiv.org/abs/2310.11638",
    "context": "Title: Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])\nAbstract: Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.",
    "path": "papers/23/10/2310.11638.json",
    "total_tokens": 901,
    "translated_title": "大型语言模型中事实知识的系统评估",
    "translated_abstract": "以往的研究依赖于现有的问答基准来评估大型语言模型（LLMs）中存储的知识。然而，这种方法在涵盖事实知识方面存在局限性，因为它主要集中在通用领域，这可能与预训练数据重叠。本文提出了一个框架，通过利用知识图谱（KGs）来系统评估LLMs的事实知识。我们的框架从给定KG中存储的事实自动生成一组问题和预期答案，然后评估LLMs回答这些问题的准确性。我们在通用领域和特定领域中系统评估了最先进的LLMs与KGs的性能。实验显示，ChatGPT在所有领域中始终是表现最好的。我们还发现，LLMs的性能取决于指令微调、领域和问题的复杂性，并容易受到对抗性环境的影响。",
    "tldr": "本研究提出了一个通过利用知识图谱来评估大型语言模型中事实知识的框架，并在通用和特定领域中对最先进的模型进行了系统的评估。实验结果表明，ChatGPT是在所有领域中表现最好的模型。",
    "en_tdlr": "This study proposes a framework to systematically evaluate the factual knowledge stored in large language models (LLMs) by leveraging knowledge graphs (KGs), and systematically evaluates the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT consistently outperforms other models in all domains."
}