{
    "title": "MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the p",
    "link": "http://arxiv.org/abs/2310.15777",
    "context": "Title: MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the p",
    "path": "papers/23/10/2310.15777.json",
    "total_tokens": 982,
    "translated_title": "MindLLM: 从零开始预训练轻量级大型语言模型，评估和领域应用",
    "translated_abstract": "大型语言模型（LLM）在各种自然语言任务中展现出了出色的性能，标志着通往通用人工智能的重要进展。虽然通用人工智能是通过开发越来越大规模的模型来实现的，但还有另一种分支，即开发轻量级定制模型，以更好地服务某些领域，考虑到训练和部署LLM的高成本和资源的稀缺性。在本文中，我们提出MindLLM，一系列新颖的双语轻量级大型语言模型，从零开始训练，通过提供13亿和30亿参数的模型来减轻这些负担。给出了在大模型开发过程中积累的经验全面的分析，包括数据构建、模型架构、评估和应用。这些见解对学者和开发者来说有价值。MindLLM始终能够与甚至超过最先进的模型性能。",
    "tldr": "本文提出了一种从零开始训练的轻量级大型语言模型MindLLM，通过提供1.3亿和3亿参数的模型，减轻了训练和部署大型语言模型的成本和资源稀缺性的压力。MindLLM在各个步骤中给出了经验教训，包括数据构建、模型架构、评估和应用，对学术界和开发者来说具有重要价值。",
    "en_tdlr": "This paper presents MindLLM, a lightweight large language model trained from scratch, alleviating the cost and resource scarcity issues of training and deploying large language models. MindLLM offers models with 1.3 billion and 3 billion parameters, and provides insights from the entire development process including data construction, model architecture, evaluation, and applications, valuable for academics and developers."
}