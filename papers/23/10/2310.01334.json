{
    "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy",
    "abstract": "arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss",
    "link": "https://arxiv.org/abs/2310.01334",
    "context": "Title: Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy\nAbstract: arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss",
    "path": "papers/23/10/2310.01334.json",
    "total_tokens": 905,
    "translated_title": "合并，然后压缩：从其路由策略中揭示高效的SMoE技术提示",
    "translated_abstract": "稀疏激活的专家混合模型（SMoE）显示出扩展神经网络学习能力的潜力，然而，它们存在诸如（a）高内存使用的问题，由于网络层的重复成为多个专家的副本；以及（b）专家中的冗余，因为常规基于学习的路由策略容易出现表示性崩溃。因此，传统SMoE模型在内存效率和可伸缩性方面效率低下，尤其对于资源受限的下游场景。在本文中，我们提出了一个问题：我们能否通过合并专家信息来制定一个紧凑的SMoE模型？如何将多个专家合并为更少但更有知识的专家的最佳方法？我们的初步调查显示，传统的模型合并方法对于SMoE的专家合并并不有效。潜在原因是：（1）冗余信息掩盖了关键专家；（2）为每个专家选择适当的神经元排列方式会丢失",
    "tldr": "本文旨在探讨如何通过合并专家信息来制定出更紧凑但更具知识的SMoE模型，因为传统的模型合并方法并不适用于SMoE的专家合并。",
    "en_tdlr": "This paper aims to explore how to craft a more compact but knowledgeable SMoE model by consolidating expert information, as traditional model merging methods prove to be ineffective for expert merging in SMoE."
}