{
    "title": "Universality of max-margin classifiers. (arXiv:2310.00176v1 [math.ST])",
    "abstract": "Maximum margin binary classification is one of the most fundamental algorithms in machine learning, yet the role of featurization maps and the high-dimensional asymptotics of the misclassification error for non-Gaussian features are still poorly understood. We consider settings in which we observe binary labels $y_i$ and either $d$-dimensional covariates ${\\boldsymbol z}_i$ that are mapped to a $p$-dimension space via a randomized featurization map ${\\boldsymbol \\phi}:\\mathbb{R}^d \\to\\mathbb{R}^p$, or $p$-dimensional features of non-Gaussian independent entries. In this context, we study two fundamental questions: $(i)$ At what overparametrization ratio $p/n$ do the data become linearly separable? $(ii)$ What is the generalization error of the max-margin classifier?  Working in the high-dimensional regime in which the number of features $p$, the number of samples $n$ and the input dimension $d$ (in the nonlinear featurization setting) diverge, with ratios of order one, we prove a unive",
    "link": "http://arxiv.org/abs/2310.00176",
    "context": "Title: Universality of max-margin classifiers. (arXiv:2310.00176v1 [math.ST])\nAbstract: Maximum margin binary classification is one of the most fundamental algorithms in machine learning, yet the role of featurization maps and the high-dimensional asymptotics of the misclassification error for non-Gaussian features are still poorly understood. We consider settings in which we observe binary labels $y_i$ and either $d$-dimensional covariates ${\\boldsymbol z}_i$ that are mapped to a $p$-dimension space via a randomized featurization map ${\\boldsymbol \\phi}:\\mathbb{R}^d \\to\\mathbb{R}^p$, or $p$-dimensional features of non-Gaussian independent entries. In this context, we study two fundamental questions: $(i)$ At what overparametrization ratio $p/n$ do the data become linearly separable? $(ii)$ What is the generalization error of the max-margin classifier?  Working in the high-dimensional regime in which the number of features $p$, the number of samples $n$ and the input dimension $d$ (in the nonlinear featurization setting) diverge, with ratios of order one, we prove a unive",
    "path": "papers/23/10/2310.00176.json",
    "total_tokens": 876,
    "translated_title": "最大间隔分类器的普适性",
    "translated_abstract": "最大间隔二元分类是机器学习中最基础的算法之一，然而对于非高斯特征的映射函数和高维渐近条件下的误分类错误的作用仍然知之甚少。我们考虑观测到二元标签 $y_i$，通过随机映射函数 ${\\boldsymbol \\phi}:\\mathbb{R}^d \\to\\mathbb{R}^p$，将 $d$ 维协变量 ${\\boldsymbol z}_i$ 映射到 $p$ 维空间，或者对于非高斯独立的 $p$ 维特征。在这个背景下，我们研究了两个基本问题：$(i)$ 数据在过参数化比率 $p/n$ 下何时变为线性可分？$(ii)$ 最大间隔分类器的泛化误差是多少？在高维条件下，特征数 $p$、样本数 $n$ 和输入维度 $d$（非线性特征化设置下）发散的情况下，我们证明了一个普适的结果。",
    "tldr": "该论文研究了最大间隔分类器在非高斯特征映射和高维渐近条件下的普适性问题。",
    "en_tdlr": "This paper investigates the universality of max-margin classifiers in the context of non-Gaussian feature maps and high-dimensional asymptotics."
}