{
    "title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?. (arXiv:2310.05079v2 [cs.LG] UPDATED)",
    "abstract": "The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has merged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a $19\\times$ higher arithmetic density and $5\\times$ memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by $2.5\\times$ in arithmetic density and $1.2\\times$ in memory density, without requiring any data calibration or re-training. We also ",
    "link": "http://arxiv.org/abs/2310.05079",
    "context": "Title: Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?. (arXiv:2310.05079v2 [cs.LG] UPDATED)\nAbstract: The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has merged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a $19\\times$ higher arithmetic density and $5\\times$ memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by $2.5\\times$ in arithmetic density and $1.2\\times$ in memory density, without requiring any data calibration or re-training. We also ",
    "path": "papers/23/10/2310.05079.json",
    "total_tokens": 976,
    "translated_title": "重新审视基于块的量化: 对于低于8位LLM推理的重要性",
    "translated_abstract": "大型语言模型（LLMs）的推理需要大量的计算和内存资源。为了降低这些成本，量化成为了一种有希望的解决方案，但现有的LLM量化主要集中在8位。在这项工作中，我们探索了LLM层的统计和学习特性，并将LLM量化的瓶颈归因于数值缩放偏移。为了解决这个问题，我们为LLMs改进了块量化方法，这是一类在打包的数字之间共享缩放因子的方法。块量化仅从算术角度有效地减小了数值缩放偏移，而不需要在计算路径中进行其他处理。我们的近乎无损量化的6位LLMs的算术密度比float32基线高出19倍，内存密度高出5倍，超过了先前的8位量化的算术密度高出2.5倍，内存密度高出1.2倍，而且不需要任何数据校准或重新训练。",
    "tldr": "这项工作重新审视了基于块的量化在低于8位LLM推理方面的重要性，通过提出一种新的方法，实现了近乎无损量化的6位LLMs，相较于先前的8位量化，在算术密度和内存密度方面都有显著提升，并且不需要进行数据校准或重新训练。"
}