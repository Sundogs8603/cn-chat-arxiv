{
    "title": "Heterogeneous Federated Learning Using Knowledge Codistillation. (arXiv:2310.02549v1 [cs.LG])",
    "abstract": "Federated Averaging, and many federated learning algorithm variants which build upon it, have a limitation: all clients must share the same model architecture. This results in unused modeling capacity on many clients, which limits model performance. To address this issue, we propose a method that involves training a small model on the entire pool and a larger model on a subset of clients with higher capacity. The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters. We present two variants of our method, which improve upon federated averaging on image classification and language modeling tasks. We show this technique can be useful even if only out-of-domain or limited in-domain distillation data is available. Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift.",
    "link": "http://arxiv.org/abs/2310.02549",
    "context": "Title: Heterogeneous Federated Learning Using Knowledge Codistillation. (arXiv:2310.02549v1 [cs.LG])\nAbstract: Federated Averaging, and many federated learning algorithm variants which build upon it, have a limitation: all clients must share the same model architecture. This results in unused modeling capacity on many clients, which limits model performance. To address this issue, we propose a method that involves training a small model on the entire pool and a larger model on a subset of clients with higher capacity. The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters. We present two variants of our method, which improve upon federated averaging on image classification and language modeling tasks. We show this technique can be useful even if only out-of-domain or limited in-domain distillation data is available. Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift.",
    "path": "papers/23/10/2310.02549.json",
    "total_tokens": 890,
    "translated_title": "使用知识共蒸合的异构联邦学习",
    "translated_abstract": "联邦平均化及其建立在其上的许多联邦学习算法变种存在一个限制：所有客户端必须共享相同的模型架构。这导致许多客户端上未使用的建模能力，从而限制了模型性能。为解决这个问题，我们提出了一种方法，该方法涉及在整个池内训练一个小模型和在具有更高容量的一部分客户端上训练一个更大模型。模型通过知识共蒸合在未共享参数的服务器上进行双向信息交换，利用一个无标签数据集。我们提出了两种改进联邦平均化在图像分类和语言模型任务上的方法。我们展示了即使只有领域外或有限领域内的蒸馏数据可用，这种技术也可以很有用。此外，双向知识共蒸合在不同池子中引入领域转移时允许模型之间的领域转移。",
    "tldr": "使用知识共蒸合的异构联邦学习方法通过在整个池子和容量较高的部分客户端上训练不同大小的模型，实现了双向信息交换和领域转移，改进了联邦平均化算法在图像分类和语言建模任务上的性能。"
}