{
    "title": "Differentially Private Non-convex Learning for Multi-layer Neural Networks. (arXiv:2310.08425v1 [cs.LG])",
    "abstract": "This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.  In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. I",
    "link": "http://arxiv.org/abs/2310.08425",
    "context": "Title: Differentially Private Non-convex Learning for Multi-layer Neural Networks. (arXiv:2310.08425v1 [cs.LG])\nAbstract: This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.  In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. I",
    "path": "papers/23/10/2310.08425.json",
    "total_tokens": 870,
    "translated_title": "多层神经网络的差分隐私非凸学习",
    "translated_abstract": "本文关注具有单输出节点的（多层）全连接神经网络的差分隐私随机优化问题。首先, 我们研究了没有隐藏节点的情况，具体关注广义线性模型（GLMs）。我们研究了随机噪声具有零均值且链接函数既有界又Lipschitz连续的特定模型。我们提出了几种算法，并分析证明了在数据维度不变的情况下实现过度群体风险的可行性。我们还探讨了涉及ReLU链接函数的场景，发现与有界链接函数的结果相似。通过使用ReLU回归作为代表性示例, 对比了特定和不正确指定的模型。在论文的第二部分，我们将这些理念扩展到具有Sigmoid或ReLU激活函数的两层神经网络中的特定模型。",
    "tldr": "本文研究了具有差分隐私的非凸学习问题在多层神经网络中的应用。通过提出新算法，实现了在数据维度不变的情况下达到过度群体风险。同时，对比了不同链接函数和不同模型设定的结果。"
}