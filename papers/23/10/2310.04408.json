{
    "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. (arXiv:2310.04408v1 [cs.CL])",
    "abstract": "Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can retur",
    "link": "http://arxiv.org/abs/2310.04408",
    "context": "Title: RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. (arXiv:2310.04408v1 [cs.CL])\nAbstract: Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can retur",
    "path": "papers/23/10/2310.04408.json",
    "total_tokens": 828,
    "translated_title": "RECOMP:通过压缩和选择性增强改善检索辅助LMs",
    "translated_abstract": "在推断时，将文档检索并在上下文中前置可以提高语言模型（LMs）在各种任务中的性能。然而，这些文档通常包含数百个词，使推断过程更加昂贵。我们提出将检索到的文档压缩为文本摘要，以降低计算成本，同时减轻LMs在长文档中识别相关信息的负担。我们提出了两种压缩器：一种是从检索文档中选择有用句子的抽取式压缩器，另一种是通过综合多个文档的信息生成摘要的生成式压缩器。这两种压缩器在将生成的摘要前置到LMs的输入时被训练以改善LMs在最终任务上的性能，同时保持摘要简洁。",
    "tldr": "RECOMP提出了一种改善检索辅助LMs性能的方法，通过压缩检索到的文档为摘要，降低计算成本，并减轻LMs在长文档中识别相关信息的负担。"
}