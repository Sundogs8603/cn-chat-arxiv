{
    "title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])",
    "abstract": "Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-bas",
    "link": "http://arxiv.org/abs/2310.03173",
    "context": "Title: $\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])\nAbstract: Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-bas",
    "path": "papers/23/10/2310.03173.json",
    "total_tokens": 838,
    "translated_title": "$\\mathcal{B}$-Coder: 基于价值的深度强化学习用于程序合成",
    "translated_abstract": "程序合成旨在从自然语言描述中创建准确可执行的代码。该领域结合了强化学习和大规模语言模型的力量，显著提高了代码生成能力。该集成主要关注直接优化功能正确性，超越了传统的监督学习损失。尽管当前文献主要支持基于策略的算法，但程序合成的属性表明与基于值的方法自然兼容。这源于人类程序员开发的丰富离线程序集合，并通过自动化单元测试对生成的程序进行直观验证（即在强化学习中容易获得奖励的语言表达）。与主要使用基于策略的算法不同，我们的工作探索了基于值的方法的适用性，从而开发了我们的$\\mathcal{B}$-Coder（发音为Bellman coder）。",
    "tldr": "$\\mathcal{B}$-Coder是一种基于价值的深度强化学习方法，用于程序合成，旨在通过结合强化学习和大规模语言模型的能力，提高代码生成的准确性和执行能力。",
    "en_tdlr": "$\\mathcal{B}$-Coder is a value-based deep reinforcement learning approach for program synthesis, aiming to enhance the accuracy and executability of code generation by leveraging the power of reinforcement learning and large language models."
}