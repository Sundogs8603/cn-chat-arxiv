{
    "title": "Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])",
    "abstract": "Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\\Vert \\nabla L(\\theta,\\gamma)\\Vert_2^2 \\leq \\epsilon$) of the performance (Lagrange) function $L(\\theta,\\gamma)$, with a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.5})$ in the case of both C",
    "link": "http://arxiv.org/abs/2310.16363",
    "context": "Title: Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])\nAbstract: Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\\Vert \\nabla L(\\theta,\\gamma)\\Vert_2^2 \\leq \\epsilon$) of the performance (Lagrange) function $L(\\theta,\\gamma)$, with a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.5})$ in the case of both C",
    "path": "papers/23/10/2310.16363.json",
    "total_tokens": 897,
    "translated_title": "受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析",
    "translated_abstract": "Actor Critic方法在广泛的强化学习任务中找到了巨大的应用，特别是当状态-动作空间很大的时候。本文考虑使用函数逼近的actor critic和natural actor critic算法来处理涉及不等式约束的马尔可夫决策过程（C-MDP），并在非 i.i.d（马尔可夫）环境中进行了非渐近分析。我们考虑长期平均成本准则，其中目标和约束函数都是某些规定成本函数的适当策略依赖的长期平均。我们使用拉格朗日乘子法处理不等式约束。我们证明这些算法保证能找到性能（拉格朗日）函数$L(\\theta,\\gamma)$的一阶稳定点（即$\\Vert \\nabla L(\\theta,\\gamma)\\Vert_2^2 \\leq \\epsilon$），并且其样本复杂度为$\\mathcal{\\tilde{O}}(\\epsilon^{-2.5})$。",
    "tldr": "本文研究了受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析，证明了这些算法能找到性能函数的一阶稳定点，并且具有较低的样本复杂度。",
    "en_tdlr": "This paper presents a finite time analysis of constrained Actor Critic and constrained Natural Actor Critic algorithms, proving that these algorithms can find first-order stationary points of the performance function with low sample complexity."
}