{
    "title": "Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])",
    "abstract": "Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine",
    "link": "http://arxiv.org/abs/2310.07478",
    "context": "Title: Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])\nAbstract: Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine",
    "path": "papers/23/10/2310.07478.json",
    "total_tokens": 846,
    "translated_title": "多模态图学习用于生成任务",
    "translated_abstract": "多模态学习结合多种数据模态，扩大了模型可以利用的数据类型和复杂度，例如从纯文本到图像-字幕对。大多数多模态学习算法专注于对两种模态的简单一对一数据进行建模，如图像-字幕对或音频-文本对。然而，在大多数实际场景中，不同模态的实体以更复杂和多样化的方式相互作用，超越了一对一映射。我们提出将这些复杂关系表示为图形，允许我们捕捉任意数量的模态数据，并捕捉模态之间的复杂关系，这些关系可以从一个样本到另一个样本灵活变化。为实现这一目标，我们提出了多模态图学习（MMGL），这是一个通用且系统的框架，用于捕获具有关系结构的多个多模态邻居的信息。特别是，我们关注基于预训练模型的MMGL用于生成任务。",
    "tldr": "多模态图学习是一种通用且系统的框架，可以捕捉多模态数据之间的复杂关系，并在生成任务中取得了良好的效果。",
    "en_tdlr": "Multimodal Graph Learning (MMGL) is a general and systematic framework that captures complex relationships between multimodal data and has achieved promising results in generative tasks."
}