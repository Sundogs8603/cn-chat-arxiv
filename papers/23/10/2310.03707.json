{
    "title": "OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])",
    "abstract": "Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant ro",
    "link": "http://arxiv.org/abs/2310.03707",
    "context": "Title: OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])\nAbstract: Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant ro",
    "path": "papers/23/10/2310.03707.json",
    "total_tokens": 903,
    "translated_title": "OMG-ATTACK:自我监督的在流形上生成可迁移的规避攻击",
    "translated_abstract": "规避攻击(EA)被用来通过扭曲输入数据来误导模型的分类，以测试训练好的神经网络的鲁棒性。创建这些攻击是一项具有挑战性的任务，特别是随着模型和数据集的日益复杂。在这项工作中，我们介绍了一种自我监督的、计算经济的生成对抗性示例的方法，设计用于未见黑盒模型的情况。通过借鉴表示学习的技术，我们的方法生成在流形上的攻击，这些攻击被鼓励与数据分布相似。与基于模型生成的攻击相比，这些攻击在攻击训练模型时具有相当的效果，但在攻击未见模型时要更加有效，因为这些攻击更加与数据相关而不是与模型本身相关。我们的实验一致表明该方法在各种模型、未见数据类别甚至防御模型上都是有效的，这表明它具有显著的贡献。",
    "tldr": "本文介绍了一种自我监督的、计算经济的方法，用于生成具有迁移性的规避攻击。实验证明该方法在各种模型、未见数据类别甚至防御模型上都是有效的。",
    "en_tdlr": "This paper introduces a self-supervised and computationally economical method for generating transferable evasion attacks. Experiments demonstrate the effectiveness of the method across various models, unseen data categories, and even defended models."
}