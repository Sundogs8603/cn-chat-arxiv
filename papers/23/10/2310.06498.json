{
    "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection. (arXiv:2310.06498v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costin",
    "link": "http://arxiv.org/abs/2310.06498",
    "context": "Title: A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection. (arXiv:2310.06498v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costin",
    "path": "papers/23/10/2310.06498.json",
    "total_tokens": 890,
    "translated_title": "一种新的基准和倒向验证方法用于段落级幻觉检测",
    "translated_abstract": "大型语言模型(LLM)在与人类在真实场景中的有效协作方面展示出了它们的能力。然而，LLM很容易生成幻觉，即编造不正确的文本和未经验证的信息，这在部署于重要任务中时可能造成重大损害。在本文中，我们提出了一种基于倒向验证的自检方法，以零资源的方式自动检测事实错误。为了便于未来的研究和评估不同方法，我们构建了一个名为PHD的幻觉检测基准，该基准由ChatGPT生成并由人类标注。与以往的零资源幻觉检测研究不同，我们的方法和基准集专注于段落级别的检测，而不是句子级别的检测。我们在两个数据集上经验性地评估了我们的方法和现有的零资源检测方法。实验结果表明，所提出的方法在性能上明显优于基准方法，同时成本相对较低。",
    "tldr": "本文提出了一种基于倒向验证的自检方法和一个名为PHD的幻觉检测基准，用于自动检测大型语言模型中的事实错误，该方法在段落级别上表现出很好的性能。",
    "en_tdlr": "This paper proposes a self-check approach based on reverse validation and a hallucination detection benchmark called PHD for automatically detecting factual errors in large language models at the passage level, achieving significant performance improvement compared to baseline methods."
}