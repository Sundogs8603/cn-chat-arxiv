{
    "title": "Estimating the Rate-Distortion Function by Wasserstein Gradient Descent. (arXiv:2310.18908v1 [cs.IT])",
    "abstract": "In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$ describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining $R(D)$ for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate $R(D)$ from the perspective of optimal transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-lik",
    "link": "http://arxiv.org/abs/2310.18908",
    "context": "Title: Estimating the Rate-Distortion Function by Wasserstein Gradient Descent. (arXiv:2310.18908v1 [cs.IT])\nAbstract: In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$ describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining $R(D)$ for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate $R(D)$ from the perspective of optimal transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-lik",
    "path": "papers/23/10/2310.18908.json",
    "total_tokens": 871,
    "translated_title": "通过Wasserstein梯度下降估计速率-失真函数",
    "translated_abstract": "在无损压缩理论中，速率-失真（R-D）函数R(D)描述了在任何给定的保真度（失真）水平下，数据源可以被压缩的程度（比特率）。从最优传输的角度提出了一种估计R(D)的新方法。与经典的Blahut-Arimoto算法在先固定复制分布的支持的基础上不同，我们的Wasserstein梯度下降算法通过移动粒子学习最优复制分布的支持。证明了其局部收敛性，并通过与熵最优传输的联系分析了我们的R-D估计器的样本复杂度。在低速率源上，我们实验上获得了与最先进的神经网络方法相当或更强的界限，同时需要较少的调整和计算工作。我们还强调了与最大似然方法的联系。",
    "tldr": "本文通过Wasserstein梯度下降方法，从最优传输的角度提出了一种估计速率-失真函数R(D)的新方法，该方法在低速率源上取得了与最先进的神经网络方法相当或更强的性能界限，并且需要较少的调整和计算工作。",
    "en_tdlr": "This paper proposes a new method to estimate the rate-distortion function R(D) using Wasserstein gradient descent. The method obtains comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources, with less tuning and computation effort."
}