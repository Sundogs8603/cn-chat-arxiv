{
    "title": "A Diachronic Perspective on User Trust in AI under Uncertainty. (arXiv:2310.13544v1 [cs.CL])",
    "abstract": "In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. We also show that this degradation in trust reduces the success of human-AI collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. Our findings highlight the importance of calibration",
    "link": "http://arxiv.org/abs/2310.13544",
    "context": "Title: A Diachronic Perspective on User Trust in AI under Uncertainty. (arXiv:2310.13544v1 [cs.CL])\nAbstract: In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. We also show that this degradation in trust reduces the success of human-AI collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. Our findings highlight the importance of calibration",
    "path": "papers/23/10/2310.13544.json",
    "total_tokens": 975,
    "translated_title": "用户在不确定情况下对人工智能的信任的历时视角",
    "translated_abstract": "在人工智能与人类的合作中，用户基于其可靠性和决策呈现方式建立了对AI系统的心理模型，例如系统置信度的展示和输出的解释。现代自然语言处理系统往往未经校准，导致自信但错误的预测破坏了用户的信任。为了建立可信赖的人工智能，我们必须了解用户信任是如何形成的，以及如何在潜在破坏信任的事件后重新获得信任。我们使用一种博弈游戏研究了用户信任在面对这些破坏信任事件时的演变。我们发现，即使只有几个错误实例和不准确的置信度估计也会破坏用户的信任和表现，并且恢复过程非常缓慢。我们还表明，信任的降级会降低人工智能与人类合作的成功，并且不同类型的校准错误，不自信但正确和自信但错误，会对用户信任产生不同的负面影响。我们的研究结果强调了校准的重要性。",
    "tldr": "用户对人工智能的信任会受到系统置信度和解释方式的影响。研究发现，即使只有少数错误预测，用户的信任和表现也会受到破坏，恢复时间缓慢。不同类型的校准错误对用户信任有不同的负面影响。这些发现强调了校准的重要性。",
    "en_tdlr": "User trust in AI is influenced by system confidence and explanation. Research finds that even a few incorrect predictions can damage user trust and performance, with slow recovery. Different types of calibration errors have varying negative effects on user trust. These findings highlight the importance of calibration."
}