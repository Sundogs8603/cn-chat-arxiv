{
    "title": "When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])",
    "abstract": "We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\\epsilon$-suboptimal policy with respect to $\\Pi$? Towards that end, we introduce a new complexity measure, called the \\emph{spanning capacity}, that depends solely on the set $\\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \\emph{sunf",
    "link": "http://arxiv.org/abs/2310.06113",
    "context": "Title: When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])\nAbstract: We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\\epsilon$-suboptimal policy with respect to $\\Pi$? Towards that end, we introduce a new complexity measure, called the \\emph{spanning capacity}, that depends solely on the set $\\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \\emph{sunf",
    "path": "papers/23/10/2310.06113.json",
    "total_tokens": 958,
    "translated_title": "什么时候是基于不可知激励强化学习的统计可处理性？",
    "translated_abstract": "本文研究了基于不可知的PAC强化学习（RL）的问题：给定一个策略类别Π，需要和一个未知的有可能有大状态和动作空间的MDP进行多少轮互动来学习一个关于Π的ε-次优策略？为此，我们引入了一个新的复杂度度量，称为“跨越容量”，它仅依赖于策略类别Π，并且与MDP的动态无关。通过一个生成模型，我们证明了对于任何策略类别Π，有界的跨越容量可以刻画PAC可学习性。然而，对于在线RL来说，情况更加微妙。我们证明了存在一个具有有界跨越容量的策略类别Π，需要超多项式数量的样本才能学习。这揭示了在不同生成和在线访问模型之间以及在线访问下的确定/随机MDP之间的不确定可学习性之间的出乎意料的差异。在积极的方面，我们识别出一个额外的“太阳”",
    "tldr": "本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。"
}