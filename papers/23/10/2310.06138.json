{
    "title": "Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])",
    "abstract": "Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th",
    "link": "http://arxiv.org/abs/2310.06138",
    "context": "Title: Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])\nAbstract: Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th",
    "path": "papers/23/10/2310.06138.json",
    "total_tokens": 897,
    "translated_title": "从嘈杂的移动模态中预测布局序列",
    "translated_abstract": "轨迹预测在理解行人移动方面扮演着重要角色，适用于自动驾驶和机器人等应用。当前的轨迹预测模型依赖于视觉模态的长、完整且准确观测到的序列。然而，现实世界中经常出现相机遮挡、遗漏对象或由于环境因素导致对象不在视线范围内的情况，导致轨迹不完整或噪声较大。为了克服这些限制，我们提出了一种新方法LTrajDiff，将被遮挡或不在视线范围内的物体与完全可见轨迹的物体同等重要。LTrajDiff利用移动手机的传感器数据克服不在视线范围内的约束，但引入了新的挑战，如模态融合、噪声数据和缺乏空间布局和物体尺寸信息。我们使用去噪扩散模型，采用从粗到精的扩散策略，从嘈杂的移动数据中预测精确的布局序列。",
    "tldr": "提出一种名为LTrajDiff的新方法，从噪声移动数据中预测精确的布局序列，克服了由嘈杂数据、不完整轨迹和环境因素导致的视觉障碍。"
}