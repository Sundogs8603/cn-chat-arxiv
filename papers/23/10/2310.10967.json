{
    "title": "EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])",
    "abstract": "The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satis",
    "link": "http://arxiv.org/abs/2310.10967",
    "context": "Title: EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])\nAbstract: The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satis",
    "path": "papers/23/10/2310.10967.json",
    "total_tokens": 941,
    "translated_title": "EXMODD:一种解释性多模态开放领域对话数据集",
    "translated_abstract": "高质量数据的需求一直是阻碍对话任务研究的关键问题。最近的研究尝试通过手工、网络爬虫和大规模预训练模型构建数据集。然而，人工数据成本高昂，从互联网收集的数据往往包含通俗回答、无意义的陈述和有害对话。通过大型模型进行自动数据生成是一种成本效益较高的方法，但对于多模态开放领域对话任务，仍存在三个缺点：1) 目前还没有能接受多模态输入的开源大型模型；2) 模型生成的内容缺乏可解释性；3) 生成的数据通常难以进行质量控制并需要大量资源进行收集。为了减轻数据收集中的重要人力和资源开支，我们提出了一种多模态数据构建框架(MDCF)。MDCF设计适当的提示来推动大规模预训练语言模型生成形式良好且令人满意的内容。",
    "tldr": "提出了一种解释性多模态开放领域对话数据集，通过多模态数据构建框架(MDCF)设计适当的提示，从而解决了大型模型对于多模态输入的缺乏、生成内容缺乏可解释性以及数据质量控制的问题。"
}