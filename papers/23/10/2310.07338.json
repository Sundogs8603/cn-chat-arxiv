{
    "title": "Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])",
    "abstract": "Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference",
    "link": "http://arxiv.org/abs/2310.07338",
    "context": "Title: Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])\nAbstract: Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference",
    "path": "papers/23/10/2310.07338.json",
    "total_tokens": 959,
    "translated_title": "面向基于表格数据的学习的基础模型研究",
    "translated_abstract": "基于表格数据的学习支撑着众多实际应用。尽管在开发针对表格数据的有效学习模型方面已经做出了相当大的努力，但目前可转移的表格模型仍然处于初级阶段，要么缺乏直接指令跟随新任务的支持，要么忽视从不同的表格数据集中获取基础知识和能力。在本文中，我们提出了Tabular Foundation Models (TabFMs)来克服这些限制。TabFMs利用生成型表格学习的潜力，采用预训练的大规模语言模型 (LLM) 作为基础模型，并使用经过专门设计的目标在大范围的表格数据集上进行微调。这种方法赋予TabFMs深刻的理解和普遍的能力，对于表格数据的学习至关重要。我们的评估强调了TabFM的有效性：它不仅在零样本和上下文推理等遵循指令的任务中明显出色，",
    "tldr": "本文提出了Tabular Foundation Models (TabFMs)，通过利用生成型表格学习的潜力，采用预训练的大规模语言模型作为基础模型，并在广泛的表格数据集上进行微调，赋予TabFMs深刻的理解和普遍的能力，从而克服了当前可转移的表格模型的限制。"
}