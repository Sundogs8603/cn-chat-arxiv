{
    "title": "X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity. (arXiv:2310.17166v1 [cs.CL])",
    "abstract": "Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is mo",
    "link": "http://arxiv.org/abs/2310.17166",
    "context": "Title: X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity. (arXiv:2310.17166v1 [cs.CL])\nAbstract: Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is mo",
    "path": "papers/23/10/2310.17166.json",
    "total_tokens": 875,
    "translated_title": "X-SNS: 通过子网络相似性进行跨语言迁移预测",
    "translated_abstract": "跨语言迁移（XLT）是多语言语言模型的一种新兴能力，当在未包含在微调过程中的语言中评估时，能够在很大程度上保留其在任务上的性能。尽管英语由于其广泛使用，通常被视为各种任务中模型适应的主要语言，但最近的研究表明，通过根据特定条件选择最合适的源语言，可以放大XLT的有效性。在这项工作中，我们提出将两种语言之间的子网络相似性利用为在XLT环境中预测语言兼容性的代理。我们的方法是以模型为导向的，更好地反映了基础模型的内在工作方式。此外，它只需要候选语言的适量原始文本，与大多数依赖外部资源的以前的方法进行区分。在实验中，我们证明了我们的方法是更有效的方法。",
    "tldr": "本工作通过利用两种语言之间的子网络相似性作为预测XLT中语言兼容性的代理，提出了一种更有效的模型导向方法，不依赖于外部资源，仅需要候选语言的适量原始文本。",
    "en_tdlr": "This work proposes a more effective model-oriented approach for predicting language compatibility in XLT by utilizing sub-network similarity between two languages as a proxy, which does not rely on external resources and only requires a moderate amount of raw text from candidate languages."
}