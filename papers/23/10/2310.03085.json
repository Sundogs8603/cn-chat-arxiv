{
    "title": "Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising. (arXiv:2310.03085v1 [cs.LG])",
    "abstract": "We consider the problem of denoising with the help of prior information taken from a database of clean signals or images. Denoising with variational methods is very efficient if a regularizer well adapted to the nature of the data is available. Thanks to the maximum a posteriori Bayesian framework, such regularizer can be systematically linked with the distribution of the data. With deep neural networks (DNN), complex distributions can be recovered from a large training database.To reduce the computational burden of this task, we adapt the compressive learning framework to the learning of regularizers parametrized by DNN. We propose two variants of stochastic gradient descent (SGD) for the recovery of deep regularization parameters from a heavily compressed database. These algorithms outperform the initially proposed method that was limited to low-dimensional signals, each iteration using information from the whole database. They also benefit from classical SGD convergence guarantees. ",
    "link": "http://arxiv.org/abs/2310.03085",
    "context": "Title: Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising. (arXiv:2310.03085v1 [cs.LG])\nAbstract: We consider the problem of denoising with the help of prior information taken from a database of clean signals or images. Denoising with variational methods is very efficient if a regularizer well adapted to the nature of the data is available. Thanks to the maximum a posteriori Bayesian framework, such regularizer can be systematically linked with the distribution of the data. With deep neural networks (DNN), complex distributions can be recovered from a large training database.To reduce the computational burden of this task, we adapt the compressive learning framework to the learning of regularizers parametrized by DNN. We propose two variants of stochastic gradient descent (SGD) for the recovery of deep regularization parameters from a heavily compressed database. These algorithms outperform the initially proposed method that was limited to low-dimensional signals, each iteration using information from the whole database. They also benefit from classical SGD convergence guarantees. ",
    "path": "papers/23/10/2310.03085.json",
    "total_tokens": 956,
    "translated_title": "无批量随机梯度下降用于图像去噪的深度正则化压缩学习",
    "translated_abstract": "本文考虑了利用来自干净信号或图像数据库的先验信息进行去噪的问题。如果能够获得适应数据性质的正则化器，采用变分方法进行去噪非常高效。通过最大后验贝叶斯框架，这样的正则化器可以与数据的分布系统地关联起来。利用深度神经网络(DNN)，可以从一大规模的训练数据库中恢复复杂的分布。为了减少任务的计算负担，我们将压缩学习框架调整为用DNN参数化的正则化器的学习方法。我们提出了两种随机梯度下降(SGD)的变体，用于从一个被大幅压缩的数据库中恢复深度正则化参数。这些算法优于最初提出的只针对低维信号的方法，每个迭代都使用整个数据库的信息。它们还可以受益于经典的SGD收敛保证。",
    "tldr": "本文介绍了一种无批量随机梯度下降方法，用于图像去噪的深度正则化压缩学习。该方法通过利用来自干净信号或图像数据库的先验信息，将正则化器与数据分布关联起来，从大规模训练数据库中恢复复杂的分布，以减少计算负担。"
}