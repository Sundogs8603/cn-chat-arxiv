{
    "title": "A qualitative difference between gradient flows of convex functions in finite- and infinite-dimensional Hilbert spaces. (arXiv:2310.17610v1 [math.OC])",
    "abstract": "We consider gradient flow/gradient descent and heavy ball/accelerated gradient descent optimization for convex objective functions. In the gradient flow case, we prove the following:  1. If $f$ does not have a minimizer, the convergence $f(x_t)\\to \\inf f$ can be arbitrarily slow.  2. If $f$ does have a minimizer, the excess energy $f(x_t) - \\inf f$ is integrable/summable in time. In particular, $f(x_t) - \\inf f = o(1/t)$ as $t\\to\\infty$.  3. In Hilbert spaces, this is optimal: $f(x_t) - \\inf f$ can decay to $0$ as slowly as any given function which is monotone decreasing and integrable at $\\infty$, even for a fixed quadratic objective.  4. In finite dimension (or more generally, for all gradient flow curves of finite length), this is not optimal: We prove that there are convex monotone decreasing integrable functions $g(t)$ which decrease to zero slower than $f(x_t)-\\inf f$ for the gradient flow of any convex function on $\\mathbb R^d$. For instance, we show that any gradient flow $x_t$",
    "link": "http://arxiv.org/abs/2310.17610",
    "context": "Title: A qualitative difference between gradient flows of convex functions in finite- and infinite-dimensional Hilbert spaces. (arXiv:2310.17610v1 [math.OC])\nAbstract: We consider gradient flow/gradient descent and heavy ball/accelerated gradient descent optimization for convex objective functions. In the gradient flow case, we prove the following:  1. If $f$ does not have a minimizer, the convergence $f(x_t)\\to \\inf f$ can be arbitrarily slow.  2. If $f$ does have a minimizer, the excess energy $f(x_t) - \\inf f$ is integrable/summable in time. In particular, $f(x_t) - \\inf f = o(1/t)$ as $t\\to\\infty$.  3. In Hilbert spaces, this is optimal: $f(x_t) - \\inf f$ can decay to $0$ as slowly as any given function which is monotone decreasing and integrable at $\\infty$, even for a fixed quadratic objective.  4. In finite dimension (or more generally, for all gradient flow curves of finite length), this is not optimal: We prove that there are convex monotone decreasing integrable functions $g(t)$ which decrease to zero slower than $f(x_t)-\\inf f$ for the gradient flow of any convex function on $\\mathbb R^d$. For instance, we show that any gradient flow $x_t$",
    "path": "papers/23/10/2310.17610.json",
    "total_tokens": 1237,
    "translated_title": "有限维和无限维希尔伯特空间中凸函数梯度流的定性差异",
    "translated_abstract": "我们研究了凸目标函数的梯度流/梯度下降和重球/加速梯度下降优化方法。在梯度流情况下，我们证明了以下结论：1. 如果$f$没有最小值，收敛$f(x_t)\\to \\inf f$可能非常缓慢。2. 如果$f$有最小值，过剩能量$f(x_t) - \\inf f$在时间上是可积/可和的。特别地，当$t\\to\\infty$时，$f(x_t) - \\inf f = o(1/t)$。3. 在希尔伯特空间中，这是最优的：$f(x_t) - \\inf f$的衰减速度可以与任何给定的且在$\\infty$处单调递减和可积的函数一样慢，即使对于固定的二次目标函数也是如此。4. 在有限维（或更一般地，对于所有有限长度的梯度流曲线），这并非最优：我们证明了对于$\\mathbb R^d$上任何凸函数的梯度流，存在单调递减可积函数$g(t)$，其衰减速度比$f(x_t)-\\inf f$更慢。例如，我们证明了任何梯度流$x_t$",
    "tldr": "本文研究了凸函数梯度流在有限维和无限维希尔伯特空间中的差异，证明了在梯度流情况下，如果函数没有最小值，收敛速度可能非常缓慢；如果函数有最小值，过剩能量在时间上是可积的。此外，在希尔伯特空间中，过剩能量的衰减速度可以很慢，甚至可以与给定的单调递减可积函数一样慢。然而，在有限维情况下，存在比过剩能量衰减更慢的单调递减可积函数。",
    "en_tdlr": "This paper investigates the differences between gradient flows of convex functions in finite- and infinite-dimensional Hilbert spaces. It proves that in the case of gradient flow, the convergence can be arbitrarily slow if the function does not have a minimum, and the excess energy is integrable in time if the function has a minimum. Moreover, in Hilbert spaces, the decay rate of the excess energy can be as slow as any given monotone decreasing integrable function, while in finite dimension, there exist monotone decreasing integrable functions that decay slower than the excess energy."
}