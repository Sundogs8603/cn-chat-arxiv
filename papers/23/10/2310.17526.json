{
    "title": "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])",
    "abstract": "Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom",
    "link": "http://arxiv.org/abs/2310.17526",
    "context": "Title: Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])\nAbstract: Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom",
    "path": "papers/23/10/2310.17526.json",
    "total_tokens": 1024,
    "translated_title": "大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。",
    "translated_abstract": "系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。",
    "tldr": "本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。"
}