{
    "title": "Accelerating Split Federated Learning over Wireless Communication Networks. (arXiv:2310.15584v1 [cs.LG])",
    "abstract": "The development of artificial intelligence (AI) provides opportunities for the promotion of deep neural network (DNN)-based applications. However, the large amount of parameters and computational complexity of DNN makes it difficult to deploy it on edge devices which are resource-constrained. An efficient method to address this challenge is model partition/splitting, in which DNN is divided into two parts which are deployed on device and server respectively for co-training or co-inference. In this paper, we consider a split federated learning (SFL) framework that combines the parallel model training mechanism of federated learning (FL) and the model splitting structure of split learning (SL). We consider a practical scenario of heterogeneous devices with individual split points of DNN. We formulate a joint problem of split point selection and bandwidth allocation to minimize the system latency. By using alternating optimization, we decompose the problem into two sub-problems and solve ",
    "link": "http://arxiv.org/abs/2310.15584",
    "context": "Title: Accelerating Split Federated Learning over Wireless Communication Networks. (arXiv:2310.15584v1 [cs.LG])\nAbstract: The development of artificial intelligence (AI) provides opportunities for the promotion of deep neural network (DNN)-based applications. However, the large amount of parameters and computational complexity of DNN makes it difficult to deploy it on edge devices which are resource-constrained. An efficient method to address this challenge is model partition/splitting, in which DNN is divided into two parts which are deployed on device and server respectively for co-training or co-inference. In this paper, we consider a split federated learning (SFL) framework that combines the parallel model training mechanism of federated learning (FL) and the model splitting structure of split learning (SL). We consider a practical scenario of heterogeneous devices with individual split points of DNN. We formulate a joint problem of split point selection and bandwidth allocation to minimize the system latency. By using alternating optimization, we decompose the problem into two sub-problems and solve ",
    "path": "papers/23/10/2310.15584.json",
    "total_tokens": 947,
    "translated_title": "加速无线通信网络上的分裂联合学习",
    "translated_abstract": "人工智能的发展为基于深度神经网络的应用提供了机遇。然而，庞大的参数量和计算复杂度使得将深度神经网络部署在资源受限的边缘设备上变得困难。针对这一挑战，一种有效的方法是模型分割/分裂，即将深度神经网络分为两部分，分别部署在设备和服务器上进行共同训练或共同推理。本文考虑了一种结合了联合学习的并行模型训练机制和分裂学习的模型分割结构的分裂联合学习（SFL）框架。我们考虑了具有个体分割点的异构设备的实际场景。我们建立了一个联合问题，即选择分割点和带宽分配以最小化系统延迟。通过使用交替优化，我们将问题分解为两个子问题，并进行求解。",
    "tldr": "本论文提出了一种加速无线通信网络上的分裂联合学习的方法。通过将深度神经网络分为两部分，在设备和服务器上分别进行共同训练或共同推理，以解决在资源受限的边缘设备上部署大规模深度神经网络的难题。通过选择分割点和带宽分配以最小化系统延迟，实现了联合优化。",
    "en_tdlr": "This paper proposes a method to accelerate split federated learning over wireless communication networks. By dividing the deep neural network into two parts and conducting co-training or co-inference on individual devices and servers, it addresses the challenge of deploying large-scale deep neural networks on resource-constrained edge devices. Through joint optimization of split point selection and bandwidth allocation, system latency is minimized."
}