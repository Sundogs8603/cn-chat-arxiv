{
    "title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])",
    "abstract": "Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world t",
    "link": "http://arxiv.org/abs/2310.15239",
    "context": "Title: CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])\nAbstract: Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world t",
    "path": "papers/23/10/2310.15239.json",
    "total_tokens": 943,
    "translated_title": "CRoW: 在真实世界任务中对常识推理进行基准测试",
    "translated_abstract": "自然语言处理（NLP）中关于常识推理的研究近年来取得了许多新的数据集和基准测试。然而，大多数这些数据集在人工场景下构建了常识推理挑战，这些场景并不能反映真实世界NLP系统所设计用于解决的任务。在这项工作中，我们提出了CRoW，一个手工策划的多任务基准测试，用于评估模型在六个真实世界NLP任务中应用常识推理的能力。CRoW使用多阶段的数据收集流程构建，通过违反常识的扰动重写现有数据集中的示例。我们利用CRoW来研究NLP系统在物理、时间和社交推理等不同常识知识维度上的表现。我们发现，在CRoW上评估NLP系统时与人类相比存在显著的性能差距，显示出常识推理在真实世界中还远未解决。",
    "tldr": "CRoW是一个手工策划的多任务基准测试，用于评估模型在真实世界NLP任务中应用常识推理的能力。该基准测试揭示了NLP系统在常识推理方面与人类之间存在显著的性能差距，表明常识推理在真实世界中仍然远未解决。",
    "en_tdlr": "CRoW is a manually-curated multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in real-world NLP tasks. The benchmark reveals a significant performance gap between NLP systems and humans in commonsense reasoning, indicating that it is still far from being solved in real-world scenarios."
}