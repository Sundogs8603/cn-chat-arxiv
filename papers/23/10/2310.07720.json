{
    "title": "Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning. (arXiv:2310.07720v1 [cs.LG])",
    "abstract": "Activation functions (AFs) are crucial components of deep neural networks (DNNs), having a significant impact on their performance. An activation function in a DNN is typically a smooth, nonlinear function that transforms an input signal into an output signal for the subsequent layer. In this paper, we propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU) activation functions. PLTanh is differentiable at all points and addresses the 'dying ReLU' problem by ensuring a non-zero gradient for negative inputs, consistent with the behavior of LReLU. By integrating the unique advantages of these two diverse activation functions, PLTanh facilitates the learning of more intricate nonlinear relationships within the network. This paper presents an empirical evaluation of PLTanh against established activation functions, namely ReLU, LReLU, and ALReLU utilizing five diverse datasets.",
    "link": "http://arxiv.org/abs/2310.07720",
    "context": "Title: Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning. (arXiv:2310.07720v1 [cs.LG])\nAbstract: Activation functions (AFs) are crucial components of deep neural networks (DNNs), having a significant impact on their performance. An activation function in a DNN is typically a smooth, nonlinear function that transforms an input signal into an output signal for the subsequent layer. In this paper, we propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU) activation functions. PLTanh is differentiable at all points and addresses the 'dying ReLU' problem by ensuring a non-zero gradient for negative inputs, consistent with the behavior of LReLU. By integrating the unique advantages of these two diverse activation functions, PLTanh facilitates the learning of more intricate nonlinear relationships within the network. This paper presents an empirical evaluation of PLTanh against established activation functions, namely ReLU, LReLU, and ALReLU utilizing five diverse datasets.",
    "path": "papers/23/10/2310.07720.json",
    "total_tokens": 838,
    "translated_title": "Parametric Leaky Tanh：一种新的深度学习混合激活函数",
    "translated_abstract": "激活函数对于深度神经网络的性能有着重要的影响。本文提出了Parametric Leaky Tanh (PLTanh)，一种新颖的混合激活函数，旨在结合Tanh和Leaky ReLU (LReLU)激活函数的优点。PLTanh在所有点上可微分，并通过保证负输入的非零梯度，解决了'dying ReLU'问题，与LReLU的行为一致。通过整合这两种不同的激活函数的独特优势，PLTanh有助于在网络内部学习更复杂的非线性关系。本文通过在五个不同的数据集上对比了PLTanh与常见的激活函数ReLU、LReLU和ALReLU的实证评估。",
    "tldr": "本文提出了一种新颖的激活函数Parametric Leaky Tanh (PLTanh)，结合了Tanh和Leaky ReLU (LReLU)的优点，解决了'dying ReLU'问题，有助于在网络内部学习更复杂的非线性关系。"
}