{
    "title": "Learning Recurrent Models with Temporally Local Rules. (arXiv:2310.13284v1 [cs.LG])",
    "abstract": "Fitting generative models to sequential data typically involves two recursive computations through time, one forward and one backward. The latter could be a computation of the loss gradient (as in backpropagation through time), or an inference algorithm (as in the RTS/Kalman smoother). The backward pass in particular is computationally expensive (since it is inherently serial and cannot exploit GPUs), and difficult to map onto biological processes. Work-arounds have been proposed; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.",
    "link": "http://arxiv.org/abs/2310.13284",
    "context": "Title: Learning Recurrent Models with Temporally Local Rules. (arXiv:2310.13284v1 [cs.LG])\nAbstract: Fitting generative models to sequential data typically involves two recursive computations through time, one forward and one backward. The latter could be a computation of the loss gradient (as in backpropagation through time), or an inference algorithm (as in the RTS/Kalman smoother). The backward pass in particular is computationally expensive (since it is inherently serial and cannot exploit GPUs), and difficult to map onto biological processes. Work-arounds have been proposed; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.",
    "path": "papers/23/10/2310.13284.json",
    "total_tokens": 792,
    "translated_title": "使用时间局部规则学习循环模型",
    "translated_abstract": "将生成模型拟合到顺序数据通常需要通过时间进行两个递归计算，一个正向计算，一个反向计算。后者可以是损失梯度的计算（如反向传播），也可以是推理算法的计算（如RTS/Kalman平滑器）。特别是反向传递计算在计算上很昂贵（因为它是串行的，无法利用GPU），而且很难映射到生物过程。已经提出了一些解决方法；在这里，我们探索了一个非常不同的方法：要求生成模型学习当前状态和先前状态的联合分布，而不仅仅是转移概率。我们在玩具数据集上展示了使用这一原则的不同架构可以学习通常需要反向传递计算的数据方面。",
    "tldr": "本研究提出一种新的方法，要求生成模型学习当前状态和先前状态的联合分布，以替代传统的反向传递计算。在玩具数据集上的实验证明，这种方法可以学习到通常需要反向传递计算的数据方面。",
    "en_tdlr": "This study proposes a novel approach that requires generative models to learn the joint distribution over current and previous states instead of traditional backward pass computation. Experimental results on toy datasets show that this approach can learn aspects of the data that typically require backward pass computation."
}