{
    "title": "Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis. (arXiv:2310.08410v1 [stat.ME])",
    "abstract": "Large language models such as ChatGPT are increasingly explored in medical domains. However, the absence of standard guidelines for performance evaluation has led to methodological inconsistencies. This study aims to summarize the available evidence on evaluating ChatGPT's performance in medicine and provide direction for future research. We searched ten medical literature databases on June 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were identified, of which 60 were reviewed and summarized in this paper and 17 were included in the meta-analysis. The analysis showed that ChatGPT displayed an overall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing medical queries. However, the studies varied in question resource, question-asking process, and evaluation metrics. Moreover, many studies failed to report methodological details, including the version of ChatGPT and whether each question was used independently or repeatedly. Our findings revealed that ",
    "link": "http://arxiv.org/abs/2310.08410",
    "context": "Title: Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis. (arXiv:2310.08410v1 [stat.ME])\nAbstract: Large language models such as ChatGPT are increasingly explored in medical domains. However, the absence of standard guidelines for performance evaluation has led to methodological inconsistencies. This study aims to summarize the available evidence on evaluating ChatGPT's performance in medicine and provide direction for future research. We searched ten medical literature databases on June 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were identified, of which 60 were reviewed and summarized in this paper and 17 were included in the meta-analysis. The analysis showed that ChatGPT displayed an overall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing medical queries. However, the studies varied in question resource, question-asking process, and evaluation metrics. Moreover, many studies failed to report methodological details, including the version of ChatGPT and whether each question was used independently or repeatedly. Our findings revealed that ",
    "path": "papers/23/10/2310.08410.json",
    "total_tokens": 955,
    "translated_title": "ChatGPT生成的医学回复的评估：一项系统评述和荟萃分析",
    "translated_abstract": "越来越多地在医学领域探索使用ChatGPT等大型语言模型。然而，缺乏性能评估的标准指南导致了方法上的不一致。本研究旨在总结关于评估ChatGPT在医学中性能的现有证据，并为未来的研究指明方向。我们在2023年6月15日使用关键词“ChatGPT”在十个医学文献数据库中进行搜索。共鉴定了3520篇文章，其中60篇在本文中进行了回顾和总结，17篇进行了荟萃分析。分析结果显示，ChatGPT在处理医学查询时的整体综合准确率为56%（95% CI：51%-60%，I2 = 87%）。然而，这些研究在问题资源、提问过程和评估指标上存在差异。此外，许多研究未能报告方法细节，包括ChatGPT的版本以及每个问题是独立使用还是重复使用。我们的研究结果表明，...",
    "tldr": "这项研究总结了关于评估ChatGPT在医学中性能的现有证据，并发现ChatGPT在处理医学查询时的准确率为56%。然而，由于缺乏评估标准，研究中存在方法上的不一致和详细信息不全的问题。",
    "en_tdlr": "This study provides a summary of the available evidence on evaluating ChatGPT's performance in medicine and finds that ChatGPT has an accuracy of 56% in addressing medical queries. However, methodological inconsistencies and lack of reporting details are identified due to the absence of evaluation standards."
}