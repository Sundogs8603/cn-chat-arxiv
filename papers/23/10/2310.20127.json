{
    "title": "Improving Prompt Tuning with Learned Prompting Layers. (arXiv:2310.20127v1 [cs.CL])",
    "abstract": "Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \\underline{S}elective \\underline{P}rompt \\underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer t",
    "link": "http://arxiv.org/abs/2310.20127",
    "context": "Title: Improving Prompt Tuning with Learned Prompting Layers. (arXiv:2310.20127v1 [cs.CL])\nAbstract: Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \\underline{S}elective \\underline{P}rompt \\underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer t",
    "path": "papers/23/10/2310.20127.json",
    "total_tokens": 935,
    "translated_title": "使用学习的提示层改进提示调整",
    "translated_abstract": "提示调整通过在输入嵌入或隐藏状态之前添加软提示，只优化提示来适应预训练模型（PTM）到下游任务。先前的工作手动选择了远非最佳的提示层，并未利用提示调整的潜力。在本工作中，我们提出了一个新颖的框架，选择性提示调整（SPT），通过在每个中间层插入一个由可学习概率门控制的提示来学习选择合适的提示层。我们进一步提出了一种新颖的双层优化框架SPT-DARTS，可以更好地优化可学习门，并改进学习提示层设置的最终提示调整性能。我们在十个基准数据集上进行了广泛实验，包括全数据和少样本情景。结果表明，我们的SPT框架可以比之前的PETuning基准表现更好，且使用的提示层设置相当或更少。",
    "tldr": "这篇论文提出了一种新颖的框架，称为选择性提示调整（SPT），通过学习选择合适的提示层来改进提示调整的性能。作者还提出了一个双层优化框架SPT-DARTS，可以更好地优化可学习的门，并提高提示调整的效果。实验证明，SPT框架在多个基准数据集上表现出色。"
}