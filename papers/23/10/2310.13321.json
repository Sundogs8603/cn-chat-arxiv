{
    "title": "Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting. (arXiv:2310.13321v1 [cs.CL])",
    "abstract": "Recent studies have revealed that grammatical error correction methods in the sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply utilizing adversarial examples in the pre-training or post-training process can significantly enhance the robustness of GEC models to certain types of attack without suffering too much performance loss on clean data. In this paper, we further conduct a thorough robustness evaluation of cutting-edge GEC methods for four different types of adversarial attacks and propose a simple yet very effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the augmenting data from the GEC models themselves in the post-training process and introducing regularization data for cycle training, our proposed method can effectively improve the model robustness of well-trained GEC models with only a few more training epochs as an extra cost. More concretely, further training on the regularization data can prevent the GEC models from over-",
    "link": "http://arxiv.org/abs/2310.13321",
    "context": "Title: Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting. (arXiv:2310.13321v1 [cs.CL])\nAbstract: Recent studies have revealed that grammatical error correction methods in the sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply utilizing adversarial examples in the pre-training or post-training process can significantly enhance the robustness of GEC models to certain types of attack without suffering too much performance loss on clean data. In this paper, we further conduct a thorough robustness evaluation of cutting-edge GEC methods for four different types of adversarial attacks and propose a simple yet very effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the augmenting data from the GEC models themselves in the post-training process and introducing regularization data for cycle training, our proposed method can effectively improve the model robustness of well-trained GEC models with only a few more training epochs as an extra cost. More concretely, further training on the regularization data can prevent the GEC models from over-",
    "path": "papers/23/10/2310.13321.json",
    "total_tokens": 952,
    "translated_title": "超越难样本：循环自增强的稳健有效的语法错误修正方法",
    "translated_abstract": "最近的研究发现，序列到序列范式下的语法错误修正方法容易受到对抗攻击，而仅仅利用对抗性示例在预训练或后训练过程中可以显著提高GEC模型对某些类型攻击的鲁棒性，而在干净数据上几乎不会损失太多性能。在本文中，我们进一步对最先进的四种不同类型对抗攻击下的GEC方法进行了彻底的鲁棒性评估，并相应地提出了一种简单而非常有效的循环自增强（CSA）方法。通过利用GEC模型自身在后训练过程中生成的增强数据，并引入循环训练的正则化数据，我们提出的方法可以在只有几个额外的训练轮次作为额外成本的情况下，有效地提高经过良好训练的GEC模型的模型鲁棒性。具体而言，对正则化数据的深入训练可以防止GEC模型过度训练。",
    "tldr": "本文研究了序列到序列范式下的语法错误修正方法的鲁棒性问题，并提出了一种循环自增强的方法，通过利用模型自身生成的增强数据和循环训练的正则化数据，有效提高模型的鲁棒性。"
}