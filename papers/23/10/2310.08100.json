{
    "title": "Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])",
    "abstract": "Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env",
    "link": "http://arxiv.org/abs/2310.08100",
    "context": "Title: Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])\nAbstract: Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env",
    "path": "papers/23/10/2310.08100.json",
    "total_tokens": 845,
    "translated_title": "生成内在优化：具有模型学习的内在控制",
    "translated_abstract": "未来序列代表在环境中执行动作后的结果。当基于信息论概念的相互信息驱动时，它寻求最具信息量的结果。显式结果可能因状态、回报或轨迹而异，用于不同目的，如学分分配或模仿学习。然而，将内在动机与奖励最大化结合的固有性质往往被忽视。在这项工作中，我们提出了一种变分方法，共同学习估计相互信息和动力学模型的必要数量，为合并不同形式的感兴趣结果提供了一个通用框架。结合到策略迭代方案中，我们的方法保证收敛到最优策略。虽然我们主要关注理论分析，但我们的方法打开了利用带有模型学习的内在控制以提高样本效率并纳入环境不确定性的可能性。",
    "tldr": "这项工作提出了一种生成内在优化的方法，通过结合模型学习和内在控制，实现了对不同形式结果的综合处理。这种方法保证了收敛到最优策略，有助于提高样本效率并考虑环境不确定性。",
    "en_tdlr": "This work presents a generative intrinsic optimization method that integrates model learning and intrinsic control, achieving comprehensive handling of different forms of outcomes. The proposed approach guarantees convergence to the optimal policy, enhancing sample efficiency and considering environmental uncertainty."
}