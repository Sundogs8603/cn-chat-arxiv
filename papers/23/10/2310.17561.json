{
    "title": "Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])",
    "abstract": "Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ",
    "link": "http://arxiv.org/abs/2310.17561",
    "context": "Title: Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])\nAbstract: Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ",
    "path": "papers/23/10/2310.17561.json",
    "total_tokens": 863,
    "translated_title": "RNN训练中的分歧和损失跳跃",
    "translated_abstract": "循环神经网络（RNN）是用于建模和预测序列数据以及从观测时间序列中推断动力系统（DS）的常用机器学习工具。DS理论的概念已被用于进一步理解经过训练的RNN如何解决复杂任务以及训练过程本身。分歧是DS中特别重要的现象，包括RNN，在系统的一个或多个参数变化时，指系统的动力行为的拓扑（定性）变化。了解RNN的分歧结构将有助于推断其许多计算和动力属性，例如对参数变化的敏感性或训练过程中的行为。特别是，分歧可能解释RNN训练中观察到的突然损失跳跃，这可能严重阻碍训练过程。在这里，我们首先数学地证明了针对一类基于ReLU的RNN，确实存在一些分歧。",
    "tldr": "这篇论文研究了在RNN训练中的分歧现象和损失跳跃，并证明了在特定类型的RNN中存在着某些分歧。",
    "en_tdlr": "This paper investigates the phenomena of bifurcations and loss jumps in RNN training, and proves the existence of certain bifurcations in a specific class of RNNs."
}