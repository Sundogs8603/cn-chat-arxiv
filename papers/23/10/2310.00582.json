{
    "title": "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
    "abstract": "arXiv:2310.00582v3 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in various multi-modal tasks. Nevertheless, their performance in fine-grained image understanding tasks is still limited. To address this issue, this paper proposes a new framework to enhance the fine-grained image understanding abilities of MLLMs. Specifically, we present a new method for constructing the instruction tuning dataset at a low cost by leveraging annotations in existing datasets. A self-consistent bootstrapping method is also introduced to extend existing dense object annotations into high-quality referring-expression-bounding-box pairs. These methods enable the generation of high-quality instruction data which includes a wide range of fundamental abilities essential for fine-grained image perception. Moreover, we argue that the visual encoder should be tuned during instruction tuning to mitigate the gap between full image perception and ",
    "link": "https://arxiv.org/abs/2310.00582",
    "context": "Title: Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs\nAbstract: arXiv:2310.00582v3 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in various multi-modal tasks. Nevertheless, their performance in fine-grained image understanding tasks is still limited. To address this issue, this paper proposes a new framework to enhance the fine-grained image understanding abilities of MLLMs. Specifically, we present a new method for constructing the instruction tuning dataset at a low cost by leveraging annotations in existing datasets. A self-consistent bootstrapping method is also introduced to extend existing dense object annotations into high-quality referring-expression-bounding-box pairs. These methods enable the generation of high-quality instruction data which includes a wide range of fundamental abilities essential for fine-grained image perception. Moreover, we argue that the visual encoder should be tuned during instruction tuning to mitigate the gap between full image perception and ",
    "path": "papers/23/10/2310.00582.json",
    "total_tokens": 855,
    "translated_title": "Pink: 揭示引用理解在多模态LLMs中的能力",
    "translated_abstract": "多模态大型语言模型（MLLMs）在各种多模态任务中表现出了显著的能力。然而，它们在细粒度图像理解任务中的性能仍然有限。为了解决这个问题，本文提出了一个增强MLLMs细粒度图像理解能力的新框架。具体地，我们提出了一种通过利用现有数据集中的注释来以低成本构建指令调优数据集的新方法。还引入了一种自一致的自举方法，将现有的密集目标注释扩展为高质量的引用表达-边界框对。这些方法使得生成包含对细粒度图像感知至关重要的广泛基本能力的高质量指令数据成为可能。另外，我们认为在指令调优过程中应该调整视觉编码器，以减轻完成图像感知和指令理解之间的差距。",
    "tldr": "提出了一个新框架来增强MLLMs细粒度图像理解能力，包括通过低成本构建指令调优数据集和引入自一致的自举方法扩展密集目标注释等关键方法。",
    "en_tdlr": "Proposed a new framework to enhance the fine-grained image understanding abilities of MLLMs, including key methods such as constructing the instruction tuning dataset at a low cost and introducing a self-consistent bootstrapping method to extend dense object annotations."
}