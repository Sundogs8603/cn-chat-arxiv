{
    "title": "Nash Regret Guarantees for Linear Bandits. (arXiv:2310.02023v1 [cs.LG])",
    "abstract": "We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening -- referred to as Nash regret -- is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.  We consider the stochastic linear bandits problem over a horizon of $T$ rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward -- associated with each arm in ${X}$ -- is a non-negative, $\\nu$-sub-Poisson random variable. For this setting, we develop an algorithm th",
    "link": "http://arxiv.org/abs/2310.02023",
    "context": "Title: Nash Regret Guarantees for Linear Bandits. (arXiv:2310.02023v1 [cs.LG])\nAbstract: We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening -- referred to as Nash regret -- is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.  We consider the stochastic linear bandits problem over a horizon of $T$ rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward -- associated with each arm in ${X}$ -- is a non-negative, $\\nu$-sub-Poisson random variable. For this setting, we develop an algorithm th",
    "path": "papers/23/10/2310.02023.json",
    "total_tokens": 933,
    "translated_title": "对于线性多臂老虎机，纳什遗憾的保证",
    "translated_abstract": "我们在随机线性多臂老虎机框架中获得了对遗憾的一个更强化的上界，称为纳什遗憾，其定义为线性多臂老虎机算法积累的预期奖励的几何平均与（先验未知的）最优解之间的差异。由于几何平均对应于众所周知的纳什社会福利（NSW）函数，因此该公式将算法的表现量化为其在各个回合中所生成的集体福利。已知NSW满足公平公理，因此对纳什遗憾的上界提供了一个有原则的公平保证。我们考虑了时间跨度为$T$回合、臂集合为${X}$、维度为$d$的随机线性多臂老虎机问题。此外，我们还关注与每个臂相关的随机奖励是非负的、$\\nu$-次泊松随机变量的设置。对于这个设置，我们开发了一个算法",
    "tldr": "本研究在随机线性多臂老虎机框架中提出了一个更强化的遗憾上界，称为纳什遗憾，它通过将算法的表现量化为其在各个回合中所生成的集体福利来提供一个基于公平性的保证。",
    "en_tdlr": "This paper presents a strengthened upper bound for regret in the stochastic linear bandits framework, called Nash regret, which provides a fairness guarantee by quantifying the performance of the algorithm as the collective welfare it generates across rounds."
}