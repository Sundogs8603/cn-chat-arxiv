{
    "title": "Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])",
    "abstract": "In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN",
    "link": "http://arxiv.org/abs/2310.03165",
    "context": "Title: Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])\nAbstract: In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN",
    "path": "papers/23/10/2310.03165.json",
    "total_tokens": 1028,
    "translated_title": "利用随机矩阵理论提高深度学习的准确性",
    "translated_abstract": "在本研究中，我们探索了随机矩阵理论在深度神经网络（DNN）训练中的应用，重点是通过层剪枝简化DNN架构和损失曲面。随机矩阵理论最近被用于解决深度学习中的过拟合问题，能够检查DNN的权重层谱。我们使用这些技术来在训练过程中通过奇异值分解（SVD）确定要从DNN的权重层中去除的奇异值的数量，这有助于简化DNN并提高准确性，在MNIST和Fashion MNIST数据集上培训简单的DNN模型得到了证明。我们的方法适用于预训练DNN的任何全连接或卷积层，减少了层的参数并简化了DNN的架构，同时保持甚至增强了模型的准确性。通过根据随机矩阵理论的标准丢弃小的奇异值，测试集的准确性保持一致，从而实现更有效的DNN训练。",
    "tldr": "本研究探索了随机矩阵理论在深度神经网络训练中的应用，通过层剪枝和损失曲面优化，实现了对DNN架构的简化和准确性的增强。通过奇异值分解，并根据随机矩阵理论的标准丢弃小的奇异值，可减少DNN层的参数，简化DNN架构，同时保持或增强模型的准确性。",
    "en_tdlr": "This study explores the applications of random matrix theory in training deep neural networks, focusing on layer pruning and loss landscape optimization to simplify DNN architecture and enhance accuracy. By utilizing singular value decomposition and discarding small singular values based on random matrix theory criteria, the method reduces parameters and simplifies the DNN architecture while maintaining or even improving model accuracy."
}