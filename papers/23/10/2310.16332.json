{
    "title": "Corrupting Neuron Explanations of Deep Visual Features. (arXiv:2310.16332v1 [cs.LG])",
    "abstract": "The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.",
    "link": "http://arxiv.org/abs/2310.16332",
    "context": "Title: Corrupting Neuron Explanations of Deep Visual Features. (arXiv:2310.16332v1 [cs.LG])\nAbstract: The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.",
    "path": "papers/23/10/2310.16332.json",
    "total_tokens": 1021,
    "translated_title": "污染神经元解释深度视觉特征",
    "translated_abstract": "深度神经网络无法解释其黑盒行为的能力，导致近来出现了大量解释性方法。然而，人们越来越担心这些解释性方法缺乏稳健性和可靠性。在这项工作中，我们首次在一个统一的流程下对神经元解释方法进行了鲁棒性分析，并展示了这些解释可以被随机噪声和精心设计的扰动所严重破坏。我们发现，即使添加标准差为0.02的小随机噪声，也可以改变深层中高达28%的神经元所分配的概念。此外，我们设计了一种新颖的污染算法，并证明我们的算法可以通过污染不到10%的探测数据来操纵超过80%的神经元的解释。这引发了在现实生活中对神经元解释方法的信任问题，特别是对于涉及安全和公平重要应用的情况。",
    "tldr": "本文通过对神经元解释方法进行鲁棒性分析，发现这些解释可以被随机噪声和精心设计的扰动严重破坏，即使添加小的随机噪声也可以改变高达28％的神经元的概念分配。此外，作者还设计了一种新的污染算法，通过污染不到10％的探测数据可以操纵超过80％的神经元的解释。这引发了在现实生活中对神经元解释方法的信任问题。"
}