{
    "title": "Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])",
    "abstract": "ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var",
    "link": "http://arxiv.org/abs/2310.19927",
    "context": "Title: Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])\nAbstract: ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var",
    "path": "papers/23/10/2310.19927.json",
    "total_tokens": 959,
    "translated_title": "基于模型的重新参数化策略梯度方法：理论和实践算法",
    "translated_abstract": "在机器人学和计算机图形学中，重新参数化策略梯度方法已被广泛应用于连续控制任务。然而，最近的研究发现，当应用于长期强化学习问题时，基于模型的重新参数化策略梯度方法可能会遇到混乱和非平滑的优化景观，导致梯度方差爆炸，从而导致收敛速度较慢。这与传统观念相反，即重新参数化方法在训练深度生成模型等问题中具有较低的梯度估计方差。为了理解这一现象，我们对基于模型的重新参数化策略梯度方法进行了理论研究，并寻找解决优化困难的方法。具体而言，我们分析了基于模型的重新参数化策略梯度方法的收敛性，并指出函数逼近器的平滑性是影响梯度估计质量的主要因素。根据我们的分析，我们提出了一种谱归一化方法来缓解爆炸方差问题。",
    "tldr": "研究者通过对基于模型的重新参数化策略梯度方法进行理论研究，发现在长期强化学习问题中可能会遇到优化困难，导致收敛速度较慢。他们提出了一种谱归一化方法来缓解梯度方差爆炸问题。",
    "en_tdlr": "Researchers conducted a theoretical examination of model-based reparameterization policy gradient methods and found that they may have optimization difficulties and slow convergence in long-term reinforcement learning problems. They proposed a spectral normalization method to mitigate the exploding gradient variance."
}