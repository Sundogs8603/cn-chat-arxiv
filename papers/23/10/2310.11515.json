{
    "title": "Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs. (arXiv:2310.11515v1 [cs.LG])",
    "abstract": "We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\\widetilde{O}(d\\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computat",
    "link": "http://arxiv.org/abs/2310.11515",
    "context": "Title: Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs. (arXiv:2310.11515v1 [cs.LG])\nAbstract: We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\\widetilde{O}(d\\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computat",
    "path": "papers/23/10/2310.11515.json",
    "total_tokens": 934,
    "translated_title": "基于价值偏见的最大似然估计在折扣线性MDPs中的模型驱动强化学习",
    "translated_abstract": "我们考虑无限时段的线性马尔可夫决策过程（MDPs），其中动态模型的转移概率可以通过预定义的低维特征映射进行线性参数化。虽然现有的基于回归的方法在理论上已被证明可以达到几乎最优的后悔，但由于在每个时间步骤中需要大量的优化运行，特别是当状态和动作空间较大时，它们在计算上效率低下。为了解决这个问题，我们提出通过基于价值偏见的最大似然估计（VBMLE）来解决线性MDPs问题，这是一种解决最大似然估计中已知闭环识别问题的经典模型驱动探索方法。我们正式证明了（i）VBMLE享有$\\widetilde{O}(d\\sqrt{T})$的后悔，其中$T$是时间段，$d$是模型参数的维度，以及（ii）VBMLE在计算上是...",
    "tldr": "这篇论文提出了一个基于价值偏见的最大似然估计方法，用于解决线性马尔可夫决策过程中的模型驱动强化学习问题。该方法在计算效率和后悔方面都取得了较好的性能。",
    "en_tdlr": "This paper proposes a value-biased maximum likelihood estimation approach for solving model-based reinforcement learning in linear Markov decision processes. The method achieves good performance in terms of computational efficiency and regret."
}