{
    "title": "Stochastic Average Gradient : A Simple Empirical Investigation. (arXiv:2310.12771v1 [cs.LG])",
    "abstract": "Despite the recent growth of theoretical studies and empirical successes of neural networks, gradient backpropagation is still the most widely used algorithm for training such networks. On the one hand, we have deterministic or full gradient (FG) approaches that have a cost proportional to the amount of training data used but have a linear convergence rate, and on the other hand, stochastic gradient (SG) methods that have a cost independent of the size of the dataset, but have a less optimal convergence rate than the determinist approaches. To combine the cost of the stochastic approach with the convergence rate of the deterministic approach, a stochastic average gradient (SAG) has been proposed. SAG is a method for optimizing the sum of a finite number of smooth convex functions. Like SG methods, the SAG method's iteration cost is independent of the number of terms in the sum. In this work, we propose to compare SAG to some standard optimizers used in machine learning. SAG converges f",
    "link": "http://arxiv.org/abs/2310.12771",
    "context": "Title: Stochastic Average Gradient : A Simple Empirical Investigation. (arXiv:2310.12771v1 [cs.LG])\nAbstract: Despite the recent growth of theoretical studies and empirical successes of neural networks, gradient backpropagation is still the most widely used algorithm for training such networks. On the one hand, we have deterministic or full gradient (FG) approaches that have a cost proportional to the amount of training data used but have a linear convergence rate, and on the other hand, stochastic gradient (SG) methods that have a cost independent of the size of the dataset, but have a less optimal convergence rate than the determinist approaches. To combine the cost of the stochastic approach with the convergence rate of the deterministic approach, a stochastic average gradient (SAG) has been proposed. SAG is a method for optimizing the sum of a finite number of smooth convex functions. Like SG methods, the SAG method's iteration cost is independent of the number of terms in the sum. In this work, we propose to compare SAG to some standard optimizers used in machine learning. SAG converges f",
    "path": "papers/23/10/2310.12771.json",
    "total_tokens": 840,
    "translated_title": "随机平均梯度：一项简单的实证研究",
    "translated_abstract": "尽管神经网络的理论研究和实证成功近年来取得了显著进展，但梯度反向传播仍然是训练这些网络的最常用算法。一方面，我们有确定性或全梯度（FG）方法，其成本与所使用的训练数据量成正比，但收敛速度为线性；另一方面，我们有随机梯度（SG）方法，其成本与数据集大小无关，但收敛速度不如确定性方法理想。为了将随机方法的成本与确定性方法的收敛速度结合起来，提出了随机平均梯度（SAG）方法。SAG是一种用于优化有限个平滑凸函数之和的方法。与SG方法一样，SAG方法的迭代成本与求和中的项数无关。在这项工作中，我们提出将SAG与机器学习中一些标准优化器进行比较。",
    "tldr": "本研究通过对比SAG与机器学习中一些标准优化器，旨在将随机方法的成本与确定性方法的收敛速度结合起来。",
    "en_tdlr": "This work aims to combine the cost of stochastic methods with the convergence rate of deterministic methods by comparing SAG with some standard optimizers used in machine learning."
}