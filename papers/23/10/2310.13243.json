{
    "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking. (arXiv:2310.13243v1 [cs.IR])",
    "abstract": "In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.",
    "link": "http://arxiv.org/abs/2310.13243",
    "context": "Title: Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking. (arXiv:2310.13243v1 [cs.IR])\nAbstract: In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.",
    "path": "papers/23/10/2310.13243.json",
    "total_tokens": 979,
    "translated_title": "开源大规模语言模型是用于文档排序的零-shot查询似然模型",
    "translated_abstract": "在信息检索领域中，查询似然模型（QLMs）根据生成查询的概率来对文档进行排序。最近，先进的大规模语言模型（LLMs）已经成为有效的QLMs，展示了有前途的排序能力。本文重点研究了最近LLMs的真实零-shot排序效果，这些模型仅在无结构文本数据上进行了预训练，没有进行监督指导微调。我们的发现揭示了这些LLMs的强大零-shot排序能力，同时强调除非微调数据集中存在问答生成任务，否则额外的指导微调可能会降低效果。此外，我们还介绍了一种新颖的最先进排序系统，该系统将基于LLM的QLMs与混合零-shot检索器集成，展示了在零-shot和少-shot场景中的出色效果。我们将我们的代码库公开在https://github.com/ielab/llm-qlm上。",
    "tldr": "最新的开源大规模语言模型展现出了强大的零-shot查询似然模型的排名能力，研究显示，这些模型在没有监督指导微调的情况下依然能够进行有效的排序，而且我们还提出了一种新的最先进的排序系统，该系统将基于语言模型的查询似然模型与零-shot检索器集成，不仅在零-shot情况下表现出色，而且在少-shot场景中也展现了卓越的效果。",
    "en_tdlr": "Recent open-source large language models have demonstrated strong zero-shot query likelihood ranking capabilities. The study shows that these models can effectively rank documents without supervised fine-tuning, and a new state-of-the-art ranking system combining language model-based query likelihood models with a hybrid zero-shot retriever exhibits exceptional performance in both zero-shot and few-shot scenarios."
}