{
    "title": "Fractal Landscapes in Policy Optimization. (arXiv:2310.15418v1 [cs.LG])",
    "abstract": "Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\\\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can b",
    "link": "http://arxiv.org/abs/2310.15418",
    "context": "Title: Fractal Landscapes in Policy Optimization. (arXiv:2310.15418v1 [cs.LG])\nAbstract: Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\\\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can b",
    "path": "papers/23/10/2310.15418.json",
    "total_tokens": 894,
    "translated_title": "政策优化中的分形景观",
    "translated_abstract": "策略梯度是连续领域深度强化学习的核心。尽管取得了许多成功，但实践中经常观察到利用策略梯度进行RL训练可能因为多种原因而失败，甚至在已知解的标准控制问题中也是如此。我们提出了一个框架来理解策略梯度方法的一个固有限制：对于某些类别的MDPs，策略空间中的优化景观可以非常非平滑或分形，以至于根本不存在需要估计的梯度。我们借鉴混沌理论和非平滑分析的方法，分析了策略优化目标的最大Lyapunov指数和Hölder指数。此外，我们还开发了一种实用方法，可以从样本中估计目标函数的局部平滑性，以便识别训练过程是否遇到分形景观。我们展示了实验来说明一些策略优化失败的情况。",
    "tldr": "该论文研究了政策优化过程中非平滑或分形的优化景观，提出了一种理解策略梯度方法固有限制的框架，并开发了一种实用方法来识别训练过程中是否遇到分形景观。",
    "en_tdlr": "This paper investigates the non-smooth or fractal optimization landscape in policy optimization, proposes a framework to understand the inherent limitation of the policy gradient approach, and develops a practical method to identify when the training process encounters fractal landscapes."
}