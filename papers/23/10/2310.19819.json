{
    "title": "Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])",
    "abstract": "Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability",
    "link": "http://arxiv.org/abs/2310.19819",
    "context": "Title: Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])\nAbstract: Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability",
    "path": "papers/23/10/2310.19819.json",
    "total_tokens": 828,
    "translated_title": "机器学习和知识：为什么鲁棒性很重要",
    "translated_abstract": "信任机器学习算法需要对其输出有信心。信心通常以模型的可靠性来解释，即模型产生高比例正确输出时可靠。然而，模型可靠性不能解决机器学习模型鲁棒性的担忧，例如模型依赖错误特征或性能在不同情境下的变化。我认为，对信任的认识维度可以通过知识的概念来理解，算法的可信度取决于其用户是否能够确认其输出的正确性。知识要求信念基于正确的原因形成，并且对错误具有鲁棒性，因此机器学习算法只有在跨反事实情景中良好运行，并基于正确特征做出决策时，才能提供知识。我认为，这可以解释为什么我们应该关心像可解释性这样的模型属性。",
    "tldr": "机器学习算法的鲁棒性对于信任和知识的形成至关重要，只有在正确特征下跨情景良好运行的算法才能提供可靠的知识。"
}