{
    "title": "SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning",
    "abstract": "This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce SWAP, an Entropic Wasserstein regression (EWR) network pruning formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. The \"swap\" of a commonly used standard linear regression (LR) with the EWR in optimization is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points, yet incurs marginal extra computational cost. The unique strength of SWAP is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy ",
    "link": "https://arxiv.org/abs/2310.04918",
    "context": "Title: SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning\nAbstract: This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce SWAP, an Entropic Wasserstein regression (EWR) network pruning formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. The \"swap\" of a commonly used standard linear regression (LR) with the EWR in optimization is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points, yet incurs marginal extra computational cost. The unique strength of SWAP is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy ",
    "path": "papers/23/10/2310.04918.json",
    "total_tokens": 950,
    "translated_title": "SWAP: 稀疏熵式Wasserstein回归用于鲁棒网络剪枝",
    "translated_abstract": "本研究解决了神经网络剪枝中计算经验Fisher信息矩阵(FIM)时存在不准确梯度的问题。我们引入了SWAP，一种基于最优输运问题的熵式Wasserstein回归(EWR)网络剪枝方法。通过在优化中将常用的标准线性回归(LR)和EWR交换展示，SWAP在采用邻近插值跨数据点时在噪声抑制方面具有显著优势，同时增加了较小的额外计算成本。SWAP的独特优势在于能够在噪声减少和协方差信息保留之间取得平衡。在多个网络上进行的广泛实验表明，SWAP与最先进的网络剪枝算法具有可比较的性能。当网络规模或目标稀疏度较大时，我们的方法优于最先进算法，且在存在噪声的情况下，优势更加明显。",
    "tldr": "本研究提出了一种名为SWAP的网络剪枝方法，采用稀疏熵式Wasserstein回归来解决神经网络剪枝中的梯度不准确问题。SWAP在噪声抑制和协方差信息保留之间取得了平衡，具有较小的计算成本，与最先进的网络剪枝算法具有可比较的性能。"
}