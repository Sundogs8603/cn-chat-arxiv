{
    "title": "Corrected generalized cross-validation for finite ensembles of penalized estimators. (arXiv:2310.01374v1 [math.ST])",
    "abstract": "Generalized cross-validation (GCV) is a widely-used method for estimating the squared out-of-sample prediction risk that employs a scalar degrees of freedom adjustment (in a multiplicative sense) to the squared training error. In this paper, we examine the consistency of GCV for estimating the prediction risk of arbitrary ensembles of penalized least squares estimators. We show that GCV is inconsistent for any finite ensemble of size greater than one. Towards repairing this shortcoming, we identify a correction that involves an additional scalar correction (in an additive sense) based on degrees of freedom adjusted training errors from each ensemble component. The proposed estimator (termed CGCV) maintains the computational advantages of GCV and requires neither sample splitting, model refitting, or out-of-bag risk estimation. The estimator stems from a finer inspection of ensemble risk decomposition and two intermediate risk estimators for the components in this decomposition. We prov",
    "link": "http://arxiv.org/abs/2310.01374",
    "context": "Title: Corrected generalized cross-validation for finite ensembles of penalized estimators. (arXiv:2310.01374v1 [math.ST])\nAbstract: Generalized cross-validation (GCV) is a widely-used method for estimating the squared out-of-sample prediction risk that employs a scalar degrees of freedom adjustment (in a multiplicative sense) to the squared training error. In this paper, we examine the consistency of GCV for estimating the prediction risk of arbitrary ensembles of penalized least squares estimators. We show that GCV is inconsistent for any finite ensemble of size greater than one. Towards repairing this shortcoming, we identify a correction that involves an additional scalar correction (in an additive sense) based on degrees of freedom adjusted training errors from each ensemble component. The proposed estimator (termed CGCV) maintains the computational advantages of GCV and requires neither sample splitting, model refitting, or out-of-bag risk estimation. The estimator stems from a finer inspection of ensemble risk decomposition and two intermediate risk estimators for the components in this decomposition. We prov",
    "path": "papers/23/10/2310.01374.json",
    "total_tokens": 894,
    "translated_title": "有限惩罚估计器集合的修正广义交叉验证",
    "translated_abstract": "广义交叉验证（GCV）是一种广泛使用的方法，用于估计在样本外进行预测的风险平方，并采用标量自由度调整（以乘法增加）来调整训练误差的平方。本文研究了GCV一致估计任意惩罚最小二乘估计器集合预测风险的能力。我们发现，对于任何大于一的有限大小的估计器集合，GCV是不一致的。为了弥补这个缺点，我们提出了一个纠正，它涉及到对每个集合成分的自由度调整训练误差的额外标量修正（以加法增加）。所提出的估计器（称为CGCV）保持了GCV的计算优势，既不需要样本分裂，模型重拟，也不需要包外风险估计。该估计器源自对集合风险分解的细致检查和该分解中各个成分的两种中间风险估计器。",
    "tldr": "本文研究了广义交叉验证（GCV）在有限惩罚估计器集合中估计预测风险的一致性问题，并提出了一种修正方法（CGCV）来解决这个问题。",
    "en_tdlr": "This paper examines the consistency of generalized cross-validation (GCV) for estimating prediction risk in finite ensembles of penalized least squares estimators, and proposes a correction method (CGCV) to address this issue."
}