{
    "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context. (arXiv:2310.16535v1 [cs.CL])",
    "abstract": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perfo",
    "link": "http://arxiv.org/abs/2310.16535",
    "context": "Title: R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context. (arXiv:2310.16535v1 [cs.CL])\nAbstract: With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perfo",
    "path": "papers/23/10/2310.16535.json",
    "total_tokens": 885,
    "translated_title": "R$^3$ Prompting：无噪声背景下大型语言模型链式思维推理的评审、改写和解决",
    "translated_abstract": "在链式思维（CoT）提示的帮助下，大型语言模型（LLMs）在各种推理任务上取得了显著的表现。然而，大多数研究都在无噪声背景下进行评估，LLMs在噪声背景下产生不准确结果的困境尚未得到充分调查。现有研究利用触发句子鼓励LLMs集中于相关信息，但触发对最终答案预测的影响有限。受交互式CoT方法的启发，该方法通过用户和LLMs之间多轮互动促进中间推理步骤，我们提出了一种新的提示方法，即R$^3$提示，用于在噪声背景下进行CoT思维推理。具体而言，R$^3$提示与LLMs进行关键句提取、变量声明和答案预测的交互，对应于复审、改写和解决的思考过程。最后一次互动中生成的响应将执行",
    "tldr": "R$^3$提示是一种用于在噪声背景下进行CoT推理的新方法，通过复审、改写和解决的思考过程，与大型语言模型交互以进行关键句提取、变量声明和答案预测。"
}