{
    "title": "Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems. (arXiv:2310.20109v1 [cs.IR])",
    "abstract": "Conversational Recommender Systems (CRS) actively elicit user preferences to generate adaptive recommendations. Mainstream reinforcement learning-based CRS solutions heavily rely on handcrafted reward functions, which may not be aligned with user intent in CRS tasks. Therefore, the design of task-specific rewards is critical to facilitate CRS policy learning, which remains largely under-explored in the literature. In this work, we propose a novel approach to address this challenge by learning intrinsic rewards from interactions with users. Specifically, we formulate intrinsic reward learning as a multi-objective bi-level optimization problem. The inner level optimizes the CRS policy augmented by the learned intrinsic rewards, while the outer level drives the intrinsic rewards to optimize two CRS-specific objectives: maximizing the success rate and minimizing the number of turns to reach a successful recommendation in conversations. To evaluate the effectiveness of our approach, we cond",
    "link": "http://arxiv.org/abs/2310.20109",
    "context": "Title: Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems. (arXiv:2310.20109v1 [cs.IR])\nAbstract: Conversational Recommender Systems (CRS) actively elicit user preferences to generate adaptive recommendations. Mainstream reinforcement learning-based CRS solutions heavily rely on handcrafted reward functions, which may not be aligned with user intent in CRS tasks. Therefore, the design of task-specific rewards is critical to facilitate CRS policy learning, which remains largely under-explored in the literature. In this work, we propose a novel approach to address this challenge by learning intrinsic rewards from interactions with users. Specifically, we formulate intrinsic reward learning as a multi-objective bi-level optimization problem. The inner level optimizes the CRS policy augmented by the learned intrinsic rewards, while the outer level drives the intrinsic rewards to optimize two CRS-specific objectives: maximizing the success rate and minimizing the number of turns to reach a successful recommendation in conversations. To evaluate the effectiveness of our approach, we cond",
    "path": "papers/23/10/2310.20109.json",
    "total_tokens": 938,
    "translated_title": "多目标内在奖励学习用于对话推荐系统",
    "translated_abstract": "对话推荐系统（CRS）积极获取用户偏好以生成适应性推荐。主流的基于强化学习的CRS解决方案严重依赖手工制作的奖励函数，这可能与CRS任务中的用户意图不一致。因此，设计任务特定的奖励对于促进CRS策略学习至关重要，但在文献中尚未得到广泛探讨。在本文中，我们提出了一种新的方法来解决这个挑战，即通过与用户的互动学习内在奖励。具体而言，我们将内在奖励学习形式化为一个多目标双层优化问题。内层优化CRS策略，通过学习到的内在奖励进行增强，而外层驱动内在奖励优化两个CRS特定目标：最大化成功率和最小化对话中达到成功推荐所需的轮次数。为了评估我们方法的有效性，我们进行了实验。",
    "tldr": "本论文提出了一种多目标内在奖励学习的方法，用于处理对话推荐系统中手工制作奖励函数无法满足用户意图的问题。通过学习与用户的交互来获得内在奖励，进而优化CRS策略，同时最大化成功率并最小化对话轮次，以达到更好的推荐效果。"
}