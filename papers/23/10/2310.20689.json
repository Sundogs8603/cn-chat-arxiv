{
    "title": "Learning From Mistakes Makes LLM Better Reasoner",
    "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a \"corrector\" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \\textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu",
    "link": "https://arxiv.org/abs/2310.20689",
    "context": "Title: Learning From Mistakes Makes LLM Better Reasoner\nAbstract: Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a \"corrector\" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \\textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu",
    "path": "papers/23/10/2310.20689.json",
    "total_tokens": 931,
    "translated_title": "学习从错误中使LLM成为更好的推理者",
    "translated_abstract": "最近，大型语言模型（LLM）在解决数学问题方面展示出了卓越的推理能力。为了进一步提高它们的推理能力，本研究探讨了LLM是否可以学习从错误中获益（LEMA），类似于人类的学习过程。考虑一个未能解决数学问题的人类学生，他会从自己犯的错误中学习，并纠正它。模仿这种错误驱动的学习过程，LEMA在LLM的微调过程中引入了错误纠正的数据对。具体而言，我们首先收集来自各种LLM的错误推理路径，然后使用GPT-4作为“纠正者”来识别错误步骤，解释错误原因，纠正错误并生成最终答案。此外，我们还应用了一种基于纠正的进化策略，有效地扩展了生成纠正数据的问题集。在各种LLM和推理任务上的实验表明，LEMA始终可以提升仅使用CoT的微调。我们...",
    "tldr": "本研究探索了大型语言模型（LLMs）是否可以从错误中学习，类似于人类学习的过程，并通过引入错误纠正的数据对来改进LLMs的推理能力。实验结果表明，这种方法能够持续提升仅使用CoT进行微调后的性能。"
}