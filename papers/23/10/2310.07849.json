{
    "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])",
    "abstract": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo",
    "link": "http://arxiv.org/abs/2310.07849",
    "context": "Title: Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])\nAbstract: The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo",
    "path": "papers/23/10/2310.07849.json",
    "total_tokens": 898,
    "translated_title": "使用大型语言模型生成合成数据用于文本分类：潜力和限制",
    "translated_abstract": "收集和整理高质量的训练数据对于开发具有卓越性能的文本分类模型至关重要，但往往伴随着巨大的成本和时间投入。研究人员最近开始探索使用大型语言模型（LLMs）生成合成数据集作为一种替代方法。然而，LLM生成的合成数据在支持模型训练方面的有效性在不同的分类任务中是不一致的。为了更好地了解调节LLM生成的合成数据有效性的因素，本研究探讨了在分类的主观性如何影响在合成数据上训练的模型的性能。我们的研究结果表明，主观性在任务层面和实例层面上都与在合成数据上训练的模型的性能呈负相关。最后，我们讨论了我们的工作对于利用LLM来生成合成数据在潜力和限制方面的影响。",
    "tldr": "本研究旨在探讨使用大型语言模型生成合成数据在文本分类模型训练中的潜力和限制。研究结果发现，主观性会负面影响模型在合成数据上的性能。这对于理解和利用合成数据的有效性具有重要的启示作用。",
    "en_tdlr": "This study explores the potential and limitations of using large language models (LLMs) to generate synthetic data for text classification. The results suggest that subjectivity has a negative impact on the performance of models trained on synthetic data, highlighting important considerations for understanding and utilizing the effectiveness of synthetic data."
}