{
    "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])",
    "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we d",
    "link": "http://arxiv.org/abs/2310.12773",
    "context": "Title: Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])\nAbstract: With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we d",
    "path": "papers/23/10/2310.12773.json",
    "total_tokens": 954,
    "translated_title": "安全RLHF：从人类反馈中进行安全强化学习",
    "translated_abstract": "随着大型语言模型（LLM）的发展，平衡AI系统的性能和安全性变得更加关键。然而，LLM训练过程中的有益和无害目标之间的固有张力在很大程度上增加了挑战。为了解决这个问题，我们提出了安全RLHF：一种用于人类价值对齐的新颖算法。安全RLHF明确解耦了关于有益性和无害性的人类偏好，有效避免了众包工作者对张力的困惑，并允许我们训练分别的奖励和成本模型。我们将LLM的安全问题形式化为一个优化任务，即在满足指定成本约束的同时最大化奖励函数。通过利用Lagrangian方法解决这个约束问题，安全RLHF在精调过程中动态调整两个目标之间的平衡。通过三轮使用安全RLHF进行精调，我们得到了一个安全且具有优良性能的AI系统。",
    "tldr": "我们提出了一种名为安全RLHF的算法，用于在大型语言模型的训练过程中平衡性能和安全性。它通过解耦人类偏好，并训练分别的奖励和成本模型，成功解决了有益和无害目标之间的固有张力，并通过动态调整平衡来优化算法性能。",
    "en_tdlr": "We propose a novel algorithm called Safe RLHF to balance performance and safety in the training process of large language models. It effectively addresses the inherent tension between helpfulness and harmlessness by decoupling human preferences and training separate reward and cost models. The algorithm dynamically adjusts the balance to optimize performance, resulting in a safe and high-performing AI system."
}