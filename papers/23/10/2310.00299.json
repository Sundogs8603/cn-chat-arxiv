{
    "title": "RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)",
    "abstract": "Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that ",
    "link": "http://arxiv.org/abs/2310.00299",
    "context": "Title: RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)\nAbstract: Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that ",
    "path": "papers/23/10/2310.00299.json",
    "total_tokens": 920,
    "translated_title": "RelBERT: 使用语言模型嵌入关系",
    "translated_abstract": "许多应用程序需要访问有关不同概念和实体之间关系的背景知识。尽管知识图谱和大型语言模型在某种程度上可以满足这种需求，但知识图谱不可避免地是不完整的，其关系模式通常过于粗粒度，而大型语言模型则效率低下且难以控制。作为替代方案，我们提出了从相对较小的语言模型中提取关系嵌入的方法。具体而言，我们展示了可以直接微调遮盖语言模型（如RoBERTa）来实现这一目的，仅使用了少量的训练数据。所得到的模型被命名为RelBERT，以意外精细的方式捕捉了关系相似性，使我们在类比基准上取得了最新的最佳表现。关键是，RelBERT能够对超出训练数据范围的关系进行建模。例如，我们使用一种模型在命名实体之间的关系领域取得了强大的结果。",
    "tldr": "RelBERT是一种能够从相对较小的语言模型中提取关系嵌入的方法，通过微调RoBERTa模型，只需少量训练数据，即可捕捉关系相似性，并在类比基准中取得最新的最佳表现。",
    "en_tdlr": "RelBERT is a method that extracts relation embeddings from relatively small language models. By fine-tuning the RoBERTa model with a small amount of training data, it captures relational similarity in a fine-grained way, achieving state-of-the-art performance in analogy benchmarks."
}