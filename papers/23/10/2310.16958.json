{
    "title": "Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])",
    "abstract": "Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.",
    "link": "http://arxiv.org/abs/2310.16958",
    "context": "Title: Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])\nAbstract: Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.",
    "path": "papers/23/10/2310.16958.json",
    "total_tokens": 780,
    "translated_title": "转移分子基础模型用于聚合物性能预测",
    "translated_abstract": "基于Transformer的大型语言模型在药物开发和材料发现等领域的设计优化方面具有显著潜力。自我监督的Transformer模型预训练需要大规模的数据集，然而在聚合物科学等专业领域，这些数据集往往非常稀疏。目前聚合物的最新方法通过数据增强生成额外的样本，但不可避免地增加了计算成本。相比之下，小分子的大规模开源数据集可通过迁移学习解决数据稀缺性问题。本研究表明，使用在小分子上预训练并在聚合物性质上微调的Transformer模型，在一系列基准预测任务上达到与在增强聚合物数据集上训练的模型相当的准确性。",
    "tldr": "本研究表明，使用在小分子上预训练并在聚合物性质上微调的Transformer模型，在聚合物性能预测任务中达到与在增强数据集上训练的模型相当的准确性。"
}