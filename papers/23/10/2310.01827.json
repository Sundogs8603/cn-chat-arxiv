{
    "title": "Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency. (arXiv:2310.01827v1 [cs.RO])",
    "abstract": "Hindsight Experience Replay (HER) is a technique used in reinforcement learning (RL) that has proven to be very efficient for training off-policy RL-based agents to solve goal-based robotic manipulation tasks using sparse rewards. Even though HER improves the sample efficiency of RL-based agents by learning from mistakes made in past experiences, it does not provide any guidance while exploring the environment. This leads to very large training times due to the volume of experience required to train an agent using this replay strategy. In this paper, we propose a method that uses primitive behaviours that have been previously learned to solve simple tasks in order to guide the agent toward more rewarding actions during exploration while learning other more complex tasks. This guidance, however, is not executed by a manually designed curriculum, but rather using a critic network to decide at each timestep whether or not to use the actions proposed by the previously-learned primitive pol",
    "link": "http://arxiv.org/abs/2310.01827",
    "context": "Title: Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency. (arXiv:2310.01827v1 [cs.RO])\nAbstract: Hindsight Experience Replay (HER) is a technique used in reinforcement learning (RL) that has proven to be very efficient for training off-policy RL-based agents to solve goal-based robotic manipulation tasks using sparse rewards. Even though HER improves the sample efficiency of RL-based agents by learning from mistakes made in past experiences, it does not provide any guidance while exploring the environment. This leads to very large training times due to the volume of experience required to train an agent using this replay strategy. In this paper, we propose a method that uses primitive behaviours that have been previously learned to solve simple tasks in order to guide the agent toward more rewarding actions during exploration while learning other more complex tasks. This guidance, however, is not executed by a manually designed curriculum, but rather using a critic network to decide at each timestep whether or not to use the actions proposed by the previously-learned primitive pol",
    "path": "papers/23/10/2310.01827.json",
    "total_tokens": 832,
    "translated_title": "学习和重复使用原始行为以提高回顾式经验重现样本效率",
    "translated_abstract": "回顾式经验重现（HER）是一种在强化学习中使用的技术，已被证明对训练基于离线策略的强化学习代理以解决基于目标的机器人操纵任务具有非常高效的效果，但尽管HER通过学习以往经验中的错误改进了强化学习代理的样本效率，它在探索环境时并不提供任何指导，这导致训练时间非常长。本文提出了一种方法，利用之前学习的解决简单任务的原始行为，来引导代理在探索过程中朝着更有回报的动作方向学习其他更复杂的任务，这种引导不是通过手动设计的课程来执行，而是使用评论家网络在每个时间步骤上决定是否使用以前学习的原始策略提供的动作。",
    "tldr": "本文提出了一种方法，利用之前学习的原始行为来引导代理在探索过程中学习复杂任务，从而提高了回顾式经验重现的样本效率。",
    "en_tdlr": "This paper proposes a method that uses previously learned primitive behaviors to guide the agent in exploring and learning complex tasks, thus improving the sample efficiency of Hindsight Experience Replay."
}