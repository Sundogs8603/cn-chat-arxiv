{
    "title": "Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions. (arXiv:2310.16277v1 [cs.LG])",
    "abstract": "Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distribution",
    "link": "http://arxiv.org/abs/2310.16277",
    "context": "Title: Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions. (arXiv:2310.16277v1 [cs.LG])\nAbstract: Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distribution",
    "path": "papers/23/10/2310.16277.json",
    "total_tokens": 833,
    "translated_title": "Bayesian领域不变学习通过参数分布的后验泛化",
    "translated_abstract": "领域不变学习旨在学习能够提取各种训练领域中不变特征的模型，从而更好地泛化到未见过的目标领域。最近，贝叶斯神经网络在领域不变学习方面取得了良好的结果，但大多数研究集中在对齐特征分布而不是参数分布。受到贝叶斯神经网络原理的启发，我们试图直接学习网络参数的领域不变后验分布。首先，我们提出了一个定理，表明可以通过聚合不同训练领域上的后验来隐式推断参数的不变后验。我们的假设更具宽松性，可以提取更多的领域不变信息。我们还提出了一种名为\"PosTerior Generalization (PTG)\"的简单而有效的方法，用于估计不变的参数分布。PTG充分利用了变分推断来近似参数分布。",
    "tldr": "本研究通过学习网络参数的领域不变后验分布，提出了一种名为PosTerior Generalization的方法，能够更好地泛化到未见过的目标领域。",
    "en_tdlr": "This study introduces a method called PosTerior Generalization that learns the domain invariant posterior distribution of network parameters, allowing for better generalization to unseen target domains."
}