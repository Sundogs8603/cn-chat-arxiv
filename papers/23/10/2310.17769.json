{
    "title": "Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])",
    "abstract": "We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr",
    "link": "http://arxiv.org/abs/2310.17769",
    "context": "Title: Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])\nAbstract: We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr",
    "path": "papers/23/10/2310.17769.json",
    "total_tokens": 990,
    "translated_title": "社会契约AI：将AI助手与隐含的群体规范对齐",
    "translated_abstract": "我们探索了通过反转模拟用户（未知）偏好的模型来对齐AI助手的思路。为了验证我们的提议，我们在经济报价游戏中进行了概念验证模拟，将用户偏好形式化为指导模拟玩家行为的策略。我们发现，AI助手能够准确地将其行为与经济文献中的标准策略（如自私的、利他的）相匹配。然而，助手学到的策略在面对未包含在训练分布中的货币（如药品克数）时缺乏鲁棒性和有限的泛化能力。此外，我们发现，当语言使用与未知策略之间存在一致性不足时（如利他策略与粗鲁语言相结合），助手学习到的策略会减慢。总体而言，我们初步的结果表明，开发模拟框架来对齐AI助手的行为是可行的。",
    "tldr": "这项研究探索了通过将机器学习模型应用于用户交互数据，使AI助手能够自动对齐用户偏好的方法。研究发现，虽然AI助手在模拟中能够准确对齐经济文献中的标准策略，但在面对未知货币以及语言与策略一致性不足的情况下，其学习能力受到限制。",
    "en_tdlr": "This study explores a method of aligning AI assistants with user preferences by applying machine learning models to user interaction data. The research finds that while the AI assistant can accurately align with standard policies from economic literature in simulations, it has limited learning ability when faced with unknown currencies and inconsistency between language use and policies."
}