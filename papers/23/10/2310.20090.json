{
    "title": "Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows. (arXiv:2310.20090v1 [stat.ML])",
    "abstract": "Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator fo",
    "link": "http://arxiv.org/abs/2310.20090",
    "context": "Title: Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows. (arXiv:2310.20090v1 [stat.ML])\nAbstract: Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator fo",
    "path": "papers/23/10/2310.20090.json",
    "total_tokens": 938,
    "translated_title": "消除变分推断与Wasserstein梯度流之间的鸿沟",
    "translated_abstract": "变分推断是一种通过在变分族参数空间内进行优化来近似目标分布的技术。而Wasserstein梯度流描述了在概率测度的空间内进行优化，其中不一定存在参数密度函数。在本文中，我们消除了这两种方法之间的差距。我们证明，在一定条件下，布雷斯-瓦瑟斯坦梯度流可以重新表示为欧氏梯度流，其前向欧拉方案是标准的黑盒变分推断算法。具体而言，梯度流的向量场通过路径导数梯度估计器生成。我们还提供了一个关于路径导数梯度的替代视角，将其框架化为对Wasserstein梯度流的蒸馏过程。蒸馏可以扩展到包含f-散度和非高斯变分族。这种扩展产生了一个新的梯度估计器",
    "tldr": "本文研究了消除变分推断与Wasserstein梯度流之间的差距，证明了布雷斯-瓦瑟斯坦梯度流可以重新表示为欧氏梯度流，提出了路径导数梯度估计器和蒸馏过程来拓展该方法，同时可以适用于f-散度和非高斯变分族。",
    "en_tdlr": "This paper bridges the gap between variational inference and Wasserstein gradient flows by demonstrating that the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow. The paper introduces the path-derivative gradient estimator and a distillation procedure to extend the method to f-divergences and non-Gaussian variational families."
}