{
    "title": "On kernel-based statistical learning in the mean field limit. (arXiv:2310.18074v1 [cs.LG])",
    "abstract": "In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds",
    "link": "http://arxiv.org/abs/2310.18074",
    "context": "Title: On kernel-based statistical learning in the mean field limit. (arXiv:2310.18074v1 [cs.LG])\nAbstract: In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds",
    "path": "papers/23/10/2310.18074.json",
    "total_tokens": 864,
    "translated_title": "关于均场极限中基于核的统计学习",
    "translated_abstract": "在许多机器学习应用中，考虑了大量的变量。受交互粒子系统的机器学习启发，我们考虑了输入变量数量趋于无穷大的情况。首先，我们进一步研究了核及其再生核希尔伯特空间的均场极限，完善了现有理论。接下来，我们提供了与均场极限下这些核的逼近相关的结果，包括一个表现定理。最后，我们将这些核应用于在均场极限中的统计学习，重点关注支持向量机。特别地，我们展示了经验和无穷样本解的均场收敛以及相应风险的收敛。一方面，我们的结果在核方法的背景下建立了严格的均场极限，为大规模问题提供了新的理论工具和见解。另一方面，我们的设置对应于...",
    "tldr": "这篇论文研究了在大规模问题中的均场极限下的核统计学习，包括核的均场极限的理论完善、逼近以及支持向量机等的应用。研究结果为大规模问题提供了新的理论工具和见解。",
    "en_tdlr": "This paper investigates kernel-based statistical learning in the mean field limit for large-scale problems, including theoretical completeness, approximation, and applications such as Support Vector Machines. The results provide new theoretical tools and insights for large-scale problems."
}