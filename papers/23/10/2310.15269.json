{
    "title": "GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])",
    "abstract": "Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase",
    "link": "http://arxiv.org/abs/2310.15269",
    "context": "Title: GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])\nAbstract: Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase",
    "path": "papers/23/10/2310.15269.json",
    "total_tokens": 975,
    "translated_title": "GradSim: 基于梯度相似性的语言分组方法用于有效的多语言训练",
    "translated_abstract": "世界上大多数语言都对自然语言处理模型提出了低资源的挑战。通过多语言训练，可以在语言之间共享知识。然而，并不是所有语言都能互相积极地影响，如何选择最合适的语言集合进行多语言训练，并避免那些特征或数据分布不兼容的语言之间的负面干扰，这是一个开放的研究问题。在本文中，我们提出了一种基于梯度相似性的语言分组方法，称为GradSim。我们在三个不同的多语言基准数据集上的实验证明，相较于其他相似性度量，GradSim可以带来最大的性能提升，并与跨语言模型的性能更好地相关。结果是，我们在 AfriSenti 上建立了新的最先进模型，这是一个用于对低资源非洲语言进行情感分析的基准数据集。在我们的广泛分析中，我们进一步揭示了除语言特征外，数据集的主题也起着重要的作用。",
    "tldr": "在本文中，我们提出了一种基于梯度相似性的语言分组方法，名为GradSim。它通过选择最合适的语言集合进行多语言训练，避免了负面干扰，并在多个基准数据集上取得了最先进的性能表现。此外，我们的分析还揭示了数据集的主题对模型的性能也起着重要作用。",
    "en_tdlr": "In this paper, we propose GradSim, a language grouping method based on gradient similarity, which selects the most suitable set of languages for multilingual training, avoids negative interference, and achieves state-of-the-art performance on multiple benchmark datasets. Additionally, our analysis reveals the importance of dataset topics for model performance."
}