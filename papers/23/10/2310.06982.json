{
    "title": "Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality. (arXiv:2310.06982v1 [cs.CV])",
    "abstract": "Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data. In this work, we are the first to argue that using just one synthetic subset for distillation will not yield optimal generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time",
    "link": "http://arxiv.org/abs/2310.06982",
    "context": "Title: Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality. (arXiv:2310.06982v1 [cs.CV])\nAbstract: Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data. In this work, we are the first to argue that using just one synthetic subset for distillation will not yield optimal generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time",
    "path": "papers/23/10/2310.06982.json",
    "total_tokens": 935,
    "translated_title": "数据蒸馏就像伏特加一样：多次蒸馏获得更好的质量",
    "translated_abstract": "数据集蒸馏旨在通过创建一小组具有与完整数据集相似的泛化性能的合成图像，从而最小化训练深度网络所需的时间和内存，在大数据集上进行训练。然而，目前的数据集蒸馏技术存在缺陷，与在原始数据上训练相比，表现出明显的性能差距。在这项工作中，我们首次提出仅使用一个合成子集进行蒸馏将无法获得最佳的泛化性能。这是因为深度网络的训练动态在训练过程中发生了显著变化。因此，需要多个合成子集来捕捉训练的不同阶段的动态特征。为了解决这个问题，我们提出了渐进式数据集蒸馏（Progressive Dataset Distillation，简称PDD）。PDD合成多个小的合成图像集，每个集合都是以前集合为条件，并在这些子集的累计联合上训练模型，无需额外的训练时间。",
    "tldr": "本论文介绍了一种名为渐进式数据集蒸馏（PDD）的方法，通过合成多个小的合成图像集，并在这些子集的累计联合上进行模型训练，以解决目前数据集蒸馏技术的问题，从而获得更好的泛化性能。"
}