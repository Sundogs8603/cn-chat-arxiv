{
    "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the nu",
    "link": "http://arxiv.org/abs/2310.01467",
    "context": "Title: FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])\nAbstract: Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the nu",
    "path": "papers/23/10/2310.01467.json",
    "total_tokens": 878,
    "translated_title": "FedBPT: 高效的适用于大型语言模型的联邦式黑盒提示调优",
    "translated_abstract": "预训练语言模型（PLM）在自然语言处理领域取得了巨大的突破，在各种任务中表现出色。然而，这些模型虽然受益于大量的训练数据，但通常需要在特定数据上进行微调以适应不同的下游任务。然而，这种数据适应过程存在安全和隐私问题，特别是在利用用户生成的设备驻留数据时。联邦学习（FL）提供了一种解决方案，允许在没有集中式数据收集的情况下进行协作模型微调。然而，将FL应用于微调PLMs面临诸多挑战，包括受限的模型参数访问、高计算要求和通信开销。本文介绍了一种名为FedBPT的联邦式黑盒提示调优框架，旨在解决这些挑战。FedBPT无需客户端访问模型参数，通过专注于训练最优提示和利用无梯度优化方法，可以减少计算量。",
    "tldr": "FedBPT是一个高效的联邦式黑盒提示调优框架，解决了在大型语言模型中应用联邦学习进行模型微调时的挑战。",
    "en_tdlr": "FedBPT is an efficient framework for federated black-box prompt tuning, addressing the challenges of applying federated learning to model fine-tuning in large language models."
}