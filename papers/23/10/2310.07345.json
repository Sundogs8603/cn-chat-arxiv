{
    "title": "Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])",
    "abstract": "In this work, we investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers. Both lattice-free and N-best-list approaches are examined. For lattice-free methods with phoneme-level LMs, we propose a method to approximate the context history to employ LMs with full-context dependency. This approximation can be extended to arbitrary context length and enables the usage of word-level LMs in lattice-free methods. Moreover, a systematic comparison is conducted across lattice-free and N-best-list-based methods. Experimental results on Librispeech show that using the word-level LM in training outperforms the phoneme-level LM. Besides, we find that the context size of the LM used for probability computation has a limited effect on performance. Moreover, our results reveal the pivotal importance of the hypothesis space quality in sequence discriminative training.",
    "link": "http://arxiv.org/abs/2310.07345",
    "context": "Title: Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers. (arXiv:2310.07345v1 [cs.CL])\nAbstract: In this work, we investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers. Both lattice-free and N-best-list approaches are examined. For lattice-free methods with phoneme-level LMs, we propose a method to approximate the context history to employ LMs with full-context dependency. This approximation can be extended to arbitrary context length and enables the usage of word-level LMs in lattice-free methods. Moreover, a systematic comparison is conducted across lattice-free and N-best-list-based methods. Experimental results on Librispeech show that using the word-level LM in training outperforms the phoneme-level LM. Besides, we find that the context size of the LM used for probability computation has a limited effect on performance. Moreover, our results reveal the pivotal importance of the hypothesis space quality in sequence discriminative training.",
    "path": "papers/23/10/2310.07345.json",
    "total_tokens": 1001,
    "translated_title": "调查语言模型在神经变换器序列辨别训练中的效果",
    "translated_abstract": "在这项工作中，我们研究了语言模型（LMs）在基于音素的神经变换器序列辨别训练中的不同上下文长度和标签单元（音素 vs. 单词）对性能的影响。我们同时考察了无网格和N最佳列表方法。对于使用音素级别LMs的无网格方法，我们提出了一种方法来近似上下文历史以利用完全上下文依赖的LMs。这种近似可以扩展到任意上下文长度，并允许在无网格方法中使用单词级别LMs。此外，我们对无网格和N最佳列表方法进行了系统比较。在Librispeech上的实验结果表明，在训练中使用单词级别LM的效果优于音素级别LM。此外，我们发现用于概率计算的LM的上下文大小对性能影响有限。此外，我们的结果揭示了序列辨别训练中假设空间质量的关键重要性。",
    "tldr": "本研究探讨了在基于音素的神经变换器序列辨别训练中使用不同上下文长度和标签单元的语言模型的效果。研究发现，在训练中使用单词级别的语言模型优于音素级别的模型，并且概率计算中的上下文大小对性能影响有限。研究结果还揭示了假设空间质量在序列辨别训练中的重要性。"
}