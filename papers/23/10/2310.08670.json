{
    "title": "Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])",
    "abstract": "Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local ",
    "link": "http://arxiv.org/abs/2310.08670",
    "context": "Title: Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])\nAbstract: Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local ",
    "path": "papers/23/10/2310.08670.json",
    "total_tokens": 1020,
    "translated_title": "每个参数都很重要：确保具有动态异构模型缩减的联邦学习的收敛性",
    "translated_abstract": "在跨设备的联邦学习中，由于底端设备存在资源瓶颈，那些可能提供独特贡献的低端设备被排除在训练大模型之外，这给联邦学习带来了重大挑战。最近的研究工作集中在异构模型的联邦学习上，通过从全局模型中提取缩小尺寸的模型，并将其应用于本地设备。尽管实证成功，但对于该方法的收敛性的一般理论保证仍然是一个未解决的问题。在本文中，我们提出了一个统一的异构联邦学习算法框架，并提供了一种通用的收敛性分析。特别地，我们证明了在某些充分条件下，对于IID和非IID数据，这些算法收敛到标准联邦学习的一个稳定点，适用于一般的平滑成本函数。此外，我们揭示了影响其收敛性的两个关键因素：模型提取噪声和最小覆盖指数，并主张了本地模型选择和全局模型选择的联合设计。",
    "tldr": "这项研究提出了一个统一的异构联邦学习算法框架，并证明了在一定条件下，这些算法对于不同类型的数据都能收敛到标准联邦学习的一个稳定点。此外，研究还揭示了两个关键因素：模型提取噪声和最小覆盖指数，提倡了本地模型选择和全局模型选择的联合设计。"
}