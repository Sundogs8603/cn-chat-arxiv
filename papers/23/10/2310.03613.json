{
    "title": "Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])",
    "abstract": "The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp",
    "link": "http://arxiv.org/abs/2310.03613",
    "context": "Title: Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])\nAbstract: The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp",
    "path": "papers/23/10/2310.03613.json",
    "total_tokens": 1053,
    "translated_title": "在联邦学习中解决一类非凸最小极大优化问题",
    "translated_abstract": "最小极大问题广泛应用于机器学习中的各种场景，包括对抗训练、增强学习中的策略评估以及 AUROC 最大化等。为了解决跨多个客户端的大规模数据挑战，在通信高效的分布式训练中，联邦学习（FL）越来越受欢迎。尽管在集中式（即单机）环境下已经开发了许多最小极大问题的优化算法，但在 FL 下的最小极大问题算法仍然不够深入研究。本文研究了一类联邦非凸最小极大优化问题。我们提出了 FL 算法（FedSGDA+ 和 FedSGDA-M）并降低了最常见最小极大问题的现有复杂度结果。对于非凸凹问题，我们提出了 FedSGDA+ 并将通信复杂度降低到 $O(\\varepsilon^{-6})$。在非凸强凹和非凸 PL 最小极大设置下，我们证明了 FedSGDA-M 具有已知的样本复杂度。",
    "tldr": "本研究针对联邦学习中的一类非凸最小极大优化问题，提出了FL算法（FedSGDA+和FedSGDA-M），并在最常见的最小极大问题中降低了复杂度。针对非凸凹问题，提出的FedSGDA+算法将通信复杂度降低到O(ε^{-6})。在非凸强凹和非凸PL最小极大设置下，证明了FedSGDA-M具有已知的样本复杂度。"
}