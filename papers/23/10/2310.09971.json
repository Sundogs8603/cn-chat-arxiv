{
    "title": "AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents",
    "abstract": "We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can sol",
    "link": "https://arxiv.org/abs/2310.09971",
    "context": "Title: AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents\nAbstract: We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can sol",
    "path": "papers/23/10/2310.09971.json",
    "total_tokens": 912,
    "translated_title": "AMAGO：可扩展的上下文强化学习适应性智能体",
    "translated_abstract": "我们介绍了AMAGO，一个上下文强化学习（RL）智能体，它使用序列模型来解决泛化、长期存储和元学习的挑战。最近的研究表明，离线学习可以使具有循环策略的上下文RL成为可能。然而，这些方法需要大量调整，并通过在智能体的内存容量、规划范围和模型大小上创建关键瓶颈而限制了可扩展性。AMAGO重新审视和重新设计了离线上下文方法，成功地训练了在RL的整个展开过程中并行地使用长序列Transformer。我们的智能体具有可扩展性，适用于各种问题，并通过元-RL和长期存储领域的实验证明了其出色的性能。AMAGO对稀疏奖励和离线数据的关注还使得上下文学习能够扩展到具有具有挑战性的探索目标问题。当与多目标延迟重新标记方案相结合时，AMAGO可以解决",
    "tldr": "AMAGO是一个可扩展的上下文强化学习智能体，使用序列模型解决了泛化、长期存储和元学习等挑战，并通过离线学习成功训练了长序列Transformer，具有强大的性能和适用性。",
    "en_tdlr": "AMAGO is a scalable in-context reinforcement learning agent that addresses challenges such as generalization, long-term memory, and meta-learning using sequence models. It successfully trains long-sequence Transformers through offline learning, demonstrating strong performance and applicability."
}