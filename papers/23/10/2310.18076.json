{
    "title": "Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])",
    "abstract": "Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at ht",
    "link": "http://arxiv.org/abs/2310.18076",
    "context": "Title: Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])\nAbstract: Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at ht",
    "path": "papers/23/10/2310.18076.json",
    "total_tokens": 929,
    "translated_title": "问答系统中知识语料错误的问题",
    "translated_abstract": "最近的一些开放领域问答研究探索了使用大型语言模型（LLMs）生成上下文段落，取代问答流程中传统的检索步骤。然而，目前尚不清楚为什么生成的段落比检索到的段落更有效。本研究重新审视了问答问题的传统公式，并引入了知识语料错误的概念。当用于检索的知识语料仅是整个字符串空间的一个子集时，可能会出现这种错误，有可能排除了存在于语料之外的更有帮助的段落。LLMs可以通过在一个更大的空间中生成段落来缓解这个缺点。我们进行了一个使用LLMs来改写人工标注的黄金上下文的实验，以经验性地观察知识语料错误。我们在三个问答基准上的结果显示，在使用改写的段落时性能提升了10%-13%，表明了知识语料错误的存在信号。我们的代码可在ht处获得。",
    "tldr": "本研究探讨了开放领域问答中生成上下文段落与传统检索步骤相比的优势，并引入了知识语料错误的概念。通过使用大型语言模型生成更大范围内的段落，我们观察到问答性能的提升，表明存在知识语料错误的情况。",
    "en_tdlr": "This study investigates the advantages of generating context passages in open-domain question answering compared to traditional retrieval steps and introduces the concept of knowledge corpus error. By using large language models to generate passages in a larger range, we observe an improvement in QA performance, indicating the presence of knowledge corpus error."
}