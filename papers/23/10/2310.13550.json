{
    "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes. (arXiv:2310.13550v1 [cs.LG])",
    "abstract": "In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, i",
    "link": "http://arxiv.org/abs/2310.13550",
    "context": "Title: Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes. (arXiv:2310.13550v1 [cs.LG])\nAbstract: In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, i",
    "path": "papers/23/10/2310.13550.json",
    "total_tokens": 1034,
    "translated_title": "多任务强化学习在非马尔可夫决策过程中的可证明受益",
    "translated_abstract": "在马尔可夫决策过程（MDPs）下的多任务强化学习（RL）中，多个MDPs之间存在共享的潜在结构已被证明相对于单任务RL能够显著提高采样效率。本文研究这种受益是否能够扩展到更一般的顺序决策问题，如部分可观察MDPs（POMDPs）和更一般的预测状态表示（PSRs）。主要挑战在于大型复杂模型空间使得很难确定多任务PSRs的共同潜在结构类型能够减少模型复杂性并提高采样效率。为了达到这个目标，我们假设了一个用于任务的联合模型类，并使用$\\eta$-bracketing number来量化其复杂性；这个数也作为一个通用指标来捕捉任务的相似性，从而决定了多任务相较于单任务RL的受益。我们首先研究了上游多任务学习在PSRs上的应用。",
    "tldr": "本文通过研究在非马尔可夫决策过程下的多任务强化学习，证明了多个MDPs之间共享的潜在结构能够显著提高采样效率。对于部分可观察MDPs和预测状态表示，我们提出了一个联合模型类，并使用$\\eta$-bracketing number来量化其复杂性和任务的相似性，从而决定了多任务相较于单任务RL的受益。"
}