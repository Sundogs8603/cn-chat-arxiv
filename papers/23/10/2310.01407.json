{
    "title": "CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation",
    "abstract": "arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio",
    "link": "https://arxiv.org/abs/2310.01407",
    "context": "Title: CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation\nAbstract: arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio",
    "path": "papers/23/10/2310.01407.json",
    "total_tokens": 740,
    "translated_title": "CoDi: 条件扩散蒸馏，用于更高保真度和更快的图像生成",
    "translated_abstract": "大型生成性扩散模型已经革新了文本到图像的生成，并为像图像增强、恢复、编辑和合成等条件生成任务提供了巨大潜力。然而，它们的广泛应用受到高计算成本的限制，这限制了它们在实时应用中的应用。为了解决这一挑战，我们引入了一种名为CoDi的新方法，它使一个预训练的潜在扩散模型能够接受额外的图像条件输入，同时显著减少了需要达到高质量结果所需的采样步骤。",
    "tldr": "CoDi引入了一种新的方法，通过适应预先训练的潜在扩散模型接受额外的图像条件输入，可以显著降低生成高质量结果所需的采样步骤。",
    "en_tdlr": "CoDi introduces a new method that significantly reduces the sampling steps required to achieve high-quality results by adapting a pre-trained latent diffusion model to accept additional image conditioning inputs."
}