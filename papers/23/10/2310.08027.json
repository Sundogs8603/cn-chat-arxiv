{
    "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection. (arXiv:2310.08027v1 [cs.CL])",
    "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistent",
    "link": "http://arxiv.org/abs/2310.08027",
    "context": "Title: Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection. (arXiv:2310.08027v1 [cs.CL])\nAbstract: Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistent",
    "path": "papers/23/10/2310.08027.json",
    "total_tokens": 910,
    "translated_title": "探索用于多模态区分程度检测的大型语言模型",
    "translated_abstract": "区分程度检测是可靠和可信任的机器学习的关键。最近的多模态区分程度检测利用来自内分布类别名称的文本信息进行视觉区分程度检测，但目前忽视了内分布类别的丰富上下文信息。大型语言模型(LLM)对世界知识进行编码，并可以生成每个类别的描述性特征。不加区分地使用这些知识会对区分程度检测造成灾难性损害，这是我们分析所观察到的。本文提出利用世界知识增强区分程度检测性能的方法，通过从LLM中选择性生成来实现。具体而言，我们引入了一种基于一致性的不确定性校准方法，来估计每个生成的置信度得分。我们进一步从每个图像中提取视觉对象，充分利用上述的世界知识。广泛的实验表明，我们的方法在区分程度检测方面是一致的。",
    "tldr": "本论文提出了一种利用大型语言模型以及图像上下文信息来增强多模态区分程度检测性能的方法。通过了解大型语言模型的生成特征和视觉对象，我们可以提高区分程度检测的准确度和可靠性。",
    "en_tdlr": "This paper proposes a method that leverages large language models and visual context information to enhance the performance of multi-modal out-of-distribution detection. By utilizing the generated features from the language model and extracting visual objects, the accuracy and reliability of out-of-distribution detection can be improved."
}