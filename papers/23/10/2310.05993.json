{
    "title": "Measuring reasoning capabilities of ChatGPT. (arXiv:2310.05993v1 [cs.AI])",
    "abstract": "I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\\cite{mccune2005release} and the finite models finder Mace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\\% only. %, while BARD for 5\\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts",
    "link": "http://arxiv.org/abs/2310.05993",
    "context": "Title: Measuring reasoning capabilities of ChatGPT. (arXiv:2310.05993v1 [cs.AI])\nAbstract: I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\\cite{mccune2005release} and the finite models finder Mace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\\% only. %, while BARD for 5\\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts",
    "path": "papers/23/10/2310.05993.json",
    "total_tokens": 739,
    "translated_title": "评估ChatGPT的推理能力",
    "translated_abstract": "本研究旨在量化ChatGPT在推理任务中产生的逻辑错误。实验使用了来自库中的144个谜题，包括算术谜题、逻辑等式、数独等谜题。使用定理证明器Prover9和有限模型查找器Mace4对这些谜题的正确解进行了验证。研究的主要输出是一个由100个逻辑谜题组成的基准数据集。对于这个数据集，ChatGPT仅提供了7%的正确答案和理由。",
    "tldr": "这项研究评估了ChatGPT在推理任务中的表现，发现其在逻辑谜题中仅能提供7%正确答案和理由。",
    "en_tdlr": "This study measures the reasoning capabilities of ChatGPT and finds that it can only provide correct answers and justifications for 7% of logic puzzles."
}