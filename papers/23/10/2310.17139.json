{
    "title": "Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])",
    "abstract": "While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We imple",
    "link": "http://arxiv.org/abs/2310.17139",
    "context": "Title: Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])\nAbstract: While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We imple",
    "path": "papers/23/10/2310.17139.json",
    "total_tokens": 938,
    "translated_title": "理解和解决基于双模拟的离线强化学习中的缺陷",
    "translated_abstract": "虽然基于双模拟的方法在强化学习任务中学习鲁棒的状态表示具有潜力，但其在离线任务中的有效性并不理想。在某些情况下，其性能甚至明显低于替代方法。我们旨在理解为什么双模拟方法在在线环境中成功，但在离线任务中失效。我们的分析揭示了数据集中缺失的转换对双模拟原则的特别有害，导致估计无效。我们还揭示了奖励缩放在限制双模拟测量范围和其引起的值误差方面起着关键作用。基于这些发现，我们建议在离线强化学习设置中应用期望值算子进行表示学习，有助于防止对不完整数据的过拟合。同时，通过引入适当的奖励缩放策略，避免了表示空间中的特征崩溃风险。",
    "tldr": "本文研究了基于双模拟的表示方法在离线强化学习中的缺陷，并发现数据集中缺失的转换以及奖励缩放对其性能有重要影响。基于此，我们提出了在离线环境中应用期望值算子与适当的奖励缩放策略来解决这些问题。",
    "en_tdlr": "This paper addresses the pitfalls of bisimulation-based representations in offline reinforcement learning and highlights the impact of missing transitions and reward scaling. It proposes the use of the expectile operator and an appropriate reward scaling strategy to overcome these issues."
}