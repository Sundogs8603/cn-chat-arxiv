{
    "title": "The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])",
    "abstract": "To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett",
    "link": "http://arxiv.org/abs/2310.19956",
    "context": "Title: The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])\nAbstract: To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett",
    "path": "papers/23/10/2310.19956.json",
    "total_tokens": 916,
    "translated_title": "深度和宽度对Transformer语言模型泛化能力的影响",
    "translated_abstract": "为了处理新的句子，语言模型（LMs）必须以组合的方式进行泛化 - 将熟悉的元素以新的方式结合起来。模型结构的哪些方面促进了组合式泛化？针对transformers，我们测试假设，即当transformers更深（具有更多层次）时，它们更容易进行组合式泛化，这个假设基于最近的理论和实证研究。由于简单地增加层数会增加总参数数量，混淆了深度和大小，我们构建了三类模型，通过以保持总参数数量恒定的方式来权衡深度和宽度（分别为41M、134M和374M个参数）。我们将所有模型预训练为LMS，然后在测试组合式泛化的任务上进行微调。我们得出了三个主要结论：（1）在微调后，深层模型在超出分布范围的情况下比浅层模型更好地进行泛化，但额外层次的相对收益迅速减小；（2）在每个模型组中，深层模型表现出更好的组合式泛化能力...（文本已截断）",
    "tldr": "深层的transformer语言模型在组合式泛化能力上比浅层模型更好，但额外层数的相对收益会迅速减小。",
    "en_tdlr": "Deeper transformer language models generalize better in terms of compositional generalization compared to shallower models, but the relative benefit of additional layers diminishes rapidly."
}