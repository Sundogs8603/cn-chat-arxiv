{
    "title": "Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])",
    "abstract": "Given a set of arms $\\mathcal{Z}\\subset \\mathbb{R}^d$ and an unknown parameter vector $\\theta_\\ast\\in\\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$, with high probability through noisy measurements of $x^{\\top}\\theta_{\\ast}$ with $x\\in \\mathcal{X}\\subset \\mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\\in \\mathcal{Z}$ or b) explicitly maintaining a subset of $\\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput",
    "link": "http://arxiv.org/abs/2310.06069",
    "context": "Title: Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])\nAbstract: Given a set of arms $\\mathcal{Z}\\subset \\mathbb{R}^d$ and an unknown parameter vector $\\theta_\\ast\\in\\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$, with high probability through noisy measurements of $x^{\\top}\\theta_{\\ast}$ with $x\\in \\mathcal{X}\\subset \\mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\\in \\mathcal{Z}$ or b) explicitly maintaining a subset of $\\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput",
    "path": "papers/23/10/2310.06069.json",
    "total_tokens": 915,
    "translated_title": "最优探索不比汤普森采样更困难",
    "translated_abstract": "在给定一组臂$\\mathcal{Z}\\subset \\mathbb{R}^d$和未知参数向量$\\theta_\\ast\\in\\mathbb{R}^d$的情况下，纯探索线性臂问题旨在通过对$x^{\\top}\\theta_{\\ast}$的噪声测量，返回$\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$，并以高概率找到正确解。现有的（渐近）最优方法要求要么为每个臂$z\\in \\mathcal{Z}$进行潜在昂贵的投影，要么在每个时间点明确地维护一部分正在考虑的$\\mathcal{Z}$。这种复杂性与流行且简单的汤普森采样算法用于最小化后悔的情况完全相反，后者只需要访问后验采样和argmax oracle，并且在任何时间点都不需要枚举$\\mathcal{Z}$。不幸的是，已知汤普森采样对于纯探索是次优的。在这项工作中，我们提出了一个自然的问题：是否存在一种算法能够进行最优探索，而且只需要相同的计算操作？",
    "tldr": "这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？"
}