{
    "title": "Multi-point Feedback of Bandit Convex Optimization with Hard Constraints. (arXiv:2310.10946v1 [cs.LG])",
    "abstract": "This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \\textit{hard} constraint violation as the metric of constraint violation, which is defined by $\\sum_{t=1}^{T} \\max\\{g_t(\\boldsymbol{x}_t), 0\\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \\textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\\max\\{c,1-c\\}})$ regret bounds and $O(d^2T^{1-\\frac{c}{2}})$ cumulative hard constr",
    "link": "http://arxiv.org/abs/2310.10946",
    "context": "Title: Multi-point Feedback of Bandit Convex Optimization with Hard Constraints. (arXiv:2310.10946v1 [cs.LG])\nAbstract: This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \\textit{hard} constraint violation as the metric of constraint violation, which is defined by $\\sum_{t=1}^{T} \\max\\{g_t(\\boldsymbol{x}_t), 0\\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \\textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\\max\\{c,1-c\\}})$ regret bounds and $O(d^2T^{1-\\frac{c}{2}})$ cumulative hard constr",
    "path": "papers/23/10/2310.10946.json",
    "total_tokens": 935,
    "translated_title": "多点反馈的带有硬约束的强盗凸优化",
    "translated_abstract": "本文研究了带有约束的强盗凸优化问题，其中学习者旨在在局部损失函数信息下生成一系列决策，以同时降低累积损失和累积约束违反。我们采用累积“硬”约束违反作为约束违反的度量，其定义为$\\sum_{t=1}^{T}\\max\\{g_t(\\boldsymbol{x}_t), 0\\}$。由于最大操作符的存在，相对于传统的“长期”约束违反度量，严格可行解无法消除违反约束的影响。我们提出了一种基于惩罚的近端梯度下降方法，该方法在遗憾和累积硬约束违反方面都实现了次线性增长，其中梯度估计使用了两个点的函数评估。准确地说，我们的算法实现了$O(d^2T^{\\max\\{c,1-c\\}})$的遗憾界和$O(d^2T^{1-\\frac{c}{2}})$的累积硬约束违反界。",
    "tldr": "本文研究了带有硬约束的强盗凸优化问题，并提出了一种基于惩罚的近端梯度下降方法，实现了次线性的遗憾和累积硬约束违反界。",
    "en_tdlr": "This paper studies bandit convex optimization with hard constraints and proposes a penalty-based proximal gradient descent method that achieves sub-linear regret and cumulative hard constraint violation bounds."
}