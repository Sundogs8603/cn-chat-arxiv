{
    "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
    "abstract": "arXiv:2310.18338v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with ",
    "link": "https://arxiv.org/abs/2310.18338",
    "context": "Title: Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning\nAbstract: arXiv:2310.18338v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with ",
    "path": "papers/23/10/2310.18338.json",
    "total_tokens": 815,
    "translated_title": "细调小型语言模型以协调更大型语言模型提高复杂推理能力",
    "translated_abstract": "大型语言模型（LLMs）在生成思维链（CoT）时展现出令人印象深刻的推理能力。最近对提示分解以解决复杂的多步推理问题的尝试取决于LLM同时分解和解决问题的能力。一个重要的缺点是，基础LLMs通常不可用于微调，使得适应性在计算上是禁锢的。我们认为（并证明）问题的分解和解决方案生成是不同的能力，最好通过单个庞大的LLM而不是一个整体模块来解决。我们引入了DaSLaM，它使用一个分解生成器将复杂问题分解成需要更少推理步骤的子问题。这些子问题由一个求解器来回答。我们使用一个相对小型（13B参数）LM作为分解生成器，我们使用策略梯度优化来与之交互进行训练。",
    "tldr": "细调小型语言模型作为分解生成器，使用策略梯度优化来协调更大型语言模型，进一步提高复杂推理能力",
    "en_tdlr": "Fine-tuning small language models as decomposition generators, using policy gradient optimization to coordinate larger language models, improves complex reasoning capabilities."
}