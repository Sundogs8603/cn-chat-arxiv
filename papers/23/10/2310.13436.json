{
    "title": "Non-linear approximations of DSGE models with neural-networks and hard-constraints. (arXiv:2310.13436v1 [econ.TH])",
    "abstract": "Recently a number of papers have suggested using neural-networks in order to approximate policy functions in DSGE models, while avoiding the curse of dimensionality, which for example arises when solving many HANK models, and while preserving non-linearity. One important step of this method is to represent the constraints of the economic model in question in the outputs of the neural-network. I propose, and demonstrate the advantages of, a novel approach to handling these constraints which involves directly constraining the neural-network outputs, such that the economic constraints are satisfied by construction. This is achieved by a combination of re-scaling operations that are differentiable and therefore compatible with the standard gradient descent approach used when fitting neural-networks. This has a number of attractive properties, and is shown to out-perform the penalty-based approach suggested by the existing literature, which while theoretically sound, can be poorly behaved p",
    "link": "http://arxiv.org/abs/2310.13436",
    "context": "Title: Non-linear approximations of DSGE models with neural-networks and hard-constraints. (arXiv:2310.13436v1 [econ.TH])\nAbstract: Recently a number of papers have suggested using neural-networks in order to approximate policy functions in DSGE models, while avoiding the curse of dimensionality, which for example arises when solving many HANK models, and while preserving non-linearity. One important step of this method is to represent the constraints of the economic model in question in the outputs of the neural-network. I propose, and demonstrate the advantages of, a novel approach to handling these constraints which involves directly constraining the neural-network outputs, such that the economic constraints are satisfied by construction. This is achieved by a combination of re-scaling operations that are differentiable and therefore compatible with the standard gradient descent approach used when fitting neural-networks. This has a number of attractive properties, and is shown to out-perform the penalty-based approach suggested by the existing literature, which while theoretically sound, can be poorly behaved p",
    "path": "papers/23/10/2310.13436.json",
    "total_tokens": 818,
    "translated_title": "用神经网络和硬约束进行DSGE模型的非线性逼近",
    "translated_abstract": "最近一些论文建议使用神经网络来逼近DSGE模型中的政策函数，以避免维度的灾难性增加，并保持非线性。其中一个重要步骤是将经济模型的约束表示为神经网络的输出。我提出并展示了一种处理这些约束的新方法，即通过直接约束神经网络的输出，以使经济约束得以满足。这是通过可微的重新缩放操作实现的，因此与拟合神经网络时使用的标准梯度下降方法兼容。这种方法具有许多有吸引力的特性，并且被证明优于现有文献中提出的基于惩罚的方法，尽管后者在理论上是正确的，但行为可能不稳定。",
    "tldr": "该论文提出了一种新方法来处理DSGE模型中的约束，通过直接约束神经网络的输出以满足经济约束。这种方法通过重新缩放操作实现，具有许多有吸引力的特性，并且优于现有的基于惩罚的方法。",
    "en_tdlr": "This paper proposes a novel approach to handling constraints in DSGE models by directly constraining the outputs of a neural network, which out-performs existing penalty-based methods."
}