{
    "title": "Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])",
    "abstract": "We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose \"foundational\" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.",
    "link": "http://arxiv.org/abs/2310.18191",
    "context": "Title: Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])\nAbstract: We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose \"foundational\" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.",
    "path": "papers/23/10/2310.18191.json",
    "total_tokens": 908,
    "translated_title": "缩放学习优化器是否值得？评估 VeLO 的 4000 个 TPU 月的价值。",
    "translated_abstract": "我们分析了 VeLO（万能学习优化器），这是迄今为止规模最大的训练通用“基础”优化器的尝试。VeLO 使用超过 4000 个 TPU 月的机器学习任务进行训练，目标是产生一个能够推广到新问题并且不需要超参数调整，并且超过 Adam 等行业标准的优化器。我们对 MLCommons 优化器基准套件独立评估了 VeLO。我们发现与初步声明相反：（1）VeLO有一个关键的超参数需要根据具体问题进行调整，（2）VeLO在找到的解的质量上不一定优于竞争对手，（3）VeLO在降低训练误差上并不比竞争优化器更快。这些观察结果对 VeLO 的通用性和培训投资的价值提出了质疑。",
    "tldr": "VeLO是迄今为止规模最大的训练通用“基础”优化器的尝试，但我们的评估发现它需要问题特定的调优，并不一定优于竞争对手的解决方案质量和训练误差降低速度，这对于VeLO的通用性和培训投资的价值提出了质疑。",
    "en_tdlr": "VeLO, the largest scale attempt to train a general purpose optimizer, was found to require problem-specific tuning and did not necessarily outperform competitors in solution quality and training loss reduction speed, calling into question its generality and the value of the investment in training it."
}