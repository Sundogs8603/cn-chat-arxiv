{
    "title": "Language Models Represent Space and Time",
    "abstract": "arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real",
    "link": "https://arxiv.org/abs/2310.02207",
    "context": "Title: Language Models Represent Space and Time\nAbstract: arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real",
    "path": "papers/23/10/2310.02207.json",
    "total_tokens": 846,
    "translated_title": "语言模型代表空间和时间",
    "translated_abstract": "大型语言模型（LLM）的能力引发了关于这些系统到底是仅仅学习了庞大的表面统计信息还是学到了更连贯、基于真实世界的表征的争论。我们通过分析Llama-2系列模型中学到的三个空间数据集（世界、美国、纽约的地点）和三个时间数据集（历史人物、艺术品、新闻头条）的学习表征找到了支持后者的证据。我们发现LLM在多个尺度上学习到了空间和时间的线性表征。这些表征对提示变化具有稳健性，并且在不同实体类型（例如城市和地标）之间是统一的。此外，我们还发现了可靠地编码空间和时间坐标的个体“空间神经元”和“时间神经元”。虽然还需要进一步的研究，但我们的结果表明现代LLM学习到了对真实世界的丰富时空表征。",
    "tldr": "现代大型语言模型学习到了丰富的时空表征，包括学习到了空间和时间的线性表征以及个体的“空间神经元”和“时间神经元”。"
}