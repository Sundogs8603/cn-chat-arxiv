{
    "title": "Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)",
    "abstract": "Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\\Theta(\\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\\Theta(\\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \\textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \\textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s",
    "link": "http://arxiv.org/abs/2310.14085",
    "context": "Title: Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)\nAbstract: Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\\Theta(\\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\\Theta(\\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \\textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \\textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s",
    "path": "papers/23/10/2310.14085.json",
    "total_tokens": 984,
    "translated_title": "在具有梯度反馈的强单调和指数凸博弈中的自适应、双重最优无悔学习",
    "translated_abstract": "在强凸性或单调性假设下，网上梯度下降（OGD）被广泛认为是双重最优的：（1）在单个代理设置中，对于强凸成本函数，它实现了$ \\Theta(\\log T) $的最优后悔；（2）在具有强单调性的多代理博弈的情况下，每个代理使用OGD，我们获得了关于联合行动的最后一次收敛到唯一纳什均衡的最优速率$ \\Theta(\\frac{1}{T}) $。尽管这些有限时间的保证突出了其优点，但OGD的缺点是需要知道强凸性/单调性的参数。在本文中，我们设计了一个完全自适应的OGD算法\\textsf{AdaOGD}，它不需要先验的知识这些参数。在单个代理设置中，我们的算法在强凸性下实现了$ O(\\log^2(T)) $的后悔，这是最优的除了一个对数因子。此外，如果在强单调博弈中每个代理都使用\\textsf{AdaOGD}，则联合行动收敛到最后一个迭代时的一次。",
    "tldr": "本文提出了一个自适应的OGD算法\\textsf{AdaOGD}，在强凸性下实现了$ O(\\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。"
}