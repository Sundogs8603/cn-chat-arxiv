{
    "title": "Goodhart's Law in Reinforcement Learning. (arXiv:2310.09144v1 [cs.LG])",
    "abstract": "Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart's law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that ",
    "link": "http://arxiv.org/abs/2310.09144",
    "context": "Title: Goodhart's Law in Reinforcement Learning. (arXiv:2310.09144v1 [cs.LG])\nAbstract: Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart's law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that ",
    "path": "papers/23/10/2310.09144.json",
    "total_tokens": 1099,
    "translated_title": "强化学习中的古哈特定律",
    "translated_abstract": "在现实世界中，实现完全捕捉复杂任务的奖励函数是不切实际的。因此，把奖励函数视为真实目标的代理而非定义是合理的。我们通过古哈特定律的视角研究了这一现象，该定律预测在某一临界点之后，对不完美代理奖励的过度优化会降低在真实目标上的性能。首先，我们提出一种衡量该效应程度的方法，并实证表明，在广泛的环境和奖励函数范围内，对不完美代理奖励进行优化常常会导致古哈特定律所预测的行为。然后，我们提供了一个几何解释，说明为什么在马尔可夫决策过程中发生古哈特定律。我们利用这些理论洞察为该问题提出了一种可避免陷阱的最优早停止方法，并推导了该方法的理论遗憾界限。此外，我们提出了一种训练方法，可以使用代理奖励进行优化，并且逐步过渡到真实奖励。",
    "tldr": "该论文通过研究古哈特定律在强化学习中的现象，提出了一种衡量效应程度的方法，并实证表明在广泛的环境和奖励函数范围内，对不完美代理奖励的过度优化会导致降低在真实目标上的性能。然后，通过几何解释马尔可夫决策过程中古哈特定律的发生，提出了一种可避免陷阱的最优早停止方法，并推导了方法的理论遗憾界限。此外，还提出了一种逐渐过渡到真实奖励的训练方法。",
    "en_tdlr": "This paper examines the phenomenon of Goodhart's law in reinforcement learning and proposes a method to quantify its impact. It empirically shows that over-optimization of imperfect proxy rewards leads to decreased performance on the true objective. The paper also provides a geometric explanation for the occurrence of Goodhart's law in Markov decision processes and suggests an optimal early stopping method to avoid this pitfall. Additionally, the paper presents a training method that gradually transitions to the true reward."
}