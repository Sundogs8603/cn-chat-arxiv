{
    "title": "AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])",
    "abstract": "Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.",
    "link": "http://arxiv.org/abs/2310.11964",
    "context": "Title: AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])\nAbstract: Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.",
    "path": "papers/23/10/2310.11964.json",
    "total_tokens": 764,
    "translated_title": "使用因果分层注意力和指针的AMR解析",
    "translated_abstract": "基于翻译的AMR解析器由于其简单和有效性而近年来备受关注。它们将线性化图预测为自由文本，避免了显式的结构建模。然而，这种简单性忽视了AMR图中的结构局部性，并引入了不必要的标记来表示共指。本文介绍了AMR解析的新目标形式和一种新的模型，CHAP，它配备了因果分层注意力和指针机制，使结构能够被整合到Transformer解码器中。我们在实验中探索了各种替代建模选项。实验证明，在没有额外数据的情况下，我们的模型在五个基准测试中有四个的性能优于基线模型。",
    "tldr": "本文介绍了一种新的AMR解析模型CHAP，它利用因果分层注意力和指针机制将结构整合到Transformer解码器中，实验证明在没有额外数据的情况下，该模型在四个基准测试中优于基线模型。",
    "en_tdlr": "This paper introduces a new AMR parsing model called CHAP, which integrates structures into the Transformer decoder using causal hierarchical attention and the pointer mechanism. Experiments show that this model outperforms baseline models on four out of five benchmarks in the setting of no additional data."
}