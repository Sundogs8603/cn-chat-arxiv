{
    "title": "Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective. (arXiv:2310.19360v1 [cs.LG])",
    "abstract": "Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the",
    "link": "http://arxiv.org/abs/2310.19360",
    "context": "Title: Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective. (arXiv:2310.19360v1 [cs.LG])\nAbstract: Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the",
    "path": "papers/23/10/2310.19360.json",
    "total_tokens": 939,
    "translated_title": "平衡、失衡和再平衡：从极小极大博弈的角度理解鲁棒过拟合",
    "translated_abstract": "对抗训练（AT）已成为提取鲁棒特征的技术典范，然而研究人员最近发现，AT在学习率下降后存在严重的鲁棒过拟合问题。本文通过将对抗训练视为模型训练者和攻击者之间的动态极小极大博弈，解释了这一现象。具体地，我们分析了学习率下降如何破坏了极小极大博弈的平衡，使得模型训练者获得了更强的记忆能力，而这种失衡导致了非鲁棒特征的过拟合现象。通过大量实验证实了这一理解，并从两个博弈参与者的角度提供了鲁棒过拟合的全面观点。这一理解进一步启发我们通过调整模型训练者的能力或提高攻击强度来缓解鲁棒过拟合问题。实验表明，",
    "tldr": "通过将对抗训练视为极小极大博弈，我们解释了学习率下降后对抗训练存在严重鲁棒过拟合问题的原因，并提出通过调整模型训练者能力或提高攻击强度来缓解这个问题。",
    "en_tdlr": "By viewing adversarial training as a minimax game, we explain the reason for severe robust overfitting in adversarial training after learning rate decay, and propose alleviating this issue by adjusting the trainer's capacity or improving the attack strength."
}