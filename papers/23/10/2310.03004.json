{
    "title": "Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization. (arXiv:2310.03004v1 [cs.LG])",
    "abstract": "Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-",
    "link": "http://arxiv.org/abs/2310.03004",
    "context": "Title: Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization. (arXiv:2310.03004v1 [cs.LG])\nAbstract: Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-",
    "path": "papers/23/10/2310.03004.json",
    "total_tokens": 858,
    "translated_title": "软凸量化：用凸优化重新思考向量量化",
    "translated_abstract": "向量量化（VQ）是深度学习中用于提取信息性离散潜在表示的一种众所周知的技术。VQ嵌入模型在包括图像和语音生成在内的一系列应用中取得了令人印象深刻的结果。VQ作为一种参数化的K-means算法，在前向传递中使用单个码书向量将输入进行量化。尽管功能强大，但该技术面临实际挑战，包括码书崩溃、不可区分性和有损压缩。为了缓解上述问题，我们提出了软凸量化（SCQ）作为VQ的直接替代。SCQ的工作方式类似于可微凸优化（DCO）层：在前向传递中，我们求解量化输入的码书向量的最优凸组合。在反向传递中，我们通过前向解的最优性条件利用可区分性。然后，我们引入了一个可扩展的SCQ优化松弛方法，并展示其在CIFAR数据集上的有效性。",
    "tldr": "本文提出了软凸量化（SCQ）作为向量量化（VQ）的替代方法，通过解决量化输入的码书向量的最优凸组合问题，缓解了VQ面临的实际挑战。"
}