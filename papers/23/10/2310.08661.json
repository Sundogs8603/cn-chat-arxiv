{
    "title": "Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])",
    "abstract": "Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.",
    "link": "http://arxiv.org/abs/2310.08661",
    "context": "Title: Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])\nAbstract: Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.",
    "path": "papers/23/10/2310.08661.json",
    "total_tokens": 745,
    "translated_title": "使用Transformer进行计数和算法推广",
    "translated_abstract": "机器学习中的算法推广是指学习生成数据的底层算法，以一种对超出分布的方式进行泛化的能力。这对大多数机器学习算法来说通常是一个困难的任务。在这里，我们分析了在计数时需要的算法推广，无论是隐式还是显式。我们表明标准的Transformer是基于阻碍这类任务的架构决策。特别是，我们讨论了使用层归一化和通过softmax归一化注意力权重的后果。通过消除这些问题操作，我们证明修改后的Transformer可以在计数方面展示出良好的算法推广性能，同时采用非常轻量级的架构。",
    "tldr": "这项研究分析了在计数任务中的算法推广，并证明了标准的Transformer的架构决策会阻碍其在超出分布任务上的性能。通过消除问题操作，修改后的Transformer在计数方面展示出了良好的算法推广性能。",
    "en_tdlr": "This research analyzes algorithmic generalization in counting tasks and demonstrates that the architectural decisions of standard Transformers hinder their performance on out-of-distribution tasks. By eliminating problematic operations, a modified Transformer shows good algorithmic generalization performance in counting."
}