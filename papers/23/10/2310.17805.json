{
    "title": "Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks. (arXiv:2310.17805v1 [cs.LG])",
    "abstract": "Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relations",
    "link": "http://arxiv.org/abs/2310.17805",
    "context": "Title: Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks. (arXiv:2310.17805v1 [cs.LG])\nAbstract: Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relations",
    "path": "papers/23/10/2310.17805.json",
    "total_tokens": 942,
    "translated_title": "通过DreamerV3技巧提高PPO的奖励规模鲁棒性",
    "translated_abstract": "大多数强化学习方法依赖于密集、规范化的环境奖励。DreamerV3最近引入了一种基于模型的方法，并采用了一些技巧来减轻这些限制，使用一组超参数在广泛的基准测试中实现了最新的状态。这个结果引发了关于这些技巧的普适性的讨论，因为它们似乎适用于其他强化学习算法。我们的工作将DreamerV3的技巧应用到PPO中，这是第一次在原始工作之外进行这样的实证研究。令人惊讶的是，我们发现这些技巧并不能作为一般的改进转移到PPO上。我们使用了高质量的PPO参考实现，并在Arcade Learning Environment和DeepMind Control Suite上进行了长达10,000个A100小时的大量消融研究。虽然我们的实验表明这些技巧并没有普遍超过PPO，但我们确定了它们成功的情况，并对它们的关系提供了深入洞察。",
    "tldr": "本研究将DreamerV3的技巧应用到PPO中，并发现这些技巧并不能普遍改善PPO的性能。通过大量的消融研究，我们确定了一些情况下这些技巧的成功，并对它们的关系提供了深入洞察。"
}