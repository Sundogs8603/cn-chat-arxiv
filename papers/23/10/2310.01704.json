{
    "title": "Transformers are efficient hierarchical chemical graph learners. (arXiv:2310.01704v1 [cs.LG])",
    "abstract": "Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited ov",
    "link": "http://arxiv.org/abs/2310.01704",
    "context": "Title: Transformers are efficient hierarchical chemical graph learners. (arXiv:2310.01704v1 [cs.LG])\nAbstract: Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited ov",
    "path": "papers/23/10/2310.01704.json",
    "total_tokens": 896,
    "translated_title": "Transformers是高效的分层化化学图学习模型",
    "translated_abstract": "Transformers，从自然语言处理中改编而来，正在成为图表示学习的主要方法。当代的图转换器通常将节点或边视为独立的标记。这种方法导致即使对于中等规模的图形也存在计算挑战，因为自我注意复杂度随标记数量的平方级增长。在本文中，我们引入了SubFormer，这是一个通过消息传递机制在子图上操作的图转换器。这种方法减少了标记数量，并增强了学习长程交互。我们在化学结构上的分子属性预测基准中展示了SubFormer，并展示了它在计算成本的一小部分下与最先进的图转换器相竞争，使用消费级显卡进行训练的时间约为几分钟。我们使用化学结构解释了注意权重。我们表明SubFormer表现出有限的关注权重。",
    "tldr": "本论文介绍了SubFormer，这是一个图转换器，通过在子图上进行消息传递机制来降低标记数量并增强学习长程交互。在化学结构的分子属性预测基准上，SubFormer在计算成本的一小部分下与最先进的图转换器相竞争，并且能够以几分钟的时间进行训练。注意权重的解释显示SubFormer展现出有限的关注权重。",
    "en_tdlr": "This paper introduces SubFormer, a graph transformer that operates on subgraphs, reducing the number of tokens and enhancing learning long-range interactions. It competes with state-of-the-art graph transformers at a fraction of the computational cost and can be trained in minutes. The interpretation of attention weights shows limited attention for SubFormer."
}