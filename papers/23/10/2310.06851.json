{
    "title": "BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer. (arXiv:2310.06851v1 [cs.CV])",
    "abstract": "Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system",
    "link": "http://arxiv.org/abs/2310.06851",
    "context": "Title: BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer. (arXiv:2310.06851v1 [cs.CV])\nAbstract: Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system",
    "path": "papers/23/10/2310.06851.json",
    "total_tokens": 924,
    "translated_title": "BodyFormer: 基于Transformer的语义引导的3D人体手势合成",
    "translated_abstract": "自动从语音中合成手势是一个吸引研究人员关注的话题，用于远程通信、视频游戏和元宇宙应用。由于问题的随机性和训练所需的丰富跨模态数据集的缺乏，学习语音和3D全身手势之间的映射是困难的。在本文中，我们提出了一种基于Transformer的新颖框架，用于自动从语音生成3D人体手势。为了学习语音时的手势的随机性，我们提出了一种变分Transformer，可以有效地建模手势的概率分布，在推理时能够产生多样化的手势。此外，我们引入了一种模态位置嵌入层，用于捕捉不同语音模式中的不同运动速度。为了应对数据稀缺问题，我们设计了一种内部模态预训练方案，可以从有限的数据中学习语音和3D手势之间的复杂映射。我们的系统",
    "tldr": "本论文提出了一个基于Transformer的框架，用于自动从语音中合成3D人体手势。通过引入变分Transformer和模态位置嵌入层，可以有效地学习并生成多样化的手势。另外，通过内部模态预训练方案来解决数据稀缺问题。"
}