{
    "title": "DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])",
    "abstract": "Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model",
    "link": "http://arxiv.org/abs/2310.02025",
    "context": "Title: DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])\nAbstract: Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model",
    "path": "papers/23/10/2310.02025.json",
    "total_tokens": 713,
    "translated_title": "DeepZero: 将零阶优化应用于深度模型训练的扩展",
    "translated_abstract": "在无法获取一阶信息时，零阶优化已成为解决机器学习问题的一种常用技术。然而，零阶优化的可扩展性仍然是一个待解决的问题：其应用主要局限在相对小规模的机器学习问题上。我们开发了DeepZero，一个基于零阶优化的深度学习框架，通过三个主要创新将零阶优化扩展到从零开始的深度神经网络训练中。",
    "tldr": "DeepZero是一个扩展零阶优化到深度神经网络训练的深度学习框架，通过创新的坐标梯度估计和稀疏诱导的零阶训练协议，实现了高准确性和计算效率的优化。"
}