{
    "title": "Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) provides a foundation model by integrating natural language into visual concepts, enabling zero-shot recognition on downstream tasks. It is usually expected that satisfactory overall accuracy can be achieved across numerous domains through well-designed textual prompts. However, we found that their performance in the worst categories is significantly inferior to the overall performance. For example, on ImageNet, there are a total of 10 categories with class-wise accuracy as low as 0\\%, even though the overall performance has achieved 64.1\\%. This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance. To address this issue, we investigate the alignment between the two modalities in the CLIP model and propose the Class-wise Matching Margin (\\cmm) to measure the inference confusion. \\cmm\\ can effectively identify the worst-per",
    "link": "http://arxiv.org/abs/2310.03324",
    "context": "Title: Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])\nAbstract: Contrastive Language-Image Pre-training (CLIP) provides a foundation model by integrating natural language into visual concepts, enabling zero-shot recognition on downstream tasks. It is usually expected that satisfactory overall accuracy can be achieved across numerous domains through well-designed textual prompts. However, we found that their performance in the worst categories is significantly inferior to the overall performance. For example, on ImageNet, there are a total of 10 categories with class-wise accuracy as low as 0\\%, even though the overall performance has achieved 64.1\\%. This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance. To address this issue, we investigate the alignment between the two modalities in the CLIP model and propose the Class-wise Matching Margin (\\cmm) to measure the inference confusion. \\cmm\\ can effectively identify the worst-per",
    "path": "papers/23/10/2310.03324.json",
    "total_tokens": 965,
    "translated_title": "研究CLIP模型的限制：表现最差的类别",
    "translated_abstract": "对比式语言-图像预训练（CLIP）通过将自然语言与视觉概念整合，提供了一个基础模型，使得在下游任务中能够进行零样本识别。通常期望通过精心设计的文本提示在众多领域中达到令人满意的整体准确率。然而，我们发现他们在最差类别的表现明显低于整体表现。例如，在ImageNet上，共有10个类别的类别准确率仅为0％，尽管整体表现已达到64.1％。这种现象揭示了使用CLIP模型可能存在的潜在风险，特别是在风险敏感的应用中，特定类别具有重要性。为了解决这个问题，我们研究了CLIP模型中两个模态之间的对齐，并提出了用于衡量推理混淆的类别匹配边界（CMM）。CMM可以有效地识别最差表现的类别。",
    "tldr": "CLIP模型表现最差的类别的性能明显低于整体表现，揭示了其在特定类别重要性较高的风险敏感应用中的潜在风险。为了解决这个问题，研究了CLIP模型的模态对齐，并提出了用于衡量最差类别的推理混淆的类别匹配边界（CMM）。"
}