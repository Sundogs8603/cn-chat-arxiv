{
    "title": "Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks. (arXiv:2310.10556v1 [cs.LG])",
    "abstract": "A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any lo",
    "link": "http://arxiv.org/abs/2310.10556",
    "context": "Title: Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks. (arXiv:2310.10556v1 [cs.LG])\nAbstract: A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any lo",
    "path": "papers/23/10/2310.10556.json",
    "total_tokens": 846,
    "translated_title": "基于人类偏好的非参数离策略评估在深度网络中的样本复杂性",
    "translated_abstract": "最近流行的解决强化学习问题的方法是使用人类偏好数据。事实上，人类偏好数据现在与经典的强化学习算法（如演员-评论家方法）一起使用，在从人类偏好数据中学习的奖励上评估中间策略，即离策略评估（OPE）。该算法包括（i）从人类偏好数据集中学习奖励函数，以及（ii）学习目标策略的累积奖励。尽管有巨大的经验成功，但现有的使用偏好数据的OPE方法通常缺乏理论理解，并且严重依赖于启发式方法。在本文中，我们研究了基于人类偏好的OPE的样本效率，并为其建立了统计保证。具体而言，我们通过使用深度神经网络进行拟合Q评估来处理OPE。通过适当选择ReLU网络的大小，我们表明可以利用任何lo",
    "tldr": "本文研究了在深度网络中使用人类偏好进行非参数离策略评估的样本复杂性，并建立了统计保证。"
}