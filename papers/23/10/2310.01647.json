{
    "title": "Equivariant Adaptation of Large Pre-Trained Models. (arXiv:2310.01647v1 [cs.LG])",
    "abstract": "Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pre-trained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are",
    "link": "http://arxiv.org/abs/2310.01647",
    "context": "Title: Equivariant Adaptation of Large Pre-Trained Models. (arXiv:2310.01647v1 [cs.LG])\nAbstract: Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pre-trained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are",
    "path": "papers/23/10/2310.01647.json",
    "total_tokens": 852,
    "translated_title": "大型预训练模型的等变适应",
    "translated_abstract": "等变网络被专门设计用于保证与一组输入变换的一致行为，从而提高样本效率并实现更准确和稳健的预测。然而，重新设计主流深度神经网络架构的每个组件以实现所选等变性是一个困难的问题，并且可能导致在训练和推理过程中计算开销很大的网络。最近提出的一种替代等变性的方法是使用一个简单的规范化网络，在将输入提供给不受约束的预测网络之前将其转换为规范形式。我们在这里展示了这种方法可以有效地使一个大型预训练网络等变。然而，我们观察到产生的规范定向可能与训练分布的方向不一致，从而影响性能。通过使用依赖于数据集的先验知识来指导规范化函数，我们正在…",
    "tldr": "本论文提出了一种适用于大型预训练网络的等变适应方法，通过使用一个简单的规范化网络来使输入转换为规范形式。然而，我们观察到规范定向可能与训练分布的方向不一致，从而影响性能。",
    "en_tdlr": "This paper presents an equivariant adaptation method for large pre-trained networks by using a simple canonicalization network to transform the input to a canonical form. However, it is observed that the produced canonical orientations may be misaligned with those of the training distribution, hindering performance."
}