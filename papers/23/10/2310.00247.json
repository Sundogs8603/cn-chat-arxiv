{
    "title": "Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning. (arXiv:2310.00247v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) offers privacy-preserving decentralized machine learning, optimizing models at edge clients without sharing private data. Simultaneously, foundation models (FMs) have gained traction in the artificial intelligence (AI) community due to their exceptional performance across various tasks. However, integrating FMs into FL presents challenges, primarily due to their substantial size and intensive resource requirements. This is especially true when considering the resource heterogeneity in edge FL systems. We present an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to address these challenges. RaFFM introduces specialized model compression algorithms tailored for FL scenarios, such as salient parameter prioritization and high-performance subnetwork extraction. These algorithms enable dynamic scaling of given transformer-based FMs to fit heterogeneous resource constraints at the network edge during both FL's optimization and deployment stag",
    "link": "http://arxiv.org/abs/2310.00247",
    "context": "Title: Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning. (arXiv:2310.00247v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) offers privacy-preserving decentralized machine learning, optimizing models at edge clients without sharing private data. Simultaneously, foundation models (FMs) have gained traction in the artificial intelligence (AI) community due to their exceptional performance across various tasks. However, integrating FMs into FL presents challenges, primarily due to their substantial size and intensive resource requirements. This is especially true when considering the resource heterogeneity in edge FL systems. We present an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to address these challenges. RaFFM introduces specialized model compression algorithms tailored for FL scenarios, such as salient parameter prioritization and high-performance subnetwork extraction. These algorithms enable dynamic scaling of given transformer-based FMs to fit heterogeneous resource constraints at the network edge during both FL's optimization and deployment stag",
    "path": "papers/23/10/2310.00247.json",
    "total_tokens": 900,
    "translated_title": "架接基础模型与异构联邦学习之间的差距",
    "translated_abstract": "联邦学习（FL）提供了隐私保护的分散式机器学习，在不共享私人数据的情况下在边缘客户端上优化模型。同时，基础模型（FMs）由于其在各种任务中的出色性能在人工智能（AI）社区中引起了关注。然而，将基础模型集成到FL中存在挑战，主要是由于其庞大的大小和对资源的需求。尤其是考虑到边缘FL系统的资源异构性。我们提出了一个自适应的Resource-aware Federated Foundation Models（RaFFM）框架来解决这些挑战。RaFFM引入了针对FL场景的专门模型压缩算法，如显著参数优先级和高性能子网络提取。这些算法使得基于Transformer的给定FMs可以在网络边缘根据异构资源约束进行动态缩放，无论是在FL的优化还是部署阶段。",
    "tldr": "提出了一种自适应的Resource-aware Federated Foundation Models (RaFFM)框架，通过引入专门的模型压缩算法来解决将基础模型集成到联邦学习中的挑战，实现基于Transformer的基础模型在网络边缘根据异构资源约束进行动态缩放。",
    "en_tdlr": "An adaptive framework called Resource-aware Federated Foundation Models (RaFFM) is proposed to address the challenges of integrating foundation models into federated learning. This framework introduces specialized model compression algorithms, enabling dynamic scaling of transformer-based foundation models at the network edge to fit heterogeneous resource constraints."
}