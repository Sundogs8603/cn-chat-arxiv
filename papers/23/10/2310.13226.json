{
    "title": "Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering. (arXiv:2310.13226v1 [cs.CL])",
    "abstract": "Blockchain technology has revolutionized the financial landscape, with cryptocurrencies gaining widespread adoption for their decentralized and transparent nature. As the sentiment expressed on social media platforms can significantly influence cryptocurrency discussions and market movements, sentiment analysis has emerged as a crucial tool for understanding public opinion and predicting market trends. Motivated by the aim to enhance sentiment analysis accuracy in the cryptocurrency domain, this paper investigates fine-tuning techniques on large language models. This paper also investigates the efficacy of supervised fine-tuning and instruction-based fine-tuning on large language models for unseen tasks. Experimental results demonstrate a significant average zero-shot performance gain of 40% after fine-tuning, highlighting the potential of this technique in optimizing pre-trained language model efficiency. Additionally, the impact of instruction tuning on models of varying scales is ex",
    "link": "http://arxiv.org/abs/2310.13226",
    "context": "Title: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering. (arXiv:2310.13226v1 [cs.CL])\nAbstract: Blockchain technology has revolutionized the financial landscape, with cryptocurrencies gaining widespread adoption for their decentralized and transparent nature. As the sentiment expressed on social media platforms can significantly influence cryptocurrency discussions and market movements, sentiment analysis has emerged as a crucial tool for understanding public opinion and predicting market trends. Motivated by the aim to enhance sentiment analysis accuracy in the cryptocurrency domain, this paper investigates fine-tuning techniques on large language models. This paper also investigates the efficacy of supervised fine-tuning and instruction-based fine-tuning on large language models for unseen tasks. Experimental results demonstrate a significant average zero-shot performance gain of 40% after fine-tuning, highlighting the potential of this technique in optimizing pre-trained language model efficiency. Additionally, the impact of instruction tuning on models of varying scales is ex",
    "path": "papers/23/10/2310.13226.json",
    "total_tokens": 893,
    "translated_title": "提升零知识加密情绪分析的精调语言模型和提示工程",
    "translated_abstract": "区块链技术已经改变了金融领域，由于去中心化和透明的特点，加密货币得到了广泛的接受。社交媒体平台上表达的情绪可以显著影响加密货币的讨论和市场变动，情绪分析已经成为理解公众舆论和预测市场趋势的重要工具。为了提高加密货币领域情绪分析的准确性，本文研究了对大型语言模型的精调技术。本文还研究了在大型语言模型上利用有监督精调和基于指令的精调的有效性。实验结果表明，在精调后，平均零知识表现提升了40%，凸显了这一技术在优化预训练语言模型效率方面的潜力。此外，对不同规模模型的指令精调的影响也被探讨。",
    "tldr": "本文研究了对大型语言模型进行精调技术以提高加密货币领域情绪分析的准确性，在未知任务上对大型语言模型进行有监督精调和基于指令的精调，并获得了显著的零知识表现提升。"
}