{
    "title": "Geometric Learning with Positively Decomposable Kernels. (arXiv:2310.13821v1 [cs.LG])",
    "abstract": "Kernel methods are powerful tools in machine learning. Classical kernel methods are based on positive-definite kernels, which map data spaces into reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces, positive-definite kernels are difficult to come by. In this case, we propose the use of reproducing kernel Krein space (RKKS) based methods, which require only kernels that admit a positive decomposition. We show that one does not need to access this decomposition in order to learn in RKKS. We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions. This makes them much easier to construct than positive-definite kernels, providing a route for learning with kernels for non-Euclidean data. By the same token, this provides theoretical foundations for RKKS-based methods in general.",
    "link": "http://arxiv.org/abs/2310.13821",
    "context": "Title: Geometric Learning with Positively Decomposable Kernels. (arXiv:2310.13821v1 [cs.LG])\nAbstract: Kernel methods are powerful tools in machine learning. Classical kernel methods are based on positive-definite kernels, which map data spaces into reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces, positive-definite kernels are difficult to come by. In this case, we propose the use of reproducing kernel Krein space (RKKS) based methods, which require only kernels that admit a positive decomposition. We show that one does not need to access this decomposition in order to learn in RKKS. We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions. This makes them much easier to construct than positive-definite kernels, providing a route for learning with kernels for non-Euclidean data. By the same token, this provides theoretical foundations for RKKS-based methods in general.",
    "path": "papers/23/10/2310.13821.json",
    "total_tokens": 844,
    "translated_title": "使用正可分解核的几何学习",
    "translated_abstract": "核方法是机器学习中强大的工具。经典的核方法基于正定核，将数据空间映射到重现核希尔伯特空间(RKHS)。对于非欧几里德数据空间，很难找到正定核。在这种情况下，我们提出使用基于重现核控制空间(RKKS)的方法，这些方法只需要具有正分解的核。我们证明了在RKKS中学习时，并不需要访问这个分解。然后我们研究了使核正可分解的条件。我们证明在可处理的正则性假设下，不变核在齐次空间上允许正分解。这使得它们比正定核更容易构造，为非欧几里德数据的核学习提供了一条路径。同样，这为RKKS方法提供了一般的理论基础。",
    "tldr": "本文提出了使用正可分解核的几何学习方法，该方法通过在RKKS中学习而不需要访问核的分解，为非欧几里德数据的核学习提供了一条路径，并为RKKS方法提供了理论基础。"
}