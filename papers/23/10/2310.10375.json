{
    "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])",
    "abstract": "As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b",
    "link": "http://arxiv.org/abs/2310.10375",
    "context": "Title: GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])\nAbstract: As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b",
    "path": "papers/23/10/2310.10375.json",
    "total_tokens": 832,
    "translated_title": "GTA：一种面向几何的多视图Transformer的注意力机制",
    "translated_abstract": "随着transformers对输入标记的排列具有等变性，对标记的位置信息进行编码对许多任务是必要的。然而，由于现有的位置编码方案最初是为自然语言处理任务设计的，对于通常在其数据中表现出不同结构特性的视觉任务来说，它们的适用性值得怀疑。我们认为现有的位置编码方案对于3D视觉任务来说是次优的，因为它们不尊重其底层的3D几何结构。基于这个假设，我们提出了一种面向几何的注意力机制，它将标记的几何结构编码为由查询和键值对之间的几何关系所确定的相对变换。通过在稀疏宽基线多视图设置中评估多个新颖视图合成（NVS）数据集，我们展示了我们的注意力机制——几何变换注意力（GTA）如何提高了最先进的Transformer的学习效率和性能。",
    "tldr": "提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。",
    "en_tdlr": "Proposed a geometry-aware attention mechanism (GTA) that encodes geometric structure as relative transformations, improving learning efficiency and performance of multi-view Transformers."
}