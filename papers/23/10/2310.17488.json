{
    "title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])",
    "abstract": "This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also sh",
    "link": "http://arxiv.org/abs/2310.17488",
    "context": "Title: LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])\nAbstract: This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also sh",
    "path": "papers/23/10/2310.17488.json",
    "total_tokens": 812,
    "translated_title": "LightLM: 一种轻量级的基于Transformer的生成推荐模型",
    "translated_abstract": "本文介绍了LightLM，一种轻量级的基于Transformer的生成推荐模型。在NLP和视觉等各个人工智能子领域中，基于Transformer的生成建模已经变得越来越重要，而生成推荐由于其对个性化生成建模的独特需求，仍处于初级阶段。现有的生成推荐方法通常使用面向NLP的Transformer架构，如T5、GPT、LLaMA和M6，这些模型比较庞大，且并没有专门针对推荐任务进行设计。LightLM通过引入轻量级深窄Transformer架构来解决这个问题，该架构特别适用于直接生成推荐项。这种结构对于直接的生成推荐非常合适，因为输入主要由适合模型容量的短标记组成，语言模型在这个任务上不需要太宽的结构。我们还...",
    "tldr": "LightLM是一种轻量级的基于Transformer的生成推荐模型，通过引入轻量级深窄Transformer架构来实现直接生成推荐项。"
}