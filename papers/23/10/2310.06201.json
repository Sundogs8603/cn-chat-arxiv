{
    "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models. (arXiv:2310.06201v1 [cs.CL])",
    "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction",
    "link": "http://arxiv.org/abs/2310.06201",
    "context": "Title: Compressing Context to Enhance Inference Efficiency of Large Language Models. (arXiv:2310.06201v1 [cs.CL])\nAbstract: Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction",
    "path": "papers/23/10/2310.06201.json",
    "total_tokens": 849,
    "translated_title": "压缩上下文以提高大型语言模型的推理效率",
    "translated_abstract": "大型语言模型在各种任务中取得了显著的性能。然而，由于计算要求显著增加，包括内存和推理时间，以及输入超出语言模型固定上下文长度时潜在的上下文截断，它们在处理长文档和扩展对话时面临挑战。本文提出了一种称为Selective Context的方法，通过识别和修剪输入上下文中的冗余信息，使输入更紧凑，从而提高了语言模型的推理效率。我们在需要处理长上下文的常见数据源上进行了测试，包括arXiv论文、新闻文章和长对话，用于摘要、问答和回答生成任务。实验结果表明，Selective Context显著减少了内存成本，并降低了生成延迟，同时保持了与使用完整上下文时相当的性能。具体来说，我们实现了50%的内存成本减少。",
    "tldr": "本研究提出了一种名为Selective Context的方法，通过识别和修剪输入上下文中的冗余信息，以降低大型语言模型的推理过程中的计算成本和延迟，并保持相当的性能。",
    "en_tdlr": "This paper proposes a method called Selective Context that reduces the computational cost and latency during inference of large language models by identifying and pruning redundancy in the input context, while maintaining comparable performance."
}