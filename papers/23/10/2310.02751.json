{
    "title": "SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning. (arXiv:2310.02751v1 [cs.LG])",
    "abstract": "In this paper, we hypothesize that gradient-based meta-learning (GBML) implicitly suppresses the Hessian along the optimization trajectory in the inner loop. Based on this hypothesis, we introduce an algorithm called SHOT (Suppressing the Hessian along the Optimization Trajectory) that minimizes the distance between the parameters of the target and reference models to suppress the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does not increase the computational complexity of the baseline model much. It is agnostic to both the algorithm and architecture used in GBML, making it highly versatile and applicable to any GBML baseline. To validate the effectiveness of SHOT, we conduct empirical tests on standard few-shot learning tasks and qualitatively analyze its dynamics. We confirm our hypothesis empirically and demonstrate that SHOT outperforms the corresponding baseline. Code is available at: https://github.com/JunHoo-Lee/SHOT",
    "link": "http://arxiv.org/abs/2310.02751",
    "context": "Title: SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning. (arXiv:2310.02751v1 [cs.LG])\nAbstract: In this paper, we hypothesize that gradient-based meta-learning (GBML) implicitly suppresses the Hessian along the optimization trajectory in the inner loop. Based on this hypothesis, we introduce an algorithm called SHOT (Suppressing the Hessian along the Optimization Trajectory) that minimizes the distance between the parameters of the target and reference models to suppress the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does not increase the computational complexity of the baseline model much. It is agnostic to both the algorithm and architecture used in GBML, making it highly versatile and applicable to any GBML baseline. To validate the effectiveness of SHOT, we conduct empirical tests on standard few-shot learning tasks and qualitatively analyze its dynamics. We confirm our hypothesis empirically and demonstrate that SHOT outperforms the corresponding baseline. Code is available at: https://github.com/JunHoo-Lee/SHOT",
    "path": "papers/23/10/2310.02751.json",
    "total_tokens": 912,
    "translated_title": "SHOT：抑制梯度优化轨迹中的海森矩阵以用于基于梯度的元学习",
    "translated_abstract": "本文假设梯度优化的元学习（GBML）在内循环中隐式地抑制了优化轨迹上的海森矩阵。基于这个假设，我们引入了一种名为SHOT（抑制梯度优化轨迹中的海森矩阵）的算法，通过最小化目标模型和参考模型参数之间的距离来抑制内循环中的海森矩阵。尽管涉及高阶项，SHOT并不显著增加基线模型的计算复杂度。SHOT对GBML中使用的算法和架构都是不可知的，使其具有高度的通用性，并适用于任何GBML基线模型。为了验证SHOT的有效性，我们在标准的少样本学习任务上进行了实证测试，并进行了动力学的定性分析。我们通过实验证实了我们的假设，并证明了SHOT优于相应的基线模型。代码可在https://github.com/JunHoo-Lee/SHOT找到。",
    "tldr": "本文提出了一种名为SHOT的算法，通过抑制梯度优化轨迹中的海森矩阵来改进基于梯度的元学习，在标准的少样本学习任务中取得了优于基线模型的效果。"
}