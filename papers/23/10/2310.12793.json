{
    "title": "OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])",
    "abstract": "Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa",
    "link": "http://arxiv.org/abs/2310.12793",
    "context": "Title: OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])\nAbstract: Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa",
    "path": "papers/23/10/2310.12793.json",
    "total_tokens": 1039,
    "translated_title": "OODRobustBench: 在分布迁移下评估和分析对抗性鲁棒性的基准",
    "translated_abstract": "现有的研究在提高对抗性鲁棒性方面取得了很大进展，但通常只在与训练数据相同分布的数据上进行测试，即内分布（ID）测试。因此，目前尚不清楚这种鲁棒性在输入分布迁移，即离群分布（OOD）测试下的泛化性能。在实际部署时，由于这种分布迁移是不可避免的，这一问题十分令人担忧。为了解决这个问题，我们提出了一个名为OODRobustBench的基准，通过使用23个基于数据集的迁移（即输入分布的自然迁移）和6个基于威胁的迁移（即未知的对抗性威胁模型）来全面评估OOD的对抗性鲁棒性。OODRobustBench用于评估了706个鲁棒模型，并进行了60.7K次对抗性评估。这个大规模分析表明：1）对抗性鲁棒性存在严重的OOD泛化问题；2）ID鲁棒性与OOD鲁棒性呈强正线性相关。",
    "tldr": "OODRobustBench是一个基准，用于评估和分析在分布迁移下的对抗性鲁棒性。大规模分析结果表明，对抗性鲁棒性在离群分布测试下存在严重的泛化问题，而内分布鲁棒性与离群分布鲁棒性呈强正线性相关。",
    "en_tdlr": "OODRobustBench is a benchmark for evaluating and analyzing adversarial robustness under distribution shift. The large-scale analysis shows that adversarial robustness has a severe generalization issue under out-of-distribution testing. Furthermore, it demonstrates a strong positive linear correlation between in-distribution robustness and out-of-distribution robustness."
}