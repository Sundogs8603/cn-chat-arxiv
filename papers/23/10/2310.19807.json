{
    "title": "Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates. (arXiv:2310.19807v1 [cs.LG])",
    "abstract": "Federated reinforcement learning (FedRL) enables agents to collaboratively train a global policy without sharing their individual data. However, high communication overhead remains a critical bottleneck, particularly for natural policy gradient (NPG) methods, which are second-order. To address this issue, we propose the FedNPG-ADMM framework, which leverages the alternating direction method of multipliers (ADMM) to approximate global NPG directions efficiently. We theoretically demonstrate that using ADMM-based gradient updates reduces communication complexity from ${O}({d^{2}})$ to ${O}({d})$ at each iteration, where $d$ is the number of model parameters. Furthermore, we show that achieving an $\\epsilon$-error stationary convergence requires ${O}(\\frac{1}{(1-\\gamma)^{2}{\\epsilon}})$ iterations for discount factor $\\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo environments, we ",
    "link": "http://arxiv.org/abs/2310.19807",
    "context": "Title: Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates. (arXiv:2310.19807v1 [cs.LG])\nAbstract: Federated reinforcement learning (FedRL) enables agents to collaboratively train a global policy without sharing their individual data. However, high communication overhead remains a critical bottleneck, particularly for natural policy gradient (NPG) methods, which are second-order. To address this issue, we propose the FedNPG-ADMM framework, which leverages the alternating direction method of multipliers (ADMM) to approximate global NPG directions efficiently. We theoretically demonstrate that using ADMM-based gradient updates reduces communication complexity from ${O}({d^{2}})$ to ${O}({d})$ at each iteration, where $d$ is the number of model parameters. Furthermore, we show that achieving an $\\epsilon$-error stationary convergence requires ${O}(\\frac{1}{(1-\\gamma)^{2}{\\epsilon}})$ iterations for discount factor $\\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo environments, we ",
    "path": "papers/23/10/2310.19807.json",
    "total_tokens": 954,
    "translated_title": "通过基于ADMM梯度更新改进联邦自然策略梯度的通信效率",
    "translated_abstract": "联邦强化学习(FedRL)使得代理能够在不共享个体数据的情况下合作训练全局策略。然而，高通信开销仍然是一个关键瓶颈，特别是对于二阶的自然策略梯度方法来说。为了解决这个问题，我们提出了FedNPG-ADMM框架，它利用交替方向乘法器方法(ADMM)来有效地近似全局自然策略梯度方向。我们从理论上证明，使用基于ADMM的梯度更新将每次迭代的通信复杂度从$O(d^2)$降低到$O(d)$，其中$d$是模型参数的数量。此外，我们还展示了实现$\\epsilon$-误差稳定收敛所需的迭代次数为$O(\\frac{1}{(1-\\gamma)^{2}\\epsilon})$，其中$\\gamma$是折扣因子，证明了FedNPG-ADMM具有与标准FedNPG相同的收敛速度。通过在MuJoCo环境中评估提出的算法，我们表明了FedNPG-ADMM的效果。",
    "tldr": "本论文提出了FedNPG-ADMM框架，通过利用ADMM方法来近似全局自然策略梯度方向，从而显著降低了联邦自然策略梯度的通信开销。",
    "en_tdlr": "This paper proposes the FedNPG-ADMM framework, which leverages the ADMM method to approximate global natural policy gradient directions, thereby significantly reducing the communication overhead in federated natural policy gradient."
}