{
    "title": "UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web",
    "abstract": "arXiv:2310.18340v2 Announce Type: replace-cross  Abstract: Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it",
    "link": "https://arxiv.org/abs/2310.18340",
    "context": "Title: UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web\nAbstract: arXiv:2310.18340v2 Announce Type: replace-cross  Abstract: Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it",
    "path": "papers/23/10/2310.18340.json",
    "total_tokens": 831,
    "translated_title": "UrbanCLIP：学习来自网络的对比语言图像预训练文本增强的城市区域描述",
    "translated_abstract": "从网络数据进行的城市区域描述对城市规划和可持续发展至关重要。我们目睹了LLM在各个领域的不断崛起，尤其是处理多模态数据研究，如视觉-语言学习，其中文本模态作为图像的补充信息。本文旨在回答两个基本问题：i）文本模态能否增强城市区域描述？ii）如果可以，以何种方式和在哪些方面？为了回答这些问题，我们利用了大型语言模型（LLMs）的强大力量，并引入了第一个集成文本模态知识到城市图像描述中的LLM增强框架，命名为对比语言-图像预训练LLM增强型城市区域描述（UrbanCLIP）。",
    "tldr": "本文介绍了第一个将文本模态融入城市图像描述的LLM增强框架UrbanCLIP，并探讨了文本模态如何增强城市区域描述以及其影响方面。",
    "en_tdlr": "This paper introduces the first LLM-enhanced framework UrbanCLIP that integrates textual modality into urban imagery profiling, exploring how textual modality enhances urban region profiling and its impacts."
}