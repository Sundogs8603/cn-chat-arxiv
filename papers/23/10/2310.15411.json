{
    "title": "Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach. (arXiv:2310.15411v1 [cs.LG])",
    "abstract": "We study the problem of computationally and label efficient PAC active learning $d$-dimensional halfspaces with Tsybakov Noise~\\citep{tsybakov2004optimal} under structured unlabeled data distributions. Inspired by~\\cite{diakonikolas2020learning}, we prove that any approximate first-order stationary point of a smooth nonconvex loss function yields a halfspace with a low excess error guarantee. In light of the above structural result, we design a nonconvex optimization-based algorithm with a label complexity of $\\tilde{O}(d (\\frac{1}{\\epsilon})^{\\frac{8-6\\alpha}{3\\alpha-1}})$\\footnote{In the main body of this work, we use $\\tilde{O}(\\cdot), \\tilde{\\Theta}(\\cdot)$ to hide factors of the form $\\polylog(d, \\frac{1}{\\epsilon}, \\frac{1}{\\delta})$}, under the assumption that the Tsybakov noise parameter $\\alpha \\in (\\frac13, 1]$, which narrows down the gap between the label complexities of the previously known efficient passive or active algorithms~\\citep{diakonikolas2020polynomial,zhang2021im",
    "link": "http://arxiv.org/abs/2310.15411",
    "context": "Title: Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach. (arXiv:2310.15411v1 [cs.LG])\nAbstract: We study the problem of computationally and label efficient PAC active learning $d$-dimensional halfspaces with Tsybakov Noise~\\citep{tsybakov2004optimal} under structured unlabeled data distributions. Inspired by~\\cite{diakonikolas2020learning}, we prove that any approximate first-order stationary point of a smooth nonconvex loss function yields a halfspace with a low excess error guarantee. In light of the above structural result, we design a nonconvex optimization-based algorithm with a label complexity of $\\tilde{O}(d (\\frac{1}{\\epsilon})^{\\frac{8-6\\alpha}{3\\alpha-1}})$\\footnote{In the main body of this work, we use $\\tilde{O}(\\cdot), \\tilde{\\Theta}(\\cdot)$ to hide factors of the form $\\polylog(d, \\frac{1}{\\epsilon}, \\frac{1}{\\delta})$}, under the assumption that the Tsybakov noise parameter $\\alpha \\in (\\frac13, 1]$, which narrows down the gap between the label complexities of the previously known efficient passive or active algorithms~\\citep{diakonikolas2020polynomial,zhang2021im",
    "path": "papers/23/10/2310.15411.json",
    "total_tokens": 927,
    "translated_title": "高效的带有Tsybakov噪声的半空间主动学习：一种非凸优化方法",
    "translated_abstract": "我们研究了在结构化无标签数据分布下，对于具有Tsybakov噪声的$d$维半空间，计算和标签的高效PAC主动学习问题。受到\\cite{diakonikolas2020learning}的启发，我们证明了平滑非凸损失函数的任何近似一阶稳定点都会产生一个具有低过量误差保证的半空间。根据上述结构性结果，我们设计了一种基于非凸优化的算法，其标签复杂度为$\\tilde{O}(d (\\frac{1}{\\epsilon})^{\\frac{8-6\\alpha}{3\\alpha-1}})$，在Tsybakov噪声参数$\\alpha \\in (\\frac13, 1]$的假设下，这缩小了先前已知的高效被动或主动算法的标签复杂度间隔。",
    "tldr": "这个论文研究了在结构化无标签数据分布下，对于具有Tsybakov噪声的$d$维半空间，计算和标签的高效PAC主动学习问题。通过设计一种基于非凸优化的算法，它能够在一定的噪声参数范围内达到较低的标签复杂度。"
}