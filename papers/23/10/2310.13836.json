{
    "title": "Foundation Model's Embedded Representations May Detect Distribution Shift",
    "abstract": "Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply",
    "link": "https://rss.arxiv.org/abs/2310.13836",
    "context": "Title: Foundation Model's Embedded Representations May Detect Distribution Shift\nAbstract: Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply",
    "path": "papers/23/10/2310.13836.json",
    "total_tokens": 959,
    "translated_title": "基础模型的嵌入表示能够检测到分布偏移",
    "translated_abstract": "采样偏差可能导致监督学习任务的训练和测试数据集之间发生分布偏移，使我们难以理解模型的泛化能力。鉴于广泛采用已预训练的基础神经网络作为迁移学习任务的工具，这一点尤为重要，而这些基础神经网络的行为仍然不太清楚。我们以Sentiment140数据集上的迁移学习为例进行了案例研究，并展示了许多预训练的基础模型对Sentiment140的手动标注的测试集M和自动标注的训练集P具有不同的表示，证实了发生了分布偏移。我们认为在P上训练并在M上评估性能是一种有偏差的泛化度量。对预训练的GPT-2进行的实验表明，从P中可学得的特征不会改善（事实上会阻碍）在M上的性能。对预训练的GPT-2的表示进行线性探测是鲁棒的，甚至可能胜过整体微调。",
    "tldr": "该研究发现基础模型的嵌入表示可以检测到数据集之间的分布偏移，且在传统的泛化度量上表现出偏差。预训练的GPT-2模型的特征学习无法在特定任务上提升性能，而对其表示进行线性探测可能优于整体微调。"
}