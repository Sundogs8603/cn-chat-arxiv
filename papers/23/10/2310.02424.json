{
    "title": "AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])",
    "abstract": "Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac",
    "link": "http://arxiv.org/abs/2310.02424",
    "context": "Title: AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])\nAbstract: Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac",
    "path": "papers/23/10/2310.02424.json",
    "total_tokens": 911,
    "translated_title": "AXNav: 从自然语言中重放无障碍测试",
    "translated_abstract": "开发者和质量保证测试人员通常依赖手动测试来在产品生命周期中测试无障碍功能。然而，手动测试可能很乏味，范围庞大，并且很难安排在其他开发里程碑之间。最近，大型语言模型（LLMs）已被用于各种任务，包括自动化用户界面，但据我们了解，迄今为止还没有人探索过它们在控制辅助技术以支持无障碍测试方面的应用。在本文中，我们从一项形成性研究开始，探讨基于自然语言的无障碍测试工作流程的要求。我们构建了一个系统，该系统以手动无障碍测试为输入（例如，“在VoiceOver中搜索一个节目”），并使用LLM结合基于像素的用户界面理解模型来执行测试并生成章节划分的可导航视频。在每个视频中，为了帮助质量保证测试人员，我们应用启发式方法来检测和标记ac。",
    "tldr": "这篇论文研究了一种从自然语言中重放无障碍测试的系统，该系统利用大型语言模型和基于像素的用户界面理解模型执行测试并生成可导航的视频。通过这种方式，开发人员和质量保证测试人员能够更高效地测试无障碍功能。"
}