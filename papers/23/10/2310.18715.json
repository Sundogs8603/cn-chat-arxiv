{
    "title": "Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])",
    "abstract": "This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.",
    "link": "http://arxiv.org/abs/2310.18715",
    "context": "Title: Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])\nAbstract: This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.",
    "path": "papers/23/10/2310.18715.json",
    "total_tokens": 816,
    "translated_title": "具有重尾奖励的强化学习离线策略评估和优化的鲁棒性提升",
    "translated_abstract": "本文旨在增强离线强化学习在现实世界应用中普遍存在的重尾奖励情况下的鲁棒性。我们提出了两个算法框架，ROAM和ROOM，用于鲁棒的离线策略评估和离线策略优化。我们的框架的核心是将中位数法与离线强化学习策略相结合，能够对值函数估计器进行直接的不确定性估计。这不仅符合离线策略优化中的保守主义原则，而且灵活处理重尾奖励。理论结果和广泛的实验证明，我们的两个框架在记录的数据集中展示了具有重尾奖励分布时超越现有方法的性能。",
    "tldr": "本文提出的ROAM和ROOM算法框架通过将中位数法与离线强化学习策略相结合，提供了对重尾奖励的直接不确定性估计，从而增强了离线强化学习在现实应用中的鲁棒性。"
}