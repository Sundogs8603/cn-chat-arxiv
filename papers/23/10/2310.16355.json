{
    "title": "Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])",
    "abstract": "The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g",
    "link": "http://arxiv.org/abs/2310.16355",
    "context": "Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])\nAbstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g",
    "path": "papers/23/10/2310.16355.json",
    "total_tokens": 868,
    "translated_title": "Redco:一个轻量级工具，可在任何GPU/TPUs上自动化分布式训练LLMs",
    "translated_abstract": "人工智能的最新进展主要归功于大型语言模型（LLMs）。然而，它们不断增长的内存需求给机器学习（ML）研究人员和工程师带来了挑战。解决这个问题需要开发人员将大型模型分区以分布在多个GPU或TPU上。这需要使用现有模型并行工具（如Megatron-LM、DeepSpeed和Alpa）进行相当的编码和复杂的配置工作。这些工具需要用户具备机器学习系统（MLSys）的专业知识，给LLM开发带来了瓶颈，特别是对于没有MLSys背景的开发人员。在这项工作中，我们提出了Redco，这是一个轻量级且用户友好的工具，旨在自动化LLMs的分布式训练和推理，以及简化ML流程的开发。Redco的设计强调了两个关键方面。首先，为了自动化模型并行，我们的研究确定了两个简单的规则，用于为任何GPU / TPU生成张量并行策略。",
    "tldr": "Redco是一个轻量级工具，旨在自动化分布式训练LLMs，并简化ML流程的开发。",
    "en_tdlr": "Redco is a lightweight tool designed to automate distributed training of LLMs and simplify the development of ML pipelines."
}