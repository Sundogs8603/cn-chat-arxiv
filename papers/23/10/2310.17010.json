{
    "title": "This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])",
    "abstract": "Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.",
    "link": "http://arxiv.org/abs/2310.17010",
    "context": "Title: This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])\nAbstract: Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.",
    "path": "papers/23/10/2310.17010.json",
    "total_tokens": 867,
    "translated_title": "这就像那样阅读：用于可解释性自然语言处理的深度学习",
    "translated_abstract": "原型学习是一种流行的机器学习方法，用于解释性决策，通过学习的原型的相似性来对新数据进行分类。尽管它主要应用于计算机视觉，但在这项工作中，我们建立在之前的研究基础上，进一步探索了原型网络在自然语言处理中的扩展。我们引入了一种学习的加权相似度度量，通过聚焦于预训练的句子嵌入的信息维度来增强相似度计算。此外，我们提出了一种事后可解释性机制，从原型和输入句子中提取与预测相关的单词。最后，我们通过实验证明，我们提出的方法不仅改善了在AG News和RT Polarity数据集上的预测性能，而且与基于合理性的递归卷积相比，还提高了解释的准确性。",
    "tldr": "本研究将原型网络方法扩展到了自然语言处理领域，引入了一种学习的加权相似度度量和事后可解释性机制，通过聚焦于句子嵌入的重要维度来提高相似度计算，并改善了预测性能和解释准确性。",
    "en_tdlr": "This research extends prototype networks to natural language processing, introducing a learned weighted similarity measure and a post-hoc explainability mechanism to enhance similarity computation by focusing on important dimensions of sentence embeddings, improving predictive performance and explanation accuracy."
}