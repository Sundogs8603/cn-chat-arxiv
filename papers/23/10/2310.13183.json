{
    "title": "Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v1 [cs.CV])",
    "abstract": "It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way. Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates. To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks. Our extensive experiments demonstrate that this approach achieves state-of-the-art performance across eight datasets from GLUE, particularly excelling at high levels of sparsity.",
    "link": "http://arxiv.org/abs/2310.13183",
    "context": "Title: Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v1 [cs.CV])\nAbstract: It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way. Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates. To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks. Our extensive experiments demonstrate that this approach achieves state-of-the-art performance across eight datasets from GLUE, particularly excelling at high levels of sparsity.",
    "path": "papers/23/10/2310.13183.json",
    "total_tokens": 864,
    "translated_title": "打破确定性限制：随机修剪掩码的生成和选取",
    "translated_abstract": "在相同模型尺寸约束下，大且稀疏的模型往往比小且密集的模型具有更高的准确性。这促使我们通过修剪来移除冗余的神经元或权重。大多数现有方法采用确定性的方式进行修剪，其性能仅依赖于单一的修剪准则，缺乏多样性。相反，本文提出了一种模型修剪策略，首先以设计好的随机方式生成多个修剪掩码。然后，通过有效的掩码选择规则，从候选掩码池中选择最优的掩码。为了进一步提高效率，我们引入了一种早期掩码评估策略，减轻了训练多个掩码所带来的开销。大量实验证明，这种方法在GLUE的八个数据集上取得了最先进的性能，特别在高稀疏程度下表现出色。",
    "tldr": "本研究提出了一种用于模型修剪的随机化策略，通过生成多个随机修剪掩码，并结合有效的选择规则选取最优掩码，实现了在八个GLUE数据集上达到最先进性能的结果。",
    "en_tdlr": "This paper proposes a randomized strategy for model pruning, generating multiple random pruning masks and selecting the optimal mask with an effective selection rule, achieving state-of-the-art performance on eight GLUE datasets."
}