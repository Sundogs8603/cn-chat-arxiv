{
    "title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)",
    "abstract": "The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.",
    "link": "http://arxiv.org/abs/2310.01119",
    "context": "Title: Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)\nAbstract: The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.",
    "path": "papers/23/10/2310.01119.json",
    "total_tokens": 764,
    "translated_title": "通过对大型语言模型进行微调在低资源环境中合成数据生成",
    "translated_abstract": "大型语言模型(LLMs)的上下文学习能力使它们能够以相对较少的标记样本推广到新的下游任务。然而，它们需要巨大的计算资源才能部署。相反，如果用足够多的标记样本对较小的模型进行微调，它们可以解决特定任务。然而，这些样本获取起来很昂贵。为了追求两全其美，我们研究了通过对经过精细调整的教师LLMs生成的训练数据进行合成的合成数据生成，以改善较小模型的下游性能。在四个文本分类和两个文本生成任务中，我们发现数据生成和注释都显著提高了相应下游模型的性能，有时只需要原始训练数据集的一小部分。",
    "tldr": "通过对大型语言模型进行微调，在低资源环境中可以通过合成数据生成来改善较小模型的性能，显著提高了下游模型的性能。"
}