{
    "title": "Proactive Human-Robot Interaction using Visuo-Lingual Transformers. (arXiv:2310.02506v1 [cs.RO])",
    "abstract": "Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures int",
    "link": "http://arxiv.org/abs/2310.02506",
    "context": "Title: Proactive Human-Robot Interaction using Visuo-Lingual Transformers. (arXiv:2310.02506v1 [cs.RO])\nAbstract: Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures int",
    "path": "papers/23/10/2310.02506.json",
    "total_tokens": 880,
    "translated_title": "使用视觉-语言转换器进行主动的人机交互",
    "translated_abstract": "人类具有提取潜在的视觉-语言线索并通过人机交互推断上下文的能力。在合作过程中，这使得能够主动预测一系列任务的潜在意图。相比之下，与人类合作的机器人智能地遵循基本指令完成任务，或者在朝着目标完成的过程中使用特定的手工触发器来启动主动合作。赋予这样的机器人思考最终目标和主动建议中间任务的能力将为人机合作提供一种更直观的方法。为此，我们提出了一种基于学习的方法，利用场景中的视觉线索、用户的语言命令以及先前的物体-物体交互知识来识别和主动预测用户意图要实现的目标。具体地，我们提出了ViLing-MMT，一种基于视觉-语言多模态转换器的架构，它捕捉到了中间目标和目标目标之间的交互关系。",
    "tldr": "本文提出了一种使用视觉-语言转换器进行主动的人机交互的方法，通过提取潜在的视觉-语言线索来推断上下文，识别和主动预测用户意图要实现的目标。",
    "en_tdlr": "This paper proposes a method for proactive human-robot interaction using visuo-lingual transformers, which extracts latent visuo-lingual cues to infer context and identifies and proactively predicts the underlying goals the user intends to achieve."
}