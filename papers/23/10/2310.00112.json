{
    "title": "Reinforcement Learning for Node Selection in Branch-and-Bound. (arXiv:2310.00112v1 [cs.LG])",
    "abstract": "A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Exp",
    "link": "http://arxiv.org/abs/2310.00112",
    "context": "Title: Reinforcement Learning for Node Selection in Branch-and-Bound. (arXiv:2310.00112v1 [cs.LG])\nAbstract: A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Exp",
    "path": "papers/23/10/2310.00112.json",
    "total_tokens": 931,
    "translated_title": "基于强化学习的分支定界算法中的节点选择",
    "translated_abstract": "分支定界算法中的一个重要挑战是从搜索树中确定最优节点。当前最先进的选择器要么使用手工制作的集合，自动切换为天真的子节点选择器，要么使用依赖于个别节点数据的学习节点选择器。我们提出了一种新颖的双模拟技术，利用强化学习（RL）考虑整个树状态，而不仅仅是孤立的节点。为了实现这一点，我们训练了一个图神经网络，它根据模型从根节点到“待选择”叶子节点的路径产生一个概率分布。将节点选择建模为概率分布使我们能够使用最先进的强化学习技术来训练模型，捕捉内在节点质量和节点评估成本。尽管只是在专门设计的合成TSP实例上进行训练，我们的方法在一组多样且复杂的问题集上引出了高质量的节点选择策略。",
    "tldr": "本论文提出了一种在分支定界算法中使用强化学习进行节点选择的新方法。我们通过训练图神经网络来模拟整个树的状态，并使用概率分布来选择节点。尽管只在合成TSP实例上进行了训练，我们的方法在各种复杂问题集上得到了高质量的节点选择策略。",
    "en_tdlr": "This paper proposes a novel approach for node selection in branch and bound algorithms using reinforcement learning. The authors train a graph neural network to simulate the entire tree state and use a probability distribution for node selection. Despite being trained on synthetic TSP instances, their method produces high-quality node selection policies for diverse and complex problem sets."
}