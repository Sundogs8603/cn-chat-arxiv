{
    "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti",
    "link": "http://arxiv.org/abs/2310.00313",
    "context": "Title: In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti",
    "path": "papers/23/10/2310.00313.json",
    "total_tokens": 911,
    "translated_title": "大型语言模型中的上下文学习: 对表示的神经科学启发式分析",
    "translated_abstract": "通过利用输入中的特定任务示例，大型语言模型（LLMs）通过上下文学习（ICL）展现了卓越的性能提升。然而，这种改进背后的机制仍然难以理解。本研究中，我们调查了Llama-2 70B和Vicuna 13B中的嵌入和注意力表示。具体而言，我们研究了上下文学习后嵌入和注意力的变化以及这些变化如何调解行为的改进。我们采用了受神经科学启发的技术，如表示相似性分析（RSA），并提出了参数化探测和注意力比率分析（ARA，衡量关注相关与无关信息的比率）的新方法。我们设计了三个具有条件之间先验关系的任务：阅读理解，线性回归和对抗提示注入。我们提出了关于任务表示中预期相似性的假设，以研究嵌入和注意力中的潜在变化。",
    "tldr": "本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。",
    "en_tdlr": "This study investigates the mechanisms of in-context learning in large language models using neuroscience-inspired techniques, revealing latent changes behind the performance improvement through the examination of embeddings and attention representations, and proposing novel methods for parameterized probing and attention ratio analysis."
}