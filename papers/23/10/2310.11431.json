{
    "title": "Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. (arXiv:2310.11431v1 [stat.ML])",
    "abstract": "Single neurons in neural networks are often ``interpretable'' in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural",
    "link": "http://arxiv.org/abs/2310.11431",
    "context": "Title: Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. (arXiv:2310.11431v1 [stat.ML])\nAbstract: Single neurons in neural networks are often ``interpretable'' in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural",
    "path": "papers/23/10/2310.11431.json",
    "total_tokens": 830,
    "translated_title": "在人工和生物神经系统中识别可解释的视觉特征",
    "translated_abstract": "神经网络中的单个神经元通常是“可解释的”，因为它们代表个别直观有意义的特征。然而，许多神经元表现出“混合选择性”，即它们代表多个不相关的特征。最近的假设认为，深度网络中的特征可能以“叠加”的方式表示，即由多个神经元沿非正交轴表示，因为自然数据中可解释的特征数通常大于给定网络中的神经元数。因此，我们应该能够在激活空间中找到与个别神经元不对齐的有意义的方向。在这里，我们提出了（1）一种自动化的方法来量化视觉可解释性，并通过与大量人类心理物理学对神经元可解释性的判断进行验证，以及（2）一种在网络激活空间中寻找有意义方向的方法。我们利用这些方法来发现卷积神经网络中的方向。",
    "tldr": "本文提出了一种量化视觉可解释性的自动化方法，并找到了卷积神经网络中有意义的方向。",
    "en_tdlr": "This paper proposes an automated method to quantify visual interpretability and discovers meaningful directions in convolutional neural network activation space."
}