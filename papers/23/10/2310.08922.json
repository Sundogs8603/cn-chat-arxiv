{
    "title": "LLaMA Rider: Spurring Large Language Models to Explore the Open World. (arXiv:2310.08922v1 [cs.LG])",
    "abstract": "Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model's performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experi",
    "link": "http://arxiv.org/abs/2310.08922",
    "context": "Title: LLaMA Rider: Spurring Large Language Models to Explore the Open World. (arXiv:2310.08922v1 [cs.LG])\nAbstract: Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model's performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experi",
    "path": "papers/23/10/2310.08922.json",
    "total_tokens": 858,
    "translated_title": "LLaMA Rider：推动大型语言模型探索开放世界",
    "translated_abstract": "近期，多项研究利用大型语言模型(LLMs)在环境中帮助决策和规划，并尝试将LLMs的知识与世界条件对齐。然而，LLMs在持续获取环境知识并在开放世界中适应的能力仍然不确定。在本文中，我们提出了一种方法来推动LLMs探索开放世界，收集经验，并学习提高其任务解决能力。在这种方法中，利用多轮反馈-修订机制，鼓励LLMs根据环境的反馈信息主动选择适当的修订操作，促进探索并增强模型的性能。此外，我们整合了子任务重标记，以帮助LLMs在子任务规划中保持一致性，并帮助模型学习任务之间的组合性质，使其能够通过基于所获得的探索经验的训练完成更广泛的任务。",
    "tldr": "本文提出了一种方法，通过多轮反馈-修订机制和子任务重标记，推动大型语言模型在开放世界中探索并提高其任务解决能力。",
    "en_tdlr": "This paper proposes an approach to spur large language models to explore and improve their task-solving capabilities in an open world, utilizing a multi-round feedback-revision mechanism and sub-task relabeling."
}