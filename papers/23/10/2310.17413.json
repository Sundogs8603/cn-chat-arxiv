{
    "title": "Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])",
    "abstract": "We propose a comprehensive study of one-stage elicitation techniques for querying a large pre-trained generative transformer (GPT-3.5-turbo) in the rhetorical role prediction task of legal cases. This task is known as requiring textual context to be addressed. Our study explores strategies such as zero-few shots, task specification with definitions and clarification of annotation ambiguities, textual context and reasoning with general prompts and specific questions. We show that the number of examples, the definition of labels, the presentation of the (labelled) textual context and specific questions about this context have a positive influence on the performance of the model. Given non-equivalent test set configurations, we observed that prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier based on the BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach the pe",
    "link": "http://arxiv.org/abs/2310.17413",
    "context": "Title: Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])\nAbstract: We propose a comprehensive study of one-stage elicitation techniques for querying a large pre-trained generative transformer (GPT-3.5-turbo) in the rhetorical role prediction task of legal cases. This task is known as requiring textual context to be addressed. Our study explores strategies such as zero-few shots, task specification with definitions and clarification of annotation ambiguities, textual context and reasoning with general prompts and specific questions. We show that the number of examples, the definition of labels, the presentation of the (labelled) textual context and specific questions about this context have a positive influence on the performance of the model. Given non-equivalent test set configurations, we observed that prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier based on the BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach the pe",
    "path": "papers/23/10/2310.17413.json",
    "total_tokens": 859,
    "translated_title": "利用GPT-3.5-turbo进行法律案例修辞角色预测的研究",
    "translated_abstract": "我们提出了一个全面的研究，探讨在法律案例的修辞角色预测任务中，使用大型预训练生成式转换器GPT-3.5-turbo的一阶引导技术。该任务需要处理文本背景。我们的研究探讨了零或少样本、任务规范化与定义、厘清标注模糊性、文本背景和基于常用提示和具体问题的推理等策略。我们的实验结果表明，样本数量、标签定义、文本背景的展示以及针对该背景的具体问题对模型的性能有积极影响。在非等价的测试设置下，我们观察到使用少量来自直接背景的标记示例进行引导可以比基于BERT编码器的监督调优多分类器（加权F1得分为72%）获得更好的性能。但仍存在差距，有待改进。",
    "tldr": "该研究利用GPT-3.5-turbo对法律案例中的修辞角色进行预测，并发现使用少量标记示例进行引导可以获得更好的性能。"
}