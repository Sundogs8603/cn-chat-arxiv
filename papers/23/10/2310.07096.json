{
    "title": "Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])",
    "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in compu",
    "link": "http://arxiv.org/abs/2310.07096",
    "context": "Title: Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])\nAbstract: The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in compu",
    "path": "papers/23/10/2310.07096.json",
    "total_tokens": 919,
    "translated_title": "稀疏通用Transformer",
    "translated_abstract": "通用Transformer（UT）是Transformer的一种变体，其在各层之间共享参数。实证证据表明，在形式语言任务中，UT比Vanilla Transformer（VT）具有更好的组合泛化能力。参数共享还使其具有比VT更好的参数效率。尽管具有许多优点，但扩展UT参数比扩展VT更需要计算和内存资源。本文提出了稀疏通用Transformer（SUT），它利用稀疏混合专家（SMoE）和基于切棍法的动态停止机制来减少UT的计算复杂性，同时保持其参数效率和泛化能力。实验表明，SUT在WMT'14上仅使用一半的计算资源和参数就能达到与强基线模型相同的性能，并在形式语言任务（逻辑推理和CFQ）上具有强大的泛化性能。新的停止机制还能使计算资源减少约50％。",
    "tldr": "本文介绍了稀疏通用Transformer（SUT），它通过利用稀疏混合专家（SMoE）和一种新的基于切棍法的动态停止机制来减少计算复杂性，同时保持参数效率和泛化能力。实验证明，SUT在形式语言任务上具有与强基线模型相当的性能，并能显著降低计算资源使用。"
}