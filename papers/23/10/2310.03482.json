{
    "title": "The Geometric Structure of Fully-Connected ReLU-Layers. (arXiv:2310.03482v1 [cs.LG])",
    "abstract": "We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such ",
    "link": "http://arxiv.org/abs/2310.03482",
    "context": "Title: The Geometric Structure of Fully-Connected ReLU-Layers. (arXiv:2310.03482v1 [cs.LG])\nAbstract: We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such ",
    "path": "papers/23/10/2310.03482.json",
    "total_tokens": 1055,
    "translated_title": "完全连接的ReLU层的几何结构",
    "translated_abstract": "我们对神经网络中d维完全连接的ReLU层的几何结构进行了形式化和解释。ReLU层的参数会引导输入域的自然划分，使得在划分的每个区域内，ReLU层可以被大大简化。这导致了将ReLU层解释为一个投影到多面体锥体，然后进行仿射变换的几何解释，与在具有ReLU激活的卷积网络中的描述一致。此外，这种结构便于简化分区区域与超平面交集的反像表达式，这在描述分类问题中的决策边界时非常有用。我们详细研究了具有一个隐藏ReLU层的前馈网络，在这个网络中，我们提供了关于这些网络生成的决策边界的几何复杂性的结果，同时证明在仿射变换的模下，这些决策边界相等。",
    "tldr": "该论文研究了神经网络中完全连接的ReLU层的几何结构。研究发现，在每个划分区域内，ReLU层可以被大大简化，可以将其解释为一个投影到多面体锥体，然后进行仿射变换。此结构还简化了分区区域与超平面交集的反像表达式，对于描述分类问题中的决策边界非常有用。此外，对于具有一个隐藏ReLU层的前馈网络，论文提供了关于这些网络生成的决策边界几何复杂性的结果，并证明了这些决策边界在仿射变换的模下是相等的。",
    "en_tdlr": "This paper investigates the geometric structure of fully-connected ReLU-layers in neural networks and finds that they can be simplified by partitioning the input domain. The ReLU-layer is interpreted as a projection onto a polyhedral cone followed by an affine transformation. This structure also simplifies the expressions for the intersection of partition sectors and hyperplanes, which is useful for describing decision boundaries in classification settings. The paper provides results on the geometric complexity of decision boundaries generated by feed-forward networks with one hidden ReLU-layer and proves their equivalence modulo an affine transformation."
}