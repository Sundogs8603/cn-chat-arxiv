{
    "title": "Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions. (arXiv:2310.16076v1 [cs.LG])",
    "abstract": "Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.",
    "link": "http://arxiv.org/abs/2310.16076",
    "context": "Title: Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions. (arXiv:2310.16076v1 [cs.LG])\nAbstract: Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.",
    "path": "papers/23/10/2310.16076.json",
    "total_tokens": 947,
    "translated_title": "线性变换器及其循环和自指扩展的实用计算能力",
    "translated_abstract": "最近关于循环神经网络（RNN）计算能力的研究揭示了一种给定实时和有限精度假设的RNN体系结构的层次结构。在这里，我们研究了具有线性化注意力的自回归变换器，也称为线性变换器（LT）或快速权重程序员（FWP）。LT在特定意义上是特殊的，因为它们等同于具有固定大小状态的类似RNN的序列处理器，同时也可以被表示为现在流行的自注意力网络。我们展示了许多关于标准变换器的知名结果如何直接转移到LTs/FWPs。我们的形式语言识别实验演示了最近提出的FWP扩展，如循环FWPs和自引用权重矩阵，如何成功克服LT的某些限制，例如在奇偶问题上的泛化。我们的代码是公开的。",
    "tldr": "线性变换器（LTs）或快速权重程序员（FWPs）是一种特殊的序列处理器，可以用于循环神经网络以及自注意力网络。研究发现，类似于标准变换器的很多结果也适用于LTs/FWPs。循环FWP和自引用权重矩阵的扩展成功地克服了LT的一些限制，例如在奇偶问题上的泛化。",
    "en_tdlr": "Linear Transformers (LTs) or Fast Weight Programmers (FWPs) are special sequence processors that can be used for recurrent neural networks as well as self-attention networks. Many results for standard Transformers also apply to LTs/FWPs. The extensions of recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of LT, such as generalization on the parity problem."
}