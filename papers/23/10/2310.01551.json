{
    "title": "Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])",
    "abstract": "We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\\sl greediness hierarchy theorem} showing that for every $k \\in \\mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\\varepsilon$, whereas the latter only achieves accuracy $\\frac1{2}+\\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent \"optimal decis",
    "link": "http://arxiv.org/abs/2310.01551",
    "context": "Title: Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])\nAbstract: We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\\sl greediness hierarchy theorem} showing that for every $k \\in \\mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\\varepsilon$, whereas the latter only achieves accuracy $\\frac1{2}+\\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent \"optimal decis",
    "path": "papers/23/10/2310.01551.json",
    "total_tokens": 954,
    "translated_title": "利用选择的能力优化决策树学习",
    "translated_abstract": "我们提出了一种简单的对标准和经验成功的决策树学习算法（如ID3、C4.5和CART）进行推广的方法。这些算法凭借贪婪的特性，在机器学习领域发挥了关键作用：它们通过迭代地基于最佳属性进行划分来构建决策树。我们的算法Top-$k$则考虑$k$个最佳属性作为可能的划分，而不仅仅是单个最佳属性。我们通过理论和实证研究展示了这个简单推广方法的优势。首先，我们证明了一个“贪婪层次定理”，对于每个$k \\in \\mathbb{N}$，Top-$(k+1)$比Top-$k$更加强大：在某些数据分布下，前者可以达到$1-\\varepsilon$的准确率，而后者只能达到$\\frac1{2}+\\varepsilon$的准确率。然后，我们通过大量实验表明，Top-$k$算法在决策树学习中优于两种主要方法：经典贪婪算法和较新的“最优决策树”算法。",
    "tldr": "该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。",
    "en_tdlr": "This study proposes a decision tree learning algorithm, Top-$k$, that harnesses the power of choices. It outperforms traditional greedy algorithms and optimal decision tree algorithms in terms of accuracy and performance."
}