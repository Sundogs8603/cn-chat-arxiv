{
    "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation. (arXiv:2310.16964v1 [cs.CL])",
    "abstract": "Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special \"text critic\" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benc",
    "link": "http://arxiv.org/abs/2310.16964",
    "context": "Title: Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation. (arXiv:2310.16964v1 [cs.CL])\nAbstract: Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special \"text critic\" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benc",
    "path": "papers/23/10/2310.16964.json",
    "total_tokens": 921,
    "translated_title": "批评驱动解码以减轻数据到文本生成中的幻觉问题",
    "translated_abstract": "文本在输入中无法得到实际支持的幻觉是神经数据到文本生成中已知的问题。虽然已经有许多方法被提出来减轻这个问题，但它们通常需要修改模型架构或收集额外的数据，因此不能轻易地应用到现有模型中。在本文中，我们探索了一种通过组合生成语言模型（LM）的概率输出与特殊的“文本批评家”分类器的输出来减轻幻觉的新方法，后者通过评估输入数据与到目前为止生成的文本之间的匹配来引导生成过程。我们的方法不需要对基础LM的架构或训练过程进行任何改动，因此可以与任何基于单词概率进行模型和解码操作的模型结合使用。批评家不需要额外的训练数据，使用基础LM的训练数据和合成的负面例子。我们的实验结果表明，我们的方法在WebNLG和OpenDialKG基准上较基线有所改善。",
    "tldr": "本文提出了一种新的方法来减轻神经数据到文本生成中的幻觉问题，通过组合生成语言模型和特殊的文本批评家分类器的输出来指导生成过程。方法不需要对模型架构或训练过程进行改动，可与任何基于单词概率进行模型和解码操作的模型结合使用，并且不需要额外的训练数据。实验证明该方法在基准测试上优于基线。"
}