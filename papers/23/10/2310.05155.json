{
    "title": "Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model",
    "abstract": "arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th",
    "link": "https://arxiv.org/abs/2310.05155",
    "context": "Title: Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model\nAbstract: arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th",
    "path": "papers/23/10/2310.05155.json",
    "total_tokens": 898,
    "translated_title": "Toolink: 链接工具包创建和使用的链式解决开源模型",
    "translated_abstract": "大语言模型（LLMs）在利用工具方面取得了显著进展，但其闭源性和高推理成本对其适应性造成了限制，需要一种有效的方法，利用较小的开源模型。在本文中，我们介绍了Toolink，这是一个全面的框架，通过链式解决（CoS）方法首先创建工具包，然后集成工具的规划和调用。我们首先验证了Toolink在利用模型的创造力和CoS能力方面的有效性。之后，我们策划了CoS-GPT，一个专为工具使用而设计的链式解决数据集，并对LLaMA-7B模型进行了微调。结果是LLaMA-CoS，一个具有先进工具规划和工具调用能力的强大开源模型。通过对BIG-bench的多样任务进行评估，表明其CoS能力与ChatGPT相匹配，而性能超过了其",
    "tldr": "提出了Toolink，一个通过链式解决方法首先创建工具包，再集成工具规划和调用的框架，成功通过对ChatGPT和CoS-GPT的实验，打造了LLaMA-CoS，一个具有先进工具规划和调用能力的强大开源模型。",
    "en_tdlr": "Introduced Toolink, a framework that creates toolkits through a chain-of-solving approach, successfully demonstrated with experiments on ChatGPT and CoS-GPT to develop LLaMA-CoS, a powerful open-source model with advanced tool-planning and calling capabilities."
}