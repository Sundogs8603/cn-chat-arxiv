{
    "title": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone. (arXiv:2310.19859v1 [cs.CV])",
    "abstract": "Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated o",
    "link": "http://arxiv.org/abs/2310.19859",
    "context": "Title: Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone. (arXiv:2310.19859v1 [cs.CV])\nAbstract: Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated o",
    "path": "papers/23/10/2310.19859.json",
    "total_tokens": 894,
    "translated_title": "Res-Tuning: 通过解绑与主干的调节器来实现灵活高效的调节范式",
    "translated_abstract": "高效参数调节已成为将大规模基础模型转移到下游应用的趋势。现有方法通常将一些轻量级调节器嵌入主干中，调节器的设计和学习都高度依赖于基础模型。本研究提出了一种名为Res-Tuning的新的调节范式，它有意将调节器从主干中解绑。通过理论和实证证据，我们展示了流行的调节方法在我们的解绑公式下拥有等效的对应物，并因此可以轻松地集成到我们的框架中。由于结构解离，我们可以自由设计调节器而不受网络架构的限制，从而方便地组合各种调节策略。我们进一步提出了一种内存高效的Res-Tuning变体，其中绕过（由一系列调节器形成）有效地从主支分离，从而实现了梯度的反向传播。",
    "tldr": "Res-Tuning是一种新的调节范式，通过解绑调节器与主干模型，实现了灵活高效的调节。这种结构解离使得调节器的设计与网络架构无关，方便了各种调节策略的灵活组合。"
}