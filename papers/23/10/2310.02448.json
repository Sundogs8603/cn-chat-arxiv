{
    "title": "Feather: An Elegant Solution to Effective DNN Sparsification. (arXiv:2310.02448v1 [cs.LG])",
    "abstract": "Neural Network pruning is an increasingly popular way for producing compact and efficient models, suitable for resource-limited environments, while preserving high performance. While the pruning can be performed using a multi-cycle training and fine-tuning process, the recent trend is to encompass the sparsification process during the standard course of training. To this end, we introduce Feather, an efficient sparse training module utilizing the powerful Straight-Through Estimator as its core, coupled with a new thresholding operator and a gradient scaling technique, enabling robust, out-of-the-box sparsification performance. Feather's effectiveness and adaptability is demonstrated using various architectures on the CIFAR dataset, while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using the ResNet-50 architecture, surpassing existing methods, including more complex and computationally heavy ones, by a considerable margin. Code is publicly available at https://git",
    "link": "http://arxiv.org/abs/2310.02448",
    "context": "Title: Feather: An Elegant Solution to Effective DNN Sparsification. (arXiv:2310.02448v1 [cs.LG])\nAbstract: Neural Network pruning is an increasingly popular way for producing compact and efficient models, suitable for resource-limited environments, while preserving high performance. While the pruning can be performed using a multi-cycle training and fine-tuning process, the recent trend is to encompass the sparsification process during the standard course of training. To this end, we introduce Feather, an efficient sparse training module utilizing the powerful Straight-Through Estimator as its core, coupled with a new thresholding operator and a gradient scaling technique, enabling robust, out-of-the-box sparsification performance. Feather's effectiveness and adaptability is demonstrated using various architectures on the CIFAR dataset, while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using the ResNet-50 architecture, surpassing existing methods, including more complex and computationally heavy ones, by a considerable margin. Code is publicly available at https://git",
    "path": "papers/23/10/2310.02448.json",
    "total_tokens": 976,
    "translated_title": "Feather:一种优雅的解决DNN稀疏化问题的解决方案",
    "translated_abstract": "神经网络剪枝是一种越来越流行的方法，可以生成适用于资源有限环境并保持高性能的紧凑高效的模型。虽然剪枝可以通过多周期训练和微调的过程来执行，但最近的趋势是在标准训练过程中同时包含稀疏化过程。为此，我们引入了Feather，一种高效的稀疏训练模块，其核心是强大的直通过估计器，配合一个新的阈值算子和梯度缩放技术，实现了强大的开箱即用的稀疏化性能。在CIFAR数据集上，我们使用不同的架构证明了Feather的有效性和适应性，而在ImageNet上，它使用ResNet-50架构实现了最新的Top-1验证准确率，超过了现有的方法，包括更复杂、计算量更大的方法，差距很大。代码公开在 https://git...",
    "tldr": "Feather是一种优雅的DNN稀疏化解决方案，它具有高效的稀疏训练模块和强大的直通过估计器核心，能够在标准训练过程中实现鲁棒的稀疏化性能，并在CIFAR数据集上展示了其有效性和适应性，在ImageNet上使用ResNet-50架构实现了最新的最佳验证准确率，超越了现有方法。",
    "en_tdlr": "Feather is an elegant solution to effective DNN sparsification, featuring an efficient sparse training module and a powerful Straight-Through Estimator core, enabling robust sparsification performance during standard training, and demonstrating its effectiveness and adaptability on the CIFAR dataset, achieving state-of-the-art Top-1 validation accuracy on ImageNet using the ResNet-50 architecture, surpassing existing methods."
}