{
    "title": "Rollout Heuristics for Online Stochastic Contingent Planning. (arXiv:2310.02345v1 [cs.AI])",
    "abstract": "Partially observable Markov decision processes (POMDP) are a useful model for decision-making under partial observability and stochastic actions. Partially Observable Monte-Carlo Planning is an online algorithm for deciding on the next action to perform, using a Monte-Carlo tree search approach, based on the UCT (UCB applied to trees) algorithm for fully observable Markov-decision processes. POMCP develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. As such, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Thus, many practitioners who use POMCP are required to create strong, domain-specific heuristics.  In this paper, we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from",
    "link": "http://arxiv.org/abs/2310.02345",
    "context": "Title: Rollout Heuristics for Online Stochastic Contingent Planning. (arXiv:2310.02345v1 [cs.AI])\nAbstract: Partially observable Markov decision processes (POMDP) are a useful model for decision-making under partial observability and stochastic actions. Partially Observable Monte-Carlo Planning is an online algorithm for deciding on the next action to perform, using a Monte-Carlo tree search approach, based on the UCT (UCB applied to trees) algorithm for fully observable Markov-decision processes. POMCP develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. As such, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Thus, many practitioners who use POMCP are required to create strong, domain-specific heuristics.  In this paper, we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from",
    "path": "papers/23/10/2310.02345.json",
    "total_tokens": 939,
    "translated_title": "在线随机即时规划的展开启发式方法",
    "translated_abstract": "部分可观察马尔可夫决策过程（POMDP）是一种在部分可观测性和随机动作下进行决策的有用模型。部分可观察蒙特卡洛规划（POMCP）是一种在线算法，用于决定下一步要执行的动作，使用了一种基于UCB（应用于树）算法的蒙特卡洛树搜索方法，该算法是针对完全可观测马尔可夫决策过程的。POMCP生成一个动作-观察树，在叶子节点使用展开策略为叶子节点提供估计值。因此，POMCP高度依赖展开策略来计算良好的估计值，并因此确定良好的动作。因此，许多使用POMCP的实践者需要创建强大的领域特定的启发式策略。在本文中，我们将POMDP建模为随机依赖规划问题。这使我们能够利用在规划社区中开发的领域无关的启发式算法。我们提出了两种启发式方法，第一种基于众所周知的h_add启发式算法。",
    "tldr": "本文将POMDP建模为随机依赖规划问题，并提出了两个领域无关的启发式方法，以解决POMCP高度依赖展开策略的问题。",
    "en_tdlr": "This paper models POMDPs as stochastic contingent planning problems and proposes two domain-independent heuristics to address the issue of high reliance on rollout policies in POMCP."
}