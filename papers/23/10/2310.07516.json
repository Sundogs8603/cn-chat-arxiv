{
    "title": "Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining. (arXiv:2310.07516v1 [cs.CY])",
    "abstract": "Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitud",
    "link": "http://arxiv.org/abs/2310.07516",
    "context": "Title: Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining. (arXiv:2310.07516v1 [cs.CY])\nAbstract: Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitud",
    "path": "papers/23/10/2310.07516.json",
    "total_tokens": 946,
    "translated_title": "跨层级计算的能量估计：从设备到机器学习在自然语言处理、科学计算和加密货币挖掘中的大规模应用",
    "translated_abstract": "对从设备到算法的计算层级中的能量消耗进行了估计和分析。在之前的分析基础上[3]，估计了包括人工智能（AI）/机器学习在自然语言处理、科学模拟和加密货币挖掘等三个大规模计算应用中所需的能量。与比特级切换不同，在应用的指令和模拟层面上都消耗了更多的能量。此外，基于AI/ML加速器的分析表明，使用较旧的半导体技术节点的架构与使用较新技术的不同架构具有可比较的能量效率。进一步将计算系统的能量与热力学和生物学界限进行比较，表明大约有27-36个数量级的差距。",
    "tldr": "该论文估计和分析了从设备到算法的计算层级中的能量消耗。在三个大规模计算应用中，能量消耗既存在于指令层面，也存在于模拟层面。此外，对使用不同架构的AI/ML加速器进行了能量效率比较。与热力学和生物学界限相比，计算系统的能量消耗相差27-36个数量级。"
}