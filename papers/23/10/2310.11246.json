{
    "title": "Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs. (arXiv:2310.11246v1 [cs.AI])",
    "abstract": "Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our",
    "link": "http://arxiv.org/abs/2310.11246",
    "context": "Title: Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs. (arXiv:2310.11246v1 [cs.AI])\nAbstract: Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our",
    "path": "papers/23/10/2310.11246.json",
    "total_tokens": 918,
    "translated_title": "Query2Triple: 统一查询编码以回答知识图谱上多样复杂查询的挑战",
    "translated_abstract": "复杂查询回答（CQA）是知识图谱（KG）的一项挑战任务。由于KG的不完整性，已经提出了查询嵌入（QE）方法，将查询和实体编码到相同的嵌入空间中，并将逻辑运算符视为神经集合运算符，以获得答案。然而，这些方法在同时对简单（一跳）和复杂（多跳和逻辑）查询进行训练时，会导致简单查询性能的下降和训练效率低下。在本文中，我们提出了Query to Triple（Q2T），一种新颖的方法，将简单和复杂查询的训练解耦。Q2T将训练分为两个阶段：（1）在简单查询上预训练神经链接预测器，以基于头实体和关系预测尾实体。（2）在复杂查询上训练查询编码器，将多样的复杂查询编码为统一的三元组形式，可以通过预训练的神经链接预测器高效地解决。",
    "tldr": "Query2Triple提出了一种新的方法，将简单查询和复杂查询的训练解耦，通过预训练神经链接预测器来编码和回答复杂查询，提高了性能和训练效率。",
    "en_tdlr": "Query2Triple proposes a novel approach that decouples the training for simple and complex queries, encoding and answering complex queries using a pretrained neural link predictor, improving performance and training efficiency."
}