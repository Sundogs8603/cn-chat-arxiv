{
    "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])",
    "abstract": "The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broad-coverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demon",
    "link": "http://arxiv.org/abs/2310.15326",
    "context": "Title: Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])\nAbstract: The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broad-coverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demon",
    "path": "papers/23/10/2310.15326.json",
    "total_tokens": 885,
    "translated_title": "专家还是通才？针对特定NLP任务的指导调优方法",
    "translated_abstract": "大型语言模型（LLMs）在同时执行多种自然语言处理（NLP）任务方面的潜力已经成为广泛研究的主题。虽然指导调优已被证明是将LLMs转化为通才模型的一种高效方法，但它们的性能仍然落后于专家模型，专家模型专门为特定任务进行训练。本文研究了将广覆盖的通才模型指导调优融入到专家模型中是否有助于构建专家模型。我们假设其效果取决于任务的特异性和技能要求。我们的实验评估了四个具有不同覆盖范围的目标任务，发现当任务覆盖广泛时，整合通才模型的指导调优能够持续提高模型性能。而当任务特定的训练数据有限时，这种效果尤为显著。进一步探究针对不同能力的三个目标任务。",
    "tldr": "本文研究了将通才模型指导调优融入到专家模型中是否有助于构建专家模型，并发现在任务覆盖广泛且任务特定的训练数据有限时，整合通才模型的指导调优能够持续提高模型性能。",
    "en_tdlr": "This paper investigates whether incorporating generalist instruction tuning into specialist models can help build better specialist models. The experiments reveal that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad, particularly when there is limited task-specific training data."
}