{
    "title": "Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v1 [cs.CL])",
    "abstract": "Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap -- which distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993). Our analysis reveals that the disparity in information between dialogue contexts and desired inferences poses a significant challenge to the inductive inference process. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.",
    "link": "http://arxiv.org/abs/2310.12467",
    "context": "Title: Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v1 [cs.CL])\nAbstract: Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap -- which distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993). Our analysis reveals that the disparity in information between dialogue contexts and desired inferences poses a significant challenge to the inductive inference process. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.",
    "path": "papers/23/10/2310.12467.json",
    "total_tokens": 852,
    "translated_title": "对话中的对比学习推理",
    "translated_abstract": "推理,尤其是那些来自归纳过程的推理,是我们对话中的一个关键组成部分，用于补充由讲话者隐含或明确传达的信息。虽然最近的大型语言模型在推理任务上取得了显著进展，但它们在归纳推理方面的表现远远落后于演绎推理。在本文中，我们根据语义信息差异来定义任务难度，分析了模型的行为，该差异区分了归纳推理和演绎推理（Johnson-Laird, 1988, 1993）。我们的分析揭示了对话上下文和所需推理之间信息差异的差距对归纳推理过程构成了重要挑战。为了缓解这种信息差距，我们研究了一种对比学习方法，通过提供负样本进行训练。我们的实验表明，负样本有助于模型理解错误并改进其推理生成能力。",
    "tldr": "本论文分析了推理任务中的信息差异对模型的影响，并提出了一种对比学习方法来缓解这种信息差异。实验证明，负样本有助于模型改进其推理生成能力。",
    "en_tdlr": "This paper analyzes the impact of information gap in inference tasks on models and proposes a contrastive learning method to mitigate this gap. The experiments demonstrate that negative samples help models improve their inference generation capabilities."
}