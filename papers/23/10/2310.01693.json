{
    "title": "Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])",
    "abstract": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts ",
    "link": "http://arxiv.org/abs/2310.01693",
    "context": "Title: Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])\nAbstract: Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts ",
    "path": "papers/23/10/2310.01693.json",
    "total_tokens": 817,
    "translated_title": "解开神经文本退化之谜",
    "translated_abstract": "尽管在语言生成中普遍使用，但为何像核采样这样的截断采样启发式方法如此有效仍然不为人所知。我们通过证明截断方法（丢弃某些概率阈值以下的记号，最常见的截断类型）可以保证所有采样出来的记号都有非零真实概率，提供了对截断采样方法有效性的理论解释。然而，这些阈值只是粗略的启发式方法，必然也丢弃了一些具有非零真实概率的记号。为了追求更精确的采样策略，我们展示了如何利用已知的模型错误源——softmax瓶颈，证明某些记号具有非零真实概率，而不依赖于阈值。基于我们的发现，我们开发了一种实验性的截断策略，并进行了展示这种算法的前期研究。我们的评估结果表明我们的方法优于基于阈值的对应方法。",
    "tldr": "本研究解释了截断采样方法的有效性，并提出了一种基于softmax瓶颈的更精确的采样策略。"
}