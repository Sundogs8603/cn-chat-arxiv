{
    "title": "In-Context Convergence of Transformers. (arXiv:2310.05249v1 [cs.LG])",
    "abstract": "Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map",
    "link": "http://arxiv.org/abs/2310.05249",
    "context": "Title: In-Context Convergence of Transformers. (arXiv:2310.05249v1 [cs.LG])\nAbstract: Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map",
    "path": "papers/23/10/2310.05249.json",
    "total_tokens": 860,
    "translated_title": "Transformers内部的收敛性研究",
    "translated_abstract": "近期，Transformers在现代机器学习的许多领域中取得了巨大的革命性进展，其中一个显著的发现是它们在上下文学习方面的出色能力，通过利用特定任务的提示而无需参数微调，模型可以解决从未见过的任务。这也启发了最近的理论研究，目标是理解Transformers在上下文学习中的机制，然而这些研究仅关注线性Transformer。本文通过梯度下降训练一层Transformer的softmax attention机制，首次在上下文学习线性函数类方面研究学习动力学。我们考虑了一个结构化数据模型，其中每个标记从一组平衡或不平衡的特征向量中随机采样。对于具有平衡特征的数据，我们通过在训练动力学的两个阶段上进行分析，建立了有限时间收敛保证，且预测误差接近零。",
    "tldr": "本研究通过对Transformers内部学习动力学的分析，研究了一层Transformer使用梯度下降进行上下文学习的能力，对于具有平衡特征的数据，建立了有限时间收敛保证，且预测误差接近零。",
    "en_tdlr": "This study investigates the learning dynamics of a one-layer Transformer using gradient descent for in-context learning. It establishes a finite-time convergence guarantee with near-zero prediction error for data with balanced features."
}