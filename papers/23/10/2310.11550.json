{
    "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])",
    "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{4}{5}}+poly\\left(\\frac{1}{\\lambda_{\\min}}\\right) \\right)$ regret, for some problem-dependent constant $\\lam",
    "link": "http://arxiv.org/abs/2310.11550",
    "context": "Title: Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])\nAbstract: We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{4}{5}}+poly\\left(\\frac{1}{\\lambda_{\\min}}\\right) \\right)$ regret, for some problem-dependent constant $\\lam",
    "path": "papers/23/10/2310.11550.json",
    "total_tokens": 1068,
    "translated_title": "面向带有强对抗损失和强盗反馈的对抗性线性MDPs的最优遗憾",
    "translated_abstract": "我们研究了在线强化学习中的线性马尔可夫决策过程，并考虑了对抗性损失和强盗反馈，没有事先了解转换或访问模拟器的先验知识。我们引入了两种算法，相较于现有方法，它们都能取得更好的遗憾性能。第一种算法虽然计算效率低，但能保证$\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$的遗憾性能，其中$K$是回合数。这是该设置下第一个具有最佳$K$依赖性的结果。第二种算法基于策略优化框架，能保证$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$的遗憾性能，并且计算效率高。我们的两个结果都显著改善了现有最先进方法：Kong等人[2023]的计算效率低的算法，其遗憾性能为$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{4}{5}}+poly\\left(\\frac{1}{\\lambda_{\\min}}\\right) \\right)$，其中$\\lambda_{\\min}$是问题相关常数。",
    "tldr": "研究带有对抗损失和强盗反馈的线性MDPs问题，提出了两种算法，分别达到了$\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$和$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$的遗憾性能。",
    "en_tdlr": "This paper presents two algorithms for online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, achieving regret performance of $\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$ and $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$ respectively, improving upon existing approaches."
}