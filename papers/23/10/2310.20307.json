{
    "title": "Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])",
    "abstract": "We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:",
    "link": "http://arxiv.org/abs/2310.20307",
    "context": "Title: Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])\nAbstract: We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:",
    "path": "papers/23/10/2310.20307.json",
    "total_tokens": 842,
    "translated_title": "自训练Transformer中自注意力的因果解释",
    "translated_abstract": "我们提出了一种对Transformer神经网络架构中自注意力进行因果解释的方法。我们将自注意力解释为一种估计给定输入符号序列的结构方程模型的机制。结构方程模型可以解释为在特定上下文中输入序列上的因果结构。重要的是，在存在潜在混淆变量的情况下，这种解释仍然有效。根据这种解释，我们通过计算最深注意层中相应表示之间的偏相关来估计输入符号之间的条件独立关系。这使得可以使用现有的基于约束的算法学习输入序列上的因果结构。从这个意义上讲，现有的预训练Transformer可以用于零样本因果发现。我们通过为两个任务中Transformer的结果提供因果解释来示范这种方法。",
    "tldr": "本研究提出了一种对自注意力进行因果解释的方法，并利用已有的预训练Transformer进行零样本因果发现。通过计算最深注意层中相应表示之间的偏相关，我们可以学习输入序列上的因果结构。该方法在两个任务中为Transformer的结果提供了因果解释。",
    "en_tdlr": "This study proposes a causal interpretation of self-attention in pre-trained Transformers, enabling zero-shot causal discovery. By calculating partial correlations between corresponding representations in the deepest attention layer, the causal structure over an input sequence can be learned. This approach provides causal explanations for the outcomes of Transformers in two tasks."
}