{
    "title": "Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning. (arXiv:2310.07558v1 [stat.ML])",
    "abstract": "We study the dynamic pricing problem where the demand function is nonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown H\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to achieve a minimax optimal regret of $\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves t",
    "link": "http://arxiv.org/abs/2310.07558",
    "context": "Title: Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning. (arXiv:2310.07558v1 [stat.ML])\nAbstract: We study the dynamic pricing problem where the demand function is nonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown H\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to achieve a minimax optimal regret of $\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves t",
    "path": "papers/23/10/2310.07558.json",
    "total_tokens": 901,
    "translated_title": "具有非参数需求学习的平滑自适应动态定价",
    "translated_abstract": "我们研究了需求函数为非参数和Holder平滑的动态定价问题，并且我们专注于适应未知的Holder平滑参数β的能力。传统上，最优的动态定价算法严重依赖于对β的了解，以达到一个最小化极限遗憾的效果，即O(T^((β+1)/(2β+1)))。然而，我们通过证明没有定价策略能够在不知道β的情况下自适应地达到这个最小化极限遗憾，突显了这个动态定价问题的适应性挑战。受到不可能性结果的启发，我们提出了一种自相似条件来实现适应性。重要的是，我们证明了自相似条件不会损害问题本身的复杂性，因为它保持了渐近遗憾下界Ω(T^((β+1)/(2β+1)))。此外，我们开发了一种平滑自适应的动态定价算法，并理论上证明了该算法的有效性。",
    "tldr": "这项研究提出了一种具有非参数需求学习和平滑自适应的动态定价算法，通过使用自相似条件实现了最小化极限遗憾。",
    "en_tdlr": "This research proposes a dynamic pricing algorithm with nonparametric demand learning and smoothness adaptivity, achieving minimization of regret by using a self-similarity condition."
}