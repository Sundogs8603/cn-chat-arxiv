{
    "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our ",
    "link": "http://arxiv.org/abs/2310.20150",
    "context": "Title: Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])\nAbstract: Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our ",
    "path": "papers/23/10/2310.20150.json",
    "total_tokens": 894,
    "translated_title": "忘记你想忘记的：LLMs的高效遗忘方法",
    "translated_abstract": "大语言模型（LLMs）通过预训练和记忆各种文本数据取得了重大进展，但这个过程可能面临隐私问题和数据保护规定的违规。因此，在不损害预测质量的情况下，能够轻松地从这些模型中删除与个人用户相关的数据变得越来越重要。为解决这些问题，本文提出了一种高效的遗忘框架，通过引入学习有选择的师生目标的轻量级遗忘层到transformers中，能够在数据删除后有效地更新LLMs，而无需对整个模型进行重新训练。此外，我们还引入了一种融合机制，以有效地组合不同的遗忘层，以处理一系列的遗忘操作。分类和生成任务的实验证明了我们方法的有效性。",
    "tldr": "本论文提出了一种高效的遗忘框架来处理大语言模型（LLMs）中的隐私问题和数据保护违规。通过引入轻量级遗忘层到transformers中，并使用有选择的师生目标学习，我们能够在删除数据后有效地更新LLMs，而无需重新训练整个模型。实验证明了该方法的有效性。"
}