{
    "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. (arXiv:2310.12109v1 [cs.LG])",
    "abstract": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters",
    "link": "http://arxiv.org/abs/2310.12109",
    "context": "Title: Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. (arXiv:2310.12109v1 [cs.LG])\nAbstract: Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters",
    "path": "papers/23/10/2310.12109.json",
    "total_tokens": 871,
    "translated_title": "Monarch Mixer: 一个基于子二次广义矩阵相乘的简单架构",
    "translated_abstract": "机器学习模型在序列长度和模型维度上的扩展越来越普遍，以达到更长的上下文和更好的性能。然而，现有的架构如Transformer在这两个方面的扩展都是二次的。我们问：是否有一种性能良好的架构可以在序列长度和模型维度上具有子二次的扩展性？我们引入了Monarch Mixer (M2)，一种使用相同子二次原语的新架构，该原语是一种简单而具有表达力的结构化矩阵，可以捕捉许多线性变换，在GPU上具有高硬件效率，并且具有子二次的扩展性。作为概念验证，我们在三个领域探索了M2的性能：非因果BERT模式的语言建模，ViT模式的图像分类和因果GPT模式的语言建模。对于非因果BERT模式的建模，M2在GLUE质量上与BERT-base和BERT-large相匹配，参数数量减少了多达27%",
    "tldr": "Monarch Mixer (M2) is a new architecture that uses sub-quadratic primitive to scale along both sequence length and model dimension, achieving high hardware efficiency and matching the performance of existing models with fewer parameters."
}