{
    "title": "Learning Optimal Advantage from Preferences and Mistaking it for Reward. (arXiv:2310.02456v1 [cs.LG])",
    "abstract": "We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\\hat{A^*_r}$. From th",
    "link": "http://arxiv.org/abs/2310.02456",
    "context": "Title: Learning Optimal Advantage from Preferences and Mistaking it for Reward. (arXiv:2310.02456v1 [cs.LG])\nAbstract: We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\\hat{A^*_r}$. From th",
    "path": "papers/23/10/2310.02456.json",
    "total_tokens": 878,
    "translated_title": "从偏好中学习最佳优势，并将其误解为奖励",
    "translated_abstract": "我们考虑从人类对轨迹片段对的偏好中学习奖励函数的算法，这在从人类反馈中进行强化学习（RLHF）中使用。最近的工作假设人类偏好仅基于这些片段中积累的奖励或其部分回报。最近的工作对这一假设的有效性提出了怀疑，并提出了一种基于遗憾的替代偏好模型。我们研究了当假设偏好是基于部分回报而实际上来自遗憾时的后果。我们认为学到的函数是最佳优势函数$\\hat{A^*_r}$的近似，而不是奖励函数。我们发现，如果解决了特定的陷阱，这种错误假设并不特别有害，结果是一个高度变形的奖励函数。尽管如此，这种错误使用$\\hat{A^*_r}$的方式不如适当且更简单的方法——贪婪最大化$\\hat{A^*_r}$。",
    "tldr": "本文研究了从人类偏好中学习奖励函数的算法，并发现实际上学到的是最佳优势函数而不是奖励函数。这种错误的使用方式虽然不特别有害，但与正确的贪婪最大化最佳优势函数相比仍不够理想。"
}