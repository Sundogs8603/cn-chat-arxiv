{
    "title": "Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])",
    "abstract": "Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.",
    "link": "http://arxiv.org/abs/2310.11079",
    "context": "Title: Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])\nAbstract: Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.",
    "path": "papers/23/10/2310.11079.json",
    "total_tokens": 940,
    "translated_title": "从红队操作中学习：大型语言模型中的性别偏见挑衅和缓解",
    "translated_abstract": "最近，随着ChatGPT和GPT-4等大型语言模型（LLM）的进展，研究人员在对话系统方面取得了可观的改进。这些基于LLM的聊天机器人在保留可能伤害人类的不平等的同时，编码了潜在的偏见。传统的偏见调查方法通常依赖于人工编写的测试用例。然而，这些测试用例通常成本高昂且有限。在这项工作中，我们提出了一种独创的方法，自动生成用于检测LLMs潜在性别偏见的测试用例。我们将该方法应用于三种著名的LLM，并发现生成的测试用例有效地识别出了偏见的存在。针对所发现的偏见，我们提出了一种缓解策略，利用生成的测试用例作为上下文学习的演示，来规避参数微调的需要。实验结果显示，LLMs通过这种提议的方法生成了更公平的响应。",
    "tldr": "这项研究提出了一种自动生成测试用例以检测大型语言模型（LLMs）中潜在性别偏见的方法，并提出了使用生成的测试用例作为上下文学习的演示来缓解偏见的存在。实验证明，采用该方法后，LLMs能够生成更公平的响应。",
    "en_tdlr": "This study presents a method to automatically generate test cases to detect potential gender biases in large language models (LLMs), and proposes a mitigation strategy that utilizes the generated test cases as demonstrations for in-context learning to address the biases. Experimental results demonstrate that LLMs generate fairer responses with this approach."
}