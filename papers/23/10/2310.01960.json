{
    "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])",
    "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prom",
    "link": "http://arxiv.org/abs/2310.01960",
    "context": "Title: Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])\nAbstract: Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prom",
    "path": "papers/23/10/2310.01960.json",
    "total_tokens": 897,
    "translated_title": "用语言模型作为视觉词义消歧的知识库",
    "translated_abstract": "视觉词义消歧（VWSD）是一项新颖且具有挑战性的任务，位于语义消歧和细粒度多模式检索之间。最近发展的视觉语言模型（VL transformers）的进展表明，一些现成的实现具有令人鼓舞的结果，但我们认为还可以进一步改进。为此，我们提出了一些增强知识的技术，通过使用大型语言模型（LLMs）作为知识库，来提高VL transformers的检索性能。具体而言，LLMs中存储的知识以零样本方式通过适当的提示进行检索，实现了性能的提升。此外，我们通过将生成的图像标题视为多项选择候选答案，将VWSD转化为纯文本问答（QA）问题。利用零样本和少样本提示策略来探索这种转换的潜力，同时借助思维链（CoT）提示从而进一步提高性能。",
    "tldr": "本文提出了一种使用大型语言模型作为知识库的方法，通过在视觉词义消歧任务中使用适当的提示，以零样本方式检索存储在语言模型中的知识，从而提高视觉词义消歧的检索性能。"
}