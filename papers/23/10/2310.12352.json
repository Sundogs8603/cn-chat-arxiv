{
    "title": "knn-seq: Efficient, Extensible kNN-MT Framework. (arXiv:2310.12352v1 [cs.CL])",
    "abstract": "k-nearest-neighbor machine translation (kNN-MT) boosts the translation quality of a pre-trained neural machine translation (NMT) model by utilizing translation examples during decoding. Translation examples are stored in a vector database, called a datastore, which contains one entry for each target token from the parallel data it is made from. Due to its size, it is computationally expensive both to construct and to retrieve examples from the datastore. In this paper, we present an efficient and extensible kNN-MT framework, knn-seq, for researchers and developers that is carefully designed to run efficiently, even with a billion-scale large datastore. knn-seq is developed as a plug-in on fairseq and easy to switch models and kNN indexes. Experimental results show that our implemented kNN-MT achieves a comparable gain to the original kNN-MT, and the billion-scale datastore construction took 2.21 hours in the WMT'19 German-to-English translation task. We publish our knn-seq as an MIT-li",
    "link": "http://arxiv.org/abs/2310.12352",
    "context": "Title: knn-seq: Efficient, Extensible kNN-MT Framework. (arXiv:2310.12352v1 [cs.CL])\nAbstract: k-nearest-neighbor machine translation (kNN-MT) boosts the translation quality of a pre-trained neural machine translation (NMT) model by utilizing translation examples during decoding. Translation examples are stored in a vector database, called a datastore, which contains one entry for each target token from the parallel data it is made from. Due to its size, it is computationally expensive both to construct and to retrieve examples from the datastore. In this paper, we present an efficient and extensible kNN-MT framework, knn-seq, for researchers and developers that is carefully designed to run efficiently, even with a billion-scale large datastore. knn-seq is developed as a plug-in on fairseq and easy to switch models and kNN indexes. Experimental results show that our implemented kNN-MT achieves a comparable gain to the original kNN-MT, and the billion-scale datastore construction took 2.21 hours in the WMT'19 German-to-English translation task. We publish our knn-seq as an MIT-li",
    "path": "papers/23/10/2310.12352.json",
    "total_tokens": 1028,
    "translated_title": "knn-seq: 高效、可扩展的kNN-MT框架",
    "translated_abstract": "k-最近邻机器翻译（kNN-MT）通过在解码过程中利用翻译示例来提高预训练神经机器翻译（NMT）模型的翻译质量。翻译示例被存储在一个向量数据库中，称为数据存储，它包含了来自并行数据的每个目标标记的一个条目。由于其规模较大，构建和检索数据存储的示例都具有计算上的昂贵性。在本文中，我们提出了一个高效且可扩展的kNN-MT框架knn-seq，为研究人员和开发者提供了一个精心设计的框架，即使在拥有十亿级大型数据存储的情况下也可以高效地运行。knn-seq是作为fairseq的一个插件开发的，易于切换模型和kNN索引。实验结果表明，我们实现的kNN-MT与原始kNN-MT获得了可比较的增益，并且十亿规模的数据存储构建在WMT'19德英翻译任务中仅花费了2.21小时。我们将我们的knn-seq发布为MIT-li。",
    "tldr": "\"knn-seq\"是一个高效、可扩展的kNN-MT框架，通过利用翻译示例来提高预训练NMT模型的翻译质量，给出了在十亿级数据存储下具有可比较增益的实验结果，并在德英翻译任务中仅花费2.21小时来构建十亿规模的数据存储。",
    "en_tdlr": "\"knn-seq\" is an efficient and extensible kNN-MT framework, which improves the translation quality of pre-trained NMT models by utilizing translation examples. It achieves comparable gains with a billion-scale datastore and takes only 2.21 hours to construct it in a German-to-English translation task."
}