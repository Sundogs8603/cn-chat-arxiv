{
    "title": "Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])",
    "abstract": "We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\\widetilde O(|S||A|t_{\\text{mix}}^2 \\epsilon^{-2})$ and a lower bound of $\\Omega(|S||A|t_{\\text{mix}} \\epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\\widetilde O(|S||A|t_{\\text{mix}}\\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea",
    "link": "http://arxiv.org/abs/2310.08833",
    "context": "Title: Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])\nAbstract: We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\\widetilde O(|S||A|t_{\\text{mix}}^2 \\epsilon^{-2})$ and a lower bound of $\\Omega(|S||A|t_{\\text{mix}} \\epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\\widetilde O(|S||A|t_{\\text{mix}}\\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea",
    "path": "papers/23/10/2310.08833.json",
    "total_tokens": 1045,
    "translated_title": "平均奖励马尔可夫决策过程的最优样本复杂度",
    "translated_abstract": "我们在假设有一个生成模型的情况下，解决了与均匀收敛的马尔可夫决策过程相关的长期平均奖励的策略学习的样本复杂性问题。在这个背景下，现有的文献提供了一个样本复杂度的上界，$ \\widetilde O(|S||A|t_{\\text{mix}}^2 \\epsilon^{-2})$，和一个下界，$\\Omega(|S||A|t_{\\text{mix}} \\epsilon^{-2})$。在这些表达式中，$|S|$和$|A|$分别表示状态空间和动作空间的势，$t_{\\text{mix}}$作为总变异混合时间的统一上限，$\\epsilon$表示误差容忍度。因此，$t_{\\text{mix}}$仍然存在一个显着的差距需要填补。我们的主要贡献是建立一个优化策略的估计器，其样本复杂度为$\\widetilde O(|S||A|t_{\\text{mix}}\\epsilon^{-2})$，有效地达到了文献中的下界。这是通过结合算法思想实现的。",
    "tldr": "本论文解决了对于均匀收敛的马尔可夫决策过程的长期平均奖励最大化策略学习的样本复杂度问题，并建立了一个样本复杂度为$\\widetilde O(|S||A|t_{\\text{mix}}\\epsilon^{-2})$的优化策略估计器。"
}