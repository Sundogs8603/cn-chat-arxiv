{
    "title": "Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])",
    "abstract": "In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called \"black boxes\" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe",
    "link": "http://arxiv.org/abs/2310.07534",
    "context": "Title: Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])\nAbstract: In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called \"black boxes\" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe",
    "path": "papers/23/10/2310.07534.json",
    "total_tokens": 897,
    "translated_title": "XAI方法的以人为中心的评估",
    "translated_abstract": "在不断发展的人工智能领域中，一个关键的挑战是解析深度学习中所谓的“黑盒子”中的决策过程。近年来，出现了许多方法，专门用于解释各种任务的决策。特别是在图像分类等任务中，这些方法通常会识别并强调对分类器预测影响最大的关键像素。有趣的是，这种方法与人类行为相似：当我们被要求解释分类图像的理由时，我们通常会指出最显著的特征或方面。利用这种类似性，我们的研究进行了以用户为中心的研究。我们试图客观地评估三种领先的解释方法的可解释性：（1）典型局部网络、（2）遮挡和（3）层次相关传播。有趣的是，我们的结果表明，尽管这些方法所突出的区域可能差异很大，但它们都提供了可解释的结果。",
    "tldr": "在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。",
    "en_tdlr": "Interpreting the decision-making processes in deep learning \"black boxes\" is a critical challenge. This user-centric study objectively evaluates the interpretability of three leading explanation methods and finds that they all provide interpretable results."
}