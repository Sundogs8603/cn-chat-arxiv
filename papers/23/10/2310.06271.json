{
    "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of \"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Cons",
    "link": "http://arxiv.org/abs/2310.06271",
    "context": "Title: Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of \"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Cons",
    "path": "papers/23/10/2310.06271.json",
    "total_tokens": 869,
    "translated_title": "通过自我反思以减轻大型语言模型中的幻觉",
    "translated_abstract": "大型语言模型（LLMs）在生成和知识密集型任务（包括问答任务）方面显示出了潜力。然而，实际部署仍面临挑战，特别是“幻觉”问题：模型生成似乎合理但不真实或荒谬的信息。在医学领域，这个问题尤为关键，因为涉及到不常见的专业概念和潜在的社会风险。本文使用广泛采用的LLMs和数据集，分析了医学生成性问答系统中的幻觉现象。我们的研究重点是识别和理解常见的问题答案，特别是幻觉。为了解决这个挑战，我们提出了一种交互式的自我反思方法，结合了知识获取和答案生成。通过这个反馈过程，我们的方法逐步增强了生成答案的事实性、一致性和蕴涵性。",
    "tldr": "本文通过分析医学生成性问答系统中的幻觉现象，提出了一种交互式的自我反思方法，通过增强事实性、一致性和蕴涵性，解决了大型语言模型中的幻觉问题。",
    "en_tdlr": "This paper addresses the issue of hallucination in medical generative QA systems by proposing an interactive self-reflection methodology, which enhances the factuality, consistency, and entailment of the generated answers in large language models."
}