{
    "title": "Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])",
    "abstract": "AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.",
    "link": "http://arxiv.org/abs/2310.06178",
    "context": "Title: Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])\nAbstract: AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.",
    "path": "papers/23/10/2310.06178.json",
    "total_tokens": 818,
    "translated_title": "借助msGeMM增加AI GeMM的性能近2.5倍的Look-Up mAI GeMM",
    "translated_abstract": "AI模型的规模不断增加，最近的研究表明，在HPC应用中需要双精度数据类型，而fp8或int4等更低精度的数据类型已经足够用于训练和推断中，而且质量相当。在此趋势下，像NVIDIA和AMD这样的GPU供应商通过张量核心提供了对fp16、fp8和int8 GeMM操作的硬件支持，具有优秀的性能。然而，本文提出了一种名为msGeMM的新算法，该算法证明了使用低精度数据类型的AI模型可以减少约2.5倍的乘法和加法指令。实现此算法的高效率需要具备与张量核心相同速率从小型查找表中添加元素的特殊CUDA核心。",
    "tldr": "这篇论文提出了一种名为msGeMM的新算法，通过使用低精度数据类型，可以使AI模型的性能提高近2.5倍。该算法需要特殊的CUDA核心来实现从小型查找表中添加元素的能力。",
    "en_tdlr": "This paper proposes a new algorithm called msGeMM which improves the performance of AI models by nearly 2.5x by using low-precision datatypes. The algorithm requires special CUDA cores that can add elements from a small look-up table at the rate of Tensor Cores."
}