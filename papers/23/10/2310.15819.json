{
    "title": "Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])",
    "abstract": "The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., \"We are...\"). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase i",
    "link": "http://arxiv.org/abs/2310.15819",
    "context": "Title: Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])\nAbstract: The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., \"We are...\"). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase i",
    "path": "papers/23/10/2310.15819.json",
    "total_tokens": 878,
    "translated_title": "生成式语言模型展示社会身份偏见",
    "translated_abstract": "大型语言模型的流行引发了人们对这些模型可能从人类中学到的偏见的担忧。在这项研究中，我们调查了51个大型语言模型是否展示了社会科学中已知的团体内团结和团体外敌对的基本社会偏见。我们发现，几乎所有基础语言模型和一些指令细调模型在被要求补全句子（例如，“我们是...”）时都展示了明显的团体内积极和团体外消极的偏见。将LLM生成的句子与互联网上人类撰写的句子进行比较表明，这些模型展示了与人类文本相似的甚至更大程度的偏见。为了查明这些偏见的根源，我们在美国民主党和共和党分裂的背景下实验性地变化了模型在细调过程中暴露给团体内积极或团体外消极句子的数量。结果，模型展示出明显的偏见增加。",
    "tldr": "该研究调查了51个大型语言模型展示的社会身份偏见，发现几乎所有模型在补全句子时都展示了明显的团体内积极和团体外消极的偏见。与人类文本相比，这些模型展示了类似或更大程度的偏见。"
}