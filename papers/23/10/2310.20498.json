{
    "title": "Generative Learning of Continuous Data by Tensor Networks. (arXiv:2310.20498v1 [cs.LG])",
    "abstract": "Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model ",
    "link": "http://arxiv.org/abs/2310.20498",
    "context": "Title: Generative Learning of Continuous Data by Tensor Networks. (arXiv:2310.20498v1 [cs.LG])\nAbstract: Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model ",
    "path": "papers/23/10/2310.20498.json",
    "total_tokens": 818,
    "translated_title": "通过张量网络生成连续数据的生成学习",
    "translated_abstract": "张量网络除了用于建模多体量子系统外，还成为解决机器学习问题的一类有前景的模型，尤其是在无监督生成学习中。然而，以量子启发式为特点的张量网络生成模型之前主要局限于二进制或类别数据，限制了它们在现实世界建模问题中的效用。我们通过引入一种能够学习包含连续随机变量的分布的新型张量网络生成模型，克服了这一局限。我们首先在矩阵积态的设置下开发了我们的方法，证明了这个模型族能够以任意精度逼近任何相对平滑的概率密度函数的一般表达性定理。然后，我们在几个合成和真实世界数据集上评估了这个模型的性能，发现该模型具有较好的表现。",
    "tldr": "张量网络生成模型一般适用于二进制或类别数据，这篇论文介绍了一种新型张量网络生成模型，它可以用于学习连续数据分布，并展示了该模型在合成和真实数据集上的性能表现。"
}