{
    "title": "Component attention network for multimodal dance improvisation recognition. (arXiv:2310.05938v1 [cs.CV])",
    "abstract": "Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potenti",
    "link": "http://arxiv.org/abs/2310.05938",
    "context": "Title: Component attention network for multimodal dance improvisation recognition. (arXiv:2310.05938v1 [cs.CV])\nAbstract: Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We propose an attention-based model, component attention network (CANet), for multimodal fusion on three levels: 1) feature fusion with CANet, 2) model fusion with CANet and graph convolutional network (GCN), and 3) late fusion with a voting strategy. We conduct thorough experiments to analyze the impact of each modality in different fusion methods and distinguish critical temporal or component features. We show that our proposed model outperforms the two baseline methods, demonstrating its potenti",
    "path": "papers/23/10/2310.05938.json",
    "total_tokens": 954,
    "translated_title": "组件注意力网络用于多模态舞蹈即兴识别",
    "translated_abstract": "舞蹈即兴是艺术领域中的一个活跃研究课题。由于其独特的动态特性，即兴舞蹈的动作分析可能具有挑战性。数据驱动的舞蹈动作分析，包括识别和生成，通常限于骨骼数据。然而，其他模态的数据，如音频，可以被记录并在下游任务中产生好处。本文探讨了在舞蹈即兴背景下应用和性能的多模态融合方法用于人体动作识别。我们提出了一种基于注意力的模型，组件注意力网络（CANet），用于三个层次的多模态融合：1）CANet进行特征融合，2）CANet和图卷积网络（GCN）进行模型融合，以及3）采用投票策略进行后期融合。我们进行了全面的实验，以分析不同融合方法中每种模态的影响，并区分出关键的时间或组件特征。我们展示了我们提出的模型优于两种基准方法，证明了其潜力。",
    "tldr": "本研究提出了一种用于多模态舞蹈即兴识别的组件注意力网络 (CANet)。通过多层次的融合方法，并通过实验分析每种模态的影响，我们证明了该模型相比基准方法具有更好的性能。",
    "en_tdlr": "An attention-based model, Component Attention Network (CANet), is proposed for multimodal dance improvisation recognition. By employing a fusion strategy on multiple levels and conducting thorough experiments to analyze the impact of each modality, the proposed model outperforms baseline methods and demonstrates its potential."
}