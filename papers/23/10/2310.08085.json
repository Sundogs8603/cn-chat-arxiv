{
    "title": "Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])",
    "abstract": "Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.",
    "link": "http://arxiv.org/abs/2310.08085",
    "context": "Title: Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])\nAbstract: Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.",
    "path": "papers/23/10/2310.08085.json",
    "total_tokens": 875,
    "translated_title": "以问答方式翻译印尼语的低资源标题党处理",
    "translated_abstract": "标题党处理旨在生成一个短文本，以满足标题党帖子引起的好奇心。由于这是一个新引入的任务，目前只有英文数据集可用。我们的贡献包括在印尼语中构建手动标注的标题党处理语料库，并评估使用跨语言零射击问答模型来处理印尼语等低资源语言的标题党处理。我们利用多语言语言模型的选择。实验结果表明，XLM-RoBERTa（大）模型在短语和段落标题党处理方面优于其他模型，而mDeBERTa（基础）模型在多部分标题党处理中优于其他模型。",
    "tldr": "本论文介绍了一项以问答方式翻译印尼语低资源标题党处理的任务，贡献包括构建手动标注的印尼语标题党处理语料库，并评估了跨语言零射击问答模型的应用。实验结果表明，XLM-RoBERTa（大）模型在短语和段落标题党处理方面表现优异，而mDeBERTa（基础）模型在多部分标题党处理中表现最佳。",
    "en_tdlr": "This paper presents a task of translating low-resource clickbait spoiling for Indonesian via question answering, with contributions including the construction of a manually labeled Indonesian clickbait spoiling corpus and an evaluation on using cross-lingual zero-shot question answering-based models. Experimental results indicate that the XLM-RoBERTa (large) model performs better for phrase and passage spoilers, while the mDeBERTa (base) model outperforms others for multipart spoilers."
}