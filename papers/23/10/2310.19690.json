{
    "title": "Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds. (arXiv:2310.19690v1 [cs.LG])",
    "abstract": "Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly bro",
    "link": "http://arxiv.org/abs/2310.19690",
    "context": "Title: Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds. (arXiv:2310.19690v1 [cs.LG])\nAbstract: Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly bro",
    "path": "papers/23/10/2310.19690.json",
    "total_tokens": 961,
    "translated_title": "通过变分界限实现实用的非对抗分布对齐",
    "translated_abstract": "分布对齐可用于学习具有公平性和鲁棒性应用的不变表示。大多数先前的工作都采用对抗对齐方法，但由此产生的极小极大问题不稳定且难以优化。非对抗的基于似然的方法要么需要模型可逆性，要么对潜在先验施加约束，要么缺乏通用的对齐框架。为了克服这些限制，我们提出了一种非对抗的基于变分自动编码器的对齐方法，可应用于任何模型管道。我们开发了一组对齐上界（包括一个含噪音的上界），其具有类似变分自动编码器的目标但具有不同的视角。我们在理论上和实证上仔细比较了我们的方法与先前的基于变分自动编码器的对齐方法。最后，我们证明我们的新颖对齐损失可以在标准的不变表示学习管道中取代对抗损失，而无需修改原始架构，从而显著拓展了应用范围。",
    "tldr": "本论文提出了一种非对抗的基于变分自动编码器的对齐方法，通过引入一组对齐上界，解决了先前方法中存在的不稳定性和限制。实验证明，这种新颖的对齐损失可以在不改变原始架构的情况下取代对抗损失，扩展了应用范围。"
}