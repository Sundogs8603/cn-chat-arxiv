{
    "title": "Learning Regularized Monotone Graphon Mean-Field Games. (arXiv:2310.08089v1 [cs.GT])",
    "abstract": "This paper studies two fundamental problems in regularized Graphon Mean-Field Games (GMFGs). First, we establish the existence of a Nash Equilibrium (NE) of any $\\lambda$-regularized GMFG (for $\\lambda\\geq 0$). This result relies on weaker conditions than those in previous works for analyzing both unregularized GMFGs ($\\lambda=0$) and $\\lambda$-regularized MFGs, which are special cases of GMFGs. Second, we propose provably efficient algorithms to learn the NE in weakly monotone GMFGs, motivated by Lasry and Lions [2007]. Previous literature either only analyzed continuous-time algorithms or required extra conditions to analyze discrete-time algorithms. In contrast, we design a discrete-time algorithm and derive its convergence rate solely under weakly monotone conditions. Furthermore, we develop and analyze the action-value function estimation procedure during the online learning process, which is absent from algorithms for monotone GMFGs. This serves as a sub-module in our optimizatio",
    "link": "http://arxiv.org/abs/2310.08089",
    "context": "Title: Learning Regularized Monotone Graphon Mean-Field Games. (arXiv:2310.08089v1 [cs.GT])\nAbstract: This paper studies two fundamental problems in regularized Graphon Mean-Field Games (GMFGs). First, we establish the existence of a Nash Equilibrium (NE) of any $\\lambda$-regularized GMFG (for $\\lambda\\geq 0$). This result relies on weaker conditions than those in previous works for analyzing both unregularized GMFGs ($\\lambda=0$) and $\\lambda$-regularized MFGs, which are special cases of GMFGs. Second, we propose provably efficient algorithms to learn the NE in weakly monotone GMFGs, motivated by Lasry and Lions [2007]. Previous literature either only analyzed continuous-time algorithms or required extra conditions to analyze discrete-time algorithms. In contrast, we design a discrete-time algorithm and derive its convergence rate solely under weakly monotone conditions. Furthermore, we develop and analyze the action-value function estimation procedure during the online learning process, which is absent from algorithms for monotone GMFGs. This serves as a sub-module in our optimizatio",
    "path": "papers/23/10/2310.08089.json",
    "total_tokens": 911,
    "translated_title": "学习正则化的单调图匹配场博弈",
    "translated_abstract": "本文研究了正则化图匹配场博弈（GMFG）中的两个基本问题。首先，我们建立了任意λ正则化GMFG（对于λ≥0）的纳什均衡存在性。这个结果所依赖的条件比以前研究非正则化GMFG（λ=0）和λ-正则化MFG的条件要弱。第二，我们提出了一种证明有效的算法来学习弱单调GMFG的纳什均衡，受到Lasry和Lions [2007]的启发。以前的文献要么只分析连续时间算法，要么需要额外的条件来分析离散时间算法。相反，我们设计了一种离散时间算法，并在弱单调条件下推导了其收敛速度。此外，我们在在线学习过程中开发和分析了动作值函数估计过程，这在单调GMFG的算法中是缺失的。这在我们的优化过程中起到了子模块的作用。",
    "tldr": "本文研究了正则化图匹配场博弈的存在性和学习算法问题，在弱单调条件下提出了一种离散时间算法，并开发了动作值函数估计过程作为优化过程的子模块。",
    "en_tdlr": "This paper studies the existence and learning algorithm problems of regularized graphon mean-field games, proposes a discrete-time algorithm under weak monotone conditions, and develops an action-value function estimation procedure as a submodule in the optimization process."
}