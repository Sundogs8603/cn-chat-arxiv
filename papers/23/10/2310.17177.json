{
    "title": "Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])",
    "abstract": "Despite the success of transformers on various computer vision tasks, they suffer from excessive memory and computational cost. Some works present dynamic vision transformers to accelerate inference by pruning redundant tokens. A key to improving token pruning is using well-trained models as initialization for faster convergence and better performance. However, current base models usually adopt full image training, i.e., using full images as inputs and keeping the whole feature maps through the forward process, which causes inconsistencies with dynamic models that gradually reduce tokens, including calculation pattern, information amount and token selection strategy inconsistencies. Inspired by MAE which performs masking and reconstruction self-supervised task, we devise masked fine-tuning to bridge the gaps between pre-trained base models used for initialization and token pruning based dynamic vision transformers, by masking image patches and predicting the image class label based on ",
    "link": "http://arxiv.org/abs/2310.17177",
    "context": "Title: Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])\nAbstract: Despite the success of transformers on various computer vision tasks, they suffer from excessive memory and computational cost. Some works present dynamic vision transformers to accelerate inference by pruning redundant tokens. A key to improving token pruning is using well-trained models as initialization for faster convergence and better performance. However, current base models usually adopt full image training, i.e., using full images as inputs and keeping the whole feature maps through the forward process, which causes inconsistencies with dynamic models that gradually reduce tokens, including calculation pattern, information amount and token selection strategy inconsistencies. Inspired by MAE which performs masking and reconstruction self-supervised task, we devise masked fine-tuning to bridge the gaps between pre-trained base models used for initialization and token pruning based dynamic vision transformers, by masking image patches and predicting the image class label based on ",
    "path": "papers/23/10/2310.17177.json",
    "total_tokens": 896,
    "translated_title": "通过遮蔽微调来弥合标记修剪和完整预训练之间的差距",
    "translated_abstract": "尽管transformers在各种计算机视觉任务中取得了成功，但它们存在内存和计算成本过高的问题。一些工作提出了动态视觉transformers来通过修剪多余的标记来加速推理。改进标记修剪的关键是使用训练有素的模型作为初始化，以实现更快的收敛和更好的性能。然而，当前的基本模型通常采用完整的图像训练，即使用完整的图像作为输入，并在前向过程中保留整个特征图，这与逐渐减少标记的动态模型存在不一致，包括计算模式、信息量和标记选择策略上的不一致。受到执行遮蔽和重构自监督任务的MAE的启发，我们设计了遮蔽微调来弥合用于初始化的预训练基本模型与基于标记修剪的动态视觉transformers之间的差距，通过遮蔽图像块并预测图像类别标签来完成微调。",
    "tldr": "本文提出了一种遮蔽微调方法，用于弥合标记修剪和完整预训练之间的差距，该方法通过遮蔽图像块并预测图像类别标签来实现动态视觉transformers的初始化和加速推理。",
    "en_tdlr": "This paper proposes a masked fine-tuning method to bridge the gaps between token pruning and full pre-training. By masking image patches and predicting image class labels, the method achieves initialization and accelerated inference for dynamic vision transformers."
}