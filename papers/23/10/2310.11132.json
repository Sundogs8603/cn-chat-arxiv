{
    "title": "Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])",
    "abstract": "Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these ",
    "link": "http://arxiv.org/abs/2310.11132",
    "context": "Title: Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])\nAbstract: Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these ",
    "path": "papers/23/10/2310.11132.json",
    "total_tokens": 912,
    "translated_title": "非参数的混合连续-分类变量条件独立性检验：一种新方法和数值评估",
    "translated_abstract": "条件独立性检验（CIT）是机器学习中常见的任务，例如变量选择，也是基于约束的因果发现的主要组成部分。大多数当下的CIT方法假设所有变量都是数值或所有变量都是分类的，然而，许多现实世界的应用涉及包含数值和分类变量的混合类型数据集。非参数的CIT可以使用基于条件互信息（CMI）估计器结合本地置换方案进行。最近，已提出了两种基于k最近邻（k-NN）的混合类型数据集的新型CMI估计器。与任何k-NN方法一样，这些估计器依赖于距离度量的定义。一种方法通过对分类变量进行one-hot编码来计算距离，从而将分类变量视为离散数值变量，而另一种方法则通过使用分类变量作为条件来通过熵来表示CMI。在这项工作中，我们研究这些方法",
    "tldr": "这项工作研究了一种针对混合类型数据集的非参数条件独立性检验方法，该方法使用基于k最近邻的CMI估计器和本地置换方案，针对现实世界中包含数值和分类变量的情况。"
}