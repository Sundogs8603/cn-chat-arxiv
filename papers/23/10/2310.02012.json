{
    "title": "Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion. (arXiv:2310.02012v1 [cs.LG])",
    "abstract": "Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: \"Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?\" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, wh",
    "link": "http://arxiv.org/abs/2310.02012",
    "context": "Title: Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion. (arXiv:2310.02012v1 [cs.LG])\nAbstract: Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: \"Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?\" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, wh",
    "path": "papers/23/10/2310.02012.json",
    "total_tokens": 926,
    "translated_title": "超越深度限制的训练：批归一化避免梯度爆炸",
    "translated_abstract": "归一化层是深度神经网络的关键组成部分之一。一些理论研究表明，批归一化改善了信号传播，通过避免表示在层之间变得共线。然而，批归一化的均场理论也得出结论，这种好处是以深度梯度爆炸的代价为。在本研究中，我们受到批归一化的这两个方面的启发，提出了以下问题：“批归一化网络能否保持最优的信号传播特性，但避免梯度爆炸？”我们肯定地回答了这个问题，通过给出一种具体的构造方法，证明了具有线性激活和批归一化的多层感知器（MLP）在任何深度都具有有界梯度。基于Weingarten微积分，我们为该构造的MLP开发了一个严格的非渐近理论，给出了前向信号传播的精确特征化。",
    "tldr": "本研究提出了一种超越深度限制的训练方法，通过批归一化避免梯度爆炸。研究给出了一种具体构造的多层感知器，在任何深度都能保持优化的信号传播特性，并具有有界梯度。",
    "en_tdlr": "This study proposes a training method that goes beyond depth limits by preventing gradient explosion through batch normalization. The research provides a specific construction of a Multi-Layer Perceptron (MLP) that maintains optimal signal propagation properties at any depth, with bounded gradients."
}