{
    "title": "Bi-Level Offline Policy Optimization with Limited Exploration. (arXiv:2310.06268v1 [cs.LG])",
    "abstract": "We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it",
    "link": "http://arxiv.org/abs/2310.06268",
    "context": "Title: Bi-Level Offline Policy Optimization with Limited Exploration. (arXiv:2310.06268v1 [cs.LG])\nAbstract: We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it",
    "path": "papers/23/10/2310.06268.json",
    "total_tokens": 929,
    "translated_title": "有限探索条件下的双层离线策略优化",
    "translated_abstract": "本文研究了基于已知数据集学习良好策略的离线强化学习。在函数逼近的情况下，由于数据集缺乏足够的探索，导致了分布偏移的困难。为了解决这个问题，我们提出了一种双层结构的策略优化算法，该算法模拟了策略（上层）和值函数（下层）之间的层次性交互。下层的重点是构建一个置信区间，以保持权重平均Bellman误差足够小，同时控制由分布不匹配引起的不确定性。随后，在上层，策略旨在最大化下层形成的置信区间中的保守估计值。这种新颖的表述保留了隐式引导的探索数据分布的最大灵活性，使得模型推广的能力得以发挥。在实践中，我们通过数值实验验证了我们的算法的有效性。",
    "tldr": "本文提出了一种双层离线策略优化算法，通过模拟策略和值函数之间的层次交互，解决了离线强化学习中数据集缺乏探索所导致的分布偏移问题。该算法通过构建置信区间和最大化保守估计值来提高策略的性能。",
    "en_tdlr": "This paper proposes a bi-level offline policy optimization algorithm that addresses the distributional shift problem caused by limited exploration in offline reinforcement learning. By modeling the interaction between the policy and the value function, the algorithm constructs a confidence set of value estimates and maximizes the conservative estimate at the upper level, improving the performance of the policy."
}