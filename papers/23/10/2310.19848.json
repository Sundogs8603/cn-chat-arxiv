{
    "title": "Efficient Exploration in Continuous-time Model-based Reinforcement Learning. (arXiv:2310.19848v1 [cs.LG])",
    "abstract": "Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of co",
    "link": "http://arxiv.org/abs/2310.19848",
    "context": "Title: Efficient Exploration in Continuous-time Model-based Reinforcement Learning. (arXiv:2310.19848v1 [cs.LG])\nAbstract: Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of co",
    "path": "papers/23/10/2310.19848.json",
    "total_tokens": 917,
    "translated_title": "连续时间模型驱动的高效探索在强化学习中",
    "translated_abstract": "强化学习算法通常考虑离散时间动态，即使底层系统通常是连续时间的。本文介绍了一种模型驱动的强化学习算法，使用非线性常微分方程（ODE）表示连续时间动态。我们使用良好校准的概率模型来捕捉认识不确定性，并使用乐观原则进行探索。我们的遗憾界表明了测量选择策略（MSS）的重要性，因为在连续时间中，我们不仅必须决定如何进行探索，还必须决定何时观察底层系统。我们的分析证明，使用高斯过程（GP）对ODE进行建模，并选择常见的MSS，如等距采样，遗憾是次线性的。此外，我们提出了一种自适应的、数据相关的实用MSS，在与GP动力学相结合时，也能以更少的样本实现次线性遗憾。我们展示了协同作用的好处。",
    "tldr": "连续时间模型驱动的强化学习算法使用非线性常微分方程（ODEs）表示连续时间动态，并通过使用乐观原则进行探索来降低遗憾，同时提出了一种自适应、数据相关的测量选择策略，具有更少的样本和次线性遗憾。",
    "en_tdlr": "The paper introduces a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). It reduces regret by using the optimistic principle for exploration and proposes an adaptive, data-dependent measurement selection strategy with fewer samples and sublinear regret."
}