{
    "title": "Equivariant Transformer is all you need. (arXiv:2310.13222v1 [hep-lat])",
    "abstract": "Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.",
    "link": "http://arxiv.org/abs/2310.13222",
    "context": "Title: Equivariant Transformer is all you need. (arXiv:2310.13222v1 [hep-lat])\nAbstract: Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.",
    "path": "papers/23/10/2310.13222.json",
    "total_tokens": 846,
    "translated_title": "等变Transformer就是你所需要的",
    "translated_abstract": "机器学习、深度学习已经在计算物理学中取得了加速，用于在格点上模拟系统。等变性对于模拟物理系统是至关重要的，因为它为机器学习模型描述的概率分布引入了强烈的归纳偏差。这降低了数据对称性和物理定律偏离的错误外推的风险。然而，将对称性强加于模型有时会导致自学习Monte-Carlo（SLMC）中的接受率不佳。另一方面，Transformers中使用的Attention实现了较大的模型容量。我们将等变性Attention引入到SLMC中。为了评估我们的架构，我们将其应用于我们提出的二维格点自旋费米模型上的新架构。我们发现它克服了线性模型的低接受率，并观察到与Transformers中的大型语言模型相似的接受率的标度律。",
    "tldr": "等变Transformer能够解决自学习Monte-Carlo中的低接受率问题，实现了较大的模型容量，在模拟物理系统中具有重要的创新和贡献。",
    "en_tdlr": "The Eqivariant Transformer addresses the issue of low acceptance rates in self-learning Monte-Carlo simulations, while also providing a larger model capacity. It presents an important innovation and contribution in simulating physical systems."
}