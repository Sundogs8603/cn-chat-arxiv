{
    "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)",
    "abstract": "We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.",
    "link": "http://arxiv.org/abs/2310.05916",
    "context": "Title: Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)\nAbstract: We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.",
    "path": "papers/23/10/2310.05916.json",
    "total_tokens": 837,
    "translated_title": "通过基于文本的分解解释CLIP图像表示",
    "translated_abstract": "本文通过分析个别模型组件对最终表示的影响，探讨了CLIP图像编码器。我们将图像表示分解为各个图像块、模型层和注意力头的求和，并使用CLIP的文本表示来解释这些求和项。通过解释注意力头，我们通过自动寻找能够跨越输出空间的文本表示来表征每个头的作用，揭示出许多头的特定属性角色（例如位置或形状）。接下来，通过解释图像块，我们揭示了CLIP中的紧密空间定位。最后，我们利用这一理解消除了CLIP中的误特征，并创建了一个强大的零样本图像分割器。我们的结果表明，可扩展的对Transformer模型的理解是可实现的，并可用于修复和改进模型。",
    "tldr": "本文通过解析CLIP图像编码器的组件，揭示了图像表示的构成方式，并利用文本表示解释了其各个部分的作用。通过理解注意力头和图像块，作者实现了对模型的修复和改进，包括消除误特征和构建零样本图像分割器等方面。",
    "en_tdlr": "This paper investigates the CLIP image encoder by decomposing the image representation and interpreting its components using text representations. It reveals the roles of attention heads and image patches, leading to the improvement of the model by removing spurious features and creating a strong zero-shot image segmenter."
}