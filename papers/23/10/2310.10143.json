{
    "title": "An Empirical Study of Simplicial Representation Learning with Wasserstein Distance. (arXiv:2310.10143v1 [stat.ML])",
    "abstract": "In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total",
    "link": "http://arxiv.org/abs/2310.10143",
    "context": "Title: An Empirical Study of Simplicial Representation Learning with Wasserstein Distance. (arXiv:2310.10143v1 [stat.ML])\nAbstract: In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total",
    "path": "papers/23/10/2310.10143.json",
    "total_tokens": 941,
    "translated_title": "用Wasserstein距离进行简化表示学习的实证研究",
    "translated_abstract": "本文探讨了在树结构上利用1-Wasserstein距离进行简化表示学习的问题，其中树- Wasserstein距离(TWD)定义为两个树嵌入向量之间的L1距离。具体而言，我们考虑了一种基于SimCLR和负TWD作为相似度度量的自监督学习方法来估计简化表示。在SimCLR中，通常使用与实向量嵌入的余弦相似度，但是尚未对利用L1距离与简化嵌入进行深入研究。一个关键挑战是训练L1距离在数值上具有挑战性，并且往往会产生不令人满意的结果，概率模型的选择也有很多。因此，本研究从实证角度探究了用TWD优化自监督学习的策略，并找到了稳定的训练过程。更具体地说，我们评估了两种类型TWD的组合（总 ...",
    "tldr": "本文研究了在树结构上利用Wasserstein距离进行简化表示学习的问题，并提出了一种基于SimCLR和负TWD的自监督学习方法来估计简化表示，通过实证研究找到了稳定的训练策略。"
}