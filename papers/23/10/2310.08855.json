{
    "title": "Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation. (arXiv:2310.08855v1 [cs.LG])",
    "abstract": "Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to ",
    "link": "http://arxiv.org/abs/2310.08855",
    "context": "Title: Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation. (arXiv:2310.08855v1 [cs.LG])\nAbstract: Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to ",
    "path": "papers/23/10/2310.08855.json",
    "total_tokens": 912,
    "translated_title": "克服连续学习中归一化统计的近期偏差：平衡和适应",
    "translated_abstract": "连续学习涉及学习一系列任务并适当平衡它们的知识。在深度神经网络中，由于对旧任务的训练样本有限，目前的研究主要关注于在基于梯度的优化中克服旧任务的灾难性遗忘。然而，归一化层提供了一种例外，因为它们通过梯度和当前观察到的训练样本的统计信息进行相互依赖的更新，这需要专门的策略来减轻近期偏差。在这项工作中，我们重点研究了最流行的批归一化（BN）并对其在连续学习中的次优性进行了深入的理论分析。我们的分析展示了BN统计的平衡和适应之间的困境，这可能影响训练的稳定性和推广能力。针对这些特定挑战，我们提出了自适应BN的平衡策略（AdaB$^2$N），它适当地将贝叶斯策略纳入其中，以更好地适应增量任务。",
    "tldr": "连续学习中的归一化统计存在近期偏差问题，我们提出了自适应BN的平衡策略AdaB$^2$N来解决这个问题。",
    "en_tdlr": "There is a recency bias issue in normalization statistics in continual learning. We propose a balanced strategy, AdaB$^2$N, for adaptive batch normalization to address this issue."
}