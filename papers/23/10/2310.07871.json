{
    "title": "Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])",
    "abstract": "Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.",
    "link": "http://arxiv.org/abs/2310.07871",
    "context": "Title: Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])\nAbstract: Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.",
    "path": "papers/23/10/2310.07871.json",
    "total_tokens": 799,
    "translated_title": "基于多模态电子健康记录的层次预训练方法",
    "translated_abstract": "预训练在自然语言处理中已被证明是一种强大的技术，在各种下游任务中取得了显著的成功。然而，在医学领域，现有的电子健康记录预训练模型无法捕捉到健康记录数据的层次性，限制了它们在使用单个预训练模型跨多个下游任务的泛化能力。为了解决这个挑战，本文介绍了一种新颖、通用且统一的预训练框架MEDHMP，专门针对层次多模态的健康记录数据进行设计。通过实验结果在三个级别上展示了所提出的MEDHMP的有效性，并通过与十八个基准模型的比较进一步突出了我们方法的有效性。",
    "tldr": "该论文提出了一种针对层次多模态电子健康记录数据的新颖、通用且统一的预训练框架MEDHMP，在三个级别上展示了其有效性，并且在与十八个基准方法的比较中验证了其高效性。",
    "en_tdlr": "This paper introduces a novel, general, and unified pretraining framework called MEDHMP for hierarchically multimodal electronic health records, demonstrating its effectiveness across three levels and highlighting its efficacy through comparisons against eighteen baselines."
}