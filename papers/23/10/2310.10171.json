{
    "title": "On permutation symmetries in Bayesian neural network posteriors: a variational perspective. (arXiv:2310.10171v1 [stat.ML])",
    "abstract": "The elusive nature of gradient-based optimization in neural networks is tied to their loss landscape geometry, which is poorly understood. However recent work has brought solid evidence that there is essentially no loss barrier between the local solutions of gradient descent, once accounting for weight-permutations that leave the network's computation unchanged. This raises questions for approximate inference in Bayesian neural networks (BNNs), where we are interested in marginalizing over multiple points in the loss landscape. In this work, we first extend the formalism of marginalized loss barrier and solution interpolation to BNNs, before proposing a matching algorithm to search for linearly connected solutions. This is achieved by aligning the distributions of two independent approximate Bayesian solutions with respect to permutation matrices. We build on the results of Ainsworth et al. (2023), reframing the problem as a combinatorial optimization one, using an approximation to the",
    "link": "http://arxiv.org/abs/2310.10171",
    "context": "Title: On permutation symmetries in Bayesian neural network posteriors: a variational perspective. (arXiv:2310.10171v1 [stat.ML])\nAbstract: The elusive nature of gradient-based optimization in neural networks is tied to their loss landscape geometry, which is poorly understood. However recent work has brought solid evidence that there is essentially no loss barrier between the local solutions of gradient descent, once accounting for weight-permutations that leave the network's computation unchanged. This raises questions for approximate inference in Bayesian neural networks (BNNs), where we are interested in marginalizing over multiple points in the loss landscape. In this work, we first extend the formalism of marginalized loss barrier and solution interpolation to BNNs, before proposing a matching algorithm to search for linearly connected solutions. This is achieved by aligning the distributions of two independent approximate Bayesian solutions with respect to permutation matrices. We build on the results of Ainsworth et al. (2023), reframing the problem as a combinatorial optimization one, using an approximation to the",
    "path": "papers/23/10/2310.10171.json",
    "total_tokens": 975,
    "translated_title": "关于贝叶斯神经网络后验中置换对称性的研究: 一个变分角度",
    "translated_abstract": "神经网络中基于梯度的优化的难以捉摸的性质与其损失函数的几何形态有关，而这个几何形态目前还不太被理解。然而，最近的研究已经提供了坚实的证据，证明在梯度下降的局部解之间基本上不存在损失阻碍，只要考虑到保持网络计算不变的权重置换。这引发了对贝叶斯神经网络（BNNs）中近似推断的问题，我们关心的是在损失函数空间中对多个点进行边缘化。在这项工作中，我们首先将边缘化的损失阻碍和解插值的形式主义扩展到BNNs中，然后提出了一种匹配算法来寻找线性相连的解决方案。这是通过将两个独立的近似贝叶斯解决方案的分布与置换矩阵对齐来实现的。我们基于Ainsworth等人（2023）的结果，将问题重新框架为一个组合优化问题，并使用一种近似方法。",
    "tldr": "本文研究了贝叶斯神经网络后验中的置换对称性，揭示了在梯度下降的局部解之间基本上不存在损失阻碍。通过使用置换矩阵对齐两个贝叶斯解决方案的分布，提出了一种寻找线性相连解的匹配算法。",
    "en_tdlr": "This paper investigates permutation symmetries in the posterior of Bayesian neural networks and reveals the essentially barrier-free nature of local solutions in gradient descent. A matching algorithm is proposed to search for linearly connected solutions by aligning the distributions of two Bayesian solutions using permutation matrices."
}