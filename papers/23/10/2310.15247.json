{
    "title": "SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis. (arXiv:2310.15247v1 [cs.SD])",
    "abstract": "Sound design involves creatively selecting, recording, and editing sound effects for various media like cinema, video games, and virtual/augmented reality. One of the most time-consuming steps when designing sound is synchronizing audio with video. In some cases, environmental recordings from video shoots are available, which can aid in the process. However, in video games and animations, no reference audio exists, requiring manual annotation of event timings from the video. We propose a system to extract repetitive actions onsets from a video, which are then used - in conjunction with audio or textual embeddings - to condition a diffusion model trained to generate a new synchronized sound effects audio track. In this way, we leave complete creative control to the sound designer while removing the burden of synchronization with video. Furthermore, editing the onset track or changing the conditioning embedding requires much less effort than editing the audio track itself, simplifying th",
    "link": "http://arxiv.org/abs/2310.15247",
    "context": "Title: SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis. (arXiv:2310.15247v1 [cs.SD])\nAbstract: Sound design involves creatively selecting, recording, and editing sound effects for various media like cinema, video games, and virtual/augmented reality. One of the most time-consuming steps when designing sound is synchronizing audio with video. In some cases, environmental recordings from video shoots are available, which can aid in the process. However, in video games and animations, no reference audio exists, requiring manual annotation of event timings from the video. We propose a system to extract repetitive actions onsets from a video, which are then used - in conjunction with audio or textual embeddings - to condition a diffusion model trained to generate a new synchronized sound effects audio track. In this way, we leave complete creative control to the sound designer while removing the burden of synchronization with video. Furthermore, editing the onset track or changing the conditioning embedding requires much less effort than editing the audio track itself, simplifying th",
    "path": "papers/23/10/2310.15247.json",
    "total_tokens": 883,
    "translated_title": "SyncFusion:多模态同步音视频霍利合成",
    "translated_abstract": "声音设计涉及到为电影、视频游戏和虚拟/增强现实等各种媒体选择、录制和编辑音效。在设计声音时，其中一个最耗时的步骤是将音频与视频同步。在某些情况下，可以利用视频拍摄的环境录音来帮助此过程。然而，在视频游戏和动画中，没有参考音频存在，需要从视频中手动注释事件的时间。我们提出了一个系统，用于从视频中提取重复动作的起始点，然后与音频或文本嵌入一起，用于条件化训练的扩散模型生成新的同步音效音轨。通过这种方式，我们将完全的创作控制权交给了声音设计师，同时消除了与视频同步的负担。此外，编辑起始轨道或更改条件嵌入所需的工作量要比编辑音频轨道本身要少得多，简化了流程。",
    "tldr": "SyncFusion是一个多模态同步的音视频霍利合成系统，能够从视频中提取重复的动作起始点，并使用音频或文本嵌入来生成新的同步音效音轨，为声音设计师提供了完全的创作控制权并简化了流程。",
    "en_tdlr": "SyncFusion is a multimodal onset-synchronized video-to-audio Foley synthesis system that extracts repetitive action onsets from a video and uses audio or textual embeddings to generate a new synchronized sound effects audio track, providing complete creative control to sound designers and simplifying the process."
}