{
    "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])",
    "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
    "link": "http://arxiv.org/abs/2310.04418",
    "context": "Title: Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])\nAbstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
    "path": "papers/23/10/2310.04418.json",
    "total_tokens": 781,
    "translated_title": "用于相对位置的函数插值改进了长上下文Transformer",
    "translated_abstract": "在扩展这些模型的上下文长度时，防止Transformer在训练以外更长输入上性能下降一直是一个重要的挑战。虽然Transformer架构在可处理的输入序列长度上基本没有限制，但在训练过程中使用的位置编码的选择可能会限制这些模型在更长输入上的性能。我们提出了一种新颖的函数相对位置编码与渐进插值方法（FIRE），以改进Transformer对更长上下文的泛化能力。我们从理论上证明了这可以表示出一些流行的相对位置编码，如T5的RPE、Alibi和Kerple。接下来，我们在零射击语言建模和长文本基准测试上经验性地展示了FIRE模型在更长上下文中具有更好的泛化能力。",
    "tldr": "这项研究提出了一种名为FIRE的函数相对位置编码与渐进插值方法，通过改进Transformer对更长上下文的泛化能力，并在零射击语言建模和长文本基准测试中进行了实证验证。",
    "en_tdlr": "This paper proposes a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. The FIRE model shows better generalization to longer contexts in zero-shot language modeling and long text benchmarks."
}