{
    "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])",
    "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false pr",
    "link": "http://arxiv.org/abs/2310.03214",
    "context": "Title: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])\nAbstract: Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false pr",
    "path": "papers/23/10/2310.03214.json",
    "total_tokens": 939,
    "translated_title": "FreshLLMs: 使用搜索引擎增强的方法刷新大型语言模型",
    "translated_abstract": "大多数大型语言模型（LLMs）只训练一次，不进行更新，因此缺乏对不断变化的世界动态适应的能力。本研究在测试当前世界知识的问题回答的背景下，对LLM生成的文本的事实性进行了详细研究。具体而言，我们引入了FreshQA，一个新颖的动态问答基准，包括广泛的问题和答案类型，包括需要快速变化的世界知识和需要揭示错误前提的问题。我们在一个双模式评估过程中对多种闭源和开源LLM进行了基准测试，可以同时测量正确性和虚构性。通过涉及超过50K个评判的人类评估，我们揭示了这些模型的局限性，并证明了改进的显著空间：例如，所有模型（无论模型大小如何）在涉及快速变化的知识和错误前提的问题上都面临困难。",
    "tldr": "本文提出了一种使用搜索引擎增强的方法，刷新大型语言模型。我们通过详细研究LLM生成的文本在回答问题方面的事实性，引入了FreshQA这一动态问答基准。通过人类评估，我们发现这些模型存在局限性，并表明有显著的改进空间。"
}