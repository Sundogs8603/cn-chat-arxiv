{
    "title": "Large Language Models for Test-Free Fault Localization. (arXiv:2310.01726v1 [cs.SE])",
    "abstract": "Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion para",
    "link": "http://arxiv.org/abs/2310.01726",
    "context": "Title: Large Language Models for Test-Free Fault Localization. (arXiv:2310.01726v1 [cs.SE])\nAbstract: Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion para",
    "path": "papers/23/10/2310.01726.json",
    "total_tokens": 860,
    "translated_title": "用于无测试故障定位的大语言模型",
    "translated_abstract": "故障定位（FL）旨在自动定位有问题的代码行，这是许多手动和自动调试任务的关键第一步。以前的FL技术假设提供输入测试，并且通常需要进行广泛的程序分析、程序插桩或数据预处理。以往的深度学习自动程序修复技术在小数据集上学习困难，并在实际程序上产生有限的结果。受到大语言模型（LLMs）在很少示例上适应新任务的能力的启发，我们研究了将LLMs应用于行级故障定位的可行性。具体地，我们建议通过在LLMs学习的表示之上微调一小组双向适配器层来克服LLMs的自左向右特性，从而产生LLMAO，这是第一个基于语言模型的故障定位方法，它可以在没有任何测试覆盖信息的情况下定位有问题的代码行。我们使用3.5亿、60亿和160亿段来微调LLMs。",
    "tldr": "该论文提出了一种基于大语言模型的故障定位方法，称为LLMAO，具有无需测试覆盖信息就能定位有问题的代码行的能力。"
}