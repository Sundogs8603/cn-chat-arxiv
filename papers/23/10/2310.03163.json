{
    "title": "FedNAR: Federated Optimization with Normalized Annealing Regularization. (arXiv:2310.03163v1 [cs.LG])",
    "abstract": "Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical ",
    "link": "http://arxiv.org/abs/2310.03163",
    "context": "Title: FedNAR: Federated Optimization with Normalized Annealing Regularization. (arXiv:2310.03163v1 [cs.LG])\nAbstract: Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical ",
    "path": "papers/23/10/2310.03163.json",
    "total_tokens": 894,
    "translated_title": "FedNAR: 带有归一化退火正则化的联邦优化",
    "translated_abstract": "在现代深度神经网络优化中，权重衰减是一种改善泛化性能的标准技术，在联邦学习中也被广泛采用以防止本地客户端过拟合。本文首先探索了权重衰减的选择，并确定权重衰减值显著影响现有联邦学习算法的收敛性。虽然防止过拟合至关重要，但权重衰减可能引入对全局目标的不同优化目标，在联邦学习中由于多个本地更新和异构数据分布进一步放大了这一影响。为了解决这个挑战，我们开发了一种简单而有效、多功能的算法插件，即“带有归一化退火正则化的联邦优化”（FedNAR），可以无缝地集成到任何现有的联邦学习算法中。基本上，我们通过对梯度和权重衰减进行联合裁剪来调节每次更新的大小。我们提供了一个全面的理论",
    "tldr": "本文提出了一种名为FedNAR的算法插件，它通过归一化退火正则化来调节每次更新的大小，从而解决了联邦学习中权重衰减对全局目标的优化目标的不同影响。"
}