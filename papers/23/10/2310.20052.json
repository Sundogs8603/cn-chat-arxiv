{
    "title": "Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning. (arXiv:2310.20052v1 [cs.AI])",
    "abstract": "Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning \"cross-task knowledge,\" where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is \"replay,\" where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is a",
    "link": "http://arxiv.org/abs/2310.20052",
    "context": "Title: Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning. (arXiv:2310.20052v1 [cs.AI])\nAbstract: Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning \"cross-task knowledge,\" where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is \"replay,\" where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is a",
    "path": "papers/23/10/2310.20052.json",
    "total_tokens": 860,
    "translated_title": "看我，不是重播！SurpriseNet：受异常检测启发的类别增量学习的异常检测。",
    "translated_abstract": "连续学习致力于创建人工神经网络，能够通过在一系列任务上的增量训练中积累知识和技能。连续学习的主要挑战是灾难性干扰，即新知识覆盖或干扰过去的知识，导致遗忘。与之相关的问题是学习“跨任务知识”，模型无法获取和保留有助于区分跨任务边界上的类别的知识。解决这两个问题的常见方法是“重播”，即利用有限的过去实例缓冲区来学习跨任务知识并减轻灾难性干扰。然而，这些方法的一个显著缺点是倾向于过度拟合有限的重播缓冲区。相比之下，我们提出的解决方案SurpriseNet通过采用参数隔离方法解决灾难性干扰，并使用受异常检测启发的自编码器学习跨任务知识。",
    "tldr": "SurpriseNet提出了一个解决灾难性干扰和跨任务知识学习问题的方案，通过参数隔离方法和受异常检测启发的自编码器来实现。",
    "en_tdlr": "SurpriseNet proposes a solution to the problems of catastrophic interference and cross-task knowledge learning, utilizing parameter isolation and an auto-encoder inspired by anomaly detection."
}