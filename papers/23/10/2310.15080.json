{
    "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
    "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improv",
    "link": "https://arxiv.org/abs/2310.15080",
    "context": "Title: Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization\nAbstract: Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improv",
    "path": "papers/23/10/2310.15080.json",
    "total_tokens": 834,
    "translated_title": "带有参数高效prompt调整和自适应优化的大型语言模型的联邦学习",
    "translated_abstract": "联邦学习是一种有前途的范式，可以实现分散数据的协同模型训练。然而，大型语言模型的训练过程通常涉及更新大量的参数，这限制了联邦学习技术在实际场景中处理大型语言模型的适用性。prompt调整可以显著减少需要更新的参数数量，但它可能导致性能下降或降低训练效率。在联邦学习中直接使用prompt调整通常会导致非平凡的通信成本和性能大幅下降。此外，分散数据通常是非独立和同分布的，并带来客户端漂移问题和因此的低性能。本文提出了一种参数高效的提示调整方法，即FedPepTAO，以实现大型语言模型的高效和有效的联邦学习。首先，提出了一种高效的部分提示调整方法来改善训练性能。",
    "tldr": "本文提出了一种带有参数高效prompt调整和自适应优化的联邦学习方法，以实现大型语言模型的高效和有效训练。",
    "en_tdlr": "This paper proposes a federated learning approach with parameter-efficient prompt tuning and adaptive optimization to achieve efficient and effective training of large language models."
}