{
    "title": "LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])",
    "abstract": "Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\\textbf{L}$ossl$\\textbf{E}$ss $\\textbf{MO}$del Expansio$\\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrat",
    "link": "http://arxiv.org/abs/2310.07999",
    "context": "Title: LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])\nAbstract: Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\\textbf{L}$ossl$\\textbf{E}$ss $\\textbf{MO}$del Expansio$\\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrat",
    "path": "papers/23/10/2310.07999.json",
    "total_tokens": 864,
    "translated_title": "LEMON：无损模型扩展",
    "translated_abstract": "深度神经网络（特别是Transformer）的扩展对于它们的出色性能至关重要，并且进一步导致了基础模型中复杂的推理能力的出现。这种扩展通常需要从头开始训练大型模型，并使用随机初始化，而无法利用已有的小型模型所获得的知识，这些小型模型已经耗费了大量资源。为了解决这种低效率问题，我们提出了无损模型扩展（LEMON），一种使用小型但已经预训练的模型的权重来初始化扩展模型的方法。然后，我们使用专门为扩展模型定制的优化学习率调度器进行模型训练，与从头训练相比，大大减少了训练时间。值得注意的是，LEMON具有通用性，能够与各种网络结构兼容，包括Vision Transformer和BERT等模型。我们的实证结果证明了LEMON的效果。",
    "tldr": "LEMON是一种无损模型扩展方法，在深度神经网络中能够通过利用小型预训练模型的知识来初始化和训练大型模型，从而大大减少训练时间，同时具有通用性适用于各种网络结构。",
    "en_tdlr": "LEMON is a lossless model expansion method that reduces training time in deep neural networks by initializing and training large models using knowledge from smaller pre-trained models, while being versatile and compatible with various network structures."
}