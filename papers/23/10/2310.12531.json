{
    "title": "ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])",
    "abstract": "Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.",
    "link": "http://arxiv.org/abs/2310.12531",
    "context": "Title: ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])\nAbstract: Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.",
    "path": "papers/23/10/2310.12531.json",
    "total_tokens": 959,
    "translated_title": "ICU：通过将任务划分为图像字幕和语言理解来克服视觉与语言建模中的语言障碍",
    "translated_abstract": "多语言视觉与语言(V&L)研究旨在在一个模型中实现多语言和多模态的能力。然而，图像的多语言字幕稀缺一直以来一直阻碍了该领域的发展。为了克服这个障碍，我们提出了ICU（Image Caption Understanding），将V&L任务分为两个阶段：一个V&L模型以英文进行图像字幕生成，然后一个多语言语言模型（mLM）以字幕作为替代文本进行跨语言语言理解。这种方式减轻了V&L模型的多语言处理负担，将其转移到了mLM上。由于多语言文本数据相对丰富和质量较高，ICU可以帮助克服V&L模型中的语言障碍。在IGLUE基准测试的两个任务中，涉及9种语言的实验中，我们展示了ICU可以在五种语言上实现新的最先进结果，并在其余语言上取得了可比较的结果。",
    "tldr": "ICU提出了一种解决视觉与语言建模中语言障碍的方法，通过将任务划分为图像字幕和语言理解两个阶段，将多语言处理负担转移到多语言语言模型上。实验结果显示，ICU在多个语言上取得了最先进的结果。",
    "en_tdlr": "ICUT proposes a method to overcome language barriers in vision-and-language modeling by dividing the task into image captioning and language understanding stages, transferring the burden of multilingual processing to a multilingual language model. Experimental results show that ICU achieves state-of-the-art performance in multiple languages."
}