{
    "title": "Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])",
    "abstract": "Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge",
    "link": "http://arxiv.org/abs/2310.08337",
    "context": "Title: Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])\nAbstract: Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge",
    "path": "papers/23/10/2310.08337.json",
    "total_tokens": 912,
    "translated_title": "神经扩散模型",
    "translated_abstract": "扩散模型在许多生成任务上表现出色。然而，尽管最近取得了一些成功，大多数扩散模型只允许对数据分布进行线性转换，受到了一定的限制。相比之下，更广泛的变换家族可能有助于更有效地训练生成分布，简化逆过程并缩小真实负对数似然和变分近似之间的差距。本文介绍了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在一个无需模拟的设置中使用变分界对NDMs进行优化。此外，我们导出了NDMs的时间连续形式，通过使用现成的数值ODE和SDE求解器，可以快速可靠地进行推理。最后，我们通过在标准图像生成任务上的实验展示了可学习变换的NDMs的实用性。",
    "tldr": "本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。",
    "en_tdlr": "This paper introduces Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimize NDMs using a variational bound in a simulation-free setting, and demonstrate the usefulness of NDMs with learnable transformations through experiments on standard image generation tasks."
}