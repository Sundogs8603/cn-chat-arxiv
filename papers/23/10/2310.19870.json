{
    "title": "Metric Flows with Neural Networks. (arXiv:2310.19870v1 [hep-th])",
    "abstract": "We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.",
    "link": "http://arxiv.org/abs/2310.19870",
    "context": "Title: Metric Flows with Neural Networks. (arXiv:2310.19870v1 [hep-th])\nAbstract: We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.",
    "path": "papers/23/10/2310.19870.json",
    "total_tokens": 814,
    "translated_title": "用神经网络实现的度量流",
    "translated_abstract": "我们发展了一种由神经网络梯度下降诱导的黎曼度量空间中流动的理论。这部分是受到近期用神经网络逼近Calabi-Yau度量的进展的推动，也是由于对神经网络空间中流动的理解的最新进展的能力。我们推导了相应的度量流动方程，其由度量神经切向核定义，这是一个复杂的非局部对象，会随时间演化。然而，许多结构在无穷宽度极限下核将变得固定且动态简化。附加假设可导致流动中的局部性，使得我们能够实现Perelman关于解决3D Poincaré猜想中使用的Ricci流的形式化。我们将这些思想应用于数值Calabi-Yau度量，包括关于特征学习重要性的讨论。",
    "tldr": "本论文开发了一种基于神经网络梯度下降的度量流理论，实现了在黎曼度量空间中的流动。其应用于数值Calabi-Yau度量，并探讨了特征学习的重要性。",
    "en_tdlr": "This paper develops a theory of metric flows induced by neural network gradient descent. It applies this theory to numerical Calabi-Yau metrics and discusses the importance of feature learning."
}