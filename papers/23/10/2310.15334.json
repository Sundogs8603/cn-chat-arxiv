{
    "title": "ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])",
    "abstract": "We design a series of serial and parallel proximal point (gradient) ADMMs for the fully connected residual networks (FCResNets) training problem by introducing auxiliary variables. Convergence of the proximal point version is proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we can ensure a locally R-linear or sublinear convergence rate depending on the different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary auxiliary function is constructed to realize our goal. Moreover, the advantages of the parallel implementation in terms of lower time complexity and less (per-node) memory consumption are analyzed theoretically. To the best of our knowledge, this is the first work analyzing the convergence, convergence rate, time complexity and (per-node) runtime memory requirement of the ADMM applied in the FCResNets training problem theoretically. Experiments are reported to show the high speed, better performance, robustness and potential in the ",
    "link": "http://arxiv.org/abs/2310.15334",
    "context": "Title: ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])\nAbstract: We design a series of serial and parallel proximal point (gradient) ADMMs for the fully connected residual networks (FCResNets) training problem by introducing auxiliary variables. Convergence of the proximal point version is proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we can ensure a locally R-linear or sublinear convergence rate depending on the different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary auxiliary function is constructed to realize our goal. Moreover, the advantages of the parallel implementation in terms of lower time complexity and less (per-node) memory consumption are analyzed theoretically. To the best of our knowledge, this is the first work analyzing the convergence, convergence rate, time complexity and (per-node) runtime memory requirement of the ADMM applied in the FCResNets training problem theoretically. Experiments are reported to show the high speed, better performance, robustness and potential in the ",
    "path": "papers/23/10/2310.15334.json",
    "total_tokens": 914,
    "translated_title": "ADMM训练算法用于残差网络：收敛性，复杂度和并行训练",
    "translated_abstract": "我们通过引入辅助变量，设计了一系列序列和并行的近端点（梯度）ADMM算法来解决完全连接的残差网络（FCResNets）训练问题。通过基于Kurdyka-Lojasiewicz（KL）属性分析框架的证明，我们证明了近端点版本的收敛性，并且可以确保在不同的Kurdyka-Lojasiewicz（KL）指数范围内，实现局部R-线性或亚线性收敛速度。此外，我们从理论上分析了并行实现在时间复杂性和（每个节点的）内存消耗方面的优势。据我们所知，这是第一个从理论上分析应用于FCResNets训练问题的ADMM算法的收敛性，收敛速度，时间复杂性和（每个节点的）内存需求的工作。我们报告了实验结果，展示了高速度，更好的性能，鲁棒性和潜力。",
    "tldr": "本论文设计了一系列序列和并行的ADMM算法解决残差网络训练问题，并通过理论分析证明了算法的收敛性和收敛速度，同时分析了并行实现的时间复杂性和内存消耗的优势。",
    "en_tdlr": "This paper presents a series of serial and parallel ADMM algorithms for training residual networks, providing theoretical analysis of their convergence, convergence rate, time complexity, and memory consumption. The advantages of parallel implementation in terms of time complexity and memory consumption are also highlighted."
}