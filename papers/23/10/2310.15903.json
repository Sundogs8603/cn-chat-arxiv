{
    "title": "Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)",
    "abstract": "We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the \"pick-all-label\" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic",
    "link": "http://arxiv.org/abs/2310.15903",
    "context": "Title: Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)\nAbstract: We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the \"pick-all-label\" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic",
    "path": "papers/23/10/2310.15903.json",
    "total_tokens": 926,
    "translated_title": "多标签学习中的神经坍缩问题研究",
    "translated_abstract": "我们通过神经坍缩（NC）的角度研究了深度神经网络在多标签分类（MLab）任务中的应用。之前的研究都局限于多类别分类，发现了一种普遍存在的NC现象，其中最后一层特征具有以下特点：（i）每个类别内的特征变异性为零，（ii）特征均值集合构成一个等角紧框架（ETF），（iii）最后一层分类器收缩到特征均值乘以某个缩放因子。我们将这个研究推广到多标签学习，并首次证明了“选择所有标签”公式存在广义NC现象。在自然的无约束特征模型（UFM）的情况下，我们证明了“选择所有标签”的交叉熵损失函数的全局分类器只显示出相同的ETF几何结构，进一步坍缩到多重性为1的特征类均值。",
    "tldr": "这项论文研究了在多标签分类任务中的神经坍缩现象。他们推广了之前在多类别分类中发现的神经坍缩现象，证明了在“选择所有标签”公式下存在广义的神经坍缩现象。他们还发现了在广义的神经坍缩中的一个组合性质。",
    "en_tdlr": "This paper investigates the neural collapse phenomenon in multi-label learning. They generalize the previous findings in multi-class classification to show the existence of a generalized neural collapse phenomenon in the \"pick-all-label\" formulation. They also discover a combinatorial property in this generalized neural collapse."
}