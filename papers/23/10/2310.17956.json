{
    "title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])",
    "abstract": "Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.",
    "link": "http://arxiv.org/abs/2310.17956",
    "context": "Title: Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])\nAbstract: Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.",
    "path": "papers/23/10/2310.17956.json",
    "total_tokens": 1031,
    "translated_title": "面向普遍医疗保健的中国大型视觉-语言模型Qilin-Med-VL",
    "translated_abstract": "大型语言模型(LLMs)在理解复杂的医疗保健和生物医学主题方面取得了新的突破。然而，除了英语之外，其他语言的模型以及能够解释多模态输入的模型相对较少，而这对于全球医疗保健的可访问性至关重要。为此，本研究介绍了Qilin-Med-VL，这是第一个设计用于整合文本和视觉数据分析的中国大型视觉-语言模型。Qilin-Med-VL将经过预训练的视觉Transformer（ViT）与基础LLM相结合。它经历了一个深入的两阶段课程训练过程，其中包括特征对齐和指导调优。这种方法增强了模型生成医疗标题和回答复杂医疗查询的能力。我们还发布了ChiMed-VL，这是一个包含超过100万个图像-文本对的数据集。该数据集经过精心策划，可以使用各种类型的图像进行详细和全面的医学数据解释。",
    "tldr": "Qilin-Med-VL是面向普遍医疗保健的中国大型视觉-语言模型，它结合了预训练的视觉Transformer和基础语言模型，通过两阶段课程训练过程提高了生成医疗标题和回答复杂医疗查询的能力，并发布了一个包含超过100万个图像-文本对的数据集ChiMed-VL。",
    "en_tdlr": "Qilin-Med-VL is a Chinese large vision-language model designed for general healthcare. It combines a pre-trained Vision Transformer with a foundational Language Model and undergoes a two-stage curriculum training process to enhance the ability to generate medical captions and answer complex medical queries. The dataset ChiMed-VL, consisting of over 1 million image-text pairs, is also released to enable detailed interpretation of medical data using various types of images."
}