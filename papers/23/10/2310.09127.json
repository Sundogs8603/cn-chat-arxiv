{
    "title": "On Generalization Bounds for Projective Clustering. (arXiv:2310.09127v1 [cs.LG])",
    "abstract": "Given a set of points, clustering consists of finding a partition of a point set into $k$ clusters such that the center to which a point is assigned is as close as possible. Most commonly, centers are points themselves, which leads to the famous $k$-median and $k$-means objectives. One may also choose centers to be $j$ dimensional subspaces, which gives rise to subspace clustering. In this paper, we consider learning bounds for these problems. That is, given a set of $n$ samples $P$ drawn independently from some unknown, but fixed distribution $\\mathcal{D}$, how quickly does a solution computed on $P$ converge to the optimal clustering of $\\mathcal{D}$? We give several near optimal results. In particular,  For center-based objectives, we show a convergence rate of $\\tilde{O}\\left(\\sqrt{{k}/{n}}\\right)$. This matches the known optimal bounds of [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] and [Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998] fo",
    "link": "http://arxiv.org/abs/2310.09127",
    "context": "Title: On Generalization Bounds for Projective Clustering. (arXiv:2310.09127v1 [cs.LG])\nAbstract: Given a set of points, clustering consists of finding a partition of a point set into $k$ clusters such that the center to which a point is assigned is as close as possible. Most commonly, centers are points themselves, which leads to the famous $k$-median and $k$-means objectives. One may also choose centers to be $j$ dimensional subspaces, which gives rise to subspace clustering. In this paper, we consider learning bounds for these problems. That is, given a set of $n$ samples $P$ drawn independently from some unknown, but fixed distribution $\\mathcal{D}$, how quickly does a solution computed on $P$ converge to the optimal clustering of $\\mathcal{D}$? We give several near optimal results. In particular,  For center-based objectives, we show a convergence rate of $\\tilde{O}\\left(\\sqrt{{k}/{n}}\\right)$. This matches the known optimal bounds of [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] and [Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998] fo",
    "path": "papers/23/10/2310.09127.json",
    "total_tokens": 898,
    "translated_title": "关于投影聚类的泛化界限",
    "translated_abstract": "给定一组点，聚类是将点集分成k个簇的过程，使得每个点被分配到的中心尽可能接近。最常见的是将中心点设置为点本身，这导致了著名的k-median和k-means目标。也可以选择将中心点设置为j维子空间，从而产生子空间聚类。本文考虑了这些问题的学习界限。也就是说，给定一个从某个未知但固定分布D中独立抽取的n个样本P集合，P上计算的解如何快速收敛到D的最佳聚类？我们给出了几个近乎最优的结果。特别地，对于基于中心的目标，我们展示了收敛速率为O(sqrt(k/n))，与已知的最优界限[Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016]和[Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998]相一致。",
    "tldr": "本文研究了投影聚类的学习界限问题，给出了几个近乎最优的结果，并且对于基于中心的目标，展示了收敛速率为O(sqrt(k/n))。",
    "en_tdlr": "This paper investigates the learning bounds for projective clustering and provides several near optimal results. For center-based objectives, a convergence rate of O(sqrt(k/n)) is shown."
}