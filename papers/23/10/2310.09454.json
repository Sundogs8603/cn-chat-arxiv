{
    "title": "LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents. (arXiv:2310.09454v1 [cs.AI])",
    "abstract": "Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the st",
    "link": "http://arxiv.org/abs/2310.09454",
    "context": "Title: LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents. (arXiv:2310.09454v1 [cs.AI])\nAbstract: Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the st",
    "path": "papers/23/10/2310.09454.json",
    "total_tokens": 903,
    "translated_title": "LgTS：使用LLM生成的子目标进行动态任务采样，用于增强学习代理",
    "translated_abstract": "最近，大型语言模型（LLM）在机器人和人工智能代理需要高级规划的问题中的推理能力方面取得了重大进展。然而，目前利用LLM进行此类规划任务的技术存在一些关键假设，比如需要访问允许微调的数据集，需要精心设计的提示信息仅向LLM提供相关且必要的信息，以及必须采用确定性方法执行LLM的响应，例如使用现有策略或计划操作。在本研究中，我们提出了LgTS（LLM引导的师生学习）这一全新的方法，它探索了LLM的规划能力，为无法访问环境转换动力学的增强学习（RL）代理提供了子目标的图形表示。RL代理利用师生学习算法学习一组成功达到目标状态的策略。",
    "tldr": "本研究提出了一种名为LgTS的新方法，利用LLM的规划能力，为无法访问环境转换动力学的RL代理提供子目标的图形表示。这个方法可以教导RL代理学习一组成功的策略来达到目标状态。",
    "en_tdlr": "This paper introduces a novel approach called LgTS that utilizes the planning abilities of LLMs to provide a graphical representation of sub-goals to RL agents that do not have access to the transition dynamics of the environment. The approach enables the RL agents to learn a set of successful policies for reaching the goal state."
}