{
    "title": "GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])",
    "abstract": "With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention ",
    "link": "http://arxiv.org/abs/2310.03030",
    "context": "Title: GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])\nAbstract: With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention ",
    "path": "papers/23/10/2310.03030.json",
    "total_tokens": 902,
    "translated_title": "GPT-MolBERTa：用于分子性质预测的GPT分子特征语言模型",
    "translated_abstract": "随着Transformer架构的出现及其对文本数据的强大理解能力，基于文本描述预测分子属性的新领域已经开启。尽管SMILES是最常见的表示形式，但它们缺乏健壮性、丰富信息和规范性，限制了它们成为可推广表示的有效性。在这里，我们提出了GPT-MolBERTa，一种自监督大型语言模型（LLM），它使用分子的详细文本描述来预测其性质。使用ChatGPT收集了326000个分子的基于文本的描述，并用于训练LLM来学习分子的表示。为了预测下游任务的性能，细调阶段使用了BERT和RoBERTa模型。实验证明，GPT-MolBERTa在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。此外，对注意力进行了进一步分析。",
    "tldr": "GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。"
}