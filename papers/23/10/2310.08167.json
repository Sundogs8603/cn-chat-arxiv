{
    "title": "Multiclass Classification of Policy Documents with Large Language Models. (arXiv:2310.08167v1 [cs.CL])",
    "abstract": "Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on G",
    "link": "http://arxiv.org/abs/2310.08167",
    "context": "Title: Multiclass Classification of Policy Documents with Large Language Models. (arXiv:2310.08167v1 [cs.CL])\nAbstract: Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on G",
    "path": "papers/23/10/2310.08167.json",
    "total_tokens": 939,
    "translated_title": "使用大规模语言模型进行政策文件的多类别分类",
    "translated_abstract": "将政策文件按照政策议题进行分类一直以来是政治科学和传播学领域的长期努力。为了实现社会科学研究目的的文本分类自动化处理，已经取得了显著的成果，但仍有很大的进展空间。在本研究中，我们测试了一种替代策略的预测性能，该策略需要比完全手动编码少得多的人工参与。我们使用OpenAI的GPT 3.5和GPT 4模型，这些模型是经过预训练并针对指令进行调整的大型语言模型，将国会议案和国会听证会分类为Comparative Agendas Project的21个主要政策议题。我们提出了三种使用场景，并估计了根据所采用的场景和GPT模型的整体准确率在58％至83％之间的范围。这三种情景分别旨在实现对人工干预的最小、中度和重要程度。总的来说，我们的结果指向了完全依赖GPT模型的不足之处。",
    "tldr": "这项工作使用GPT 3.5和GPT 4等大型语言模型对政策文件进行多类别分类，提出了三种使用场景，并估计了整体准确率在58％至83％之间。结果表明，完全依赖语言模型仍存在不足之处。"
}