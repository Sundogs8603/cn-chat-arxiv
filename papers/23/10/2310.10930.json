{
    "title": "Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])",
    "abstract": "Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.",
    "link": "http://arxiv.org/abs/2310.10930",
    "context": "Title: Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])\nAbstract: Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.",
    "path": "papers/23/10/2310.10930.json",
    "total_tokens": 696,
    "translated_title": "增强的Transformer架构用于自然语言处理",
    "translated_abstract": "Transformer是自然语言处理领域中最先进的模型。当前的NLP模型主要通过增加transformer的数量来提高处理性能。然而，这种技术需要大量的训练资源，如计算能力。本文提出了一种新颖的Transformer结构，具有全层标准化、加权残差连接、利用强化学习的位置编码和零掩码自注意力等特点。提出的增强Transformer模型通过使用Multi30k翻译数据集的双语评估实验得到的BLEU分数进行验证。结果表明，与原始Transformer相比，增强Transformer的BLEU分数提高了202.96%。",
    "tldr": "增强的Transformer引入了全层标准化、加权残差连接、强化学习位置编码和零掩码自注意力等创新技术，通过在Multi30k翻译数据集上验证，相比于原始Transformer，实现了202.96%的BLEU分数提升。"
}