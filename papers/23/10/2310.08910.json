{
    "title": "Scalarization for Multi-Task and Multi-Domain Learning at Scale. (arXiv:2310.08910v1 [cs.LG])",
    "abstract": "Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and ",
    "link": "http://arxiv.org/abs/2310.08910",
    "context": "Title: Scalarization for Multi-Task and Multi-Domain Learning at Scale. (arXiv:2310.08910v1 [cs.LG])\nAbstract: Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and ",
    "path": "papers/23/10/2310.08910.json",
    "total_tokens": 911,
    "translated_title": "在规模化的多任务和多领域学习中的标量化方法",
    "translated_abstract": "在多个输入领域和/或输出任务上训练单个模型可以将多个信息源压缩为统一的主干，从而提高模型效率。它还可以实现任务/领域之间的正向知识转移，从而提高准确性和数据有效性。然而，优化这样的网络是一个挑战，特别是由于不同任务或领域之间的差异：尽管多年来提出了几个假设和解决方案，但最近的研究表明，统一的标量化训练，即简单地最小化任务损失的平均值，与更昂贵的最新优化方法相比，性能相当。这引发了我们对多任务和多领域网络的训练动态理解程度的问题。在这项工作中，我们首先设计了一个大规模的统一分析多领域和多任务学习，以更好地理解标量化在各种任务/领域组合中的动态性。",
    "tldr": "通过标量化方法在多任务和多领域学习中训练单个模型可以提高模型效率和准确性，并实现跨任务/领域的正向知识转移。该研究通过大规模分析多领域和多任务学习的动态性来更好地理解标量化方法的训练动态。",
    "en_tdlr": "Training a single model using scalarization method in multi-task and multi-domain learning can improve model efficiency and accuracy, and enable positive knowledge transfer across tasks and domains. This research provides a large-scale analysis of the dynamics of multi-domain and multi-task learning to better understand the training dynamics of scalarization method."
}