{
    "title": "Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. (arXiv:2310.17966v1 [cs.LG])",
    "abstract": "Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a",
    "link": "http://arxiv.org/abs/2310.17966",
    "context": "Title: Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. (arXiv:2310.17966v1 [cs.LG])\nAbstract: Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a",
    "path": "papers/23/10/2310.17966.json",
    "total_tokens": 990,
    "translated_title": "一次训练，获得一个家庭：离线到在线强化学习的状态自适应平衡",
    "translated_abstract": "离线到在线强化学习是一种训练范式，它将预先收集的数据集上的预训练与在线环境中的微调相结合。然而，引入在线微调可能会加剧已知的分布偏移问题。现有的解决方案通过对离线和在线学习中的政策改进目标施加策略约束来解决这个问题。它们通常主张在不同的数据集上采用一种平衡政策改进和约束的通用方法。然而，这种一刀切的方式可能无法充分利用每个收集到的样本，因为不同状态的数据质量变化很大。为此，我们引入了家族离线到在线强化学习(FamO2O)，这是一个简单而有效的框架，能够赋予现有算法确定状态自适应改进-约束平衡的能力。FamO2O利用一个通用模型训练一个家族的策略，每个策略具有不同的改进/约束强度，和一个",
    "tldr": "在离线到在线强化学习中，现有解决方案往往只使用一种平衡策略，无法充分利用不同状态的数据质量。本论文提出了一种家族式离线到在线强化学习框架(FamO2O)，它通过训练一系列具有不同改进和约束强度的策略，实现了状态自适应的平衡。"
}