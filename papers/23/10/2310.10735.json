{
    "title": "Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning. (arXiv:2310.10735v1 [cs.CL])",
    "abstract": "Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show",
    "link": "http://arxiv.org/abs/2310.10735",
    "context": "Title: Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning. (arXiv:2310.10735v1 [cs.CL])\nAbstract: Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show",
    "path": "papers/23/10/2310.10735.json",
    "total_tokens": 914,
    "translated_title": "使用离线强化学习构建个性一致的对话代理",
    "translated_abstract": "保持一致的个性是任何开放领域对话系统的关键品质。目前的最先进系统通过使用监督学习或在线强化学习来训练代理，以实现个性一致性。然而，使用监督学习训练的系统往往缺乏一致性，因为它们从来没有因言之不准而受到惩罚。使用强化学习进行额外训练可以缓解其中一些问题，但是训练过程很昂贵。相反，我们提出了一个离线强化学习框架来改善对话系统的个性一致性。我们的框架使我们能够将以前的方法的优点结合起来，既可以像监督学习那样廉价地在现有数据上训练模型，又可以像强化学习那样对特定发言进行奖惩。我们还引入了一种简单的重要性采样方法，以减少离线强化学习训练中的重要性权重方差，我们将其称为方差减少MLE初始化（VaRMI）重要性采样。我们的自动评估和人工评估显示",
    "tldr": "本研究提出了一种离线强化学习框架，通过廉价地在现有数据上训练模型并对特定发言进行奖惩，提高对话系统的个性一致性。",
    "en_tdlr": "This study proposes an offline reinforcement learning framework to improve the persona consistency of dialogue systems by training the model on existing data and punishing/rewarding specific utterances."
}