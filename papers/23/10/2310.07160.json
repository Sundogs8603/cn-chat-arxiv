{
    "title": "LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])",
    "abstract": "Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code",
    "link": "http://arxiv.org/abs/2310.07160",
    "context": "Title: LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])\nAbstract: Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code",
    "path": "papers/23/10/2310.07160.json",
    "total_tokens": 902,
    "translated_title": "LLark: 一种用于音乐的多模态基础模型",
    "translated_abstract": "音乐具有独特且复杂的结构，对于专业人士和现有的AI系统来说都具有挑战性，并相对于其他形式的音频呈现出独特的挑战。我们提出了LLark，一种针对音乐理解的指令调谐多模型模型。我们详细介绍了我们的数据集创建过程，其中包括增强多样化的开源音乐数据集的注释，并将其转换为统一的指令调谐格式。我们提出了一种多模态架构用于LLark，将预训练的音乐生成模型与预训练的语言模型相结合。在对三种类型的任务（音乐理解、字幕生成和推理）进行评估时，我们展示了我们的模型在音乐理解的零样本泛化上与现有基准模型相匹配或超出，并且在字幕生成和推理任务中人类与模型的响应显示出高度一致性。LLark完全是根据开源音乐数据和模型进行训练的，并且我们公开了我们的训练代码。",
    "tldr": "LLark是一个通过多模态架构实现音乐理解的模型，能够在零样本泛化上匹配或超出现有基准模型，在字幕生成和推理任务中与人类响应高度一致。",
    "en_tdlr": "LLark is a multimodal model for music understanding, which achieves matching or outperforming existing baselines in zero-shot generalization and shows high consistency with human responses in captioning and reasoning tasks."
}