{
    "title": "LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])",
    "abstract": "Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \\%$ improvement in perplexity over our baselines for noisy channels.",
    "link": "http://arxiv.org/abs/2310.13033",
    "context": "Title: LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])\nAbstract: Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \\%$ improvement in perplexity over our baselines for noisy channels.",
    "path": "papers/23/10/2310.13033.json",
    "total_tokens": 896,
    "translated_title": "LASER：无线分布式优化中的线性压缩",
    "translated_abstract": "数据并行SGD是分布式优化的事实上的算法，尤其适用于大规模机器学习。尽管它有很多优点，但通信瓶颈是其中持久存在的问题之一。大多数压缩方案要么假设通信链路无噪声，要么在实际任务中无法取得良好的性能。在本文中，我们填补了这一空白，介绍了LASER：无线分布式优化中的线性压缩。LASER利用梯度的固有低秩结构，在噪声通道上高效传输梯度。尽管享受与经典SGD相似的理论保证，LASER在各种实际基准测试中表现出持续的优势。特别是，在具有挑战性的计算机视觉和GPT语言建模任务中，它优于最先进的压缩方案。在后者中，我们相对于噪声通道上的基准模型在困惑度上获得了50-64%的提升。",
    "tldr": "LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。",
    "en_tdlr": "LASER is a new compression scheme that efficiently transmits gradients over noisy channels by exploiting the low-rank structure of the gradients, showing consistent advantages over existing schemes in computer vision and GPT language modeling tasks."
}