{
    "title": "Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-",
    "link": "http://arxiv.org/abs/2310.12558",
    "context": "Title: Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-",
    "path": "papers/23/10/2310.12558.json",
    "total_tokens": 971,
    "translated_title": "大语言模型帮助人类验证真实性——除非它们令人信服地错误。",
    "translated_abstract": "大语言模型（LLMs）越来越多地被用于获取网络上的信息。因此，它们的真实性和事实性备受关注。为了帮助用户做出正确的信息决策，LLMs不仅应提供信息，还应帮助用户事实核查。本文通过与80名众包工作者进行实验，比较了语言模型与搜索引擎（信息检索系统）在帮助人类用户事实核查方面的效果。我们引导LLMs验证给定的声明并提供相应的解释。与使用准确率相似的搜索引擎相比，阅读LLM的解释的用户效率显著提高。然而，当解释错误时，他们往往过度依赖LLMs。为了减少对LLMs的过度依赖，我们要求LLMs提供对比信息，解释为什么声明为真和为假，并将两方面的解释呈现给用户。这种对比解释减轻了用户的过度依赖。",
    "tldr": "本研究比较了语言模型与搜索引擎在帮助用户事实核查方面的效果。结果显示，用户阅读语言模型的解释比使用搜索引擎更高效，但当解释错误时，用户容易过度依赖语言模型。为了减少过度依赖，研究提出了使用对比信息进行解释的方法。",
    "en_tdlr": "This study compares the effectiveness of language models and search engines in assisting users with fact-checking. The results show that users are more efficient when reading explanations from language models compared to using search engines, but they tend to over-rely on language models when the explanations are wrong. To mitigate over-reliance, the study proposes using contrastive information for explanations."
}