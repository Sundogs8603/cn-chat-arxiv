{
    "title": "Thin and Deep Gaussian Processes. (arXiv:2310.11527v1 [stat.ML])",
    "abstract": "Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis",
    "link": "http://arxiv.org/abs/2310.11527",
    "context": "Title: Thin and Deep Gaussian Processes. (arXiv:2310.11527v1 [stat.ML])\nAbstract: Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis",
    "path": "papers/23/10/2310.11527.json",
    "total_tokens": 833,
    "translated_title": "薄而深的高斯过程",
    "translated_abstract": "高斯过程（GPs）可以提供一种可靠的方法来量化不确定性，具有易于解释的内核超参数，如长度尺度，可以控制函数值的相关距离。然而，选择合适的内核可能具有挑战性。深度高斯过程通过逐层参数化GP层的内核，避免了手动内核工程，使其能够学习解释输出数据的低维嵌入方法。沿用深度神经网络的架构，最常见的深度高斯过程逐层变形输入空间，但失去了浅层高斯过程的所有解释性。另一种构建方法是逐层参数化内核的长度尺度，提高了解释性，但最终放弃了学习低维嵌入的概念。不幸的是，这两种方法都容易受到特定的病态影响，可能会阻碍拟合并限制其可解释性。本文提出了一种新的综合方法",
    "tldr": "本文提出了一种薄而深的高斯过程方法，克服了传统方法的局限性，并在学习低维嵌入和解释性之间取得了平衡。",
    "en_tdlr": "This paper proposes a thin and deep Gaussian process approach that overcomes the limitations of traditional methods and strikes a balance between learning low-dimensional embeddings and interpretability."
}