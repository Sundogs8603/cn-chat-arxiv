{
    "title": "Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning. (arXiv:2310.01366v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients' data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-side computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily ",
    "link": "http://arxiv.org/abs/2310.01366",
    "context": "Title: Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning. (arXiv:2310.01366v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients' data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-side computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily ",
    "path": "papers/23/10/2310.01366.json",
    "total_tokens": 972,
    "translated_title": "基于窗口的模型平均改善异构联邦学习的泛化能力",
    "translated_abstract": "联邦学习旨在从分布式用户中学习一个全局模型，并保护他们的隐私。然而，当数据分布异构时，学习过程变得嘈杂、不稳定，并且倾向于最后一次观察到的客户端数据，从而减慢收敛速度。为了解决这些问题并提高全局模型的鲁棒性和泛化能力，我们提出了基于窗口的模型平均方法（WIMA）。WIMA使用基于窗口的方法来聚合不同轮次的全局模型，有效地捕获多个用户的知识并减少最后几个用户的偏差。通过采用窗口视图来处理轮次，WIMA可以从训练的初始阶段就应用。重要的是，我们的方法不引入额外的通信或客户端计算开销。我们的实验表明，WIMA对分布偏移和不好的客户端采样具有鲁棒性，结果呈现出更平滑和更稳定的学习趋势。此外，WIMA可以轻松地进行扩展和集成到现有的联邦学习框架中。",
    "tldr": "基于窗口的模型平均方法(WIMA)通过聚合不同轮次的全局模型，有效捕获多个用户的知识，减少偏差，提高了异构联邦学习的泛化能力和稳定性，无需额外的通信或客户端计算开销。",
    "en_tdlr": "Window-based Model Averaging (WIMA) improves the generalization and stability of heterogeneous federated learning by aggregating global models from different rounds, effectively capturing knowledge from multiple users and reducing bias, without additional communication or client-side computation overhead."
}