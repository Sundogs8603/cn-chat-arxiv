{
    "title": "The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \\textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particul",
    "link": "http://arxiv.org/abs/2310.11877",
    "context": "Title: The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])\nAbstract: Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \\textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particul",
    "path": "papers/23/10/2310.11877.json",
    "total_tokens": 973,
    "translated_title": "大型语言模型中幻觉性无法回答性的好奇案例：在过度自信的隐藏状态中寻找真理",
    "translated_abstract": "大型语言模型(LLMs)展示了令人印象深刻的能力，同时也引发了对其回答准确性的关键担忧。在这个背景下出现的一个主要问题是LLMs如何处理无法回答的查询，往往会导致幻觉行为，原因是过度自信。在本文中，我们探讨了LLMs面对无法回答的查询时的行为。我们问：当生成幻觉回答时，模型是否表示问题无法回答的事实？我们的结果强烈表明，这样的模型对输入查询的可回答性进行编码，第一个解码的标记的表示往往是一个强有力的指示符。这些发现揭示了LLMs潜在表示中的空间组织，揭示了这些模型先前未被探索的方面。此外，它们为开发更好地遵守事实生成的改进解码技术铺平了道路。",
    "tldr": "本研究探讨了大型语言模型(LLMs)在面对无法回答的查询时的行为，发现模型能够编码查询的可回答性，并且第一个解码的标记是一个强有力的指示符。这些发现揭示了LLMs潜在表示中的空间组织，并为改进解码技术提供了新的思路。"
}