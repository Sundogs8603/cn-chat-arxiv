{
    "title": "VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores. (arXiv:2310.02065v1 [cs.DC])",
    "abstract": "The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no los",
    "link": "http://arxiv.org/abs/2310.02065",
    "context": "Title: VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores. (arXiv:2310.02065v1 [cs.DC])\nAbstract: The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no los",
    "path": "papers/23/10/2310.02065.json",
    "total_tokens": 975,
    "translated_title": "VENOM：一种用于释放稀疏张量核心巨大潜力的矢量化N:M格式",
    "translated_abstract": "深度学习模型的成功和规模扩大要求更高的计算效率和功率。稀疏化可以实现模型更小和更高的计算效率，并且可以利用加速硬件。然而，要有效地利用稀疏向量单元的硬件支持，需要进行核心实现、修剪算法和存储格式。其中一个例子是NVIDIA的稀疏张量核心（SPTC），可以实现2倍的加速。然而，SPTC只支持2:4格式，限制了可实现的稀疏比率为50%。我们提出了V:N:M格式，可以在SPTC上执行任意N:M比率。为了有效地利用新格式，我们提出了Spatha，一个针对深度学习例程的高性能稀疏库。我们展示了Spatha比cuBLAS实现了高达37倍的加速。我们还展示了一种二阶修剪技术，可以实现高稀疏比率的V:N:M形式，并几乎没有损失。",
    "tldr": "VENOM提出了一种新的V:N:M格式，可以在NVIDIA的稀疏张量核心上实现任意N:M比率。使用Spatha稀疏库，可以实现高达37倍的加速，并且展示了一种二阶修剪技术来实现高稀疏比率的V:N:M形式，并几乎没有损失。",
    "en_tdlr": "VENOM introduces a new V:N:M format for achieving arbitrary N:M ratios on NVIDIA's Sparse Tensor Cores. Leveraging the Spatha sparse library, it achieves up to 37x speedup and demonstrates a second-order pruning technique for achieving high sparsity ratios with V:N:M format without significant loss."
}