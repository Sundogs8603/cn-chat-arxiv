{
    "title": "Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve",
    "link": "http://arxiv.org/abs/2310.06341",
    "context": "Title: Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve",
    "path": "papers/23/10/2310.06341.json",
    "total_tokens": 889,
    "translated_title": "减少信息泄漏和计算的联邦学习",
    "translated_abstract": "联邦学习是一种分布式学习范式，允许多个分散的客户端在不共享本地数据的情况下共同学习一个公共模型。尽管本地数据没有直接暴露，但仍存在隐私问题，因为客户端的敏感信息可以从中间计算中推断出来。此外，随着相同数据在迭代学习过程中的重复使用，这种信息泄漏会不断积累。因此，在设计保护隐私的联邦学习算法时，很难平衡隐私和准确性之间的权衡。在本文中，我们引入了一种新的联邦学习框架Upcycled-FL，它在每个偶数迭代中都应用了一阶近似。在这个框架下，一半的联邦学习更新不会造成信息泄漏，并且需要更少的计算。我们首先对Upcycled-FL的收敛（速率）进行理论分析，然后应用扰动机制来保护隐私。",
    "tldr": "Upcycled-FL是一种减少信息泄漏和计算的联邦学习框架，在每个偶数迭代中应用一阶近似，使得一半的联邦学习更新不会泄漏信息并且需要更少的计算。",
    "en_tdlr": "Upcycled-FL is a federated learning framework that reduces information leakage and computation by applying a first-order approximation at every even iteration, resulting in half of the updates not leaking information and requiring less computation."
}