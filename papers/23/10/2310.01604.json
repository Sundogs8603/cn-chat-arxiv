{
    "title": "Solving the Quadratic Assignment Problem using Deep Reinforcement Learning. (arXiv:2310.01604v1 [cs.LG])",
    "abstract": "The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary",
    "link": "http://arxiv.org/abs/2310.01604",
    "context": "Title: Solving the Quadratic Assignment Problem using Deep Reinforcement Learning. (arXiv:2310.01604v1 [cs.LG])\nAbstract: The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary",
    "path": "papers/23/10/2310.01604.json",
    "total_tokens": 851,
    "translated_title": "使用深度强化学习解决二次分配问题",
    "translated_abstract": "二次分配问题 (QAP) 是一个 NP 困难问题，对其进行解决一直是一个挑战：与旅行推销员问题 (TSP) 等其他组合问题不同，使用先进的整数规划技术可以在包含数百甚至数千个位置的实例上精确解决，尚未找到解决大小超过30的 QAP 实例的方法。然而，解决 QAP 问题至关重要，因为它具有许多关键的应用，如电子布线设计和设备布局选择。我们提出了一种使用深度强化学习解决 QAP 问题的方法。我们的方法依赖于一种新颖的双指针网络，这个网络在选择下一个位置来放置设施和选择前一个位置来放置设施之间进行交替。我们使用合成实例的大型数据集在 A2C 上训练我们的模型，产生不需要特定实例重新训练的解决方案。",
    "tldr": "本文使用深度强化学习方法解决二次分配问题，提出了一种新颖的双指针网络，可以在没有特定实例重新训练的情况下产生解决方案。"
}