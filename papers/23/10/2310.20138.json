{
    "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])",
    "abstract": "Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate th",
    "link": "http://arxiv.org/abs/2310.20138",
    "context": "Title: DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])\nAbstract: Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate th",
    "path": "papers/23/10/2310.20138.json",
    "total_tokens": 921,
    "translated_title": "DEPN: 检测和编辑预训练语言模型中的隐私神经元",
    "translated_abstract": "在大规模数据上预训练的语言模型可以捕捉到丰富的知识和信息，但先前的研究揭示了其对数据记忆和重复的能力带来了数据泄露的风险。为了有效降低这些风险，我们提出了一个名为DEPN的框架，用于检测和编辑预训练语言模型中的隐私神经元，部分受到知识神经元和模型编辑的启发。在DEPN中，我们引入了一种称为隐私神经元检测器的新方法，用于定位与隐私信息相关的神经元，然后通过将它们的激活设置为零来编辑这些检测到的隐私神经元。此外，我们提出了一种隐私神经元聚合器，以批处理方式去除隐私信息。实验结果表明，我们的方法能够显著有效地降低私人数据泄露的风险，而不会降低模型的性能。",
    "tldr": "本论文提出了一个框架 DEPN，用于检测和编辑预训练语言模型中的隐私神经元。通过引入隐私神经元检测器和隐私神经元聚合器，我们能够有效降低私人数据泄露的风险，并且不会影响模型的性能。"
}