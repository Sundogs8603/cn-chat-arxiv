{
    "title": "Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])",
    "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
    "link": "http://arxiv.org/abs/2310.16960",
    "context": "Title: Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])\nAbstract: Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
    "path": "papers/23/10/2310.16960.json",
    "total_tokens": 835,
    "translated_title": "通过强化学习实现隐私保护的语言模型对齐",
    "translated_abstract": "在预训练和用户部署之间，通过强化学习对齐大型语言模型(LLMs)已经成为培训指令跟踪模型(如ChatGPT)的主流策略。本文在强化学习的基础上，引入差分隐私(DP)来研究隐私保护的LLMs对齐问题。我们研究了两种主要的范式：(i)不需要人工干预的强化学习对齐方法(如积极评价生成)，(ii)通过人类反馈的强化学习对齐方法(RLHF)(如以人类首选方式进行摘要生成)。我们提出了一种新的DP框架来实现强化学习的对齐，并证明了其正确性。实验结果验证了我们方法的有效性，能够在确保强隐私保护的同时，提供有竞争力的实用性。",
    "tldr": "本文研究了通过强化学习实现隐私保护的语言模型对齐问题，提出了一种新的差分隐私框架，并通过实验证明了其有效性和实用性。",
    "en_tdlr": "This paper investigates the problem of aligning language models with privacy protection through reinforcement learning, proposes a new framework for differential privacy, and validates its effectiveness and utility through experiments."
}