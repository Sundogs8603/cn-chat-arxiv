{
    "title": "Neural Sampling in Hierarchical Exponential-family Energy-based Models. (arXiv:2310.08431v2 [cs.AI] UPDATED)",
    "abstract": "Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To",
    "link": "http://arxiv.org/abs/2310.08431",
    "context": "Title: Neural Sampling in Hierarchical Exponential-family Energy-based Models. (arXiv:2310.08431v2 [cs.AI] UPDATED)\nAbstract: Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To",
    "path": "papers/23/10/2310.08431.json",
    "total_tokens": 877,
    "translated_title": "分层指数族能量模型中的神经采样",
    "translated_abstract": "贝叶斯脑理论认为大脑使用生成模型来理解外部世界。基于采样的观点认为，大脑通过随机神经响应样本推断后验分布。此外，大脑还不断更新其生成模型以逼近外部世界的真实分布。本研究中，我们引入了分层指数族能量模型(HEE)，该模型捕捉了推断和学习的动态过程。在HEE模型中，我们将分区函数分解为多个层次，并利用一组具有较短时间常数的神经元来采样分解的归一化项的梯度。这使得我们的模型能够同时估计分区函数并进行推断，从而避免了传统能量模型(EBMs)中遇到的负相位问题。因此，学习过程在时间和空间上都是局部化的，模型收敛容易。",
    "tldr": "本研究引入了分层指数族能量模型(HEE)，该模型通过采样梯度从而估计分区函数和进行推断，解决了传统能量模型中的负相位问题，从而使学习过程局部化且容易收敛。",
    "en_tdlr": "This paper introduces a Hierarchical Exponential-family Energy-based (HEE) model that addresses the negative phase problem in conventional energy-based models. The HEE model estimates the partition function and performs inference simultaneously by sampling gradients, leading to a localized and easy-to-converge learning process."
}