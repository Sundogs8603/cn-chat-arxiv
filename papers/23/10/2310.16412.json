{
    "title": "FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning. (arXiv:2310.16412v1 [cs.LG])",
    "abstract": "Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case that needs to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original ",
    "link": "http://arxiv.org/abs/2310.16412",
    "context": "Title: FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning. (arXiv:2310.16412v1 [cs.LG])\nAbstract: Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case that needs to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original ",
    "path": "papers/23/10/2310.16412.json",
    "total_tokens": 902,
    "translated_title": "FlatMatch: 使用交叉锋利度来衔接标记和未标记数据的半监督学习",
    "translated_abstract": "半监督学习（SSL）一直是一种有效利用丰富的未标记数据与极其稀缺的标记数据的方法。然而，大多数SSL方法通常基于不同数据转换之间的实例一致性。因此，对标记数据的标签指导很难传播到未标记数据中。结果，标记数据上的学习过程比未标记数据上的学习过程快得多，很可能陷入不利于未标记数据的局部极小值，导致次优的泛化性能。在本文中，我们提出了FlatMatch，通过最小化交叉锋利度度量来确保两个数据集之间的一致学习性能。具体而言，我们增加了标记数据上的经验风险，得到了一个最坏情况模型，即需要增强的失败情况。然后，通过利用未标记数据的丰富性，我们惩罚最坏情况模型与原始模型之间的预测差异（即交叉锋利度）。",
    "tldr": "本文提出了FlatMatch方法，通过最小化交叉锋利度度量来确保标记数据和未标记数据之间的一致学习性能，提高了半监督学习的泛化性能。",
    "en_tdlr": "This paper proposes the FlatMatch method, which minimizes the cross-sharpness measure to ensure consistent learning performance between labeled and unlabeled data, improving the generalization performance of semi-supervised learning."
}