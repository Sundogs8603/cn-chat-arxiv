{
    "title": "The Less the Merrier? Investigating Language Representation in Multilingual Models. (arXiv:2310.13228v1 [cs.CL])",
    "abstract": "Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported in popular multilingual models and which languages are left behind. Then, for included languages, we look at models' learned representations based on language family and dialect and try to understand how models' learned representations for~(1) seen and~(2) unseen languages vary across different language groups. In addition, we test and analyze performance on downstream tasks such as text generation and Named Entity Recognition. We observe from our experiments that community-centered models -- mo",
    "link": "http://arxiv.org/abs/2310.13228",
    "context": "Title: The Less the Merrier? Investigating Language Representation in Multilingual Models. (arXiv:2310.13228v1 [cs.CL])\nAbstract: Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported in popular multilingual models and which languages are left behind. Then, for included languages, we look at models' learned representations based on language family and dialect and try to understand how models' learned representations for~(1) seen and~(2) unseen languages vary across different language groups. In addition, we test and analyze performance on downstream tasks such as text generation and Named Entity Recognition. We observe from our experiments that community-centered models -- mo",
    "path": "papers/23/10/2310.13228.json",
    "total_tokens": 912,
    "translated_title": "越少越好？探究多语言模型中的语言表示",
    "translated_abstract": "多语言语言模型提供了一种将多种语言合并到一个模型中，并利用跨语言迁移学习来提高不同自然语言处理（NLP）任务性能的方法。尽管在多语言模型方面取得了进展，但并不是所有语言都得到了同样的支持，特别是在资源有限的情况下。在这项工作中，我们调查了多语言模型中不同语言的语言表示。我们首先提出了一个问题：在流行的多语言模型中支持哪些语言，哪些语言被忽视。然后，对于包含的语言，我们基于语系和方言来观察模型的学习表示，并尝试理解模型在不同语言组中对于（1）已知语言和（2）未知语言的学习表示如何变化。此外，我们还测试并分析了在文本生成和命名实体识别等下游任务中的性能。通过实验，我们观察到以社区为中心的模型——",
    "tldr": "该研究调查了多语言模型中不同语言的语言表示，探讨了流行的多语言模型中支持和忽视的语言，并研究了模型对于已知和未知语言的学习表示的变化。此外，还测试了模型在文本生成和命名实体识别等任务中的性能。"
}