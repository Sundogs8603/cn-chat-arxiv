{
    "title": "SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])",
    "abstract": "Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us t",
    "link": "http://arxiv.org/abs/2310.12072",
    "context": "Title: SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])\nAbstract: Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us t",
    "path": "papers/23/10/2310.12072.json",
    "total_tokens": 781,
    "translated_title": "SPEED: 用于高效解码的推测流水线执行",
    "translated_abstract": "基于Transformer架构的生成型大型语言模型（LLM）近来已成为广泛应用于自然语言处理任务的主导基础模型。然而，由于这些模型的推理延迟显著，它们在实时场景中的应用受到了很大限制。这主要是由于生成型LLM推理的自回归特性，其中每个标记依赖于所有先前的输出标记，因此很难实现任何标记级的并行性，使得推理过程极度受内存限制。在这项工作中，我们提出了SPEED，通过使用基于早期隐藏状态的预测值来并行地推测执行当前标记与多个未来标记，从而提高推理效率。对于采用参数共享的Transformer解码器，可以将并行执行的标记的内存操作分摊，从而允许我们...",
    "tldr": "SPEED通过推测执行多个未来标记，加快Transformer解码器的推理效率，从而提高生成型大型语言模型在实时场景中的应用性能。",
    "en_tdlr": "SPEED improves inference efficiency of Transformer decoders for generative large language models in real-time scenarios by speculatively executing multiple future tokens."
}