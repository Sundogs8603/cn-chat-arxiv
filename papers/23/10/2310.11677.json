{
    "title": "Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])",
    "abstract": "We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\\mathcal{O}({\\epsilon^{-2}})$ sample complexity and $\\mathcal{O}(\\epsilon^{-1})$ iteration complexity with general parameterization where $\\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\\log(\\frac{1}{\\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\\mathcal{O}(\\epsilon^{-\\frac{1}{2}})$ and simultaneously matches their state-of-the-art it",
    "link": "http://arxiv.org/abs/2310.11677",
    "context": "Title: Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])\nAbstract: We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\\mathcal{O}({\\epsilon^{-2}})$ sample complexity and $\\mathcal{O}(\\epsilon^{-1})$ iteration complexity with general parameterization where $\\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\\log(\\frac{1}{\\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\\mathcal{O}(\\epsilon^{-\\frac{1}{2}})$ and simultaneously matches their state-of-the-art it",
    "path": "papers/23/10/2310.11677.json",
    "total_tokens": 969,
    "translated_title": "无限时间无折扣奖励马尔可夫决策过程自然策略梯度算法的改进样本复杂度分析",
    "translated_abstract": "本文考虑设计样本高效的学习算法，用于无限时间无折扣奖励马尔可夫决策过程。具体地，我们提出了加速自然策略梯度（ANPG）算法，利用加速随机梯度下降过程来获取自然策略梯度。ANPG算法在一般参数化情况下实现了O(ε^{-2})的样本复杂度和O(ε^{-1})的迭代复杂度，其中ε定义了最优性误差。这将样本复杂度提高了一个log(1/ε)的因子。ANPG是一个一阶算法，并且不需要现有文献中可能无法验证的重要性采样(IS)权重方差上界的假设。在无Hessian和无IS算法类中，ANPG超过了已知样本复杂度的一个O(ε^{-\\frac{1}{2}})的因子，并同时达到了它们的最新技术成果。",
    "tldr": "本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。",
    "en_tdlr": "The paper proposes an accelerated natural policy gradient algorithm (ANPG) for solving infinite horizon discounted reward Markov Decision Processes. ANPG achieves significant improvements in sample and iteration complexity, overcomes the limitations of existing algorithms, and achieves state-of-the-art results."
}