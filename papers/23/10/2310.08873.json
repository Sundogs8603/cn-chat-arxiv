{
    "title": "Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)",
    "abstract": "This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like \"Can you pass through the curtains to deliver medicines to me?\", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object",
    "link": "http://arxiv.org/abs/2310.08873",
    "context": "Title: Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)\nAbstract: This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like \"Can you pass through the curtains to deliver medicines to me?\", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object",
    "path": "papers/23/10/2310.08873.json",
    "total_tokens": 1033,
    "translated_title": "在带有可通行障碍的环境中使用大型语言和视觉-语言模型进行交互式导航",
    "translated_abstract": "本文提出了一种使用大型语言和视觉-语言模型的交互式导航框架，使机器人能够在带有可通行障碍的环境中进行导航。我们利用大型语言模型(GPT-3.5)和开放式视觉-语言模型(基于Grounding DINO)创建了一个动作感知成本地图，用于进行有效的路径规划而无需微调。通过大型模型，我们可以实现从文本指令（例如“你能通过窗帘给我送药吗？”）到具有动作感知属性的边界框（例如窗帘）的端到端系统。它们可以用于将激光雷达点云分成两部分：可通行和不可通行部分，然后构建一个动作感知成本地图用于生成可行路径。预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，可以快速部署于交互式导航任务中。我们选择使用多个可通行对象。",
    "tldr": "本文提出了一个使用大型语言和视觉-语言模型的交互式导航框架，使机器人能够在带有可通行障碍的环境中进行导航。通过使用这些模型，我们可以实现从文本指令到动作感知边界框的端到端系统，无需微调和额外的训练数据。同时，我们还使用大型模型划分激光雷达点云，生成动作感知成本地图以生成可行路径。",
    "en_tdlr": "This paper proposes an interactive navigation framework using large language and vision-language models, enabling robots to navigate in environments with traversable obstacles. By utilizing these models, an end-to-end system can be achieved from textual instructions to action-aware bounding boxes, without fine-tuning and additional training data. Additionally, a costmap is generated for path planning by segmenting LiDAR point clouds using the large models."
}