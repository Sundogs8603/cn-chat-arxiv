{
    "title": "Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])",
    "abstract": "What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while ",
    "link": "http://arxiv.org/abs/2310.08049",
    "context": "Title: Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])\nAbstract: What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while ",
    "path": "papers/23/10/2310.08049.json",
    "total_tokens": 853,
    "translated_title": "探索模型架构与上下文学习能力之间的关系",
    "translated_abstract": "模型架构和执行上下文学习任务的能力之间有什么关联？在这项实证研究中，我们首次尝试回答这个问题。具体而言，我们评估了十五种模型架构在一套合成的上下文学习任务中的表现。所选的架构代表了各种范式，包括循环和卷积神经网络，变换器以及新兴的注意力替代方案。我们发现，在特定条件下，所有考虑的架构都能够执行上下文学习任务。然而，当任务复杂性增加时，当代架构表现最好。此外，我们的后续实验探索了一些影响上下文学习的因素。我们观察到，不同架构对超参数设置有不同的敏感性。我们对训练动态的研究揭示了某些架构呈现出平稳、渐进的学习轨迹，而...",
    "tldr": "现有的模型架构在上下文学习中表现最好，特别是在任务复杂性增加的情况下。不同架构对超参数设置的敏感度有所差异，且一些架构展现出平稳的学习轨迹。",
    "en_tdlr": "Contemporary model architectures perform the best in in-context learning, especially as task complexity grows. Different architectures exhibit varied sensitivities to hyperparameter settings, and certain architectures show a smooth learning trajectory."
}