{
    "title": "A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes re",
    "link": "http://arxiv.org/abs/2310.11761",
    "context": "Title: A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])\nAbstract: Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes re",
    "path": "papers/23/10/2310.11761.json",
    "total_tokens": 1049,
    "translated_title": "对大规模语言模型在法律判决预测上的综合评估",
    "translated_abstract": "大规模语言模型（LLMs）已经在特定领域的应用（如法律领域）中展示出了巨大的潜力。然而，关于GPT-4在法律评估方面的争议引发了对它们在现实法律任务中表现的质疑。为了系统地调查它们在法律领域的能力，我们设计了基于LLMs的实用基准解决方案，并在法律判决预测任务上进行了测试。在我们的解决方案中，LLMs可以单独回答开放性问题，或与信息检索（IR）系统配合，从类似案例中学习或解决简化的多项选择问题。我们展示了类似案例和多项选择选项（即提示中包含的标签候选项）可以帮助LLMs回忆起对专家法律推理至关重要的领域知识。此外，我们还提出了一个有趣的悖论，即由于较弱的LLMs从强大的IR系统获得的收益有限，导致IR系统的绩效超过LLM+IR。在这种情况下，LLMs的角色变得重要起来。",
    "tldr": "本研究对大规模语言模型在法律判决预测上进行了综合评估，结果表明LLMs可以通过类似案例和多项选择的方式提高其在专家法律推理中的表现。此外，我们发现在某些情况下，较弱的LLMs从强大的信息检索系统中获得的收益有限，从而导致信息检索系统的绩效超越LLM+IR的组合。",
    "en_tdlr": "This study provides a comprehensive evaluation of large language models (LLMs) on legal judgment prediction. The results show that LLMs can improve their performance in expert legal reasoning through the use of similar cases and multiple-choice options. Additionally, it is found that in some cases, weaker LLMs benefit limitedly from powerful information retrieval systems, leading to the surpassing of LLM+IR combination by the information retrieval system."
}