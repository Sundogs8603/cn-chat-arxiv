{
    "title": "Positivity-free Policy Learning with Observational Data. (arXiv:2310.06969v1 [stat.ME])",
    "abstract": "Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed fr",
    "link": "http://arxiv.org/abs/2310.06969",
    "context": "Title: Positivity-free Policy Learning with Observational Data. (arXiv:2310.06969v1 [stat.ME])\nAbstract: Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed fr",
    "path": "papers/23/10/2310.06969.json",
    "total_tokens": 876,
    "translated_title": "无偏性政策学习与观测数据",
    "translated_abstract": "利用观测数据进行政策学习在各个领域都非常重要，其目标是学习最优的处理分配策略，同时满足特定的约束条件，如公平性、预算和简单性。本研究引入了一种新颖的无偏性（随机）政策学习框架，旨在应对现实情境中无法满足假设的困境。该框架利用增量倾向分数策略来调整倾向分数值，而不是给治疗分配固定值。我们对这些增量倾向分数策略进行了表征，并建立了识别条件，利用半参数效率理论提出了能够实现快速收敛率的高效估计器，即使是与先进的机器学习算法集成在一起。本文对政策学习的理论保证进行了深入探讨，并验证了所提出的框架。",
    "tldr": "本研究提出了一种无偏性的政策学习框架，该框架利用观测数据进行政策学习，并克服了现实情境中无法满足假设的难题。该框架利用增量倾向分数策略调整倾向分数值，从而实现了快速的收敛率。",
    "en_tdlr": "This study introduces a positivity-free policy learning framework that utilizes observational data and overcomes the challenge of impracticality in real-world scenarios. The framework leverages incremental propensity score policies to adjust propensity score values, achieving rapid convergence rates."
}