{
    "title": "Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms. (arXiv:2310.10810v1 [cs.LG])",
    "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To ",
    "link": "http://arxiv.org/abs/2310.10810",
    "context": "Title: Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms. (arXiv:2310.10810v1 [cs.LG])\nAbstract: Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To ",
    "path": "papers/23/10/2310.10810.json",
    "total_tokens": 971,
    "translated_title": "鲁棒多智能体强化学习: 对抗性规范化的理论基础和稳定算法",
    "translated_abstract": "多智能体强化学习在多个领域都显示出良好的结果。然而，由于对环境的微小变化敏感，多智能体强化学习策略往往缺乏鲁棒性。这对于在现实世界中部署多智能体强化学习算法构成了严重问题，因为测试环境可能与训练环境略有不同。本文通过控制策略的Lipschitz常数来提高鲁棒性，并在温和条件下证明了Lipschitz且接近最优策略的存在。基于这些见解，我们提出了一个新的鲁棒多智能体强化学习框架ERNIE，通过对抗性规范化促进策略对于状态观察和动作的Lipschitz连续性。ERNIE框架对于噪声观察、转换动态的变化和智能体的恶意行为具有鲁棒性。然而，ERNIE的对抗性规范化可能引入一些训练不稳定性。",
    "tldr": "本文提出了一种新的鲁棒多智能体强化学习框架ERNIE，通过对抗性规范化促进策略的Lipschitz连续性，以提高鲁棒性和对抗噪声观察、转换动态的变化和智能体的恶意行为。"
}