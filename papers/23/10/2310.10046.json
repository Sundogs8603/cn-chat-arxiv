{
    "title": "TRANSOM: An Efficient Fault-Tolerant System for Training LLMs. (arXiv:2310.10046v2 [cs.DC] UPDATED)",
    "abstract": "Large language models (LLMs), exemplified by chatGPT, have made significant strides in various domains, indicating that LLMs with hundreds of billions or even trillions of parameters will continue to revolutionize our daily lives. However, training such super-large-scale models demands even more powerful GPU clusters and extended training periods spanning months. Maintaining uninterrupted and long-duration training has become exceptionally challenging due to hardware and software failures in these extensive clusters. A substantial amount of training time is devoted to tasks checkpointing saving and loading, ananomaly detection and restarts, leading to a notable reduction in overall training efficiency.To address these challenges, we introduce novel fault-tolerant large-scale model training system named TRANSOM. This system comprises three integral components: the training pipeline automatic fault tolerance and recovery mechanism (TOL), the training task multi-dimensional metric automat",
    "link": "http://arxiv.org/abs/2310.10046",
    "context": "Title: TRANSOM: An Efficient Fault-Tolerant System for Training LLMs. (arXiv:2310.10046v2 [cs.DC] UPDATED)\nAbstract: Large language models (LLMs), exemplified by chatGPT, have made significant strides in various domains, indicating that LLMs with hundreds of billions or even trillions of parameters will continue to revolutionize our daily lives. However, training such super-large-scale models demands even more powerful GPU clusters and extended training periods spanning months. Maintaining uninterrupted and long-duration training has become exceptionally challenging due to hardware and software failures in these extensive clusters. A substantial amount of training time is devoted to tasks checkpointing saving and loading, ananomaly detection and restarts, leading to a notable reduction in overall training efficiency.To address these challenges, we introduce novel fault-tolerant large-scale model training system named TRANSOM. This system comprises three integral components: the training pipeline automatic fault tolerance and recovery mechanism (TOL), the training task multi-dimensional metric automat",
    "path": "papers/23/10/2310.10046.json",
    "total_tokens": 952,
    "translated_title": "TRANSOM:一种用于训练LLMs的高效容错系统",
    "translated_abstract": "大型语言模型（LLMs）如chatGPT在各个领域取得了显著进展，表明拥有数百亿甚至数万亿参数的LLMs将继续改变我们的日常生活。然而，训练如此大规模的模型需要更强大的GPU集群和持续数月的训练周期。在这样庞大的集群中，由于硬件和软件故障，保持不中断和长时间的训练变得异常困难。相当多的训练时间被用于任务检查点的保存和加载、异常检测和重启，导致整体训练效率显著降低。为了解决这些挑战，我们引入了一种名为TRANSOM的新型容错大规模模型训练系统。该系统包括三个核心组件:训练流水线自动容错和恢复机制（TOL）、训练任务多维度度量的自动决策和调整机制（ADAM）以及在群集恢复之间自动决策和管理任务移动的模型（RMM）。",
    "tldr": "TRANSOM是一种用于训练LLMs的高效容错系统，包括训练流水线自动容错和恢复机制（TOL）、训练任务多维度度量的自动决策和调整机制（ADAM）以及在群集恢复之间自动决策和管理任务移动的模型（RMM）。",
    "en_tdlr": "TRANSOM is an efficient fault-tolerant system for training LLMs, consisting of the training pipeline automatic fault tolerance and recovery mechanism (TOL), the training task multi-dimensional metric automatic decision and adjustment mechanism (ADAM), and the model (RMM) that automatically decides and manages task migration between cluster recoveries."
}