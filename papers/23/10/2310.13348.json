{
    "title": "Analyzing Cognitive Plausibility of Subword Tokenization. (arXiv:2310.13348v1 [cs.CL])",
    "abstract": "Subword tokenization has become the de-facto standard for tokenization, although comparative evaluations of subword vocabulary quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.",
    "link": "http://arxiv.org/abs/2310.13348",
    "context": "Title: Analyzing Cognitive Plausibility of Subword Tokenization. (arXiv:2310.13348v1 [cs.CL])\nAbstract: Subword tokenization has become the de-facto standard for tokenization, although comparative evaluations of subword vocabulary quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.",
    "path": "papers/23/10/2310.13348.json",
    "total_tokens": 824,
    "translated_title": "分析子词分词的认知合理性",
    "translated_abstract": "子词分词已经成为标准的分词方法，然而对于不同语言中子词词汇质量的比较评估却非常稀缺。现有的评估研究主要关注分词算法对下游任务性能的影响，或者工程标准如压缩率。我们提出了一种新的评估范式，专注于子词分词的认知合理性。我们分析了Tokenizer输出与人类在词汇决策任务中的反应时间和准确性之间的相关性。我们比较了几种子词分词算法在几种语言和词汇大小上的表现。我们的结果表明，与先前的工作相比，UnigramLM算法产生了更少认知合理的分词行为和更差的派生形态素覆盖。",
    "tldr": "本文提出了一种新的评估范式，分析了子词分词的认知合理性，并比较了几种不同算法在多种语言和词汇大小上的表现。结果发现UnigramLM算法的分词行为和派生形态素覆盖较差。",
    "en_tdlr": "This paper presents a new evaluation paradigm to analyze the cognitive plausibility of subword tokenization and compares several algorithms across different languages and vocabulary sizes. The results indicate that the UnigramLM algorithm has less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes compared to previous work."
}