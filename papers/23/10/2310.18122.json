{
    "title": "OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])",
    "abstract": "Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.",
    "link": "http://arxiv.org/abs/2310.18122",
    "context": "Title: OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])\nAbstract: Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.",
    "path": "papers/23/10/2310.18122.json",
    "total_tokens": 908,
    "translated_title": "OpinSummEval:再考自动化评估在意见摘要中的应用",
    "translated_abstract": "与其他类型的摘要任务不同，意见摘要专注于观点和情感，因此与众不同。虽然像ROUGE这样的某些自动化评估方法很受欢迎，但我们发现它们对评估意见摘要的质量是不可靠的。在本文中，我们提出了一个数据集OpinSummEval，它包括来自14个意见摘要模型的人工判断和输出。我们进一步探讨了24个自动度量与人工评分之间的相关性，涵盖了四个维度。我们的研究结果表明，基于神经网络的度量通常优于非神经网络的度量。然而，即使是基于强大模型（如BART和GPT-3/3.5）构建的度量也不能在所有维度上始终保持良好的相关性，突出了需要改进意见摘要的自动化评估方法的需求。代码和数据公开可用于https://github.com/A-Chicharito-S/OpinSummEval/tree/main。",
    "tldr": "本文提出了一个新的数据集OpinSummEval，对意见摘要进行自动化评估的可靠性进行重新评估。研究发现基于神经网络的度量通常优于非神经网络的度量，但即使是基于强大模型构建的度量也不能在所有维度上始终保持良好的相关性，突出了对意见摘要自动化评估方法的进一步改进的需求。"
}