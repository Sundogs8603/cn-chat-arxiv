{
    "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])",
    "abstract": "We discover the presence of quantization artifacts in Vision Transformers (ViTs), which arise due to the image tokenization step inherent in these architectures. These artifacts result in coarsely quantized features, which negatively impact performance, especially on downstream dense prediction tasks. We present a zero-shot method to improve how pre-trained ViTs handle spatial quantization. In particular, we propose to ensemble the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance, a method traditionally applied to climate dynamics and signal processing. We term our method ``Stochastic Resonance Transformer\" (SRT), which we show can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT can be applied at any layer, on any task, and does not require any fine-tuning. The advantage of the former is evident",
    "link": "http://arxiv.org/abs/2310.03967",
    "context": "Title: Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])\nAbstract: We discover the presence of quantization artifacts in Vision Transformers (ViTs), which arise due to the image tokenization step inherent in these architectures. These artifacts result in coarsely quantized features, which negatively impact performance, especially on downstream dense prediction tasks. We present a zero-shot method to improve how pre-trained ViTs handle spatial quantization. In particular, we propose to ensemble the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance, a method traditionally applied to climate dynamics and signal processing. We term our method ``Stochastic Resonance Transformer\" (SRT), which we show can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT can be applied at any layer, on any task, and does not require any fine-tuning. The advantage of the former is evident",
    "path": "papers/23/10/2310.03967.json",
    "total_tokens": 928,
    "translated_title": "基于随机共振变压器的子代币ViT嵌入",
    "translated_abstract": "我们发现Vision Transformers（ViTs）中存在量化伪影，这是由于这些架构中的图像标记步骤引起的。这些伪影导致了粗糙的量化特征，对下游的密集预测任务特别是有负面影响。我们提出了一种零shot方法来改进预训练的ViTs处理空间量化的方式。特别是，我们建议通过子代币空间平移来集合通过扰动输入图像获得的特征，这受到了随机共振的启发，随机共振是传统上应用于气候动力学和信号处理的方法。我们将这种方法称为“随机共振变压器”（SRT），我们展示了SRT能够有效地超分辨率预训练的ViTs的特征，捕捉到了作为标记结果可能被忽略的更多局部细粒度结构。SRT可以在任何层面、任何任务上应用，并且不需要进行任何微调。前者的优势是明显的。",
    "tldr": "通过子代币空间平移集合以解决ViTs量化伪影问题的随机共振变压器方法在不需要微调的情况下能够有效超分辨率预训练的ViTs特征，捕捉到细粒度结构。",
    "en_tdlr": "The Stochastic Resonance Transformer method, which ensemble the features obtained from perturbing input images via sub-token spatial translations, effectively solves the quantization artifacts problem in Vision Transformers (ViTs) without requiring fine-tuning, and captures fine-grained structures in pre-trained ViTs."
}