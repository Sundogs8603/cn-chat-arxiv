{
    "title": "What's In My Big Data?",
    "abstract": "arXiv:2310.20707v2 Announce Type: replace  Abstract: Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance,",
    "link": "https://arxiv.org/abs/2310.20707",
    "context": "Title: What's In My Big Data?\nAbstract: arXiv:2310.20707v2 Announce Type: replace  Abstract: Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance,",
    "path": "papers/23/10/2310.20707.json",
    "total_tokens": 856,
    "translated_title": "我的大数据中有什么？",
    "translated_abstract": "大型文本语料库是语言模型的基础。然而，我们对这些语料库的内容，包括一般统计信息、质量、社会因素和评估数据（污染）的理解有限。在这项工作中，我们提出了“What's In My Big Data?”（WIMBD），这是一个平台和一组十六个分析方法，使我们能够揭示和比较大型文本语料库的内容。WIMBD基于两种基本能力——计数和搜索——在规模上进行，这使我们能够在标准计算节点上分析超过35TB的数据。我们将WIMBD应用于用于训练流行语言模型的十个不同语料库，包括C4、The Pile和RedPajama。我们的分析揭示了关于这些语料库的几个令人惊讶且以前未记录的发现，包括重复内容、合成内容、低质量内容、个人可识别信息、有毒语言和基准污染的高流行率。",
    "tldr": "通过提出的平台和分析方法，我们揭示和比较了大型文本语料库的内容，发现了关于语料库内容的几个令人惊讶且以前未被记录的发现。",
    "en_tdlr": "By introducing a platform and analysis methods, we revealed and compared the contents of large text corpora, discovering several surprising and previously undocumented findings about the content of the corpora."
}