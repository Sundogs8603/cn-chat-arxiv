{
    "title": "Calibration by Distribution Matching: Trainable Kernel Calibration Metrics. (arXiv:2310.20211v1 [cs.LG])",
    "abstract": "Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification ",
    "link": "http://arxiv.org/abs/2310.20211",
    "context": "Title: Calibration by Distribution Matching: Trainable Kernel Calibration Metrics. (arXiv:2310.20211v1 [cs.LG])\nAbstract: Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification ",
    "path": "papers/23/10/2310.20211.json",
    "total_tokens": 879,
    "translated_title": "分布匹配校准：可训练的核校准度量",
    "translated_abstract": "校准确保概率预测能够有效地捕捉不确定性，要求预测概率与经验频率相吻合。然而，许多现有的校准方法专门用于事后再校准，可能会恶化预测的尖锐性。基于将校准视为分布匹配任务的洞察，我们引入了基于核的校准度量，统一和推广了分类和回归中常见的校准形式。这些度量可以产生可微的样本估计，可以很容易地将校准目标纳入经验风险最小化中。此外，我们提供了直观的机制来定制决策任务的校准度量，并强制准确的损失估计和无遗憾决策。我们的实证评估表明，使用这些度量作为正则化项可以提高在一系列回归和分类任务中的校准性、尖锐性和决策性能。",
    "tldr": "该论文提出了基于核的校准度量方法，统一和推广了分类和回归中常见的校准形式。这些度量可以产生可微的样本估计，易于纳入经验风险最小化，并通过定制校准度量来优化决策任务。",
    "en_tdlr": "This paper introduces a kernel-based calibration metric that unifies and generalizes popular forms of calibration for both classification and regression. These metrics allow for differentiable sample estimation and can be easily incorporated into empirical risk minimization. They also provide intuitive mechanisms for tailoring calibration metrics to decision tasks."
}