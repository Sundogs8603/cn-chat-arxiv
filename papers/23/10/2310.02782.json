{
    "title": "Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design. (arXiv:2310.02782v1 [cs.LG])",
    "abstract": "The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment",
    "link": "http://arxiv.org/abs/2310.02782",
    "context": "Title: Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design. (arXiv:2310.02782v1 [cs.LG])\nAbstract: The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment",
    "path": "papers/23/10/2310.02782.json",
    "total_tokens": 936,
    "translated_title": "使用对抗性环境设计探索通用强化学习算法",
    "translated_abstract": "在过去的十年中，深度强化学习取得了巨大的进展，这些进展是由人类研究人员手动设计的算法推动的。最近，已经证明可以元学习更新规则，希望发现在各种强化学习任务中表现良好的算法。尽管像学习策略梯度（LPG）这样的算法取得了令人印象深刻的初步结果，但是当这些算法应用于未见过的环境时仍存在泛化差距。在这项工作中，我们研究了元训练分布的特征如何影响这些算法的泛化性能。受到这个分析的启发，并借鉴了无监督环境设计（UED）的思想，我们提出了一种自动生成课程的新方法，以最大化元学习优化器的遗憾，此外还提出了一种新的遗憾近似方法，我们称之为算法遗憾（AR）。我们的方法是通过环境设计获得的通用强化学习优化器。",
    "tldr": "通过对抗性环境设计，我们提出了一种通用强化学习算法，通过元学习更新规则和自动生成课程来提高算法的泛化性能，并引入了一种新的遗憾近似方法，名为算法遗憾（AR）。",
    "en_tdlr": "We propose a general reinforcement learning algorithm by using adversarial environment design, which improves the generalization performance of the algorithm through meta-learning update rules and automatic curriculum generation. We also introduce a novel approximation method of regret, called algorithmic regret (AR)."
}