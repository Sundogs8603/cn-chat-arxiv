{
    "title": "What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])",
    "abstract": "When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward",
    "link": "http://arxiv.org/abs/2310.07932",
    "context": "Title: What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])\nAbstract: When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward",
    "path": "papers/23/10/2310.07932.json",
    "total_tokens": 890,
    "translated_title": "你关心什么？为机器人学习实现视觉表示对齐",
    "translated_abstract": "在为人类服务时，机器人需要优化与最终用户偏好一致的奖励。由于机器人将依赖原始感知输入如RGB图像，它们的奖励将不可避免地使用视觉表示。最近，使用预训练视觉模型的表示引发了人们的兴趣，但在机器人领域使其起作用的关键是微调，通常通过代理任务如动力学预测或强制时间循环一致性来完成。然而，所有这些代理任务都绕过了人类对自己关心的事物的输入，加剧了虚假关联，并最终导致机器人的行为与用户偏好不一致。在这项工作中，我们提议机器人应该利用人类的反馈来与最终用户的视觉表示对齐，并区分任务的关键要素。我们提出了一种解决视觉表示对齐问题和视觉奖励问题的方法，即基于偏好的表示对齐学习（RAPL）方法。",
    "tldr": "该论文提出了一种名为RAPL的方法，用于解决机器人学习中的视觉表示对齐问题，通过利用人类反馈将机器人的视觉表示与用户偏好对齐，并区分任务的关键要素。",
    "en_tdlr": "This paper proposes a method called RAPL to solve the problem of visual representation alignment in robot learning, by leveraging human feedback to align the robot's visual representation with user preferences and disentangle the key elements of the task."
}