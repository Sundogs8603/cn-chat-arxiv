{
    "title": "Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control. (arXiv:2310.17011v1 [cs.AI])",
    "abstract": "Different people have different facial expressions while speaking emotionally. A realistic facial animation system should consider such identity-specific speaking styles and facial idiosyncrasies to achieve high-degree of naturalness and plausibility. Existing approaches to personalized speech-driven 3D facial animation either use one-hot identity labels or rely-on person specific models which limit their scalability. We present a personalized speech-driven expressive 3D facial animation synthesis framework that models identity specific facial motion as latent representations (called as styles), and synthesizes novel animations given a speech input with the target style for various emotion categories. Our framework is trained in an end-to-end fashion and has a non-autoregressive encoder-decoder architecture with three main components: expression encoder, speech encoder and expression decoder. Since, expressive facial motion includes both identity-specific style and speech-related conte",
    "link": "http://arxiv.org/abs/2310.17011",
    "context": "Title: Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control. (arXiv:2310.17011v1 [cs.AI])\nAbstract: Different people have different facial expressions while speaking emotionally. A realistic facial animation system should consider such identity-specific speaking styles and facial idiosyncrasies to achieve high-degree of naturalness and plausibility. Existing approaches to personalized speech-driven 3D facial animation either use one-hot identity labels or rely-on person specific models which limit their scalability. We present a personalized speech-driven expressive 3D facial animation synthesis framework that models identity specific facial motion as latent representations (called as styles), and synthesizes novel animations given a speech input with the target style for various emotion categories. Our framework is trained in an end-to-end fashion and has a non-autoregressive encoder-decoder architecture with three main components: expression encoder, speech encoder and expression decoder. Since, expressive facial motion includes both identity-specific style and speech-related conte",
    "path": "papers/23/10/2310.17011.json",
    "total_tokens": 832,
    "translated_title": "个性化语音驱动的具有风格控制的表情三维动画合成",
    "translated_abstract": "不同的人在情感表达时有不同的面部表情。一个真实的面部动画系统应该考虑到这种身份特定的表达风格和面部习性，以实现高度自然和可信度。现有的个性化语音驱动的三维面部动画方法要么使用独热标签，要么依赖于特定个体的模型，限制了其可扩展性。我们提出了一个个性化的语音驱动的表情三维动画合成框架，将身份特定的面部动作建模为潜在表示（称为风格），并根据情感类别，用目标风格合成新的动画。我们的框架以端到端的方式训练，并具有三个主要组成部分：表情编码器、语音编码器和表情解码器。由于富有表情的面部动作包括身份特定的风格和语音相关的内容。",
    "tldr": "提出了一个个性化语音驱动的表情三维动画合成框架，通过建模身份特定的面部动作风格，以及语音相关内容，实现高度自然和可信度的动画合成。"
}