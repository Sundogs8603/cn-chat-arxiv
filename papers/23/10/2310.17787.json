{
    "title": "Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])",
    "abstract": "Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language u",
    "link": "http://arxiv.org/abs/2310.17787",
    "context": "Title: Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])\nAbstract: Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language u",
    "path": "papers/23/10/2310.17787.json",
    "total_tokens": 885,
    "translated_title": "评估使用印度语LGBTI+词汇表的大型语言模型",
    "translated_abstract": "大型语言模型（LLMs）通常通过基于任务的基准（如MMLU）进行评估。这样的基准无法在特定语境中检查LLMs的负责任行为。在LGBTI+语境中，社会陈规可能导致LGBTI+术语的变异。因此，领域特定的词汇表可以作为对LLM行为进行评估的代表性单词列表。本文提出了一种使用印度语LGBTI+词汇表评估LLMs的方法。该方法包括四个步骤：制定与预期行为相关的NLP任务，创建测试LLMs的提示，使用LLMs获取输出，最后进行手动评估结果。我们的定性分析显示，我们实验的三个LLMs无法检测到潜在的仇恨内容。同样，我们观察到使用机器翻译作为评估自然语言的手段存在局限性。",
    "tldr": "该论文提出了一种使用印度语LGBTI+词汇表评估大型语言模型的方法。研究发现，现有的语言模型无法有效检测潜藏的仇恨内容，使用机器翻译作为评估手段也存在局限性。"
}