{
    "title": "Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])",
    "abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight con",
    "link": "http://arxiv.org/abs/2310.17303",
    "context": "Title: Demonstration-Regularized RL. (arXiv:2310.17303v1 [stat.ML])\nAbstract: Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight con",
    "path": "papers/23/10/2310.17303.json",
    "total_tokens": 929,
    "translated_title": "通过演示-正则化强化学习增强采样效率",
    "translated_abstract": "通过将专家演示纳入其中，可以在提高强化学习(SRL)的采样效率方面产生经验效果。本文在理论上量化这些额外信息降低了SRL的采样复杂性的程度。具体而言，我们研究了通过KL正则化利用专家演示学习的策略的演示-正则化强化学习。我们的研究发现，在有限状态下，在$\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$的样本复杂度内，使用$N^{\\mathrm{E}}$个专家演示能够找到最优策略，并在线性马尔科夫决策过程中，在$\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$的样本复杂度内找到最优策略，其中$\\varepsilon$是目标精度，$H$是规定，$A$是动作的数量，$S$是有限状态的数量，在线性情况下，$d$是特征空间的维数。",
    "tldr": "通过演示-正则化提高强化学习的采样效率，并找到最优策略的样本复杂度，该复杂度与专家演示数量成反比。"
}