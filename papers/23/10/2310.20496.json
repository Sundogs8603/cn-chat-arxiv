{
    "title": "BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis. (arXiv:2310.20496v2 [cs.LG] UPDATED)",
    "abstract": "Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attenti",
    "link": "http://arxiv.org/abs/2310.20496",
    "context": "Title: BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis. (arXiv:2310.20496v2 [cs.LG] UPDATED)\nAbstract: Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attenti",
    "path": "papers/23/10/2310.20496.json",
    "total_tokens": 896,
    "translated_title": "BasisFormer:基于可学习和可解释的基础的注意力机制时间序列预测",
    "translated_abstract": "基础已经成为现代基于深度学习的时间序列预测模型的重要组成部分，因为它们具有作为特征提取者或未来参考的能力。为了有效，基础必须根据特定的时间序列数据集进行定制，并且在数据集中与每个时间序列展现出明显的相关性。然而，当前最先进的方法在同时满足这两个要求方面存在局限性。为了解决这个挑战，我们提出了BasisFormer，一个端到端的时间序列预测架构，它利用了可学习和可解释的基础。该架构由三个组件组成：首先，我们通过自适应的自监督学习获得基础，该学习将时间序列的历史和未来部分视为两个不同的视图，并采用对比学习。接下来，我们设计了一个Coef模块，通过双向交叉注意力计算历史视图中时间序列与基础之间的相似系数。",
    "tldr": "BasisFormer提出了一种基于可学习和可解释的基础的注意力机制时间序列预测方法，通过自适应的自监督学习获得基础，并设计了一个模块计算时间序列与基础之间的相似系数。"
}