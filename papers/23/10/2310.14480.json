{
    "title": "Attention-Enhancing Backdoor Attacks Against BERT-based Models. (arXiv:2310.14480v2 [cs.LG] UPDATED)",
    "abstract": "Recent studies have revealed that \\textit{Backdoor Attacks} can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model's vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).",
    "link": "http://arxiv.org/abs/2310.14480",
    "context": "Title: Attention-Enhancing Backdoor Attacks Against BERT-based Models. (arXiv:2310.14480v2 [cs.LG] UPDATED)\nAbstract: Recent studies have revealed that \\textit{Backdoor Attacks} can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model's vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).",
    "path": "papers/23/10/2310.14480.json",
    "total_tokens": 914,
    "translated_title": "Attention-Enhancing Backdoor Attacks Against BERT-based Models（基于BERT模型的增强注意力的后门攻击）",
    "translated_abstract": "最近的研究表明，“后门攻击”可能威胁到自然语言处理（NLP）模型的安全性。研究后门攻击的策略将有助于了解模型的脆弱性。大部分现有的文本后门攻击集中在生成隐蔽的触发词或修改模型权重上。本文直接针对神经网络的内部结构和后门机制，提出了一种新颖的特洛伊注意力损失（TAL），通过直接操纵注意力模式来增强特洛伊行为。我们的损失函数可以应用于不同的攻击方法，从而提高它们的攻击成功率和污染率。它不仅适用于传统的脏标签攻击，也适用于更具挑战性的干净标签攻击。我们在不同的骨干模型（BERT，RoBERTa和DistilBERT）和各种任务（情感分析，有害检测和主题分类）上验证了我们的方法。",
    "tldr": "本文提出了一种基于BERT模型的增强注意力的后门攻击方法，通过直接操纵注意力模式来增强特洛伊行为，有效提高了攻击成功率和污染率。该方法适用于不同的任务和模型。",
    "en_tdlr": "This paper proposes an attention-enhancing backdoor attack method against BERT-based models, which manipulates the attention patterns to enhance the Trojan behavior, effectively improving the attack success rate and poisoning rate. This method is applicable to different tasks and models."
}