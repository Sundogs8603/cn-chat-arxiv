{
    "title": "CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])",
    "abstract": "Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r",
    "link": "http://arxiv.org/abs/2310.06380",
    "context": "Title: CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])\nAbstract: Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r",
    "path": "papers/23/10/2310.06380.json",
    "total_tokens": 893,
    "translated_title": "CAST：面向表格数据的群集感知自训练",
    "translated_abstract": "自训练由于其简单和多功能性而受到吸引，然而它容易受到有噪音的伪标签的影响。几项研究提出了成功解决这个问题的方法，但它们削弱了自训练的优势，因为它们需要对自训练算法或模型架构进行特定的修改。此外，大多数方法与在表格领域中占主导地位的梯度提升决策树不兼容。为了解决这个问题，我们重新考虑了群集假设，即相互接近的数据样本往往属于同一类。在此假设的启发下，我们提出了一种针对表格数据的群集感知自训练（CAST）方法。CAST是一种简单且普遍适应的方法，可以改进现有的自训练算法而无需进行大幅修改。具体而言，我们的方法规范了分类器的置信度，即伪标签的值，强制在低密度区域对伪标签进行限制。",
    "tldr": "本文提出了一种面向表格数据的群集感知自训练方法（CAST），通过规范伪标签的置信度，弥补了自训练算法中的一些弱点，具有普适性和适应性。",
    "en_tdlr": "This paper proposes a cluster-aware self-training (CAST) method for tabular data, which addresses the limitations of self-training algorithms by regularizing the confidence of pseudo-labels, providing universality and adaptability."
}