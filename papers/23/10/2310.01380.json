{
    "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning. (arXiv:2310.01380v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. O",
    "link": "http://arxiv.org/abs/2310.01380",
    "context": "Title: Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning. (arXiv:2310.01380v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. O",
    "path": "papers/23/10/2310.01380.json",
    "total_tokens": 983,
    "translated_title": "悲观非线性最小二乘值迭代算法用于离线强化学习",
    "translated_abstract": "离线强化学习（Offline RL）是指智能体根据由行为策略收集的数据学习最优策略的任务，近年来引起了越来越多的关注。虽然在线性函数逼近下的离线强化学习已经得到了广泛研究，并在一定假设下取得了最优结果，但很多研究将兴趣转向了非线性函数逼近下的离线强化学习。然而，关于非线性函数逼近下的离线强化学习的具有实例依赖后悔保证的研究工作却很有限。在本文中，我们提出了一种名为悲观非线性最小二乘值迭代（PNLSVI）的高效算法，用于非线性函数逼近下的离线强化学习。我们的算法设计包括三个创新的组成部分：（1）一种基于方差加权回归的方案，适用于广泛的函数类；（2）一种方差估计子程序；和（3）一个利用悲观值迭代方法的规划阶段。",
    "tldr": "本论文提出了一种悲观非线性最小二乘值迭代算法（PNLSVI），用于离线强化学习中的非线性函数逼近问题。该算法具有创新的方差加权回归方案、方差估计子程序和悲观值迭代方法的规划阶段。"
}