{
    "title": "Diverse Conventions for Human-AI Collaboration. (arXiv:2310.15414v1 [cs.AI])",
    "abstract": "Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \\emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits",
    "link": "http://arxiv.org/abs/2310.15414",
    "context": "Title: Diverse Conventions for Human-AI Collaboration. (arXiv:2310.15414v1 [cs.AI])\nAbstract: Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \\emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits",
    "path": "papers/23/10/2310.15414.json",
    "total_tokens": 921,
    "translated_title": "人工智能与人类协作的多样化约定",
    "translated_abstract": "在合作多智体游戏中，约定对于强大的性能至关重要，因为它们允许玩家在没有明确交流的情况下进行共同战略的协调。然而，标准的多智体强化学习技术，如自我对弈，会收敛到任意和非多样化的约定，导致在与新的合作伙伴互动时表现不佳。本文提出了一种通过在自我对弈过程中最大化其奖励，并在与先前发现的约定进行交互时最小化其奖励（交叉对弈），以刺激约定在语义上有所不同的技术，来生成多样化约定。为了确保学到的策略在交叉对弈的对抗性优化过程中始终遵守善意行事，我们引入了混合对弈（mixed-play）的概念，即通过从自我对弈和交叉对弈的转换中随机生成初始状态，并学习在此初始状态下最大化自我对弈的奖励。我们分析了这种方法的优势",
    "tldr": "本研究通过最大化自我对弈的奖励并最小化与先前发现的约定交互时的奖励来生成多样化约定，确保学到的策略在交叉对弈的对抗性优化过程中遵守善意行事"
}