{
    "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark. (arXiv:2310.08958v1 [cs.CL])",
    "abstract": "Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong ",
    "link": "http://arxiv.org/abs/2310.08958",
    "context": "Title: xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark. (arXiv:2310.08958v1 [cs.CL])\nAbstract: Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong ",
    "path": "papers/23/10/2310.08958.json",
    "total_tokens": 1000,
    "translated_title": "xDial-Eval: 一种多语言开放领域对话评估基准",
    "translated_abstract": "最近在无参考学习度量开放领域对话评估方面取得的进展，得益于预训练语言模型的进步和具有高质量人工注释的对话数据的可用性。然而，当前的研究主要集中在英语对话上，这些度量的泛化到其他语言尚未充分考虑。这主要是由于缺乏多语言对话评估基准。为解决这个问题，我们引入了xDial-Eval，它是基于开源英语对话评估数据集构建的。xDial-Eval包括12个轮次级别和6个对话级别的英语数据集，分别包含14930个注释轮次和8691个注释对话。英语对话数据还通过商业机器翻译系统扩展到其他九种语言。在xDial-Eval上，我们对先前的基于BERT的度量和最近出现的大型语言模型进行了全面的分析。最后，我们建立了强大的评估基准，为多语言对话评估提供了基础。",
    "tldr": "本论文提出了xDial-Eval，这是一个基于开源英语对话评估数据集构建的多语言开放领域对话评估基准。通过对12个轮次级别和6个对话级别的英语数据集进行扩展，实现了对其他九种语言的对话评估。通过对先前的基于BERT的度量和最近出现的大型语言模型的全面分析，建立了强大的评估基准。",
    "en_tdlr": "This paper proposes xDial-Eval, a multilingual open-domain dialogue evaluation benchmark built on top of open-source English dialogue evaluation datasets. It extends the evaluation to nine other languages by expanding the English datasets at turn-level and dialogue-level. Through comprehensive analyses of previous BERT-based metrics and recently-emerged large language models, a strong evaluation benchmark is established for multilingual dialogue evaluation."
}