{
    "title": "Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization. (arXiv:2310.18860v1 [stat.ML])",
    "abstract": "We present a novel method for tuning the regularization hyper-parameter, $\\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\\lambda$ and the regression coefficients to be jointly learned wi",
    "link": "http://arxiv.org/abs/2310.18860",
    "context": "Title: Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization. (arXiv:2310.18860v1 [stat.ML])\nAbstract: We present a novel method for tuning the regularization hyper-parameter, $\\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\\lambda$ and the regression coefficients to be jointly learned wi",
    "path": "papers/23/10/2310.18860.json",
    "total_tokens": 861,
    "translated_title": "Bayes战胜交叉验证：通过期望最大化实现高效准确的岭回归",
    "translated_abstract": "我们提出了一种新的方法来调节岭回归的正则化超参数λ，该方法的计算速度比留一交叉验证(LOOCV)快，同时在稀疏协变量的情况下可以获得与LOOCV相等或更好的回归参数估计。对于有限的n，LOOCV风险可能受到多个和不好的局部最小值的影响，因此需要指定一组候选的λ，这可能无法提供良好的解决方案。相反，我们证明了所提出的方法在足够大的n下可以找到唯一的最优解，并且不需要指定任何难以确定的超参数。这是基于岭回归的贝叶斯公式，我们证明了对于足够大的n，后验是单峰的，可以同时学习最优的λ和回归系数。",
    "tldr": "本文提出了一种基于贝叶斯公式的岭回归方法，通过期望最大化来调节正则化超参数，该方法不需要指定候选的λ并且在大样本下可以找到唯一的最优解。"
}