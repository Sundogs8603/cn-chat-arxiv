{
    "title": "Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])",
    "abstract": "Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at ",
    "link": "http://arxiv.org/abs/2310.01452",
    "context": "Title: Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])\nAbstract: Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at ",
    "path": "papers/23/10/2310.01452.json",
    "total_tokens": 903,
    "translated_title": "通过随机化潜在表示来迷惑文本愚弄者",
    "translated_abstract": "尽管在各种自然语言处理任务中表现出色，但近期的研究表明，自然语言处理模型容易受到敌对攻击的影响，即微小地改变输入以导致模型的错误行为。其中，敌对词级扰动是一个被广泛研究和有效的攻击策略。这些攻击在黑盒设置中起作用，不需要访问模型结构或参数，因此可能对现有的自然语言处理应用产生不利影响。为了进行攻击，对手多次查询受害模型，以确定输入文本中最重要的单词，并用它们对应的同义词替换这些单词。在这项工作中，我们提出了一种轻量级和攻击无关的防御，其主要目标是困惑基于查询的黑盒攻击中产生敌对示例的过程；即愚弄文本愚弄者。这种防御名为AdvFooler，通过随机化输入的潜在表示来实现。",
    "tldr": "该论文提出了一种轻量级的攻击无关防御策略AdvFooler，通过随机化输入的潜在表示来困惑基于查询的黑盒攻击，从而迷惑文本愚弄者。"
}