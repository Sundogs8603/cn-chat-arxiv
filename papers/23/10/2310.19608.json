{
    "title": "On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])",
    "abstract": "Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.",
    "link": "http://arxiv.org/abs/2310.19608",
    "context": "Title: On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])\nAbstract: Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.",
    "path": "papers/23/10/2310.19608.json",
    "total_tokens": 824,
    "translated_title": "论费曼-卡克训练部分贝叶斯神经网络",
    "translated_abstract": "最近，部分贝叶斯神经网络(pBNNs)被证明与全贝叶斯神经网络具有竞争力，但pBNNs在潜变量空间中往往是多峰的，因此用参数模型来近似是具有挑战性的。为了解决这个问题，我们提出了一种高效的基于采样的训练策略，即将pBNN的训练转化为模拟费曼-卡克模型。我们还描述了序贯蒙特卡洛采样器的变种，使我们能够以可行的计算成本同时估计参数和该模型的潜在后验分布。我们在各种合成和真实世界的数据集上展示了我们提出的训练方案在预测性能方面优于现有技术。",
    "tldr": "本文提出了一种将部分贝叶斯神经网络训练转化为模拟费曼-卡克模型的高效采样训练策略，并通过各种数据集的实验证明其在预测性能方面优于现有技术。",
    "en_tdlr": "This paper proposes an efficient sampling-based training strategy that formulates the training of partial Bayesian neural networks as simulating a Feynman-Kac model, and it demonstrates superior predictive performance compared to existing techniques on various datasets."
}