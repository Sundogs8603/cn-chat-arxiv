{
    "title": "Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])",
    "abstract": "A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando",
    "link": "http://arxiv.org/abs/2310.10943",
    "context": "Title: Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])\nAbstract: A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando",
    "path": "papers/23/10/2310.10943.json",
    "total_tokens": 891,
    "translated_title": "在自主赛车中达到极限: 最优控制与强化学习比较",
    "translated_abstract": "机器人学中一个核心问题是如何为敏捷移动机器人设计控制系统。本文系统地研究了这个问题，重点是自主无人机赛车。我们发现，在这个设置下，使用强化学习(RL)训练的神经网络控制器胜过最优控制(OC)方法。然后我们调查了哪些基本因素对RL的成功或OC的限制有所贡献。我们的研究表明，RL相对于OC的根本优势不在于优化目标的效果更好，而是在于它优化了一个更好的目标。OC将问题分解为规划和控制，使用一个明确的中间表示，如轨迹，作为接口。这种分解限制了控制器可以表达的行为范围，当面临未建模的影响时，导致控制性能较差。相反，RL可以直接优化任务层面的目标，并且可以利用领域随机因素。",
    "tldr": "这项研究比较了最优控制 (OC)和强化学习 (RL)方法在自主无人机赛车中的效果，发现强化学习方法优于最优控制方法。研究表明，强化学习能够直接优化任务层面的目标，并利用领域的随机因素，而最优控制的分解限制了控制器的行为范围。"
}