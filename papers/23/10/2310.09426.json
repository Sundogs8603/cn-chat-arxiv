{
    "title": "Offline Reinforcement Learning for Optimizing Production Bidding Policies. (arXiv:2310.09426v1 [cs.LG])",
    "abstract": "The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary ba",
    "link": "http://arxiv.org/abs/2310.09426",
    "context": "Title: Offline Reinforcement Learning for Optimizing Production Bidding Policies. (arXiv:2310.09426v1 [cs.LG])\nAbstract: The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary ba",
    "path": "papers/23/10/2310.09426.json",
    "total_tokens": 925,
    "translated_title": "线下强化学习用于优化生产竞标策略",
    "translated_abstract": "在线广告市场每秒进行数千次拍卖，对于希望在预算限制下优化支出的广告商来说，这是一个艰巨的挑战。因此，广告平台通常为他们的客户提供自动化代理人，代表他们实时大规模竞标。由于这些代理人由平台拥有但使用广告商的资金进行操作，因此在平衡代理人的可靠性和可解释性与优化性能方面存在着强烈的实际需求。我们提出了一种通过离线强化学习从真实数据中学习的方法，以优化生产环境中的竞标策略。这种方法可以用于优化任何可微分的基础策略（实际上是基于广告商能够轻松理解的原则的启发式策略），并且只需要由基础策略本身生成的数据。我们使用混合代理架构，将任意基础策略与强化学习模块相结合。",
    "tldr": "该论文介绍了一种使用离线强化学习方法来优化生产环境中竞标策略的通用方法，该方法可以优化任何可微分的基础策略，只需要使用基础策略生成的数据。论文提出了一种混合代理架构，将基础策略与强化学习模块相结合。"
}