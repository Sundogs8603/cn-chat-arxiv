{
    "title": "CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training. (arXiv:2310.13292v1 [cs.CV])",
    "abstract": "A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.",
    "link": "http://arxiv.org/abs/2310.13292",
    "context": "Title: CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training. (arXiv:2310.13292v1 [cs.CV])\nAbstract: A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.",
    "path": "papers/23/10/2310.13292.json",
    "total_tokens": 1008,
    "translated_title": "CXR-CLIP：面向大规模胸部 X 射线语言-图像预训练的方法",
    "translated_abstract": "一个大规模的图文配对数据集对于视觉-语言预训练模型的发展做出了巨大贡献，这些模型使得在没有昂贵标注的情况下实现了零样本或少样本分类。然而，在医学领域，数据的稀缺性仍然是开发强大的视觉-语言预训练模型的重大挑战。本文通过扩展图像-标签对为图像-文本对，并利用放射学报告中的多个图像和多个部分来解决胸部 X 射线图像中的图文数据缺乏问题。我们还设计了两种对比损失，分别称为 ICL 和 TCL，用于学习医学图像和报告的研究层面特征。我们的模型在相同条件下的训练中优于现有技术水平的模型。此外，扩大的数据集提高了我们预训练模型的分类能力，而牺牲了较小的检索性能。代码可在 https://github.com/kakaobrain/cxr-clip 获得。",
    "tldr": "本文提出了一种面向大规模胸部 X 射线语言-图像预训练的方法，通过扩展图像-标签对为图像-文本对，并利用放射学报告中的多个图像和多个部分来解决胸部 X 射线图像中的数据稀缺性问题。我们的模型在相同条件下的训练中优于现有技术水平的模型，并通过扩充数据集提高了我们预训练模型的分类能力。",
    "en_tdlr": "This paper proposes a method for large-scale chest X-ray language-image pre-training, which tackles the scarcity of data by expanding image-label pairs to image-text pairs and utilizing multiple images and sections in radiologic reports. The model outperforms existing models under the same conditions and improves classification capabilities through an enlarged dataset."
}