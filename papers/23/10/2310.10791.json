{
    "title": "Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs. (arXiv:2310.10791v1 [cs.LG])",
    "abstract": "Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experime",
    "link": "http://arxiv.org/abs/2310.10791",
    "context": "Title: Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs. (arXiv:2310.10791v1 [cs.LG])\nAbstract: Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experime",
    "path": "papers/23/10/2310.10791.json",
    "total_tokens": 935,
    "translated_title": "神经切向核函数为具有交叉协方差图的图神经网络提供了动力学",
    "translated_abstract": "神经切向核函数（NTKs）提供了分析过参数化神经网络的学习和泛化行为的理论基础。对于有监督学习任务，NTK核函数的特征向量与给定数据之间的关联（在本文中称为对齐）可以控制梯度下降的收敛速度以及对未见数据的泛化能力。在这个概念的基础上，我们研究了NTKs和对齐在图神经网络（GNNs）的背景下的应用，我们的分析揭示了优化对齐等价于优化GNN中的图表示或图移位运算符。我们的结果进一步建立了对于两层GNN对齐的最优性的理论保证，这些保证由图移位运算符作为输入和输出数据之间的交叉协方差函数的函数所决定。通过对NTKs的分析得出的理论洞察力，通过我们的实验证实了这些洞察力。",
    "tldr": "本文研究了神经切向核函数（NTKs）在图神经网络（GNNs）中的应用。我们发现优化对齐等价于优化GNN中的图表示或图移位运算符，并建立了对于两层GNN对齐的最优性的理论保证。"
}