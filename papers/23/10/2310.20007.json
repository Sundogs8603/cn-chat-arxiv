{
    "title": "Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])",
    "abstract": "In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\\widetilde{O}(H\\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.",
    "link": "http://arxiv.org/abs/2310.20007",
    "context": "Title: Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])\nAbstract: In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\\widetilde{O}(H\\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.",
    "path": "papers/23/10/2310.20007.json",
    "total_tokens": 897,
    "translated_title": "提升强化学习中汤普森采样的贝叶斯遗憾界",
    "translated_abstract": "本文证明了在多种情境下，汤普森采样在强化学习中的第一个贝叶斯遗憾界。我们利用离散的代理环境简化学习问题，并通过后验一致性对信息比进行了精确分析。这导致了一个基于时间不均匀强化学习问题的上界估计为$\\widetilde{O}(H\\sqrt{d_{l_1}T})$，其中$H$为回合长度，$d_{l_1}$为环境空间的Kolmogorov $l_1$维度。然后，我们在各种设置中找到了$d_{l_1}$的具体界限，比如表格、线性和有限混合，讨论了我们的结果是第一个其类别或改进了最先进方法的情况。",
    "tldr": "本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。",
    "en_tdlr": "This paper proves the Bayesian regret bounds for Thompson Sampling in reinforcement learning in various settings, and presents an upper bound estimation for time inhomogeneous reinforcement learning problem through a refined analysis of the information ratio. Concrete bounds for different settings are obtained, and the paper discusses how these results are either the first of their kind or improve the state-of-the-art."
}