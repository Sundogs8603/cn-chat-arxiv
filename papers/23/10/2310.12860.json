{
    "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities. (arXiv:2310.12860v1 [cs.CL])",
    "abstract": "Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select three large language models (GPT-3.5, text-davinci and Flan-T5) and three datasets HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (i",
    "link": "http://arxiv.org/abs/2310.12860",
    "context": "Title: Probing LLMs for hate speech detection: strengths and vulnerabilities. (arXiv:2310.12860v1 [cs.CL])\nAbstract: Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select three large language models (GPT-3.5, text-davinci and Flan-T5) and three datasets HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (i",
    "path": "papers/23/10/2310.12860.json",
    "total_tokens": 912,
    "translated_title": "探究用于检测仇恨言论的LLM：优势和脆弱性",
    "translated_abstract": "社交媒体平台和研究人员最近做出了努力，使用大型语言模型来检测具有仇恨或有毒语言的存在。然而，这些研究并未在检测过程中利用解释、额外上下文和受害社区的信息。我们使用不同的提示变化和输入信息，在零样本设置下评估大型语言模型（不添加任何上下文示例）。我们选择了三个大型语言模型（GPT-3.5、text-davinci和Flan-T5）和三个数据集HateXplain、implicit hate和ToxicSpans。我们发现，平均而言，在管道中包含目标信息大大提高了模型性能（约20-30%）在各个数据集上相比基线。将理由/解释添加到管道中也有显著效果（约10-20%）在各个数据集上相比基线。此外，我们进一步提供了这些大型语言模型无法分类和预测错误的错误案例的分类方法。",
    "tldr": "本研究探究了使用大型语言模型检测仇恨言论的方法，并发现在模型中包含目标信息和理由/解释可以显著提高其性能。同时，我们提供了模型分类和预测错误的错误案例的分类方法。",
    "en_tdlr": "This study investigates the use of large language models for hate speech detection and finds that including target information and explanations in the model significantly improves its performance. Additionally, it provides a typology of error cases where these models fail to classify and predict accurately."
}