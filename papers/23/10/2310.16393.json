{
    "title": "ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters. (arXiv:2310.16393v1 [cs.CL])",
    "abstract": "We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource unseen languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and o",
    "link": "http://arxiv.org/abs/2310.16393",
    "context": "Title: ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters. (arXiv:2310.16393v1 [cs.CL])\nAbstract: We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource unseen languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and o",
    "path": "papers/23/10/2310.16393.json",
    "total_tokens": 988,
    "translated_title": "ZGUL: 使用多源合并的语言适配器实现对未见语言的零样本泛化",
    "translated_abstract": "我们通过使用语言适配器（LAs）来解决NLP任务中的零样本跨语言转移问题。之前的大部分工作都是探索训练使用单一源（通常是英文）的适配器，然后使用目标适配器或其他相关语言的适配器进行测试。训练目标适配器需要无标记数据，但对于低资源未见语言，这些数据可能不容易获得：这些语言既不被基础的多语言语言模型（如mBERT）所见，也没有任何标记或无标记的数据。我们认为，为了更有效地进行跨语言转移，我们需要在训练和测试时利用多个（在语言或地理上相关的）源语言的适配器，我们通过我们的新颖神经网络架构ZGUL进行了研究。在涵盖15种未见目标语言的四个语言组之间进行了大量实验，结果显示相对于标准微调和只使用一个源适配器，我们的方法平均F1得分提高了高达3.2个百分点。",
    "tldr": "ZGUL是一个使用多源合并的语言适配器的神经网络架构，用于解决零样本跨语言转移问题。在15个未见目标语言的实验中，相对于标准微调和单一源适配器，我们的方法平均F1得分提高了3.2个百分点。"
}