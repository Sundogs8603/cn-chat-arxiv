{
    "title": "Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation. (arXiv:2310.11026v1 [cs.CL])",
    "abstract": "Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.",
    "link": "http://arxiv.org/abs/2310.11026",
    "context": "Title: Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation. (arXiv:2310.11026v1 [cs.CL])\nAbstract: Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.",
    "path": "papers/23/10/2310.11026.json",
    "total_tokens": 992,
    "translated_title": "基于解码器的LLM的文本生成自动评估方法探索",
    "translated_abstract": "对于提高生成任务准确性，文本生成的自动评估至关重要。鉴于当前趋势是使用越来越大的基于解码器的语言模型，我们研究基于这种模型的文本生成自动评估方法。本文在相同条件下比较了不同方法，包括使用基于编码器的模型和大型语言模型进行调优，分别在两种不同任务（机器翻译评估和语义文本相似性）以及两种语言（日语和英语）上进行实验。实验结果表明，与调优后的基于编码器的模型相比，调优后的基于解码器的模型表现较差。对于造成这种情况的原因的分析表明，基于解码器的模型更关注表面词序列而忽略了含义。研究还显示，像ChatGPT这样的非常大的解码器模型的上下文学习使得难以识别细粒度的语义差异。",
    "tldr": "本文探索了基于解码器的LLM的文本生成自动评估方法，发现相比调优后的基于编码器的模型，调优后的基于解码器的模型在机器翻译评估和语义文本相似性任务上表现较差，原因是解码器模型更关注表面词序列而忽略了含义，并且非常大的解码器模型的上下文学习使得难以识别细粒度的语义差异。",
    "en_tdlr": "This paper explores automatic evaluation methods for text generation based on decoder-based LLM. It compares the performance of tuned encoder-based models and tuned decoder-based models on machine translation evaluation and semantic textual similarity tasks in both Japanese and English. The results show that the decoder-based models perform poorly compared to the encoder-based models, indicating their focus on surface word sequences rather than capturing meaning. The study also reveals that context learning in very large decoder-based models like ChatGPT makes it challenging to identify fine-grained semantic differences."
}