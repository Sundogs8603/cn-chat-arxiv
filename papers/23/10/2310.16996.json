{
    "title": "Towards Continually Learning Application Performance Models. (arXiv:2310.16996v1 [cs.LG])",
    "abstract": "Machine learning-based performance models are increasingly being used to build critical job scheduling and application optimization decisions. Traditionally, these models assume that data distribution does not change as more samples are collected over time. However, owing to the complexity and heterogeneity of production HPC systems, they are susceptible to hardware degradation, replacement, and/or software patches, which can lead to drift in the data distribution that can adversely affect the performance models. To this end, we develop continually learning performance models that account for the distribution drift, alleviate catastrophic forgetting, and improve generalizability. Our best model was able to retain accuracy, regardless of having to learn the new distribution of data inflicted by system changes, while demonstrating a 2x improvement in the prediction accuracy of the whole data sequence in comparison to the naive approach.",
    "link": "http://arxiv.org/abs/2310.16996",
    "context": "Title: Towards Continually Learning Application Performance Models. (arXiv:2310.16996v1 [cs.LG])\nAbstract: Machine learning-based performance models are increasingly being used to build critical job scheduling and application optimization decisions. Traditionally, these models assume that data distribution does not change as more samples are collected over time. However, owing to the complexity and heterogeneity of production HPC systems, they are susceptible to hardware degradation, replacement, and/or software patches, which can lead to drift in the data distribution that can adversely affect the performance models. To this end, we develop continually learning performance models that account for the distribution drift, alleviate catastrophic forgetting, and improve generalizability. Our best model was able to retain accuracy, regardless of having to learn the new distribution of data inflicted by system changes, while demonstrating a 2x improvement in the prediction accuracy of the whole data sequence in comparison to the naive approach.",
    "path": "papers/23/10/2310.16996.json",
    "total_tokens": 876,
    "translated_title": "迈向持续学习应用性能模型",
    "translated_abstract": "基于机器学习的性能模型越来越被用于构建关键的作业调度和应用程序优化决策。然而，传统上，这些模型假设随着时间的推移，数据分布不会发生改变。然而，由于生产HPC系统的复杂性和异构性，它们容易受到硬件退化、更换和/或软件补丁的影响，这可能导致数据分布漂移，从而对性能模型产生不利影响。为此，我们开发了一种持续学习的性能模型，考虑了分布漂移，缓解了灾难性遗忘，并提高了泛化能力。我们的最佳模型能够在学习系统变化引起的新数据分布的同时保持准确性，而且相对于简单方法，整个数据序列的预测准确性提高了2倍。",
    "tldr": "本论文提出了一种能够持续学习的性能模型，考虑到数据分布的漂移，缓解灾难性遗忘，并提高了泛化能力。最佳模型在学习系统变化引起的新数据分布的同时保持了准确性，并相对于简单方法，全面数据序列的预测准确性提高了2倍。"
}