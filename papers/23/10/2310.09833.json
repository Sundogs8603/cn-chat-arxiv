{
    "title": "MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)",
    "abstract": "Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust",
    "link": "http://arxiv.org/abs/2310.09833",
    "context": "Title: MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)\nAbstract: Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust",
    "path": "papers/23/10/2310.09833.json",
    "total_tokens": 966,
    "translated_title": "MIR2:面向通过互信息正则化进行可证明鲁棒多智能体强化学习",
    "translated_abstract": "鲁棒多智能体强化学习(MARL)对于未知盟友的不确定或最坏情况行动需要具备弹性。现有的鲁棒MARL中的最大最小优化技术通过训练智能体抵抗最坏情况的对手来增强鲁棒性，但随着智能体数量的增加，这种方法变得难以操作，导致最坏情况的数量呈指数级增长。试图简化这种复杂性往往会导致过于悲观的策略、在各种情况下鲁棒性不足和高计算需求。与这些方法不同，人类在学习适应性和鲁棒行为时自然而然地不需要准备每种可能的最坏情况。受此启发，我们提出了MIR2，它在常规情况下训练策略，并将互信息最小化作为鲁棒正则化。从理论上讲，我们将鲁棒性视为一个推理问题，并证明了在历史和行动之间最小化互信息隐含地最大化了鲁棒性的下界。",
    "tldr": "MIR2提出了一种针对鲁棒多智能体强化学习的方法，通过在常规情况下训练策略并最小化互信息作为鲁棒正则化，实现了在不准备每种可能的最坏情况的情况下提升鲁棒性的目标。"
}