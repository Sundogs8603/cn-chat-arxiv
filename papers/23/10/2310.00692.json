{
    "title": "The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])",
    "abstract": "Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.",
    "link": "http://arxiv.org/abs/2310.00692",
    "context": "Title: The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])\nAbstract: Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.",
    "path": "papers/23/10/2310.00692.json",
    "total_tokens": 951,
    "translated_title": "随机梯度下降的噪声几何：定量和分析特征的研究",
    "translated_abstract": "实证研究表明，随机梯度下降（SGD）中的噪声与损失函数的局部几何特征有利的一致性。然而，对于这种现象的理论和定量解释仍然不足。本文对过参数化线性模型和两层神经网络的上述“噪声几何”进行了全面的理论研究。我们细致地研究了平均和方向的一致性，特别关注样本大小和输入数据退化对一致性强度的影响。作为特定应用，我们利用噪声几何特征研究了SGD如何从尖锐极小值中逃脱，发现逃脱方向在平坦方向上有显著分量，这与只在最尖锐方向逃脱的梯度下降方法GD形成鲜明对比。为了验证我们的理论发现，我们进行了合成和真实世界的实验。",
    "tldr": "本文对随机梯度下降（SGD）中的噪声几何进行了全面的理论研究，发现噪声与损失函数的局部几何特征有利的一致性。通过实验证明，SGD在逃脱尖锐极小值时与GD形成鲜明对比，逃脱方向在平坦方向上有显著分量。"
}