{
    "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
    "abstract": "arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins",
    "link": "https://arxiv.org/abs/2310.05175",
    "context": "Title: Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity\nAbstract: arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins",
    "path": "papers/23/10/2310.05175.json",
    "total_tokens": 893,
    "translated_title": "Outlier Weighed Layerwise Sparsity (OWL): 为剪枝LLMs达到高稀疏度提供缺失的秘密调味料",
    "translated_abstract": "大型语言模型（LLMs）以在各个领域展现出的卓越性能而闻名，在实际部署时由于模型庞大而面临挑战。为了解决这一挑战，人们努力将传统的网络剪枝技术应用于LLMs，发现可以在不影响性能的情况下一次性剪掉大量参数。现有的LLM剪枝策略一直坚持以等价稀疏度均匀剪裁所有层的做法，结果表现强劲。然而，这个观察结果与在视觉模型领域观察到的非均匀逐层稀疏的主流趋势相矛盾，后者通常会产生更好的结果。为了了解这种差异背后的原因，我们进行了全面研究，并发现与LLMs中异常值的出现强相关。",
    "tldr": "本研究发现LLMs中的激活异常值与网络层稀疏度的非均匀性相关，并提出了Outlier Weighed Layerwise Sparsity（OWL）作为剪枝LLMs到高稀疏度的秘密调味料。",
    "en_tdlr": "This study discovers a strong correlation between activation outliers in LLMs and non-uniform layerwise sparsity, and proposes Outlier Weighed Layerwise Sparsity (OWL) as a missing secret sauce for pruning LLMs to high sparsity."
}