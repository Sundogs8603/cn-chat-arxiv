{
    "title": "Multilingual Jailbreak Challenges in Large Language Models",
    "abstract": "arXiv:2310.06474v2 Announce Type: replace  Abstract: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases",
    "link": "https://arxiv.org/abs/2310.06474",
    "context": "Title: Multilingual Jailbreak Challenges in Large Language Models\nAbstract: arXiv:2310.06474v2 Announce Type: replace  Abstract: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases",
    "path": "papers/23/10/2310.06474.json",
    "total_tokens": 842,
    "translated_title": "大语言模型中的多语言越狱挑战",
    "translated_abstract": "虽然大语言模型(LLMs)在各种任务中表现出色，但它们存在潜在的安全问题，例如“越狱”问题，即恶意指令可能操纵LLMs表现出不良行为。尽管已经制定了几种预防措施来减轻与LLMs相关的潜在风险，但这些措施主要集中在英语上。本研究揭示了LLMs中存在的多语言越狱挑战，并考虑了两种潜在的风险场景：非故意和故意。非故意场景涉及用户使用非英语提示查询LLMs并无意中绕过安全机制，而故意场景涉及恶意用户将恶意指令与多语言提示结合起来，故意攻击LLMs。实验结果显示，在非故意场景中，不安全内容的比率增加了。",
    "tldr": "该研究揭示了大语言模型中存在的多语言越狱挑战，包括用户使用非英语提示绕过安全机制的非故意场景和恶意用户利用多语言提示恶意攻击LLMs的故意场景。"
}