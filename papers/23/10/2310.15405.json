{
    "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])",
    "abstract": "There is growing interest in systems that generate captions for scientific figures. However, assessing these systems output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates, achieving a Kendall correlat",
    "link": "http://arxiv.org/abs/2310.15405",
    "context": "Title: GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])\nAbstract: There is growing interest in systems that generate captions for scientific figures. However, assessing these systems output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates, achieving a Kendall correlat",
    "path": "papers/23/10/2310.15405.json",
    "total_tokens": 917,
    "translated_title": "GPT-4作为科学图表标题的有效零-shot评估器",
    "translated_abstract": "越来越多的人对生成科学图表标题的系统产生了兴趣。然而，评估这些系统的输出带来了很大的挑战。人工评估需要学术专长，并且成本较高，而自动评估依赖于通常质量较低的作者编写的标题。本文研究了使用大型语言模型（LLMs）作为一种经济实惠、无参考方法来评估图表标题。首先，我们构建了SCICAP-EVAL，这是一个人工评估数据集，包含了3600个科学图表标题的人工判断，包括原始标题和机器生成的标题，涵盖了600个arXiv图表。然后，我们对GPT-4和GPT-3等LLMs进行评分（1-6），评估它们在给定相关上下文（如提及图表的段落）的情况下，每个标题对读者理解的潜力。结果显示，作为零-shot评估器，GPT-4表现优于其他所有模型，甚至超过了计算机科学和信息学本科生的评估，达到了Kendall相关系数",
    "tldr": "本文研究了使用大型语言模型作为经济实惠的方法来评估科学图表标题，并发现GPT-4作为零-shot评估器表现优于其他模型和人工评估。"
}