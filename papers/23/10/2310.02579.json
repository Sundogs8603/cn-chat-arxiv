{
    "title": "On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])",
    "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \\emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \"hard partition\" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \"softly partition\" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp",
    "link": "http://arxiv.org/abs/2310.02579",
    "context": "Title: On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])\nAbstract: Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \\emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \"hard partition\" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \"softly partition\" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp",
    "path": "papers/23/10/2310.02579.json",
    "total_tokens": 1043,
    "translated_title": "关于图神经网络中表达位置编码的稳定性",
    "translated_abstract": "设计有效的图位置编码对构建强大的图转换器和增强消息传递图神经网络非常关键。尽管广泛使用，使用拉普拉斯特征向量作为位置编码面临两个根本性挑战：（1）\\emph{非唯一性}：同一拉普拉斯矩阵存在许多不同的特征分解，以及（2）\\emph{不稳定性}：对拉普拉斯矩阵的微小扰动可能导致完全不同的特征空间，从而导致位置编码的不可预测性变化。尽管有很多尝试解决非唯一性的方法，但大多数方法忽视了稳定性，导致在未见过的图结构上表现不佳。我们发现，不稳定性的原因是特征空间的\"硬分割\"。因此，我们引入了稳定且表达丰富的位置编码（SPE），这是一种用于处理特征向量的架构，利用特征值将特征空间进行\"软分割\"。SPE是首个（1）可证明稳定的架构，以及（2）普适地提升图结构泛化性能的架构。",
    "tldr": "本研究针对图神经网络中使用拉普拉斯特征向量作为位置编码面临的非唯一性和不稳定性问题，提出了稳定且表达丰富的位置编码方法（SPE），该方法通过利用特征值对特征空间进行\"软分割\"，在未见过的图结构上表现出良好的泛化能力。",
    "en_tdlr": "This study addresses the issues of non-uniqueness and instability in using Laplacian eigenvectors as positional encodings for graph neural networks. The proposed Stable and Expressive Positional Encodings (SPE) method soft partitions the eigenspaces using eigenvalues, resulting in improved generalization on unseen graph structures."
}