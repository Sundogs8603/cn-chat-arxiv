{
    "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. (arXiv:2310.02054v1 [cs.AI])",
    "abstract": "Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director fo",
    "link": "http://arxiv.org/abs/2310.02054",
    "context": "Title: AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. (arXiv:2310.02054v1 [cs.AI])\nAbstract: Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director fo",
    "path": "papers/23/10/2310.02054.json",
    "total_tokens": 929,
    "translated_title": "AlignDiff: 通过可定制行为扩散模型对齐多样的人类偏好",
    "translated_abstract": "将代理行为与多样的人类偏好相一致仍然是强化学习中的一个挑战问题，这是由于人类偏好的抽象性和可变性的内在特性所致。为了解决这些问题，我们提出了AlignDiff，一种利用强化学习从人类反馈中量化人类偏好的新颖框架，涵盖了抽象性，并利用这些偏好来引导零-shot行为定制的扩散规划，涵盖了可变性。AlignDiff能够准确匹配用户定制的行为并高效地在不同行为之间切换。为了构建这个框架，我们首先建立了多角度的人类反馈数据集，其中包含了对不同行为属性进行比较的数据，并训练了一个属性强度模型来预测量化的相对强度。在使用相对强度重新标记行为数据集之后，我们继续训练了一个属性条件的扩散模型，该模型作为一个规划器，属性强度模型作为导演。",
    "tldr": "本研究提出了一种新的框架AlignDiff，通过将强化学习与人类反馈相结合，量化人类偏好并指导零-shot行为定制的扩散规划，解决了将代理行为与多样的人类偏好相一致的挑战问题。",
    "en_tdlr": "This paper proposes a novel framework called AlignDiff, which combines reinforcement learning and human feedback to quantify human preferences and guide zero-shot behavior customization through diffusion planning, addressing the challenge of aligning agent behaviors with diverse human preferences."
}