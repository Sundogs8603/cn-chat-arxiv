{
    "title": "Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])",
    "abstract": "Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $",
    "link": "http://arxiv.org/abs/2310.18919",
    "context": "Title: Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])\nAbstract: Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $",
    "path": "papers/23/10/2310.18919.json",
    "total_tokens": 734,
    "translated_title": "延迟反馈的线性函数逼近强化学习中的后验采样",
    "translated_abstract": "运用函数逼近在强化学习中取得了显著进展，但现有的高效算法通常依赖于即时反馈。本文通过采用后验采样来解决延迟反馈对强化学习中线性函数逼近的挑战，首先介绍了Delayed-PSVI算法，通过后验采样中的噪声扰动有效地探索价值函数空间。我们提供了延迟反馈强化学习中后验采样算法的首次分析，并展示了我们的算法在一系列情况下的优越性。",
    "tldr": "本研究解决了强化学习中延迟反馈对线性函数逼近的挑战，通过后验采样算法实现了在不同情况下的优越性能。",
    "en_tdlr": "This study addresses the challenge of delayed feedback in reinforcement learning with linear function approximation, and shows superior performance in various scenarios through the use of posterior sampling algorithms."
}