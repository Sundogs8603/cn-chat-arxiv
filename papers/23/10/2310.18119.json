{
    "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])",
    "abstract": "In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance whi",
    "link": "http://arxiv.org/abs/2310.18119",
    "context": "Title: Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])\nAbstract: In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance whi",
    "path": "papers/23/10/2310.18119.json",
    "total_tokens": 867,
    "translated_title": "实现统一的对话推荐系统：通过上下文化知识蒸馏的多任务学习",
    "translated_abstract": "在对话推荐系统中，要求代理向用户推荐一组项目，而推荐过程发生在自然语言对话中。为了解决对话能力和个性化推荐的需求，之前的研究使用了分离的推荐和对话模块。然而，这种方法不可避免地导致推荐结果和生成的回应之间存在差异。为了弥合这一差距，我们提出了一种通过上下文化知识蒸馏的多任务学习方法来实现统一的对话推荐系统。我们引入了两个版本的上下文化知识蒸馏方法：硬门和软门。前者在两个任务特定的教师之间进行有选择的门控，而后者整合了两个教师的知识。我们的门控以上下文特定的方式实时计算，便于灵活地整合相关知识。大量实验证明我们的单一模型显著提高了推荐性能，同时也提高了对话生成的一致性。",
    "tldr": "通过上下文化知识蒸馏的多任务学习方法，我们提出了一种统一的对话推荐系统，该系统在推荐性能和对话生成的一致性方面取得了显著改进。"
}