{
    "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
    "abstract": "arXiv:2310.17653v2 Announce Type: replace  Abstract: Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such \"complementary\" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer i",
    "link": "https://arxiv.org/abs/2310.17653",
    "context": "Title: Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model\nAbstract: arXiv:2310.17653v2 Announce Type: replace  Abstract: Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such \"complementary\" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer i",
    "path": "papers/23/10/2310.17653.json",
    "total_tokens": 875,
    "translated_title": "关于任何预训练模型之间存在和前景的通用知识转移",
    "translated_abstract": "训练深度网络需要关于架构、数据增强或优化等各种设计决策。在这项工作中，我们发现这些训练变化导致网络从数据中学习出独特的特征集。利用包括数千个模型在诸如ImageNet之类的经典数据集上训练的公共模型库，我们观察到对于任意预训练模型的配对，一个模型提取出另一个模型中不可用的重要数据背景--而这与整体性能无关。鉴于任何任意预训练模型的配对以及没有外部排名（如独立测试集，例如由于数据隐私），我们探讨是否可能在不降低性能的情况下从一个模型转移这种“互补”知识到另一个模型--这一任务尤其困难，因为更强大、性能相同或更弱的模型中可能包含额外知识。然而，促进强大的转移",
    "tldr": "通过研究预训练模型之间的知识转移，并尝试在不降低性能的情况下实现“互补”知识的传递，本研究探讨了如何提高模型之间的通用知识转移。",
    "en_tdlr": "By investigating knowledge transfer between pretrained models and attempting to transfer \"complementary\" knowledge without performance degradation, this study explores ways to enhance general knowledge transfer between models."
}