{
    "title": "Language Models are Universal Embedders. (arXiv:2310.08232v1 [cs.CL])",
    "abstract": "In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines ",
    "link": "http://arxiv.org/abs/2310.08232",
    "context": "Title: Language Models are Universal Embedders. (arXiv:2310.08232v1 [cs.CL])\nAbstract: In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines ",
    "path": "papers/23/10/2310.08232.json",
    "total_tokens": 869,
    "translated_title": "语言模型是通用的嵌入器",
    "translated_abstract": "在大型语言模型（LLM）革命中，嵌入是各种系统的关键组成部分。例如，它被用于为LLMs检索知识或记忆，构建内容过滤器等。由于这些情况涉及从英语到其他自然或编程语言，从检索到分类等各种情况，因此建立一个统一的嵌入模型而不是为每个场景专门建立一个是可取的。在这项工作中，我们迈出了朝这个目标迈出了初始的一步，证明了多语言（自然语言和编程语言）预训练的Transformer解码器在有限的英文数据微调后能够通用地进行嵌入。我们提供了全面的实践，并进行了彻底的评估。在英文MTEB上，我们的模型在不使用大量训练数据的情况下在不同的嵌入任务上达到了竞争性的性能。在其他基准测试中，例如多语言分类和代码搜索，我们的模型（没有任何监督）表现出与或甚至超过大量监督基线的可比性。",
    "tldr": "该论文证明了多语言预训练的Transformer解码器在有限英文数据微调后能够通用地进行嵌入，实现了统一嵌入模型的目标。",
    "en_tdlr": "This paper demonstrates that multilingual pre-trained Transformer decoders can universally embed data after limited English fine-tuning, achieving the goal of a unified embedding model."
}