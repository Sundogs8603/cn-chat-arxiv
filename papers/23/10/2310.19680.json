{
    "title": "Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)",
    "abstract": "Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW",
    "link": "http://arxiv.org/abs/2310.19680",
    "context": "Title: Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)\nAbstract: Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW",
    "path": "papers/23/10/2310.19680.json",
    "total_tokens": 919,
    "translated_title": "将预训练语言模型整合到神经机器翻译中",
    "translated_abstract": "通过广泛的研究和开发，神经机器翻译（NMT）已成为自然语言处理中的重要技术。然而，高质量的双语语言对数据的不足仍然是提高NMT性能的主要挑战。最近的研究一直在探索使用预训练语言模型（PLM）的上下文信息来解决这个问题。然而，PLM和NMT模型之间的不兼容问题尚未解决。本研究提出了PLM整合的NMT（PiNMT）模型来解决这些问题。PiNMT模型由三个关键组成部分组成，分别是PLM多层转换器，嵌入融合和余弦对齐，每个部分在向NMT提供有效的PLM信息方面发挥着重要作用。此外，本文还介绍了两种训练策略，分别是分离学习率和双步训练。通过实施所提出的PiNMT模型和训练策略，在IW数据集上实现了最先进的性能。",
    "tldr": "该论文提出了PiNMT模型，将预训练语言模型整合到神经机器翻译中，通过三个关键部分和两种训练策略，实现了在IW数据集上的最先进性能。",
    "en_tdlr": "This paper presents the PiNMT model, which integrates pre-trained language model into neural machine translation. By utilizing three critical components and two training strategies, it achieves state-of-the-art performance on the IW dataset."
}