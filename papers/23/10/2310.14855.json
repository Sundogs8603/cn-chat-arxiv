{
    "title": "Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing",
    "abstract": "arXiv:2310.14855v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM's) have demonstrated considerable success in various Natural Language Processing tasks, but they have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLM's for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation. To overcome this, we propose an alternative approach: adapting LLM's as Automatic Post-Editors (APE) rather than direct translators. Building on the LLM's exceptional ability to process and generate lengthy sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-t",
    "link": "https://arxiv.org/abs/2310.14855",
    "context": "Title: Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing\nAbstract: arXiv:2310.14855v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM's) have demonstrated considerable success in various Natural Language Processing tasks, but they have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLM's for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation. To overcome this, we propose an alternative approach: adapting LLM's as Automatic Post-Editors (APE) rather than direct translators. Building on the LLM's exceptional ability to process and generate lengthy sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-t",
    "path": "papers/23/10/2310.14855.json",
    "total_tokens": 869,
    "translated_title": "翻译的上下文细化：句子和文档级后编辑的大型语言模型",
    "translated_abstract": "大型语言模型(LLM)已在各种自然语言处理任务中取得了相当大的成功，但它们尚未在神经机器翻译(NMT)中达到最先进的性能。然而，在要求广泛理解和上下文处理的任务中表现出的显著性能显示了它们在翻译中的潜力。为了利用这些能力，我们研究了使用LLM进行MT，并探索了最近的参数高效微调技术。令人惊讶的是，我们的初步实验发现，即使为翻译目的微调，也会导致性能下降。为了克服这一问题，我们提出了一种替代方法：将LLM作为自动后编辑器(APE)而不是直接转换器。基于LLM处理和生成长序列的异常能力，我们还提出将我们的方法扩展到文档级翻译。我们展示了利用低秩适配器进行 ...",
    "tldr": "大型语言模型在神经机器翻译中表现尚未达到最先进水平，提出使用LLM作为自动后编辑器(APE)的替代方法，同时探索了扩展到文档级翻译的可能性。",
    "en_tdlr": "Large Language Models have not achieved state-of-the-art performance in Neural Machine Translation yet, proposing an alternative approach of using LLM as Automatic Post-Editors (APE) and exploring the extension to document-level translation."
}