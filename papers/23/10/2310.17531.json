{
    "title": "Learning Regularized Graphon Mean-Field Games with Unknown Graphons. (arXiv:2310.17531v1 [cs.GT])",
    "abstract": "We design and analyze reinforcement learning algorithms for Graphon Mean-Field Games (GMFGs). In contrast to previous works that require the precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of the regularized GMFGs when the graphons are unknown. Our contributions are threefold. First, we propose the Proximal Policy Optimization for GMFG (GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$ after $T$ iterations with an estimation oracle, improving on a previous work by Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we design efficient algorithms to estimate the transition kernels, reward functions, and graphons from sampled agents. Convergence rates are then derived when the positions of the agents are either known or unknown. Results for the combination of the optimization algorithm GMFG-PPO and the estimation algorithm are then provided. These algorithms are the first specifically designed for learning graphons f",
    "link": "http://arxiv.org/abs/2310.17531",
    "context": "Title: Learning Regularized Graphon Mean-Field Games with Unknown Graphons. (arXiv:2310.17531v1 [cs.GT])\nAbstract: We design and analyze reinforcement learning algorithms for Graphon Mean-Field Games (GMFGs). In contrast to previous works that require the precise values of the graphons, we aim to learn the Nash Equilibrium (NE) of the regularized GMFGs when the graphons are unknown. Our contributions are threefold. First, we propose the Proximal Policy Optimization for GMFG (GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$ after $T$ iterations with an estimation oracle, improving on a previous work by Xie et al. (ICML, 2021). Second, using kernel embedding of distributions, we design efficient algorithms to estimate the transition kernels, reward functions, and graphons from sampled agents. Convergence rates are then derived when the positions of the agents are either known or unknown. Results for the combination of the optimization algorithm GMFG-PPO and the estimation algorithm are then provided. These algorithms are the first specifically designed for learning graphons f",
    "path": "papers/23/10/2310.17531.json",
    "total_tokens": 945,
    "translated_title": "学习带未知图核的规范化图均场博弈",
    "translated_abstract": "我们设计并分析了用于图均场博弈的强化学习算法。与之前需要精确图核值的工作不同，我们旨在学习当图核未知时的规范化图均场博弈的纳什均衡。我们的贡献有三个：首先，我们提出了用于图均场博弈的近端策略优化算法（GMFG-PPO），并通过一个估计预言机证明了其在T次迭代后以$O(T^{-1/3})$的速率收敛，改进了Xie等人（ICML，2021）的前期工作。其次，利用分布的核嵌入，我们设计了高效的算法来估计采样代理的转移核、奖励函数和图核。当代理的位置已知或未知时，推导了收敛速率。然后提供了运用GMFG-PPO优化算法和估计算法的结果。这些算法是首次专门设计用于学习图核的算法。",
    "tldr": "我们设计了用于学习图核均值场博弈的强化学习算法，通过近端策略优化算法GMFG-PPO以及核嵌入的方法来估计未知图核。我们的算法在收敛速度和效率方面取得了改进，并提供了相关的理论分析。",
    "en_tdlr": "We propose reinforcement learning algorithms for learning graphon mean-field games with unknown graphons. Our contributions include the GMFG-PPO algorithm, which converges at a rate of $O(T^{-1/3})$ and improves on previous work, and efficient algorithms for estimating unknown graphons using kernel embedding."
}