{
    "title": "Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])",
    "abstract": "Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard",
    "link": "http://arxiv.org/abs/2310.03354",
    "context": "Title: Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])\nAbstract: Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard",
    "path": "papers/23/10/2310.03354.json",
    "total_tokens": 1062,
    "translated_title": "虚构交叉玩: 在混合合作竞争游戏中学习全局纳什均衡",
    "translated_abstract": "自我对弈（SP）是一种常用的多智能体强化学习（MARL）框架，用于解决竞争游戏，在这种框架下，每个智能体通过将其他智能体视为环境的一部分来优化策略。尽管在实证研究中取得了成功，但是SP方法的理论性质仅限于两人零和游戏。然而，在混合合作竞争游戏中，需要团队中的智能体相互合作，我们可以通过一个简单的反例来证明SP方法无法以高概率收敛到全局纳什均衡（NE）。作为替代方法，策略空间响应预测（PSRO）是一种学习NE的迭代框架，其中在每次迭代中学习相对于先前策略的最佳响应。PSRO可以直接扩展为混合合作竞争场景，同时学习团队最佳响应而所有收敛性质均保持不变。然而，PSRO需要重复从头开始训练联合策略直到收敛，这使得它变得困难。",
    "tldr": "该论文介绍了自我对弈和策略空间响应预测（PSRO）作为解决竞争游戏的强化学习框架，发现自我对弈方法在混合合作竞争游戏中无法收敛到全局纳什均衡（NE），而PSRO能够在这种情况下有效地学习到最佳响应。然而，PSRO需要重复训练联合策略，增加了难度。"
}