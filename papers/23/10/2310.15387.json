{
    "title": "Error analysis of generative adversarial network. (arXiv:2310.15387v1 [stat.ML])",
    "abstract": "The generative adversarial network (GAN) is an important model developed for high-dimensional distribution learning in recent years. However, there is a pressing need for a comprehensive method to understand its error convergence rate. In this research, we focus on studying the error convergence rate of the GAN model that is based on a class of functions encompassing the discriminator and generator neural networks. These functions are VC type with bounded envelope function under our assumptions, enabling the application of the Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli lemma, we establish a tight convergence rate for the error of GAN. This method can also be applied on existing error estimations of GAN and yields improved convergence rates. In particular, the error defined with the neural network distance is a special case error in our definition.",
    "link": "http://arxiv.org/abs/2310.15387",
    "context": "Title: Error analysis of generative adversarial network. (arXiv:2310.15387v1 [stat.ML])\nAbstract: The generative adversarial network (GAN) is an important model developed for high-dimensional distribution learning in recent years. However, there is a pressing need for a comprehensive method to understand its error convergence rate. In this research, we focus on studying the error convergence rate of the GAN model that is based on a class of functions encompassing the discriminator and generator neural networks. These functions are VC type with bounded envelope function under our assumptions, enabling the application of the Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli lemma, we establish a tight convergence rate for the error of GAN. This method can also be applied on existing error estimations of GAN and yields improved convergence rates. In particular, the error defined with the neural network distance is a special case error in our definition.",
    "path": "papers/23/10/2310.15387.json",
    "total_tokens": 784,
    "translated_title": "生成对抗网络的误差分析",
    "translated_abstract": "生成对抗网络（GAN）是近年来用于高维分布学习的重要模型。然而，急需一种全面的方法来理解其误差收敛速度。本研究着重研究基于鉴别器和生成器神经网络的函数类别构建的GAN模型的误差收敛速度。在我们的假设下，这些函数属于VC类型，并具有有界信封函数，可以应用Talagrand不等式。通过运用Talagrand不等式和Borel-Cantelli引理，我们建立了GAN误差的紧致收敛速度。该方法还可以应用于现有的GAN误差估计，并获得改进的收敛速度。特别是，在我们的定义中，用神经网络距离定义的误差是一种特殊情况误差。",
    "tldr": "该论文利用Talagrand不等式和Borel-Cantelli引理，建立了生成对抗网络（GAN）的误差紧致收敛速度，并提供了改进的收敛速度方法。"
}