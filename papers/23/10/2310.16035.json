{
    "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models. (arXiv:2310.16035v1 [cs.CV])",
    "abstract": "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like \"left\" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with",
    "link": "http://arxiv.org/abs/2310.16035",
    "context": "Title: What's Left? Concept Grounding with Logic-Enhanced Foundation Models. (arXiv:2310.16035v1 [cs.CV])\nAbstract: Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like \"left\" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with",
    "path": "papers/23/10/2310.16035.json",
    "total_tokens": 925,
    "translated_title": "《剩下什么？逻辑增强的基础模型用于概念赋权》",
    "translated_abstract": "最近的研究，如VisProg和ViperGPT，巧妙地结合了基础模型与视觉推理-使用大型语言模型（LLMs）生成可以由预训练的视觉语言模型执行的程序。然而，它们只在有限的领域中操作，比如2D图像，没有充分利用语言的概括性：抽象概念如“左”也可以通过3D、时间和动作数据进行赋权，例如向左移动。这种有限的概括性源于这些仅推理的方法无法在新领域中学习或调整预训练模型。我们提出了逻辑增强的基础模型（LEFT），它是一个统一的框架，通过一个可微分、与领域无关的一阶逻辑程序执行器，学习在不同领域中对概念进行赋权和推理。LEFT具有一个LLM解释器，输出用一种通用的基于逻辑推理的推理语言表示的程序，该程序在所有领域和任务之间共享。LEFT的推理器然后执行该程序，",
    "tldr": "逻辑增强的基础模型（LEFT）提出了一个统一的框架，通过一个可微分、与领域无关的一阶逻辑程序执行器，在不同领域中对概念进行赋权和推理。",
    "en_tdlr": "The Logic-Enhanced Foundation Model (LEFT) proposes a unified framework that learns to ground and reason with concepts across domains, using a differentiable, domain-independent, first-order logic-based program executor."
}