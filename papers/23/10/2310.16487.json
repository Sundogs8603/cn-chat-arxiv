{
    "title": "Hyperparameter Optimization for Multi-Objective Reinforcement Learning. (arXiv:2310.16487v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) has emerged as a powerful approach for tackling complex problems. The recent introduction of multi-objective reinforcement learning (MORL) has further expanded the scope of RL by enabling agents to make trade-offs among multiple objectives. This advancement not only has broadened the range of problems that can be tackled but also created numerous opportunities for exploration and advancement. Yet, the effectiveness of RL agents heavily relies on appropriately setting their hyperparameters. In practice, this task often proves to be challenging, leading to unsuccessful deployments of these techniques in various instances. Hence, prior research has explored hyperparameter optimization in RL to address this concern.  This paper presents an initial investigation into the challenge of hyperparameter optimization specifically for MORL. We formalize the problem, highlight its distinctive challenges, and propose a systematic methodology to address it. The proposed me",
    "link": "http://arxiv.org/abs/2310.16487",
    "context": "Title: Hyperparameter Optimization for Multi-Objective Reinforcement Learning. (arXiv:2310.16487v1 [cs.LG])\nAbstract: Reinforcement learning (RL) has emerged as a powerful approach for tackling complex problems. The recent introduction of multi-objective reinforcement learning (MORL) has further expanded the scope of RL by enabling agents to make trade-offs among multiple objectives. This advancement not only has broadened the range of problems that can be tackled but also created numerous opportunities for exploration and advancement. Yet, the effectiveness of RL agents heavily relies on appropriately setting their hyperparameters. In practice, this task often proves to be challenging, leading to unsuccessful deployments of these techniques in various instances. Hence, prior research has explored hyperparameter optimization in RL to address this concern.  This paper presents an initial investigation into the challenge of hyperparameter optimization specifically for MORL. We formalize the problem, highlight its distinctive challenges, and propose a systematic methodology to address it. The proposed me",
    "path": "papers/23/10/2310.16487.json",
    "total_tokens": 816,
    "translated_title": "多目标强化学习的超参数优化",
    "translated_abstract": "强化学习已经成为解决复杂问题的一种强大方法。多目标强化学习(MORL)的引入进一步扩展了强化学习的范围，使代理能够在多个目标之间进行权衡。这一进展不仅扩大了可以解决的问题范围，还为探索和进步创造了许多机会。然而，强化学习代理的有效性严重依赖于适当设置它们的超参数。在实践中，这一任务常常很具有挑战性，导致这些技术在各种实例中无法成功部署。因此，先前的研究在强化学习中探索了超参数优化以解决这一问题。本文针对MORL对超参数优化的挑战进行了初步研究。我们规范化了该问题，突出了其独特的挑战，并提出了一种系统的方法来解决它。",
    "tldr": "该论文研究了多目标强化学习中的超参数优化的挑战，并提出了一种系统的方法来解决这一问题。"
}