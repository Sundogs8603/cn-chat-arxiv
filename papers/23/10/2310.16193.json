{
    "title": "Length is a Curse and a Blessing for Document-level Semantics. (arXiv:2310.16193v1 [cs.CL])",
    "abstract": "In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically robust sentence r",
    "link": "http://arxiv.org/abs/2310.16193",
    "context": "Title: Length is a Curse and a Blessing for Document-level Semantics. (arXiv:2310.16193v1 [cs.CL])\nAbstract: In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically robust sentence r",
    "path": "papers/23/10/2310.16193.json",
    "total_tokens": 1040,
    "translated_title": "长度对于文档级语义而言既是诅咒也是福音",
    "translated_abstract": "最近几年，对比学习（CL）已经广泛应用于从预训练的语言模型中恢复句子和文档级别的编码能力。在这项工作中，我们质疑基于CL的模型的长度泛化能力，即它们对于长度引起的语义变化的易受攻击的程度。我们验证了长度易受攻击是一个重要但被忽视的研究空白，并且我们可以设计仅依赖于文档长度提供的语义信号的无监督CL方法。我们首先推导了长度攻击的理论基础，表明延长文档会加 intensify 已经由CL带来的高内部文档相似性。此外，我们发现CL承诺的等向性高度依赖于训练中暴露的文本长度范围。受到这些发现的启发，我们引入了一个简单而通用的文档表示学习框架，LA(SER)$^{3}$: 长度不受限的自我参照用于语义鲁棒的句子表示学习",
    "tldr": "本文研究了基于对比学习的模型在长度上的泛化能力，并提出了一个仅依赖于文档长度的无监督学习方法。研究发现，延长文档的长度会加 intensify 达到的高内部相似性，并且这种等向性的表现高度依赖于文本长度范围。基于这些发现，提出了一个简单而通用的文档表示学习框架，用于实现语义鲁棒的句子表示学习。",
    "en_tdlr": "This paper investigates the length generalizability of CL-based models and proposes an unsupervised learning method that solely depends on document length. It is found that elongating document length intensifies the high intra-document similarity brought by CL, and the isotropic performance is highly dependent on the length range of text. Inspired by these findings, a simple and universal document representation learning framework is introduced for semantically robust sentence representation learning."
}