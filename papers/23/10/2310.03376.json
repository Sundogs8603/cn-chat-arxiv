{
    "title": "Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])",
    "abstract": "Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training ",
    "link": "http://arxiv.org/abs/2310.03376",
    "context": "Title: Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])\nAbstract: Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training ",
    "path": "papers/23/10/2310.03376.json",
    "total_tokens": 859,
    "translated_title": "使用大型语言模型的程序性文本挖掘",
    "translated_abstract": "最近在自然语言处理领域取得的进展，特别是在预训练了大量知识的大规模语言模型的发展，为知识工程领域带来了新机遇。本文主要研究了在零样本学习和上下文学习环境中使用大语言模型(LLMs)来解决从非结构化PDF文本中以增量问答方式提取程序的问题。具体而言，我们利用了目前最先进的GPT-4 (Generative Pre-trained Transformer 4)模型，结合了两种上下文学习的变体，包括带有程序和步骤定义的本体和有限数量的少样本学习。研究结果突出了这种方法的潜力和上下文学习定制化的价值。这些修改有望显著解决获取足够训练的挑战。",
    "tldr": "本文研究了使用大型语言模型进行程序性文本挖掘的方法。通过在零样本和上下文学习环境中使用GPT-4模型和自定义技术，有效地从非结构化PDF文本中提取程序。实验结果证明了该方法的潜力和价值。"
}