{
    "title": "Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach. (arXiv:2310.16647v1 [cs.LG])",
    "abstract": "Regularizing Deep Neural Networks (DNNs) is essential for improving generalizability and preventing overfitting. Fixed penalty methods, though common, lack adaptability and suffer from hyperparameter sensitivity. In this paper, we propose a novel approach to DNN regularization by framing the training process as a constrained optimization problem. Where the data fidelity term is the minimization objective and the regularization terms serve as constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method to achieve a more flexible and efficient regularization mechanism. Our approach extends beyond black-box regularization, demonstrating significant improvements in white-box models, where weights are often subject to hard constraints to ensure interpretability. Experimental results on image-based classification on MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our approach. SAL consistently achieves higher Accuracy while also achieving better constrain",
    "link": "http://arxiv.org/abs/2310.16647",
    "context": "Title: Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach. (arXiv:2310.16647v1 [cs.LG])\nAbstract: Regularizing Deep Neural Networks (DNNs) is essential for improving generalizability and preventing overfitting. Fixed penalty methods, though common, lack adaptability and suffer from hyperparameter sensitivity. In this paper, we propose a novel approach to DNN regularization by framing the training process as a constrained optimization problem. Where the data fidelity term is the minimization objective and the regularization terms serve as constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method to achieve a more flexible and efficient regularization mechanism. Our approach extends beyond black-box regularization, demonstrating significant improvements in white-box models, where weights are often subject to hard constraints to ensure interpretability. Experimental results on image-based classification on MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our approach. SAL consistently achieves higher Accuracy while also achieving better constrain",
    "path": "papers/23/10/2310.16647.json",
    "total_tokens": 856,
    "translated_title": "实现神经网络中的约束：一种随机增广拉格朗日方法",
    "translated_abstract": "正则化深度神经网络（DNNs）对于改善泛化能力和防止过拟合至关重要。固定惩罚方法虽然常见，但缺乏适应性并且对超参数敏感。在本文中，我们提出了一种新颖的DNN正则化方法，将训练过程构建为一个约束优化问题。其中数据保真度项是最小化目标，正则化项作为约束。然后，我们采用随机增广拉格朗日（SAL）方法实现更加灵活和高效的正则化机制。我们的方法不仅适用于黑盒正则化，还在白盒模型中展现了显著的提升，其中权重常常受到硬约束以确保可解释性。在MNIST、CIFAR10和CIFAR100数据集上的图像分类实验结果验证了我们方法的有效性。SAL始终能够获得更高的准确度同时实现更好的约束。",
    "tldr": "本文提出了一种使用随机增广拉格朗日方法实现更灵活和高效的神经网络正则化机制的方法，在黑盒和白盒模型上都取得了显著改进。",
    "en_tdlr": "This paper presents a method for achieving more flexible and efficient neural network regularization using the Stochastic Augmented Lagrangian approach, which demonstrates significant improvements in both black-box and white-box models."
}