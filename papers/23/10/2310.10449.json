{
    "title": "Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)",
    "abstract": "Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large",
    "link": "http://arxiv.org/abs/2310.10449",
    "context": "Title: Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)\nAbstract: Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large",
    "path": "papers/23/10/2310.10449.json",
    "total_tokens": 991,
    "translated_title": "使用大型语言模型的文本摘要: MPT-7b-instruct、Falcon-7b-instruct和OpenAI Chat-GPT模型的比较研究",
    "translated_abstract": "文本摘要是一项重要的自然语言处理任务，应用范围包括信息检索和内容生成。利用大型语言模型在提升摘要技术方面展示了显著的潜力。本文使用多种大型语言模型（包括MPT-7b-instruct，falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型）进行文本摘要的探索。实验使用不同的超参数，并使用诸如双语评估衡量（BLEU）分数，面向回忆的视角评估（ROUGE）分数和双向编码器表示转换器（BERT）分数等广泛接受的指标评估生成的摘要。根据实验，text-davinci-003的性能优于其他模型。本次研究涉及CNN Daily Mail和XSum这两个不同的数据集，主要目标是全面了解大型语言模型在文本摘要中的性能。",
    "tldr": "本研究通过比较MPT-7b-instruct, Falcon-7b-instruct和OpenAI Chat-GPT模型，在不同的数据集上使用不同的超参数进行了文本摘要实验。实验结果表明，text-davinci-003模型表现最佳，并且提供了大型语言模型在文本摘要中的性能综述。",
    "en_tdlr": "This study presents a comparative analysis of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT models for text summarization. With experiments conducted on different datasets and using various hyperparameters, it is found that the text-davinci-003 model performs the best. The study aims to provide a comprehensive understanding of the performance of large language models in text summarization."
}