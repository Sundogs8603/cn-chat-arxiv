{
    "title": "Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])",
    "abstract": "We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\\texttt{MT-CO}_2\\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\\texttt{MT-CO}_2\\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.",
    "link": "http://arxiv.org/abs/2310.17385",
    "context": "Title: Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])\nAbstract: We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\\texttt{MT-CO}_2\\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\\texttt{MT-CO}_2\\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.",
    "path": "papers/23/10/2310.17385.json",
    "total_tokens": 842,
    "translated_title": "多任务在线学习：倾听社区的喧嚣",
    "translated_abstract": "我们研究了一种多任务在线学习的设置，其中代理只能在任意通信网络上与其邻居交换信息。我们介绍了一种分散算法$\\texttt{MT-CO}_2\\texttt{OL}$，其遗憾值取决于任务相似性和网络结构的相互作用。我们的分析表明，$\\texttt{MT-CO}_2\\texttt{OL}$的遗憾值（常数除外）永远不会比代理不共享信息时获得的上界差。另一方面，当相邻代理在相似的任务上操作时，我们的界限显著改善。此外，我们证明了在损失函数为线性函数时，我们的算法可以在隐私保护性上做到微不足道的影响。最后，我们提供了实验证据支持我们的理论。",
    "tldr": "我们提出了一种多任务在线学习的算法，代理只能通过邻居交换信息。我们的分析表明，当代理在相似的任务上操作时，我们的算法的遗憾值显著改善。此外，我们证明了算法在损失函数为线性函数时可以保护隐私。"
}