{
    "title": "Projecting basis functions with tensor networks for Gaussian process regression. (arXiv:2310.20630v1 [stat.ML])",
    "abstract": "This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the",
    "link": "http://arxiv.org/abs/2310.20630",
    "context": "Title: Projecting basis functions with tensor networks for Gaussian process regression. (arXiv:2310.20630v1 [stat.ML])\nAbstract: This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the",
    "path": "papers/23/10/2310.20630.json",
    "total_tokens": 813,
    "translated_title": "使用张量网络为高斯过程回归投影基函数",
    "translated_abstract": "本文提出了一种使用张量网络对高斯过程回归进行近似的方法。GP的参数化近似使用基函数的线性组合，其中近似的准确性取决于总基函数数量 $M$。我们提出了一种能够使用指数数量基函数而不引起相应指数计算复杂性的方法。实现这一点的关键思想是使用低秩张量网络。我们首先从数据中找到一个适当的低维子空间，该子空间由低秩张量网络描述。在这个低维子空间中，我们通过求解贝叶斯推理问题来推断出模型的权重。最后，我们将得到的权重投影回原始空间进行GP预测。我们的方法的好处在于投影到一个较小的子空间：它根据给定数据适应性地修改基函数的形状，并且允许进行高效的计算。",
    "tldr": "本文提出了一种使用低秩张量网络实现高斯过程回归的方法，该方法允许在指数数量的基函数情况下进行高效计算。",
    "en_tdlr": "This paper proposes a method for Gaussian process regression using low-rank tensor networks, enabling efficient computations with exponential number of basis functions."
}