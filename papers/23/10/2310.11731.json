{
    "title": "Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning. (arXiv:2310.11731v1 [cs.AI])",
    "abstract": "The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\\\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed",
    "link": "http://arxiv.org/abs/2310.11731",
    "context": "Title: Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning. (arXiv:2310.11731v1 [cs.AI])\nAbstract: The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\\\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed",
    "path": "papers/23/10/2310.11731.json",
    "total_tokens": 909,
    "translated_title": "机器人技能学习的离线强化学习方法——动作量化离线强化学习",
    "translated_abstract": "离线强化学习（RL）范式提供了一种将静态行为数据集转化为比收集数据的策略表现更好的策略的通用方法。尽管策略约束、保守性和其他缓解分布偏移的方法使得离线强化学习更加有效，但在连续动作设置中，往往需要各种近似方法来应用这些技术。许多这些挑战在离散动作设置中得到了很大的缓解，离线RL约束和规则化器往往可以更精确或甚至完全计算出来。在本文中，我们提出了一种自适应的动作量化方案。我们使用VQ-VAE来学习状态条件下的动作量化，避免了动作空间的朴素离散化所带来的指数级增长。我们展示了一些最先进的离线RL方法，如IQL、CQL和BRAC，在基准测试中与我们提出的方法相结合后取得了更好的性能。",
    "tldr": "本文提出了一种适应性动作量化方案，通过使用VQ-VAE来学习状态条件下的动作量化，从而改善了机器人技能学习的离线强化学习在离散动作设置下的表现。",
    "en_tdlr": "This paper proposes an adaptive scheme for action quantization in offline reinforcement learning for robotic skill learning, by using VQ-VAE to learn state-conditioned action quantization, thereby improving the performance of offline RL methods in discrete action settings."
}