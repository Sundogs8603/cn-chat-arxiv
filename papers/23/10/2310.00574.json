{
    "title": "SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs. (arXiv:2310.00574v2 [cs.AR] UPDATED)",
    "abstract": "We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.",
    "link": "http://arxiv.org/abs/2310.00574",
    "context": "Title: SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs. (arXiv:2310.00574v2 [cs.AR] UPDATED)\nAbstract: We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.",
    "path": "papers/23/10/2310.00574.json",
    "total_tokens": 792,
    "translated_title": "SIMD数据流共优化用于CPU上高效的神经网络推理",
    "translated_abstract": "我们针对在CPU上部署神经网络所面临的挑战提出了解决方案，特别关注的是在保持准确性的同时最小化推理时间。我们的新颖方法是利用神经网络的数据流（即计算顺序），通过启发式引导分析和代码生成框架来探索数据重用机会，从而实现各种单指令多数据（SIMD）实现以实现优化的神经网络执行。我们的结果表明，将输出保持在SIMD寄存器中的数据流同时最大化输入和权重重用，在各种推理工作负载下始终能够获得最佳性能，相比今天的神经网络优化实现，8位神经网络的加速比可达3倍，而二进制神经网络的加速比可达4.8倍。",
    "tldr": "我们提出了一种通过共优化数据流和SIMD实现来高效地在CPU上进行神经网络推理的方法，实验结果表明，这种方法能够在保持准确性的同时大幅提升推理速度。",
    "en_tdlr": "We propose a method that efficiently performs neural network inference on CPUs by co-optimizing the dataflow and SIMD implementation. Experimental results show that this method can significantly improve inference speed while maintaining accuracy."
}