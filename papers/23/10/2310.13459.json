{
    "title": "Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])",
    "abstract": "This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of ",
    "link": "http://arxiv.org/abs/2310.13459",
    "context": "Title: Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])\nAbstract: This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of ",
    "path": "papers/23/10/2310.13459.json",
    "total_tokens": 941,
    "translated_title": "稳定的非凸-非凹训练通过线性插值",
    "translated_abstract": "本文提出了一种关于线性插值的理论分析，作为一种稳定（大规模）神经网络训练的方法。我们认为优化过程中的不稳定性通常是由损失函数的非单调性引起的，并展示了线性插值如何通过利用“非扩张算子”的理论来帮助解决这个问题。我们构建了一种新的优化方案，称为松弛近似近端点（RAPP），这是第一个明确的方法，能够实现完整范围内的协调部分单调问题的最后迭代收敛速率。该构造可扩展到约束和正则化设置。通过替换RAPP中的内部优化器，我们重新发现了Lookahead算法族，我们证明了这些算法在协调部分单调问题中的收敛性，即使基本优化器采用梯度下降升级算法。通过利用Lookahead继承性质，我们进一步扩展了Lookahead在协调部分单调问题中收敛的范围。",
    "tldr": "本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。"
}