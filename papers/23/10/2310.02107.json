{
    "title": "Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance. (arXiv:2310.02107v2 [cs.CL] UPDATED)",
    "abstract": "Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dat",
    "link": "http://arxiv.org/abs/2310.02107",
    "context": "Title: Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance. (arXiv:2310.02107v2 [cs.CL] UPDATED)\nAbstract: Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dat",
    "path": "papers/23/10/2310.02107.json",
    "total_tokens": 966,
    "translated_title": "实例需要更加细致的关怀：为实例重写提示提高了零样本性能",
    "translated_abstract": "让大型语言模型能够在零样本情况下执行任务一直是一个吸引人的目标，因为它可以节省人力（即无需任务特定的注释）；因此，零样本提示方法也享有更好的任务泛化能力。为了提高大型语言模型的零样本性能，先前的工作着重于设计更有效的任务指导（例如“我们一步一步思考”）。然而，我们认为，为了让大型语言模型能够在零样本情况下正确解决问题，每个单独的测试实例需要更仔细地设计和定制的指导。为此，我们提出了PRoMPTd，一种为每个个别的测试输入重新写作任务提示的方法，使其更加具体、明确和完整，以更好地指导任务的大型语言模型。我们使用GPT-4作为任务型大型语言模型，在涵盖算术、逻辑推理和代码生成等任务的8个数据集上对PRoMPTd进行了评估。值得注意的是，PRoMPTd在复杂MATH数据集上的绝对改进约为10%。",
    "tldr": "为了提高大型语言模型的零样本性能，本文提出了一种新的方法，通过为每个个别的测试输入重新写作任务提示，使其更具体、明确和完整，从而提供更好的指导，实现了约10%的改进。"
}