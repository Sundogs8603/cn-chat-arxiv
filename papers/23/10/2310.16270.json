{
    "title": "Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])",
    "abstract": "Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.",
    "link": "http://arxiv.org/abs/2310.16270",
    "context": "Title: Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])\nAbstract: Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.",
    "path": "papers/23/10/2310.16270.json",
    "total_tokens": 792,
    "translated_title": "注意力镜头：一种解释注意力头信息检索机制的工具",
    "translated_abstract": "基于Transformer的大型语言模型(LLMs)是自然语言任务的最先进技术。最近的研究尝试通过逆向工程线性层的作用，解码LLMs为文本完成任务做出最终预测的内部机制。然而，关于注意力头在生成最终标记预测中的具体作用还知之甚少。我们提出了Attention Lens，一个工具，可以通过学习的注意力头特定转换(称为镜头)将注意力头的输出翻译为词汇标记。我们训练的镜头的初步发现表明，注意力头在语言模型中扮演着高度专门化的角色。Attention Lens的代码可在github.com/msakarvadia/AttentionLens上获得。",
    "tldr": "Attention Lens是一种工具，它能够通过学习的注意力头特定转换将注意力头的输出翻译为词汇标记。使用Attention Lens，我们可以解释注意力头在生成最终标记预测中的作用。注意力头在语言模型中扮演着高度专门化的角色。",
    "en_tdlr": "Attention Lens is a tool that translates the outputs of attention heads into vocabulary tokens through learned attention-head-specific transformations called lenses. It allows us to interpret the role of attention heads in generating final token predictions, revealing their highly specialized roles in language models."
}