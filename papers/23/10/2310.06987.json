{
    "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])",
    "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as \"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computat",
    "link": "http://arxiv.org/abs/2310.06987",
    "context": "Title: Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])\nAbstract: The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as \"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computat",
    "path": "papers/23/10/2310.06987.json",
    "total_tokens": 988,
    "translated_title": "通过利用生成技术实施开源LLM的灾难性越狱",
    "translated_abstract": "开源大规模语言模型（LLMs）的快速发展极大地推进了人工智能的发展。在发布模型之前，人们进行了大量的努力，以确保模型与人类价值观的一致性，主要目标是确保其有益且无害。然而，即使经过精心对齐的模型也可能被恶意操纵，导致意外行为，即所谓的“越狱”。这些越狱通常由特定的文本输入触发，通常称为对抗性提示。在这项工作中，我们提出了一种生成攻击技术，这是一种极其简单的方法，仅通过操纵解码方法的变化来扰乱模型的对齐。通过利用不同的生成策略，包括变化的解码超参数和采样方法，我们将11个语言模型，包括LLaMA2、Vicuna、Falcon和MPT家族的错位率从0%提高到了95%以上，超过了最先进的攻击方法，计算量降低了30倍。",
    "tldr": "本文介绍了一种生成攻击技术，通过操纵解码方法的变化，可以导致开源LLMs的灾难性越狱，将错位率从0%提高到了超过95%。这项攻击方法简单而有效，在11个语言模型中表现优于最先进的攻击方法，且计算量降低了30倍。"
}