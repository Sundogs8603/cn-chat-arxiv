{
    "title": "On Learning Gaussian Multi-index Models with Gradient Flow. (arXiv:2310.19793v1 [stat.ML])",
    "abstract": "We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.  We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these pos",
    "link": "http://arxiv.org/abs/2310.19793",
    "context": "Title: On Learning Gaussian Multi-index Models with Gradient Flow. (arXiv:2310.19793v1 [stat.ML])\nAbstract: We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.  We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these pos",
    "path": "papers/23/10/2310.19793.json",
    "total_tokens": 845,
    "translated_title": "关于使用梯度流学习高斯多索引模型的研究",
    "translated_abstract": "我们研究了高维高斯数据的多索引回归问题中的梯度流。多索引函数由未知的低秩线性投影和任意未知的低维连接函数组成。因此，它们构成了神经网络中特征学习的自然模板。我们考虑了一个两时间尺度的算法，其中低维连接函数通过非参数模型比参数化低秩投影的低维空间更快地学习。通过适当地利用框架的相关矩阵上的矩阵半群结构，我们建立了由Grassmannian人口梯度流动力学引起的全局收敛性，并对其相关的“鞍点到鞍点”动力学提供了定量描述。值得注意的是，每个鞍的时间尺度可以明确地用目标连接函数的适当Hermite分解来表征。与这些位置相反的是。",
    "tldr": "本研究探讨了在高维高斯数据的多索引回归问题中，通过梯度流学习低秩线性投影和低维连接函数，建立了全局收敛性和定量描述的算法。",
    "en_tdlr": "This research examines learning low-rank linear projections and low-dimensional link functions with gradient flow in the multi-index regression problem for high-dimensional Gaussian data, and establishes global convergence and quantitative description of the algorithm."
}