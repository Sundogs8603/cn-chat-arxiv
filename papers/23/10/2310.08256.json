{
    "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])",
    "abstract": "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear",
    "link": "http://arxiv.org/abs/2310.08256",
    "context": "Title: Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])\nAbstract: Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear",
    "path": "papers/23/10/2310.08256.json",
    "total_tokens": 1054,
    "translated_title": "大型语言模型中共现对事实知识的影响",
    "translated_abstract": "尽管大型语言模型在各种应用中取得了成功，但它们经常在事实上做出错误的回答。本文假设过度依赖于预训练语料库的简单共现统计是导致事实错误的主要因素之一。我们的结果表明，大型语言模型容易受到共现偏见的影响，即更倾向于选择频繁共现的词而不是正确答案。因此，尽管在微调期间已经见过这些事实的主题和对象在预训练数据集中很少共现，大型语言模型仍然难以回忆起这些事实。我们展示了即使扩大模型规模或进行微调，共现偏见仍然存在。因此，我们建议在去偏数据集上进行微调，通过过滤掉主题-对象共现计数高的偏见样本来减轻偏见。尽管去偏微调允许大型语言模型记忆训练集中的稀有事实，但在微调期间未见过的稀有事实的回忆效果并不明显。",
    "tldr": "大型语言模型在回答问题时常常出现事实错误，这主要是因为过度依赖预训练语料库的共现统计。研究结果表明，大型语言模型容易受到共现偏见的影响，导致难以回忆起预训练数据集中很少共现的事实。建议使用去偏数据集进行微调来减轻偏见，但这对于微调期间未见过的稀有事实的回忆效果并不明显。",
    "en_tdlr": "Large language models often provide factually incorrect responses due to their heavy reliance on simple co-occurrence statistics from pre-training corpora. This co-occurrence bias leads to difficulties in recalling facts that have limited co-occurrence in the training data. Finetuning on a debiased dataset is suggested to mitigate this bias, although it may not effectively recall rare facts unseen during finetuning."
}