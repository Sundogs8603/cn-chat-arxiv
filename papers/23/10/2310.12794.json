{
    "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and al",
    "link": "http://arxiv.org/abs/2310.12794",
    "context": "Title: Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v1 [cs.CL])\nAbstract: Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and al",
    "path": "papers/23/10/2310.12794.json",
    "total_tokens": 1031,
    "translated_title": "结构概念在Transformer语言模型中是否具有普适性？走向可解释的跨语言泛化",
    "translated_abstract": "大型语言模型(LLMs)展示了显著的跨语言泛化能力，即它们通过隐式知识传输在不同语言之间进行转移。然而，这种转移对于所有语言而言并不均衡，特别是对于资源匮乏的语言，这是一个持续存在的挑战。目前尚不清楚我们是否已经达到了隐式跨语言泛化的极限，并且明确的知识传输是否可行。在本文中，我们调查了明确对齐语言之间概念对应关系的潜力，以增强跨语言泛化能力。通过将语法方面作为测试平台，我们对43种语言的分析显示，无论是仅有编码器还是仅有解码器的LLMs，各种语言内的结构概念空间之间存在高度的对准性。然后，我们提出了一种基于元学习的方法来学习对齐不同语言的概念空间，从而便于在概念分类和对齐上进行零样本和少样本泛化。",
    "tldr": "本文研究了在Transformer语言模型中明确对齐语言之间的概念对应关系的潜力，以强化跨语言泛化能力。研究发现，无论是仅有编码器还是仅有解码器的模型，各语言内的结构概念空间对齐度高。通过基于元学习的方法，可以学习对齐不同语言的概念空间，实现零样本和少样本泛化。",
    "en_tdlr": "This paper investigates the potential of explicitly aligning conceptual correspondence between languages in Transformer language models to enhance cross-lingual generalization. The study reveals a high degree of alignability among the spaces of structural concepts within each language, and proposes a meta-learning-based method to learn to align conceptual spaces of different languages, facilitating zero-shot and few-shot generalization."
}