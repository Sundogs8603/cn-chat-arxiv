{
    "title": "Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])",
    "abstract": "Variational autoencoders (VAEs) face a notorious problem wherein the variational posterior often aligns closely with the prior, a phenomenon known as posterior collapse, which hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter $\\beta$ and a strategy for annealing this parameter, called KL annealing, are proposed. This study presents a theoretical analysis of the learning dynamics in a minimal VAE. It is rigorously proved that the dynamics converge to a deterministic process within the limit of large input dimensions, thereby enabling a detailed dynamical analysis of the generalization error. Furthermore, the analysis shows that the VAE initially learns entangled representations and gradually acquires disentangled representations. A fixed-point analysis of the deterministic process reveals that when $\\beta$ exceeds a certain threshold, posterior collapse becomes inevitable regardless of the learning period. Additionally, the superfluou",
    "link": "http://arxiv.org/abs/2310.15440",
    "context": "Title: Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])\nAbstract: Variational autoencoders (VAEs) face a notorious problem wherein the variational posterior often aligns closely with the prior, a phenomenon known as posterior collapse, which hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter $\\beta$ and a strategy for annealing this parameter, called KL annealing, are proposed. This study presents a theoretical analysis of the learning dynamics in a minimal VAE. It is rigorously proved that the dynamics converge to a deterministic process within the limit of large input dimensions, thereby enabling a detailed dynamical analysis of the generalization error. Furthermore, the analysis shows that the VAE initially learns entangled representations and gradually acquires disentangled representations. A fixed-point analysis of the deterministic process reveals that when $\\beta$ exceeds a certain threshold, posterior collapse becomes inevitable regardless of the learning period. Additionally, the superfluou",
    "path": "papers/23/10/2310.15440.json",
    "total_tokens": 1076,
    "translated_title": "Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing. (arXiv:2310.15440v1 [stat.ML])的学习动态：后验崩塌阈值，多余的潜在空间陷阱以及KL退火的加速。",
    "translated_abstract": "变分自编码器（VAEs）存在一个臭名昭著的问题，即变分后验通常与先验非常接近，这种现象称为后验崩塌，它阻碍了表示学习的质量。为了缓解这个问题，提出了一个可调的超参数β以及一种称为KL退火的策略来调整该参数。本研究在一个简化的VAE中对学习动态进行了理论分析。经过严格证明，在输入维度趋于无穷大的极限下，动态收敛为确定性过程，从而实现了对泛化误差的详细动态分析。此外，分析还表明，VAE最初学习到纠缠表示，并逐渐获得不纠缠的表示。对确定性过程的固定点分析揭示，当β超过一定阈值时，无论学习周期多长，后验崩塌都是不可避免的。此外，研究还探讨了超fluous潜在空间的问题。",
    "tldr": "该论文对线性VAE中的学习动态进行了理论分析，证明了在大输入维度的限制下，学习动态收敛为确定性过程，并提出了后验崩塌阈值和KL退火加速策略。研究发现，VAE最初学习到纠缠表示，并逐渐获得不纠缠的表示。在确定性过程中，超fluous潜在空间是一个潜在的问题。",
    "en_tdlr": "This paper provides a theoretical analysis of the learning dynamics in linear VAE and demonstrates that the dynamics converge to a deterministic process in the limit of large input dimensions. It introduces the concepts of posterior collapse threshold and KL annealing for addressing the notorious problem of posterior collapse. The paper also highlights the progression from entangled representations to disentangled representations in VAE learning. Additionally, it discusses the pitfalls of superfluous latent space."
}