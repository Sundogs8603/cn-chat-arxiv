{
    "title": "Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v1 [cs.CR])",
    "abstract": "Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhib",
    "link": "http://arxiv.org/abs/2310.06372",
    "context": "Title: Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v1 [cs.CR])\nAbstract: Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhib",
    "path": "papers/23/10/2310.06372.json",
    "total_tokens": 916,
    "translated_title": "利用基于扩散的图像变化为鲁棒训练提供支持",
    "translated_abstract": "后门攻击对训练神经网络构成严重安全威胁，它们在模型中秘密引入隐藏功能。这些后门在对干净输入进行推理时保持沉默，由于隐蔽的行为而避免被检测。然而，一旦输入数据中出现特定的触发模式，后门就会激活，导致模型执行其隐藏的功能。通过手动检查，在庞大的数据集中检测到这种被污染的样本几乎是不可能的。为了解决这个挑战，我们提出了一种新颖的方法，通过利用最近的扩散模型的强大功能，实现对可能被污染的数据集进行模型训练。具体而言，我们通过在所有训练样本上创建合成变化，利用扩散模型对数据中的潜在触发模式具有固有的弹性。通过将这种生成方法与知识蒸馏相结合，我们生成了保持任务上的总体性能的学生模型。",
    "tldr": "该文提出了一种利用扩散模型的新方法，通过合成变化并结合知识蒸馏，使模型能够在可能被污染的数据集上进行鲁棒训练，解决后门攻击带来的安全威胁。"
}