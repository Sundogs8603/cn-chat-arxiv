{
    "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])",
    "abstract": "Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\\,\\to\\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\\,\\to\\,$43",
    "link": "http://arxiv.org/abs/2310.16955",
    "context": "Title: Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])\nAbstract: Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\\,\\to\\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\\,\\to\\,$43",
    "path": "papers/23/10/2310.16955.json",
    "total_tokens": 943,
    "translated_title": "打破、模仿、修复：通过生成人类攻击提高鲁棒性",
    "translated_abstract": "现实世界中的自然语言处理系统需要对抗人类对手具有鲁棒性。收集人类对手的示例进行训练是一种有效但昂贵的解决方案。另一方面，训练针对小扰动（如词替换）的合成攻击实际上并不能提高对抗人类对手的鲁棒性。本文提出了一个对抗训练框架，使用有限的人类对手示例来生成更有用的大规模对抗示例。我们通过ANLI和仇恨言论检测基准数据集进行实验，这两个数据集是通过迭代的对抗人类和模型的过程收集得到的。与仅在观察到的人类攻击上进行训练相比，也在我们的合成对抗示例上进行训练可以提高模型对未来回合的鲁棒性。在ANLI上，我们看到了对当前攻击集的准确率提升（44.1% -> 50.1%），以及对两个未见过的人类生成攻击回合的准确率提升（32.5% -> 43%）。",
    "tldr": "本研究提出了一个对抗训练框架，使用有限的人类对手示例生成更有用的大规模对抗示例，有效提高了自然语言处理系统对于人类对手的鲁棒性。"
}