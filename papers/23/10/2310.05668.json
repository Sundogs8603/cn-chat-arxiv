{
    "title": "LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)",
    "abstract": "Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late",
    "link": "http://arxiv.org/abs/2310.05668",
    "context": "Title: LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)\nAbstract: Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late",
    "path": "papers/23/10/2310.05668.json",
    "total_tokens": 962,
    "translated_title": "LARA：一种轻量级且抗过拟合的无监督异常检测再训练方法",
    "translated_abstract": "当前大部分异常检测模型都假设正常模式始终保持不变。然而，Web服务的正常模式经常发生剧烈变化。在这种变化之后，使用旧分布数据训练的模型已经过时。每次都重新训练整个模型是昂贵的。此外，在正常模式变化开始时，新分布的观察数据不足。用有限的数据对大型神经网络模型进行重新训练容易过拟合。因此，我们提出了一种轻量级且抗过拟合的再训练方法（LARA），用于基于深度变分自编码器的时间序列异常检测方法（VAEs）。本工作旨在提出三个新颖的贡献：1）将重新训练过程形式化为一个凸问题，并能够以快速收敛以及防止过拟合；2）设计了一个反思模块，可以利用历史数据而无需储存它们；3）数学证明了在微调后可以获得更好的性能。",
    "tldr": "LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。",
    "en_tdlr": "LARA is a light and anti-overfitting retraining approach for unsupervised anomaly detection. It formulates the retraining process as a convex problem, designs a ruminate block to leverage historical data, and mathematically proves improved performance after fine-tuning."
}