{
    "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])",
    "abstract": "It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or \"lazy\") regime and the feature learning (\"rich\") regime identified in prior work. Moreover, PT+FT can exhibit a novel \"nested feature learning\" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned",
    "link": "http://arxiv.org/abs/2310.02396",
    "context": "Title: Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])\nAbstract: It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or \"lazy\") regime and the feature learning (\"rich\") regime identified in prior work. Moreover, PT+FT can exhibit a novel \"nested feature learning\" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned",
    "path": "papers/23/10/2310.02396.json",
    "total_tokens": 1089,
    "translated_title": "用过参数化的神经网络中的隐式正则化方法进行多任务学习和微调",
    "translated_abstract": "在深度学习中，常常使用训练辅助任务的方法来期望学习可以部分地转移到其他感兴趣的任务上。本研究探讨了学习辅助任务所产生的归纳偏置，包括同时学习（多任务学习，MTL）和依序学习（预训练和随后微调，PT+FT）。在使用梯度下降法训练两层对角线线性网络的简化环境中，我们发现了与MTL和PT+FT相关的隐式正则化惩罚，两者都鼓励任务之间的特征共享和学习任务特定特征的稀疏性。值得注意的是，我们的结果表明，在微调过程中，网络在先前研究中确定的内核（或“惰性”）状态和特征学习（“丰富”）状态之间具有混合状态。此外，PT+FT还可以展现一种新颖的“嵌套特征学习”行为，该行为无法被任何状态所捕捉，使其偏向于提取一组稀疏的特征子集。",
    "tldr": "本文研究了在过参数化神经网络中，多任务学习和微调所带来的隐式正则化效果。在简化的线性网络环境中，我们发现了多任务学习和微调所对特征共享和学习特定特征稀疏性的鼓励作用，并发现微调过程同时具有内核和特征学习的混合状态。此外，微调还可以展现一种嵌套特征学习行为，使其偏向于提取一组稀疏的特征子集。",
    "en_tdlr": "This paper investigates the implicit regularization effects of multi-task learning and fine-tuning in overparameterized neural networks. In a simplified linear network environment, it is discovered that both multi-task learning and fine-tuning encourage feature sharing and sparsity in task-specific features, and fine-tuning operates in a hybrid state of kernel and feature learning. Additionally, fine-tuning exhibits a novel behavior of nested feature learning, resulting in the extraction of a sparse subset of features."
}