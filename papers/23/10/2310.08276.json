{
    "title": "Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])",
    "abstract": "Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su",
    "link": "http://arxiv.org/abs/2310.08276",
    "context": "Title: Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])\nAbstract: Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su",
    "path": "papers/23/10/2310.08276.json",
    "total_tokens": 953,
    "translated_title": "面向方向的视觉-语义嵌入模型在遥感图像-文本检索中的应用",
    "translated_abstract": "图像-文本检索在近年来得到了快速发展，然而在遥感领域仍然存在着视觉-语义不平衡的挑战，这导致了非语义视觉和文本特征的错误匹配。为了解决这个问题，我们提出了一种新颖的面向方向的视觉-语义嵌入模型（DOVE），来挖掘视觉和语言之间的关系。具体而言，通过区域导向的注意力模块（ROAM），在潜在的语义空间中，根据区域视觉特征自适应地调整最终的视觉和文本嵌入之间的距离。同时，设计了一个轻量级的文字基因辅助模块（DTGA），用较少的注意力操作来扩展可处理的文本表示范围，增强全局词级语义连接。最后，我们利用全局视觉-语义约束来减少单一视觉依赖，并为最终的视觉和文本表示提供外部约束。",
    "tldr": "这篇论文提出了一种面向方向的视觉-语义嵌入模型（DOVE），通过区域导向的注意力模块和轻量级的文字基因辅助模块，解决了遥感图像-文本检索中的视觉-语义不平衡问题，提高了检索准确性。",
    "en_tdlr": "This paper proposes a direction-oriented visual-semantic embedding model (DOVE) that addresses the visual-semantic imbalance in remote sensing image-text retrieval. By using a regional-oriented attention module and a lightweight digging text genome assistant, the model improves retrieval accuracy by mining the relationship between vision and language."
}