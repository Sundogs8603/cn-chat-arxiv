{
    "title": "Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])",
    "abstract": "We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.",
    "link": "http://arxiv.org/abs/2310.03018",
    "context": "Title: Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])\nAbstract: We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.",
    "path": "papers/23/10/2310.03018.json",
    "total_tokens": 912,
    "translated_title": "使用语音语句对的零资源切换语音基准进行多语言评估",
    "translated_abstract": "我们引入了一个新的零资源切换语音基准，旨在直接评估自监督语音编码器的切换语言能力。我们展示了一种基于离散单元语言建模的基线系统，以展示如何以零资源的方式评估语音编码器的切换语言能力。我们的实验涵盖了各种知名的语音编码器，包括Wav2vec 2.0、HuBERT、XLSR等。我们研究了预训练语言和模型大小对基准性能的影响。值得注意的是，尽管我们的结果表明，在切换语言场景中，具有多语言预训练的语音编码器（例如XLSR）优于单语变体（Wav2vec 2.0、HuBERT），但它们的切换语言能力仍有很大的改进空间。",
    "tldr": "本论文介绍了一个新的零资源切换语音基准，用于评估自监督语音编码器的切换语言能力，研究了预训练语言和模型大小对基准性能的影响，结果显示多语言预训练的语音编码器在切换语言场景中表现优于单语变体，但仍有改进空间。",
    "en_tdlr": "This paper introduces a new zero-resource code-switched speech benchmark to assess the code-switching capabilities of self-supervised speech encoders. It investigates the impact of pre-training languages and model size on benchmark performance, revealing that speech encoders with multilingual pre-training outperform monolingual variants in code-switching scenarios, but there is still room for improvement."
}