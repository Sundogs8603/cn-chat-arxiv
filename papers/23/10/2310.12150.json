{
    "title": "Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])",
    "abstract": "We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for ",
    "link": "http://arxiv.org/abs/2310.12150",
    "context": "Title: Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])\nAbstract: We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for ",
    "path": "papers/23/10/2310.12150.json",
    "total_tokens": 902,
    "translated_title": "理解用于长篇问答的检索增强",
    "translated_abstract": "我们在长篇问答中提出了一项检索增强的语言模型（LMs）研究。我们通过比较使用相同证据文档的模型生成的答案，分析了检索增强对不同LMs的影响，以及检索文档集质量对相同LMs生成的答案的影响。我们研究了生成答案的各种属性（例如，流畅度、长度、变异性），重点在于将生成的长篇答案归因于文本中的证据文档。我们进行了答案归因的人工标注并评估了自动评判归因的方法。我们的研究为检索增强如何影响LMs生成长篇、知识丰富的文本提供了新的见解。我们进一步确定了长文本生成的归因模式并分析了归因错误的主要原因。综上所述，我们的分析揭示了检索增强对长篇、知识丰富的文本生成的影响，并提供了方向。",
    "tldr": "这项研究分析了长篇问答中的检索增强语言模型的影响，研究了生成答案的属性和归因模式，并找出了归因错误的主要原因。研究结果对长篇、知识丰富的文本生成提供了新的见解。",
    "en_tdlr": "This study analyzes the impact of retrieval-augmented language models on long-form question answering, investigates the attributes and attribution patterns of generated answers, and identifies the main causes of attribution errors. The findings provide new insights into generating long, knowledge-rich text."
}