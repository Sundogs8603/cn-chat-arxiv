{
    "title": "Fine tuning Pre trained Models for Robustness Under Noisy Labels. (arXiv:2310.17668v1 [cs.LG])",
    "abstract": "The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find ",
    "link": "http://arxiv.org/abs/2310.17668",
    "context": "Title: Fine tuning Pre trained Models for Robustness Under Noisy Labels. (arXiv:2310.17668v1 [cs.LG])\nAbstract: The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find ",
    "path": "papers/23/10/2310.17668.json",
    "total_tokens": 926,
    "translated_title": "为了在存在噪声标签的情况下提高鲁棒性，对预训练模型进行微调",
    "translated_abstract": "训练数据集中存在噪声标签会显著影响机器学习模型的性能。为了解决这个问题，研究人员已经探索了学习噪声标签的方法，以识别干净样本并减少噪声标签的影响。然而，限制训练数据集的某一部分的影响可能会导致整体泛化性能的降低。为了缓解这个问题，最近的研究考虑了通过利用巨大的计算资源来谨慎利用噪声标签。因此，不断增加的训练成本需要重新评估效率。在其他研究领域，人们正在专注于开发针对大型预训练模型的微调技术，旨在实现高泛化性能和效率之间的平衡。然而，这些方法主要集中在干净的数据集上，对于噪声标签情景的探索有限。在这项研究中，我们的目标是找到一种适用于噪声标签场景的微调方法。",
    "tldr": "该论文研究了在存在噪声标签的情况下，为了提高鲁棒性，对预训练模型进行微调。目前的研究主要集中在干净数据集上，对于噪声标签情景的探索有限。",
    "en_tdlr": "This paper investigates fine-tuning pre-trained models for robustness in the presence of noisy labels. The current research mainly focuses on clean datasets, with limited exploration of the noisy label scenario."
}