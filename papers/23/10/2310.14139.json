{
    "title": "Are LSTMs Good Few-Shot Learners?. (arXiv:2310.14139v1 [cs.LG])",
    "abstract": "Deep learning requires large amounts of data to learn new tasks well, limiting its applicability to domains where such data is available. Meta-learning overcomes this limitation by learning how to learn. In 2001, Hochreiter et al. showed that an LSTM trained with backpropagation across different tasks is capable of meta-learning. Despite promising results of this approach on small problems, and more recently, also on reinforcement learning problems, the approach has received little attention in the supervised few-shot learning setting. We revisit this approach and test it on modern few-shot learning benchmarks. We find that LSTM, surprisingly, outperform the popular meta-learning technique MAML on a simple few-shot sine wave regression benchmark, but that LSTM, expectedly, fall short on more complex few-shot image classification benchmarks. We identify two potential causes and propose a new method called Outer Product LSTM (OP-LSTM) that resolves these issues and displays substantial p",
    "link": "http://arxiv.org/abs/2310.14139",
    "context": "Title: Are LSTMs Good Few-Shot Learners?. (arXiv:2310.14139v1 [cs.LG])\nAbstract: Deep learning requires large amounts of data to learn new tasks well, limiting its applicability to domains where such data is available. Meta-learning overcomes this limitation by learning how to learn. In 2001, Hochreiter et al. showed that an LSTM trained with backpropagation across different tasks is capable of meta-learning. Despite promising results of this approach on small problems, and more recently, also on reinforcement learning problems, the approach has received little attention in the supervised few-shot learning setting. We revisit this approach and test it on modern few-shot learning benchmarks. We find that LSTM, surprisingly, outperform the popular meta-learning technique MAML on a simple few-shot sine wave regression benchmark, but that LSTM, expectedly, fall short on more complex few-shot image classification benchmarks. We identify two potential causes and propose a new method called Outer Product LSTM (OP-LSTM) that resolves these issues and displays substantial p",
    "path": "papers/23/10/2310.14139.json",
    "total_tokens": 950,
    "translated_title": "LSTM对于少样本学习是否有效？",
    "translated_abstract": "深度学习需要大量数据来很好地学习新任务，这限制了其在数据较少的领域的应用。元学习通过学习如何学习来克服这一限制。2001年，Hochreiter等人展示了一个经过不同任务上的反向传播训练的LSTM能够进行元学习。尽管该方法在小问题上取得了有希望的结果，并且最近在强化学习问题上也取得了成功，但在有监督的少样本学习设置中，该方法却受到了较少的关注。我们重新考察了这个方法，并在现代少样本学习基准上进行了测试。我们发现，令人惊讶的是，LSTM在一个简单的少样本正弦波回归基准上表现优于流行的元学习技术MAML，但在更复杂的少样本图像分类基准上不及预期。我们确定了两个潜在原因，并提出了一种名为Outer Product LSTM (OP-LSTM)的新方法来解决这些问题，并显示出显著的性能改进。",
    "tldr": "本研究重新考察了使用LSTM进行少样本学习的方法，在简单的回归问题上表现优于流行的元学习技术MAML，但在复杂的图像分类问题上不及预期。研究提出了一种名为OP-LSTM的新方法来解决这些问题，并取得了显著的性能改进。",
    "en_tdlr": "This study reexamines the use of LSTM for few-shot learning and finds that it outperforms the popular meta-learning technique MAML on a simple regression problem but falls short on complex image classification problems. The study proposes a new method called OP-LSTM to address these issues and achieves significant performance improvements."
}