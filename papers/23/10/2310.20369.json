{
    "title": "Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm. (arXiv:2310.20369v1 [cs.LG])",
    "abstract": "The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such",
    "link": "http://arxiv.org/abs/2310.20369",
    "context": "Title: Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm. (arXiv:2310.20369v1 [cs.LG])\nAbstract: The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such",
    "path": "papers/23/10/2310.20369.json",
    "total_tokens": 925,
    "translated_title": "分布式随机梯度上升算法的稳定性和泛化性能研究",
    "translated_abstract": "可用数据的规模不断增长，对于解决分布式机器学习任务中的极小极大问题，分布式方式引起了越来越多的关注。以前的理论研究主要集中在分布式极小极大算法的收敛速度和通信复杂度，对其泛化性能关注较少。本文通过算法稳定性的方法，研究了分布式随机梯度上升算法（D-SGDA）在凸凹和非凸非凹情况下的原始-对偶泛化界。我们的理论对分布式稳定性进行了精细化探究，并表明分布式结构并不会破坏D-SGDA的稳定性和泛化性能，在某些情况下其泛化性能可以媲美原始的SGDA。我们的结果分析了不同拓扑结构对D-SGDA算法的泛化界的影响，超越了一些微不足道的因素。",
    "tldr": "本文研究了分布式随机梯度上升算法（D-SGDA）在各种情况下的稳定性和泛化性能，并证明分布式结构不会破坏D-SGDA的稳定性和泛化性能，在某些情况下其泛化性能可以媲美原始的SGDA。",
    "en_tdlr": "This paper investigates the stability and generalization performance of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm and demonstrates that the decentralized structure does not compromise its stability and generalization, making it comparable to the original SGDA in certain scenarios."
}