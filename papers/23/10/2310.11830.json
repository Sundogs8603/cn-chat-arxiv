{
    "title": "CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])",
    "abstract": "This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data",
    "link": "http://arxiv.org/abs/2310.11830",
    "context": "Title: CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])\nAbstract: This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data",
    "path": "papers/23/10/2310.11830.json",
    "total_tokens": 832,
    "translated_title": "CLARA: 多语言对比学习用于音频表示获取的论文",
    "translated_abstract": "本文提出了一个新的多语言语音和音频表示学习框架，使用对比学习。标注数据不足制约了跨语言语音处理研究的发展。最近对比学习的进展提供了自监督技术来从无标签数据中学习。为了减少数据依赖性和改善在不同语言和条件下的泛化能力，我们开发了一个多语言对比学习框架。该框架使模型能够在多语言中获得共享表示，有助于使用有限的目标语言数据进行跨语言迁移。此外，由于主观感知评估的挑战，捕捉语音中的情感线索是困难的。通过自监督的方式从多样的多语言数据中学习表达性表示，我们的方法旨在开发编码情感维度的语音表示。我们的方法在大规模的多语言音频数据集上训练编码器。",
    "tldr": "本文提出了一个多语言对比学习框架，通过自监督学习从无标签数据中获取音频表示，实现了跨语言迁移和情感维度的编码。",
    "en_tdlr": "This paper introduces a multilingual contrastive learning framework that acquires audio representations through self-supervised learning from unlabeled data, enabling cross-lingual transfer and encoding of emotional dimensions."
}