{
    "title": "Equivariant Matrix Function Neural Networks. (arXiv:2310.10434v1 [stat.ML])",
    "abstract": "Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standa",
    "link": "http://arxiv.org/abs/2310.10434",
    "context": "Title: Equivariant Matrix Function Neural Networks. (arXiv:2310.10434v1 [stat.ML])\nAbstract: Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standa",
    "path": "papers/23/10/2310.10434.json",
    "total_tokens": 863,
    "translated_title": "等变矩阵函数神经网络",
    "translated_abstract": "图神经网络（GNNs），尤其是消息传递神经网络（MPNNs），已经成为在各种应用中学习图形的强大架构。然而，当建模非局部相互作用时，MPNNs在大共轭分子，金属或非晶态材料等系统中面临挑战。尽管谱GNN和传统的神经网络（例如循环神经网络和Transformer）可以缓解这些挑战，但它们常常缺乏扩展性，适应性，泛化能力，计算效率，或者不能捕捉数据中的详细结构关系或对称性。为了解决这些问题，我们引入了矩阵函数神经网络（MFNs），一种通过解析矩阵等变函数来参数化非局部相互作用的新型架构。采用解析矩阵展开提供了一种直接的实现方法，并具有随系统大小线性扩展的潜力。该MFN架构在标准任务中实现了最先进的性能。",
    "tldr": "矩阵函数神经网络（MFNs）是一种通过解析矩阵等变函数来参数化非局部相互作用的新型架构，能够在各种应用中实现最先进的性能。"
}