{
    "title": "Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])",
    "abstract": "Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderatel",
    "link": "http://arxiv.org/abs/2310.11628",
    "context": "Title: Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])\nAbstract: Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderatel",
    "path": "papers/23/10/2310.11628.json",
    "total_tokens": 838,
    "translated_title": "学习您的标记：用于语言模型的单词池化标记化",
    "translated_abstract": "语言模型通常将文本标记化为子词，使用确定性的、手工设计的启发式方法将字符组合成更长的表层字符串（如 'ing'）或整个单词。最近的文献反复展示了这种标记化策略的局限性，特别是对于非英文的文档和表示数字。另一方面，字节/字符级语言模型受限制较少，但在自我注意计算中存在序列描述长度增加和后续二次扩展的问题。最近对这些上下文长度进行固定大小卷积压缩和限制的尝试是有益的，但完全忽略了单词边界。本文考虑了一种替代的“学习您的标记”方案，利用单词边界将字节/字符汇聚成单词表示形式，然后将其馈送到主要语言模型中，再对每个单词并行解码个别的字符/字节。我们发现我们的",
    "tldr": "使用\"学习您的标记\"方案，利用单词边界将字节/字符汇聚成单词表示形式，以改善标记化策略的局限性并提高语言模型的性能。"
}