{
    "title": "FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])",
    "abstract": "The emergence of foundation models, including language and vision models, has reshaped AI's landscape, offering capabilities across various applications. Deploying and fine-tuning these large models, like GPT-3 and BERT, presents challenges, especially in the current foundation model era. We introduce Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning (PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses adapters, emulators, and PEFT for federated model tuning, enhancing model privacy and memory efficiency. Adapters adjust pre-trained models, while emulators give a compact representation of original models, addressing both privacy and efficiency. Adaptable to various neural networks, our approach also uses deep reinforcement learning for hyper-parameter optimization. We tested FedPEAT in a unique scenario with a server participating in collaborative federate",
    "link": "http://arxiv.org/abs/2310.17491",
    "context": "Title: FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])\nAbstract: The emergence of foundation models, including language and vision models, has reshaped AI's landscape, offering capabilities across various applications. Deploying and fine-tuning these large models, like GPT-3 and BERT, presents challenges, especially in the current foundation model era. We introduce Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning (PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses adapters, emulators, and PEFT for federated model tuning, enhancing model privacy and memory efficiency. Adapters adjust pre-trained models, while emulators give a compact representation of original models, addressing both privacy and efficiency. Adaptable to various neural networks, our approach also uses deep reinforcement learning for hyper-parameter optimization. We tested FedPEAT in a unique scenario with a server participating in collaborative federate",
    "path": "papers/23/10/2310.17491.json",
    "total_tokens": 928,
    "translated_title": "FedPEAT: 联邦学习、参数高效微调与辅助调优在基础人工智能模型与移动边缘计算中的融合",
    "translated_abstract": "基础模型的出现，包括语言和视觉模型，改变了人工智能的领域，为各种应用提供了能力。部署和微调这些大型模型，如GPT-3和BERT，在当前的基础模型时代面临挑战。我们介绍了辅助调优（EAT）与参数高效微调（PEFT）相结合，形成了参数高效辅助调优（PEAT）。此外，我们将其扩展到联邦学习作为联邦PEAT（FedPEAT）。FedPEAT使用适配器、仿真器和PEFT进行联邦模型调优，提高模型隐私和内存效率。适配器调整预训练模型，而仿真器给出原始模型的紧凑表示，同时解决隐私和效率问题。我们的方法适用于各种神经网络，还利用深度强化学习进行超参数优化。我们在一个独特的场景中使用FedPEAT进行了测试，其中服务器参与协作联邦学习。",
    "tldr": "FedPEAT是将辅助调优和参数高效微调应用于联邦学习的方法，能够提升基础人工智能模型的模型隐私和内存效率。",
    "en_tdlr": "FedPEAT is a method that combines Emulator-Assisted Tuning and Parameter-Efficient Fine-Tuning for federated learning, improving model privacy and memory efficiency of artificial intelligence foundation models."
}