{
    "title": "Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])",
    "abstract": "Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising",
    "link": "http://arxiv.org/abs/2310.17550",
    "context": "Title: Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])\nAbstract: Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising",
    "path": "papers/23/10/2310.17550.json",
    "total_tokens": 901,
    "translated_title": "人类引导的复杂度控制抽象化",
    "translated_abstract": "神经网络通常学习任务特定的潜在表示，但这些表示无法推广到新的环境或任务。相反，人类在各种抽象级别（例如，“鸟”与“麻雀”）上学习离散表示（即概念或单词），并根据任务使用适当的抽象。受此启发，我们训练神经模型生成一系列离散表示，并通过调整表示分布的熵来控制表示的复杂性（大致上是为编码输入分配了多少位）。在微调实验中，仅使用少量带标签的示例用于新任务，我们展示了（1）调整表示以适当的复杂性水平支持最高的微调性能，以及（2）在一个人类参与者的研究中，用户能够根据离散表示的可视化来确定下游任务的适当复杂性水平。我们的结果表明一个有希望的方向。",
    "tldr": "本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。",
    "en_tdlr": "This study trains neural models to generate a range of discrete representations and controls the complexity of the representations to improve task generalization. Experiment results show that tuning the complexity level appropriately supports the best performance in fine-tuning, and this finding is validated in a human participant study as well."
}