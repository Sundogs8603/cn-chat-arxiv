{
    "title": "ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])",
    "abstract": "Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.",
    "link": "http://arxiv.org/abs/2310.17723",
    "context": "Title: ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])\nAbstract: Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.",
    "path": "papers/23/10/2310.17723.json",
    "total_tokens": 860,
    "translated_title": "ZeroQuant-HERO: W8A8 Transformer的硬件增强的优化后训练量化框架",
    "translated_abstract": "量化技术在减少深度神经网络推理的内存和计算需求方面起着关键作用。现有的解决方案，如ZeroQuant，为BERT和GPT等模型提供了动态量化，但忽视了关键的内存受限运算符和每个标记的量化复杂性。针对这些差距，我们提出了一种全新的、完全由硬件增强的、经过优化的、后训练W8A8量化框架ZeroQuant-HERO。该框架独特地集成了内存带宽和计算密集型运算符，旨在实现最佳硬件性能。此外，它通过允许特定的INT8模块切换到FP16/BF16模式来提高准确性。",
    "tldr": "ZeroQuant-HERO是一种硬件增强的量化框架，针对W8A8 Transformer模型进行优化，旨在减少内存和计算需求，并在处理复杂的量化问题和内存受限运算符方面提供了新的解决方案。同时，该框架还具有灵活性，允许特定模块切换至FP16/BF16模式以提高准确性。",
    "en_tdlr": "ZeroQuant-HERO is a hardware-enhanced quantization framework optimized for W8A8 Transformers, aiming to reduce memory and computational demands while providing a novel solution for handling complex quantization issues and memory-bounded operators. Additionally, it offers flexibility by allowing specific modules to switch to FP16/BF16 mode, enhancing accuracy."
}