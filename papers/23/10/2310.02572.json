{
    "title": "Improving Knowledge Distillation with Teacher's Explanation. (arXiv:2310.02572v1 [cs.LG])",
    "abstract": "Knowledge distillation (KD) improves the performance of a low-complexity student model with the help of a more powerful teacher. The teacher in KD is a black-box model, imparting knowledge to the student only through its predictions. This limits the amount of transferred knowledge. In this work, we introduce a novel Knowledge Explaining Distillation (KED) framework, which allows the student to learn not only from the teacher's predictions but also from the teacher's explanations. We propose a class of superfeature-explaining teachers that provide explanation over groups of features, along with the corresponding student model. We also present a method for constructing the superfeatures. We then extend KED to reduce complexity in convolutional neural networks, to allow augmentation with hidden-representation distillation methods, and to work with a limited amount of training data using chimeric sets. Our experiments over a variety of datasets show that KED students can substantially outp",
    "link": "http://arxiv.org/abs/2310.02572",
    "context": "Title: Improving Knowledge Distillation with Teacher's Explanation. (arXiv:2310.02572v1 [cs.LG])\nAbstract: Knowledge distillation (KD) improves the performance of a low-complexity student model with the help of a more powerful teacher. The teacher in KD is a black-box model, imparting knowledge to the student only through its predictions. This limits the amount of transferred knowledge. In this work, we introduce a novel Knowledge Explaining Distillation (KED) framework, which allows the student to learn not only from the teacher's predictions but also from the teacher's explanations. We propose a class of superfeature-explaining teachers that provide explanation over groups of features, along with the corresponding student model. We also present a method for constructing the superfeatures. We then extend KED to reduce complexity in convolutional neural networks, to allow augmentation with hidden-representation distillation methods, and to work with a limited amount of training data using chimeric sets. Our experiments over a variety of datasets show that KED students can substantially outp",
    "path": "papers/23/10/2310.02572.json",
    "total_tokens": 924,
    "translated_title": "使用教师解释改进知识蒸馏",
    "translated_abstract": "知识蒸馏通过一个强大的教师来提高低复杂度的学生模型的性能。在知识蒸馏中，教师是一个黑盒模型，只通过其预测来传授知识给学生。这限制了传输知识的数量。在这项工作中，我们引入了一种新颖的知识解释蒸馏（KED）框架，它允许学生不仅从教师的预测中学习，还可以从教师的解释中学习。我们提出了一类能够解释特征组的超特征教师，以及相应的学生模型。我们还提出了一种构建超特征的方法。然后，我们扩展了KED，以减少卷积神经网络的复杂性，以允许与隐藏表示蒸馏方法的增强，以及使用嵌合集合处理有限的训练数据。我们在多个数据集上的实验证明，KED学生的性能可以显著超越传统的知识蒸馏方法。",
    "tldr": "本论文提出了一种新颖的知识解释蒸馏（KED）框架，允许学生从教师的解释中学习，并扩展了KED的应用范围，以提高卷积神经网络的性能和处理有限训练数据的能力。",
    "en_tdlr": "This paper proposes a novel Knowledge Explaining Distillation (KED) framework, which allows students to learn from teacher's explanations and extends KED for improved performance of convolutional neural networks and handling limited training data."
}