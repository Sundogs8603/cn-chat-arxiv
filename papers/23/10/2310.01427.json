{
    "title": "Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])",
    "abstract": "Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.",
    "link": "http://arxiv.org/abs/2310.01427",
    "context": "Title: Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])\nAbstract: Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.",
    "path": "papers/23/10/2310.01427.json",
    "total_tokens": 773,
    "translated_title": "注意力排序在长文本语言模型中应对新近偏见",
    "translated_abstract": "当前的语言模型在生成过程中往往未能高效地整合长上下文。我们表明，这个问题的一个主要原因是在预训练期间可能学到的注意力先验：上下文中较早出现的相关信息平均来说被关注的较少。然而，即使模型未能在回应中使用来自相关文档的信息，它们仍然相对于同一位置上的无关文档给予偏爱的注意力。基于这一事实，我们利用\"注意力排序\"：在解码过程中执行一步，按照他们接收到的注意力进行排序（最高的注意力排在最后），重复该过程，使用新排序的上下文生成答案。我们发现，注意力排序提高了长上下文模型的性能。我们的研究结果突显了使用现成的语言模型进行检索增强生成的一些挑战。",
    "tldr": "注意力排序可以改善长文本语言模型的性能，通过对注意力进行排序并重复生成，解决了当前模型在整合长上下文时的问题。",
    "en_tdlr": "Attention sorting improves the performance of long context language models by sorting the attention and generating repeatedly, addressing the issue of incorporating long contexts efficiently."
}