{
    "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack",
    "link": "http://arxiv.org/abs/2310.12955",
    "context": "Title: Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack",
    "path": "papers/23/10/2310.12955.json",
    "total_tokens": 916,
    "translated_title": "构建具有多样数据损坏情况下鲁棒性的离线强化学习",
    "translated_abstract": "离线强化学习（RL）是一种有前途的方法，可以从离线数据集中学习强化策略，而无需与环境进行昂贵或不安全的交互。然而，人们在真实环境中收集的数据集往往存在噪声，甚至可能被恶意损坏，这可能会严重影响离线强化学习的性能。本研究首先对当前离线强化学习算法在包括状态、动作、奖励和动力学在内的全面数据损坏情况下的性能进行了调查。我们的大量实验显示，隐式Q-learning（IQL）在各种离线强化学习算法中表现出了可靠的抗数据损坏能力。此外，我们还进行了经验和理论分析，以了解IQL的鲁棒性能，并将其监督策略学习方案确定为关键因素。尽管相对鲁棒，但IQL在动力学损坏下仍然存在Q函数的重尾目标问题。",
    "tldr": "本文研究了在多种数据损坏情况下，离线强化学习算法的性能。研究发现，隐式Q-learning（IQL）在各种离线强化学习算法中展现出了较强的鲁棒性能，其采用的监督策略学习方案为关键。然而，在动力学损坏下，IQL仍然存在Q函数的重尾目标问题。"
}