{
    "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
    "abstract": "arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th",
    "link": "https://arxiv.org/abs/2310.05861",
    "context": "Title: Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models\nAbstract: arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th",
    "path": "papers/23/10/2310.05861.json",
    "total_tokens": 849,
    "translated_title": "重新表述、增强、推理：视觉问题的视觉基础与语言模型",
    "translated_abstract": "越来越多的视觉-语言任务可以在零至少训练的情况下处理，即通过将大型语言模型（LLMs）与视觉编码器相结合，形成大型视觉-语言模型（LVLMs）。尽管这具有巨大的优势，比如不需要训练数据或自定义架构，但如何将输入呈现给LVLM会对零参考模型的性能产生重大影响。特别是，以不充分方式表达的输入可能会导致错误答案，原因包括缺失视觉信息、复杂的隐含推理或语言歧义。因此，通过在输入中添加具有视觉基础的信息作为预防性澄清，应该能够通过减少不充分性提高模型性能，例如通过定位对象和消除引用歧义。类似地，在VQA设置中，改变问题的构思方式可以使模型更容易回答。",
    "tldr": "通过在输入中添加具有视觉基础的信息作为预防性澄清，可以提高模型性能，减少不充分性，并简化模型回答问题的方式。",
    "en_tdlr": "Adding visually-grounded information to the input as a preemptive clarification can improve model performance by reducing underspecification and simplifying the way models answer questions."
}