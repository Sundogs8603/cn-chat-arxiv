{
    "title": "On the Dimensionality of Sentence Embeddings. (arXiv:2310.15285v1 [cs.CL])",
    "abstract": "Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the dimensionality of sentence embeddings. First, we demonstrate that the optimal dimension of sentence embeddings is usually smaller than the default value. Subsequently, to compress the dimension of sentence embeddings with minimum performance degradation, we identify two components contributing to the overall performance loss: the encoder's performance loss and the pooler's performance loss. Therefore, we propose a two-step training method for sentence representation learning models, wherein the encoder and the pooler are optimized separately to mitigate the overall performance loss in low-dimension scenarios. Experimental results on seven STS tasks and seven sentence classification tasks dem",
    "link": "http://arxiv.org/abs/2310.15285",
    "context": "Title: On the Dimensionality of Sentence Embeddings. (arXiv:2310.15285v1 [cs.CL])\nAbstract: Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the dimensionality of sentence embeddings. First, we demonstrate that the optimal dimension of sentence embeddings is usually smaller than the default value. Subsequently, to compress the dimension of sentence embeddings with minimum performance degradation, we identify two components contributing to the overall performance loss: the encoder's performance loss and the pooler's performance loss. Therefore, we propose a two-step training method for sentence representation learning models, wherein the encoder and the pooler are optimized separately to mitigate the overall performance loss in low-dimension scenarios. Experimental results on seven STS tasks and seven sentence classification tasks dem",
    "path": "papers/23/10/2310.15285.json",
    "total_tokens": 835,
    "translated_title": "论句子嵌入的维度问题",
    "translated_abstract": "学习句子嵌入是自然语言处理中一个基本问题。现有的研究主要集中在提高句子嵌入的质量上，而对句子嵌入的维度探索有限。本文对句子嵌入的维度进行了全面而实证的分析。首先，我们证明了句子嵌入的最佳维度通常比默认值要小。接着，为了在维度压缩时最小化性能损失，我们识别了两个影响整体性能损失的组成部分：编码器的性能损失和池化器的性能损失。因此，我们提出了一种两步训练方法来进行句子表示学习模型的训练，其中编码器和池化器分别进行优化以减轻低维度情况下的整体性能损失。在七个STS任务和七个句子分类任务上的实验结果表明...",
    "tldr": "本文对句子嵌入的维度进行了全面而实证的分析，证明了最佳维度通常比默认值要小，并提出了一种两步训练方法来在维度压缩时最小化性能损失。",
    "en_tdlr": "This paper presents a comprehensive and empirical analysis of the dimensionality of sentence embeddings, demonstrating that the optimal dimension is usually smaller than the default value. It also proposes a two-step training method to minimize performance loss when compressing the dimension."
}