{
    "title": "Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])",
    "abstract": "Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l",
    "link": "http://arxiv.org/abs/2310.08091",
    "context": "Title: Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])\nAbstract: Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l",
    "path": "papers/23/10/2310.08091.json",
    "total_tokens": 950,
    "translated_title": "分辨时间差分学习",
    "translated_abstract": "时间差分学习(TD)是强化学习中的基本概念，旨在高效评估策略的价值函数。TD($\\lambda$)是一种有效的变体，通过引入记忆轨迹将预测误差分散到历史上下文中。然而，这种方法经常忽视历史状态的重要性以及传播TD误差的相对重要性，这受到访问失衡或结果噪声等挑战的影响。为了解决这个问题，我们提出了一种名为分辨TD学习(DTD)的新型TD算法，它允许灵活的强调函数-在训练过程中预先确定或自适应地分配资源以提高状态之间的效果。我们在特定类别的强调函数内建立了我们方法的收敛性质，并展示了它在深度RL环境中的潜在应用。实证结果表明，使用合理的强调函数不仅可以改进值估计，还可以加速学习。",
    "tldr": "该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。",
    "en_tdlr": "This paper proposes a novel TD algorithm called discerning TD learning (DTD) that addresses the neglect of the significance of historical states and the relative importance of propagating TD errors in traditional TD learning. By using flexible emphasis functions to allocate resources, DTD improves the allocation of importance between states. Empirical results demonstrate that employing a judicious emphasis function not only improves value estimation but also expedites learning."
}