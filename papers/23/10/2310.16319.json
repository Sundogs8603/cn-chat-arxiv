{
    "title": "DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment. (arXiv:2310.16319v1 [cs.CL])",
    "abstract": "Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the assessment criteria based on the dimensions conforming to human judgements on dialogue qualities, and (2) annotate large-scale dialogues that conversed between real users based on these annotation criteria, which contains around 100,000 dialogues. We conduct several experiments and report the performances of the baselines as the benchmark on DiQAD. The dataset is openly accessible at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.",
    "link": "http://arxiv.org/abs/2310.16319",
    "context": "Title: DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment. (arXiv:2310.16319v1 [cs.CL])\nAbstract: Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the assessment criteria based on the dimensions conforming to human judgements on dialogue qualities, and (2) annotate large-scale dialogues that conversed between real users based on these annotation criteria, which contains around 100,000 dialogues. We conduct several experiments and report the performances of the baselines as the benchmark on DiQAD. The dataset is openly accessible at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.",
    "path": "papers/23/10/2310.16319.json",
    "total_tokens": 941,
    "translated_title": "DiQAD：一种用于端到端开放域对话评估的基准数据集",
    "translated_abstract": "对话评估在开放域对话系统的开发中扮演着关键的角色。现有的工作无法提供端到端和人类认知评估数据集，它们只提供一些子指标，如连贯性或对话在远离实际用户环境的注释者之间进行。在本文中，我们发布了一个大规模的对话质量评估数据集（DiQAD），用于自动评估开放域对话质量。具体来说，我们（1）根据符合人类对话质量判断的维度建立评估标准，并（2）根据这些标准对实际用户之间对话进行大规模注释，其中包含约100,000个对话。我们进行了几个实验，并报告了DiQAD上基准的基线性能。该数据集可在https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation上公开获取。",
    "tldr": "本文发布了一个大规模的对话质量评估数据集（DiQAD），用于自动评估开放域对话质量。该数据集是端到端和人类认知评估数据集，其中包含约100,000个对话。对话质量的评估标准是根据符合人类对话质量判断的维度建立的。数据集可在https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation上公开获取。",
    "en_tdlr": "This paper presents DiQAD, a large-scale dataset for automatically assessing open-domain dialogue quality. The dataset contains around 100,000 dialogues and is based on end-to-end and human-epistemic assessment criteria. The dataset is openly accessible and can be found at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation."
}