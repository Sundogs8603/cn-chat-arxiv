{
    "title": "Adaptive importance sampling for heavy-tailed distributions via $\\alpha$-divergence minimization. (arXiv:2310.16653v1 [stat.CO])",
    "abstract": "Adaptive importance sampling (AIS) algorithms are widely used to approximate expectations with respect to complicated target probability distributions. When the target has heavy tails, existing AIS algorithms can provide inconsistent estimators or exhibit slow convergence, as they often neglect the target's tail behaviour. To avoid this pitfall, we propose an AIS algorithm that approximates the target by Student-t proposal distributions. We adapt location and scale parameters by matching the escort moments - which are defined even for heavy-tailed distributions - of the target and the proposal. These updates minimize the $\\alpha$-divergence between the target and the proposal, thereby connecting with variational inference. We then show that the $\\alpha$-divergence can be approximated by a generalized notion of effective sample size and leverage this new perspective to adapt the tail parameter with Bayesian optimization. We demonstrate the efficacy of our approach through applications t",
    "link": "http://arxiv.org/abs/2310.16653",
    "context": "Title: Adaptive importance sampling for heavy-tailed distributions via $\\alpha$-divergence minimization. (arXiv:2310.16653v1 [stat.CO])\nAbstract: Adaptive importance sampling (AIS) algorithms are widely used to approximate expectations with respect to complicated target probability distributions. When the target has heavy tails, existing AIS algorithms can provide inconsistent estimators or exhibit slow convergence, as they often neglect the target's tail behaviour. To avoid this pitfall, we propose an AIS algorithm that approximates the target by Student-t proposal distributions. We adapt location and scale parameters by matching the escort moments - which are defined even for heavy-tailed distributions - of the target and the proposal. These updates minimize the $\\alpha$-divergence between the target and the proposal, thereby connecting with variational inference. We then show that the $\\alpha$-divergence can be approximated by a generalized notion of effective sample size and leverage this new perspective to adapt the tail parameter with Bayesian optimization. We demonstrate the efficacy of our approach through applications t",
    "path": "papers/23/10/2310.16653.json",
    "total_tokens": 885,
    "translated_title": "通过$\\alpha$-散度最小化实现重尾分布的自适应重要性采样",
    "translated_abstract": "自适应重要性采样（AIS）算法被广泛用于逼近复杂目标概率分布的期望。当目标分布具有重尾特性时，现有的AIS算法可能提供不一致的估计或收敛缓慢，因为它们常常忽略目标的尾部行为。为了避免这个问题，我们提出了一种通过使用Student-t建议分布来逼近目标的AIS算法。通过匹配目标和建议分布的護航矩（在重尾分布下也可定义），我们适应了位置和尺度参数。这些更新通过最小化目标和建议分布之间的$\\alpha$-散度来与变分推断相联系。然后，我们展示了$\\alpha$-散度可以通过广义的有效样本大小的概念来逼近，并利用这个新的视角来通过贝叶斯优化来适应尾部参数。通过应用实例，我们验证了我们方法的有效性。",
    "tldr": "该论文提出了一种通过匹配目标和建议分布的護航矩，最小化$\\alpha$-散度的自适应重要性采样算法，以便在处理重尾分布时获得更准确和更快速的估计结果。",
    "en_tdlr": "This paper presents an adaptive importance sampling algorithm that approximates heavy-tailed distributions by matching the escort moments and minimizing the $\\alpha$-divergence, leading to more accurate and faster estimation results."
}