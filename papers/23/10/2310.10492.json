{
    "title": "UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking",
    "abstract": "arXiv:2310.10492v2 Announce Type: replace  Abstract: Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, ignoring unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.",
    "link": "https://arxiv.org/abs/2310.10492",
    "context": "Title: UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking\nAbstract: arXiv:2310.10492v2 Announce Type: replace  Abstract: Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, ignoring unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.",
    "path": "papers/23/10/2310.10492.json",
    "total_tokens": 832,
    "translated_title": "UNO-DST: 利用未标注数据在零样本对话状态跟踪中的应用",
    "translated_abstract": "先前的零样本对话状态跟踪（DST）方法仅应用迁移学习，忽略了目标域中的未标注数据。我们通过联合和自训练方法将零样本DST转换为少样本DST，利用这些未标注数据。我们的方法包括生成槽类型的辅助任务，作为主要任务的逆提示，在联合训练过程中创建槽值。这两个任务之间的循环一致性使得在未知目标域中生成和选择高质量样本，用于后续微调。这种方法还有助于自动标签创建，从而优化DST模型的训练和微调。我们在零样本场景中展示了这种方法在通用语言模型上的有效性，提高了在MultiWOZ中所有领域的平均联合目标准确性8%。",
    "tldr": "通过联合和自训练方法，UNO-DST利用未标注数据，将零样本对话状态跟踪转变为少样本DST，并在未知目标域中生成和选择高质量样本，最终提高了DST模型的训练和微调效果。",
    "en_tdlr": "UNO-DST leverages unlabelled data to transform zero-shot dialogue state tracking into few-shot DST, generating and selecting high-quality samples in unknown target domains, ultimately improving the training and fine-tuning of DST models."
}