{
    "title": "Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])",
    "abstract": "Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te",
    "link": "http://arxiv.org/abs/2310.11003",
    "context": "Title: Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])\nAbstract: Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te",
    "path": "papers/23/10/2310.11003.json",
    "total_tokens": 941,
    "translated_title": "为语音识别进行纠错的语言模型训练",
    "translated_abstract": "语言模型（LM）通常被用于提高自动语音识别（ASR）的性能，尤其是在领域适应任务中。传统的LM训练方式将语料库中的所有单词平等对待，导致ASR性能的改进效果不佳。本文引入了一种新颖的纠错型LM训练方法，旨在优先处理ASR易出错的单词。我们定义了单词级ASR易出错分数，表示ASR错误识别的可能性，并将其形成一个先验单词分布以指导LM训练。为了在仅有文本语料库的情况下实现纠错型训练，我们采用大型语言模型（LLM）作为易出错分数预测器和文本生成器，并进行多任务微调。领域适应任务的实验结果表明了我们提出的方法的有效性。与传统的LM相比，纠错型训练在足够数据集上可以达到相对5.5%的单词错误率（WER）降低。",
    "tldr": "本研究介绍了一种为语音识别进行纠错的语言模型训练方法，通过定义易出错单词的分数并将其用作先验分布来指导训练，并利用大型语言模型进行纠错型训练。实验证明该方法在领域适应任务中有效，相对传统方法可以显著降低单词错误率。"
}