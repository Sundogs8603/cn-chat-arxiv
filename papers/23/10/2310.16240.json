{
    "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models. (arXiv:2310.16240v1 [cs.CL])",
    "abstract": "In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores. Our experiment results with three different pre-trained models show that our approach can outperform state-of-the-art PEFT methods with a comparable number of parameters. In addition, we provide additional analysis to examine the experts selected by each model at each layer to provide insights for future studies.",
    "link": "http://arxiv.org/abs/2310.16240",
    "context": "Title: Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models. (arXiv:2310.16240v1 [cs.CL])\nAbstract: In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores. Our experiment results with three different pre-trained models show that our approach can outperform state-of-the-art PEFT methods with a comparable number of parameters. In addition, we provide additional analysis to examine the experts selected by each model at each layer to provide insights for future studies.",
    "path": "papers/23/10/2310.16240.json",
    "total_tokens": 859,
    "translated_title": "使用混合语言专家适配器改进和解释预训练语言模型",
    "translated_abstract": "本文提出了一种方法，通过将语言结构注入到预训练语言模型中，在参数高效的微调设置下，将两个热门研究领域相结合。在我们的方法中，使用一种新颖的混合语言专家架构组合编码不同语言结构的并行适配器模块，通过Gumbel-Softmax门确定模型每一层中这些模块的重要性。为了减少参数数量，在修剪专家的重要性评分后，我们首先对模型进行一定数量的固定步骤的训练。我们在三种不同的预训练模型上进行了实验，结果表明我们的方法在具有可比较参数数量的情况下，可以超越最先进的参数高效微调方法。此外，我们提供了额外的分析来检查每个模型在每层选择的专家，为未来的研究提供参考。",
    "tldr": "本研究提出了一种方法，在预训练语言模型中引入语言结构，通过混合语言专家架构来改进和解释其性能，并且实验证明该方法在参数高效微调中比其他方法表现更好。",
    "en_tdlr": "This paper proposes a method to improve and interpret pre-trained language models by injecting linguistic structures, using a mixture-of-linguistic-experts architecture. The experiments show that this method outperforms other parameter-efficient fine-tuning methods."
}