{
    "title": "Who Said That? Benchmarking Social Media AI Detection. (arXiv:2310.08240v1 [cs.CL])",
    "abstract": "AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text ",
    "link": "http://arxiv.org/abs/2310.08240",
    "context": "Title: Who Said That? Benchmarking Social Media AI Detection. (arXiv:2310.08240v1 [cs.CL])\nAbstract: AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text ",
    "path": "papers/23/10/2310.08240.json",
    "total_tokens": 916,
    "translated_title": "谁说的？社交媒体AI检测的基准测试",
    "translated_abstract": "AI生成的文本在各种在线平台上广泛存在，既带来了变革的前景，也带来了与虚假信息和操纵相关的重大风险。为了应对这些挑战，本文介绍了SAID（社交媒体AI检测），这是一个新颖的基准测试，用于评估真实社交媒体平台上AI文本检测模型的能力。它包含来自知乎和Quora等热门社交媒体平台的真实AI生成文本。与现有的基准测试不同，SAID处理反映真实AI用户在互联网上使用的复杂策略的内容，这些策略可能逃避检测或获得可见性，提供了一个更加真实和具有挑战性的评估环境。基于知乎数据集的一个显著发现是，注释员可以以96.5%的平均准确率区分AI生成文本和人类生成文本。这一发现需要重新评估人类识别AI生成文本的能力。",
    "tldr": "本文介绍了SAID，一个用于评估AI文本检测模型在真实社交媒体平台上的能力的新基准测试。研究发现，基于知乎数据集，注释员可以以96.5%的准确率区分AI生成文本和人类生成文本。",
    "en_tdlr": "This paper introduces SAID, a novel benchmark for evaluating the capabilities of AI text detection models on real social media platforms. The study shows that annotators can distinguish between AI-generated and human-generated text with an average accuracy rate of 96.5% based on the Zhihu dataset."
}