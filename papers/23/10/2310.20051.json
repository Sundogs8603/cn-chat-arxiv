{
    "title": "The Expressibility of Polynomial based Attention Scheme. (arXiv:2310.20051v1 [cs.LG])",
    "abstract": "Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.  In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability o",
    "link": "http://arxiv.org/abs/2310.20051",
    "context": "Title: The Expressibility of Polynomial based Attention Scheme. (arXiv:2310.20051v1 [cs.LG])\nAbstract: Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.  In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability o",
    "path": "papers/23/10/2310.20051.json",
    "total_tokens": 778,
    "translated_title": "基于多项式的注意力机制的表达能力",
    "translated_abstract": "大型语言模型（LLM）显著改善了我们日常生活的各个方面。这些模型影响了许多领域，从医疗保健到教育，提高了生产力、决策过程和可访问性。然而，在扩展这些模型以处理长文本内容时，transformer架构中的注意力的二次复杂度提出了挑战。这个问题使得在训练非常大的模型或在推断过程中高效地使用它们变得不切实际。最近由[KMZ23]提出了一种使用多项式函数和多项式草图替换softmax以加速注意力机制的技术，但对这种新方法的理论理解尚不完善。在本文中，我们对多项式注意力的表达能力进行了理论分析。我们的研究揭示了多项式注意力能力的差异性。",
    "tldr": "本文研究了基于多项式的注意力机制的表达能力，揭示了其与传统softmax的差异。"
}