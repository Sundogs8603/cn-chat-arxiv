{
    "title": "TEQ: Trainable Equivalent Transformation for Quantization of LLMs. (arXiv:2310.10944v1 [cs.CL])",
    "abstract": "As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.",
    "link": "http://arxiv.org/abs/2310.10944",
    "context": "Title: TEQ: Trainable Equivalent Transformation for Quantization of LLMs. (arXiv:2310.10944v1 [cs.CL])\nAbstract: As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.",
    "path": "papers/23/10/2310.10944.json",
    "total_tokens": 810,
    "translated_title": "TEQ：用于LLM量化的可训练等效转换",
    "translated_abstract": "随着大型语言模型（LLMs）变得越来越普遍，需要新的、改进的量化方法以满足这些现代架构的计算要求，同时保持准确性。在本文中，我们提出了TEQ，一种可训练的等效转换，可以保留模型输出的FP32精度，同时利用低精度量化，特别是3位和4位的仅权重量化。训练过程轻量化，只需1K步骤和少于原模型可训练参数的0.1%。此外，在推理过程中，该转换不会增加任何计算开销。我们的结果与典型LLMs的最先进方法相媲美。我们的方法可以与其他方法结合使用，以实现更好的性能。代码可在https://github.com/intel/neural-compressor获得。",
    "tldr": "本文提出了TEQ，一种可训练的等效转换方法，可以在保持模型准确性的同时，利用低精度的量化方式满足大型语言模型的计算要求。与现有的最先进方法相比，我们的方法在典型的LLMs上具有相似的表现，且不增加推理时的计算开销。"
}