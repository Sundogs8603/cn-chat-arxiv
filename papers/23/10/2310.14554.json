{
    "title": "Making RL with Preference-based Feedback Efficient via Randomization",
    "abstract": "arXiv:2310.14554v2 Announce Type: replace-cross  Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by th",
    "link": "https://arxiv.org/abs/2310.14554",
    "context": "Title: Making RL with Preference-based Feedback Efficient via Randomization\nAbstract: arXiv:2310.14554v2 Announce Type: replace-cross  Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by th",
    "path": "papers/23/10/2310.14554.json",
    "total_tokens": 871,
    "translated_title": "通过随机化使基于偏好反馈的强化学习变得高效",
    "translated_abstract": "强化学习算法需要从人类反馈中学习，而且在统计复杂性、计算复杂性和查询复杂性方面需要高效。本研究考虑了使用偏好来表达对轨迹对的反馈的强化学习设置。在线性MDP模型中，通过在算法设计中引入随机化，我们提出了一种算法，具有样本高效性（即具有近乎最优的最坏情况遗憾界限）和多项式运行时间（即计算复杂度与相关参数是多项式关系）。我们的算法进一步通过一种新颖的随机化主动学习过程最小化了查询复杂性。特别地，我们的算法展示了遗憾界限和查询复杂性之间的近乎最优折衷。为了将结果推广到更一般的非线性函数逼近，我们设计了一个受线性MDP模型的随机化算法启发而来的基于模型的随机化算法。",
    "tldr": "在基于偏好反馈的强化学习中，通过引入随机化设计的算法在线性MDP模型下表现出样本高效性和多项式运行时间，并通过随机化主动学习过程最小化了查询复杂性。",
    "en_tdlr": "An algorithm designed with randomization in the reinforcement learning with preference-based feedback setting shows sample efficiency and polynomial running time in the linear MDP model, while minimizing query complexity through a randomized active learning process."
}