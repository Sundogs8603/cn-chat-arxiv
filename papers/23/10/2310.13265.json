{
    "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model. (arXiv:2310.13265v1 [cs.CL])",
    "abstract": "Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap wit",
    "link": "http://arxiv.org/abs/2310.13265",
    "context": "Title: MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model. (arXiv:2310.13265v1 [cs.CL])\nAbstract: Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap wit",
    "path": "papers/23/10/2310.13265.json",
    "total_tokens": 975,
    "translated_title": "MoqaGPT: 大型语言模型的零射模式多模态开放域问答",
    "translated_abstract": "多模态开放域问答通常需要从不同的模态（如图像、表格、段落等）的数据库中检索证据。即使是像GPT-4这样的大型语言模型在这个任务中也存在困难。为了让大型语言模型能够以零射模式处理这个任务，我们引入了MoqaGPT，这是一个直观灵活的框架。我们的框架使用一种分而治之的策略，绕过复杂的多模态排序，可以容纳新的模态，并无缝转换到任务的新模型。基于大型语言模型，MoqaGPT分别从每个模态中检索和提取答案，然后使用大型语言模型将这些多模态信息融合在一起，产生最终的答案。我们的方法在MMCoQA数据集上提升了性能，相比有监督基准线，F1提高了37.91个点，EM提高了34.07个点。在MultiModalQA数据集上，MoqaGPT超越了零射基准线，F1提高了9.5个点，EM提高了10.1个点，并显著缩小了差距。",
    "tldr": "MoqaGPT是一个基于大型语言模型的零射模式多模态开放域问答框架，通过分而治之的策略来处理多模态信息，并在两个数据集上取得显著性能提升。",
    "en_tdlr": "MoqaGPT is a zero-shot multi-modal open-domain question answering framework based on large language models. It utilizes a divide-and-conquer strategy to handle multi-modal information and achieves significant performance improvement on two datasets."
}