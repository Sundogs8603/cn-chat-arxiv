{
    "title": "ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer. (arXiv:2310.02588v1 [cs.CV])",
    "abstract": "This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed ",
    "link": "http://arxiv.org/abs/2310.02588",
    "context": "Title: ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer. (arXiv:2310.02588v1 [cs.CV])\nAbstract: This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed ",
    "path": "papers/23/10/2310.02588.json",
    "total_tokens": 942,
    "translated_title": "ViT-ReciproCAM: 不需要梯度和注意力的Vision Transformer的视觉解释方法",
    "translated_abstract": "本文提出了一种新的方法来解决Vision Transformers (ViT)在理解预测过程和调试预测错误方面的挑战。ViT在各种计算机视觉任务（如图像分类和目标检测）中表现出了卓越的性能。虽然对于卷积神经网络（CNN）已经广泛研究了一些可视化解释技术，如CAM、Grad-CAM、Score-CAM和Recipro-CAM，但在ViT上的研究仍然有限。当前ViT的最新解决方案依赖于类不可知的Attention-Rollout和Relevance技术。本文提出了一种新的不依赖梯度的ViT视觉解释方法，称为ViT-ReciproCAM，它不需要注意力矩阵和梯度信息。ViT-ReciproCAM利用令牌遮罩和从目标层的输入产生的新层输出来利用激活的令牌与目标类别的网络预测之间的关联。",
    "tldr": "本文提出了一种新的方法，名为ViT-ReciproCAM，用于解释Vision Transformer中的预测过程和调试预测错误。该方法不依赖梯度和注意力矩阵，并使用令牌遮罩和新的层输出来利用激活的令牌与目标类别的网络预测之间的关联。",
    "en_tdlr": "This paper proposes a novel approach called ViT-ReciproCAM to explain the prediction process and debug prediction errors in Vision Transformers. It does not rely on gradients and attention matrices, and utilizes token masking and new layer outputs to exploit the correlation between activated tokens and network predictions for target classes."
}