{
    "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing. (arXiv:2310.18413v1 [cs.LG])",
    "abstract": "In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which ensures that the predictor is unbiased not only in terms of expectations over the whole population, but also within any subregion of the feature space, unknown at training time. To enforce this objective, we introduce ROAD, a novel approach that leverages the Distributionally Robust Optimization (DRO) framework within a fair adversarial learning objective, where an adversary tries to infer the sensitive attribute from the predictions. Using an instance-level re-weighting strategy, ROAD is designed to prioritize inputs that are likely to be locally unfair, i.e. where the adversary faces the least difficulty in reconstructing the sensitive a",
    "link": "http://arxiv.org/abs/2310.18413",
    "context": "Title: On the Fairness ROAD: Robust Optimization for Adversarial Debiasing. (arXiv:2310.18413v1 [cs.LG])\nAbstract: In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which ensures that the predictor is unbiased not only in terms of expectations over the whole population, but also within any subregion of the feature space, unknown at training time. To enforce this objective, we introduce ROAD, a novel approach that leverages the Distributionally Robust Optimization (DRO) framework within a fair adversarial learning objective, where an adversary tries to infer the sensitive attribute from the predictions. Using an instance-level re-weighting strategy, ROAD is designed to prioritize inputs that are likely to be locally unfair, i.e. where the adversary faces the least difficulty in reconstructing the sensitive a",
    "path": "papers/23/10/2310.18413.json",
    "total_tokens": 872,
    "translated_title": "关于道路公平性的研究：针对对抗去偏的鲁棒优化",
    "translated_abstract": "在算法公平性领域，人们一直关注群体公平性准则，如人口统计学平价和平等赔率。然而，这些以全局平均值衡量的目标引发了有关敏感群体之间持续局部差异的担忧。本研究解决了局部公平性问题，即确保预测器在整个人群中的期望值以及在训练时未知的任何特征空间的子区域内均无偏见。为了实现这个目标，我们引入了ROAD，这是一种新颖的方法，利用分布鲁棒优化（DRO）框架来进行公平对抗学习，其中对手试图从预测中推断出敏感属性。通过实例级重新加权策略，ROAD被设计为优先考虑可能出现局部不公平的输入，即对手在重建敏感属性时最困难的情况。",
    "tldr": "本论文研究了在算法公平性领域中存在的局部差异问题，并提出了一种基于分布鲁棒优化的公平对抗学习方法，以实现针对局部特征空间的公平性标准。"
}