{
    "title": "Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])",
    "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.",
    "link": "http://arxiv.org/abs/2310.10971",
    "context": "Title: Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])\nAbstract: Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.",
    "path": "papers/23/10/2310.10971.json",
    "total_tokens": 849,
    "translated_title": "上下文感知元学习",
    "translated_abstract": "ChatGPT等大型语言模型展示了在推理过程中无需微调就能学习新概念的卓越能力。然而，用于推理过程中检测新对象的视觉模型尚未能够复制这种能力，而是表现糟糕或需要对类似对象进行元训练和/或微调。在这项工作中，我们提出了一种元学习算法，通过在推理过程中学习新的视觉概念而无需微调来模仿大型语言模型。我们的方法利用一个冻结的预训练特征提取器，并类似于上下文学习，将元学习重新定义为在已知标签的数据点和未知标签的测试数据点上的序列建模。在11个元学习基准中的8个中，我们的方法 - 无需元训练或微调 - 超过或与在这些基准上经过元训练的最先进算法P>M>F相匹配。",
    "tldr": "本文提出了一种上下文感知的元学习算法，可以在推理过程中学习新的视觉概念而无需微调。该方法在多个元学习基准中表现优异，超过或与目前的最先进算法相匹配。"
}