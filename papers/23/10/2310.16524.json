{
    "title": "Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])",
    "abstract": "Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines -- including real test data alone -- in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing ",
    "link": "http://arxiv.org/abs/2310.16524",
    "context": "Title: Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])\nAbstract: Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines -- including real test data alone -- in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing ",
    "path": "papers/23/10/2310.16524.json",
    "total_tokens": 899,
    "translated_title": "你能依靠模型评估吗？使用合成测试数据改进模型评估。",
    "translated_abstract": "在保证公平和可靠性的真实应用中，评估机器学习模型在多样化和代表性不足的子群体上的性能至关重要。然而，由于两个主要问题，准确评估模型性能变得具有挑战性：（1）测试数据稀缺，特别是对于小的子群体，（2）模型在部署环境中可能出现的分布偏移与可用的测试数据不一致。在这项工作中，我们介绍了一个名为3S Testing的深度生成建模框架，通过生成小型子群体的合成测试集和模拟分布偏移来促进模型评估。我们的实验表明，相比于仅使用真实测试数据，3S Testing在评估少数子群体的模型性能和在可能的分布偏移下表现更优。此外，3S Testing提供了性能估计的区间，相比现有方法，展现了更优的覆盖率。",
    "tldr": "本文介绍了3S Testing，这是一个深度生成建模框架，通过生成合成测试集和模拟分布偏移来改进模型评估，实验结果表明3S Testing在评估少数子群体的模型性能和在分布偏移下表现更优。",
    "en_tdlr": "This paper introduces 3S Testing, a deep generative modeling framework that improves model evaluation by generating synthetic test sets and simulating distributional shifts. Experimental results demonstrate that 3S Testing outperforms in evaluating model performance on minority subgroups and under distributional shifts."
}