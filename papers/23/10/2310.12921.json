{
    "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
    "abstract": "arXiv:2310.12921v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second \"baseline\" prom",
    "link": "https://arxiv.org/abs/2310.12921",
    "context": "Title: Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning\nAbstract: arXiv:2310.12921v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second \"baseline\" prom",
    "path": "papers/23/10/2310.12921.json",
    "total_tokens": 859,
    "translated_title": "视觉语言模型是强化学习的零样本奖励模型",
    "translated_abstract": "强化学习（RL）要求手动指定奖励函数，这通常是不可行的，或者通过大量人类反馈学习奖励模型，这通常是非常昂贵的。本文研究了一种更加样本高效的替代方案：使用预训练的视觉语言模型（VLM）作为零样本奖励模型（RM），通过自然语言指定任务。我们提出了一种自然和通用的使用VLM作为奖励模型的方法，称为VLM-RMs。我们使用基于CLIP的VLM-RMs来训练MuJoCo人形模型学习复杂任务，而无需手动指定奖励函数，例如跪下、劈叉和盘腿坐。对于每个任务，我们仅提供一个描述所需任务的单个句子文本提示，减少提示工程。我们提供训练代理的视频链接：https://sites.google.com/view/vlm-rm。我们可以通过提供第二个“基准”提示来改善性能。",
    "tldr": "使用预训练的视觉语言模型作为零样本奖励模型，在强化学习中指定任务，提高训练效率。",
    "en_tdlr": "Using pretrained vision-language models as zero-shot reward models to specify tasks via natural language in reinforcement learning improves training efficiency."
}