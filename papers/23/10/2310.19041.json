{
    "title": "On Linear Separation Capacity of Self-Supervised Representation Learning. (arXiv:2310.19041v1 [stat.ML])",
    "abstract": "Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. ",
    "link": "http://arxiv.org/abs/2310.19041",
    "context": "Title: On Linear Separation Capacity of Self-Supervised Representation Learning. (arXiv:2310.19041v1 [stat.ML])\nAbstract: Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. ",
    "path": "papers/23/10/2310.19041.json",
    "total_tokens": 845,
    "translated_title": "自监督表示学习的线性分离能力研究",
    "translated_abstract": "自监督学习的最新进展强调了数据增强在从无标签数据中学习数据表示中的有效性。在这些增强表示之上训练线性模型可以得到一个熟练的分类器。尽管在实践中表现出色，但是数据增强如何将非线性数据结构解开为线性可分离表示的机制仍然不清楚。本文旨在通过研究在从多流形模型中绘制数据时，学习到的表示在何种条件下可以线性分离流形来填补这一差距。我们的研究揭示了数据增强除了提供观察数据外，还提供了额外的信息，从而可以改善线性分离容量的信息论最优速率。特别是，我们证明自监督学习可以以比无监督学习更小的距离线性分离流形，突显了数据增强的额外好处。",
    "tldr": "本研究通过探究在多流形模型下，学习的表示何时可以线性分离流形，揭示了自监督学习在数据增强方面的额外好处，从而改善了线性分离能力的信息论最优速率。",
    "en_tdlr": "This study investigates the conditions under which learned representations can linearly separate manifolds, revealing the additional benefits of data augmentation in improving the information-theoretic optimal rate of linear separation capacity."
}