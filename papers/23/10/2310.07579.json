{
    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)",
    "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i",
    "link": "http://arxiv.org/abs/2310.07579",
    "context": "Title: In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)\nAbstract: Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i",
    "path": "papers/23/10/2310.07579.json",
    "total_tokens": 939,
    "translated_title": "In-Context Unlearning: 基于少样本学习的语言模型的消除研究",
    "translated_abstract": "机器消除学习是研究如何高效地去除特定训练数据对训练模型的影响，近来引起了更多的关注，主要是由于需要遵守诸如被遗忘权等隐私法规的需求。尽管在版权问题上LLM（语言模型）尤其相关，但在非常大的模型上实现精确消除是计算上不可行的。为此，最近的研究提出了几种算法，可以在不重新训练模型的情况下近似消除训练数据。这些算法关键依赖于对模型参数的访问来更新它们，但在实践中可能由于计算约束或通过API访问LLM而无法满足这种假设。在这项工作中，我们提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。为了消除特定的训练实例，我们提供了i",
    "tldr": "这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。",
    "en_tdlr": "This work proposes a new method for unlearning LLMs called \"In-Context Unlearning\", which provides inputs in context and does not require updating model parameters. This method addresses the computational difficulties of unlearning for large models and offers higher feasibility and convenience in practice."
}