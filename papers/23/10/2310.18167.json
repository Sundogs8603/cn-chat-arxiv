{
    "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension. (arXiv:2310.18167v1 [cs.CL])",
    "abstract": "The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we ",
    "link": "http://arxiv.org/abs/2310.18167",
    "context": "Title: MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension. (arXiv:2310.18167v1 [cs.CL])\nAbstract: The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we ",
    "path": "papers/23/10/2310.18167.json",
    "total_tokens": 863,
    "translated_title": "MPrompt: 探索用于机器阅读理解的多级提示调整",
    "translated_abstract": "大型语言模型在各种自然语言任务上取得了优异的性能。这种方法的一个主要缺点是在对新数据集进行微调时需要大量资源。软提示调整提供了一种资源高效的解决方法，可以在保持预训练语言模型（PLM）固定权重的同时进行微调。现有的软提示方法主要关注设计与新数据集领域匹配的与输入无关的提示。这些方法通常忽略了任务和文本上下文的细粒度信息。在本文中，我们提出了一种多级提示调整（MPrompt）方法用于机器阅读理解。它在任务特定、领域特定和上下文特定级别上利用提示来增强不同细粒度的输入语义理解。我们还提出了一个独立性约束，使每个领域特定的提示集中在其领域内的信息，避免冗余。",
    "tldr": "MPrompt是一种用于机器阅读理解的多级提示调整方法，通过在任务特定、领域特定和上下文特定级别上利用提示来增强不同细粒度的输入语义理解，并通过独立性约束避免冗余。",
    "en_tdlr": "MPrompt is a multi-level prompt tuning method for machine reading comprehension that enhances the understanding of input semantics at different granularities by utilizing prompts at task-specific, domain-specific, and context-specific levels, and avoids redundancy through an independence constraint."
}