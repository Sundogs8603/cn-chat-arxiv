{
    "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])",
    "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than sm",
    "link": "http://arxiv.org/abs/2310.07289",
    "context": "Title: Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators. (arXiv:2310.07289v1 [cs.CL])\nAbstract: Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than sm",
    "path": "papers/23/10/2310.07289.json",
    "total_tokens": 922,
    "translated_title": "超越事实性：对大型语言模型作为知识生成器的全面评估",
    "translated_abstract": "在提示生成世界知识时，大型语言模型（LLMs）优于信息检索技术，用于知识密集型任务。然而，社区对使用这种未经审查的知识的事实性和潜在影响的担忧不绝于耳。鉴于此，我们介绍了CONNER，一种综合的知识评估框架，旨在系统地和自动地从六个重要的角度评估生成的知识--事实性、相关性、连贯性、信息性、有用性和有效性。我们对来自三种不同类型的LLMs的生成知识进行了广泛的实证分析，这三种LLMs用于两个广泛研究的知识密集型任务，即开放域问答和基于知识的对话。令人惊讶的是，我们的研究发现，即使生成的知识的事实性较低，也不会显著阻碍下游任务。相反，产出的相关性和连贯性比准确度更重要。",
    "tldr": "该论文对大型语言模型（LLMs）作为知识生成器的能力进行了全面评估，介绍了一个评估框架CONNER。研究发现，生成的知识的事实性不如准确性重要，而生成的相关性和连贯性更为重要。"
}