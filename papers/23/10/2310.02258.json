{
    "title": "A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])",
    "abstract": "Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\\alpha}$, $\\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\\alpha=1$) from their predictions ($\\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more \"lottery tickets\", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the \"central limit theorem\" of lottery tickets. Finally, we discuss its potential implications f",
    "link": "http://arxiv.org/abs/2310.02258",
    "context": "Title: A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])\nAbstract: Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\\alpha}$, $\\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\\alpha=1$) from their predictions ($\\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more \"lottery tickets\", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the \"central limit theorem\" of lottery tickets. Finally, we discuss its potential implications f",
    "path": "papers/23/10/2310.02258.json",
    "total_tokens": 954,
    "translated_title": "《来自彩票票集成的神经规模定律》",
    "translated_abstract": "神经规模定律（NSL）指的是模型性能随着规模增加而提高的现象。Sharma＆Kaplan使用近似理论分析了NSL，并预测了MSE损失的衰减方式为$N^{-\\alpha}$，其中$\\alpha=4/d$，$N$为模型参数数量，$d$为内在输入维度。尽管他们的理论在某些情况下效果良好（例如ReLU网络），但令人惊讶的是，我们发现在简单的1D问题$y=x^2$中，表现出了与他们预测不同的缩放定律（$\\alpha=1$而不是$\\alpha=4$）。我们打开了神经网络并发现新的缩放定律源于彩票票集成：平均而言，更宽的网络有更多的“彩票票”，它们被集成来减小输出的方差。我们通过对单个神经网络进行机械解释以及对它们进行统计研究来支持集成机制。我们将$N^{-1}$的缩放定律归因于“彩票票的中心极限定理”。最后，我们讨论了它的潜在影响。",
    "tldr": "《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。",
    "en_tdlr": "\"A Neural Scaling Law from Lottery Ticket Ensembling\" investigates the neural scaling law phenomenon and reveals its connection to lottery ticket ensembling, resulting in a novel scaling law and potential implications."
}