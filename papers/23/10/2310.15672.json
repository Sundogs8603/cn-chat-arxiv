{
    "title": "How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])",
    "abstract": "For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.",
    "link": "http://arxiv.org/abs/2310.15672",
    "context": "Title: How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])\nAbstract: For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.",
    "path": "papers/23/10/2310.15672.json",
    "total_tokens": 877,
    "translated_title": "我的基于注意力的ASR系统需要多少上下文信息？",
    "translated_abstract": "对于语音识别任务，使用超过30秒的声学上下文进行训练在文献中是不常见的，并且得到了很少的研究。在这项工作中，我们研究了训练/评估（基于密集注意力的）声学模型和语言模型时，序列长度的缩放对语音识别性能的影响。我们使用了大约100,000个伪标记的Spotify播客数据集进行了这些实验，探索了5秒到1小时的上下文长度。对长格式数据集Earnings-22和Tedlium进行了零-shot评估，结果表明使用大约80秒的声学上下文进行训练可以带来高达14.9%相对改进。此外，我们通过束搜索使用长上下文变换语言模型与系统组合形成了一个全长上下文ASR系统，其结果与当前最先进的方法具有竞争力。",
    "tldr": "本研究考察了对于音频识别任务，训练和评估使用不同长度的序列对语音识别性能的影响。结果表明，使用大约80秒的声学上下文进行训练可以相对提高14.9%的性能，并且与当前最先进的方法具有竞争力。",
    "en_tdlr": "This study investigates the impact of using different sequence lengths for training and evaluation on speech recognition performance. Results show that training with approximately 80 seconds of acoustic context can result in a relative improvement of 14.9% and achieve competitive performance compared to the current state-of-the-art methods."
}