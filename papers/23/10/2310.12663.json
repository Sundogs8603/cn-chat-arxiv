{
    "title": "Knowledge from Uncertainty in Evidential Deep Learning. (arXiv:2310.12663v1 [cs.LG])",
    "abstract": "This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and",
    "link": "http://arxiv.org/abs/2310.12663",
    "context": "Title: Knowledge from Uncertainty in Evidential Deep Learning. (arXiv:2310.12663v1 [cs.LG])\nAbstract: This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and",
    "path": "papers/23/10/2310.12663.json",
    "total_tokens": 963,
    "translated_title": "从不确定性中获取知识的证据深度学习",
    "translated_abstract": "这项工作揭示了在证据深度学习（EDL）中从不确定性值中产生的证据信号。 EDL是一类考虑不确定性的深度学习方法的示例，旨在提供关于当前测试样本的置信度（或认知不确定性）。特别是对于计算机视觉和双向编码器大型语言模型，EDL中的Dirichlet强度引发的“证据信号”在某些情况下可以区分类别，尤其是在使用大型语言模型时表现得非常强。我们假设KL正则化项导致EDL将aleatoric不确定性和认知不确定性耦合在一起。在本文中，我们通过实证研究了误分类和评估不确定性之间的相关性，并展示了EDL的“证据信号”是由于误分类偏差而产生的。我们批判性地评估了EDL与其他基于Dirichlet的方法，即生成式证据神经网络（EDL-GEN）和先验网络，并理论上和实验上证明了。",
    "tldr": "本论文研究了证据深度学习（EDL）中产生的来自不确定性的证据信号，并发现这些信号可以在某些情况下区分类别。研究还探讨了EDL与其他基于Dirichlet的方法之间的关联，并表明EDL的“证据信号”是由于误分类偏差而产生的。"
}