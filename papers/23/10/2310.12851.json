{
    "title": "EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks. (arXiv:2310.12851v1 [cs.SD])",
    "abstract": "In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature e",
    "link": "http://arxiv.org/abs/2310.12851",
    "context": "Title: EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks. (arXiv:2310.12851v1 [cs.SD])\nAbstract: In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature e",
    "path": "papers/23/10/2310.12851.json",
    "total_tokens": 854,
    "translated_title": "EmoDiarize: 使用卷积神经网络从语音信号中进行说话人日程和情感识别",
    "translated_abstract": "在先进的人工智能和人机交互时代，识别口头语言中的情感至关重要。本研究探讨了深度学习技术在语音情感识别中的应用，提供了一个综合的解决方案来应对说话人日程和情感识别所面临的挑战。它引入了一个框架，将现有的说话人日程流程和基于卷积神经网络 (CNN) 的情感识别模型相结合，以实现更高的精度。所提出的模型是在五个语音情感数据集 (RAVDESS，CREMA-D，SAVEE，TESS和电影片段) 上进行训练的，其中后者是专门为本研究创建的一个语音情感数据集。从每个样本中提取的特征包括Mel频率倒谱系数 (MFCC)，过零率 (ZCR)，均方根 (RMS) 和各种数据增强算法，如音高、噪声、拉伸和移位。",
    "tldr": "本研究提出了一个综合的解决方案，将说话人日程和情感识别相结合，使用深度学习技术和卷积神经网络提高了精度。"
}