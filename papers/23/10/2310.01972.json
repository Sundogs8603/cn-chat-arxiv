{
    "title": "Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])",
    "abstract": "We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\\mathcal{O}(\\frac{n^3}{s^2})$ which outperforms the best-known bound $\\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu",
    "link": "http://arxiv.org/abs/2310.01972",
    "context": "Title: Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])\nAbstract: We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\\mathcal{O}(\\frac{n^3}{s^2})$ which outperforms the best-known bound $\\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu",
    "path": "papers/23/10/2310.01972.json",
    "total_tokens": 976,
    "translated_title": "流行学习：通过随机通信增强分散式学习",
    "translated_abstract": "我们提出了一种名为流行学习（EL）的简单而强大的分散式学习（DL）算法，利用不断变化的通信拓扑结构，以比传统的DL方法更快的模型收敛速度。在EL的每一轮中，每个节点将其模型更新发送给一个随机样本的$s$个其他节点（在$n$个节点的系统中）。我们对EL进行了广泛的理论分析，证明其变化的拓扑结构导致了比现有的（静态和动态）拓扑结构更好的收敛性能。对于平滑的非凸损失函数，EL的暂态迭代次数，即达到渐近线性加速所需的轮数，是$\\mathcal{O}(\\frac{n^3}{s^2})$，超过了已知的最佳界限$\\mathcal{O}({n^3})$，增加了$s^2$倍，表明了随机通信在DL中的好处。我们在一个96个节点的网络中对EL进行了实证评估，并将其性能与现有的DL方法进行了比较。我们的结果显示，EL达到了更快的模型收敛速度和更好的收敛性能。",
    "tldr": "流行学习是一种简单而强大的分散式学习算法，通过利用变化的通信拓扑结构实现了比传统方法更快的模型收敛速度，具有更好的收敛性能。",
    "en_tdlr": "Epidemic Learning (EL) is a simple yet powerful decentralized learning (DL) algorithm that achieves faster model convergence compared to conventional DL approaches by leveraging changing communication topologies. It outperforms state-of-the-art methods in terms of convergence properties and achieves faster model convergence."
}