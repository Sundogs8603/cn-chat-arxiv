{
    "title": "Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model. (arXiv:2310.13010v1 [eess.AS])",
    "abstract": "We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also benefic",
    "link": "http://arxiv.org/abs/2310.13010",
    "context": "Title: Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model. (arXiv:2310.13010v1 [eess.AS])\nAbstract: We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also benefic",
    "path": "papers/23/10/2310.13010.json",
    "total_tokens": 1004,
    "translated_title": "使用基于Perceiver的序列分类器和通用语音模型检测语音异常",
    "translated_abstract": "我们提出了一种基于Perceiver的序列分类器，用于检测反映多种神经系统疾病的语音异常。我们将这个分类器与一个训练有素的通用语音模型（USM）相结合，该模型在1200万小时的多样化音频记录上进行无监督训练。我们的模型将长序列压缩为一小组特定类别的潜在表示，并使用分解投影预测输入语音的不同属性。我们的方法的好处是可以为不同类别的输入建模不同的区域，并且同时具有数据效率性。我们对Mayo Clinic的精选语料库对我们提出的模型进行了广泛评估。我们的模型在标准Transformer（80.9%）和Perceiver（81.8%）模型上表现出色，平均准确率达到83.1%。在有限的任务特定数据情况下，我们发现预训练非常重要，而且令人惊讶的是，与不相关的自动语音识别（ASR）任务进行预训练也是有益的。",
    "tldr": "该论文提出了一种使用基于Perceiver的序列分类器和通用语音模型检测语音异常的方法，该方法可以针对不同类别的输入建模不同的区域，同时具有数据效率性。通过在Mayo Clinic的语料库上进行广泛评估，该模型表现出色，平均准确率达到83.1%。预训练是重要的，而且意外的是，与不相关的自动语音识别（ASR）任务进行预训练也是有益的。"
}