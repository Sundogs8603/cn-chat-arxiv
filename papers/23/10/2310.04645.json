{
    "title": "Do self-supervised speech and language models extract similar representations as human brain?",
    "abstract": "Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept",
    "link": "https://arxiv.org/abs/2310.04645",
    "context": "Title: Do self-supervised speech and language models extract similar representations as human brain?\nAbstract: Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept",
    "path": "papers/23/10/2310.04645.json",
    "total_tokens": 942,
    "translated_title": "自我监督的语音和语言模型是否提取了与人类大脑类似的表示？",
    "translated_abstract": "通过自我监督学习（SSL）训练的语音和语言模型在语音和语言感知期间展现出与大脑活动的强大对齐性。然而，由于它们的不同训练方式，它们是否与相同的神经方面相关仍然不清楚。我们通过评估两种代表性的SSL模型（Wav2Vec2.0和GPT-2）在语音和语言任务中的大脑预测性能来直接回答这个问题。我们的研究结果显示，这两种模型都能准确预测听觉皮层中的语音响应，并且它们的大脑预测之间存在显著相关性。值得注意的是，Wav2Vec2.0和GPT-2之间的共享语音上下文信息解释了大脑活动中的大部分变异，超过了静态语义和较低级的声音-音位信息。这些结果强调了SSL模型中语音上下文表示的收敛性，以及它们与语音知觉底层神经网络的对齐。",
    "tldr": "通过评估Wav2Vec2.0和GPT-2模型的大脑预测能力，我们发现自我监督的语音和语言模型能够准确预测语音反应，其大脑预测之间存在显著相关性，且共享的语音上下文信息是解释大脑活动中变异的主要因素。"
}