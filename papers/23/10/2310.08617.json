{
    "title": "The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features. (arXiv:2310.08617v1 [cs.AI])",
    "abstract": "AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model bia",
    "link": "http://arxiv.org/abs/2310.08617",
    "context": "Title: The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features. (arXiv:2310.08617v1 [cs.AI])\nAbstract: AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model bia",
    "path": "papers/23/10/2310.08617.json",
    "total_tokens": 904,
    "translated_title": "AI决策中解释对公平性的影响：受保护特征 vs 代理特征",
    "translated_abstract": "已知AI系统可能会放大现实世界数据中的偏见。解释可能有助于人工智能团队解决这些偏见，从而实现更公平的决策。通常，解释侧重于突出的输入特征。如果模型对某个受保护的群体存在偏见，则解释可能包括显示这种偏见的特征，但当偏见通过代理特征实现时，这个代理特征与受保护特征之间的关系可能对人类而言不太清晰。在这项工作中，我们研究受保护特征和代理特征对参与者对模型公平性和提高人口平衡能力的感知的影响。此外，我们还研究了不同处理方式（解释、模型偏见披露和代理相关性披露）对公平感知和平等性的影响。我们发现，解释有助于人们检测直接偏见，但无法发现间接偏见。此外，无论偏见类型如何，解释倾向于增加对模型偏见的认同度。",
    "tldr": "论文研究了解释对AI决策公平性的影响。结果发现，解释有助于人们检测直接偏见，但对间接偏见的发现能力有限。",
    "en_tdlr": "This paper investigates the impact of explanations on fairness in AI decision-making. The results show that explanations help detect direct biases but have limited ability to uncover indirect biases."
}