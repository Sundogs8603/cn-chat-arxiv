{
    "title": "Understanding prompt engineering may not require rethinking generalization. (arXiv:2310.03957v1 [cs.LG])",
    "abstract": "Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that",
    "link": "http://arxiv.org/abs/2310.03957",
    "context": "Title: Understanding prompt engineering may not require rethinking generalization. (arXiv:2310.03957v1 [cs.LG])\nAbstract: Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that",
    "path": "papers/23/10/2310.03957.json",
    "total_tokens": 903,
    "translated_title": "理解提示工程可能不需要重新思考泛化能力",
    "translated_abstract": "在引导的视觉语言模型中进行零样本学习，即通过构建提示来构建分类器而无需明确的训练过程，在许多场景中取得了令人印象深刻的性能。这一成功带来了一个看似令人惊讶的观察：这些方法在过拟合方面相对较少受到影响，即当手动设计提示以在给定的训练集上实现低错误率时（因此实际上不再是零样本学习），该方法在保留的测试数据上仍然表现良好。在本文中，我们展示通过经典的PAC-Bayes边界可以很好地解释这种性能。具体而言，我们展示了提示的离散性质，结合由语言模型给出的PAC-Bayes先验，导致了文献标准下相对紧密的泛化界限：例如，ImageNet分类器的泛化界限通常与真实测试错误率相差几个百分点。我们经验性地证明了...",
    "tldr": "提示工程在零样本学习中取得了令人印象深刻的性能，其成功归因于经典的PAC-Bayes边界将离散的提示特性与由语言模型给出的先验相结合，形成了相对紧密的泛化界限。",
    "en_tdlr": "Prompt engineering has achieved impressive performance in zero-shot learning, and its success can be attributed to the combination of the discrete nature of prompts and a PAC-Bayes prior given by a language model, resulting in remarkably tight generalization bounds."
}