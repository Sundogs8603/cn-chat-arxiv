{
    "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)",
    "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee",
    "link": "http://arxiv.org/abs/2310.14053",
    "context": "Title: Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)\nAbstract: Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee",
    "path": "papers/23/10/2310.14053.json",
    "total_tokens": 844,
    "translated_title": "超越准确性：用IdentityChain评估大型代码语言模型的自一致性",
    "translated_abstract": "代码语言模型(Code LLMs)在实际应用中的使用越来越多，因此对它们进行评估至关重要。传统的准确性评估方法评估Code LLMs在一系列独立任务上的性能，但忽视了其在不同任务上的自一致性。直观来讲，一个可信赖的模型在为其自身的代码生成自然语言规范以及为其自身的规范生成代码时应该是自一致的。未能保持自一致性揭示了对自然语言和编程语言共享语义的理解的不足，从而削弱了模型的可信度。本文首先正式定义了Code LLMs的自一致性，然后设计了一个名为IdentityChain的框架，可以同时有效且高效地评估模型的自一致性和传统准确性。我们研究了11个Code LLMs，并表明它们未能保持自一致性。",
    "tldr": "这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。"
}