{
    "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training. (arXiv:2310.16484v1 [cs.CL])",
    "abstract": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task simil",
    "link": "http://arxiv.org/abs/2310.16484",
    "context": "Title: Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training. (arXiv:2310.16484v1 [cs.CL])\nAbstract: Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task simil",
    "path": "papers/23/10/2310.16484.json",
    "total_tokens": 998,
    "translated_title": "维度空间编年史: 自然语言模型训练过程中的语言信息如何出现、变化和交互",
    "translated_abstract": "通过语言建模学习到的表示空间是自然语言处理(NLP)的基础，然而关于不同类型的语言信息在训练过程中如何以及何时出现和交互的理解还有限。利用一种新颖的信息论探测套件，我们不仅能直接比较任务性能，还能比较它们的表示子空间，我们分析了包括句法、语义和推理在内的九个任务，涵盖了2M个预训练步骤和五个种子。我们确定了任务和时间上的关键学习阶段，在这些阶段，子空间会出现、共享信息，然后分化成特定的子空间。在这些阶段中，句法知识在完成全部训练的0.5%之后很快被获得。持续的性能提升主要来源于对开放领域知识的习得，而语义和推理任务则受益于更高级别的远程上下文和更高层次的特化。我们测量了跨任务的相似度…",
    "tldr": "通过对语言模型训练过程中的表示空间进行分析，研究发现九个任务中的语言信息在不同阶段和时间点逐渐出现、共享和分化。句法知识在训练的早期阶段迅速习得，而后期的性能提升主要来自开放领域知识和长距离上下文理解的提升。语义和推理任务则受益于更高层次的特化。",
    "en_tdlr": "Linguistic information in different tasks gradually emerges, shares and specializes at different stages and time during language model training. Rapid acquisition of syntactic knowledge in early stages, while performance improvements primarily come from open-domain knowledge and enhanced long-range contextualization. Semantics and reasoning tasks benefit from higher specialization."
}