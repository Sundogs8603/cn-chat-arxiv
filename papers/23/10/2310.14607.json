{
    "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
    "abstract": "arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti",
    "link": "https://arxiv.org/abs/2310.14607",
    "context": "Title: Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications\nAbstract: arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti",
    "path": "papers/23/10/2310.14607.json",
    "total_tokens": 784,
    "translated_title": "与传统机器学习对抗：重新思考大型语言模型在表格分类中的公平性",
    "translated_abstract": "最近的文献表明，使用大型语言模型（LLMs）进行表格任务的分类具有潜力。然而，LLMs已被证明存在表现出社会偏见的有害因素，反映了社会中存在的刻板印象和不平等。为此，以及在许多高风险应用中广泛使用表格数据，探讨以下问题至关重要：LLMs在进行表格任务分类时利用了哪些信息源；LLMs对表格数据的分类在多大程度上受到社会偏见和刻板印象的影响；以及这对公平性可能产生的重要影响是什么？通过一系列实验，我们深入探讨这些问题，并表明LLMs倾向于继承来自训练数据的社会偏见，这显著影响了它们在表格分类任务中的公平性。",
    "tldr": "LLMs在表格分类任务中存在社会偏见，影响了它们的公平性。",
    "en_tdlr": "LLMs exhibit social biases in tabular classification tasks, affecting their fairness."
}