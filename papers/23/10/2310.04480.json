{
    "title": "Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)",
    "abstract": "We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.",
    "link": "http://arxiv.org/abs/2310.04480",
    "context": "Title: Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)\nAbstract: We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.",
    "path": "papers/23/10/2310.04480.json",
    "total_tokens": 855,
    "translated_title": "自动调查挑战。（arXiv:2310.04480v2 [cs.CL]已更新）",
    "translated_abstract": "我们提出了一个新颖的平台，用于评估大型语言模型（LLMs）在各个学科领域，包括科学、人文、教育和法律中，自主撰写和评论调查论文的能力。在这个框架内，人工智能系统进行类似于传统学术期刊的模拟同行评审机制，而人类组织者则充当编辑监督角色。在这个框架内，我们组织了一次针对2023年AutoML会议的竞赛。参赛者的任务是呈现能够根据指定提示撰写文章并进行评估的独立模型。评估标准包括清晰度、参考适当性、可追溯性和内容的实质价值。本文介绍了竞赛的设计，包括实施基准提交和评估方法。",
    "tldr": "这项研究提出了一个新的平台，用于评估大型语言模型在撰写和评论调查论文方面的能力，并组织了一次竞赛来测试该平台。评估标准包括清晰度、参考适当性、可追溯性和内容的实质价值。",
    "en_tdlr": "This research proposes a novel platform for evaluating the capability of Large Language Models (LLMs) in composing and critiquing survey papers. It also organizes a competition to assess the performance of stand-alone models in writing and appraising articles, based on criteria such as clarity, reference appropriateness, accountability, and substantive value."
}