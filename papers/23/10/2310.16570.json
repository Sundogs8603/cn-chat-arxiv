{
    "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models. (arXiv:2310.16570v1 [cs.CL])",
    "abstract": "Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.",
    "link": "http://arxiv.org/abs/2310.16570",
    "context": "Title: Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models. (arXiv:2310.16570v1 [cs.CL])\nAbstract: Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.",
    "path": "papers/23/10/2310.16570.json",
    "total_tokens": 798,
    "translated_title": "给我事实！关于预训练语言模型中事实知识探测的调查",
    "translated_abstract": "预训练语言模型(PLMs)在世界知识丰富的无标记数据上进行训练。这一事实引起了社区对于量化PLMs中存在的事实知识量的兴趣，因为这解释了它们在下游任务中的性能，同时可能证明它们作为知识库使用的合理性。在这项工作中，我们调查了用于探测PLMs事实知识的方法和数据集。我们的贡献有：(1) 我们提出了一个基于输入、输出和被探测的PLMs如何适应的分类方案；(2) 我们提供了用于事实探测的数据集概述；(3) 我们综合了关于PLMs中知识保留和提示优化的观点，分析了将PLMs作为知识库应用的障碍，并提出了未来工作的方向。",
    "tldr": "这项工作调查了预训练语言模型中的事实知识探测方法和数据集，并提出了分类方案和未来工作方向。"
}