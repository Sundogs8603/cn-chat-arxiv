{
    "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])",
    "abstract": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.",
    "link": "http://arxiv.org/abs/2310.11564",
    "context": "Title: Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])\nAbstract: While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.",
    "path": "papers/23/10/2310.11564.json",
    "total_tokens": 910,
    "translated_title": "个性化汤：通过事后合并参数进行个性化大型语言模型对齐",
    "translated_abstract": "尽管来自人类反馈的强化学习（RLHF）能够将大型语言模型（LLM）与一般的、综合的人类偏好进行对齐，但对于学习多样化的个体观点来说并不是最优的。在这项工作中，我们研究了从个性化人类反馈中进行强化学习（RLPHF）的问题，其中LLM通过将对齐建模为多目标强化学习（MORL）问题，以与多个（有时相互冲突的）偏好进行对齐。与强单目标基线相比，我们展示了通过将偏好分解为多个维度可以实现个性化对齐。这些维度是基于用户声明为理想的个性化特征进行定义的。在这项工作中，我们展示了它们可以通过分布式训练进行高效独立地训练，并通过参数合并进行事后有效地组合。代码可以在https://github.com/joeljang/RLPHF上获得。",
    "tldr": "本研究研究了将大型语言模型与个性化的人类反馈对齐的问题，通过将对齐建模为多目标强化学习，将偏好分解为多个维度，可以实现个性化对齐，并通过参数合并进行有效组合。"
}