{
    "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation. (arXiv:2310.04064v1 [cs.DS])",
    "abstract": "In the classical transformer attention scheme, we are given three $n \\times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \\times d$ size matrix $D^{-1} \\exp(QK^\\top) V$ where $D = \\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:  $\\bul",
    "link": "http://arxiv.org/abs/2310.04064",
    "context": "Title: How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation. (arXiv:2310.04064v1 [cs.DS])\nAbstract: In the classical transformer attention scheme, we are given three $n \\times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \\times d$ size matrix $D^{-1} \\exp(QK^\\top) V$ where $D = \\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:  $\\bul",
    "path": "papers/23/10/2310.04064.json",
    "total_tokens": 918,
    "translated_title": "如何捕捉高阶相关性？将矩阵Softmax Attention推广到Kronecker计算。",
    "translated_abstract": "在经典的transformer attention方案中，我们给定三个大小为$n \\times d$的矩阵$Q, K, V$（查询、键和值标记），目标是计算一个新的大小为$n \\times d$的矩阵$D^{-1} \\exp(QK^\\top) V$，其中$D = \\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$。在这项工作中，我们研究了一种能够捕捉三元相关性的注意力的泛化。这种泛化能够解决关于检测transformers无法解决的三元连接的问题。这种泛化的潜在缺点是，计算似乎更加困难，因为直接的算法在$n$的立方时间内完成。然而，我们证明在有界输入的情况下（实践中经常出现，并且在理论和实践中都有广泛研究），实际上存在一个近线性时间的算法。更准确地说，我们证明有界输入既是快速执行广义计算的必要条件也是充分条件： $\\bul",
    "tldr": "本文研究了将经典transformer attention泛化到能够捕捉三元相关性的问题，并提出了一个在有界输入下具有近线性时间复杂度的算法。",
    "en_tdlr": "This paper studies the generalization of classical transformer attention to capture triple-wise correlations and proposes an algorithm with near-linear time complexity for bounded inputs."
}