{
    "title": "Multimodal Question Answering for Unified Information Extraction. (arXiv:2310.03017v1 [cs.CL])",
    "abstract": "Multimodal information extraction (MIE) aims to extract structured information from unstructured multimedia content. Due to the diversity of tasks and settings, most current MIE models are task-specific and data-intensive, which limits their generalization to real-world scenarios with diverse task requirements and limited labeled data. To address these issues, we propose a novel multimodal question answering (MQA) framework to unify three MIE tasks by reformulating them into a unified span extraction and multi-choice QA pipeline. Extensive experiments on six datasets show that: 1) Our MQA framework consistently and significantly improves the performances of various off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla prompting. 2) In the zero-shot setting, MQA outperforms previous state-of-the-art baselines by a large margin. In addition, the effectiveness of our framework can successfully transfer to the few-shot setting, enhancing LMMs on a scale of 10B param",
    "link": "http://arxiv.org/abs/2310.03017",
    "context": "Title: Multimodal Question Answering for Unified Information Extraction. (arXiv:2310.03017v1 [cs.CL])\nAbstract: Multimodal information extraction (MIE) aims to extract structured information from unstructured multimedia content. Due to the diversity of tasks and settings, most current MIE models are task-specific and data-intensive, which limits their generalization to real-world scenarios with diverse task requirements and limited labeled data. To address these issues, we propose a novel multimodal question answering (MQA) framework to unify three MIE tasks by reformulating them into a unified span extraction and multi-choice QA pipeline. Extensive experiments on six datasets show that: 1) Our MQA framework consistently and significantly improves the performances of various off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla prompting. 2) In the zero-shot setting, MQA outperforms previous state-of-the-art baselines by a large margin. In addition, the effectiveness of our framework can successfully transfer to the few-shot setting, enhancing LMMs on a scale of 10B param",
    "path": "papers/23/10/2310.03017.json",
    "total_tokens": 984,
    "translated_title": "统一信息提取的多模态问答",
    "translated_abstract": "多模态信息提取旨在从非结构化的多媒体内容中提取结构化信息。由于任务和设置的多样性，大多数现有的多模态信息提取模型都是特定于任务和数据密集型的，这限制了它们在具有各种任务需求和有限标注数据的现实场景中的泛化能力。为了解决这些问题，我们提出了一种新颖的多模态问答（MQA）框架，通过将其重新组合成统一的框架提取回答范围并进行多选题问答。对六个数据集进行的大量实验表明：1）与基准提示相比，我们的MQA框架在多模态信息提取任务上始终显著提高各种现成的大规模多模态模型（LMM）的性能。2）在零样本设置下，MQA比先前的最先进基线模型有很大优势。此外，我们的框架的有效性可以成功地转化为少样本设置，提升了规模为10B参数的LMMs。",
    "tldr": "本研究提出了一种多模态问答（MQA）框架，通过统一三种多模态信息提取任务，通过重新组合成统一的抽取回答范围和多选题问答管道。实验证明，该框架在各种现成的大规模多模态模型上显著提高了性能，并在零样本和少样本设置下超过了最先进的基线模型。"
}