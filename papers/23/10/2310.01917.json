{
    "title": "Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])",
    "abstract": "Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate ",
    "link": "http://arxiv.org/abs/2310.01917",
    "context": "Title: Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])\nAbstract: Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate ",
    "path": "papers/23/10/2310.01917.json",
    "total_tokens": 869,
    "translated_title": "分层评估框架：人工评估的最佳实践",
    "translated_abstract": "人工评估在自然语言处理（NLP）中起着至关重要的作用，它评估了开发系统的质量和相关性，从而促进了系统的改进。然而，在NLP中缺乏广泛接受的人工评估指标，阻碍了不同系统之间的公平比较和建立普遍的评估标准。通过对现有文献中的人工评估指标进行广泛分析，我们发现了NLP评估方法中的几个缺口。这些缺口成为我们开发自己的分层评估框架的动力。所提出的框架具有显著优势，特别是在提供更全面的NLP系统性能表示方面。我们将此框架应用于评估开发的机器阅读理解系统，该系统在人工智能与人类的共生模型中被使用。结果突出了输入与输出质量之间的关联，强调了评估系统性能的必要性。",
    "tldr": "这篇论文提出了一个分层评估框架，解决了自然语言处理中人工评估指标不统一的问题，并应用于机器阅读理解系统的评估，突出了输入与输出质量之间的关联。",
    "en_tdlr": "This paper presents a hierarchical evaluation framework to address the lack of standardized human evaluation metrics in Natural Language Processing (NLP). The framework is applied to evaluate a Machine Reading Comprehension system, highlighting the correlation between input and output quality."
}