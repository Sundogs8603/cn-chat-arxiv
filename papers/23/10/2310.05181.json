{
    "title": "Unified speech and gesture synthesis using flow matching. (arXiv:2310.05181v2 [eess.AS] UPDATED)",
    "abstract": "As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks. Please see https://shivammehta25.g",
    "link": "http://arxiv.org/abs/2310.05181",
    "context": "Title: Unified speech and gesture synthesis using flow matching. (arXiv:2310.05181v2 [eess.AS] UPDATED)\nAbstract: As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks. Please see https://shivammehta25.g",
    "path": "papers/23/10/2310.05181.json",
    "total_tokens": 951,
    "translated_title": "使用流匹配进行统一的语音和手势合成",
    "translated_abstract": "随着文本转语音技术在朗读任务中达到了显著的自然性，人们越来越关注语言和非语言交流行为的多模态合成，例如自发语言和相关的身体手势。本文提出了一种新颖的统一架构，用于从文本中联合合成语音声学和基于骨骼的三维手势运动，训练使用最优传输条件流匹配（OT-CFM）。所提出的架构比先前的最新技术更简单，占用的内存更小，并且能够捕捉语音和手势的联合分布，在一个单一过程中生成两种模态。与以前相比，新的训练机制在更少的步骤（网络评估）中实现了更好的合成质量。单一和多模态的主观测试表明，与现有基准相比，语音自然度、手势人类化和跨模态的适当性都得到了改善。",
    "tldr": "本文提出了一种使用流匹配进行统一的语音和手势合成的架构，相比于先前的技术，它更简单、占用内存更小，并能够同时生成语音和手势模态。新的训练机制在少量步骤中实现了更好的合成质量，并通过主观测试证明了在语音自然度、手势人类化和跨模态适当性方面的改进。",
    "en_tdlr": "This paper proposes a unified architecture for speech and gesture synthesis using flow matching, which is simpler and has a smaller memory footprint compared to previous techniques. The new training regime achieves better synthesis quality in fewer steps and improves speech naturalness, gesture human-likeness, and cross-modal appropriateness."
}