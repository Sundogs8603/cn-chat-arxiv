{
    "title": "Understanding In-Context Learning from Repetitions. (arXiv:2310.00297v2 [cs.CL] UPDATED)",
    "abstract": "This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \\emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.",
    "link": "http://arxiv.org/abs/2310.00297",
    "context": "Title: Understanding In-Context Learning from Repetitions. (arXiv:2310.00297v2 [cs.CL] UPDATED)\nAbstract: This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \\emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.",
    "path": "papers/23/10/2310.00297.json",
    "total_tokens": 794,
    "translated_title": "理解上下文学习中的重复现象",
    "translated_abstract": "本论文探索了大型语言模型（LLM）中上下文学习的难以捉摸的机制。我们通过研究表面重复现象的角度来检视上下文学习，并定量地研究了表面特征在文本生成中的作用，同时实证了一种被称为“标记共现强化”的原则，该原则通过增强两个标记之间的关系来基于它们的上下文共现。通过研究这些特征的双重影响，我们的研究阐明了上下文学习的内在机制，并对其失败的原因进行了解释。本论文对于理解上下文学习及其潜在局限性做出了重要贡献，为这一激动人心的能力提供了新的视角。",
    "tldr": "本论文通过研究表面重复现象的角度来探索大型语言模型中上下文学习的机制，并实证了一种增强标记关系的原则，为理解上下文学习及其潜在局限性做出了重要贡献。"
}