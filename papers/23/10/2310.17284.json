{
    "title": "Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])",
    "abstract": "Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.",
    "link": "http://arxiv.org/abs/2310.17284",
    "context": "Title: Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])\nAbstract: Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.",
    "path": "papers/23/10/2310.17284.json",
    "total_tokens": 895,
    "translated_title": "使用非参数变分信息瓶颈学习摘要",
    "translated_abstract": "在字符、子词、词和句子级别上学习表示在理解不同自然语言处理任务和语言现象方面都有所贡献。然而，学习文本嵌入的成本高昂，因为它们是特定于分词的，并且需要训练不同的模型来处理不同的抽象级别。我们介绍了一种新颖的语言表示模型，可以在同一模型的不同层级上学习不同级别的抽象。我们在编码器的堆叠Transformer自注意层中应用了非参数变分信息瓶颈（NVIB），通过该模型鼓励对表示进行信息论压缩。我们发现模型内的层对应着越来越高级的抽象级别，并且它们的表示更具语言信息。最后，我们证明了NVIB压缩导致的模型对于敌对扰动具有更强的鲁棒性。",
    "tldr": "本论文提出了一种新颖的语言表示模型，可以在同一模型的不同层级上学习不同级别的抽象，并通过非参数变分信息瓶颈实现信息论压缩。这种模型具有更高级的抽象并且更具语言信息，同时对于敌对扰动具有更强的鲁棒性。",
    "en_tdlr": "This paper introduces a novel language representation model that can learn different levels of abstraction at different layers of the same model and achieve information-theoretic compression through nonparametric variational information bottleneck. The model has higher-level abstractions and is more linguistically informed, while being more robust to adversarial perturbations."
}