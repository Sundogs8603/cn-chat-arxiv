{
    "title": "ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])",
    "abstract": "Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.",
    "link": "http://arxiv.org/abs/2310.17342",
    "context": "Title: ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])\nAbstract: Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.",
    "path": "papers/23/10/2310.17342.json",
    "total_tokens": 924,
    "translated_title": "ACT-SQL: 基于上下文学习的自动生成链式思维的文本到SQL技术",
    "translated_abstract": "最近，大型语言模型（LLMs）在各个领域和任务中已被证明具有很强的能力。我们研究文本到SQL任务中提示设计的问题，并尝试提高LLMs在生成SQL查询时的推理能力。除了传统的少样本上下文学习设置外，我们采用类似于模式链接的方法设计了我们的链式思维（CoT）提示。我们提供了一种名为ACT-SQL的方法来自动生成自动CoT示例，因此整个过程不需要手动标记。我们的方法具有成本节省，因为在生成一个SQL查询时，我们只使用LLMs的API调用一次。此外，我们将我们的上下文学习方法扩展到多轮文本到SQL任务中。实验结果表明，LLMs的性能可以受益于我们的ACT-SQL方法。我们的方法在现有的上下文学习方法中，在Spider开发集上达到了最佳性能。",
    "tldr": "ACT-SQL是一种基于上下文学习的自动生成链式思维的文本到SQL技术，在文本到SQL任务中设计了类似模式链接的链式思维提示，通过自动生成示例实现了成本节省。实验结果表明ACT-SQL方法在Spider开发集上实现了最佳性能。"
}