{
    "title": "Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models. (arXiv:2310.13315v1 [cs.CL])",
    "abstract": "Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Ext",
    "link": "http://arxiv.org/abs/2310.13315",
    "context": "Title: Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models. (arXiv:2310.13315v1 [cs.CL])\nAbstract: Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Ext",
    "path": "papers/23/10/2310.13315.json",
    "total_tokens": 910,
    "translated_title": "零样本锐度感知量化预训练语言模型",
    "translated_abstract": "量化是一种减少内存开销和加速推断的有前景的方法，尤其在大规模预训练语言模型（PLM）场景下。由于安全和隐私问题，无法访问原始训练数据，因此对零样本量化的需求逐渐增加。大部分最新的零样本量化方法主要适用于计算机视觉任务，并忽视了生成对抗学习过程中过拟合问题，导致性能不佳。受此启发，我们提出了一种新颖的针对各种PLM的零样本锐度感知量化（ZSAQ）框架。解决ZSAQ的关键算法是SAM-SGA优化，旨在通过优化极小极大问题来提高量化准确性和模型泛化能力。我们在理论上证明了极小极大优化问题的收敛速度，并且这个结果可以应用于其他非凸PL极小极大优化框架。",
    "tldr": "本论文提出了一种新的零样本锐度感知量化（ZSAQ）框架，用于解决大规模预训练语言模型中的量化问题。通过优化极小极大问题，该框架可以提高量化准确性和模型泛化能力。",
    "en_tdlr": "This paper proposes a novel zero-shot sharpness-aware quantization (ZSAQ) framework for addressing the quantization problem in large pre-trained language models. By optimizing a minimax problem, the framework improves quantization accuracy and model generalization."
}