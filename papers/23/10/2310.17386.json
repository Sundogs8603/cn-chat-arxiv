{
    "title": "A Challenge in Reweighting Data with Bilevel Optimization. (arXiv:2310.17386v1 [stat.ML])",
    "abstract": "In many scenarios, one uses a large training set to train a model with the goal of performing well on a smaller testing set with a different distribution. Learning a weight for each data point of the training set is an appealing solution, as it ideally allows one to automatically learn the importance of each training point for generalization on the testing set. This task is usually formalized as a bilevel optimization problem. Classical bilevel solvers are based on a warm-start strategy where both the parameters of the models and the data weights are learned at the same time. We show that this joint dynamic may lead to sub-optimal solutions, for which the final data weights are very sparse. This finding illustrates the difficulty of data reweighting and offers a clue as to why this method is rarely used in practice.",
    "link": "http://arxiv.org/abs/2310.17386",
    "context": "Title: A Challenge in Reweighting Data with Bilevel Optimization. (arXiv:2310.17386v1 [stat.ML])\nAbstract: In many scenarios, one uses a large training set to train a model with the goal of performing well on a smaller testing set with a different distribution. Learning a weight for each data point of the training set is an appealing solution, as it ideally allows one to automatically learn the importance of each training point for generalization on the testing set. This task is usually formalized as a bilevel optimization problem. Classical bilevel solvers are based on a warm-start strategy where both the parameters of the models and the data weights are learned at the same time. We show that this joint dynamic may lead to sub-optimal solutions, for which the final data weights are very sparse. This finding illustrates the difficulty of data reweighting and offers a clue as to why this method is rarely used in practice.",
    "path": "papers/23/10/2310.17386.json",
    "total_tokens": 801,
    "translated_title": "在使用双层优化中重新加权数据的挑战",
    "translated_abstract": "在许多情况下，我们使用一个大的训练集来训练模型，目标是在一个不同分布的较小测试集上获得良好的性能。为训练集中的每个数据点学习一个权重是一个有吸引力的解决方案，因为理想情况下它可以自动学习每个训练点在测试集上的泛化重要性。这个任务通常被形式化为一个双层优化问题。传统的双层求解器基于一个热启动策略，即同时学习模型的参数和数据权重。我们展示了这种联合动态可能导致次优解，其中最终的数据权重非常稀疏。这一发现说明了数据重新加权的困难，并提示了为什么这种方法在实践中很少被使用的原因。",
    "tldr": "在重新加权数据的任务中，经典的双层优化方法可能会导致次优解，使得最终的数据权重非常稀疏，这解释了为什么这种方法在实践中很少被使用。"
}