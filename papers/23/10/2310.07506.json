{
    "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])",
    "abstract": "Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea",
    "link": "http://arxiv.org/abs/2310.07506",
    "context": "Title: Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])\nAbstract: Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea",
    "path": "papers/23/10/2310.07506.json",
    "total_tokens": 863,
    "translated_title": "利用分层特征共享进行高效数据集压缩",
    "translated_abstract": "在真实世界数据集中，数据压缩（DC）旨在合成一个显著较小的数据集，以高性能进行模型训练。最近的研究提出使用数据参数化增强DC，将数据压缩为参数化的数据容器而不是像素空间。数据参数化的直觉是编码图像的共享特征，以避免额外的存储成本。本文认识到由于分类系统的内在分层结构，图像以分层方式共享共同的特征，这是当前数据参数化方法所忽视的。为了更好地使DC与这种分层性质对齐，并在数据容器内部鼓励更高效的信息共享，我们提出了一种新颖的数据参数化架构，分层记忆网络（HMN）。HMN将压缩数据存储在三层结构中，表示数据集级别、类别级别和样本级别的特征。",
    "tldr": "本文提出了一种利用分层特征共享的数据参数化架构（HMN），旨在更高效地压缩数据。通过将数据存储在三层结构中，HMN能够捕捉到数据集级别、类别级别和样本级别的特征。",
    "en_tdlr": "This paper proposes a data parameterization architecture (HMN) that leverages hierarchical feature sharing to efficiently condense data. By storing data in a three-tier structure, HMN captures features at the dataset-level, class-level, and instance-level."
}