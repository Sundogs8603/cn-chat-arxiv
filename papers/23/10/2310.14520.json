{
    "title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing. (arXiv:2310.14520v2 [cs.CL] UPDATED)",
    "abstract": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored hi",
    "link": "http://arxiv.org/abs/2310.14520",
    "context": "Title: QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing. (arXiv:2310.14520v2 [cs.CL] UPDATED)\nAbstract: Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored hi",
    "path": "papers/23/10/2310.14520.json",
    "total_tokens": 950,
    "translated_title": "QUDEVAL：问句讨论的语篇解析评估",
    "translated_abstract": "问句讨论（QUD）是一个多功能的语言框架，在其中，语篇通过不断提问和回答而进行。将语篇进行自动解析以生成QUD结构，因此涉及到一个复杂的问题生成任务：给定一个文档和一个回答句子，生成满足QUD语言约束并可以在先前环境中绑定到一个锚定句子的问题。这些问题被称为好奇心驱动和开放性的。这项工作引入了第一个用于自动评估QUD解析的框架，将QUD的理论约束具体化为一个具体的协议。我们提出了QUDeval，一个对来自调优系统和LLMs的2,190个QUD问题进行细粒度评估的数据集。使用QUDeval，我们展示了满足所有QUD约束对于现代LLMs仍然具有挑战性，并且现有的评估指标很差地近似解析器质量。令人鼓舞的是，人工编写的QUD得分较高。",
    "tldr": "本文介绍了第一个用于自动评估问句讨论（QUD）语篇解析的框架QUDeval。使用QUDeval数据集，展示了现代语言模型（LLMs）仍然面临解析所有QUD约束的挑战，并且现有的评估指标很差地近似解析器质量。"
}