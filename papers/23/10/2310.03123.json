{
    "title": "Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])",
    "abstract": "With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins",
    "link": "http://arxiv.org/abs/2310.03123",
    "context": "Title: Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])\nAbstract: With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins",
    "path": "papers/23/10/2310.03123.json",
    "total_tokens": 950,
    "translated_title": "高效的黑盒大型预训练模型联合提示调整",
    "translated_abstract": "随着预训练模型（PTMs）的迅猛发展，对这些模型进行高效调整以适用于不同的下游应用已成为一个关键的研究关注点。尽管最近关于提示调整的研究提供了有希望的途径，但仍然存在三个突出的挑战：（1）内存限制：开源PTMs大小的持续增长使得即使对其参数的一小部分进行微调也对许多从业者来说是具有挑战性的。（2）模型隐私性：现有的PTMs通常作为公共API服务，其参数无法有效或定制地进行微调。（3）数据隐私性：对PTMs进行微调需要高质量的数据集，这些数据集通常是局部化的并且不共享给公众。为了在处理内存限制和保持隐私性的同时充分利用每个局部数据集，我们提出了联合黑盒提示调整（Fed-BBPT）。这种创新方法摒弃了对参数结构和私有数据集访问的依赖，可以有效地解决这些挑战。",
    "tldr": "这项研究提出了一种高效的黑盒大型预训练模型联合提示调整（Fed-BBPT）方法，通过摒弃对参数结构和私有数据集访问的依赖，可以在处理内存限制和保持隐私性的同时充分利用每个局部数据集。"
}