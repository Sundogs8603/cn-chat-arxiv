{
    "title": "Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])",
    "abstract": "Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\\widetilde{\\mathbf{O}}\\left({\\epsilon^{-2}{\\mu^*}^{-2}\\log^2\\left({\\epsilon^{-1}}\\right)}\\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\\widetilde{\\mathbf{O}}\\left({{\\epsilon}^{-4}{\\lambda^{*}}^{-1}\\log^5\\left({\\epsilon^{-1}}\\right)}\\right)$). Moreo",
    "link": "http://arxiv.org/abs/2310.16320",
    "context": "Title: Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])\nAbstract: Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\\widetilde{\\mathbf{O}}\\left({\\epsilon^{-2}{\\mu^*}^{-2}\\log^2\\left({\\epsilon^{-1}}\\right)}\\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\\widetilde{\\mathbf{O}}\\left({{\\epsilon}^{-4}{\\lambda^{*}}^{-1}\\log^5\\left({\\epsilon^{-1}}\\right)}\\right)$). Moreo",
    "path": "papers/23/10/2310.16320.json",
    "total_tokens": 1089,
    "translated_title": "增强低精度采样：随机梯度Hamiltonian Monte Carlo",
    "translated_abstract": "低精度训练已经成为一种有前景的低成本技术，可以在不牺牲太多准确性的情况下提高深度神经网络的训练效率。其贝叶斯对应物可以进一步提供不确定性量化和改进的泛化准确性。本文研究了在强对数凹和非对数凹分布下，使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)。从理论上讲，我们的结果表明，为了在非对数凹分布下实现2-Wasserstein距离的ε误差，低精度SGHMC相对于低精度采样器（随机梯度Langevin动力学，SGLD）实现了二次改进（$\\widetilde{\\mathbf{O}}\\left({\\epsilon^{-2}{\\mu^*}^{-2}\\log^2\\left({\\epsilon^{-1}}\\right)}\\right)$ vs $\\widetilde{\\mathbf{O}}\\left({{\\epsilon}^{-4}{\\lambda^{*}}^{-1}\\log^5\\left({\\epsilon^{-1}}\\right)}\\right)$）。另外，基于真实数据集的实验证明了低精度SGHMC相对于SGLD在非对数凹分布下的优越性。",
    "tldr": "本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。",
    "en_tdlr": "This paper investigates the application of Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators in low-precision sampling. Experimental results demonstrate a quadratic improvement of low-precision SGHMC compared to the state-of-the-art low-precision sampler (SGLD) in non-log-concave distributions."
}