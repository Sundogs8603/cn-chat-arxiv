{
    "title": "Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:2310.08823v1 [cs.LG])",
    "abstract": "Inverse reinforcement learning (IRL) aims to explicitly infer an underlying reward function based on collected expert demonstrations. Considering that obtaining expert demonstrations can be costly, the focus of current IRL techniques is on learning a better-than-demonstrator policy using a reward function derived from sub-optimal demonstrations. However, existing IRL algorithms primarily tackle the challenge of trajectory ranking ambiguity when learning the reward function. They overlook the crucial role of considering the degree of difference between trajectories in terms of their returns, which is essential for further removing reward ambiguity. Additionally, it is important to note that the reward of a single transition is heavily influenced by the context information within the trajectory. To address these issues, we introduce the Distance-rank Aware Sequential Reward Learning (DRASRL) framework. Unlike existing approaches, DRASRL takes into account both the ranking of trajectories",
    "link": "http://arxiv.org/abs/2310.08823",
    "context": "Title: Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:2310.08823v1 [cs.LG])\nAbstract: Inverse reinforcement learning (IRL) aims to explicitly infer an underlying reward function based on collected expert demonstrations. Considering that obtaining expert demonstrations can be costly, the focus of current IRL techniques is on learning a better-than-demonstrator policy using a reward function derived from sub-optimal demonstrations. However, existing IRL algorithms primarily tackle the challenge of trajectory ranking ambiguity when learning the reward function. They overlook the crucial role of considering the degree of difference between trajectories in terms of their returns, which is essential for further removing reward ambiguity. Additionally, it is important to note that the reward of a single transition is heavily influenced by the context information within the trajectory. To address these issues, we introduce the Distance-rank Aware Sequential Reward Learning (DRASRL) framework. Unlike existing approaches, DRASRL takes into account both the ranking of trajectories",
    "path": "papers/23/10/2310.08823.json",
    "total_tokens": 870,
    "translated_title": "Distance-rank感知顺序奖励学习用于具有次优演示的逆强化学习",
    "translated_abstract": "逆强化学习（IRL）旨在基于收集到的专家演示明确推断出潜在的奖励函数。考虑到获取专家演示可能是昂贵的，当前IRL技术的重点是使用从次优演示中导出的奖励函数学习一个优于演示者的策略。然而，现有的IRL算法主要解决了在学习奖励函数时的轨迹排名模糊的挑战，却忽视了在进一步消除奖励模糊性方面，考虑轨迹之间的回报差异程度的关键作用。此外，需要注意的是，单个转换的奖励受到轨迹中的上下文信息的重要影响。为了解决这些问题，我们引入了Distance-rank感知顺序奖励学习（DRASRL）框架。与现有方法不同，DRASRL同时考虑了轨迹的排名和回报之间的差异度。",
    "tldr": "提出了Distance-rank感知顺序奖励学习（DRASRL）框架，旨在解决逆强化学习中轨迹排名模糊和奖励模糊的问题。",
    "en_tdlr": "A Distance-rank Aware Sequential Reward Learning (DRASRL) framework is proposed to address the issues of trajectory ranking ambiguity and reward ambiguity in inverse reinforcement learning."
}