{
    "title": "MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training",
    "abstract": "arXiv:2310.00967v3 Announce Type: replace  Abstract: Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.   To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimisi",
    "link": "https://arxiv.org/abs/2310.00967",
    "context": "Title: MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training\nAbstract: arXiv:2310.00967v3 Announce Type: replace  Abstract: Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.   To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimisi",
    "path": "papers/23/10/2310.00967.json",
    "total_tokens": 744,
    "translated_title": "MiCRO：用于扩展和加速分布式DNN训练的近零成本梯度稀疏化技术",
    "translated_abstract": "梯度稀疏化是一种用于扩展和加速分布式深度神经网络（DNN）训练的通信优化技术，它减少了梯度聚合的通信流量增加。然而，现有的稀疏化方法由于梯度选择的高计算成本和/或通信流量增加而具有较差的可扩展性。为了解决这些挑战，我们提出了一种名为MiCRO的新型梯度稀疏化方法。",
    "tldr": "MiCRO是一种新颖的梯度稀疏化方法，通过对梯度向量进行分区，并由每个工作者选择其分区中的梯度，从而减少了通信流量并避免了梯度堆积。",
    "en_tdlr": "MiCRO is a novel gradient sparsification method that reduces communication traffic and prevents gradient build-up by partitioning the gradient vector and allowing each worker to select gradients from its partition."
}