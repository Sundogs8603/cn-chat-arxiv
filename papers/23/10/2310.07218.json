{
    "title": "Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization. (arXiv:2310.07218v1 [cs.MA])",
    "abstract": "Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocatio",
    "link": "http://arxiv.org/abs/2310.07218",
    "context": "Title: Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization. (arXiv:2310.07218v1 [cs.MA])\nAbstract: Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocatio",
    "path": "papers/23/10/2310.07218.json",
    "total_tokens": 939,
    "translated_title": "在多智能体强化学习中量化智能体相互作用以实现成本效益的泛化",
    "translated_abstract": "泛化是多智能体强化学习中的一个重要挑战。一个智能体受未知合作智能体的影响程度取决于该智能体的策略和具体场景。对这种关系的定量研究有助于有效培训适用于多样化场景的智能体。在本研究中，我们提出了影响水平（Level of Influence，LoI），一种度量给定场景和环境中智能体之间交互强度的指标。我们观察到，一般来说，在训练过程中使用更多样化的合作智能体可以提高自我智能体的泛化性能；然而，这种改进因不同场景和环境而异。LoI在预测特定场景中这些改进差异方面表现出了有效性。此外，我们引入了一种以LoI为指导的资源分配方法，针对有限预算训练适用于多样化场景的策略集合。我们的结果表明，战略性资源分配可以明显提高智能体的泛化性能。",
    "tldr": "本研究针对多智能体强化学习中的泛化问题，提出了一种量化智能体相互作用的指标，并通过该指标设计了资源分配方法，可以在有限预算下训练适用于多样化场景的智能体策略集合。"
}