{
    "title": "Disentangling Voice and Content with Self-Supervision for Speaker Recognition. (arXiv:2310.01128v2 [eess.AS] UPDATED)",
    "abstract": "For speaker recognition, it is difficult to extract an accurate speaker representation from speech because of its mixture of speaker traits and content. This paper proposes a disentanglement framework that simultaneously models speaker traits and content variability in speech. It is realized with the use of three Gaussian inference layers, each consisting of a learnable transition model that extracts distinct speech components. Notably, a strengthened transition model is specifically designed to model complex speech dynamics. We also propose a self-supervision method to dynamically disentangle content without the use of labels other than speaker identities. The efficacy of the proposed framework is validated via experiments conducted on the VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and minDCF, respectively. Since neither additional model training nor data is specifically needed, it is easily applicable in practical use.",
    "link": "http://arxiv.org/abs/2310.01128",
    "context": "Title: Disentangling Voice and Content with Self-Supervision for Speaker Recognition. (arXiv:2310.01128v2 [eess.AS] UPDATED)\nAbstract: For speaker recognition, it is difficult to extract an accurate speaker representation from speech because of its mixture of speaker traits and content. This paper proposes a disentanglement framework that simultaneously models speaker traits and content variability in speech. It is realized with the use of three Gaussian inference layers, each consisting of a learnable transition model that extracts distinct speech components. Notably, a strengthened transition model is specifically designed to model complex speech dynamics. We also propose a self-supervision method to dynamically disentangle content without the use of labels other than speaker identities. The efficacy of the proposed framework is validated via experiments conducted on the VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and minDCF, respectively. Since neither additional model training nor data is specifically needed, it is easily applicable in practical use.",
    "path": "papers/23/10/2310.01128.json",
    "total_tokens": 882,
    "translated_title": "利用自监督方法解离发音和内容，用于说话人识别",
    "translated_abstract": "对于说话人识别来说，由于语音中混合了说话人特征和内容，提取准确的说话人表示是困难的。本文提出了一种解离框架，可以同时模型化语音中的说话人特征和内容的变异性。该框架使用三个高斯推理层实现，每个推理层都由可学习的转换模型组成，用于提取不同的语音成分。值得注意的是，为了建模复杂的语音动态，专门设计了一个强化的转换模型。我们还提出了一种自监督方法，用于在没有除说话人身份之外的标签的情况下动态解离内容。通过在VoxCeleb和SITW数据集上进行的实验证明了所提框架的有效性，EER和minDCF分别平均降低了9.56%和8.24%。由于不需要额外的模型训练和数据，因此可以方便地应用于实际使用。",
    "tldr": "本研究提出了一种利用自监督方法解离语音中的发音和内容的框架，用于说话人识别。实验证明该方法在VoxCeleb和SITW数据集上对EER和minDCF有明显的降低。",
    "en_tdlr": "This study proposes a framework that uses self-supervision to disentangle voice and content in speech for speaker recognition. Experiments show significant reductions in EER and minDCF on the VoxCeleb and SITW datasets."
}