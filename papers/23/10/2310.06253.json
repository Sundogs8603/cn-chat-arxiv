{
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])",
    "abstract": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \\emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.",
    "link": "http://arxiv.org/abs/2310.06253",
    "context": "Title: A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])\nAbstract: Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \\emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.",
    "path": "papers/23/10/2310.06253.json",
    "total_tokens": 898,
    "translated_title": "面向模型驱动的强化学习中目标不匹配问题的统一观点",
    "translated_abstract": "模型驱动的强化学习（MBRL）旨在通过学习环境的显式模型使代理更节约样本、适应性更强和更易解释。虽然近年来MBRL代理的能力有了显著提升，但如何最好地学习模型仍然是一个未解决的问题。大多数MBRL算法的目标是训练模型以对环境进行准确预测，然后使用模型确定最有益的动作。然而，最近的研究表明，模型的预测准确性通常与动作质量不相关，将根本原因归结为准确的动态模型学习与奖励策略优化之间的“目标不匹配”。随着MBRL作为一个研究领域的不断成熟，涌现出了一些互相关联的解决目标不匹配问题的解决方案类别。在本文中，我们对这些解决方案类别进行了深入调查，并提出了一个分类法以促进未来的研究。",
    "tldr": "这项工作提供了一个关于解决模型驱动的强化学习中目标不匹配问题的统一观点，对解决方案进行了分类，并提出了一个分类法以促进未来的研究。",
    "en_tdlr": "This work provides a unified view on solving the objective mismatch problem in model-based reinforcement learning, categorizes the solutions, and proposes a taxonomy to foster future research."
}