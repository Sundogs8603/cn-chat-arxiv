{
    "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])",
    "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a ma",
    "link": "http://arxiv.org/abs/2310.10873",
    "context": "Title: IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])\nAbstract: In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a ma",
    "path": "papers/23/10/2310.10873.json",
    "total_tokens": 900,
    "translated_title": "IDEAL: 强化大型语言模型中上下文学习的影响驱动选择性注释方法",
    "translated_abstract": "上下文学习是一种有前景的范式，它利用上下文示例作为大型语言模型预测的提示。这些提示对于获得强大的性能至关重要。然而，由于这些提示需要从大量注释的示例中进行采样，找到正确的提示可能导致高昂的注释成本。为解决这一挑战，本文引入了一种基于影响驱动的选择性注释方法，旨在在改善上下文示例质量的同时最大程度地降低注释成本。我们的方法的核心是从大规模未标记的数据池中选择一个关键子集进行注释，以用于后续的提示采样。具体地，首先构建一个有向图来表示未标记的数据，然后利用扩散过程量化候选未标记子集的影响力，最后引入一个简单又有效的贪心算法来选择未标记的数据。如果数据提供了最大的影响力，算法就会迭代地选择这些数据。",
    "tldr": "本文提出了一种影响驱动的选择性注释方法，用于在大型语言模型中改善上下文学习。该方法通过选择关键的未标记数据子集进行注释，在降低注释成本的同时提高了上下文示例的质量。"
}