{
    "title": "A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])",
    "abstract": "Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. t",
    "link": "http://arxiv.org/abs/2310.16656",
    "context": "Title: A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])\nAbstract: Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. t",
    "path": "papers/23/10/2310.16656.json",
    "total_tokens": 955,
    "translated_title": "一张图片胜过千言万语：原则性重写改善图像生成",
    "translated_abstract": "文本到图像扩散模型在过去几年中取得了显著的突破，使得从文本提示中高质量且多样化地合成图像成为可能。然而，即使是最先进的模型也常常难以准确地遵循其提示中的所有指令。这些模型中绝大部分是在由（图像，字幕）对组成的数据集上进行训练的，其中图像通常来自网络，而字幕则是它们的HTML替代文本。一个显著的例子是LAION数据集，被Stable Diffusion和其他模型使用。在这项工作中，我们观察到这些字幕通常质量较低，并认为这显著影响了模型理解文本提示中微妙语义的能力。我们展示通过使用专门的自动字幕模型重新标注语料库，并在重写后的数据集上训练文本到图像模型，模型在各个方面都会得到大幅度的改善。首先，在整体图像质量方面：例如FID 14.84 vs. t",
    "tldr": "本文提出了一种原则性重写方法来改善图像生成模型的效果，通过使用专门的自动字幕模型重新标注语料库，并在重写后的数据集上训练文本到图像模型，模型在整体图像质量方面得到了显著改善。"
}