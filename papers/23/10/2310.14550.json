{
    "title": "Corruption-Robust Offline Reinforcement Learning with General Function Approximation",
    "abstract": "arXiv:2310.14550v3 Announce Type: replace  Abstract: We investigate the problem of corruption robustness in offline reinforcement learning (RL) with general function approximation, where an adversary can corrupt each sample in the offline dataset, and the corruption level $\\zeta\\geq0$ quantifies the cumulative corruption amount over $n$ episodes and $H$ steps. Our goal is to find a policy that is robust to such corruption and minimizes the suboptimality gap with respect to the optimal policy for the uncorrupted Markov decision processes (MDPs). Drawing inspiration from the uncertainty-weighting technique from the robust online RL setting \\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight iteration procedure to efficiently compute on batched samples and propose a corruption-robust algorithm for offline RL. Notably, under the assumption of single policy coverage and the knowledge of $\\zeta$, our proposed algorithm achieves a suboptimality bound that is worsen",
    "link": "https://arxiv.org/abs/2310.14550",
    "context": "Title: Corruption-Robust Offline Reinforcement Learning with General Function Approximation\nAbstract: arXiv:2310.14550v3 Announce Type: replace  Abstract: We investigate the problem of corruption robustness in offline reinforcement learning (RL) with general function approximation, where an adversary can corrupt each sample in the offline dataset, and the corruption level $\\zeta\\geq0$ quantifies the cumulative corruption amount over $n$ episodes and $H$ steps. Our goal is to find a policy that is robust to such corruption and minimizes the suboptimality gap with respect to the optimal policy for the uncorrupted Markov decision processes (MDPs). Drawing inspiration from the uncertainty-weighting technique from the robust online RL setting \\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight iteration procedure to efficiently compute on batched samples and propose a corruption-robust algorithm for offline RL. Notably, under the assumption of single policy coverage and the knowledge of $\\zeta$, our proposed algorithm achieves a suboptimality bound that is worsen",
    "path": "papers/23/10/2310.14550.json",
    "total_tokens": 922,
    "translated_title": "具有通用函数逼近的耐腐败离线强化学习",
    "translated_abstract": "我们研究了具有通用函数逼近的耐腐败离线强化学习中的腐败鲁棒性问题，其中对手可以破坏离线数据集中的每个样本，而腐败水平 $\\zeta\\geq0$ 量化了 $n$ 个周期和 $H$ 步中的累积破坏量。我们的目标是找到一种对此类破坏具有鲁棒性并最小化相对于未受损的马尔可夫决策过程（MDP）最优策略的次优差距的策略。受到鲁棒在线强化学习环境中的不确定性加权技术的启发，我们设计了一种新的不确定性加权迭代过程，可以高效地在批量样本上计算，并提出了一种耐腐败的离线强化学习算法。值得注意的是，在单策略覆盖和 $\\zeta$ 知识假设的前提下，我们提出的算法达到了一个次优性界，该界是恶化的",
    "tldr": "设计了一种新的不确定性加权迭代过程以提高耐腐败的离线强化学习算法的效率，并在单策略覆盖和已知腐败水平的情况下实现了次优性界。"
}