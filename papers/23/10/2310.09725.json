{
    "title": "KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models. (arXiv:2310.09725v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs o",
    "link": "http://arxiv.org/abs/2310.09725",
    "context": "Title: KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models. (arXiv:2310.09725v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs o",
    "path": "papers/23/10/2310.09725.json",
    "total_tokens": 868,
    "translated_title": "KGQuiz: 评估大型语言模型中编码知识的泛化能力",
    "translated_abstract": "大型语言模型(LLMs)在知识密集型任务上表现出色，这表明真实世界的知识被编码在它们的模型参数中。然而，除了在有限的知识领域上进行一些探索性任务之外，我们对于如何系统评估LLMs的知识能力以及它们的知识能力在不同领域和逐渐复杂的任务格式中的泛化效果并不了解。为了解决这个问题，我们提出了KGQuiz，一个知识密集型基准测试，全面调查LLMs的知识泛化能力。KGQuiz是一个可扩展的框架，由基于三元组的知识构建，涵盖了三个知识领域，并包括五个任务，难度递增：真假判断、多项选择问题、填空、事实编辑和开放式知识生成。为了更好地理解LLMs的知识能力和它们的泛化效果，我们评估了10种开源和黑盒LLMs。",
    "tldr": "KGQuiz是一个知识密集型基准测试，通过涵盖三个知识领域和五个任务，全面评估了大型语言模型(LLMs)的知识泛化能力。",
    "en_tdlr": "KGQuiz is a knowledge-intensive benchmark that comprehensively evaluates the knowledge generalization abilities of large language models (LLMs) by covering three knowledge domains and five tasks."
}