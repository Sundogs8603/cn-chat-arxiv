{
    "title": "Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning",
    "abstract": "arXiv:2310.05723v2 Announce Type: replace  Abstract: Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modif",
    "link": "https://arxiv.org/abs/2310.05723",
    "context": "Title: Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning\nAbstract: arXiv:2310.05723v2 Announce Type: replace  Abstract: Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modif",
    "path": "papers/23/10/2310.05723.json",
    "total_tokens": 841,
    "translated_title": "在线到离线强化学习中的超领域规划",
    "translated_abstract": "离线预训练配合静态数据集，然后在线微调（离线到在线，即OtO）是一个与现实世界RL部署过程很匹配的范式。在这种情况下，我们的目标是在有限的在线交互预算内找到性能最佳的策略。以前在OtO设置中的工作集中在纠正由离线RL算法的策略约束机制引入的偏差上。这些约束使得学习的策略接近收集数据集的行为策略，但我们表明，如果行为策略远非最优，则这可能会不必要地限制策略性能。相反，我们放弃约束，把OtO RL作为一个探索问题来框定，旨在最大化在线数据采集的好处。我们首先研究了基于内在奖励和UCB的主要在线RL探索方法在OtO设置中的效果，显示内在奖励通过奖励函数的修改增加了训练的不稳定性。",
    "tldr": "将在线到离线强化学习框定为一个探索问题，并研究了内在奖励和UCB在该设置中的效果。",
    "en_tdlr": "Framing offline-to-online reinforcement learning as an exploration problem and studying the effects of intrinsic rewards and UCB in this setting."
}