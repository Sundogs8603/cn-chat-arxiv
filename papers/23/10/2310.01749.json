{
    "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])",
    "abstract": "Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more",
    "link": "http://arxiv.org/abs/2310.01749",
    "context": "Title: Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])\nAbstract: Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more",
    "path": "papers/23/10/2310.01749.json",
    "total_tokens": 907,
    "translated_title": "堆栈注意力: 提升Transformers对层次模式建模的能力",
    "translated_abstract": "注意力机制，尤其是缩放点积注意力，在自然语言处理中已被证明非常有效，但它对于处理任意嵌套深度的层次模式没有机制，这限制了它识别某些句法结构的能力。为解决这一缺陷，我们提出了堆栈注意力：一种结合了堆栈的注意力操作符，受到它们与上下文无关语言（CFLs）的理论联系的启发。我们展示了堆栈注意力类似于标准注意力，但它具有不需要句法监督的语法潜在模型。我们提出了两种变体：与确定性下推自动机（PDAs）相关的一种，以及基于非确定性PDAs的一种，这使得transformers能够识别任意的CFLs。我们展示了使用堆栈注意力的transformers在学习标准transformers难以应对的CFLs方面非常有效，并在最大解析难度的CFL上取得了强大的结果。我们还展示了堆栈注意力的效果更好。",
    "tldr": "堆栈注意力为Transformers模型处理层次模式提供了能力，通过结合堆栈和注意力机制，它能有效地学习和识别任意深度的语法结构，特别适用于具有解析难度的上下文无关语言。"
}