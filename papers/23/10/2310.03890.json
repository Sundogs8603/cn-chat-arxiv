{
    "title": "Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])",
    "abstract": "Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-ar",
    "link": "http://arxiv.org/abs/2310.03890",
    "context": "Title: Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])\nAbstract: Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-ar",
    "path": "papers/23/10/2310.03890.json",
    "total_tokens": 889,
    "translated_title": "基于根化逻辑目标函数的加速神经网络训练",
    "translated_abstract": "许多在实际场景中部署的神经网络是使用基于交叉熵的损失函数进行训练的。从优化的角度来看，我们知道一阶方法（如梯度下降）的行为在很大程度上取决于数据集的可分性。事实上，即使在最简单的二分类情况下，收敛速度取决于两个因素：（1）数据矩阵的条件数，和（2）数据集的可分性。在没有进一步预处理技术（如超参数化、数据增强等）的情况下，可分性是所考虑的数据分布固有的量。我们专注于逻辑函数的优化，并推导出一系列新颖的严格凸函数，这些函数至少和逻辑损失一样严格。这些函数的最小化点与最小范数解的最小化点一致，以尽可能使用。这个推导的函数的严格凸性可以扩展到微调最先进的模型时。",
    "tldr": "该论文基于根化逻辑目标函数，设计了一种加速神经网络训练的方法，通过推导出一系列严格凸函数，实现了与最小范数解相同的最小化点，从而提高了最优结果的达成速度。",
    "en_tdlr": "This paper introduces a method for accelerating neural network training using rooted logistic objectives. By deriving a series of strictly convex functions that have the same minimizers as the minimum norm solution, the paper improves the convergence rate and achieves optimal results."
}