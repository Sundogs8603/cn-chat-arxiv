{
    "title": "A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])",
    "abstract": "Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and \"LogiQAv2-plus\", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by \"none of the other options are correct\", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb",
    "link": "http://arxiv.org/abs/2310.09430",
    "context": "Title: A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])\nAbstract: Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named \"ReClor-plus\", \"LogiQA-plus\" and \"LogiQAv2-plus\", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by \"none of the other options are correct\", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb",
    "path": "papers/23/10/2310.09430.json",
    "total_tokens": 982,
    "translated_title": "对大型语言模型在非分布式逻辑推理任务上的系统评估",
    "translated_abstract": "大型语言模型（LLMs），如GPT-3.5和GPT-4，已经将人工系统在各种自然语言处理任务上的性能提升到接近人类水平。然而，它们在逻辑推理方面的泛化和鲁棒性仍未得到充分评估。为了探索这种能力，我们提出了三个新的逻辑推理数据集，分别名为\"ReClor-plus\"、\"LogiQA-plus\"和\"LogiQAv2-plus\"，每个数据集都包含三个子集：第一个是选项随机打乱，第二个是将正确选项替换为\"没有其他选项是正确的\"，第三个是前两个子集的组合。我们在这些数据集上进行了实验，使用了鉴别和生成型的LLMs，并表明这些简单的技巧极大地阻碍了语言模型的性能。尽管在原始的公开可用数据集上表现出优秀的性能，但我们发现所有模型都很难回答我们新构建的数据集。我们展示了通过扰动引入任务变化可以提高模型的性能。",
    "tldr": "通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。",
    "en_tdlr": "Through a systematic evaluation of large language models on out-of-distribution logical reasoning tasks, we found that these models struggle to handle our newly constructed datasets, despite their good performance on other natural language processing tasks. This indicates the need for further research on the generalization and robustness of these models in logical reasoning."
}