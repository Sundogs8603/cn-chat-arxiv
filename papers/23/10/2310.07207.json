{
    "title": "Robust Safe Reinforcement Learning under Adversarial Disturbances. (arXiv:2310.07207v1 [cs.LG])",
    "abstract": "Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the m",
    "link": "http://arxiv.org/abs/2310.07207",
    "context": "Title: Robust Safe Reinforcement Learning under Adversarial Disturbances. (arXiv:2310.07207v1 [cs.LG])\nAbstract: Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the m",
    "path": "papers/23/10/2310.07207.json",
    "total_tokens": 919,
    "translated_title": "在对抗性干扰下的鲁棒安全强化学习",
    "translated_abstract": "安全性是将强化学习应用于现实世界控制任务时的首要关注点，特别是在存在外部干扰的情况下。然而，现有的安全强化学习算法很少考虑外部干扰，限制了它们在实践中的适用性和鲁棒性。为了解决这个挑战，本文提出了一个鲁棒的安全强化学习框架来应对最坏情况下的干扰。首先，本文提出了一个策略迭代方案，以解决鲁棒不变集，即安全集的子集，其中只有状态在这个集合内才能实现持久安全。关键思想是通过利用Hamilton-Jacobi可达性分析中的安全值函数建立一个双人零和游戏，其中主角（即控制输入）旨在保持安全，而对手（即外部干扰）试图破坏安全。本文证明了所提出的策略迭代算法单调收敛到m。",
    "tldr": "本文针对现有的安全强化学习算法很少考虑外部干扰的问题，提出了一个鲁棒的安全强化学习框架，通过建立双人零和游戏来解决最坏情况下的干扰，并证明了策略迭代算法的收敛性。"
}