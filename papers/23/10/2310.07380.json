{
    "title": "Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])",
    "abstract": "Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac",
    "link": "http://arxiv.org/abs/2310.07380",
    "context": "Title: Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])\nAbstract: Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac",
    "path": "papers/23/10/2310.07380.json",
    "total_tokens": 932,
    "translated_title": "基于联邦学习的组织病理图像分类和脆弱性分析",
    "translated_abstract": "医疗保健是机器学习(Machine Learning，ML)的主要应用之一。传统上，ML模型由中央服务器训练，通过汇总来自各个分布式设备的数据来预测新生成数据的结果。这是一个主要关注点，因为模型可以访问敏感的用户信息，引发隐私问题。联邦学习(FL)方法可以帮助解决这个问题：全局模型将其副本发送给所有客户端，这些客户端训练这些副本，并将更新(权重)发送回给全局模型。随着时间的推移，全局模型变得更加准确。训练过程中保护了数据隐私，因为训练是在客户端设备上本地进行的。然而，全局模型容易受到数据污染的影响。我们开发了一种保护隐私的联邦学习技术，应用于一个皮肤癌数据集，并展示了模型容易受到数据污染攻击的情况。十个客户端训练模型，但其中一个故意引入了翻转标签作为攻击。这降低了模型的准确性。",
    "tldr": "这项研究开发了一种基于联邦学习的隐私保护技术，应用于皮肤癌数据集，发现该模型容易受到数据污染攻击的影响。",
    "en_tdlr": "This research develops a privacy-preserving technique based on federated learning for a skin cancer dataset and shows that the model is susceptible to data poisoning attacks."
}