{
    "title": "ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])",
    "abstract": "Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co",
    "link": "http://arxiv.org/abs/2310.19961",
    "context": "Title: ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])\nAbstract: Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co",
    "path": "papers/23/10/2310.19961.json",
    "total_tokens": 911,
    "translated_title": "ExPT: 用于少样本实验设计的合成预训练方法",
    "translated_abstract": "实验设计是许多科学和工程领域中的一个基本问题。在这个问题中，由于现实世界设计评估的时间、金钱和安全成本，样本效率非常重要。现有的方法要么依赖主动数据收集，要么依赖对过去实验的大规模标记数据集的访问，这使得它们在许多现实场景中不可行。在这项工作中，我们解决了少样本实验设计的更具挑战性、现实的环境，其中只有少量带标签的输入设计样本及其相应的数值可用。我们将这个问题看作是一个条件生成任务，在这个任务中，模型根据少量带标签的样本和期望的输出条件生成最优的输入设计。为此，我们引入了实验预训练变换器（ExPT），这是一个用于少样本实验设计的基础模型，它采用合成预训练与上下文学习的新颖组合。在ExPT中，我们只假设对有限的上下文知识有了解。",
    "tldr": "ExPT是一种用于少样本实验设计的合成预训练方法，通过将条件生成任务应用于少量带标签的样本和期望输出，生成最优的输入设计。这种方法在不依赖主动数据收集或大规模标记数据集的情况下，在现实场景中具有实用性。",
    "en_tdlr": "ExPT is a synthetic pretraining method for few-shot experimental design. It generates optimal input designs by applying conditional generation to a few labeled examples and the desired output, without relying on active data collection or large labeled datasets."
}