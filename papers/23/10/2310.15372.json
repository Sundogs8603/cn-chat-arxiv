{
    "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])",
    "abstract": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from ",
    "link": "http://arxiv.org/abs/2310.15372",
    "context": "Title: EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])\nAbstract: In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from ",
    "path": "papers/23/10/2310.15372.json",
    "total_tokens": 940,
    "translated_title": "EpiK-Eval：将语言模型作为认识模型的评估",
    "translated_abstract": "在人工智能时代，大型语言模型（LLMs）的作用越来越重要。尽管它们日益普及，但它们在从不同训练文档中整合知识的能力——在许多应用中都是关键能力——仍未得到探索。本文首次研究了LLMs在其参数空间内有效地结合这种信息的能力。我们引入了EpiK-Eval，一个新颖的问答基准，旨在评估LLMs在从分割的叙述中构建一种连贯和一致的知识表示方面的能力。对各种LLMs的评估揭示了在这一领域存在的显著弱点。我们认为这些缺点源于现有训练目标的固有性质。因此，我们主张改进知识整合的方法，因为这有潜力显著提高LLMs的整体效果和性能。",
    "tldr": "这项研究介绍了一种新的评估方法EpiK-Eval，旨在评估大型语言模型（LLMs）在从分割的叙述中构建连贯和一致的知识表示方面的能力。研究发现当前的训练目标存在固有的缺陷，因此提出了改进知识整合方法的建议，以大幅提高LLMs的整体效果和性能。",
    "en_tdlr": "This study introduces a novel evaluation method called EpiK-Eval to assess the ability of large language models (LLMs) in constructing coherent and consistent knowledge representations from segmented narratives. The research findings reveal inherent shortcomings in current training objectives, leading to a recommendation of refining the approach towards knowledge consolidation for significant improvements in the overall effectiveness and performance of LLMs."
}