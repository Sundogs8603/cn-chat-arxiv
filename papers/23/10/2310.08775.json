{
    "title": "When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])",
    "abstract": "We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}",
    "link": "http://arxiv.org/abs/2310.08775",
    "context": "Title: When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])\nAbstract: We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}",
    "path": "papers/23/10/2310.08775.json",
    "total_tokens": 813,
    "translated_title": "当机器学习模型泄漏：合成训练数据的探索",
    "translated_abstract": "我们研究了对一个机器学习模型的攻击，该模型用于预测一个人或家庭在接下来的两年内是否会搬迁，即迁移倾向分类器。攻击假设攻击者可以查询模型以获取预测，并且模型训练时使用的数据的边际分布是公开可用的。攻击还假设攻击者已经获取了一定数量目标个体的非敏感属性值。攻击的目标是推断这些目标个体的敏感属性值。我们探讨了在训练模型时用合成数据替代原始数据对攻击者成功推断敏感属性的影响。",
    "tldr": "本论文研究了针对一个预测人员或家庭是否会在接下来的两年内搬迁的机器学习模型的攻击，攻击者利用模型的预测以及公开的训练数据边际分布来推断目标个体的敏感属性值，同时探讨了用合成数据替代原始数据训练模型对攻击的影响。",
    "en_tdlr": "This paper investigates an attack on a machine learning model that predicts relocation, where the attacker uses the model's predictions and publicly available data to infer sensitive attributes of target individuals, and explores the impact of using synthetic data instead of original data on the success of the attack."
}