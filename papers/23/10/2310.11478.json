{
    "title": "ASP: Automatic Selection of Proxy dataset for efficient AutoML. (arXiv:2310.11478v1 [cs.LG])",
    "abstract": "Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other ",
    "link": "http://arxiv.org/abs/2310.11478",
    "context": "Title: ASP: Automatic Selection of Proxy dataset for efficient AutoML. (arXiv:2310.11478v1 [cs.LG])\nAbstract: Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other ",
    "path": "papers/23/10/2310.11478.json",
    "total_tokens": 869,
    "translated_title": "ASP: 用于高效AutoML的自动选择代理数据集",
    "translated_abstract": "由于数据量的增加和多样有效的神经网络设计, 深度神经网络取得了巨大的成功。然而, 这也给计算带来了沉重的负担, 因为训练数据量与训练时间成正比。此外, 一个良好的模型需要重复尝试不同的结构设计和超参数, 即使使用了最先进的超参数优化算法和神经架构搜索算法, 这可能也需要大量的时间。本文提出了一个自动选择代理数据集框架 (ASP), 旨在在每个epoch动态地找到信息丰富的代理子集, 减小训练数据大小并节省AutoML处理时间。我们在CIFAR10、CIFAR100、ImageNet16-120和ImageNet-1k上验证了ASP的有效性和泛化性能, 通过对不同公共模型基准的实验结果表明ASP可以获得比其他方法更好的结果。",
    "tldr": "本文提出了一个自动选择代理数据集框架 (ASP)，通过动态地找到信息丰富的代理子集来减小训练数据大小并节省AutoML处理时间，实验证明ASP在不同基准测试上获得了优于其他方法的结果。",
    "en_tdlr": "This paper proposes an Automatic Selection of Proxy dataset framework (ASP) that dynamically finds informative proxy subsets of training data to reduce training data size and save AutoML processing time. Experimental results demonstrate that ASP outperforms other methods on different benchmarks."
}