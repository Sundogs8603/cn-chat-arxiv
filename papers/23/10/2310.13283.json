{
    "title": "FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])",
    "abstract": "Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous ",
    "link": "http://arxiv.org/abs/2310.13283",
    "context": "Title: FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])\nAbstract: Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous ",
    "path": "papers/23/10/2310.13283.json",
    "total_tokens": 874,
    "translated_title": "FedLoRA：基于LoRA调整的模型异构个性化联邦学习",
    "translated_abstract": "联邦学习是一种新兴的机器学习范 Paradig，其中一个中央服务器协调多个参与者（即FL客户端）在分散的数据上进行协作训练模型，同时保护隐私。这种范 Paradig 限制了所有客户端必须使用相同结构的模型（同构）。实践中，FL经常面临统计异质性、系统异质性和模型异质性等挑战。这些问题激发了模型异构个性化联邦学习（MHPFL）领域的研究，旨在为每个FL客户端训练一个个性化且异构的本地模型。现有的MHPFL方法无法同时实现令人满意的模型性能、可接受的计算开销和高效的通信。为了填补这一差距，我们提出了一种基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架（FedLoRA）。",
    "tldr": "FedLoRA是基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架，旨在为每个联邦学习客户端训练个性化且异构的本地模型。",
    "en_tdlr": "FedLoRA is a computation- and communication-efficient model-heterogeneous personalized federated learning framework based on LoRA tuning, aiming to train personalized and heterogeneous local models for each federated learning client."
}