{
    "title": "Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])",
    "abstract": "A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.",
    "link": "http://arxiv.org/abs/2310.06977",
    "context": "Title: Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings. (arXiv:2310.06977v1 [cs.CL])\nAbstract: A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.",
    "path": "papers/23/10/2310.06977.json",
    "total_tokens": 822,
    "translated_title": "为什么在几何学中要费心？关于Transformer嵌入的线性分解的相关性。",
    "translated_abstract": "最近的一些研究表明，Transformer嵌入可以线性分解为明确定义的因子之和，这些因子可以与特定的网络输入或组件相关联。然而，仍然缺乏研究这些数学重述是否在实证上具有意义。在本研究中，我们使用两种嵌入分解方法研究了机器翻译解码器的表示。我们的结果表明，尽管分解导出的指标与模型性能有效相关，但不同运行之间的变异性表明对这个问题需要更加细致的考虑。我们测量的高度变异性表明，几何反映的是模型特定的特征，而不是特定于句子的计算，而相似的训练条件并不能保证相似的向量空间。",
    "tldr": "本研究研究了Transformer嵌入的线性分解方法在机器翻译解码器中的应用。结果显示，虽然分解指标与模型表现有效相关，但不同运行之间的变异性表明对于数学重述是否在实证上具有意义还需要进一步研究。",
    "en_tdlr": "This study investigates the application of linear decomposition methods for Transformer embeddings in machine translation decoders. The results indicate that while decomposition-derived indicators effectively correlate with model performance, the variation across different runs suggests further research is needed to determine the empirical significance of these mathematical reformulations."
}