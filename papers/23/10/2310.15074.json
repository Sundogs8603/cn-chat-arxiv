{
    "title": "MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)",
    "abstract": "Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec",
    "link": "http://arxiv.org/abs/2310.15074",
    "context": "Title: MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)\nAbstract: Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec",
    "path": "papers/23/10/2310.15074.json",
    "total_tokens": 849,
    "translated_title": "MGAS: 多粒度架构搜索以实现高效且有效的神经网络",
    "translated_abstract": "可微分架构搜索(DAS)通过时间高效的自动化改变了神经网络架构搜索(NAS)的方式，从离散候选采样和评估转变为可微分超网络优化和离散化。然而，现有的DAS方法要么只进行粗粒度的操作级搜索，要么手动定义剩余的细粒度的核级和权重级单位的比例，从而无法同时优化模型大小和模型性能。此外，这些方法为了减少内存消耗而牺牲了搜索质量。为了解决这些问题，我们引入了多粒度架构搜索(MGAS)，这是一个统一的框架，旨在全面而内存高效地探索多粒度搜索空间，发现既有效又高效的神经网络。具体来说，我们学习了针对每个粒度级别的离散化函数，根据不断演化的架构自适应地确定剩余的比例。",
    "tldr": "MGAS是一个多粒度架构搜索的统一框架，通过学习特定粒度级别的离散化函数，自适应地确定剩余比例，从而实现同时优化模型大小和模型性能。",
    "en_tdlr": "MGAS is a unified framework for multi-granularity architecture search that optimizes both model size and performance by learning specific discretization functions for each granularity level and adaptively determining the remaining ratios."
}