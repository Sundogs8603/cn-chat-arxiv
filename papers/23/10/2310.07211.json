{
    "title": "Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration. (arXiv:2310.07211v1 [cs.LG])",
    "abstract": "Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite",
    "link": "http://arxiv.org/abs/2310.07211",
    "context": "Title: Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration. (arXiv:2310.07211v1 [cs.LG])\nAbstract: Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite",
    "path": "papers/23/10/2310.07211.json",
    "total_tokens": 945,
    "translated_title": "缩小牛顿-拉弗森方法和正规化策略迭代之间的差距",
    "translated_abstract": "正则化是强化学习算法中最重要的技术之一。众所周知，软演员-评论家算法是正则化策略迭代的一个特例，其中正则化项选择为Shannon熵。尽管正则化策略迭代在实践中取得了一些成功，但其理论基础仍不清楚。本文证明，在使用强凸函数对贝尔曼方程平滑化的条件下，正则化策略迭代在严格意义上等价于标准的牛顿-拉弗森方法。这种等价性为正则化策略迭代的全局和局部收敛行为奠定了基础。我们证明正则化策略迭代具有全局线性收敛性，收敛速度为$\\gamma$（折扣因子）。此外，一旦进入最优值周围的局部区域，该算法将二次收敛。我们还展示了正则化策略迭代的改进版本，即有限的正-----------此处省略部分内容---------------",
    "tldr": "正则化策略迭代和牛顿-拉弗森方法在使用强凸函数对贝尔曼方程平滑化的条件下严格等价，为正则化策略迭代的全局和局部收敛行为提供了统一分析。该算法具有全局线性收敛和局部二次收敛特性。",
    "en_tdlr": "Regularized policy iteration is proven to be strictly equivalent to the Newton-Raphson method under the condition of smoothing out the Bellman equation with strongly convex functions, providing a unified analysis for its global and local convergence behaviors. The algorithm exhibits global linear convergence and quadratic convergence in a local region."
}