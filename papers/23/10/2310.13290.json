{
    "title": "Interpreting Indirect Answers to Yes-No Questions in Multiple Languages. (arXiv:2310.13290v1 [cs.CL])",
    "abstract": "Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data. We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages). Additionally, we show that cross-lingual fine-tuning is always beneficial (8 languages).",
    "link": "http://arxiv.org/abs/2310.13290",
    "context": "Title: Interpreting Indirect Answers to Yes-No Questions in Multiple Languages. (arXiv:2310.13290v1 [cs.CL])\nAbstract: Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data. We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages). Additionally, we show that cross-lingual fine-tuning is always beneficial (8 languages).",
    "path": "papers/23/10/2310.13290.json",
    "total_tokens": 798,
    "translated_title": "在多种语言中解读对于是否问题的间接回答",
    "translated_abstract": "是与否的问题期望得到是或否的回答，但人们经常跳过极性关键词。相反，他们用长篇解释来回答，而这些解释需要进行解读。在本文中，我们专注于这个具有挑战性的问题，并在八种语言中发布了新的基准。我们提出了一种远程监督的方法来收集训练数据。我们还证明了直接回答（即带有极性关键词）对于训练模型解读间接回答（即不带有极性关键词）是有用的。实验结果表明，如果可以通过远程监督获得感兴趣语言的训练数据，则单语微调是有益的（5种语言）。此外，我们还展示了跨语言微调总是有益的（8种语言）。",
    "tldr": "本文关注解读对于是否问题的间接回答的挑战性问题，并在八种语言中发布新的基准。实验证明，通过远程监督获取训练数据后，单语微调和跨语言微调都能带来益处。",
    "en_tdlr": "This paper focuses on the challenging problem of interpreting indirect answers to yes-no questions and releases new benchmarks in eight languages. Experimental results demonstrate the benefits of monolingual and cross-lingual fine-tuning when training data can be obtained via distant supervision."
}