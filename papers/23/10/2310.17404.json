{
    "title": "Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])",
    "abstract": "Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.",
    "link": "http://arxiv.org/abs/2310.17404",
    "context": "Title: Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])\nAbstract: Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.",
    "path": "papers/23/10/2310.17404.json",
    "total_tokens": 941,
    "translated_title": "神经网络的不变性测量",
    "translated_abstract": "神经网络中的不变性对许多任务都是有用且必要的。然而，大多数神经网络模型中的不变性表示尚未被明确表征。我们提出了一种量化神经网络不变性的测量方法，该方法基于其内部表示。这些测量方法高效且可解释，并可应用于任何神经网络模型。与之前定义的测量方法相比，它们对不变性更为敏感。我们在仿射变换领域和CIFAR10和MNIST数据集上验证了这些测量方法及其属性，包括其稳定性和可解释性。利用这些测量方法，我们对CNN模型进行了首次分析，并展示了它们的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。我们相信这些测量方法将为不变性表示的新研究方向提供可能性。",
    "tldr": "本文提出了一种用于量化神经网络不变性的测量方法，该方法敏感且可解释，并能应用于任何神经网络模型。在仿射变换领域和CIFAR10和MNIST数据集上的验证表明，神经网络的内部不变性对于随机权重初始化非常稳定，但对于数据集或变换的改变不稳定。这些测量方法将为不变性表示的新研究方向提供可能性。",
    "en_tdlr": "This paper proposes a measure for quantifying the invariance of neural networks, which is sensitive and interpretable, and can be applied to any neural network model. The validation on affine transformations and CIFAR10 and MNIST datasets shows that the internal invariance of CNN models is stable to random weight initializations but not to changes in dataset or transformation. These measures enable new avenues of research in invariance representation."
}