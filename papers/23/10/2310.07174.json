{
    "title": "Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])",
    "abstract": "Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
    "link": "http://arxiv.org/abs/2310.07174",
    "context": "Title: Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])\nAbstract: Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
    "path": "papers/23/10/2310.07174.json",
    "total_tokens": 834,
    "translated_title": "具有无误差的可微分交换函数的广义神经排序网络",
    "translated_abstract": "排序是所有计算机系统的基本操作，一直是一个长期的重要研究课题。除了传统排序算法的问题表述，我们通过神经排序网络考虑了更抽象但具有表达力的输入，例如多位数字图像和图像片段。为了学习从高维输入到次序变量的映射，需要保证排序网络的可微分性。在本文中，我们通过可微分的交换函数定义一个柔化误差，并开发了一个无误差的交换函数，该函数满足非减和可微分的条件。此外，采用了具有多头注意力机制的置换等变Transformer网络，以捕捉给定输入之间的依赖关系，并利用其自注意力的模型能力。在多样的排序基准上进行的实验证明，我们的方法优于或与基准方法相当。",
    "tldr": "本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。",
    "en_tdlr": "This paper proposes a generalized neural sorting network with error-free differentiable swap functions. It also utilizes a permutation-equivariant Transformer network to capture dependencies between inputs. Experimental results demonstrate that this method outperforms or is comparable to baseline methods on various sorting benchmarks."
}