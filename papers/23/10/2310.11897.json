{
    "title": "Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])",
    "abstract": "Policy gradient methods have recently been shown to enjoy global convergence at a $\\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \\textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con",
    "link": "http://arxiv.org/abs/2310.11897",
    "context": "Title: Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])\nAbstract: Policy gradient methods have recently been shown to enjoy global convergence at a $\\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \\textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con",
    "path": "papers/23/10/2310.11897.json",
    "total_tokens": 940,
    "translated_title": "加速策略梯度：关于应用Nesterov动量在强化学习中的论文",
    "translated_abstract": "最近研究表明，策略梯度方法在非正则化表格softmax设置中以Θ(1/t)的速率全局收敛。因此，一个重要的研究问题是是否可以通过仅使用一阶更新进一步改进这种收敛速度。本文从动量的角度回答了上述问题，通过将著名的Nesterov加速梯度（NAG）方法应用于强化学习（RL），称之为 \\textit{加速策略梯度}（APG）。为了展示APG在实现更快全局收敛方面的潜力，我们正式证明了使用真实梯度时，具有 softmax 策略参数化的APG以 $\\tilde{O}(1/t^2)$ 的速率收敛到最优策略。据我们所知，这是NAG在RL领域中全局收敛速率的第一个表征。值得注意的是，我们的分析依赖于一个有趣的发现：不论初始化如何，APG最终可以达到近乎局部收敛的地方。",
    "tldr": "本文介绍了一种名为加速策略梯度的方法，通过应用Nesterov动量在强化学习中实现更快的全局收敛速度。通过在softmax策略参数化中收敛到最优策略，它以 $\\tilde{O}(1/t^2)$ 的速率收敛。这是第一个基于Nesterov加速梯度在强化学习中全局收敛速率的研究。"
}