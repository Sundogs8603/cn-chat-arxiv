{
    "title": "Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models. (arXiv:2310.04039v1 [cs.CL])",
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks, especially in reasoning, a cornerstone for achieving Artificial General Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate the inferential abilities of these models in real-world scenarios. To address this gap, a new form of Question-Answering (QA) task, termed Reasoning with Redundant Information Provided (RRIP), is introduced. The study designed a modified version of the grade school math 8K (GSM-8K) dataset which has several variants focusing on different attributes of redundant information. This investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on traditional QA tasks against the RRIP tasks. Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines",
    "link": "http://arxiv.org/abs/2310.04039",
    "context": "Title: Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models. (arXiv:2310.04039v1 [cs.CL])\nAbstract: Recent advancements in Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks, especially in reasoning, a cornerstone for achieving Artificial General Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate the inferential abilities of these models in real-world scenarios. To address this gap, a new form of Question-Answering (QA) task, termed Reasoning with Redundant Information Provided (RRIP), is introduced. The study designed a modified version of the grade school math 8K (GSM-8K) dataset which has several variants focusing on different attributes of redundant information. This investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on traditional QA tasks against the RRIP tasks. Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines",
    "path": "papers/23/10/2310.04039.json",
    "total_tokens": 971,
    "translated_title": "大型语言模型的冗余信息解释能力分析",
    "translated_abstract": "最近对大型语言模型（LLMs）的研究取得了令人印象深刻的成果，在一系列自然语言处理任务中展示了出色的能力，尤其是在推理方面，这是实现人工通用智能（AGI）的基石。然而，常用的基准测试可能无法完全捕捉到这些模型在真实场景中的推理能力。为了弥补这一差距，引入了一种新的问答（QA）任务形式，称为冗余信息提供的推理（RRIP）。该研究设计了修改版的8K小学数学（GSM-8K）数据集，该数据集有多种变体，专注于冗余信息的不同属性。该研究评估了两种常用的LLMs，LlaMA2-13B-chat和生成预训练变压器3.5（GPT-3.5），对比了它们在传统QA任务和RRIP任务中的表现。结果表明，尽管这些模型在标准QA基准测试中取得了适度的成功，但它们的性能明显下降了。",
    "tldr": "该论文研究了大型语言模型在推理能力上的表现，通过引入一种新的问答任务（RRIP）来评估这些模型在冗余信息提供的推理中的性能。研究发现，尽管这些模型在传统QA任务上表现良好，但在RRIP任务中性能明显下降。",
    "en_tdlr": "This paper examines the performance of large language models in reasoning, evaluating their performance in reasoning with redundant information provided (RRIP) by introducing a new form of question-answering task. The study finds that while these models excel in traditional QA tasks, their performance notably declines in RRIP tasks."
}