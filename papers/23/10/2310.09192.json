{
    "title": "Does Graph Distillation See Like Vision Dataset Counterpart?. (arXiv:2310.09192v1 [cs.LG])",
    "abstract": "Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD ",
    "link": "http://arxiv.org/abs/2310.09192",
    "context": "Title: Does Graph Distillation See Like Vision Dataset Counterpart?. (arXiv:2310.09192v1 [cs.LG])\nAbstract: Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD ",
    "path": "papers/23/10/2310.09192.json",
    "total_tokens": 893,
    "translated_title": "图状压缩：是否看起来像视觉数据集？(arXiv:2310.09192v1 [cs.LG])",
    "translated_abstract": "在图表示学习中，对大规模图进行训练取得了显著的结果，但其成本和存储引起了越来越多的关注。现有的图压缩方法主要关注优化压缩图的特征矩阵，而忽视了原始图的结构信息的影响。为了调查结构信息的影响，我们从谱域进行分析，并经验性地确定了先前工作中的重要的拉普拉斯能量分布（LED）的偏移。这种偏移导致在跨架构泛化和特定任务（包括异常检测和链接预测）中的性能较差。在本文中，我们提出了一种新颖的结构广播图数据集压缩（SGDD）方案，将原始结构信息广播到合成图的生成过程中，显式地防止忽视原始结构信息。从理论上讲，SGDD生成的合成图具有保留原始图的结构信息的能力。",
    "tldr": "本文研究了图压缩方法中对结构信息的忽视问题，并提出了一种新的结构广播图数据集压缩方案（SGDD），该方案能够保留原始图的结构信息，并在跨架构泛化和特定任务中取得了优秀的性能。",
    "en_tdlr": "This paper investigates the problem of overlooking structure information in graph compression methods and proposes a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme that preserves the original structure information and achieves excellent performance in cross-architecture generalization and specific tasks."
}