{
    "title": "Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])",
    "abstract": "Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.  This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate",
    "link": "http://arxiv.org/abs/2310.03150",
    "context": "Title: Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])\nAbstract: Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.  This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate",
    "path": "papers/23/10/2310.03150.json",
    "total_tokens": 976,
    "translated_title": "在非常边缘上对LLMs进行联邦微调：好、坏和丑(arXiv:2310.03150v1 [cs.LG])",
    "translated_abstract": "大规模语言模型（LLM）和基础模型因为提供了改进自然语言处理、与数据交互和快速检索信息的新机会而受到欢迎。然而，训练或微调LLMs需要大量的数据，由于法律或技术限制可能难以访问，并且可能需要私有计算资源。联邦学习（FL）是一种旨在克服这些挑战并扩大深度学习应用数据访问的解决方案。本文采用硬件为中心的方法，探索了如何将LLMs引入现代边缘计算系统。我们对FLAN-T5模型系列进行微调，参数范围从80M到3B，并应用于文本摘要任务。我们提供了微观水平的硬件基准测试，将模型的FLOP利用率与最先进的数据中心GPU进行了比较，并研究了在实际条件下的网络利用率。我们的贡献具有两个方面：首先，我们评估了...",
    "tldr": "本论文以硬件为中心，探讨了如何将LLMs引入现代边缘计算系统。通过联邦学习（FL）对FLAN-T5模型进行微调，并对其在文本摘要任务上的性能进行了评估。同时提供了硬件基准测试和与数据中心GPU的比较。",
    "en_tdlr": "This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems using federated learning (FL). The FLAN-T5 model family is fine-tuned for a text summarization task, and a micro-level hardware benchmark is provided, comparing the model's performance to a data center GPU."
}