{
    "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
    "abstract": "arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen",
    "link": "https://arxiv.org/abs/2310.00754",
    "context": "Title: Analyzing and Mitigating Object Hallucination in Large Vision-Language Models\nAbstract: arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen",
    "path": "papers/23/10/2310.00754.json",
    "total_tokens": 842,
    "translated_title": "分析和减轻大型视觉语言模型中的物体幻觉",
    "translated_abstract": "大型视觉语言模型（LVLMs）在理解图像信息和人类语言方面显示出卓越的能力。然而，LVLMs 仍然存在物体幻觉问题，即生成包含图像中实际不存在的对象的描述。这可能对许多视觉语言任务产生负面影响，如视觉总结和推理。为解决这一问题，我们提出了一种简单而强大的算法，LVLM 幻觉修正器（LURE），通过重构较少具有幻觉性的描述来事后纠正LVLM 中的物体幻觉。LURE根据对导致物体幻觉的关键因素的严格统计分析，包括共现（图像中某些对象经常与其他对象一起出现）、不确定性（在LVLM解码过程中不确定性较高的对象）和对象位置（幻觉通常出现在生成描述的后部分）。",
    "tldr": "提出了一种名为LVLM Hallucination Revisor（LURE）的算法，通过重新构建较少具有幻觉性的描述，来事后纠正大型视觉语言模型中的物体幻觉问题。",
    "en_tdlr": "A simple yet powerful algorithm named LVLM Hallucination Revisor (LURE) is proposed to post-hoc rectify object hallucination in Large Vision-Language Models by reconstructing less hallucinatory descriptions."
}