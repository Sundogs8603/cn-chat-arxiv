{
    "title": "Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop. (arXiv:2310.17671v1 [cs.LG])",
    "abstract": "The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents fo",
    "link": "http://arxiv.org/abs/2310.17671",
    "context": "Title: Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop. (arXiv:2310.17671v1 [cs.LG])\nAbstract: The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents fo",
    "path": "papers/23/10/2310.17671.json",
    "total_tokens": 872,
    "translated_title": "将基于强化学习的控制器从模型传递到硬件在环中的研究",
    "translated_abstract": "开发嵌入式系统的控制功能是资源、时间和数据密集型的过程，经常导致次优的成本和解决方法。强化学习（RL）具有在最小人为干预下自动训练代理执行复杂控制任务的潜力。然而，由于数据生成的成本和安全约束，其应用大多限于纯粹的模拟领域。为了有效地在嵌入式系统功能开发中使用RL，生成的代理必须能够处理真实世界的应用。在这个背景下，本研究通过结合迁移学习（TL）和环境中的模型（XiL）模拟来加速RL代理的训练过程。对于内燃机的瞬态废气再循环控制案例，使用计算成本较低的模型在环（MiL）模拟来选择合适的算法，微调超参数，最后训练候选代理。",
    "tldr": "本研究通过结合迁移学习和环境中的模拟来加速强化学习代理的训练过程，以实现在嵌入式系统中有效使用强化学习的目标。",
    "en_tdlr": "This research focuses on accelerating the training process of reinforcement learning agents by combining transfer learning and simulation in the environment, aiming to achieve effective use of reinforcement learning in embedded systems."
}