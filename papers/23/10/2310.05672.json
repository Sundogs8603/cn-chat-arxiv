{
    "title": "Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])",
    "abstract": "In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi",
    "link": "http://arxiv.org/abs/2310.05672",
    "context": "Title: Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])\nAbstract: In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi",
    "path": "papers/23/10/2310.05672.json",
    "total_tokens": 868,
    "translated_title": "多步模型的基于模型的强化学习",
    "translated_abstract": "在基于模型的强化学习中，大多数算法依赖于从数据中学习到的一步动力学模型来模拟轨迹。这种方法的一个关键挑战是随着轨迹长度的增长，一步预测误差的累积。本文通过使用多步目标来训练一步模型来解决这个问题。我们的目标是在各种未来时间段上的一个损失函数（例如，负对数似然）的加权和。我们探索和测试了一系列权重方案。我们发现指数衰减权重导致模型在长时间段的R2得分显著提高。当模型在噪声数据上进行评估时，这种改进尤为明显。最后，我们在纯批量强化学习（RL）和迭代批量RL场景中使用软件演员-评论家（SAC）代理，发现我们的多步模型优于或与标准的一步模型相匹配。这在考虑环境的噪声变体中尤为明显。",
    "tldr": "多步模型的基于模型的强化学习算法通过使用多步目标来训练一步模型，解决了轨迹长度增长时一步预测误差的累积问题，并在噪声数据上表现出显著的性能提升。"
}