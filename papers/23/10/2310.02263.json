{
    "title": "Automatic Pair Construction for Contrastive Post-training",
    "abstract": "arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from \"easier\" pairs and transitioning to \"harder\" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.",
    "link": "https://arxiv.org/abs/2310.02263",
    "context": "Title: Automatic Pair Construction for Contrastive Post-training\nAbstract: arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from \"easier\" pairs and transitioning to \"harder\" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.",
    "path": "papers/23/10/2310.02263.json",
    "total_tokens": 926,
    "translated_title": "对比后训练的自动对构建",
    "translated_abstract": "对齐作为引导大型语言模型（LLMs）走向人类偏好的重要步骤。本文提出了一种自动构建LLM对比数据的方法，使用来自多个不同强度模型（例如InstructGPT、ChatGPT和GPT-4）的偏好对。我们比较了SLiC和DPO的对比技术与SFT基线，并发现即使在继续SFT饱和后，DPO仍然提供了一个阶跃式的改善。我们还探讨了一种对比后训练的数据课程学习方案，该方案从“更容易”的对开始学习，然后过渡到“更难”的对，进一步提高了对齐效果。最后，我们通过使用更多数据和像Orca这样的更大型模型来扩大实验规模。值得注意的是，我们的自动对比后训练进一步提高了Orca的性能，它已经是一个通过GPT-4输出调优的最先进指导学习模型，从而超越了ChatGPT。",
    "tldr": "提出了一种自动构建对比数据的方法，使用多个模型的偏好对，提高了大型语言模型的对齐效果，并且通过DPO对比技术得到了改善，进一步优化了对齐，最终使经过调优的指导学习模型Orca超越了ChatGPT。",
    "en_tdlr": "Proposed an automatic method for constructing contrastive data using preference pairs from multiple models, improving alignment of large language models, with further enhancement through DPO contrastive technique, ultimately surpassing ChatGPT with the tuned instruction learning model Orca."
}