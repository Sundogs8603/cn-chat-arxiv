{
    "title": "ImageManip: Image-based Robotic Manipulation with Affordance-guided Next View Selection. (arXiv:2310.09069v1 [cs.RO])",
    "abstract": "In the realm of future home-assistant robots, 3D articulated object manipulation is essential for enabling robots to interact with their environment. Many existing studies make use of 3D point clouds as the primary input for manipulation policies. However, this approach encounters challenges due to data sparsity and the significant cost associated with acquiring point cloud data, which can limit its practicality. In contrast, RGB images offer high-resolution observations using cost effective devices but lack spatial 3D geometric information. To overcome these limitations, we present a novel image-based robotic manipulation framework. This framework is designed to capture multiple perspectives of the target object and infer depth information to complement its geometry. Initially, the system employs an eye-on-hand RGB camera to capture an overall view of the target object. It predicts the initial depth map and a coarse affordance map. The affordance map indicates actionable areas on the ",
    "link": "http://arxiv.org/abs/2310.09069",
    "context": "Title: ImageManip: Image-based Robotic Manipulation with Affordance-guided Next View Selection. (arXiv:2310.09069v1 [cs.RO])\nAbstract: In the realm of future home-assistant robots, 3D articulated object manipulation is essential for enabling robots to interact with their environment. Many existing studies make use of 3D point clouds as the primary input for manipulation policies. However, this approach encounters challenges due to data sparsity and the significant cost associated with acquiring point cloud data, which can limit its practicality. In contrast, RGB images offer high-resolution observations using cost effective devices but lack spatial 3D geometric information. To overcome these limitations, we present a novel image-based robotic manipulation framework. This framework is designed to capture multiple perspectives of the target object and infer depth information to complement its geometry. Initially, the system employs an eye-on-hand RGB camera to capture an overall view of the target object. It predicts the initial depth map and a coarse affordance map. The affordance map indicates actionable areas on the ",
    "path": "papers/23/10/2310.09069.json",
    "total_tokens": 909,
    "translated_title": "ImageManip: 基于图像的带有可供识别引导的下一个视图选择的机器人操控",
    "translated_abstract": "在未来家庭助手机器人领域，三维关节物体操控对于使机器人与环境进行交互至关重要。许多现有的研究使用三维点云作为操纵策略的主要输入。然而，这种方法由于数据稀疏性和获取点云数据的显著成本而面临挑战，这可能限制了实际应用性。相反，RGB图像利用成本效益高的设备提供高分辨率的观察，但缺乏三维空间几何信息。为了克服这些限制，我们提出了一种新颖的基于图像的机器人操纵框架。该框架旨在捕捉目标物体的多个视角，并推断深度信息以补充其几何形状。初始阶段，系统使用一个手上有眼的RGB摄像头捕捉目标物体的整体视图。它预测初始深度图和粗略可供识别图。可供识别图指示了可行动的区域。",
    "tldr": "ImageManip是一个基于图像的机器人操纵框架，通过捕捉目标物体多个视角和推断深度信息，克服了使用三维点云数据进行操控的挑战。",
    "en_tdlr": "ImageManip is an image-based robotic manipulation framework that overcomes the challenges of using 3D point cloud data for manipulation by capturing multiple perspectives of the target object and inferring depth information."
}