{
    "title": "Off-Policy Evaluation for Large Action Spaces via Policy Convolution. (arXiv:2310.15433v1 [cs.LG])",
    "abstract": "Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled ",
    "link": "http://arxiv.org/abs/2310.15433",
    "context": "Title: Off-Policy Evaluation for Large Action Spaces via Policy Convolution. (arXiv:2310.15433v1 [cs.LG])\nAbstract: Developing accurate off-policy estimators is crucial for both evaluating and optimizing for new policies. The main challenge in off-policy estimation is the distribution shift between the logging policy that generates data and the target policy that we aim to evaluate. Typically, techniques for correcting distribution shift involve some form of importance sampling. This approach results in unbiased value estimation but often comes with the trade-off of high variance, even in the simpler case of one-step contextual bandits. Furthermore, importance sampling relies on the common support assumption, which becomes impractical when the action space is large. To address these challenges, we introduce the Policy Convolution (PC) family of estimators. These methods leverage latent structure within actions -- made available through action embeddings -- to strategically convolve the logging and target policies. This convolution introduces a unique bias-variance trade-off, which can be controlled ",
    "path": "papers/23/10/2310.15433.json",
    "total_tokens": 891,
    "translated_title": "基于策略卷积的大动作空间离策略评估",
    "translated_abstract": "发展准确的离策略估计器对于评估和优化新策略至关重要。离策略估计的主要挑战在于生成数据的记录策略和我们要评估的目标策略之间的分布转移。通常，纠正分布转移的技术涉及某种形式的重要性采样。这种方法导致了无偏值估计，但往往会带来高方差的代价，即使在简单的一步情境多臂老虎机的情况下也是如此。此外，重要性采样依赖于共同支持假设，在动作空间很大时变得不切实际。为了解决这些挑战，我们引入了策略卷积 (PC)家族的估计器。这些方法利用通过动作嵌入提供的动作内部结构进行策略的策略卷积。这种卷积引入了独特的偏差-方差权衡，可以进行控制",
    "tldr": "本研究提出了一种名为策略卷积（PC）的离策略估计方法，该方法通过动作嵌入来解决大动作空间下的分布转移问题，可以在偏差和方差之间进行权衡",
    "en_tdlr": "This paper introduces a method called Policy Convolution (PC) for off-policy evaluation, which leverages action embeddings to address distribution shift in large action spaces, providing a bias-variance trade-off."
}