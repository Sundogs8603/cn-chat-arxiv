{
    "title": "Which mode is better for federated learning? Centralized or Decentralized. (arXiv:2310.03461v1 [cs.LG])",
    "abstract": "Both centralized and decentralized approaches have shown excellent performance and great application value in federated learning (FL). However, current studies do not provide sufficient evidence to show which one performs better. Although from the optimization perspective, decentralized methods can approach the comparable convergence of centralized methods with less communication, its test performance has always been inefficient in empirical studies. To comprehensively explore their behaviors in FL, we study their excess risks, including the joint analysis of both optimization and generalization. We prove that on smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) from perspectives of the excess risk and test error in CFL, adopting partial participation is superior to full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. Based on some",
    "link": "http://arxiv.org/abs/2310.03461",
    "context": "Title: Which mode is better for federated learning? Centralized or Decentralized. (arXiv:2310.03461v1 [cs.LG])\nAbstract: Both centralized and decentralized approaches have shown excellent performance and great application value in federated learning (FL). However, current studies do not provide sufficient evidence to show which one performs better. Although from the optimization perspective, decentralized methods can approach the comparable convergence of centralized methods with less communication, its test performance has always been inefficient in empirical studies. To comprehensively explore their behaviors in FL, we study their excess risks, including the joint analysis of both optimization and generalization. We prove that on smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) from perspectives of the excess risk and test error in CFL, adopting partial participation is superior to full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. Based on some",
    "path": "papers/23/10/2310.03461.json",
    "total_tokens": 941,
    "translated_title": "哪种模式更适合联合学习？中央化还是分散化。",
    "translated_abstract": "在联合学习（FL）中，集中化和分散化方法都表现出了出色的性能和重要的应用价值。然而，目前的研究并没有提供足够的证据来表明哪种方法表现更好。虽然从优化的角度来看，分散化的方法可以通过较少的通信实现与集中化方法相比较的收敛性，但在实证研究中，它的测试性能始终效率低下。为了全面探索它们在联合学习中的行为，我们研究了它们的过度风险，包括优化和泛化的联合分析。我们证明了在光滑非凸目标上，1）集中化的FL（CFL）总是比分散化的FL（DFL）更好地进行泛化；2）从CFL的过度风险和测试误差的角度来看，部分参与比全参与更好；3）在DFL中，为了避免随着训练规模增加而性能崩溃，拓扑结构有必要满足一定的要求。基于一些实证结果",
    "tldr": "中国总结出的一句话要点：研究发现在联合学习中，集中化的方法总是比分散化的方法更好地进行泛化，同时，部分参与在集中化方法中表现更好，而在分散化方法中，拓扑结构对性能的影响十分重要。",
    "en_tdlr": "TLDR: The study found that in federated learning, centralized methods always generalize better than decentralized methods. Partial participation is superior in centralized methods, while the topology in decentralized methods is crucial for performance."
}