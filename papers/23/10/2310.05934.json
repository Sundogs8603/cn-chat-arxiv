{
    "title": "DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion. (arXiv:2310.05934v1 [cs.CV])",
    "abstract": "Speech-driven 3D facial animation has gained significant attention for its ability to create realistic and expressive facial animations in 3D space based on speech. Learning-based methods have shown promising progress in achieving accurate facial motion synchronized with speech. However, one-to-many nature of speech-to-3D facial synthesis has not been fully explored: while the lip accurately synchronizes with the speech content, other facial attributes beyond speech-related motions are variable with respect to the speech. To account for the potential variance in the facial attributes within a single speech, we propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis. DF-3DFace captures the complex one-to-many relationships between speech and 3D face based on diffusion. It concurrently achieves aligned lip motion by exploiting audio-mesh synchronization and masked conditioning. Furthermore, the proposed method jointly models identity and pose in addition to facial motions ",
    "link": "http://arxiv.org/abs/2310.05934",
    "context": "Title: DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion. (arXiv:2310.05934v1 [cs.CV])\nAbstract: Speech-driven 3D facial animation has gained significant attention for its ability to create realistic and expressive facial animations in 3D space based on speech. Learning-based methods have shown promising progress in achieving accurate facial motion synchronized with speech. However, one-to-many nature of speech-to-3D facial synthesis has not been fully explored: while the lip accurately synchronizes with the speech content, other facial attributes beyond speech-related motions are variable with respect to the speech. To account for the potential variance in the facial attributes within a single speech, we propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis. DF-3DFace captures the complex one-to-many relationships between speech and 3D face based on diffusion. It concurrently achieves aligned lip motion by exploiting audio-mesh synchronization and masked conditioning. Furthermore, the proposed method jointly models identity and pose in addition to facial motions ",
    "path": "papers/23/10/2310.05934.json",
    "total_tokens": 903,
    "translated_title": "DF-3DFace: 一对多语音同步的3D面部动画，基于扩散模型",
    "translated_abstract": "受到其根据语音创建逼真和表达丰富的3D面部动画能力的影响，由语音驱动的3D面部动画已经引起了相当大的关注。基于学习的方法在实现与语音同步的准确面部动作方面取得了显著进展。然而，语音到3D面部综合的一对多特性尚未得到充分探索：虽然唇部与语音内容准确同步，但与语音相关运动之外的其他面部属性在语音方面是可变的。为了解决单一语音中面部属性潜在的差异，我们提出了DF-3DFace，一种基于扩散模型的语音到3D面部网格合成方法。DF-3DFace根据扩散模型捕捉了语音和3D面部之间复杂的一对多关系。它通过利用音频-网格同步和掩蔽条件同时实现了对齐的唇部动作。此外，所提出的方法还共同建模身份和姿势，除了面部运动之外。",
    "tldr": "DF-3DFace是一种采用扩散模型的语音到3D面部网格合成方法，能够准确同步唇部动作，并综合了身份、姿势和面部运动。"
}