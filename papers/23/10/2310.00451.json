{
    "title": "On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning. (arXiv:2310.00451v2 [cs.LG] UPDATED)",
    "abstract": "Meta-learning frameworks for few-shot learning aims to learn models that can learn new skills or adapt to new environments rapidly with a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial. Neural collapse ($\\mathcal{NC}$) is a recently discovered phenomenon which showcases unique properties at the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning fra",
    "link": "http://arxiv.org/abs/2310.00451",
    "context": "Title: On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning. (arXiv:2310.00451v2 [cs.LG] UPDATED)\nAbstract: Meta-learning frameworks for few-shot learning aims to learn models that can learn new skills or adapt to new environments rapidly with a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial. Neural collapse ($\\mathcal{NC}$) is a recently discovered phenomenon which showcases unique properties at the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning fra",
    "path": "papers/23/10/2310.00451.json",
    "total_tokens": 895,
    "translated_title": "关于神经崩溃在元学习模型中在Few-shot学习中的作用",
    "translated_abstract": "Few-shot学习的元学习框架旨在学习能够在很少的训练样本下迅速学习新技能或适应新环境的模型。这导致所开发的模型具有向新类别的泛化能力，只需很少的标记样本。然而，这些网络被看作是黑盒模型，理解在不同学习场景下学到的表示是至关重要的。神经崩溃（NC）是最近发现的一种现象，它展示了网络在趋近于零损失时的独特特性。输入特征会崩溃到各自的类别平均值，类别平均值会形成一个简单的等角紧框架（ETF），其中类别平均值最大地分离且线性可分，分类器充当一个简单的最近邻分类器。尽管这些现象在简单的分类网络中已经观察到，但这个研究是第一个探索和理解元学习框架中神经崩溃属性的研究。",
    "tldr": "该研究首次探索了在元学习框架中神经崩溃属性的特性，并揭示了神经崩溃现象中输入特征、类别平均值和分类器的关系。",
    "en_tdlr": "This study explores the properties of neural collapse in meta learning frameworks and reveals the relationship between input features, class means, and the classifier in the phenomenon of neural collapse."
}