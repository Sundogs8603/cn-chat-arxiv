{
    "title": "Optimal Estimator for Linear Regression with Shuffled Labels. (arXiv:2310.01326v1 [stat.ML])",
    "abstract": "This paper considers the task of linear regression with shuffled labels, i.e., $\\mathbf Y = \\mathbf \\Pi \\mathbf X \\mathbf B + \\mathbf W$, where $\\mathbf Y \\in \\mathbb R^{n\\times m}, \\mathbf Pi \\in \\mathbb R^{n\\times n}, \\mathbf X\\in \\mathbb R^{n\\times p}, \\mathbf B \\in \\mathbb R^{p\\times m}$, and $\\mathbf W\\in \\mathbb R^{n\\times m}$, respectively, represent the sensing results, (unknown or missing) corresponding information, sensing matrix, signal of interest, and additive sensing noise. Given the observation $\\mathbf Y$ and sensing matrix $\\mathbf X$, we propose a one-step estimator to reconstruct $(\\mathbf \\Pi, \\mathbf B)$. From the computational perspective, our estimator's complexity is $O(n^3 + np^2m)$, which is no greater than the maximum complexity of a linear assignment algorithm (e.g., $O(n^3)$) and a least square algorithm (e.g., $O(np^2 m)$). From the statistical perspective, we divide the minimum $snr$ requirement into four regimes, e.g., unknown, hard, medium, and easy reg",
    "link": "http://arxiv.org/abs/2310.01326",
    "context": "Title: Optimal Estimator for Linear Regression with Shuffled Labels. (arXiv:2310.01326v1 [stat.ML])\nAbstract: This paper considers the task of linear regression with shuffled labels, i.e., $\\mathbf Y = \\mathbf \\Pi \\mathbf X \\mathbf B + \\mathbf W$, where $\\mathbf Y \\in \\mathbb R^{n\\times m}, \\mathbf Pi \\in \\mathbb R^{n\\times n}, \\mathbf X\\in \\mathbb R^{n\\times p}, \\mathbf B \\in \\mathbb R^{p\\times m}$, and $\\mathbf W\\in \\mathbb R^{n\\times m}$, respectively, represent the sensing results, (unknown or missing) corresponding information, sensing matrix, signal of interest, and additive sensing noise. Given the observation $\\mathbf Y$ and sensing matrix $\\mathbf X$, we propose a one-step estimator to reconstruct $(\\mathbf \\Pi, \\mathbf B)$. From the computational perspective, our estimator's complexity is $O(n^3 + np^2m)$, which is no greater than the maximum complexity of a linear assignment algorithm (e.g., $O(n^3)$) and a least square algorithm (e.g., $O(np^2 m)$). From the statistical perspective, we divide the minimum $snr$ requirement into four regimes, e.g., unknown, hard, medium, and easy reg",
    "path": "papers/23/10/2310.01326.json",
    "total_tokens": 1034,
    "translated_title": "具有打乱标签的线性回归的最优估计器",
    "translated_abstract": "本文考虑了具有打乱标签的线性回归问题，即 $\\mathbf Y = \\mathbf \\Pi \\mathbf X \\mathbf B + \\mathbf W$，其中 $\\mathbf Y \\in \\mathbb R^{n\\times m}, \\mathbf Pi \\in \\mathbb R^{n\\times n}, \\mathbf X\\in \\mathbb R^{n\\times p}, \\mathbf B \\in \\mathbb R^{p\\times m}$，和 $\\mathbf W\\in \\mathbb R^{n\\times m}$ 分别表示传感结果，（未知或缺失的）对应信息，传感矩阵，感兴趣的信号和附加的感知噪声。给定观测值 $\\mathbf Y$ 和感知矩阵 $\\mathbf X$，我们提出了一种一步估计器来重构 $(\\mathbf \\Pi, \\mathbf B)$。从计算的角度来看，我们估计器的复杂度为 $O(n^3 + np^2m)$，不大于线性分配算法（例如 $O(n^3)$）和最小二乘算法（例如 $O(np^2 m)$）的最大复杂度。从统计学的角度来看，我们将最小信噪比要求分为四个区间，即未知、困难、中等和简单区间。",
    "tldr": "本文提出了一种用于解决具有打乱标签的线性回归问题的最优估计器。该估计器的复杂度不超过线性分配算法和最小二乘算法，并对信噪比要求进行了细分。",
    "en_tdlr": "This paper proposes an optimal estimator for solving linear regression problems with shuffled labels. The estimator has a complexity no greater than linear assignment algorithms and least square algorithms, and divides the SNR requirements into different regimes."
}