{
    "title": "Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])",
    "abstract": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \"frugal\" setting where it is desired to reduce the number of expert model evaluations at test time.",
    "link": "http://arxiv.org/abs/2310.01542",
    "context": "Title: Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])\nAbstract: Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \"frugal\" setting where it is desired to reduce the number of expert model evaluations at test time.",
    "path": "papers/23/10/2310.01542.json",
    "total_tokens": 799,
    "translated_title": "融合具有互补专长的模型",
    "translated_abstract": "训练能够在不同任务和领域中进行泛化的AI模型一直是推动AI研究的开放问题之一。基础模型的出现使得获得特定任务的专家模型变得更加容易，但是在测试时可能会遇到的数据的异质性意味着任何单个专家都不足够。我们考虑融合专家模型输出的Fusion of Experts（FoE）问题，这些专家模型具有对数据分布的互补知识，并将其形式化为监督学习的一个实例。我们的方法适用于判别任务和生成任务，并在图像和文本分类、文本摘要、多项选择问答和生成文本自动评估等方面取得了显著的性能提升。我们还将我们的方法扩展到“节俭”设置，即希望在测试时减少专家模型的评估次数。",
    "tldr": "该论文研究了融合具有互补专长的模型的问题，并将其应用于不同任务和领域中，通过监督学习的方式实现了显著的性能提升。",
    "en_tdlr": "This paper investigates the problem of fusing models with complementary expertise and applies it to various tasks and domains, achieving significant performance improvements through supervised learning."
}