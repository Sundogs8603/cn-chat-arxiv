{
    "title": "Unnatural language processing: How do language models handle machine-generated prompts?. (arXiv:2310.15829v1 [cs.CL])",
    "abstract": "Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model's embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of t",
    "link": "http://arxiv.org/abs/2310.15829",
    "context": "Title: Unnatural language processing: How do language models handle machine-generated prompts?. (arXiv:2310.15829v1 [cs.CL])\nAbstract: Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model's embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of t",
    "path": "papers/23/10/2310.15829.json",
    "total_tokens": 836,
    "translated_title": "非自然语言处理：语言模型处理机器生成的提示的方式。",
    "translated_abstract": "语言模型的提示优化研究显示，语义上和语法上构造良好的手动制定的提示常常被无明显含义或句法结构的自动生成的令牌序列所超越，包括来自模型嵌入空间的向量序列。我们使用机器生成的提示来探索模型对非自然语言输入的响应。我们研究了不同大小模型在多个语义任务中对连续和离散的机器生成提示的行为，并将其与对人工生成的自然语言提示的行为进行了比较。即使产生了类似的输出，机器生成的和人工提示通过网络处理路径引发了不同的响应模式，包括不同的困惑度、注意力和输出熵分布，以及不同的单元激活配置文件。我们初步揭示了提示的性质。",
    "tldr": "通过使用机器生成的提示，研究人员探索了语言模型对非自然语言输入的响应。他们发现，即使产生了类似的输出，机器生成的和人工生成的提示会触发不同的响应模式。这项研究为我们初步揭示了提示的性质。"
}