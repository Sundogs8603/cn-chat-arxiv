{
    "title": "High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])",
    "abstract": "A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of",
    "link": "http://arxiv.org/abs/2310.03624",
    "context": "Title: High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])\nAbstract: A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of",
    "path": "papers/23/10/2310.03624.json",
    "total_tokens": 986,
    "translated_title": "用于机器人自建模和运动规划的高自由度动态神经领域",
    "translated_abstract": "机器人自建模是机器人物理形态的任务无关表示，可用于在没有经典几何运动学模型的情况下进行运动规划任务。特别是当后者很难工程化或机器人的运动学出现意外变化时，无人自建模是真正自主控制机器人的必要功能。在这项工作中，我们利用神经领域来使机器人能够以神经隐式查询模型的形式自建模其运动学，该模型仅通过带有相机姿态和配置的2D图像进行学习。这比现有方法具有更广泛的适用性，因为它不依赖深度图像或几何知识。为此，我们提出了一种基于编码器的新型动态物体中心场景的神经密度场架构，该架构可以适应高自由度（DOFs）的条件。在一个7自由度的机器人测试设置中，学得的自建模与真实模型之间的Chamfer-L2距离仅为2%。",
    "tldr": "本论文介绍了一种使用神经领域进行机器人自建模和运动规划的方法。通过利用2D图像和相机姿态进行学习，无需深度图像或几何知识，实现了对高自由度物体的建模。在7自由度机器人测试中，所学的自建模与真实模型的差距仅为2%。"
}