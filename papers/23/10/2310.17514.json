{
    "title": "The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks. (arXiv:2310.17514v1 [cs.CL])",
    "abstract": "NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compos",
    "link": "http://arxiv.org/abs/2310.17514",
    "context": "Title: The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks. (arXiv:2310.17514v1 [cs.CL])\nAbstract: NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compos",
    "path": "papers/23/10/2310.17514.json",
    "total_tokens": 921,
    "translated_title": "评估结果的有效性：评估组合性基准的一致性",
    "translated_abstract": "最近几年，根据提出的许多数据集，NLP模型取得了巨大的进步。然而，关于特定数据集设计选择如何影响我们对模型能力的结论仍然存在疑问。在本研究中，我们调查了组合泛化领域中这个问题。我们在四个数据集上对六种建模方法进行了性能评估，并根据八种组合性分割策略将模型按照18个组合泛化拆分进行排名。我们的结果表明：i）尽管所有数据集都设计用于评估组合泛化，但它们对建模方法的排名有所不同；ii）由人类生成的数据集彼此之间的一致性比它们与合成数据集的一致性更高，或者比合成数据集之间的一致性更高；iii）通常情况下，数据集是否来自同一来源更能预测结果的模型排名，而不是它们是否保持相同的组合解释。",
    "tldr": "该研究调查了在组合泛化领域中特定数据集设计选择对模型能力结论的影响，发现不同数据集对建模方法的排名存在差异，人类生成的数据集之间的一致性高于合成数据集之间的一致性，数据集是否来自同一来源更能预测结果的模型排名。",
    "en_tdlr": "This study investigates the impact of specific dataset design choices on the conclusions about model capabilities in the field of compositional generalization. The results show variations in ranking modeling approaches across different datasets, higher consistency among datasets generated by humans compared to synthetic datasets, and the predictive power of whether datasets come from the same source on model rankings."
}