{
    "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
    "abstract": "arXiv:2310.16049v2 Announce Type: replace  Abstract: While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains",
    "link": "https://arxiv.org/abs/2310.16049",
    "context": "Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\nAbstract: arXiv:2310.16049v2 Announce Type: replace  Abstract: While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains",
    "path": "papers/23/10/2310.16049.json",
    "total_tokens": 929,
    "translated_title": "MuSR: 用多步软推理测试思维链的极限",
    "translated_abstract": "虽然装备了诸如思维链提示等技术的大型语言模型(LLMs)展示出了令人印象深刻的能力，但它们在复杂环境中进行鲁棒推理的能力仍然有所欠缺。然而，评估LLM推理是具有挑战性的，因为系统能力不断增长，而用于逻辑推理等任务的基准数据集仍然保持不变。我们引入了MuSR，这是一个用于评估语言模型在自然语言叙事中执行多步软推理任务的数据集。该数据集具有两个关键特征。首先，它是通过一种新颖的神经符号综合到自然语言生成算法创建的，使得能够构建挑战GPT-4的复杂推理实例(例如，大约1000字长的谋杀悬疑故事)，并且随着发行更有能力的LLM，它可以进一步扩展。其次，我们的数据集实例是对应于真实领域的自由文本叙事。",
    "tldr": "MuSR引入了一个用于评估语言模型在自然语言叙事中进行多步软推理任务的数据集，通过新颖的神经符号综合到自然语言生成算法创建复杂推理实例，挑战了目前的语言模型能力，并且可以随着模型能力的增强进行进一步扩展。",
    "en_tdlr": "MuSR introduces a dataset for evaluating language models on multistep soft reasoning tasks in natural language narratives, created through a novel neurosymbolic synthetic-to-natural generation algorithm to challenge current language model capabilities and allow for further scalability as models improve."
}