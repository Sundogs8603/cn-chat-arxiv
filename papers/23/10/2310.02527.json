{
    "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])",
    "abstract": "The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to ",
    "link": "http://arxiv.org/abs/2310.02527",
    "context": "Title: CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])\nAbstract: The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to ",
    "path": "papers/23/10/2310.02527.json",
    "total_tokens": 959,
    "translated_title": "CITING: 大型语言模型通过课程设计进行指导调优",
    "translated_abstract": "最近大型语言模型（LLMs）的进展是通过指导调优和人工对齐的结合实现的。然而，构建手工制作的指导数据集和进行人工对齐成为了扩展LLMs开发的瓶颈。在本文中，我们利用AI模型代替人类作为教师来训练学生LLMs的方法。我们的方法灵感来自于人类学生通过遵循评分标准和从导师提供的修改中学习来提高写作技巧。具体来说，我们使用教师LLM来为学生LLM的指导调优创建课程，即Curriculum Instruction TunING (CITING)。该方法包括两个主要步骤：（1）教师LLM制定评估各种类型问题答案的评分标准；（2）学生LLM学习遵循评分标准并通过教师的修改进行自我纠正。我们进一步迭代进行这个过程。",
    "tldr": "本文提出了一种利用大型语言模型作为指导教师来训练学生语言模型的方法，通过设计课程来进行指导调优，称为CITING。该方法通过教师模型制定评分标准，学生模型通过遵循评分标准和自我纠正进行学习。该方法可以解决手工制作指导数据集和人工对齐的瓶颈问题。",
    "en_tdlr": "This paper proposes a method for training student language models using large language models as instructive teachers, by designing a curriculum for instruction tuning called CITING. The method involves the teacher model creating rubrics for evaluating answers and the student model learning to follow these rubrics and self-correct. This approach addresses the bottleneck of manually crafting instruction datasets and performing human alignment."
}