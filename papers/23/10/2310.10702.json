{
    "title": "Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])",
    "abstract": "Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data",
    "link": "http://arxiv.org/abs/2310.10702",
    "context": "Title: Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])\nAbstract: Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data",
    "path": "papers/23/10/2310.10702.json",
    "total_tokens": 881,
    "translated_title": "透明的基于概念解释的异常检测",
    "translated_abstract": "深度学习技术的进步提升了异常检测的性能。然而，现实世界和安全关键应用需要超出准确性的透明度和推理能力。异常检测的任务集中在找出给定样本是否遵循学习到的分布。现有方法缺乏对其结果进行清晰解释的能力。因此，为了克服这一挑战，我们提出了透明的异常检测概念解释（ACE）方法。ACE能够以概念的形式提供人类可解释的解释和异常预测。据我所知，这是第一篇提出设计可解释异常检测的论文。除了促进异常检测的透明度，它还可以实现有效的人机交互。我们提出的模型结果要么更高，要么与黑盒不可解释模型相当。我们验证了ACE在三个现实数据集上的性能。",
    "tldr": "本论文提出了一种透明的基于概念解释的异常检测方法（ACE），能够提供人类可解释的解释和异常预测。该方法在推进异常检测的透明度的同时，实现了有效的人机交互，并且在性能上要么更高，要么与黑盒不可解释模型相当。",
    "en_tdlr": "This paper presents ACE, a transparent anomaly detection method that provides human-interpretable explanations and anomaly predictions. It promotes transparency in anomaly detection, enables effective human-model interaction, and achieves performance either higher or comparable to black-box uninterpretable models."
}