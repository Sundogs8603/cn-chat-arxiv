{
    "title": "Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer. (arXiv:2310.19902v1 [cs.AI])",
    "abstract": "Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despit",
    "link": "http://arxiv.org/abs/2310.19902",
    "context": "Title: Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer. (arXiv:2310.19902v1 [cs.AI])\nAbstract: Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despit",
    "path": "papers/23/10/2310.19902.json",
    "total_tokens": 827,
    "translated_title": "Herd：通过智能组合器使用多个较小的LLM来与专有的大型LLM达到相同的性能匹配",
    "translated_abstract": "目前存在超过一千个多功能的LLM，可以执行实际任务，包括问答、文本摘要、内容生成等。然而，免费模型的可访问性、规模和可靠性限制了它们在日常使用中的广泛应用。为了解决访问和规模的问题，像HuggingFace这样的组织已经创建了模型仓库，用户可以上传已经过训练的模型权重和量化版本，以及描述训练过程的模型卡片。尽管一些模型报告了常用基准测试的性能，但并非所有模型都如此，并且在性能与模型部署成本之间进行权衡的真实世界影响并不清楚。在这里，我们展示了开源模型的群可以通过智能路由器达到或超过专有模型的性能。我们证明了一个开源模型的群能够达到ChatGPT的准确性。",
    "tldr": "使用智能组合器，一群开源模型可以达到或超过专有模型的性能。",
    "en_tdlr": "Using an intelligent router, a herd of open-source models can match or exceed the performance of proprietary models."
}