{
    "title": "A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning. (arXiv:2310.18988v1 [stat.ML])",
    "abstract": "Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-genera",
    "link": "http://arxiv.org/abs/2310.18988",
    "context": "Title: A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning. (arXiv:2310.18988v1 [stat.ML])\nAbstract: Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-genera",
    "path": "papers/23/10/2310.18988.json",
    "total_tokens": 865,
    "translated_title": "对统计学习中参数计数的重新思考：对双下降的转变",
    "translated_abstract": "传统的统计学智慧确立了模型复杂度和预测误差之间的关系，通常以一个U形曲线来表示，反映了欠拟合和过拟合之间的转变。然而，受到过参数化神经网络的成功的启发，最近有一些有影响力的工作认为这个理论通常是不完整的，并引入了一个额外的区域，即在参数个数p超过样本大小n时，测试误差会出现第二次下降的现象，被称为双下降。虽然大部分关注自然而然地集中在深度学习的设置上，但双下降现象已经在非神经网络模型中更一般地出现，已知的案例包括线性回归、树和Boosting。在这项工作中，我们对围绕这些更经典的统计机器学习方法的证据进行了更详细的分析，并质疑了双下降现象扩展了传统U形复杂度-泛类关系的界限的说法。",
    "tldr": "本研究重新思考了统计学习中参数计数的理论，挑战了双下降现象扩展传统复杂度-泛类关系界限的观点。",
    "en_tdlr": "This work rethinks the theory of parameter counting in statistical learning, challenging the idea that the double descent phenomenon extends the traditional complexity-generalization relationship."
}