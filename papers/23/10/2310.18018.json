{
    "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. (arXiv:2310.18018v1 [cs.CL])",
    "abstract": "In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromise",
    "link": "http://arxiv.org/abs/2310.18018",
    "context": "Title: NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. (arXiv:2310.18018v1 [cs.CL])\nAbstract: In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromise",
    "path": "papers/23/10/2310.18018.json",
    "total_tokens": 1018,
    "translated_title": "NLP评估遇到麻烦：需要针对每个基准测试测量LLM数据污染问题",
    "translated_abstract": "在这篇立场论文中，我们认为使用带有注释的基准测试对自然语言处理（NLP）任务进行经典评估存在问题。最严重的数据污染发生在使用基准测试的测试集对一个大型语言模型（LLM）进行训练，并在同一基准测试中进行评估的情况下。该问题的程度尚不清楚，因为很难直接衡量。污染会导致在目标基准测试和相关任务中，受到污染的模型的性能被高估，与未受污染的对应模型相比。后果可能非常严重，会出现错误的科学结论被发表，而其他正确的结论被忽视。本立场论文定义了不同级别的数据污染，并提倡社区共同努力，包括开发自动和半自动的测量方法来检测基准测试数据是否暴露给了模型，并提出了标记具有妥协性结论的论文的建议。",
    "tldr": "这篇立场论文指出NLP任务的经典评估存在数据污染的问题，尤其是当一个LLM模型在基准测试的测试集上进行训练和评估时。该问题导致污染模型在目标基准测试和任务中的性能被高估，可能会产生错误的科学结论。建议开发自动和半自动的测量方法来检测数据污染，并提醒评审论文时要注意具有妥协性结论的论文。",
    "en_tdlr": "This position paper highlights the issue of data contamination in NLP evaluation, particularly when a LLM model is trained and evaluated on the same benchmark. It emphasizes the need for measuring and detecting data contamination, as it can lead to overestimation of model performance and publication of incorrect scientific conclusions. The paper calls for community efforts to develop automatic and semi-automatic measures and suggests flagging papers with compromised conclusions during peer reviews."
}