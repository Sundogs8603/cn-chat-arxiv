{
    "title": "Certifiably Robust Graph Contrastive Learning. (arXiv:2310.03312v1 [cs.CR])",
    "abstract": "Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any G",
    "link": "http://arxiv.org/abs/2310.03312",
    "context": "Title: Certifiably Robust Graph Contrastive Learning. (arXiv:2310.03312v1 [cs.CR])\nAbstract: Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any G",
    "path": "papers/23/10/2310.03312.json",
    "total_tokens": 953,
    "translated_title": "可证明鲁棒的图对比学习",
    "translated_abstract": "图对比学习（GCL）作为一种流行的无监督图表示学习方法已经出现。然而，已经表明GCL对图结构和节点属性的对抗性攻击是脆弱的。尽管已经提出了经验性方法来增强GCL的鲁棒性，但GCL的可证明鲁棒性仍未被探索。在本文中，我们在GCL中开发了第一个可证明鲁棒的框架。具体而言，我们首先提出了一个统一的评估和证明GCL鲁棒性的标准。然后，我们引入了一种新的技术，即随机边缘平滑（RES），以确保任何GCL模型的可证明鲁棒性，并且这种证明的鲁棒性可以被保留在下游任务中。此外，我们提出了一种用于鲁棒GCL的有效训练方法。对真实世界数据集的大量实验表明了我们所提出的方法在提供有效的可证明鲁棒性和增强任何GCL模型的鲁棒性方面的有效性。",
    "tldr": "本文提出了第一个可证明鲁棒的图对比学习（GCL）框架，并引入了随机边缘平滑（RES）技术，以确保任何GCL模型的可证明鲁棒性，并通过有效的训练方法增强了GCL的鲁棒性。",
    "en_tdlr": "This paper proposes the first certifiably robust framework for Graph Contrastive Learning (GCL), introducing the Randomized Edgedrop Smoothing (RES) technique to ensure the certifiable robustness of any GCL model, and enhancing the robustness through effective training methods."
}