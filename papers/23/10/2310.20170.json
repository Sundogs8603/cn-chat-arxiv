{
    "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving informat",
    "link": "http://arxiv.org/abs/2310.20170",
    "context": "Title: DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving informat",
    "path": "papers/23/10/2310.20170.json",
    "total_tokens": 883,
    "translated_title": "DIVKNOWQA: 通过知识库和文本进行开放领域问答来评估LLMs的推理能力",
    "translated_abstract": "大型语言模型（LLMs）展现了令人印象深刻的生成能力，但是当仅仅依赖其内部知识时，它们容易出现幻觉，尤其是在回答需要少见知识的问题时。基于检索的LLMs已经成为将LLMs牢固根植于外部知识的潜在解决方案。然而，最近的方法主要强调从非结构化文本语料库中检索，因为这种方法可以无缝集成到提示中。当使用结构化数据（如知识图谱）时，大多数方法会将其简化为自然文本，忽略了底层结构。此外，当前领域中存在一个重要的缺口，即缺乏一个真实的基准来评估将LLMs与异质知识源（如知识库和文本）连接起来的有效性。为了填补这个缺口，我们创建了一个综合数据集，提出了两个独特的挑战：（1）需要检索信息的两跳多源问题。",
    "tldr": "DIVKNOWQA提出了一个综合数据集，评估LLMs在连接异构知识源上的推理能力，填补了现有领域中的一个空白。",
    "en_tdlr": "DIVKNOWQA introduces a comprehensive dataset to evaluate the reasoning ability of LLMs in connecting heterogeneous knowledge sources, filling a gap in the current landscape."
}