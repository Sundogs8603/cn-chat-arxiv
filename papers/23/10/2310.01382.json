{
    "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
    "abstract": "arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and ",
    "link": "https://arxiv.org/abs/2310.01382",
    "context": "Title: Compressing LLMs: The Truth is Rarely Pure and Never Simple\nAbstract: arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and ",
    "path": "papers/23/10/2310.01382.json",
    "total_tokens": 926,
    "translated_title": "压缩LLM：真相很少纯粹，也绝不简单",
    "translated_abstract": "尽管现代大型语言模型（LLMs）取得了显著成就，但面临着巨大的计算和内存占用。 最近，几项工作显示出在无需训练和数据的情况下对LLMs进行压缩（修剪和量化）取得了显著成功，达到了50-60％的稀疏度，并将位宽减小到每个权重3或4位，并且与未压缩基线相比，困惑度的降低可以忽略不计。 随着最近研究工作集中在开发越来越复杂的压缩方法上，我们的工作退一步重新评估了现有SoTA压缩方法的有效性，这些方法依赖于一种相当简单且广受质疑的度量标准，困惑度（即使对于稠密的LLMs）。 我们引入了知识密集型压缩的LLM基准（LLM-KICK），这是一个精心策划的任务集合，用于重新定义对压缩LLMs的评估协议，这些LLMs与其稠密对应物有显著的对齐性，",
    "tldr": "本研究重新评估了现有最先进的压缩LLM方法对稠密LLM的有效性，并引入了一个新的压缩LLM基准来重新定义评估协议。",
    "en_tdlr": "This study reevaluates the effectiveness of existing state-of-the-art compression methods for LLMs and introduces a new benchmark for compressed LLMs to redefine the evaluation protocol."
}