{
    "title": "Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])",
    "abstract": "Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t",
    "link": "http://arxiv.org/abs/2310.07301",
    "context": "Title: Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions. (arXiv:2310.07301v1 [cs.CL])\nAbstract: Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We t",
    "path": "papers/23/10/2310.07301.json",
    "total_tokens": 888,
    "translated_title": "Parrot:通过学习提问来增强多轮聊天模型",
    "translated_abstract": "最近，基于大型语言模型的聊天模型取得了令人印象深刻的进展；然而，在开源聊天模型（如Alpaca和Vicuna）与领先的聊天模型（如ChatGPT和GPT-4）之间存在明显的多轮对话滞后。通过一系列的分析，我们将滞后归因于缺乏足够高质量的多轮指令调优数据。社区提供的调优数据要么是单轮会话，要么是存在某些问题的多轮会话，例如非人类的指令，响应不够详细，或者很少出现主题转换。本文通过引入Parrot，一个高度可扩展的解决方案，来解决这些挑战，该解决方案旨在自动生成高质量的指令调优数据，然后用于增强多轮聊天模型在对话中的效果。具体而言，我们首先训练Parrot-Ask模型，该模型旨在模拟真实用户生成指令。",
    "tldr": "本文提出了Parrot，一个高度可扩展的解决方案，通过自动生成高质量的指令调优数据，进一步完善了多轮聊天模型在对话中的效果。",
    "en_tdlr": "This paper proposes Parrot, a highly scalable solution that enhances multi-turn chat models by automatically generating high-quality instruction-tuning data, thereby improving the effectiveness of chat models in conversations."
}