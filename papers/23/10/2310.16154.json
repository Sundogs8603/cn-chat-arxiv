{
    "title": "Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations. (arXiv:2310.16154v1 [cs.LG])",
    "abstract": "Artificial intelligence, particularly the subfield of machine learning, has seen a paradigm shift towards data-driven models that learn from and adapt to data. This has resulted in unprecedented advancements in various domains such as natural language processing and computer vision, largely attributed to deep learning, a special class of machine learning models. Deep learning arguably surpasses traditional approaches by learning the relevant features from raw data through a series of computational layers.  This thesis explores the theoretical foundations of deep learning by studying the relationship between the architecture of these models and the inherent structures found within the data they process. In particular, we ask What drives the efficacy of deep learning algorithms and allows them to beat the so-called curse of dimensionality-i.e. the difficulty of generally learning functions in high dimensions due to the exponentially increasing need for data points with increased dimensio",
    "link": "http://arxiv.org/abs/2310.16154",
    "context": "Title: Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations. (arXiv:2310.16154v1 [cs.LG])\nAbstract: Artificial intelligence, particularly the subfield of machine learning, has seen a paradigm shift towards data-driven models that learn from and adapt to data. This has resulted in unprecedented advancements in various domains such as natural language processing and computer vision, largely attributed to deep learning, a special class of machine learning models. Deep learning arguably surpasses traditional approaches by learning the relevant features from raw data through a series of computational layers.  This thesis explores the theoretical foundations of deep learning by studying the relationship between the architecture of these models and the inherent structures found within the data they process. In particular, we ask What drives the efficacy of deep learning algorithms and allows them to beat the so-called curse of dimensionality-i.e. the difficulty of generally learning functions in high dimensions due to the exponentially increasing need for data points with increased dimensio",
    "path": "papers/23/10/2310.16154.json",
    "total_tokens": 892,
    "translated_title": "通过学习不变表示打破深度神经网络中的维度诅咒",
    "translated_abstract": "人工智能，特别是机器学习领域，已经经历了从数据驱动模型到基于数据学习和适应的范式转变。这导致了在自然语言处理和计算机视觉等各个领域取得了前所未有的进展，这主要归功于深度学习，一种特殊的机器学习模型。深度学习通过一系列计算层从原始数据中学习相关特征，可以说超越了传统方法。本论文通过研究这些模型的结构和处理的数据中固有结构之间的关系，探索了深度学习的理论基础。特别地，我们问：是什么使得深度学习算法有效，并使其能够战胜所谓的维度诅咒——即由于维度增加导致对数据点的指数级需求增加，从而在高维度中通常学习函数变得困难。",
    "tldr": "本论文研究了深度学习模型的结构与处理数据中固有结构之间的关系，探索了深度学习的理论基础，旨在打破所谓的维度诅咒，并理解深度学习算法的有效性和其超越传统方法的原因。",
    "en_tdlr": "This paper investigates the relationship between the architecture of deep learning models and the inherent structures in the data they process, aiming to break the curse of dimensionality and understand the effectiveness of deep learning algorithms and their superiority over traditional methods."
}