{
    "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
    "abstract": "arXiv:2310.05737v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representati",
    "link": "https://arxiv.org/abs/2310.05737",
    "context": "Title: Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation\nAbstract: arXiv:2310.05737v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representati",
    "path": "papers/23/10/2310.05737.json",
    "total_tokens": 879,
    "translated_title": "语言模型击败扩散模型--分词器是视觉生成的关键",
    "translated_abstract": "大型语言模型(LLMs)是语言生成任务中的主导模型，但在图像和视频生成方面表现不如扩散模型。为了有效地利用LLMs进行视觉生成，一个至关重要的组件是视觉分词器，它将像素空间输入映射到适合LLM学习的离散标记中。本文介绍了MAGVIT-v2，一个视频分词器，旨在使用共同的标记词汇为视频和图像生成简洁和富有表现力的标记。配备了这个新的分词器，我们展示了LLMs在标准图像和视频生成基准上优于扩散模型，包括ImageNet和Kinetics。此外，我们证明了我们的分词器在两项任务上超过了先前表现最佳的视频分词器：(1)根据人类评估，视频压缩与下一代视频编解码器(VCC)相媲美，(2)学习有效的表示。",
    "tldr": "分词器是视觉生成的关键，新的视频分词器MAGVIT-v2使得大型语言模型LLMs在图像和视频生成任务上胜过扩散模型，并在视频压缩和有效表示学习方面表现优异。",
    "en_tdlr": "The key to visual generation is the tokenizer, and the new video tokenizer MAGVIT-v2 enables Large Language Models (LLMs) to outperform diffusion models in image and video generation tasks, excelling in video compression and effective representation learning."
}