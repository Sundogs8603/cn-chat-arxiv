{
    "title": "In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. (arXiv:2310.13220v1 [cs.LG])",
    "abstract": "Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the f",
    "link": "http://arxiv.org/abs/2310.13220",
    "context": "Title: In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. (arXiv:2310.13220v1 [cs.LG])\nAbstract: Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the f",
    "path": "papers/23/10/2310.13220.json",
    "total_tokens": 907,
    "translated_title": "使用Transformer的上下文学习与对比学习模式是等价的",
    "translated_abstract": "基于Transformer的预训练大型语言模型展示了惊人的上下文学习能力。在给定几个示例的情况下，模型可以在不进行任何参数更新的情况下执行新任务。然而，理解上下文学习的机制仍然是一个未解决的问题。在本文中，我们将上下文学习的推理过程解释为对比学习模式中的梯度下降过程。首先，利用核方法建立梯度下降与自注意机制之间的关系，在一般使用的softmax注意设置下而不是线性注意设置下。然后，我们从对比学习的角度分析了上下文学习的对应梯度下降过程，讨论了在这种对比学习模式下可能的改进，基于这些改进可以进一步修改自注意层。最后，我们设计了实验来支持我们的观点。据我们所知，我们的工作是第一个将上下文学习与对比学习模式等价的研究。",
    "tldr": "本文将上下文学习的推理过程解释为对比学习模式中的梯度下降过程，通过建立梯度下降与自注意机制之间的关系，并分析了对应梯度下降过程，提出了可能的改进，并设计实验证明了这一观点。",
    "en_tdlr": "This paper interprets the inference process of in-context learning as a gradient descent process in a contrastive learning pattern. It establishes the relationship between gradient descent and self-attention mechanism, analyzes the corresponding gradient descent process, proposes possible improvements, and designs experiments to support the viewpoint."
}