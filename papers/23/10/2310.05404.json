{
    "title": "Exploring the Maze of Multilingual Modeling",
    "abstract": "Multilingual language models have gained significant attention in recent years, enabling the development of applications that meet diverse linguistic contexts. In this paper, we present a comprehensive evaluation of three popular multilingual language models: mBERT, XLM-R, and GPT-3. We assess their performance across a diverse set of languages, with a focus on understanding the impact of resource availability (general and model-specific), language family, script type, and word order on model performance, under two distinct tasks - text classification and text generation. Our findings reveal that while the amount of language-specific pretraining data plays a crucial role in model performance, we also identify other factors such as general resource availability, language family, and script type, as important features. We hope that our study contributes to a deeper understanding of multilingual language models to enhance their performance across languages and linguistic contexts.",
    "link": "https://arxiv.org/abs/2310.05404",
    "context": "Title: Exploring the Maze of Multilingual Modeling\nAbstract: Multilingual language models have gained significant attention in recent years, enabling the development of applications that meet diverse linguistic contexts. In this paper, we present a comprehensive evaluation of three popular multilingual language models: mBERT, XLM-R, and GPT-3. We assess their performance across a diverse set of languages, with a focus on understanding the impact of resource availability (general and model-specific), language family, script type, and word order on model performance, under two distinct tasks - text classification and text generation. Our findings reveal that while the amount of language-specific pretraining data plays a crucial role in model performance, we also identify other factors such as general resource availability, language family, and script type, as important features. We hope that our study contributes to a deeper understanding of multilingual language models to enhance their performance across languages and linguistic contexts.",
    "path": "papers/23/10/2310.05404.json",
    "total_tokens": 820,
    "translated_title": "探索多语言建模的迷宫",
    "translated_abstract": "近年来，多语言语言模型引起了极大关注，可以开发适应不同语言环境的应用。本文对三种流行的多语言语言模型（mBERT、XLM-R和GPT-3）进行了全面评估。我们评估了它们在不同语言上的性能，重点研究了资源可用性（通用和模型特定）、语言家族、脚本类型和词序对模型性能的影响，针对两种不同任务-文本分类和文本生成。我们的研究结果表明，语言特定预训练数据的数量在模型性能中起着关键作用，同时我们也确定了其他因素，如通用资源可用性、语言家族和脚本类型的重要性。我们希望我们的研究能够加深对多语言语言模型的理解，以提高它们在不同语言和语言环境中的性能。",
    "tldr": "本文综合评估了mBERT、XLM-R和GPT-3等三种流行的多语言语言模型在不同语言上的性能，并研究了资源可用性、语言家族、脚本类型和词序等因素对模型性能的影响。研究结果表明，语言特定预训练数据的数量对模型性能至关重要，同时还发现了其他重要因素。"
}