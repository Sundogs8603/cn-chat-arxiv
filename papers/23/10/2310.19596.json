{
    "title": "LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)",
    "abstract": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu",
    "link": "http://arxiv.org/abs/2310.19596",
    "context": "Title: LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)\nAbstract: Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu",
    "path": "papers/23/10/2310.19596.json",
    "total_tokens": 906,
    "translated_title": "LLMaAA: 将大型语言模型作为主动批注器",
    "translated_abstract": "在自然语言处理（NLP）中，普遍的监督学习方法因需求大量高质量标注数据而声名狼藉。实际上，获取这样的数据是一项昂贵的事业。最近，大型语言模型（LLM）出色的少样本性能推动了数据集生成的发展，其中训练数据仅从LLM中合成。然而，这种方法通常存在质量低的问题，并且需要多个数量级的已标记数据才能实现令人满意的性能。为了充分发挥LLM的潜力并利用大量未标记数据，我们提出了LLMaAA，它将LLM作为批注器，并将它们放入主动学习循环中以高效确定批注内容。为了利用伪标签进行稳健学习，我们优化了批注和训练过程：（1）我们从小的示范池中抽取k-NN示例作为上下文示例，（2）我们采用示例加权技术。",
    "tldr": "LLMaAA是一种利用大型语言模型作为主动批注器的方法，通过在主动学习循环中使用LLM确定高效批注内容，以最大限度地利用LLM的潜力并利用大量未标记数据。",
    "en_tdlr": "LLMaAA is a method that utilizes large language models as active annotators to determine efficient annotation content in an active learning loop, aiming to fully exploit the potential of LLMs and make use of massive unlabeled data."
}