{
    "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models. (arXiv:2310.05015v2 [cs.AI] UPDATED)",
    "abstract": "Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we furthe",
    "link": "http://arxiv.org/abs/2310.05015",
    "context": "Title: Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models. (arXiv:2310.05015v2 [cs.AI] UPDATED)\nAbstract: Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we furthe",
    "path": "papers/23/10/2310.05015.json",
    "total_tokens": 871,
    "translated_title": "Compresso：结构化修剪与协作促进学习紧凑大型语言模型",
    "translated_abstract": "尽管大型语言模型（LLMs）取得了显着的成功，但庞大的模型尺寸给资源受限硬件带来了重大部署挑战。尽管现有的LLM压缩方法关注量化，但修剪在训练成本高和数据收集困难等方面相对未被探索。单次修剪方法虽然成本低且无需数据，已成为LLM修剪的主导方式，但在结构化修剪设置下会导致性能下降。在这项工作中，我们引入了一种新的结构化修剪LLMs的范例，称为Compresso。我们的方法通过提出的资源高效修剪算法和LLM自身的协作，在训练过程中学习最优的修剪决策。Compresso通过在指令调整过程中将低秩适应（LoRA）与$L_0$正则化相结合，解决了昂贵的培训成本和数据收集的挑战。",
    "tldr": "Compresso是一种结构化修剪LLMs的新方法，通过协作促进学习最优的修剪决策，解决了训练成本高和数据收集困难的挑战。",
    "en_tdlr": "Compresso is a new method for structurally pruning LLMs that learns optimal pruning decisions through collaboration, addressing the challenges of high training costs and data collection."
}