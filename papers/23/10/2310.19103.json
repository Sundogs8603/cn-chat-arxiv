{
    "title": "Proving Linear Mode Connectivity of Neural Networks via Optimal Transport",
    "abstract": "arXiv:2310.19103v2 Announce Type: replace  Abstract: The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension ",
    "link": "https://arxiv.org/abs/2310.19103",
    "context": "Title: Proving Linear Mode Connectivity of Neural Networks via Optimal Transport\nAbstract: arXiv:2310.19103v2 Announce Type: replace  Abstract: The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension ",
    "path": "papers/23/10/2310.19103.json",
    "total_tokens": 838,
    "translated_title": "通过最优输运证明神经网络的线性模态连通性",
    "translated_abstract": "高维非凸优化问题的能量景观对于理解现代深度神经网络架构的有效性至关重要。最近的研究实验证明，在随机训练的两次运行之后找到的两个不同解通常可以通过非常简单的连续路径（如线性路径）相连，除了权重的排列。本文提供了一个理论框架，用于理论上解释这一经验观察。基于经验度量的Wasserstein距离的收敛速率，我们展示了在很高的概率下，使用随机梯度下降训练的两个足够宽的两层神经网络之间是线性连接的。此外，我们还表达了具有独立神经元权重的两个深度神经网络每层宽度线性连接的上下界。最后，我们通过展示如何通过保持维度来实证我们的方法的有效性。",
    "tldr": "证明了通过最优输运的方法，使用随机梯度下降训练的两层神经网络可以线性连接，同时提供了每层神经元权重独立的深度神经网络线性连接的上下界。"
}