{
    "title": "Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])",
    "abstract": "Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as",
    "link": "http://arxiv.org/abs/2310.11593",
    "context": "Title: Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])\nAbstract: Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as",
    "path": "papers/23/10/2310.11593.json",
    "total_tokens": 936,
    "translated_title": "使用大型语言模型自动评价个性化文本生成",
    "translated_abstract": "个性化文本生成提供了一种针对用户个人背景交付内容的专门机制。尽管在这个领域的研究进展迅速，但评估仍然是一个挑战。传统的自动评价指标（如BLEU和ROUGE）主要衡量与人工参考文本的词汇相似度，并不能区分个性化与其他微妙的语义方面，因此无法捕捉个性化生成内容质量的细微差别。另一方面，人工判断是昂贵的，特别是在个性化评估领域。受到这些挑战的启发，我们探索了使用大型语言模型（LLMs）来评估个性化文本生成，并检验它们理解细致的用户背景的能力。我们提出了AuPEL，一种新颖的评估方法，将生成文本的个性化、质量和相关性三个主要语义方面提取并自动测量。",
    "tldr": "这项研究提出了一种使用大型语言模型自动评价个性化文本生成的方法。传统的自动评价指标无法捕捉个性化质量的微妙差别，而人工判断又昂贵且困难。因此，本研究提出了一种新颖的评估方法，能够自动测量个性化、质量和相关性这三个重要语义方面。"
}