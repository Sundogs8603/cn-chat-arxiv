{
    "title": "Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])",
    "abstract": "In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.",
    "link": "http://arxiv.org/abs/2305.03938",
    "context": "Title: Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])\nAbstract: In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.",
    "path": "papers/23/05/2305.03938.json",
    "total_tokens": 894,
    "translated_title": "Adam家族算法在无平滑优化中的收敛性保证研究",
    "translated_abstract": "本文对Adam家族算法在无平滑优化中的收敛性进行了全面研究，特别是在无平滑神经网络的训练中。我们提出了一种新的双时间尺度框架，采用双时间尺度更新方案，证明了其在温和条件下的收敛性。我们的框架包括了各种流行的Adam家族算法，在训练无平滑神经网络中提供了收敛性保证。此外，我们还开发了随机次梯度方法，结合梯度裁剪技术，用于训练具有重尾噪声的无平滑神经网络。通过我们的框架，我们展示了我们提出的方法甚至在仅假定评估噪声可积的情况下也会收敛。广泛的数值实验证明了我们提出的方法的高效性和稳健性。",
    "tldr": "本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。",
    "en_tdlr": "This paper proposes a novel two-timescale framework with convergence guarantees for Adam-family methods in nonsmooth optimization, especially in the training of nonsmooth neural networks. It provides convergence guarantees for popular Adam-family methods and develops stochastic subgradient methods that incorporate gradient clipping techniques to handle heavy-tailed noise. Extensive numerical experiments demonstrate the efficiency and robustness of the proposed methods."
}