{
    "title": "Meta-Learning Online Adaptation of Language Models. (arXiv:2305.15076v2 [cs.CL] UPDATED)",
    "abstract": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective \"shelf life.\" While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gr",
    "link": "http://arxiv.org/abs/2305.15076",
    "context": "Title: Meta-Learning Online Adaptation of Language Models. (arXiv:2305.15076v2 [cs.CL] UPDATED)\nAbstract: Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective \"shelf life.\" While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gr",
    "path": "papers/23/05/2305.15076.json",
    "total_tokens": 880,
    "translated_title": "Meta-Learning Online Adaptation of Language Models. （arXiv:2305.15076v2 [cs.CL] 更新）",
    "translated_abstract": "大型语言模型在其参数中编码了令人印象深刻的广泛世界知识。然而，静态语言模型中的知识很快过时，限制了模型的有效 \"货架寿命\"。虽然在线微调可以减轻这种退化，但我们发现简单地在一系列文档上进行微调会导致信息吸收水平较低。我们假设在线微调没有充分关注重要信息。也就是说，用于表示事实信息的重要标记的梯度信号被从本质上嘈杂的标记的梯度淹没，这表明动态的、上下文感知的学习率可能是有益的。因此，我们提出了学习如何增加权重的标记。我们元训练一个小的自回归模型，以在在线微调过程中重新调整每个标记的语言模型损失，其目标是最大化过时的基础问答模型在单个加权梯度之后对文档的问题回答能力。",
    "tldr": "提出了一种元学习方法，通过学习如何分配语言模型损失的权重，实现在在线微调过程中延长语言模型的有效时间，并提升问题回答能力。"
}