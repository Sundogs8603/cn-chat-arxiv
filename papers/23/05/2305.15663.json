{
    "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR. (arXiv:2305.15663v1 [cs.CL])",
    "abstract": "End-to-end models with large capacity have significantly improved multilingual automatic speech recognition, but their computation cost poses challenges for on-device applications. We propose a streaming truly multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn to only activate a subset of parameters in training and inference. The MoE layer consists of a softmax gate which chooses the best two experts among many in forward propagation. The proposed MoE layer offers efficient inference by activating a fixed number of parameters as the number of experts increases. We evaluate the proposed model on a set of 12 languages, and achieve an average 11.9% relative improvement in WER over the baseline. Compared to an adapter model using ground truth information, our MoE model achieves similar WER and activates similar number of parameters but without any language information. We further show around 3% relative WER improvement by multilingual shallow fusion.",
    "link": "http://arxiv.org/abs/2305.15663",
    "context": "Title: Mixture-of-Expert Conformer for Streaming Multilingual ASR. (arXiv:2305.15663v1 [cs.CL])\nAbstract: End-to-end models with large capacity have significantly improved multilingual automatic speech recognition, but their computation cost poses challenges for on-device applications. We propose a streaming truly multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn to only activate a subset of parameters in training and inference. The MoE layer consists of a softmax gate which chooses the best two experts among many in forward propagation. The proposed MoE layer offers efficient inference by activating a fixed number of parameters as the number of experts increases. We evaluate the proposed model on a set of 12 languages, and achieve an average 11.9% relative improvement in WER over the baseline. Compared to an adapter model using ground truth information, our MoE model achieves similar WER and activates similar number of parameters but without any language information. We further show around 3% relative WER improvement by multilingual shallow fusion.",
    "path": "papers/23/05/2305.15663.json",
    "total_tokens": 1031,
    "translated_title": "流式多语言自动语音识别中的专家混合Conformer模型",
    "translated_abstract": "大容量的端到端模型已经显著提高了多语种自动语音识别的性能，但它们的计算成本对于设备应用来说仍然具有挑战性。我们提出了一种混合专家（MoE）层的流式真正多语言Conformer，该层能够在训练和推理过程中学习仅激活子集参数。MoE层包括一个softmax门，该门在前馈传播中选择多个专家中的最佳两个。所提出的MoE层通过激活固定数量的参数来提供高效的推理，随着专家数量的增加，激活的参数数量也会增加。我们在12种语言的数据集上对所提出的模型进行评估，并获得相对基线的平均11.9％的识别错误率改进。与使用基准信息的适配器模型相比，我们的MoE模型实现了类似的识别错误率，并且激活了相似数量的参数，但不需要任何语言信息。我们进一步展示了多语言浅融合约3％的相对识别错误率改进。",
    "tldr": "本文提出了一种流式多语言Conformer模型，引入了混合专家层，能够在训练和推理过程中学习仅激活子集参数，实现了高效的推理。与基准模型相比具有显著的WRE性能提升而与适配器模型相比具有类似的性能，无需语言信息，同时利用多语言浅融合还实现了进一步的性能提升。",
    "en_tdlr": "This paper proposes a streaming multilingual Conformer model with mixture-of-expert layer, which can efficiently activate a subset of parameters in training and inference. Compared to the baseline, the proposed model achieved an average 11.9% relative improvement in WER, and achieved similar performance without any language information compared to adapter model. Furthermore, multilingual shallow fusion leads to around 3% relative improvement in WER."
}