{
    "title": "On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])",
    "abstract": "Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and ",
    "link": "http://arxiv.org/abs/2305.15734",
    "context": "Title: On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])\nAbstract: Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and ",
    "path": "papers/23/05/2305.15734.json",
    "total_tokens": 841,
    "translated_title": "关于知识蒸馏在模型可解释性上的影响",
    "translated_abstract": "近期的研究表明，知识蒸馏(KD)为何可以提高模型性能已被阐明，但是很少有人研究除提高模型性能外KD的其他优点。本研究旨在证明KD不仅可以提高模型准确性，还可以提高模型的可解释性。我们通过网络分解算法中检测的概念检测器数量对模型的可解释性进行了定量比较。我们将可解释性的提高归因于从老师模型传递给学生模型的类相似信息。我们首先通过逻辑回归蒸馏确认了类相似信息的传递。然后，我们分析了类相似信息在模型可解释性方面的作用，包括其存在或缺失以及相似信息的程度。我们进行了各种定量和定性实验，并在不同数据集、不同的KD方法和不同的模型结构上检查了结果。",
    "tldr": "本文研究表明，知识蒸馏可以提高模型的可解释性和准确性，并归因于从老师模型传递给学生模型的类相似信息的存在。",
    "en_tdlr": "This paper shows that knowledge distillation not only improves model accuracy but also enhances model interpretability, attributing the improvement to the transfer of class-similarity information from teacher to student models."
}