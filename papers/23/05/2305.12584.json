{
    "title": "Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces",
    "abstract": "arXiv:2305.12584v2 Announce Type: replace-cross  Abstract: Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investiga",
    "link": "https://arxiv.org/abs/2305.12584",
    "context": "Title: Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces\nAbstract: arXiv:2305.12584v2 Announce Type: replace-cross  Abstract: Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investiga",
    "path": "papers/23/05/2305.12584.json",
    "total_tokens": 806,
    "translated_title": "稀疏表示定理用于在再生核Banach空间中学习",
    "translated_abstract": "学习解的稀疏性是机器学习中一种理想的特征。某些再生核Banach空间(RKBSs)是稀疏学习方法的适当假设空间。本文旨在了解什么样的RKBS可以促进学习解的稀疏性。我们考虑RKBS中的两种典型学习模型：最小范数插值(MNI)问题和正则化问题。我们首先为这些问题的解建立了一个显式的表示定理，该定理通过极值点的线性组合来表示解集的极值点，即规范函数的次梯度集，这是依赖于数据的。然后我们提出了对RKBS的充分条件，可以将解的显式表示转换为具有比观测数据数量更少项的稀疏核表示。在所提出的充分条件下，我们进行了研究",
    "tldr": "该论文研究了在再生核Banach空间中促进稀疏学习解的条件，并建立了相应的表示定理和转换机制。"
}