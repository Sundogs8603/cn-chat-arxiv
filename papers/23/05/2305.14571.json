{
    "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])",
    "abstract": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our me",
    "link": "http://arxiv.org/abs/2305.14571",
    "context": "Title: From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])\nAbstract: Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our me",
    "path": "papers/23/05/2305.14571.json",
    "total_tokens": 887,
    "translated_title": "从字符到词：用于开放词汇语言理解的分层预训练语言模型",
    "translated_abstract": "当前自然语言理解的最新模型需要对原始文本进行预处理，将其转换为离散标记。这个过程称为分词，依赖于预先构建的单词或子词。但这种固定词汇表限制了模型对拼写错误的容忍度和适应新领域的能力。本文提出了一种新颖的开放词汇语言模型，采用了分层的两级方法：一个在词级别，另一个在序列级别。具体来说，我们设计了一个内部单词模块，使用浅层Transformer体系结构从其字符中学习单词表示，并另一个深层Transformer模块，将每个单词表示置于整个单词序列中。因此，我们的模型直接在具有显式单词边界意识的字符序列上运行，但没有偏见的子单词或单词级词汇表。各种下游任务的实验表明我们的模型比其他模型有更好的性能。",
    "tldr": "本文提出了一种新颖的开放词汇语言模型，它采用了分层两级方法和浅层Transformer体系结构从字符中学习单词，能够提高模型的容忍度和适应性。"
}