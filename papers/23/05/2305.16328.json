{
    "title": "Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])",
    "abstract": "What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically & relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether & how models compose visually grounded semantics, and how we might improve their ability to do so.  Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital f",
    "link": "http://arxiv.org/abs/2305.16328",
    "context": "Title: Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])\nAbstract: What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically & relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether & how models compose visually grounded semantics, and how we might improve their ability to do so.  Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital f",
    "path": "papers/23/05/2305.16328.json",
    "total_tokens": 916,
    "translated_title": "视觉上下文语言模型中的语义组合",
    "translated_abstract": "句子的意义和其理想表达方式是什么？人类语言表现力的很大一部分来自语义组合，即人类心智以层次化和关系性方式表示意义的能力。与此同时，大部分句子的意义存在于文本之外，需要基于感官、运动和体验模态进行充分的学习。尽管大型语言模型显示出相当的组合能力，但最近的研究表明，有视觉基础的语言模型在表示组合结构时严重失败。在本论文中，我们探讨了模型是否及如何组合视觉上下文语义以及如何提高其组合能力。具体而言，我们介绍了 1) WinogroundVQA，一个新的组合视觉问答基准，2) 句子嵌入模型中组合能力的句法神经模块蒸馏，3) 对于图像字幕模型的因果追踪，以定位重要的神经表示。",
    "tldr": "本论文研究了视觉上下文语言模型中的语义组合能力，提出了新的组合视觉问答基准，句法神经模块蒸馏等方法以提高组合能力，并探索了对图像字幕模型的因果追踪以定位重要神经表示。",
    "en_tdlr": "This paper investigates semantic composition ability in visually grounded language models, proposing new benchmarks and methods such as WinogroundVQA and Syntactic Neural Module Distillation to improve compositional ability, as well as exploring causal tracing for image captioning models to locate important neural representations."
}