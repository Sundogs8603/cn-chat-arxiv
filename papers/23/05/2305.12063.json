{
    "title": "Efficient Multimodal Neural Networks for Trigger-less Voice Assistants. (arXiv:2305.12063v1 [cs.LG])",
    "abstract": "The adoption of multimodal interactions by Voice Assistants (VAs) is growing rapidly to enhance human-computer interactions. Smartwatches have now incorporated trigger-less methods of invoking VAs, such as Raise To Speak (RTS), where the user raises their watch and speaks to VAs without an explicit trigger. Current state-of-the-art RTS systems rely on heuristics and engineered Finite State Machines to fuse gesture and audio data for multimodal decision-making. However, these methods have limitations, including limited adaptability, scalability, and induced human biases. In this work, we propose a neural network based audio-gesture multimodal fusion system that (1) Better understands temporal correlation between audio and gesture data, leading to precise invocations (2) Generalizes to a wide range of environments and scenarios (3) Is lightweight and deployable on low-power devices, such as smartwatches, with quick launch times (4) Improves productivity in asset development processes.",
    "link": "http://arxiv.org/abs/2305.12063",
    "context": "Title: Efficient Multimodal Neural Networks for Trigger-less Voice Assistants. (arXiv:2305.12063v1 [cs.LG])\nAbstract: The adoption of multimodal interactions by Voice Assistants (VAs) is growing rapidly to enhance human-computer interactions. Smartwatches have now incorporated trigger-less methods of invoking VAs, such as Raise To Speak (RTS), where the user raises their watch and speaks to VAs without an explicit trigger. Current state-of-the-art RTS systems rely on heuristics and engineered Finite State Machines to fuse gesture and audio data for multimodal decision-making. However, these methods have limitations, including limited adaptability, scalability, and induced human biases. In this work, we propose a neural network based audio-gesture multimodal fusion system that (1) Better understands temporal correlation between audio and gesture data, leading to precise invocations (2) Generalizes to a wide range of environments and scenarios (3) Is lightweight and deployable on low-power devices, such as smartwatches, with quick launch times (4) Improves productivity in asset development processes.",
    "path": "papers/23/05/2305.12063.json",
    "total_tokens": 955,
    "translated_title": "无需触发器语音助手的高效多模神经网络",
    "translated_abstract": "语音助手（VA）采用多模互动以增强人机交互的方式正在迅速增长。智能手表现在已经融合了无需显式触发器的VA调用方法，如Raise To Speak（RTS），用户将手表举起并向VA说话而无需显式触发器。当前最先进的RTS系统依靠启发式和设计的有限状态机来融合手势和音频数据以进行多模决策。然而，这些方法存在一些限制，包括适应性有限、可扩展性不足和人类产生的偏差。在这项工作中，我们提出了一种基于神经网络的音频-手势多模融合系统，其具有以下特点：（1）更好地理解音频和手势数据之间的时间相关性，从而进行精确的调用（2）在广泛的环境和场景下具有通用性（3）轻便且可在低功率设备上部署，如智能手表，并具有快速启动时间（4）提高资产开发流程的生产率。",
    "tldr": "本文提出了一种新方法，使用基于神经网络的音频-手势多模融合系统来实现无需触发器的语音助手，该方法可以更好地理解音频和手势数据之间的时间相关性，通用性强，并可以快速启动，提高资产开发流程的生产率。"
}