{
    "title": "SHARP: Sparsity and Hidden Activation RePlay for Neuro-Inspired Continual Learning. (arXiv:2305.18563v1 [cs.LG])",
    "abstract": "Deep neural networks (DNNs) struggle to learn in dynamic environments since they rely on fixed datasets or stationary environments. Continual learning (CL) aims to address this limitation and enable DNNs to accumulate knowledge incrementally, similar to human learning. Inspired by how our brain consolidates memories, a powerful strategy in CL is replay, which involves training the DNN on a mixture of new and all seen classes. However, existing replay methods overlook two crucial aspects of biological replay: 1) the brain replays processed neural patterns instead of raw input, and 2) it prioritizes the replay of recently learned information rather than revisiting all past experiences. To address these differences, we propose SHARP, an efficient neuro-inspired CL method that leverages sparse dynamic connectivity and activation replay. Unlike other activation replay methods, which assume layers not subjected to replay have been pretrained and fixed, SHARP can continually update all layers",
    "link": "http://arxiv.org/abs/2305.18563",
    "context": "Title: SHARP: Sparsity and Hidden Activation RePlay for Neuro-Inspired Continual Learning. (arXiv:2305.18563v1 [cs.LG])\nAbstract: Deep neural networks (DNNs) struggle to learn in dynamic environments since they rely on fixed datasets or stationary environments. Continual learning (CL) aims to address this limitation and enable DNNs to accumulate knowledge incrementally, similar to human learning. Inspired by how our brain consolidates memories, a powerful strategy in CL is replay, which involves training the DNN on a mixture of new and all seen classes. However, existing replay methods overlook two crucial aspects of biological replay: 1) the brain replays processed neural patterns instead of raw input, and 2) it prioritizes the replay of recently learned information rather than revisiting all past experiences. To address these differences, we propose SHARP, an efficient neuro-inspired CL method that leverages sparse dynamic connectivity and activation replay. Unlike other activation replay methods, which assume layers not subjected to replay have been pretrained and fixed, SHARP can continually update all layers",
    "path": "papers/23/05/2305.18563.json",
    "total_tokens": 934,
    "translated_title": "SHARP: 稀疏性和隐藏激活回放用于神经启发式的连续学习",
    "translated_abstract": "深度神经网络(DNNs)在动态环境下学习困难，因为它们依赖于固定的数据集或恒定的环境。连续学习(CL)的目标是解决这个限制，使DNN能够逐步积累知识，类似于人类学习。受到大脑如何巩固记忆的启发，回放是CL的强有力策略，它涉及对DNN进行新和所有已见类别的混合训练。然而，现有的回放方法忽略了生物回放的两个关键方面: 1) 大脑回放处理过的神经模式而不是原始输入，2)它优先回放最近学到的信息，而不是重温所有过去的经验。为了解决这些差异，我们提出了SHARP，这是一种高效的神经启发式CL方法，利用稀疏的动态连接和激活回放。与其他激活回放方法不同，它假定没有受到回放的层已经经过预训练并固定，SHARP可以持续更新所有层。",
    "tldr": "SHARP是一种神经启发式的连续学习方法，利用稀疏的动态连接和激活回放来回放处理过的神经模式，优先回放最近学到的信息，并可以持续更新所有层。",
    "en_tdlr": "SHARP is a neuro-inspired continual learning method which leverages sparse dynamic connectivity and activation replay to replay processed neural patterns, prioritize recently learned information, and update all layers continuously."
}