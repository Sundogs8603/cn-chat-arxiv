{
    "title": "Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks. (arXiv:2305.09057v1 [cs.LG])",
    "abstract": "In this work we introduce a self-supervised pretraining framework for transformers on functional Magnetic Resonance Imaging (fMRI) data. First, we pretrain our architecture on two self-supervised tasks simultaneously to teach the model a general understanding of the temporal and spatial dynamics of human auditory cortex during music listening. Our pretraining results are the first to suggest a synergistic effect of multitask training on fMRI data. Second, we finetune the pretrained models and train additional fresh models on a supervised fMRI classification task. We observe significantly improved accuracy on held-out runs with the finetuned models, which demonstrates the ability of our pretraining tasks to facilitate transfer learning. This work contributes to the growing body of literature on transformer architectures for pretraining and transfer learning with fMRI data, and serves as a proof of concept for our pretraining tasks and multitask pretraining on fMRI data.",
    "link": "http://arxiv.org/abs/2305.09057",
    "context": "Title: Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks. (arXiv:2305.09057v1 [cs.LG])\nAbstract: In this work we introduce a self-supervised pretraining framework for transformers on functional Magnetic Resonance Imaging (fMRI) data. First, we pretrain our architecture on two self-supervised tasks simultaneously to teach the model a general understanding of the temporal and spatial dynamics of human auditory cortex during music listening. Our pretraining results are the first to suggest a synergistic effect of multitask training on fMRI data. Second, we finetune the pretrained models and train additional fresh models on a supervised fMRI classification task. We observe significantly improved accuracy on held-out runs with the finetuned models, which demonstrates the ability of our pretraining tasks to facilitate transfer learning. This work contributes to the growing body of literature on transformer architectures for pretraining and transfer learning with fMRI data, and serves as a proof of concept for our pretraining tasks and multitask pretraining on fMRI data.",
    "path": "papers/23/05/2305.09057.json",
    "total_tokens": 925,
    "translated_title": "基于自监督预训练的功能磁共振成像序列数据用于脑解码任务的迁移学习",
    "translated_abstract": "本文介绍了一种基于自监督预训练框架的transformer方法，并应用于功能磁共振成像(fMRI)数据。首先，我们同时在两个自监督任务上对架构进行预训练，以教会模型对音乐听觉皮层的时间和空间动态有一个普遍的理解。我们的预训练结果首次表明，多任务训练对fMRI数据具有协同效应。其次，我们对预训练的模型进行微调和在一个受监督的fMRI分类任务上进行额外的模型训练。我们观察到，在微调模型上，保留数据的准确性显著提高，这证明了我们预训练任务促进迁移学习的能力。本研究为预训练和fMRI数据的迁移学习的transformer架构增加了数量上的证据，并证明了我们的预训练任务和fMRI数据上的多任务预训练的概念证明。",
    "tldr": "本文介绍一种基于自监督预训练框架的transformer方法并应用于fMRI数据，首次表明多任务训练对fMRI数据具有协同效应。预训练任务促进迁移学习的能力得到了证明。",
    "en_tdlr": "This paper introduces a self-supervised pretraining framework for transformers on fMRI data, showing for the first time the synergy of multitask training on fMRI data. The ability of pretraining tasks to facilitate transfer learning is demonstrated."
}