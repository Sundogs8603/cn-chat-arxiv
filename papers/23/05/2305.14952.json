{
    "title": "Focus Your Attention (with Adaptive IIR Filters). (arXiv:2305.14952v2 [cs.LG] UPDATED)",
    "abstract": "We present a new layer in which dynamic (i.e.,input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems.",
    "link": "http://arxiv.org/abs/2305.14952",
    "context": "Title: Focus Your Attention (with Adaptive IIR Filters). (arXiv:2305.14952v2 [cs.LG] UPDATED)\nAbstract: We present a new layer in which dynamic (i.e.,input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems.",
    "path": "papers/23/05/2305.14952.json",
    "total_tokens": 925,
    "translated_title": "聚焦您的注意力（通过自适应IIR滤波器）",
    "translated_abstract": "我们提出了一种新的层，在其中使用动态（即输入相关的）二阶无限冲激响应（IIR）滤波器来处理输入序列，然后再应用传统的注意力机制。输入被分成块，并且这些滤波器的系数基于前面的块来确定以保持因果性。尽管它们相对较低阶，但这些因果自适应滤波器被证明可以将注意力集中在相关的序列元素上。这一新层基于控制理论，并展示了能够推广对角状态空间层。该层的表现与最先进的网络相当，但参数量较少，并且时间复杂度随输入大小的增长是次二次的。所得到的层在多个长程序列问题上的参数数量和性能水平都优于Heyna、GPT2和Mega等层。",
    "tldr": "该论文提出了使用自适应IIR滤波器来聚焦注意力的新层。这些滤波器基于输入序列的前几个块来确定系数，能够将注意力集中在相关的序列元素上，并且相比于其他网络具有更少的参数和次二次时间复杂度。该层在多个长程序列问题上表现优异，超越了Heyna、GPT2和Mega等层。",
    "en_tdlr": "This paper presents a new layer that uses adaptive IIR filters to focus attention. These filters determine their coefficients based on previous blocks of the input sequence, allowing them to concentrate attention on relevant elements. The layer outperforms other networks in terms of parameter efficiency and time complexity, and achieves impressive results on various long-range sequence problems, surpassing layers like Heyna, GPT2, and Mega."
}