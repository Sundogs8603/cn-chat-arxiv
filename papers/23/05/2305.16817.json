{
    "title": "Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])",
    "abstract": "Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress",
    "link": "http://arxiv.org/abs/2305.16817",
    "context": "Title: Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])\nAbstract: Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress",
    "path": "papers/23/05/2305.16817.json",
    "total_tokens": 900,
    "translated_title": "选择性混合有助于应对分布偏移，但不仅仅是因为混合技术",
    "translated_abstract": "Mixup是一种提高神经网络泛化性能的高度成功的技术，它通过随机配对的组合来增强训练数据。选择性mixup是一系列将mixup应用于特定对的方法，例如仅在类别或领域之间组合示例。这些方法声称在具有分布偏移的基准测试中有显着的提高，但它们的机制和限制尚不清楚。本文研究了选择性mixup的一个被忽视的方面，从一个全新的角度解释了它的成功。我们发现，非随机选择对会影响训练分布，并通过与混合技术完全无关的方式提高泛化性能。例如，在二元分类中，类别之间的mixup隐含地对数据进行重采样，以实现标签偏移的经典解决方案。我们经验证实，这种隐含重采样解释了先前工作中的大部分改进。在理论上，这些结果依赖于一个回归问题，其中我们需要区分真正的重采样和混合技术。",
    "tldr": "选择性mixup通过非随机选择对提高训练分布，实现标签偏移的经典解决方案，从而提高了神经网络的泛化性能。",
    "en_tdlr": "Selective mixup improves the training distribution by non-randomly selecting pairs, implementing a classical solution to label shift such as uniform resampling across classes in binary classification, and thereby improving the generalization performance of neural networks."
}