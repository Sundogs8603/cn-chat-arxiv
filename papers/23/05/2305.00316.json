{
    "title": "The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])",
    "abstract": "The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r",
    "link": "http://arxiv.org/abs/2305.00316",
    "context": "Title: The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])\nAbstract: The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r",
    "path": "papers/23/05/2305.00316.json",
    "total_tokens": 937,
    "translated_title": "理想的不断学习者: 不会遗忘的代理",
    "translated_abstract": "持续学习的目标是找到一个模型，解决按顺序呈现给学习者的多个学习任务。在这种情况下，一个主要的挑战是，当学习新任务时，学习者可能会忘记如何解决先前的任务，这种现象称为灾难性遗忘。为了应对这一挑战，已经提出了许多实用的方法，包括基于记忆、基于正则化和基于扩展的方法。然而，对这些方法的严格理论理解仍然是困难的。本文旨在通过提出一种新的持续学习框架——理想的持续学习者（ICL），从理论与实践之间的鸿沟中跨越过去，ICL的构建保证避免灾难性遗忘，统一了多个成熟的持续学习方法，并为这些方法的优缺点提供了新的理论见解。我们还为ICL推导出了泛化界限，这使我们能够在理论上量化如何控制模型的复杂度来减轻灾难性遗忘。",
    "tldr": "本文提出了一个新的持续学习框架——理想的持续学习者（ICL），通过构建避免灾难性遗忘，统一了多个持续学习方法，并为这些方法的优缺点提供了新的理论见解。",
    "en_tdlr": "This paper proposes a new continual learning framework, the Ideal Continual Learner (ICL), which guarantees to avoid catastrophic forgetting by construction and unifies multiple established continual learning methods while providing new theoretical insights into their strengths and weaknesses."
}