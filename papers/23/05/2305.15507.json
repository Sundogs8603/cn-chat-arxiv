{
    "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training ",
    "link": "http://arxiv.org/abs/2305.15507",
    "context": "Title: The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training ",
    "path": "papers/23/05/2305.15507.json",
    "total_tokens": 908,
    "translated_title": "越大的语言模型错误越难以捉摸：Python中的标识符交换不被语言模型识别",
    "translated_abstract": "大型语言模型已成功应用于代码生成任务，这引发了一个问题，即这些模型对编程的理解程度如何。传统的编程语言具有不变性和等变性，人类程序员能直观地理解和利用这些性质，如标识符重命名（近似）不变性。我们发现，LLM在默认函数名称交换时不仅无法正确生成Python代码，有些模型在模型大小增加时甚至变得更加自信地进行错误预测，这是最近发现的逆比例缩放现象的实例，与通常观察到的模型大小增加会提高预测质量的趋势相反。我们的发现表明，尽管LLM的典型情况表现出色，但它们仍然缺乏一个深刻的、抽象的理解据以操纵内容，使它们无法胜任与训练偏差的任务。",
    "tldr": "该论文揭示了大型语言模型对Python标识符交换的识别问题，尤其是在逆比例缩放现象影响下表现更为显著。这表明LLM缺乏深刻、抽象的理解，无法胜任与训练偏差的任务。",
    "en_tdlr": "This paper reveals the recognition problem of Python identifier swaps by large language models, particularly showing more significant performance under the influence of the inverse scaling phenomenon. It indicates LLMs lack a deep, abstract understanding, making them unsuitable for tasks that statistically deviate from their training."
}