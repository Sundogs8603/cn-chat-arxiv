{
    "title": "The False Promise of Imitating Proprietary LLMs. (arXiv:2305.15717v1 [cs.CL])",
    "abstract": "An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation",
    "link": "http://arxiv.org/abs/2305.15717",
    "context": "Title: The False Promise of Imitating Proprietary LLMs. (arXiv:2305.15717v1 [cs.CL])\nAbstract: An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation",
    "path": "papers/23/05/2305.15717.json",
    "total_tokens": 1012,
    "translated_title": "模仿专有语言模型的错误承诺",
    "translated_abstract": "一种提高较弱语言模型性能的新方法是基于较强模型的输出进行微调，例如专有系统ChatGPT（例如Alpaca、Self-Instruct等）。这种方法旨在使用较弱的开源模型廉价地模仿专有模型的能力。本文对这种方法进行了批判性分析。我们首先 使用不同的基础模型大小（1.5B-13B）、数据源和模仿数据量（0.3M-150M令牌）来微调一系列模仿ChatGPT的LM。然后，我们使用众包评估和规范的NLP基准对模型进行评估。最初，我们对模仿模型的输出质量感到惊讶——它们似乎更擅长按照指示进行操作，并且众包工作者将它们的输出评为与ChatGPT具有竞争力。然而，当进行更有针对性的自动评估时，我们发现，与未在模仿模型中得到充分支持的任务相比，模仿模型在缩小基础语言模型与ChatGPT之间差距方面帮助不大。",
    "tldr": "本文分析了使用较弱开源模型模仿专有语言模型的可靠度，发现在某些情况下，表现可能非常出色，但在大多数任务中仍无法取代专有语言模型。",
    "en_tdlr": "This paper critically analyzes the reliability of imitating proprietary language models using weaker open-source models and finds that while they may perform impressively in some cases, they are still unable to replace proprietary models in most tasks."
}