{
    "title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation. (arXiv:2305.09312v1 [cs.CL])",
    "abstract": "This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.",
    "link": "http://arxiv.org/abs/2305.09312",
    "context": "Title: Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation. (arXiv:2305.09312v1 [cs.CL])\nAbstract: This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.",
    "path": "papers/23/05/2305.09312.json",
    "total_tokens": 916,
    "translated_title": "探究层归一化在零样本神经机器翻译中的影响",
    "translated_abstract": "本文研究了层归一化（LayerNorm）对零样本翻译（ZST）的影响。最近的ZST研究通常使用Transformer架构作为主干，并将层的输入设置为带有LayerNorm的PreNorm。然而，徐等人（2019）揭示了PreNorm存在过度拟合训练数据的风险。基于此，我们假设PreNorm可能会过度拟合监督方向，因此在ZST中具有低的泛化能力。通过在OPUS、IWSLT和Europarl数据集上进行54个ZST方向的实验，我们证明了在残差连接后使用原始的Transformer设置LayerNorm（PostNorm）的表现始终优于PreNorm达12.3 BLEU分。然后，我们通过分析PreNorm和PostNorm之间的离靶率和结构变化的差异，研究了性能差异。这项研究强调了对ZST的LayerNorm设置需要仔细考虑。",
    "tldr": "研究发现，在零样本翻译中，使用残差连接后的Transformer设置的层归一化（PostNorm）始终优于带有层归一化的PreNorm，最高可提高12.3 BLEU分。"
}