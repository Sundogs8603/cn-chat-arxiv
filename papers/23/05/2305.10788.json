{
    "title": "Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])",
    "abstract": "Due to the rapid development of computing hardware resources and the dramatic growth of data, pre-trained models in speech recognition, such as Whisper, have significantly improved the performance of speech recognition tasks. However, these models usually have a high computational overhead, making it difficult to execute effectively on resource-constrained devices. To speed up inference and reduce model size while maintaining performance, we propose a novel guided knowledge distillation and quantization for large pre-trained model Whisper. The student model selects distillation and quantization layers based on quantization loss and distillation loss, respectively. We compressed $\\text{Whisper}_\\text{small}$ to $\\text{Whisper}_\\text{base}$ and $\\text{Whisper}_\\text{tiny}$ levels, making $\\text{Whisper}_\\text{small}$ 5.18x/10.48x smaller, respectively. Moreover, compared to the original $\\text{Whisper}_\\text{base}$ and $\\text{Whisper}_\\text{tiny}$, there is also a relative character erro",
    "link": "http://arxiv.org/abs/2305.10788",
    "context": "Title: Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])\nAbstract: Due to the rapid development of computing hardware resources and the dramatic growth of data, pre-trained models in speech recognition, such as Whisper, have significantly improved the performance of speech recognition tasks. However, these models usually have a high computational overhead, making it difficult to execute effectively on resource-constrained devices. To speed up inference and reduce model size while maintaining performance, we propose a novel guided knowledge distillation and quantization for large pre-trained model Whisper. The student model selects distillation and quantization layers based on quantization loss and distillation loss, respectively. We compressed $\\text{Whisper}_\\text{small}$ to $\\text{Whisper}_\\text{base}$ and $\\text{Whisper}_\\text{tiny}$ levels, making $\\text{Whisper}_\\text{small}$ 5.18x/10.48x smaller, respectively. Moreover, compared to the original $\\text{Whisper}_\\text{base}$ and $\\text{Whisper}_\\text{tiny}$, there is also a relative character erro",
    "path": "papers/23/05/2305.10788.json",
    "total_tokens": 918,
    "translated_title": "Whisper-KDQ: 通过引导知识蒸馏和量化实现高效ASR的轻型Whisper",
    "translated_abstract": "随着计算硬件资源的快速发展和数据的显著增长，预训练模型在语音识别等任务中的应用显著提高了性能。然而，这些模型通常具有很高的计算开销，使其难以在资源受限的设备上有效执行。为了加速推理、减少模型大小，并保持性能，我们提出了一种新颖的引导知识蒸馏和量化方法，用于大型预训练模型Whisper。学生模型基于量化损失和蒸馏损失选择蒸馏和量化层。我们将$\\text{Whisper}_\\text{small}$压缩到$\\text{Whisper}_\\text{base}$和$\\text{Whisper}_\\text{tiny}$级别，使$\\text{Whisper}_\\text{small}$分别小5.18x/10.48x。此外，与原始$\\text{Whisper}_\\text{base}$和$\\text{Whisper}_\\text{tiny}$相比，还有相对字符错误率降低.",
    "tldr": "本文提出了一种通过引导知识蒸馏和量化，实现对大型预训练语音识别模型Whisper进行压缩优化的方法，可以将模型大小缩小并提高性能。",
    "en_tdlr": "This paper proposes a novel guided knowledge distillation and quantization method to compress and optimize the large pre-trained speech recognition model Whisper, resulting in reduced model size and improved performance."
}