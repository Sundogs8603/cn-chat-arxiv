{
    "title": "DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning. (arXiv:2305.02607v1 [cs.CL])",
    "abstract": "In recent years, sentiment analysis has gained significant importance in natural language processing. However, most existing models and datasets for sentiment analysis are developed for high-resource languages, such as English and Chinese, leaving low-resource languages, particularly African languages, largely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this gap by evaluating sentiment analysis models on low-resource African languages. In this paper, we present our solution to the shared task, where we employed different multilingual XLM-R models with classification head trained on various data, including those retrained in African dialects and fine-tuned on target languages. Our team achieved the third-best results in Subtask B, Track 16: Multilingual, demonstrating the effectiveness of our approach. While our model showed relatively good results on multilingual data, it performed poorly in some languages. Our findings highlight the importance of developing more",
    "link": "http://arxiv.org/abs/2305.02607",
    "context": "Title: DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning. (arXiv:2305.02607v1 [cs.CL])\nAbstract: In recent years, sentiment analysis has gained significant importance in natural language processing. However, most existing models and datasets for sentiment analysis are developed for high-resource languages, such as English and Chinese, leaving low-resource languages, particularly African languages, largely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this gap by evaluating sentiment analysis models on low-resource African languages. In this paper, we present our solution to the shared task, where we employed different multilingual XLM-R models with classification head trained on various data, including those retrained in African dialects and fine-tuned on target languages. Our team achieved the third-best results in Subtask B, Track 16: Multilingual, demonstrating the effectiveness of our approach. While our model showed relatively good results on multilingual data, it performed poorly in some languages. Our findings highlight the importance of developing more",
    "path": "papers/23/05/2305.02607.json",
    "total_tokens": 962,
    "translated_title": "SemEval-2023任务12中的低资源语言文本分类：通过多语言预训练语言模型微调",
    "translated_abstract": "最近几年，情感分析在自然语言处理中变得非常重要。然而，大部分用于情感分析的模型和数据集都是针对高资源语言，比如英语和中文开发的，而低资源语言，特别是非洲语言，往往鲜有研究。AfriSenti-SemEval 2023共享任务12旨在填补这一空白，通过在低资源的非洲语言上评估情感分析模型。在本文中，我们介绍了我们针对共享任务的解决方案，我们使用了不同的多语言XLM-R模型，并使用在非洲方言上重新训练并在目标语言上微调的分类头训练了这些模型。我们的团队在Subtask B，Track 16: Multilingual中取得了第三好的成绩，证明了我们的方法的有效性。虽然我们的模型在多语言数据上表现出了相对不错的结果，但在某些语言上表现较差。我们的发现强调了开发更多针对低资源语言的有效方法的重要性。",
    "tldr": "该论文介绍了针对SemEval-2023低资源非洲语言文本分类任务的解决方案，使用多语言预训练语言模型进行微调取得了第三名成绩，并突显了开发针对低资源语言的有效方法的重要性。",
    "en_tdlr": "This paper presents a solution for low-resource African language text classification in the SemEval-2023 task, which employs multilingual pretrained language models fine-tuned with classification heads. The approach achieved third place in Subtask B, Track 16: Multilingual, highlighting the importance of developing effective methods for low-resource languages."
}