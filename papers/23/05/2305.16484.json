{
    "title": "Batch Model Consolidation: A Multi-Task Model Consolidation Framework. (arXiv:2305.16484v1 [cs.LG])",
    "abstract": "In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\\textit{regularization}$ phase, BMC trains multiple $\\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\\textit{base model}$ through a $\\textit{stability loss}$, and constructs a $\\textit{buffer}$ from a fraction of the task's data. During the $\\textit{consolidation}$ phase, we combine the ",
    "link": "http://arxiv.org/abs/2305.16484",
    "context": "Title: Batch Model Consolidation: A Multi-Task Model Consolidation Framework. (arXiv:2305.16484v1 [cs.LG])\nAbstract: In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\\textit{regularization}$ phase, BMC trains multiple $\\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\\textit{base model}$ through a $\\textit{stability loss}$, and constructs a $\\textit{buffer}$ from a fraction of the task's data. During the $\\textit{consolidation}$ phase, we combine the ",
    "path": "papers/23/05/2305.16484.json",
    "total_tokens": 912,
    "translated_title": "批次模型整合：一个多任务模型整合框架",
    "translated_abstract": "在连续学习中，模型需要按顺序学习一系列任务，而不会在之前学习的任务上出现显着的性能下降。现有方法在面对各种领域和难度的长序列任务时效果不佳。许多现有的连续学习方法由于内存资源消耗过大或训练时间过长而难以在实践中应用，或只能在单个设备上紧密耦合。本文提出批次模型整合（BMC）来支持更现实的连续学习，面对多个代理在各种任务中接触的情况。在正则化阶段，BMC并行训练多个专家模型来学习一组不相交的任务。每个专家通过稳定性损失与一个基础模型保持权重相似性，并从任务数据的一部分构建缓冲区。在整合阶段，我们将多个专家模型整合为一个模型，并在目标任务上进行微调。",
    "tldr": "本文提出了批次模型整合（BMC）来支持更现实的连续学习，它通过在正则化阶段训练多个专家模型来学习一组不相交的任务，并在整合阶段将多个专家模型整合为一个模型。"
}