{
    "title": "SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])",
    "abstract": "Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, \"fast\" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. ",
    "link": "http://arxiv.org/abs/2305.09028",
    "context": "Title: SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])\nAbstract: Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, \"fast\" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. ",
    "path": "papers/23/05/2305.09028.json",
    "total_tokens": 1111,
    "translated_title": "SKI加速Toeplitz神经网络：通过非对称核实现加速",
    "translated_abstract": "Toeplitz神经网络（TNNs）是最近出现并取得令人印象深刻结果的序列模型。它们需要O(n log n)的计算复杂度和O(n)的相对位置编码器（RPE）多层感知器（MLP）和衰减偏差调用。我们的目标是减少它们。我们首先指出，RPE是一个非对称正定核，而Toeplitz矩阵是伪格拉姆矩阵。此外：1）学习的核在主对角线附近显示出刺状行为，而在其他位置则表现出平滑行为；2）RPE MLP较慢。对于双向模型，这促使我们进行稀疏加低秩Toeplitz矩阵分解。对于稀疏组件的操作，我们进行小型1D卷积。对于低秩组件，我们将RPE MLP替换为线性插值，并使用非对称有结构的内核插值（SKI）（Wilson等，2015）以实现O（n）复杂度：我们提供了严格的误差分析。对于因果模型，“快速”因果屏蔽（Katharopoulos等，2020）抵消了SKI的好处。",
    "tldr": "本论文提出使用非对称核（asymmetric kernels）实现Toeplitz神经网络（TNNs）的加速，通过稀疏加低秩Toeplitz矩阵分解、小型1D卷积和替换相对位置编码器（RPE）多层感知器（MLP）实现O（n）复杂度，针对因果模型，提出了“快速”因果屏蔽来抵消这种方法的限制。",
    "en_tdlr": "This paper proposes to use asymmetric kernels to accelerate Toeplitz Neural Networks (TNNs) with sparse plus low-rank Toeplitz matrix decomposition, small 1D convolution and replacing relative positional encoder (RPE) multi-layer perceptron (MLP) to achieve O(n) complexity. For causal models, \"fast\" causal masking is proposed to offset the limitations of this approach."
}