{
    "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])",
    "abstract": "Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w",
    "link": "http://arxiv.org/abs/2305.16381",
    "context": "Title: DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])\nAbstract: Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w",
    "path": "papers/23/05/2305.16381.json",
    "total_tokens": 985,
    "translated_title": "DPOK: 强化学习用于微调文本到图像扩散模型",
    "translated_abstract": "已经证明，从人类反馈中学习可以改善文本到图像模型。这些技术首先学习一个捕捉任务中人类关心的特征的奖励函数，然后根据学习到的奖励函数改进模型。虽然已经研究了相对简单的方法（例如基于奖励得分的拒绝采样），但使用奖励函数微调文本到图像模型仍然具有挑战性。在这项工作中，我们提出使用在线强化学习（RL）来微调文本到图像模型。我们专注于扩散模型，将微调任务定义为RL问题，并使用策略梯度更新预训练文本到图像扩散模型，以最大化反馈训练奖励。我们的方法DPOK集成了KL正则化的策略优化。我们对RL微调和监督微调的KL正则化进行了分析。在我们的实验中，我们展示了DPOK通常优于使用交叉熵损失的监督微调和以前的RL微调技术。DPOK在COCO数据集上实现了最先进的性能，IS和FID得分显著优于现有方法。",
    "tldr": "本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。"
}