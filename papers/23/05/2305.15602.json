{
    "title": "Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness. (arXiv:2305.15602v1 [eess.SY])",
    "abstract": "Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety",
    "link": "http://arxiv.org/abs/2305.15602",
    "context": "Title: Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness. (arXiv:2305.15602v1 [eess.SY])\nAbstract: Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety",
    "path": "papers/23/05/2305.15602.json",
    "total_tokens": 957,
    "translated_title": "控制不变集增强的安全强化学习：改进的采样效率、保证的稳定性和健壮性",
    "translated_abstract": "强化学习是一个备受关注的研究领域，特别是安全强化学习因其能够处理真实应用中至关重要的安全约束而备受瞩目。本文提出了一种名为控制不变集增强强化学习（CIS）的新型学习方法，该方法利用使用CIS的显式形式来提高稳定性保证和采样效率。此外，本研究还在存在不确定性的情况下研究了所提方法的健壮性。该方法包括两个学习阶段：离线和在线。在离线阶段，CIS被纳入到奖励设计、初始状态采样和状态重置程序中。这种纳入CIS的方式有助于提高离线训练过程中的采样效率。在在线阶段，当预测的下一步状态在CIS之外时，即引入安全准则时，RL将会重新进行训练。",
    "tldr": "本文提出了一种控制不变集增强强化学习的方法，该方法包括两个学习阶段以提高稳定性保证和采样效率。作者研究了该方法的健壮性，并将CIS纳入到奖励设计、初始状态采样和状态重置程序中。",
    "en_tdlr": "This paper proposes a novel approach to reinforcement learning called Control Invariant Set (CIS) enhanced RL that improves stability guarantees and sampling efficiency by utilizing the explicit form of CIS. The method consists of two stages: offline and online, and incorporates CIS into the reward design, initial state sampling, and state reset procedures. The paper also investigates the robustness of the approach in the presence of uncertainty."
}