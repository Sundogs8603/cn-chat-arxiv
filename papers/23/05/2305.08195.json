{
    "title": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])",
    "abstract": "Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full",
    "link": "http://arxiv.org/abs/2305.08195",
    "context": "Title: Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])\nAbstract: Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full",
    "path": "papers/23/05/2305.08195.json",
    "total_tokens": 923,
    "translated_title": "学习模拟自然语言反馈以进行交互式语义解析",
    "translated_abstract": "基于自然语言反馈的交互式语义解析已经成为比传统的一次语义解析更实用的场景，其中用户提供反馈来纠正解析器的错误。然而，以往的研究极大地依赖于人工注释的反馈数据来训练交互式语义解析器，这种方法代价高昂且不可扩展。在本研究中，我们提出了一个新任务，即模拟自然语言反馈以用于交互式语义解析。我们配合该任务使用了一个新的反馈评估器。该评估器专门设计用于评估模拟反馈的质量，基于此我们可以决定最佳的反馈模拟器。在一个文本到SQL的数据集上，我们展示了我们的反馈模拟器可以生成高质量的自然语言反馈以增强特定解析器的错误纠正能力。在低数据情况下，我们的反馈模拟器可以帮助达到与使用代价高昂的完整人工注释反馈数据训练所获得的相当的错误纠正性能。",
    "tldr": "本研究提出了通过模拟自然语言反馈来进行交互式语义解析的新任务，该方法可以在低数据情况下取得与使用人工注释反馈数据训练所获得的相当的错误纠正性能。",
    "en_tdlr": "This study proposes a new task of simulating natural language feedback for interactive semantic parsing, which can achieve comparable error correction performance as using costly human-annotated feedback data in low-data settings."
}