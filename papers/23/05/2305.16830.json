{
    "title": "Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])",
    "abstract": "Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, \"How can the structure of a decision-making task be used to tailor ML models for that specific task?\" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho",
    "link": "http://arxiv.org/abs/2305.16830",
    "context": "Title: Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])\nAbstract: Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, \"How can the structure of a decision-making task be used to tailor ML models for that specific task?\" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho",
    "path": "papers/23/05/2305.16830.json",
    "total_tokens": 883,
    "translated_title": "离巢：超越本地损失函数的预测优化问题",
    "translated_abstract": "预测优化问题是一种使用机器学习在不确定性条件下进行决策制定的框架。它的中心研究问题是，“如何利用决策任务的结构来定制特定任务的机器学习模型？”为此，最近的研究已经提出了学习任务特定的损失函数来捕捉这种潜在的结构。然而，当前的方法对这些损失的形式和对机器学习模型行为的影响做出了限制性的假设。这些假设既导致了高计算成本的方法，也在实践中被违反时导致了性能下降。在本文中，我们提出了解决这些问题的解决方案，避免了上述假设，利用机器学习模型的特性来提高学习损失函数的样本效率。我们从文献中的四个领域实验证明了我们的方法取得了最先进的结果，通常需要比可比方法少一个数量级的样本。",
    "tldr": "本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。",
    "en_tdlr": "This paper proposes a solution that avoids restrictive assumptions and utilizes the features of machine learning models to increase the sample efficiency of learning loss functions, achieving state-of-the-art results in the predict-then-optimize framework."
}