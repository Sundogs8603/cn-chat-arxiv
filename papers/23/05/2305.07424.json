{
    "title": "Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])",
    "abstract": "Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% ",
    "link": "http://arxiv.org/abs/2305.07424",
    "context": "Title: Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])\nAbstract: Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% ",
    "path": "papers/23/05/2305.07424.json",
    "total_tokens": 978,
    "translated_title": "无监督句子嵌入的实例平滑对比学习",
    "translated_abstract": "基于对比学习的方法，如unsup-SimCSE，在学习无监督句子嵌入方面取得了最先进（SOTA）的性能。然而，在以前的研究中，用于对比学习的每个嵌入仅来自于一个句子实例，我们称这些嵌入为实例级嵌入。换句话说，在这种情况下，每个嵌入被视为是一类独特的类，这可能会损害泛化性能。在本研究中，我们提出了IS-CSE（实例平滑对比句子嵌入）来平滑特征空间中嵌入的边界。具体而言，我们根据语义相似性从动态内存缓冲区中检索嵌入以获得正嵌入组。然后我们通过自注意力操作对组中的嵌入进行聚合，以生成平滑实例嵌入以进行进一步分析。我们在标准的语义文本相似性（STS）任务中评估了我们的方法，并实现了平均78.30％，79.47％，77.73％和79.42％的得分。",
    "tldr": "本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。",
    "en_tdlr": "This paper proposes the IS-CSE method, which uses instance smoothed contrastive learning to learn unsupervised sentence embeddings, smooths the boundaries of embeddings in the feature space, and improves the generalization performance of the model. The method achieves good performance in standard STS tasks."
}