{
    "title": "BAD: BiAs Detection for Large Language Models in the context of candidate screening. (arXiv:2305.10407v1 [cs.CL])",
    "abstract": "Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.",
    "link": "http://arxiv.org/abs/2305.10407",
    "context": "Title: BAD: BiAs Detection for Large Language Models in the context of candidate screening. (arXiv:2305.10407v1 [cs.CL])\nAbstract: Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.",
    "path": "papers/23/05/2305.10407.json",
    "total_tokens": 807,
    "translated_title": "在候选人筛选中进行大型语言模型的偏倚检测：以 BAD 模型为例",
    "translated_abstract": "应用跟踪系统（ATS）使得人才经理、招聘人员和大学招生委员会能够高效地处理大量的候选人申请。传统上，这个筛选过程是手工进行的，由于申请数的数量，存在很多瓶颈问题，并引入了许多的人为偏见。随着 ChatGPT 等大型语言模型（LLMs）的推出以及将方法应用到当前的自动化应用筛选中，这导致了进一步的偏见和公平性问题需要解决。在这个项目中，我们希望在候选人筛选的背景下，识别和量化 ChatGPT 和其他 OpenAI LLMs 中的社会偏见，以证明使用这些模型可能会延续现有的偏见和不平等在招聘过程中存在的问题。",
    "tldr": "本文介绍了 BAD 模型，通过检测大型语言模型中的偏见来证明在候选人筛选过程中存在的不公平和偏见，以解决人为干预问题。",
    "en_tdlr": "This paper presents the BAD model, which aims to identify and quantify social bias in large language models in the context of candidate screening. The study demonstrates how the use of these models could perpetuate existing biases and inequalities in the hiring process."
}