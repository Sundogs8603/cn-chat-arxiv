{
    "title": "Intelligent gradient amplification for deep neural networks. (arXiv:2305.18445v1 [cs.LG])",
    "abstract": "Deep learning models offer superior performance compared to other machine learning techniques for a variety of tasks and domains, but pose their own challenges. In particular, deep learning models require larger training times as the depth of a model increases, and suffer from vanishing gradients. Several solutions address these problems independently, but there have been minimal efforts to identify an integrated solution that improves the performance of a model by addressing vanishing gradients, as well as accelerates the training process to achieve higher performance at larger learning rates. In this work, we intelligently determine which layers of a deep learning model to apply gradient amplification to, using a formulated approach that analyzes gradient fluctuations of layers during training. Detailed experiments are performed for simpler and deeper neural networks using two different intelligent measures and two different thresholds that determine the amplification layers, and a t",
    "link": "http://arxiv.org/abs/2305.18445",
    "context": "Title: Intelligent gradient amplification for deep neural networks. (arXiv:2305.18445v1 [cs.LG])\nAbstract: Deep learning models offer superior performance compared to other machine learning techniques for a variety of tasks and domains, but pose their own challenges. In particular, deep learning models require larger training times as the depth of a model increases, and suffer from vanishing gradients. Several solutions address these problems independently, but there have been minimal efforts to identify an integrated solution that improves the performance of a model by addressing vanishing gradients, as well as accelerates the training process to achieve higher performance at larger learning rates. In this work, we intelligently determine which layers of a deep learning model to apply gradient amplification to, using a formulated approach that analyzes gradient fluctuations of layers during training. Detailed experiments are performed for simpler and deeper neural networks using two different intelligent measures and two different thresholds that determine the amplification layers, and a t",
    "path": "papers/23/05/2305.18445.json",
    "total_tokens": 919,
    "translated_title": "智能梯度放大用于深度神经网络",
    "translated_abstract": "深度学习模型在各种任务和领域中表现出卓越的性能，但也带来了一些挑战，特别是模型深度增加会导致训练时间增加，且会出现梯度消失的问题。虽然已有很多解决方案来单独解决这些问题，但是很少有综合性的解决方案来同时解决梯度消失和加速训练的问题。本文提出了一种智能地确定何时对深度学习模型中的哪些层应用梯度放大的方法，使用一种分析训练期间层梯度波动的方法进行计算。分别使用两种不同的智能度量和两个不同的阈值来确定放大的层，并提出了一种迁移学习的方法。结果表明，使用智能梯度放大可以提高深度神经网络的收敛速度，并在性能上优于传统的训练方法。",
    "tldr": "本文提出了一种智能确定梯度放大层次的方法，用于解决深度学习模型中的梯度消失问题和加速训练，实验结果表明可以提高深度神经网络的收敛速度和性能。",
    "en_tdlr": "This paper proposes an intelligent approach to determine which layers of a deep learning model to apply gradient amplification to, addressing the issues of vanishing gradients and slow training. The approach uses a formulated method to analyze gradient fluctuations of layers during training, and experiments show significant improvement in convergence rate and performance compared to traditional training methods."
}