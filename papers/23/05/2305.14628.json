{
    "title": "Mixture of Prompt Experts for Generalizable and Interpretable Question Answering. (arXiv:2305.14628v1 [cs.CL])",
    "abstract": "One of the ultimate quests of question answering (QA) is to deploy a system that can answer any type of question from the users, and refrain from answering when it does not know the answer. While recent advancements in scaling large language models (LLMs) brought significant improvements on various QA datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. In this paper, we first provide empirical evidence that state-of-the-art LLMs such as Codex suffer from poor generalizability on question types beyond those seen in the prompt. To address this, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized LLMs. We first implement each specialized model based on the same backbone model (Codex) but with prompts optimized for different reasoning categories including factual, multihop, mathematical, and commonsense reasoning. By strategically selecting the best specialized model for each g",
    "link": "http://arxiv.org/abs/2305.14628",
    "context": "Title: Mixture of Prompt Experts for Generalizable and Interpretable Question Answering. (arXiv:2305.14628v1 [cs.CL])\nAbstract: One of the ultimate quests of question answering (QA) is to deploy a system that can answer any type of question from the users, and refrain from answering when it does not know the answer. While recent advancements in scaling large language models (LLMs) brought significant improvements on various QA datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. In this paper, we first provide empirical evidence that state-of-the-art LLMs such as Codex suffer from poor generalizability on question types beyond those seen in the prompt. To address this, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized LLMs. We first implement each specialized model based on the same backbone model (Codex) but with prompts optimized for different reasoning categories including factual, multihop, mathematical, and commonsense reasoning. By strategically selecting the best specialized model for each g",
    "path": "papers/23/05/2305.14628.json",
    "total_tokens": 1126,
    "translated_abstract": "问答（QA）的终极目标之一是部署一个可以回答用户任何类型的问题并在不知道答案时避免回答的系统。虽然最近在扩展大型语言模型（LLM）方面取得了显著进展，但单个模型很难在具有不同推理能力需求的问答类型中实现泛化。本文首先提供经验证据表明，最先进的LLM（如Codex）在超出提示范围的问题类型上具有很差的泛化性能。为了解决这个问题，我们提出了一种混合提示专家（MOPE）系统，该系统可以集成多个专门的LLM。我们首先基于相同的骨干模型（Codex）实现每个专门的模型，但使用针对不同推理类别进行优化的提示，包括事实，多跳，数学和常识推理。通过为每个给定提示策略性地选择最佳的专业模型，MOPE可以在一系列基准QA数据集上实现最先进的性能，同时保持可解释性和可控性。我们的方法还显示出在领域外QA任务上的改进鲁棒性。最后，我们进行了大规模人工评估，并展示了MOPE可以生成比单个LLM基线更可解释且自然的答案。",
    "tldr": "本文提出了一种Mixture-of-Prompt-Experts（MOPE）系统，该系统通过集成多个专门的大型语言模型来解决单一模型难以泛化的问题，实现了在各种基准问答数据集上的最先进性能。同时，该方法还提高了领域外QA任务的鲁棒性，并可以生成更可解释和自然的回答。",
    "en_tdlr": "This paper proposes a Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized large language models to achieve state-of-the-art performance on various benchmark QA datasets, while also improving robustness on out-of-domain QA tasks. MOPE strategically selects the best specialized model for each given prompt, providing more interpretable and natural answers than a single large language model."
}