{
    "title": "Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.",
    "link": "http://arxiv.org/abs/2305.08048",
    "context": "Title: Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.",
    "path": "papers/23/05/2305.08048.json",
    "total_tokens": 726,
    "translated_title": "探索图神经网络泛化的理解",
    "translated_abstract": "图神经网络是应用于图结构数据的学习和表示中最广泛使用的模型。尽管它们在实际应用中取得了非凡的成功，但从理论上理解它们的工作机制仍处于初级阶段。本文从泛化的角度出发，首先考虑了随机优化的情况下传递学习的泛化间隙和梯度的高概率界限。其次，为流行的GNN提供了泛化间隙的高概率上限。理论结果揭示了影响泛化间隙的体系结构特定因素。基准数据集上的实验结果显示了理论结果和经验证据之间的一致性。我们的结果提供了理解GNN泛化的新思路。",
    "tldr": "本文探索了图神经网络的泛化；通过理论分析和实验结果发现了影响泛化间隙的体系结构因素。"
}