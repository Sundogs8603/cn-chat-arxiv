{
    "title": "Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])",
    "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-",
    "link": "http://arxiv.org/abs/2305.14788",
    "context": "Title: Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])\nAbstract: Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-",
    "path": "papers/23/05/2305.14788.json",
    "total_tokens": 1032,
    "translated_title": "将语言模型改进为自动压缩器以提高模型上下文的利用效率",
    "translated_abstract": "基于Transformer的语言模型是功能强大且广泛应用的工具，但其有限的上下文窗口和高计算成本约束了其实用性。本文提出了将预训练的语言模型改进为自动压缩器，能够将长篇文本压缩成紧凑的摘要向量，从而提高上下文的利用效率和降低计算成本。同时，摘要向量通过无监督学习的方式进行训练，并作为软提示被模型使用。",
    "tldr": "本论文提出了一种将预训练的语言模型改进为自动压缩器的方法，能够将长篇文本压缩成紧凑的摘要向量，提高上下文的利用效率和降低计算成本，同时通过在上下文学习中的应用，证明了该方法能够提高精度并降低推断成本。",
    "en_tdlr": "This paper proposes a method to adapt pre-trained language models into AutoCompressors, which can compress long texts into compact summary vectors to improve context utilization and reduce computational cost. Through evaluation on in-context learning, it shows that compressed summary vectors can improve accuracy and reduce inference cost."
}