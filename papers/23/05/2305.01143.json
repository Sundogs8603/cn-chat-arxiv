{
    "title": "Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective. (arXiv:2305.01143v1 [stat.ML])",
    "abstract": "Recently, information theoretic analysis has become a popular framework for understanding the generalization behavior of deep neural networks. It allows a direct analysis for stochastic gradient/Langevin descent (SGD/SGLD) learning algorithms without strong assumptions such as Lipschitz or convexity conditions. However, the current generalization error bounds within this framework are still far from optimal, while substantial improvements on these bounds are quite challenging due to the intractability of high-dimensional information quantities. To address this issue, we first propose a novel information theoretical measure: kernelized Renyi's entropy, by utilizing operator representation in Hilbert space. It inherits the properties of Shannon's entropy and can be effectively calculated via simple random sampling, while remaining independent of the input dimension. We then establish the generalization error bounds for SGD/SGLD under kernelized Renyi's entropy, where the mutual informati",
    "link": "http://arxiv.org/abs/2305.01143",
    "context": "Title: Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective. (arXiv:2305.01143v1 [stat.ML])\nAbstract: Recently, information theoretic analysis has become a popular framework for understanding the generalization behavior of deep neural networks. It allows a direct analysis for stochastic gradient/Langevin descent (SGD/SGLD) learning algorithms without strong assumptions such as Lipschitz or convexity conditions. However, the current generalization error bounds within this framework are still far from optimal, while substantial improvements on these bounds are quite challenging due to the intractability of high-dimensional information quantities. To address this issue, we first propose a novel information theoretical measure: kernelized Renyi's entropy, by utilizing operator representation in Hilbert space. It inherits the properties of Shannon's entropy and can be effectively calculated via simple random sampling, while remaining independent of the input dimension. We then establish the generalization error bounds for SGD/SGLD under kernelized Renyi's entropy, where the mutual informati",
    "path": "papers/23/05/2305.01143.json",
    "total_tokens": 912,
    "translated_title": "深度学习算法的泛化能力理解：基于核化Renyi熵的视角",
    "translated_abstract": "最近，信息理论分析已成为理解深度神经网络泛化行为的流行框架。它允许对随机梯度/ Langevin下降（SGD / SGLD）学习算法进行直接分析，而无需诸如Lipschitz或凸性条件等强假设。然而，在这个框架内的当前泛化误差界限仍然远非最优，而对这些界限的实质性改进由于高维信息量的不可计算性而相当具有挑战性。为解决这个问题，我们首先提出了一种新的信息理。论衡量：基于核化Renyi熵，利用希尔伯特空间中的算子表示。它继承了香农熵的属性，可通过简单的随机抽样进行有效计算，同时保持独立于输入维度。然后，我们在核化Renyi熵下建立了SGD / SGLD的泛化误差界限，其中相互信息...",
    "tldr": "本论文提出了一种新的信息理论度量方法——基于核化Renyi熵，用于在不假设Lipschitz或凸性条件的前提下对SGD / SGLD等下降学习算法进行分析，旨在提高当前泛化误差界限的优化水平。",
    "en_tdlr": "This paper proposes a new information theoretical measure, kernelized Renyi's entropy, for analyzing SGD/SGLD learning algorithms without assuming Lipschitz or convexity conditions, with the aim of improving the current suboptimal generalization error bounds within this framework."
}