{
    "title": "Masked Bayesian Neural Networks : Theoretical Guarantee and its Posterior Inference. (arXiv:2305.14765v1 [stat.ML])",
    "abstract": "Bayesian approaches for learning deep neural networks (BNN) have been received much attention and successfully applied to various applications. Particularly, BNNs have the merit of having better generalization ability as well as better uncertainty quantification. For the success of BNN, search an appropriate architecture of the neural networks is an important task, and various algorithms to find good sparse neural networks have been proposed. In this paper, we propose a new node-sparse BNN model which has good theoretical properties and is computationally feasible. We prove that the posterior concentration rate to the true model is near minimax optimal and adaptive to the smoothness of the true model. In particular the adaptiveness is the first of its kind for node-sparse BNNs. In addition, we develop a novel MCMC algorithm which makes the Bayesian inference of the node-sparse BNN model feasible in practice.",
    "link": "http://arxiv.org/abs/2305.14765",
    "context": "Title: Masked Bayesian Neural Networks : Theoretical Guarantee and its Posterior Inference. (arXiv:2305.14765v1 [stat.ML])\nAbstract: Bayesian approaches for learning deep neural networks (BNN) have been received much attention and successfully applied to various applications. Particularly, BNNs have the merit of having better generalization ability as well as better uncertainty quantification. For the success of BNN, search an appropriate architecture of the neural networks is an important task, and various algorithms to find good sparse neural networks have been proposed. In this paper, we propose a new node-sparse BNN model which has good theoretical properties and is computationally feasible. We prove that the posterior concentration rate to the true model is near minimax optimal and adaptive to the smoothness of the true model. In particular the adaptiveness is the first of its kind for node-sparse BNNs. In addition, we develop a novel MCMC algorithm which makes the Bayesian inference of the node-sparse BNN model feasible in practice.",
    "path": "papers/23/05/2305.14765.json",
    "total_tokens": 898,
    "translated_title": "掩码贝叶斯神经网络: 理论保证及后验推断",
    "translated_abstract": "贝叶斯方法在学习深度神经网络（BNN）方面备受关注并成功应用于各种应用中。特别地，BNN具有更好的泛化能力和更好的不确定性量化。为了BNN的成功，寻找适当的神经网络架构是一项重要任务，已经提出了各种算法来找到好的稀疏神经网络。本文提出了一种新的节点稀疏BNN模型，具有良好的理论性质和计算可行性。我们证明了对真实模型的后验浓度速率接近最小化最优，并且适应真实模型的平滑度。特别是，该适应性是节点稀疏BNN的首个适应性。此外，我们开发了一种新颖的MCMC算法，使节点稀疏BNN模型的贝叶斯推断在实践中变得可行。",
    "tldr": "本文提出了一种新的节点稀疏BNN模型，并证明了其后验浓度速率接近最小化最优。同时，开发了一种新颖的MCMC算法，使节点稀疏BNN模型的贝叶斯推断在实践中变得可行。",
    "en_tdlr": "This paper proposes a new node-sparse BNN model with good theoretical properties and develops a novel MCMC algorithm, which makes the Bayesian inference of the node-sparse BNN model feasible in practice. The posterior concentration rate to the true model is proved to be near minimax optimal and adaptive to the smoothness of the true model."
}