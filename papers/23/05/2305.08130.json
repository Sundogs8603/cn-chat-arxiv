{
    "title": "Inverse Reinforcement Learning With Constraint Recovery. (arXiv:2305.08130v1 [cs.LG])",
    "abstract": "In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.",
    "link": "http://arxiv.org/abs/2305.08130",
    "context": "Title: Inverse Reinforcement Learning With Constraint Recovery. (arXiv:2305.08130v1 [cs.LG])\nAbstract: In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.",
    "path": "papers/23/05/2305.08130.json",
    "total_tokens": 834,
    "translated_title": "带有约束恢复的逆强化学习算法",
    "translated_abstract": "本文提出了一种针对约束马尔可夫决策过程（CMDP）问题的新型逆强化学习（IRL）算法。在标准的IRL问题中，逆学习者或代理人寻求恢复MDP的奖励函数，给定最优策略的轨迹演示集。在本文中，我们不仅寻求推断CMDP的奖励函数，还要推断约束。利用最大熵原理，我们表明IRL与约束恢复（IRL-CR）问题可以被视为一个受限制的非凸优化问题。我们将其减少为交替受限制的优化问题，这些子问题是凸的。我们使用指数梯度下降算法来解决它。 最后，我们证明了我们的算法在网格世界环境中的有效性。",
    "tldr": "本论文提出了一种带有约束恢复的逆强化学习算法，以针对约束马尔可夫决策过程问题。通过最大熵原理，将其视为一个受限制的非凸优化问题，并使用指数梯度下降算法来解决它。",
    "en_tdlr": "This paper proposes a novel inverse reinforcement learning algorithm with constraint recovery for constrained Markov decision process problems. By using the principle of maximum entropy, the problem is viewed as a constrained non-convex optimization problem and solved using exponentiated gradient descent algorithm."
}