{
    "title": "Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)",
    "abstract": "Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\\times$ faster and $18.8\\%$ more accurate than the strong baselines.",
    "link": "http://arxiv.org/abs/2305.14065",
    "context": "Title: Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)\nAbstract: Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\\times$ faster and $18.8\\%$ more accurate than the strong baselines.",
    "path": "papers/23/05/2305.14065.json",
    "total_tokens": 946,
    "translated_title": "不要训练它：图神经网络的线性神经架构搜索",
    "translated_abstract": "图神经网络的神经架构搜索（NAS-GNN）已经显著地提高了手动设计的图神经网络的性能。然而，这些方法继承了传统NAS方法的问题，如高计算成本和优化难度。更重要的是，以前的NAS方法忽视了GNN的独特性，即GNN具有无需训练就具有表现力的特点。采用随机初始化的权重，我们可以通过稀疏编码目标寻找最优的架构参数，并得出一种新的NAS-GNN方法，即神经结构编码（NAC）。因此，我们的NAC在GNN上实现了无更新方案，可以在线性时间内高效计算。在多个GNN基准数据集上的实证评估表明，我们的方法导致了最先进的性能，比强基线方法快200倍，精度提高了18.8％。",
    "tldr": "本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。",
    "en_tdlr": "This paper proposes a novel neural architecture search method, Neural Architecture Coding (NAC), for graph neural networks (GNNs), which finds optimal architecture parameters through sparse coding objective without training, exploiting the expressive power of GNNs. NAC holds a no-update scheme on GNNs and is computationally efficient in linear time, achieving state-of-the-art performance on multiple benchmark datasets with 200x faster computation and 18.8% higher accuracy than strong baselines."
}