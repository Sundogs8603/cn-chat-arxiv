{
    "title": "On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])",
    "abstract": "Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa",
    "link": "http://arxiv.org/abs/2305.03686",
    "context": "Title: On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])\nAbstract: Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa",
    "path": "papers/23/05/2305.03686.json",
    "total_tokens": 808,
    "translated_title": "关于神经网络原像近似的研究",
    "translated_abstract": "神经网络验证主要关注局部鲁棒性，然而，通常需要知道给定属性是否在整个输入域内全局成立，如果不成立，则需要知道属性成立的输入比例是多少。尽管精确的原像生成可以构建神经网络的等价表示，但在规模上是难以处理的。本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似。我们的算法通过将输入区域划分为子区域，其中神经网络松弛边界变得更紧，迭代地最小化体积逼近误差。我们进一步采用采样和可微体积逼近来优先划分区域，并优化松弛的参数，从而实现更快的改进和更高的压缩度。",
    "tldr": "本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。",
    "en_tdlr": "This paper proposes an efficient and practical anytime algorithm based on linear relaxation for generating symbolic under-approximations of the preimage of neural networks, which leads to faster improvement and higher compression."
}