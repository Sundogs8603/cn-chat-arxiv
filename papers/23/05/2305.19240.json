{
    "title": "NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)",
    "abstract": "Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.",
    "link": "http://arxiv.org/abs/2305.19240",
    "context": "Title: NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)\nAbstract: Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.",
    "path": "papers/23/05/2305.19240.json",
    "total_tokens": 934,
    "translated_title": "NetHack很难被黑客入侵。",
    "translated_abstract": "神经策略学习方法在各种控制问题中取得了显著的成果，从Atari游戏到模拟运动。然而，在长视野任务中，尤其是在具有多模态观测的开放式环境中，比如流行的地牢探险游戏NetHack中，这些方法都面临困难。有趣的是，NeurIPS 2021 NetHack挑战赛表明，符号代理在中位游戏得分上超过神经方法四倍。在本文中，我们深入探讨了这种性能差距背后的原因，并对NetHack的神经策略学习进行了广泛研究。为了进行这项研究，我们分析了获胜的符号代理，并扩展了其代码库以跟踪内部策略选择，以生成其中一个最大的可用演示数据集。利用这个数据集，我们研究了以下几个方面：(i) 动作层级的优势；(ii) 神经架构的改进；以及 (iii) 强化学习与模仿学习的整合。",
    "tldr": "本文研究了神经策略学习在NetHack中的性能差距，并分析了获胜的符号代理，提出了动作层级的优势、神经架构的改进以及强化学习与模仿学习的整合等方面可能提升性能的方法。",
    "en_tdlr": "This paper investigates the performance gap of neural policy learning in NetHack and analyzes the winning symbolic agent, proposing potential improvements in action hierarchy, neural architecture, and the integration of reinforcement learning with imitation learning."
}