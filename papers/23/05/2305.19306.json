{
    "title": "A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])",
    "abstract": "While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio",
    "link": "http://arxiv.org/abs/2305.19306",
    "context": "Title: A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])\nAbstract: While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio",
    "path": "papers/23/05/2305.19306.json",
    "total_tokens": 922,
    "translated_title": "一张图值得一比特的差异性：当图的对比学习遇到脉冲神经网络时。",
    "translated_abstract": "虽然对比自监督学习已经成为图神经网络的事实上的学习范式，但对高任务准确性的追求需要大的隐藏维度来学习信息丰富、有区别性的全精度表示，这引发了对计算、存储和能源消耗负担（在现实世界应用中大多被忽略）的担忧。本文探索了一种有前途的方向，即用脉冲神经网络（SNNs）进行图的对比学习（GCL），利用稀疏和二元特性来学习更具生物可行性和紧凑性的表示。我们提出了SpikeGCL，一种学习图的二值化1比特表示的新型GCL框架，平衡了效率和性能之间的权衡。我们提供了理论保证，证明SpikeGCL在表达能力上与其全精度对应物具有可比性。实验结果表明，通过将表示存储压缩近32倍，SpikeGCL在保持高准确性的同时可以实现高效的学习。",
    "tldr": "本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。",
    "en_tdlr": "This paper proposes SpikeGCL, a graph contrastive learning framework that uses binarized 1-bit representations to improve efficiency and save resources. The experimental results demonstrate high efficiency learning with nearly 32x representation storage compression."
}