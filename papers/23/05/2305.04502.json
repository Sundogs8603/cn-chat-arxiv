{
    "title": "MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization. (arXiv:2305.04502v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter optimization (HPO) is a powerful technique for automating the tuning of machine learning (ML) models. However, in many real-world applications, accuracy is only one of multiple performance criteria that must be considered. Optimizing these objectives simultaneously on a complex and diverse search space remains a challenging task. In this paper, we propose MO-DEHB, an effective and flexible multi-objective (MO) optimizer that extends the recent evolutionary Hyperband method DEHB. We validate the performance of MO-DEHB using a comprehensive suite of 15 benchmarks consisting of diverse and challenging MO problems, including HPO, neural architecture search (NAS), and joint NAS and HPO, with objectives including accuracy, latency and algorithmic fairness. A comparative study against state-of-the-art MO optimizers demonstrates that MO-DEHB clearly achieves the best performance across our 15 benchmarks.",
    "link": "http://arxiv.org/abs/2305.04502",
    "context": "Title: MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization. (arXiv:2305.04502v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter optimization (HPO) is a powerful technique for automating the tuning of machine learning (ML) models. However, in many real-world applications, accuracy is only one of multiple performance criteria that must be considered. Optimizing these objectives simultaneously on a complex and diverse search space remains a challenging task. In this paper, we propose MO-DEHB, an effective and flexible multi-objective (MO) optimizer that extends the recent evolutionary Hyperband method DEHB. We validate the performance of MO-DEHB using a comprehensive suite of 15 benchmarks consisting of diverse and challenging MO problems, including HPO, neural architecture search (NAS), and joint NAS and HPO, with objectives including accuracy, latency and algorithmic fairness. A comparative study against state-of-the-art MO optimizers demonstrates that MO-DEHB clearly achieves the best performance across our 15 benchmarks.",
    "path": "papers/23/05/2305.04502.json",
    "total_tokens": 823,
    "translated_title": "MO-DEHB:多目标优化的进化超带方法",
    "translated_abstract": "超参数优化是自动调整机器学习模型的强大技术。然而，在许多现实应用中，准确性只是必须考虑的多个性能标准之一。在复杂多元化的搜索空间中同时优化这些目标仍然是一项具有挑战性的任务。在本文中，我们提出了MO-DEHB，一种有效灵活的多目标优化器，它扩展了最近的进化Hyperband方法DEHB。我们使用一套包括15个不同且具有挑战性的多目标问题的全面基准套件来验证MO-DEHB的性能，其中包括超参数优化、神经架构搜索(NAS)和联合NAS和HPO，目标包括准确性、延迟和算法公平性。与最先进的多目标优化器进行比较研究表明，MO-DEHB在这15个基准测试中表现最好。",
    "tldr": "MO-DEHB是一种扩展自DEHB的多目标优化器，可应用于各种多目标问题，并在多个性能指标上表现出最优性能。",
    "en_tdlr": "MO-DEHB is a multi-objective optimizer that extends DEHB and has been validated on a comprehensive suite of 15 benchmarks, demonstrating superior performance across a range of performance criteria."
}