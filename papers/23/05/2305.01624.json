{
    "title": "UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v1 [cs.CL])",
    "abstract": "Recent research demonstrates that external knowledge injection can advance pre-trained language models (PLMs) in a variety of downstream NLP tasks. However, existing knowledge injection methods are either applicable to structured knowledge or unstructured knowledge, lacking a unified usage. In this paper, we propose a UNified knowledge inTERface, UNTER, to provide a unified perspective to exploit both structured knowledge and unstructured knowledge. In UNTER, we adopt the decoder as a unified knowledge interface, aligning span representations obtained from the encoder with their corresponding knowledge. This approach enables the encoder to uniformly invoke span-related knowledge from its parameters for downstream applications. Experimental results show that, with both forms of knowledge injected, UNTER gains continuous improvements on a series of knowledge-driven NLP tasks, including entity typing, named entity recognition and relation extraction, especially in low-resource scenarios.",
    "link": "http://arxiv.org/abs/2305.01624",
    "context": "Title: UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v1 [cs.CL])\nAbstract: Recent research demonstrates that external knowledge injection can advance pre-trained language models (PLMs) in a variety of downstream NLP tasks. However, existing knowledge injection methods are either applicable to structured knowledge or unstructured knowledge, lacking a unified usage. In this paper, we propose a UNified knowledge inTERface, UNTER, to provide a unified perspective to exploit both structured knowledge and unstructured knowledge. In UNTER, we adopt the decoder as a unified knowledge interface, aligning span representations obtained from the encoder with their corresponding knowledge. This approach enables the encoder to uniformly invoke span-related knowledge from its parameters for downstream applications. Experimental results show that, with both forms of knowledge injected, UNTER gains continuous improvements on a series of knowledge-driven NLP tasks, including entity typing, named entity recognition and relation extraction, especially in low-resource scenarios.",
    "path": "papers/23/05/2305.01624.json",
    "total_tokens": 878,
    "translated_title": "UNTER: 一种用于增强预训练语言模型的统一知识接口",
    "translated_abstract": "最近的研究表明，外部知识注入可以提高预训练语言模型（PLMs）在各种下游NLP任务中的性能。但是，现有的知识注入方法适用于结构化知识或非结构化知识，缺乏统一的使用方式。本文提出了一种名为UNTER的统一知识接口，以提供利用结构化知识和非结构化知识的统一视角。在UNTER中，我们采用解码器作为统一的知识接口，将从编码器获取的跨度表示与其对应的知识进行对齐。这种方法使编码器能够从其参数中统一调用下游应用程序的跨度相关的知识。实验结果表明，通过注入两种形式的知识，UNTER在一系列知识驱动的NLP任务中获得了不断的改进，包括实体类型、命名实体识别和关系抽取，尤其在低资源场景中效果明显。",
    "tldr": "本篇论文提出了一种名为UNTER的统一知识接口，可以同时利用结构化和非结构化知识，从而提高预训练语言模型（PLMs）性能，在实验中表现出不断的改进。",
    "en_tdlr": "This paper proposes a unified knowledge interface called UNTER, which can exploit both structured and unstructured knowledge to enhance pre-trained language models (PLMs), and achieves continuous improvements on a series of knowledge-driven NLP tasks in experiments."
}