{
    "title": "CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing. (arXiv:2305.02865v1 [cs.CL])",
    "abstract": "Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics, is getting increasing attention for its detrimental effect on the generalization ability of NLU models. Existing works focus on eliminating dataset bias by down-weighting problematic data in the training process, which induce the omission of valid feature information while mitigating bias. In this work, We analyze the causes of dataset bias from the perspective of causal inference and propose CausalAPM, a generalizable literal disentangling framework to ameliorate the bias problem from feature granularity. The proposed approach projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and QQP) demonstrate that our proposed framework significantly improves the OOD generalization performance while maintaining ID performance.",
    "link": "http://arxiv.org/abs/2305.02865",
    "context": "Title: CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing. (arXiv:2305.02865v1 [cs.CL])\nAbstract: Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics, is getting increasing attention for its detrimental effect on the generalization ability of NLU models. Existing works focus on eliminating dataset bias by down-weighting problematic data in the training process, which induce the omission of valid feature information while mitigating bias. In this work, We analyze the causes of dataset bias from the perspective of causal inference and propose CausalAPM, a generalizable literal disentangling framework to ameliorate the bias problem from feature granularity. The proposed approach projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and QQP) demonstrate that our proposed framework significantly improves the OOD generalization performance while maintaining ID performance.",
    "path": "papers/23/05/2305.02865.json",
    "total_tokens": 813,
    "translated_title": "CausalAPM: 用于NLU去偏置的通用文本解缠框架",
    "translated_abstract": "数据集偏置问题越来越引起人们对NLU模型推广能力的关注。本文从因果推断的角度分析了数据集偏置问题的原因，提出了CausalAPM，一种通用的文本解缠框架来解决偏置问题。该方法将文字和语义信息投射到独立的特征子空间中，并限制了文字信息在后续预测中的参与程度。在三个NLP基准测试中广泛的实验表明，我们的框架显著提高了OOD的推广性能，同时保持了ID的性能水平。",
    "tldr": "CausalAPM是一个通用的NLU文本解缠框架，它将文字和语义信息投射到独立的特征子空间中，并限制了文字信息在后续预测中的参与程度。该框架可以有效地应对数据集偏置问题，提高了推广性能而不损失性能水平。",
    "en_tdlr": "CausalAPM is a generalizable literal disentangling framework for NLU debiasing. It projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. The proposed framework significantly improves the OOD generalization performance while maintaining ID performance, effectively addressing the dataset bias problem."
}