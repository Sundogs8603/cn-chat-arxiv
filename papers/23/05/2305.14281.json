{
    "title": "Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining. (arXiv:2305.14281v2 [cs.CL] UPDATED)",
    "abstract": "Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data.",
    "link": "http://arxiv.org/abs/2305.14281",
    "context": "Title: Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining. (arXiv:2305.14281v2 [cs.CL] UPDATED)\nAbstract: Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data.",
    "path": "papers/23/05/2305.14281.json",
    "total_tokens": 875,
    "translated_title": "弱监督学习在多模态预训练中的视觉关系",
    "translated_abstract": "最近在视觉与语言预训练中的研究中，调查了从目标检测数据中的监督信号，以学习更好、细粒度的多模态表示。在本研究中，我们进一步探讨了如何利用小规模的视觉关系数据进行监督。具体而言，我们提出了两种预训练方法来在多模态设置中对视觉实体进行语境化。通过言语化场景图，我们将视觉关系三元组转换为结构化标题，并将其作为额外的图像描述。通过遮罩关系预测，我们进一步鼓励将图像区域中的实体与视觉上遮罩的上下文进行关联。当应用于在大量Web数据上预训练的强基线模型时，对于粗粒度和细粒度任务的零样本评估显示了我们的方法在从弱监督关系数据中学习多模态表示方面的有效性。",
    "tldr": "本研究提出了两种弱监督学习方法在多模态预训练中学习视觉关系，通过转换视觉关系数据为结构化标题和遮罩关系预测，实现了从弱监督关系数据中学习多模态表示的有效性。",
    "en_tdlr": "This study proposes two weakly-supervised learning approaches for visual relations in multimodal pretraining, by converting visual relation data into structured captions and using masked relation prediction, effectively learning multimodal representations from weakly-supervised relations data."
}