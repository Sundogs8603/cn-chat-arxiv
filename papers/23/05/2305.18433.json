{
    "title": "Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models. (arXiv:2305.18433v1 [cs.LG])",
    "abstract": "Most existing cross-modal generative methods based on diffusion models use guidance to provide control over the latent space to enable conditional generation across different modalities. Such methods focus on providing guidance through separately-trained models, each for one modality. As a result, these methods suffer from cross-modal information loss and are limited to unidirectional conditional generation. Inspired by how humans synchronously acquire multi-modal information and learn the correlation between modalities, we explore a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase to better mimic the learning process in the brain. Our empirical results demonstrate that our approach can achieve data generation conditioned on all correlated modalities.",
    "link": "http://arxiv.org/abs/2305.18433",
    "context": "Title: Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models. (arXiv:2305.18433v1 [cs.LG])\nAbstract: Most existing cross-modal generative methods based on diffusion models use guidance to provide control over the latent space to enable conditional generation across different modalities. Such methods focus on providing guidance through separately-trained models, each for one modality. As a result, these methods suffer from cross-modal information loss and are limited to unidirectional conditional generation. Inspired by how humans synchronously acquire multi-modal information and learn the correlation between modalities, we explore a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase to better mimic the learning process in the brain. Our empirical results demonstrate that our approach can achieve data generation conditioned on all correlated modalities.",
    "path": "papers/23/05/2305.18433.json",
    "total_tokens": 743,
    "translated_title": "基于扩散模型的认知跨模态数据生成",
    "translated_abstract": "多数基于扩散模型的跨模态生成方法使用指导方式在潜在空间上提供控制，以实现不同模态的条件生成。这些方法通过分别训练每个模态的模型来提供指导，因此受到跨模态信息丢失的影响，且仅能实现单向条件生成。本文灵感来自于人类同步获取多模态信息并学习模态间相关性的方式，我们探讨了使用通道图像调节的多模态扩散模型训练和采样方案，以在训练阶段学习跨模态相关性，更好地模仿大脑中的学习过程。我们的实验结果表明，我们的方法可以实现基于所有相关模态的数据生成。",
    "tldr": "本文提出一种基于多模态扩散模型训练和采样的数据生成方法，使用通道图像调节来学习跨模态相关性，实现条件生成。",
    "en_tdlr": "This paper proposes a data generation method based on a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase, achieving conditional generation."
}