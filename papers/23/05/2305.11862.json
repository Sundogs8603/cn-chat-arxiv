{
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, it is inefficient to generate all target tokens because a prediction error of a target token may cause a catastrophe in predicting subsequent tokens and because the computational cost grows quadratically with the target sequence length. This paper proposes to predict a set of edit operations for the source text for local sequence transduction tasks. Representing an edit operation with a span of the source text and changed tokens, we can reduce the length of the target sequence and thus the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit operations. Experiments show that the proposed method achieves comparable performanc",
    "link": "http://arxiv.org/abs/2305.11862",
    "context": "Title: Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, it is inefficient to generate all target tokens because a prediction error of a target token may cause a catastrophe in predicting subsequent tokens and because the computational cost grows quadratically with the target sequence length. This paper proposes to predict a set of edit operations for the source text for local sequence transduction tasks. Representing an edit operation with a span of the source text and changed tokens, we can reduce the length of the target sequence and thus the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit operations. Experiments show that the proposed method achieves comparable performanc",
    "path": "papers/23/05/2305.11862.json",
    "total_tokens": 848,
    "translated_title": "利用大语言模型预测编辑操作来减少序列长度",
    "translated_abstract": "大型语言模型在各种任务中展示出了卓越的性能，并获得了显着的关注。 LLMs也被用于本地序列转换任务，包括语法错误修正（GEC）和形式风格转换，在这些任务中，源文本中的大部分记号保持不变。 然而，生成所有目标记号是低效率的，因为目标记号的预测错误可能导致在预测后续记号时出现灾难，并且随着目标序列长度的增加，计算成本呈二次增长。 本文提出了一种预测本地序列转换任务的源文本的一组编辑操作的方法。 通过表示一个编辑操作的源文本跨度和更改的记号，我们可以减少目标序列的长度，从而减少推断的计算成本。 我们在编辑操作的监督数据上应用指导调优LLMs。 实验表明，所提出的方法达到了可比较的性能。",
    "tldr": "本文提出利用大型语言模型预测源文本中的编辑操作来减少序列长度的方法，从而减少计算成本，实验表明其达到了可比较的性能。",
    "en_tdlr": "This paper proposes using large language models to predict edit operations in source text for local sequence transduction tasks, reducing the length of target sequences and thus computational cost, with comparable performance shown in experiments."
}