{
    "title": "CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])",
    "abstract": "Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to down-stream tasks such as object detection and segmentation. However, adapting these models for object counting, which involves estimating the number of objects in an image, remains a formidable challenge. In this study, we conduct the first exploration of transferring visual-language models for class-agnostic object counting. Specifically, we propose CLIP-Count, a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, without requiring any finetuning on specific object classes. To align the text embedding with dense image features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level image representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module that propagates semantic information across different resolution level",
    "link": "http://arxiv.org/abs/2305.07304",
    "context": "Title: CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])\nAbstract: Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to down-stream tasks such as object detection and segmentation. However, adapting these models for object counting, which involves estimating the number of objects in an image, remains a formidable challenge. In this study, we conduct the first exploration of transferring visual-language models for class-agnostic object counting. Specifically, we propose CLIP-Count, a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, without requiring any finetuning on specific object classes. To align the text embedding with dense image features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level image representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module that propagates semantic information across different resolution level",
    "path": "papers/23/05/2305.07304.json",
    "total_tokens": 911,
    "translated_title": "CLIP-Count：面向文本引导下零样本物体计数",
    "translated_abstract": "最近视觉-语言模型的进展显示出卓越的零样本文本-图像匹配能力，可转移到对象检测和分割等下游任务。然而，将这些模型适应于目标计数——估计图像中对象的数量，仍然是一个巨大的挑战。在本研究中，我们进行首次探索，将视觉-语言模型转移用于无类别偏见的物体计数。具体而言，我们提出了CLIP-Count，这是一种新颖的流程，它通过零样本的文本引导，为开放词汇对象估计密度图，而不需要在特定对象类别上进行任何微调。为了对齐文本嵌入和密集图像特征，我们引入了一个补丁-文本对比损失，指导模型学习有用的补丁级图像表示以进行密集预测。此外，我们设计了一个分层的patch-text交互模块，可以在不同的分辨率级别上传递语义信息。",
    "tldr": "该研究提出了CLIP-Count，一种基于零样本文本引导的物体计数方法，不需要对特定对象类别进行微调，通过引入补丁-文本对比损失和分层的patch-text交互模块，获得了高效的密集预测结果。",
    "en_tdlr": "CLIP-Count is proposed as a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, and it achieves efficient dense prediction by introducing patch-text contrastive loss and a hierarchical patch-text interaction module, without requiring any finetuning on specific object classes."
}