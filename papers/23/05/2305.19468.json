{
    "title": "Efficient Implementation of a Multi-Layer Gradient-Free Online-Trainable Spiking Neural Network on FPGA. (arXiv:2305.19468v1 [cs.NE])",
    "abstract": "This paper presents an efficient hardware implementation of the recently proposed Optimized Deep Event-driven Spiking Neural Network Architecture (ODESA). ODESA is the first network to have end-to-end multi-layer online local supervised training without using gradients and has the combined adaptation of weights and thresholds in an efficient hierarchical structure. This research shows that the network architecture and the online training of weights and thresholds can be implemented efficiently on a large scale in hardware. The implementation consists of a multi-layer Spiking Neural Network (SNN) and individual training modules for each layer that enable online self-learning without using back-propagation. By using simple local adaptive selection thresholds, a Winner-Takes-All (WTA) constraint on each layer, and a modified weight update rule that is more amenable to hardware, the trainer module allocates neuronal resources optimally at each layer without having to pass high-precision er",
    "link": "http://arxiv.org/abs/2305.19468",
    "context": "Title: Efficient Implementation of a Multi-Layer Gradient-Free Online-Trainable Spiking Neural Network on FPGA. (arXiv:2305.19468v1 [cs.NE])\nAbstract: This paper presents an efficient hardware implementation of the recently proposed Optimized Deep Event-driven Spiking Neural Network Architecture (ODESA). ODESA is the first network to have end-to-end multi-layer online local supervised training without using gradients and has the combined adaptation of weights and thresholds in an efficient hierarchical structure. This research shows that the network architecture and the online training of weights and thresholds can be implemented efficiently on a large scale in hardware. The implementation consists of a multi-layer Spiking Neural Network (SNN) and individual training modules for each layer that enable online self-learning without using back-propagation. By using simple local adaptive selection thresholds, a Winner-Takes-All (WTA) constraint on each layer, and a modified weight update rule that is more amenable to hardware, the trainer module allocates neuronal resources optimally at each layer without having to pass high-precision er",
    "path": "papers/23/05/2305.19468.json",
    "total_tokens": 1038,
    "translated_title": "FPGA上多层梯度自由在线可训练脉冲神经网络的有效实现",
    "translated_abstract": "本文介绍了最近提出的优化深度事件驱动脉冲神经网络体系结构（ODESA）的高效硬件实现。ODESA是第一个具有端到端多层在线本地监督训练且不使用梯度的网络，并具有有效的层次结构中的权重和阈值的组合适应性。本研究表明，可以在硬件上高效地实现网络架构和重量和阈值的在线培训。实现包括多层脉冲神经网络（SNN）和每个层的单独训练模块，可以实现在线自学习而不使用反向传播。通过使用简单的本地自适应选择阈值，在每个层上进行赢家通吃（WTA）约束，并使用更适合硬件的修改重量更新规则，训练模块可以在每个层中最优地分配神经元资源，而不必将高精度误差信号传递给较低层的神经元。在可扩展的FPGA上提出的硬件实现在MNIST数据集上实现了99.5％的高分类精度，同时运行速度更快，功耗更低。",
    "tldr": "本文介绍了一种在FPGA上有效实现多层梯度自由在线可训练脉冲神经网络的方法，通过使用本地自适应选择阈值和适合硬件的修改重量更新规则，并实现了99.5％的高分类精度。",
    "en_tdlr": "This paper presents an efficient hardware implementation of a multi-layer gradient-free online-trainable spiking neural network on FPGA, achieving a high classification accuracy of 99.5% on the MNIST dataset by using local adaptive selection thresholds and a modified weight update rule that is more amenable to hardware."
}