{
    "title": "Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution. (arXiv:2305.17325v1 [cs.CL])",
    "abstract": "Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68% and improves the ROUGE-L score by 1.5 on average.",
    "link": "http://arxiv.org/abs/2305.17325",
    "context": "Title: Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution. (arXiv:2305.17325v1 [cs.CL])\nAbstract: Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68% and improves the ROUGE-L score by 1.5 on average.",
    "path": "papers/23/05/2305.17325.json",
    "total_tokens": 877,
    "translated_title": "为什么零样本跨语言生成失败？原因及解决方案",
    "translated_abstract": "零样本跨语言转移是指在一种语言中训练多语言模型来执行任务，然后将其应用于另一种语言。虽然零样本跨语言转移方法在各种分类任务中取得了成功，但其在自然语言生成任务中的性能则不足，并且有时会输出错误的语言。在我们的研究中，我们展示了微调过程学习语言不变表示的好处是分类任务但对于生成任务有害。基于此，我们提出了一种简单的方法来规范化模型，使其不会学习语言不变的表示，并提出了一种在目标语言中选择不需要开发集的模型检查点的方法，两者都可以提高生成质量。对三个语义多样的生成任务的实验表明，我们的方法将偶然翻译问题减少了68％，平均提高了1.5的ROUGE-L得分。",
    "tldr": "零样本跨语言生成失败的原因是神经网络模型在学习分类任务中的语言不变表示时，会影响在生成任务中的准确性，因此我们提出一种简单而有效的方法通过规范化模型来解决这个问题并提高生成质量。",
    "en_tdlr": "The reason why zero-shot cross-lingual generation fails is that the neural network model learning language invariant representations in the classification task may affect the accuracy in the generation task. Therefore, we propose a simple and effective method to solve this problem by regulating the model and improving the generation quality."
}