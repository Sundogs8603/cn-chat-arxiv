{
    "title": "Can Public Large Language Models Help Private Cross-device Federated Learning?. (arXiv:2305.12132v1 [cs.LG])",
    "abstract": "We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private model by taking advantage ",
    "link": "http://arxiv.org/abs/2305.12132",
    "context": "Title: Can Public Large Language Models Help Private Cross-device Federated Learning?. (arXiv:2305.12132v1 [cs.LG])\nAbstract: We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private model by taking advantage ",
    "path": "papers/23/05/2305.12132.json",
    "total_tokens": 923,
    "translated_title": "公共大型语言模型能否帮助私有交叉设备联邦学习？",
    "translated_abstract": "本文研究了（差分）私有联邦学习（FL）中的语言模型。交叉设备FL中的语言模型相对较小，在训练中的大规模并行性参与下可以使用有意义的形式化用户级差分隐私（DP）保证进行训练。最近，公共数据已用于改善大型和小型语言模型的隐私-效用权衡。在本研究中，我们系统地研究了使用大规模公共数据和LLMs来帮助设备上的FL模型进行差分私有训练，并通过蒸馏技术进一步改善隐私-效用权衡。此外，我们提出了一种新颖的分布匹配算法，通过理论基础对公共数据进行接近私有数据分布的采样，显著提高了（预）训练公共数据的样本效率。所提出的方法是通过利用公共大型语言模型训练私有模型的高效和有效方法。",
    "tldr": "本文探讨在差分私有联邦学习中如何利用大型公共语言模型提升隐私和效用权衡，并提出一种分布匹配算法提高公共数据的训练效率和隐私性，为训练私有模型提供有效方法。",
    "en_tdlr": "This paper explores how to use large-scale public language models to improve the privacy-utility tradeoff in differentially private federated learning, proposes a novel distribution matching algorithm to improve the efficiency and privacy of pre-training public data, and provides an effective method for training private models by leveraging public language models."
}