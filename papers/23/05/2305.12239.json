{
    "title": "Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)",
    "abstract": "The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed ARO-DDPG algorithm and observe better empir",
    "link": "http://arxiv.org/abs/2305.12239",
    "context": "Title: Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)\nAbstract: The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed ARO-DDPG algorithm and observe better empir",
    "path": "papers/23/05/2305.12239.json",
    "total_tokens": 1078,
    "translated_title": "带有确定性策略搜索的离策略平均回报行动者-评论家算法",
    "translated_abstract": "平均回报准则相对较少被研究，因为强化学习文献中的大多数现有工作考虑了贴现回报准则。近期有一些关于基于策略的平均回报行动者-评论家算法的工作，但离策略平均回报行动者-评论家算法相对较少探索。本文提出了关于平均回报性能准则的基于策略和离策略确定性策略梯度定理。利用这些定理，我们还提出了一种平均回报离策略深度确定性策略梯度算法（ARO-DDPG）。我们首先使用基于ODE的方法展示了渐近收敛性分析。随后，我们提供了结果随机逼近方案的有限时间分析，使用线性函数逼近器获得了一个$\\epsilon$-最优稳定策略，其样本复杂度为$\\Omega(\\epsilon^{-2.5})$。我们比较了我们提出的ARO-DDPG算法的平均回报性能，并观察到更好的经验表现。",
    "tldr": "本文介绍了带有确定性策略搜索的离策略平均回报行动者-评论家算法，并提出了基于策略和离策略的确定性策略梯度定理。使用这些定理，本文还提出了一种平均回报离策略深度确定性策略梯度算法（ARO-DDPG）。该算法在渐近收敛性分析和有限时间分析中展示了较好的性能，并获得了$\\epsilon$-最优稳定策略。",
    "en_tdlr": "This paper introduces an off-policy average reward actor-critic algorithm with deterministic policy search, and presents both on-policy and off-policy deterministic policy gradient theorems. Using these theorems, the paper proposes an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. The algorithm demonstrates good performance in asymptotic convergence analysis and finite time analysis, and achieves an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$."
}