{
    "title": "Are Large Kernels Better Teachers than Transformers for ConvNets?. (arXiv:2305.19412v1 [cs.CV])",
    "abstract": "This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. KD is widely used to boost the performance of small-kernel ConvNets. However, previous research shows that it is not quite effective to distill knowledge (e.g., global information) from Transformers to small-kernel ConvNets, presumably due to their disparate architectures. We hereby carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. Our findings are backe",
    "link": "http://arxiv.org/abs/2305.19412",
    "context": "Title: Are Large Kernels Better Teachers than Transformers for ConvNets?. (arXiv:2305.19412v1 [cs.CV])\nAbstract: This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. KD is widely used to boost the performance of small-kernel ConvNets. However, previous research shows that it is not quite effective to distill knowledge (e.g., global information) from Transformers to small-kernel ConvNets, presumably due to their disparate architectures. We hereby carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. Our findings are backe",
    "path": "papers/23/05/2305.19412.json",
    "total_tokens": 834,
    "translated_title": "大核与变形金刚网络，哪个更适合作为ConvNet的教师？",
    "translated_abstract": "本文揭示了新出现的大核卷积神经网络（ConvNets）的一种新优点：作为小核ConvNets在知识蒸馏（KD）中的教师。虽然变形金刚网络在各个领域的性能已经达到了最先进水平，但小核ConvNets由于卷积运算和紧凑的权重共享，被视为更适合资源有限的应用。KD被广泛用于提高小核ConvNets的性能。然而，以前的研究表明，从变形金刚网络中蒸馏知识（例如全局信息）到小核ConvNets并不是非常有效，可能是因为它们的不同体系结构。我们的研究表明，现代大核ConvNets是小核ConvNets更为有效的教师，因为它们的体系结构更为相似。",
    "tldr": "本文揭示了新出现的大核ConvNets有效地用作小核ConvNets知识蒸馏（KD）的教师的优点。"
}