{
    "title": "Model Debiasing via Gradient-based Explanation on Representation. (arXiv:2305.12178v1 [cs.LG])",
    "abstract": "Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicti",
    "link": "http://arxiv.org/abs/2305.12178",
    "context": "Title: Model Debiasing via Gradient-based Explanation on Representation. (arXiv:2305.12178v1 [cs.LG])\nAbstract: Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicti",
    "path": "papers/23/05/2305.12178.json",
    "total_tokens": 853,
    "translated_title": "基于梯度说明的表示法去偏见模型",
    "translated_abstract": "机器学习系统会对某些人口统计学群体产生偏见，即不公平现象。近期的解决方法是通过分离式表示学习学习潜在码（即表示法），然后丢弃与敏感属性（如性别）相关的码。但这些方法在处理现实数据（特别是非结构化数据）时，可能会遗漏代理属性（敏感属性的代理），并且受到不完全分离的影响，导致公平性能下降和下游任务中损失有用信息。本文提出了一种新的公平性框架，针对敏感属性和代理属性进行去偏见处理，提高下游任务模型的预测性能而不需要完全分离。主要思路是利用梯度说明找到两个模型焦点：1）其中一个焦点用于预测值，2）另一个焦点用于代理属性，然后对潜在码进行修正以减轻这些属性之间的相关性。",
    "tldr": "本文提出了一种新的公平性框架，通过梯度说明找到两个模型焦点进行去偏见处理，提高下游任务模型的预测性能。",
    "en_tdlr": "This paper proposes a novel fairness framework that leverages gradient-based explanation to find two model focuses, one for prediction and the other for proxy attributes, to perform debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models."
}