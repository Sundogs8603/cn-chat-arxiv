{
    "title": "Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])",
    "abstract": "We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\\ell_2$-regularised $\\ell_2$, $\\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an",
    "link": "http://arxiv.org/abs/2305.18974",
    "context": "Title: Asymptotic Characterisation of Robust Empirical Risk Minimisation Performance in the Presence of Outliers. (arXiv:2305.18974v1 [stat.ML])\nAbstract: We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\\ell_2$-regularised $\\ell_2$, $\\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an",
    "path": "papers/23/05/2305.18974.json",
    "total_tokens": 1044,
    "translated_title": "异常点存在时健壮经验风险最小化性能的渐进特性研究",
    "translated_abstract": "我们研究了高维健壮线性回归问题，当维度$d$和数据点数量$n$以固定比率$\\alpha=n/d$发散，并研究了包括异常点在内的数据模型。我们对使用$\\ell_2$ -正则化$\\ell_2$，$\\ell_1$，和 Huber 损失的经验风险最小化（ERM）性能提供了精确的渐近特性，这是解决这类问题的标准方法。我们关注性能的两个指标：具有异常点的相似数据集的泛化误差和原始无污染函数的估计误差。我们将结果与信息论贝叶斯最优估计界进行了比较。对于泛化误差，我们发现如果进行简单的校准并计算收敛速率，则最优正则化ERM在大样本复杂度限制下是渐近一致的。然而，对于估计误差，由于范数校准不匹配，我们表明估计器的一致性需要一个较强的收敛假设，这对问题的解决还需要进一步的研究。",
    "tldr": "该论文研究了包括异常点的高维健壮线性回归问题，提供了使用不同损失函数的精确渐近特性，对泛化误差进行了简单校准并计算了收敛速率，但由于范数校准不匹配，对估计误差的一致性需要一个较强的收敛假设。",
    "en_tdlr": "This paper studies robust linear regression in high-dimensions in the presence of outliers and provides exact asymptotics for the performance of empirical risk minimisation using different loss functions, calibrating for generalization error and computing the rates of convergence. However, due to a mismatch in norm calibration, strong convergence assumptions are needed for consistency of the estimator for estimation error."
}