{
    "title": "On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])",
    "abstract": "Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\\left[1,1,...,1\\right]$ vector, and (b) scaling of all vectors to the same norm of $\\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a",
    "link": "http://arxiv.org/abs/2305.02582",
    "context": "Title: On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])\nAbstract: Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\\left[1,1,...,1\\right]$ vector, and (b) scaling of all vectors to the same norm of $\\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a",
    "path": "papers/23/05/2305.02582.json",
    "total_tokens": 884,
    "translated_title": "关于LayerNorm在Transformers的Attention中表达能力的作用",
    "translated_abstract": "Layer Normalization（LayerNorm）是所有基于Transformer的模型中都具有的组件。在本文中，我们展示LayerNorm对随后的多头注意力层的表达能力至关重要，这与通常认为LayerNorm仅在前向传播期间归一化激活值和在反向传播期间归一化梯度的共识不同。我们考虑了LayerNorm的几何解释，并认为它由两个组成部分组成：（a）将输入向量投影到正交于$\\left[1,1,...,1\\right]$向量的$d-1$空间，并且（b）将所有向量缩放到相同的$\\sqrt{d}$范数。我们展示了每个组件对随后的Transformers中的注意力层都很重要：（a）投影允许注意机制创建一个关注所有键等量的注意查询，从而减轻了注意机制学习此操作的需要；（b）缩放使每个键都有可能接收到最高的注意力分配权重。",
    "tldr": "本文揭示了LayerNorm在Transformers的Attention层中有着至关重要的作用，通过对输入向量进行投影并对所有向量进行缩放，LayerNorm可以帮助注意力机制更好地处理输入。"
}