{
    "title": "Redundancy and Concept Analysis for Code-trained Language Models",
    "abstract": "arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \\textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9",
    "link": "https://arxiv.org/abs/2305.00875",
    "context": "Title: Redundancy and Concept Analysis for Code-trained Language Models\nAbstract: arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \\textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9",
    "path": "papers/23/05/2305.00875.json",
    "total_tokens": 844,
    "translated_title": "基于代码训练的语言模型的冗余和概念分析",
    "translated_abstract": "针对代码训练的语言模型在各种代码智能任务中表现出高效性。然而，由于计算瓶颈和内存限制，对于许多软件工程应用来说，它们可能难以训练和部署。为了解决这些问题，实施有效的策略需要更好地理解这些“黑匣子”模型。本文针对源代码模型进行了首次神经元级别分析，以识别潜在表示中的“重要”神经元。我们通过消除与给定任务高度相似或不相关的神经元来实现这一点。这种方法有助于我们了解哪些神经元和层可以被消除（冗余分析），以及网络中的重要代码属性位于何处（概念分析）。利用冗余分析，我们得出了与知识转移和模型优化应用相关的观察结果。",
    "tldr": "本文针对代码训练的语言模型进行冗余和概念分析，通过消除与给定任务不相关的神经元，帮助理解网络中哪些神经元和层可以被消除，并在哪里找到重要的代码属性。"
}