{
    "title": "Modified Gauss-Newton Algorithms under Noise. (arXiv:2305.10634v1 [math.OC])",
    "abstract": "Gauss-Newton methods and their stochastic version have been widely used in machine learning and signal processing. Their nonsmooth counterparts, modified Gauss-Newton or prox-linear algorithms, can lead to contrasting outcomes when compared to gradient descent in large-scale statistical settings. We explore the contrasting performance of these two classes of algorithms in theory on a stylized statistical example, and experimentally on learning problems including structured prediction. In theory, we delineate the regime where the quadratic convergence of the modified Gauss-Newton method is active under statistical noise. In the experiments, we underline the versatility of stochastic (sub)-gradient descent to minimize nonsmooth composite objectives.",
    "link": "http://arxiv.org/abs/2305.10634",
    "context": "Title: Modified Gauss-Newton Algorithms under Noise. (arXiv:2305.10634v1 [math.OC])\nAbstract: Gauss-Newton methods and their stochastic version have been widely used in machine learning and signal processing. Their nonsmooth counterparts, modified Gauss-Newton or prox-linear algorithms, can lead to contrasting outcomes when compared to gradient descent in large-scale statistical settings. We explore the contrasting performance of these two classes of algorithms in theory on a stylized statistical example, and experimentally on learning problems including structured prediction. In theory, we delineate the regime where the quadratic convergence of the modified Gauss-Newton method is active under statistical noise. In the experiments, we underline the versatility of stochastic (sub)-gradient descent to minimize nonsmooth composite objectives.",
    "path": "papers/23/05/2305.10634.json",
    "total_tokens": 778,
    "translated_title": "在噪声下修改的高斯牛顿算法",
    "translated_abstract": "高斯牛顿方法及其随机版本已广泛用于机器学习和信号处理中。它们的非光滑版本，修改的高斯牛顿或近端线性算法，在大规模统计设置中相对于梯度下降可以导致截然不同的结果。我们探索了这两类算法在理论上对一个理想化的统计示例和实验中学习问题（包括结构化预测）的截然不同的表现。在理论上，我们勾勒出在统计噪声下活跃的修改高斯牛顿法的二次收敛区域。在实验中，我们强调了随机（次）梯度下降优化非光滑复合目标的多功能性。",
    "tldr": "本文探讨了在大规模统计设置中高斯牛顿方法及其随机版本的性能和非光滑版本的修改高斯牛顿或近端线性算法的对比表现，并勾勒出在统计噪声下修改高斯牛顿法的二次收敛区域。",
    "en_tdlr": "In this paper, we explore the contrasting performance of Gauss-Newton methods and their stochastic versions with modified Gauss-Newton or prox-linear algorithms in large-scale statistical settings. We also delineate the regime where the quadratic convergence of the modified Gauss-Newton method is active under statistical noise."
}