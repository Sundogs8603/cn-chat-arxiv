{
    "title": "Faster Federated Learning with Decaying Number of Local SGD Steps. (arXiv:2305.09628v1 [cs.LG])",
    "abstract": "In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K > 1$ steps of SGD are performed on clients per round. In this work we propose decaying $K$ as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$. We analyse the convergence of FedAvg with decayi",
    "link": "http://arxiv.org/abs/2305.09628",
    "context": "Title: Faster Federated Learning with Decaying Number of Local SGD Steps. (arXiv:2305.09628v1 [cs.LG])\nAbstract: In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K > 1$ steps of SGD are performed on clients per round. In this work we propose decaying $K$ as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$. We analyse the convergence of FedAvg with decayi",
    "path": "papers/23/05/2305.09628.json",
    "total_tokens": 798,
    "translated_title": "具有衰减局部SGD步数的快速联邦学习",
    "translated_abstract": "在联邦学习中，连接在互联网上的客户端设备可以在不与中央服务器或其他客户端共享私人数据的情况下协同训练机器学习模型。FedAvg算法通过在客户端上执行局部训练并进行模型平均来训练单个全球模型。本文提出了一种随着训练的进行而衰减的K值方法，并分析了FedAvg在此情况下的收敛性。同时，在各种非独立同分布的数据集上实验验证了我们的方法的有效性。",
    "tldr": "本研究提出了一种具有衰减局部SGD步数的方法，可以提高联邦学习模型的最终性能，同时减少训练的时间和计算成本。"
}