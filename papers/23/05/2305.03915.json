{
    "title": "HateMM: A Multi-Modal Dataset for Hate Video Classification. (arXiv:2305.03915v1 [cs.CV])",
    "abstract": "Hate speech has become one of the most significant issues in modern society, having implications in both the online and the offline world. Due to this, hate speech research has recently gained a lot of traction. However, most of the work has primarily focused on text media with relatively little work on images and even lesser on videos. Thus, early stage automated video moderation techniques are needed to handle the videos that are being uploaded to keep the platform safe and healthy. With a view to detect and remove hateful content from the video sharing platforms, our work focuses on hate video detection using multi-modalities. To this end, we curate ~43 hours of videos from BitChute and manually annotate them as hate or non-hate, along with the frame spans which could explain the labelling decision. To collect the relevant videos we harnessed search keywords from hate lexicons. We observe various cues in images and audio of hateful videos. Further, we build deep learning multi-modal",
    "link": "http://arxiv.org/abs/2305.03915",
    "context": "Title: HateMM: A Multi-Modal Dataset for Hate Video Classification. (arXiv:2305.03915v1 [cs.CV])\nAbstract: Hate speech has become one of the most significant issues in modern society, having implications in both the online and the offline world. Due to this, hate speech research has recently gained a lot of traction. However, most of the work has primarily focused on text media with relatively little work on images and even lesser on videos. Thus, early stage automated video moderation techniques are needed to handle the videos that are being uploaded to keep the platform safe and healthy. With a view to detect and remove hateful content from the video sharing platforms, our work focuses on hate video detection using multi-modalities. To this end, we curate ~43 hours of videos from BitChute and manually annotate them as hate or non-hate, along with the frame spans which could explain the labelling decision. To collect the relevant videos we harnessed search keywords from hate lexicons. We observe various cues in images and audio of hateful videos. Further, we build deep learning multi-modal",
    "path": "papers/23/05/2305.03915.json",
    "total_tokens": 941,
    "translated_title": "HateMM：用于仇恨视频分类的多模态数据集",
    "translated_abstract": "仇恨言论已成为现代社会中最重要的问题之一，对线上和线下世界都产生了影响。由于这一点，近来对仇恨言论的研究引起了广泛关注。然而，大部分工作主要集中在文本媒体上，对图像和视频的研究相对较少。因此，需要早期的自动化视频审核技术来处理被上传的视频，以保持平台的安全和健康。我们的工作旨在使用多模式检测仇恨视频并从视频共享平台中检测和删除仇恨内容。为此，我们从BitChute中策划了约43个小时的视频，并手动注释了它们是否属于仇恨或非仇恨，并提供了可以解释标注决策的帧跨度。我们利用仇恨词汇表的搜索关键字收集相关的视频，观察到在仇恨视频的图像和音频中存在各种线索。此外，我们构建了深度学习的多模态模型。",
    "tldr": "仇恨言论的研究主要集中在文本媒体上，使用多模态检测仇恨视频并从视频共享平台中删除仇恨内容成为重点。因为仇恨视频的图像和音频中存在各种线索，因此我们构建了深度学习的多模态模型进行研究。",
    "en_tdlr": "Hate speech research mostly focused on text media, while this work focuses on hate video detection using multi-modalities to remove hateful content from video sharing platforms with deep learning multi-modal models, due to various cues observed in the images and audio of hateful videos."
}