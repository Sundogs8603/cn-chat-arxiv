{
    "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)",
    "abstract": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \\textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense re",
    "link": "http://arxiv.org/abs/2305.14825",
    "context": "Title: Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)\nAbstract: The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \\textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense re",
    "path": "papers/23/05/2305.14825.json",
    "total_tokens": 923,
    "translated_title": "大型语言模型是上下文语义推理器而不是符号推理器",
    "translated_abstract": "近年来，大型语言模型(Large Language Models, LLMs)的出现引起了自然语言和机器学习界的极大兴趣，其出色的应用性能备受推崇。然而，LLMs在上下文语境下的推理能力背后的机制仍然不清楚。本文提出我们的假设：在推理过程中，语言序列中学习到的\"语义\"发挥了至关重要的作用。与人类的符号推理过程不同，LLMs的语义表示可以在语言序列中建立强连接，因此组成一个表面逻辑链。为了测试我们的假设，我们将语义从语言推理过程中分离出来，并评估了三种推理能力：演绎、归纳和拟合。我们的发现表明，在上下文语境中，语义对LLMs的推理能力起着至关重要的作用，当语义与常识一致时，LLMs的表现更佳，但在解决符号或反常识推理问题上会出现困难。",
    "tldr": "本文研究了大型语言模型的内部机制，发现在上下文语境中，语言序列的语义应起到至关重要的作用，与人类符号推理不同，大型语言模型可以在语言序列中建立强连接，并组成一个表面逻辑链。"
}