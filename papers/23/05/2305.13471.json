{
    "title": "Fast Convergence in Learning Two-Layer Neural Networks with Separable Data. (arXiv:2305.13471v1 [cs.LG])",
    "abstract": "Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds.",
    "link": "http://arxiv.org/abs/2305.13471",
    "context": "Title: Fast Convergence in Learning Two-Layer Neural Networks with Separable Data. (arXiv:2305.13471v1 [cs.LG])\nAbstract: Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds.",
    "path": "papers/23/05/2305.13471.json",
    "total_tokens": 805,
    "translated_title": "基于可分数据的双层神经网络的快速收敛",
    "translated_abstract": "在具有可分数据的线性分类器上，归一化梯度下降在加速指数尾部损失函数（包括指数和逻辑损失）收敛方面取得了显着成功。本文通过研究归一化 GD 对双层神经网络的作用超越了线性模型。对于指数尾部损失，我们证明了使用归一化 GD 导致训练损失对全局最优解的线性收敛速率。这是通过展示一定的梯度自限制条件和对数利普希茨特性实现的。我们还通过算法稳定性分析研究了用于凸目标的归一化 GD 的泛化。特别是，我们通过建立有限时间的泛化边界证明了训练期间归一化 GD 不会过拟合。",
    "tldr": "本文研究了使用归一化梯度下降算法在双层神经网络中进行训练的方法，证明了对于指数尾部损失函数，其收敛速率为线性，同时建立了有限时间的泛化边界。",
    "en_tdlr": "This paper studies the use of normalized gradient descent algorithm for training two-layer neural networks and proves that the convergence rate for exponentially-tailed loss functions is linear. The study also establishes finite-time generalization bounds."
}