{
    "title": "Soft Prompt Decoding for Multilingual Dense Retrieval. (arXiv:2305.09025v1 [cs.IR])",
    "abstract": "In this work, we explore a Multilingual Information Retrieval (MLIR) task, where the collection includes documents in multiple languages. We demonstrate that applying state-of-the-art approaches developed for cross-lingual information retrieval to MLIR tasks leads to sub-optimal performance. This is due to the heterogeneous and imbalanced nature of multilingual collections -some languages are better represented in the collection and some benefit from large-scale training data. To address this issue, we present KD-SPD, a novel soft prompt decoding approach for MLIR that implicitly \"translates\" the representation of documents in different languages into the same embedding space. To address the challenges of data scarcity and imbalance, we introduce a knowledge distillation strategy. The teacher model is trained on rich English retrieval data, and by leveraging bi-text data, our distillation framework transfers its retrieval knowledge to the multilingual document encoder. Therefore, our",
    "link": "http://arxiv.org/abs/2305.09025",
    "context": "Title: Soft Prompt Decoding for Multilingual Dense Retrieval. (arXiv:2305.09025v1 [cs.IR])\nAbstract: In this work, we explore a Multilingual Information Retrieval (MLIR) task, where the collection includes documents in multiple languages. We demonstrate that applying state-of-the-art approaches developed for cross-lingual information retrieval to MLIR tasks leads to sub-optimal performance. This is due to the heterogeneous and imbalanced nature of multilingual collections -some languages are better represented in the collection and some benefit from large-scale training data. To address this issue, we present KD-SPD, a novel soft prompt decoding approach for MLIR that implicitly \"translates\" the representation of documents in different languages into the same embedding space. To address the challenges of data scarcity and imbalance, we introduce a knowledge distillation strategy. The teacher model is trained on rich English retrieval data, and by leveraging bi-text data, our distillation framework transfers its retrieval knowledge to the multilingual document encoder. Therefore, our",
    "path": "papers/23/05/2305.09025.json",
    "total_tokens": 855,
    "tldr": "本文介绍了一种适用于多语言信息检索的软提示解码方法KD-SPD，并提出了知识蒸馏策略来解决数据稀缺和不平衡的挑战。"
}