{
    "title": "Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)",
    "abstract": "As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on",
    "link": "http://arxiv.org/abs/2305.15328",
    "context": "Title: Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)\nAbstract: As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on",
    "path": "papers/23/05/2305.15328.json",
    "total_tokens": 933,
    "translated_title": "文本到图像生成与评估的可视化编程",
    "translated_abstract": "随着大型语言模型在许多领域表现出卓越性能，最近的研究采用语言模型作为视觉任务中的视觉模块的控制器。虽然现有的工作集中在为语言模型提供视觉理解能力，但我们提出了两种新颖的可解释/可解释的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，我们引入了VPGen，一种可解释的逐步T2I生成框架，将T2I生成分解为三个步骤：对象/计数生成、布局生成和图像生成。我们使用语言模型处理前两个步骤（对象/计数生成和布局生成），通过在文本布局对上微调它。我们的逐步T2I生成框架提供了比端到端模型更强的空间控制能力，而端到端模型是这个任务的主要方法。此外，我们利用了预训练语言模型的世界知识，克服了以前的布局引导T2I作品的局限。",
    "tldr": "本文提出了两种新颖的可解释/可理解的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，引入了VPGen，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤，通过语言模型处理前两个步骤，提供了比端到端模型更强的空间控制能力，并利用了预训练语言模型的世界知识。"
}