{
    "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
    "link": "http://arxiv.org/abs/2305.14279",
    "context": "Title: Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
    "path": "papers/23/05/2305.14279.json",
    "total_tokens": 816,
    "translated_title": "LLM的多步推理中的两个自洽失败",
    "translated_abstract": "大型语言模型（LLM）在各种上下文为基础的少样本任务上取得了广泛成功，但这种成功通常是通过正确性而不是一致性来评估的。我们认为在解决由多个子步骤的答案组成的任务的多步推理中，自洽性是一个重要的标准。我们提出了两种对于多步推理特别重要的自洽性类型：假设自洽性（模型在假设的其他上下文中的输出预测能力）和组合自洽性（当将中间子步骤替换为模型对这些步骤的输出时，模型的最终输出的一致性）。我们证明了GPT-3/-4模型的多个变体在多种任务上都表现出了低一致性率。",
    "tldr": "本论文研究了大型语言模型在多步推理中的自洽性问题，提出了假设自洽性和组合自洽性两个重要特性，并发现GPT-3/-4模型在这两方面都表现出了较差的一致性。"
}