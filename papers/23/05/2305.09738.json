{
    "title": "CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])",
    "abstract": "Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and",
    "link": "http://arxiv.org/abs/2305.09738",
    "context": "Title: CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])\nAbstract: Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and",
    "path": "papers/23/05/2305.09738.json",
    "total_tokens": 858,
    "translated_title": "CQural：一种基于混合CNN的量子持续机器学习架构",
    "translated_abstract": "在增量式学习中训练机器学习模型不仅很重要，而且是实现人工通用智能的有效方法。然而，当前的神经网络模型在持续学习方面很容易出现灾难性遗忘的问题。许多研究人员提出了许多技术来减少神经网络的遗忘影响，但是所有的技术都是在经典学习上研究的，很少有人关注机器学习模型结构的改变。在本研究中，我们展示了使用新的混合经典 - 量子神经网络可以避免持续学习中的灾难性遗忘，并解释了哪些功能对于分类最重要。此外，我们还声称如果使用这些解释来训练模型，则会获得更好的性能。",
    "tldr": "本文展示了一种基于混合CNN的量子持续机器学习架构，可以通过解释哪些特征对于分类最重要来避免遗忘，并声称如果使用这些解释来训练模型，则会获得更好的性能。",
    "en_tdlr": "This paper presents a novel hybrid classical-quantum neural network architecture based on CNN for quantum continual machine learning, which can avoid catastrophic forgetting by explaining the most important features for classification. It is also claimed that better performance can be achieved if the model is trained with these explanations."
}