{
    "title": "Minimum-Risk Recalibration of Classifiers. (arXiv:2305.10886v1 [cs.LG])",
    "abstract": "Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order $\\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge of label",
    "link": "http://arxiv.org/abs/2305.10886",
    "context": "Title: Minimum-Risk Recalibration of Classifiers. (arXiv:2305.10886v1 [cs.LG])\nAbstract: Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order $\\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge of label",
    "path": "papers/23/05/2305.10886.json",
    "total_tokens": 995,
    "translated_title": "分类器最小风险重新校准",
    "translated_abstract": "重新校准概率分类器对于提高预测模型的可靠性和准确性至关重要。尽管已经开发了许多重新校准算法，但仍缺乏一个综合的理论来整合校准和锐度（这对于保持预测力至关重要）。在本文中，我们在均方误差（MSE）分解框架内介绍了最小风险重新校准的概念，提供了一种评估和重新校准概率分类器的原则性方法。利用这个框架，我们分析了均匀质量分桶（UMB）重新校准方法，并建立了一个有限样本风险上界，其顺序为$\\tilde{O}(B/n+1/B^2)$，其中$B$是桶的数量，$n$是样本大小。通过平衡校准和锐度，我们进一步确定了UMB的最优桶数与$n^{1/3}$成比例，从而产生了大约$O(n^{-2/3})$的风险界。此外，我们还应对了标签稀少问题。",
    "tldr": "本文介绍了最小风险重新校准的概念，在均方误差分解框架内提供了一种原则性方法，用于评估和重新校准概率分类器，并通过平衡校准和锐度确定了最优的桶数，从而产生了大约$O(n^{-2/3})$的风险上界。",
    "en_tdlr": "This paper introduces the concept of minimum-risk recalibration within the mean-squared-error framework, offering a principled approach for evaluating and recalibrating probabilistic classifiers. The optimal number of bins for uniform-mass binning scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$, achieved by balancing calibration and sharpness."
}