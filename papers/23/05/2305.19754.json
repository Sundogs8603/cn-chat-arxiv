{
    "title": "Sentence Simplification Using Paraphrase Corpus for Initialization. (arXiv:2305.19754v1 [cs.CL])",
    "abstract": "Neural sentence simplification method based on sequence-to-sequence framework has become the mainstream method for sentence simplification (SS) task. Unfortunately, these methods are currently limited by the scarcity of parallel SS corpus. In this paper, we focus on how to reduce the dependence on parallel corpus by leveraging a careful initialization for neural SS methods from paraphrase corpus. Our work is motivated by the following two findings: (1) Paraphrase corpus includes a large proportion of sentence pairs belonging to SS corpus. (2) We can construct large-scale pseudo parallel SS data by keeping these sentence pairs with a higher complexity difference. Therefore, we propose two strategies to initialize neural SS methods using paraphrase corpus. We train three different neural SS methods with our initialization, which can obtain substantial improvements on the available WikiLarge data compared with themselves without initialization.",
    "link": "http://arxiv.org/abs/2305.19754",
    "context": "Title: Sentence Simplification Using Paraphrase Corpus for Initialization. (arXiv:2305.19754v1 [cs.CL])\nAbstract: Neural sentence simplification method based on sequence-to-sequence framework has become the mainstream method for sentence simplification (SS) task. Unfortunately, these methods are currently limited by the scarcity of parallel SS corpus. In this paper, we focus on how to reduce the dependence on parallel corpus by leveraging a careful initialization for neural SS methods from paraphrase corpus. Our work is motivated by the following two findings: (1) Paraphrase corpus includes a large proportion of sentence pairs belonging to SS corpus. (2) We can construct large-scale pseudo parallel SS data by keeping these sentence pairs with a higher complexity difference. Therefore, we propose two strategies to initialize neural SS methods using paraphrase corpus. We train three different neural SS methods with our initialization, which can obtain substantial improvements on the available WikiLarge data compared with themselves without initialization.",
    "path": "papers/23/05/2305.19754.json",
    "total_tokens": 823,
    "translated_title": "使用释义语料库进行初始化的句子简化",
    "translated_abstract": "基于序列到序列框架的神经句子简化方法已经成为句子简化(SS)任务的主流方法。不幸的是，这些方法目前受到平行SS语料库的稀缺性的限制。在本文中，我们关注如何通过利用释义语料库对神经SS方法进行精心初始化，从而降低对平行语料库的依赖。我们的工作得到以下两个发现的启发：(1)释义语料库包括大量属于SS语料库的句子对。(2)我们可以通过保留这些句子对中更高复杂度差异的句子对来构建大规模的伪并行SS数据。因此，我们提出了两种使用释义语料库初始化神经SS方法的策略。我们用初始化训练了三种不同的神经SS方法，在可用的WikiLarge数据上与自身没有初始化相比，可以获得显着的改进。",
    "tldr": "本文提出了两种使用释义语料库对神经句子简化方法进行初始化的策略，从而降低对平行语料库的依赖，并在WikiLarge数据上获得了显着的改进。",
    "en_tdlr": "This paper proposes two strategies for initializing neural sentence simplification methods using paraphrase corpus to reduce the reliance on parallel corpus, and significant improvements are obtained on the WikiLarge dataset compared to non-initialized methods."
}