{
    "title": "FAVAS: Federated AVeraging with ASynchronous clients. (arXiv:2305.16099v1 [cs.LG])",
    "abstract": "In this paper, we propose a novel centralized Asynchronous Federated Learning (FL) framework, FAVAS, for training Deep Neural Networks (DNNs) in resource-constrained environments. Despite its popularity, ``classical'' federated learning faces the increasingly difficult task of scaling synchronous communication over large wireless networks. Moreover, clients typically have different computing resources and therefore computing speed, which can lead to a significant bias (in favor of ``fast'' clients) when the updates are asynchronous. Therefore, practical deployment of FL requires to handle users with strongly varying computing speed in communication/resource constrained setting. We provide convergence guarantees for FAVAS in a smooth, non-convex environment and carefully compare the obtained convergence guarantees with existing bounds, when they are available. Experimental results show that the FAVAS algorithm outperforms current methods on standard benchmarks.",
    "link": "http://arxiv.org/abs/2305.16099",
    "context": "Title: FAVAS: Federated AVeraging with ASynchronous clients. (arXiv:2305.16099v1 [cs.LG])\nAbstract: In this paper, we propose a novel centralized Asynchronous Federated Learning (FL) framework, FAVAS, for training Deep Neural Networks (DNNs) in resource-constrained environments. Despite its popularity, ``classical'' federated learning faces the increasingly difficult task of scaling synchronous communication over large wireless networks. Moreover, clients typically have different computing resources and therefore computing speed, which can lead to a significant bias (in favor of ``fast'' clients) when the updates are asynchronous. Therefore, practical deployment of FL requires to handle users with strongly varying computing speed in communication/resource constrained setting. We provide convergence guarantees for FAVAS in a smooth, non-convex environment and carefully compare the obtained convergence guarantees with existing bounds, when they are available. Experimental results show that the FAVAS algorithm outperforms current methods on standard benchmarks.",
    "path": "papers/23/05/2305.16099.json",
    "total_tokens": 852,
    "translated_title": "FAVAS: 带有异步客户端的联邦平均的新型中心化框架",
    "translated_abstract": "本文提出了一种新型的中心化异步联邦学习框架FAVAS，用于在资源有限的环境下训练深度神经网络。尽管联邦学习越来越受欢迎，但在大型无线网络上伸缩同步通信变得越来越困难。此外，由于客户端通常具有不同的计算资源和计算速度，异步更新可能会导致显着的偏差（对“快速”客户端更有利）。因此，FL的实际部署需要处理在通信/资源受限的环境中具有强烈变化的计算速度的用户。我们提供了FAVAS在平滑的非凸环境中的收敛性保证，并仔细比较了获得的收敛保证与现有边界（如果有）的差异。实验结果表明，FAVAS算法在标准基准测试中优于当前方法。",
    "tldr": "本研究提出了FAVAS算法，是一种用于在资源有限环境下训练DNNs的新型中心化异步联邦学习框架。实验结果表明FAVAS算法优于当前方法。",
    "en_tdlr": "FAVAS algorithm, a novel centralized asynchronous federated learning framework for training DNNs in resource-constrained environments, is proposed and outperforms current methods according to experimental results."
}