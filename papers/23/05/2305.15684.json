{
    "title": "Perturbation-based Self-supervised Attention for Attention Bias in Text Classification. (arXiv:2305.15684v1 [cs.CL])",
    "abstract": "In text classification, the traditional attention mechanisms usually focus too much on frequent words, and need extensive labeled data in order to learn. This paper proposes a perturbation-based self-supervised attention approach to guide attention learning without any annotation overhead. Specifically, we add as much noise as possible to all the words in the sentence without changing their semantics and predictions. We hypothesize that words that tolerate more noise are less significant, and we can use this information to refine the attention distribution. Experimental results on three text classification tasks show that our approach can significantly improve the performance of current attention-based models, and is more effective than existing self-supervised methods. We also provide a visualization analysis to verify the effectiveness of our approach.",
    "link": "http://arxiv.org/abs/2305.15684",
    "context": "Title: Perturbation-based Self-supervised Attention for Attention Bias in Text Classification. (arXiv:2305.15684v1 [cs.CL])\nAbstract: In text classification, the traditional attention mechanisms usually focus too much on frequent words, and need extensive labeled data in order to learn. This paper proposes a perturbation-based self-supervised attention approach to guide attention learning without any annotation overhead. Specifically, we add as much noise as possible to all the words in the sentence without changing their semantics and predictions. We hypothesize that words that tolerate more noise are less significant, and we can use this information to refine the attention distribution. Experimental results on three text classification tasks show that our approach can significantly improve the performance of current attention-based models, and is more effective than existing self-supervised methods. We also provide a visualization analysis to verify the effectiveness of our approach.",
    "path": "papers/23/05/2305.15684.json",
    "total_tokens": 810,
    "translated_title": "基于干扰的自我监督注意力用于文本分类中的注意偏差",
    "translated_abstract": "在文本分类中，传统的注意力机制通常过于关注频繁出现的单词，并且需要大量已注释的数据才能学习。本文提出了一种基于干扰的自我监督注意力方法来引导注意力学习，无需任何注释开销。具体而言，我们尽可能地添加噪声到句子中的所有单词，而不改变它们的语义和预测。我们假设能够容忍更多噪声的单词意义更不重要，并且可以使用该信息来优化注意力分布。在三个文本分类任务上的实验结果表明，我们的方法可以显著提高当前基于注意力的模型的性能，且比现有的自我监督方法更有效。我们还提供了可视化分析，以验证我们方法的有效性。",
    "tldr": "本论文提出了一种基于干扰的自我监督注意力方法来引导注意力学习，无需任何注释开销，能够在三个文本分类任务中显著提高当前基于注意力的模型的性能，该方法比现有的自我监督方法更有效。",
    "en_tdlr": "This paper proposes a perturbation-based self-supervised attention approach to guide attention learning without any annotation overhead, which can significantly improve the performance of current attention-based models on three text classification tasks and is more effective than existing self-supervised methods."
}